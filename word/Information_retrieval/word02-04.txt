in this paper we study how we can design an effective parallel crawler a the size of the web grows it becomes imperative to parallelize a crawling process in order to finish downloading page in a reasonable amount of time we first propose multiple architecture for a parallel crawler and identify fundamental issue related to parallel crawling based on this understanding we then propose metric to evaluate a parallel crawler and compare the proposed architecture using million page collected from the web our result clarify the relative merit of each architecture and provide a good guideline on when to adopt which architecture 
an evaluation methodology that target ineffective topic is needed to support research on obtaining more consistent retrieval across topic using average value of traditional evaluation measure is not an appropriate methodology because it emphasizes effective topic poorly performing topic score are by definition small and they are therefore difficult to distinguish from the noise inherent in retrieval evaluation we examine two new measure that emphasize a system s worst topic while these measure focus on different aspect of retrieval behavior than traditional measure the measure are le stable than traditional measure and the margin of error associated with the new measure is large relative to the observed difference in score 
activity such a web service and the semantic web are working to create a web of distributed machine understandable data in this paper we present an application called semantic search which is built on these supporting technology and is designed to improve traditional web searching we provide an overview of tap the application framework upon which the semantic search is built we describe two implemented semantic search system which based on the denotation of the search query augment traditional search result with relevant data aggregated from distributed source we also discus some general issue related to searching and the semantic web and outline how an understanding of the semantics of the search term can be used to provide better result 
we discus a retrieval model in which the task is to complete a sentence given an initial fragment and given an application specific document collection this model is motivated by administrative and call center environment in which user have to write document with a certain repetitiveness we formulate the problem setting and discus appropriate performance metric we present an index based retrieval algorithm and a cluster based approach and evaluate our algorithm using collection of email that have been written by two distinct service center 
this paper investigates how the vision of the semantic web can be carried overto the realm of email we introduce a general notion of semantice mail in which an email message consists of an rdf query or update coupled with corresponding explanatory text semantic email open the door to a wide range of automated email mediated application with formally guaranteed property in particular this paper introduces a broad class of semantic email process for example consider the process of sending an email to a program committee asking who will attend the pc dinner automatically collecting the response and tallying them up we define bothlogical and decision theoretic model where an email process ismodeled a a set of update to a data set on which we specify goal via certain constraint or utility we then describe a set ofinference problem that arise while trying to satisfy these goal and analyze their computational tractability in particular weshow that for the logical model it is possible to automatically infer which email response are acceptable w r t a set ofconstraints in polynomial time and for the decision theoreticmodel it is possible to compute the optimal message handling policy in polynomial time finally we discus our publicly available implementation of semantic email and outline research challenge inthis realm 
finding page on the web that are similar to a query page related page is an important component of modern search engine a variety of strategy have been proposed for answering related page query but comparative evaluation by user study is expensive especially when large strategy space must be searched e g when tuning parameter we present a technique for automatically evaluating strategy using web hierarchy such a open directory in place of user feedback we apply this evaluation methodology to a mix of document representation strategy including the use of text anchor text and link we discus the relative advantage and disadvantage of the various approach examined finally we describe how to efficiently construct a similarity index out of our chosen strategy and provide sample result from our index 
web link analysis ha been proved to provide significant enhancement to the precision of web search in practice the pagerank algorithm which is used in google search engine play an important role on improving the quality of it resuls by employing the explicit hyperlink structure among the web page the prestige of web page defined by pagerank is purely derived from surfer random walk on the web graph without textual content content consideration however in the practical sense user surfing behavior is far from random jumping in this paper we present a unified model for a more accurate page rank user s surfing is guided by a probabilistic model that is based on literal matching between connected page the result show that our proposed ranking algorithm do perform better than the original pagerank 
we discus the definition of key for xml document paying particular attention to the concept of a relative key which is commonly used in hierarchically structured document and scientific database 
providing knowledge worker with access to expert and community of practice is central to sharing expertise and crucial to organizational performance adaptation and even survival this paper cover ongoing research to develop an expert locator prototype a model based system for detecting expert and broader community of practice the underlying expertise model is extensible and support aggregation of evidence across diverse source the prototype is being used to locate critical expertise in key project area and current evaluation indicates it potential effectiveness 
cosine pivoted document length normalization ha reached a point of stability where many researcher indiscriminately apply a specific value of regardless of the collection our effort however demonstrate that applying this specific value without tuning for the document collection degrades average precision by a much a 
we present a framework in which probabilistic model for textual and visual information retrieval can be integrated seamlessly the framework facilitates searching for imagery using textual description and visual example simultaneously the underlying language model for text and gaussian mixture model for image have proven successful in various retrieval task 
we develop a method for predicting query performance by computing the relative entropy between a query language model and the corresponding collection language model the resulting clarity score measure the coherence of the language usage in document whose model are likely to generate the query we suggest that clarity score measure the ambiguity of a query with respect to a collection of document and show that they correlate positively with average precision in a variety of trec test set thus the clarity score may be used to identify ineffective query on average without relevance information we develop an algorithm for automatically setting the clarity score threshold between predicted poorly performing query and acceptable query and validate it using trec data in particular we compare the automatic threshold to optimum threshold and also check how frequently result a good are achieved in sampling experiment that randomly assign query to the two class 
the success of the semantic web crucially depends on the easy creation integration and use of semantic data for this purpose we consider an integration scenario that defies core assumption of current metadata construction method we describe a framework of metadata creation when web page are generated from a database and the database owner is cooperatively participating in the semantic web this lead u to the definition of ontology mapping rule by manual semantic annotation and the usage of the mapping rule and of web service for semantic query in order to create metadata the framework combine the presentation layer with the data description layer in contrast to conventional annotation which remains at the presentation layer therefore we refer to the framework a deep annotation we consider deep annotation a particularly valid because i web page generated from database outnumber static web page ii annotation of web page may be a very intuitive way to create semantic data from a database and iii data from database should not be materialized a rdf file it should remain where it can be handled most efficiently in it database 
we consider the problem of modeling annotated data data with multiple type where the instance of one type such a a caption serf a a description of the other type such a an image we describe three hierarchical probabilistic mixture model which aim to describe such data culminating in correspondence latent dirichlet allocation a latent variable model that is effective at modeling the joint distribution of both type and the conditional distribution of the annotation given the primary type we conduct experiment on the corel database of image and caption assessing performance in term of held out likelihood automatic annotation and text based image retrieval 
it is difficult to serialize an rdf graph a a humanly readable rdf xml document this paper describes the approach taken in jena in which a design pattern of guarded procedure invoked using top down recursive descent is used each procedure corresponds to a grammar rule the guard make the choice about the applicability of the production this approach is seen to correspond closely to the design of an ll k parser and a theoretical justification of this correspondence is found in universal algebra 
transitive retrieval and triangulation have been proposed a way to improve cross language retrieval quality when translation resource have poor lexical coverage we demonstrate that cross language retrieval is viable for european language with no translation resource at all that transitive retrieval without translation doe not suffer the drop off in retrieval quality sometimes reported for transitive retrieval with translation and that triangulation that combine multiple transitive run with no translation can boost performance over direct translation free retrieval 
in this paper we present a method based on document probe to quantify and diagnose topic structure distinguishing topic a monolithic structured or diffuse the method also yield a structure analysis that can be used directly to optimize filter classifier creation preliminary result illustrate the predictive value of the approach on trec reuters topic 
in the original pagerank algorithm for improving the ranking of search query result a single pagerank vector is computed using the link structure of the web to capture the relative importance of web page independent of any particular search query to yield more accurate search result we propose computing a set of pagerank vector biased using a set of representative topic to capture more accurately the notion of importance with respect to a particular topic by using these precomputed biased pagerank vector to generate query specific importance score for page at query time we show that we can generate more accurate ranking than with a single generic pagerank vector for ordinary keyword search query we compute the topic sensitive pagerank score for page satisfying the query using the topic of the query keywords for search done in context e g when the search query is performed by highlighting word in a web page we compute the topic sensitive pagerank score using the topic of the context in which the query appeared 
an empirical study ha been conducted investigating the relationship between the performance of an aspect based language model in term of perplexity and the corresponding information retrieval performance obtained it is observed on the corpus considered that the perplexity of the language model ha a systematic relationship with the achievable precision recall performance though it is not statistically significant 
this paper describes a question answering system that is designed to capitalize on the tremendous amount of data that is now available online most question answering system use a wide variety of linguistic resource we focus instead on the redundancy available in large corpus a an important resource we use this redundancy to simplify the query rewrite that we need to use and to support answer mining from returned snippet our system performs quite well given the simplicity of the technique being utilized experimental result show that question answering accuracy can be greatly improved by analyzing more and more matching passage simple passage ranking and n gram extraction technique work well in our system making it efficient to use with many backend retrieval engine 
if an e service approach to electronic commerce is to become widespread standardisation of ontology message content and message protocol will be necessary in this paper we present a lifecycle of a business to business e commerce interaction and show how the semantic web can support a service description language that can be used throughout this lifecycle by using daml we develop a service description language sufficiently expressive and flexible to be used not only in advertisement but also in matchmaking query negotiation proposal and agreement we also identify which operation must be carried out on this description language if the b b lifecycle is to be fully supported we do not propose specific standard protocol but instead argue that our operator are able to support a wide variety of interaction protocol and so will be fundamental irrespective of which protocol are finally adopted 
much attention ha been accorded to location based service and location tracking a necessary component in active trigger based lb application tracking the location of a large population of moving object requires very high update and query performance of the underlying spatial index in this paper we investigate the performance and scalability of three main memory based spatial indexing method under dynamic update and query load an r tree a zb tree and an array hashtable method by leveraging the locus performance evaluation testbed and the city simulator dynamic spatial data generator we are able to demonstrate the scalability of these method and determine the maximum population size supported by each method a useful parameter for capacity planning by wireless carrier 
this paper investigates the pre condition for successful combination of document representation formed from structural markup for the task of known item search a this task is very similar to work in meta search and data fusion we adapt several hypothesis from those research area and investigate them in this context to investigate these hypothesis we present a mixture based language model and also examine many of the current meta search algorithm we find that compatible output from system is important for successful combination of document representation we also demonstrate that combining low performing document representation can improve performance but not consistently we find that the technique best suited for this task are robust to the inclusion of poorly performing document representation we also explore the role of variance of result across system and it impact on the performance of fusion with the surprising result that the correct document have higher variance across document representation than highly ranking incorrect document 
many web information service utilize technique of information extraction ie to collect important fact from the web to create more advanced service one possible method is to discover thematic information from the collected fact through text classification however most conventional text classification technique rely on manual labelled corpus and are thus ill suited to cooperate with web information service with open domain in this work we present a system named liveclassifier that can automatically train classifiersthrough web corpus based on user defined topic hierarchy due to it flexibility and convenience liveclassifier can be easily adapted for various purpose new web information service can be created to fully exploit it human user can use it to create classifier for their personal application the effectiveness of classifier created by liveclassifier is well supportedby empirical evidence 
retrieval mechanism are frequently compared by computing the respective average score for some effectiveness metric across a common set of information need or topic with researcher concluding one method is superior based on those average since comparative retrieval system behavior is known to be highly variable across topic good experimental design requires that a sufficient number of topic be used in the test this paper us trec result to empirically derive error rate based on the number of topic used in a test and the observed difference in the average score the error rate quantify the likelihood that a different set of topic of the same size would lead to a different conclusion we directly compute error rate for topic set up to size and extrapolate those rate for larger topic set size the error rate found are larger than anticipated indicating researcher need to take care when concluding one method is better than another especially if few topic are used 
while being successful in providing keyword based access to web page commercial search portal still lack the ability to answer question expressed in a natural language we present a probabilistic approach to automated question answering on the web based on trainable pattern answer triangulation and semantic filtering in contrast to the other shallow approach our approach is entirely self learning it doe not require any manually created scoring and filtering rule while still performing comparably it also performs better than other fully trainable approach 
an important issue in the dissemination of time varying web data such a sport score and stock price is the maintenance of temporal coherency in the case of server adhering to the http protocol client need to frequently pull the data based on the dynamic of the data and a user s coherency requirement in contrast server that posse push capability maintain state information pertaining to client and push only those change that are of interest to a user these two canonical technique have complementary property with respect to the level of temporal coherency maintained communication overhead state space overhead and loss of coherency due to server failure in this paper we show how to combine push and pull based technique to achieve the best feature of both approach our combined technique tailor the dissemination of data from server to client based on the capability and load at server and proxy and client coherency requirement our experimental result demonstrate that such adaptive data dissemination is essential to meet diverse temporal coherency requirement to be resilient to failure and for the efficient and scalable utilization of server and network resource 
a problem facing many textbook author including one of the author of this paper is the inevitable delay between new advance in the subject area and their incorporation in a new paper edition of the textbook this mean that some textbook are quickly considered out of date particularly in active technological area such a the web even though the idea presented in the textbook are still valid and important to the community this paper describes our approach to building a companion website for the textbook hypermedia and the web an engineering approach we use bloom s taxonomy of educational objective to critically evaluate a number of authoring and presentation technique used in existing companion website and adapt these technique to create our own companion website using semantic web technology in order to overcome the identified weakness finally we discus a potential model of future companion website in the context of an e publishing e commerce semantic web service scenario 
content extraction signature ce enable selective disclosure of verifiable content provide privacy for blinded content and enable the signer to specify the content the document owner is allowed to extract or blind combined these property give what we call ce functionality in this paper we describe our work in developing custom transform algorithm to expand the functionality of an xml signature to include ce functionality in xml signature core validation we also describe a custom revocation mechanism and our implementation for non xml content where the custom transforms are dynamically loaded demonstrating that custom signing and verification is not constrained to a closed system through the use of dynamic loading we show that a verifier can still verify an xml signature compliant signature even though a custom signature wa produced 
the semantic web envisions a world wide web in which data is described with rich semantics and application can pose complex query to this point researcher have defined new language for specifying meaning for concept and developed technique for reasoning about them using rdf a the data model to flourish the semantic web need to be able to accommodate the huge amount of existing data and the application operating on them to achieve this we are faced with two problem first most of the world s data is available not in rdf but in xml xml and the application consuming it rely not only on the domain structure of the data but also on it document structure hence to provide interoperability between such source we must map between both their domain structure and their document structure second data management practitioner often prefer to exchange data through local point to point data translation rather than mapping to common mediated schema or ontology this paper describes the piazza system which address these challenge piazza offer a language for mediating between data source on the semantic web which map both the domain structure and document structure piazza also enables interoperation of xml data with rdf data that is accompanied by rich owl ontology mapping in piazza are provided at a local scale between small set of node and our query answering algorithm is able to chain set mapping together to obtain relevant data from across the piazza network we also describe an implemented scenario in piazza and the lesson we learned from it 
information retrieval using word sens is emerging a a good research challenge on semantic information retrieval in this paper we propose a new method using word sens in information retrieval root sense tagging method this method assigns coarse grained word sens defined in wordnet to query term and document term by unsupervised way using co occurrence information constructed automatically our sense tagger is crude but performs consistent disambiguation by considering only the single most informative word a evidence to disambiguate the target word we also allow multiple sense assignment to alleviate the problem caused by incorrect disambiguation experimental result on a large scale trec collection show that our approach to improve retrieval effectiveness is successful while most of the previous work failed to improve performance even on small text collection our method also show promising result when is combined with pseudo relevance feedback and state of the art retrieval function such a bm 
we address the problem of integrating object from a source taxonomy into a master taxonomy this problem is not only currently pervasive on the web but also important to the emerging semantic web a straightforward approach to automating this process would be to train a classifier for each category in the master taxonomy and then classify object from the source taxonomy into these category in this paper we attempt to use a powerful classification method support vector machine svm to attack this problem our key insight is that the availability of the source taxonomy data could be helpful to build better classifier in this scenario therefore it would be beneficial to do transductive learning rather than inductive learning i e learning to optimize classification performance on a particular set of test example noticing that the categorization of the master and source taxonomy often have some semantic overlap we propose a method cluster shrinkage c to further enhance the classification by exploiting such implicit knowledge our experiment with real world web data show substantial improvement in the performance of taxonomy integration 
we present result for automated text categorization of the reuters collection of news story our experiment use the entire one year collection of story and the entire subject index we divide the data into monthly group and provide an initial benchmark of text categorization performance on the complete collection experimental result show that efficient sparse feature implementation of linear method and decision tree using a global unstemmed dictionary can readily handle application of this size predictive performance is approximately a strong a the best result for the much smaller older reuters collection detailed result are provided over time period it is shown that a smaller time horizon doe not diminish predictive quality implying reduced demand for retraining when sample size is large 
this research explores the relationship between information seeking strategy i and information retrieval ir system design when people seek information they engage in a variety of i in order to search for specific item learn about the content of the database evaluate retrieved information and so on the theoretical foundation of the work are based on the information seeking episode model developed by belkin and the multi facet classification scheme of information behavior proposed by cool belkin the goal of this research is to construct and evaluate an interactive retrieval system which us different combination of ir technique to support different i example ir technique include comparison using exact and probabilistic matching algorithm summarization of information object using title snippet or abstract visualization technique such a list or classified result and navigation technique such a scrolling or following link by designing a retrieval system with diverse strategy in mind we can adaptively support multiple i permitting a user to move seamlessly from one strategy to another choosing instantiation of each support technique tailored to the specific i the research will be conducted in a series of four step develop an object oriented framework for representing basic ir technique design implement and evaluate system which support individual i such a browsing and searching specify an interaction structure for guiding and controlling sequence of different supporting technique design implement and evaluate a dynamically adaptive system supporting multiple i in comparison to a non adaptive baseline system 
a computationally enhanced message contains some embedded programmatic component that are interpreted and executed automatically upon receipt unlike ordinary text email or instant message they make possible a number of useful application in this paper we describe a general and flexible messaging system called shock that extends the functionality of prior computational email system by allowing xml encoded shock message to interact with an automatically created profile of a user these profile consist of information about the most common task user perform such a their web browsing behavior their conventional email usage etc since user are sensitive about such data the system is designed with privacy a a central design goal and employ a distributed peer to peer architecture to achieve it the system is largely implemented with commodity web technology and provides both a web interface a well a one that is tightly integrated with user ordinary email client with shock user can send highly targeted message without violating others privacy and engage in structured conversation appropriate to the context without disrupting their existing work practice we describe our implementation in detail the most useful novel application of the system and our experience with the system in a pilot field test 
for cross language information retrieval clir based on bilingual translation dictionary good performance depends upon lexical coverage in the dictionary this is especially true for language possessing few inter language cognate such a between japanese and english in this paper we describe a method for automatically creating and validating candidate japanese transliterated term of english word a phonetic english dictionary and a set of probabilistic mapping rule are used for automatically generating transliteration candidate a monolingual japanese corpus is then used for automatically validating the transliterated term we evaluate the usage of the extracted english japanese transliteration pair with japanese to english retrieval experiment over the clef bilingual test collection the use of our automatically derived extension to a bilingual translation dictionary improves average precision both before and after pseudo relevance feedback with gain ranging from to 
this paper explores the use of bayesian online classifier to classify text document empirical result indicate that these classifier are comparable with the best text classification system furthermore the online approach offer the advantage of continuous learning in the batch adaptive text filtering task 
a novel hardware assisted top doc hat component is disclosed hat is an optimized content indexing device based on a modified inverted index structure hat accommodates pattern of different length and support a varied posting list versus term count feature sustaining high reusability and efficiency the developed component can be used either a an internal slave component or a an external co processor and is efficient in resource demand a the component controller take only a minimal percentage of the target device space leaving the majority of the space to term and posting entry a very high speed integrated circuit vhsic hardware description language vhdl is used to model the hat system 
in this paper we study how we can design an effective parallel crawler a the size of the web grows it becomes imperative to parallelize a crawling process in order to finish downloading page in a reasonable amount of time we first propose multiple architecture for a parallel crawler and identify fundamental issue related to parallel crawling based on this understanding we then propose metric to evaluate a parallel crawler and compare the proposed architecture using million page collected from the web our result clarify the relative merit of each architecture and provide a good guideline on when to adopt which architecture 
an evaluation methodology that target ineffective topic is needed to support research on obtaining more consistent retrieval across topic using average value of traditional evaluation measure is not an appropriate methodology because it emphasizes effective topic poorly performing topic score are by definition small and they are therefore difficult to distinguish from the noise inherent in retrieval evaluation we examine two new measure that emphasize a system s worst topic while these measure focus on different aspect of retrieval behavior than traditional measure the measure are le stable than traditional measure and the margin of error associated with the new measure is large relative to the observed difference in score 
activity such a web service and the semantic web are working to create a web of distributed machine understandable data in this paper we present an application called semantic search which is built on these supporting technology and is designed to improve traditional web searching we provide an overview of tap the application framework upon which the semantic search is built we describe two implemented semantic search system which based on the denotation of the search query augment traditional search result with relevant data aggregated from distributed source we also discus some general issue related to searching and the semantic web and outline how an understanding of the semantics of the search term can be used to provide better result 
we discus a retrieval model in which the task is to complete a sentence given an initial fragment and given an application specific document collection this model is motivated by administrative and call center environment in which user have to write document with a certain repetitiveness we formulate the problem setting and discus appropriate performance metric we present an index based retrieval algorithm and a cluster based approach and evaluate our algorithm using collection of email that have been written by two distinct service center 
this paper investigates how the vision of the semantic web can be carried overto the realm of email we introduce a general notion of semantice mail in which an email message consists of an rdf query or update coupled with corresponding explanatory text semantic email open the door to a wide range of automated email mediated application with formally guaranteed property in particular this paper introduces a broad class of semantic email process for example consider the process of sending an email to a program committee asking who will attend the pc dinner automatically collecting the response and tallying them up we define bothlogical and decision theoretic model where an email process ismodeled a a set of update to a data set on which we specify goal via certain constraint or utility we then describe a set ofinference problem that arise while trying to satisfy these goal and analyze their computational tractability in particular weshow that for the logical model it is possible to automatically infer which email response are acceptable w r t a set ofconstraints in polynomial time and for the decision theoreticmodel it is possible to compute the optimal message handling policy in polynomial time finally we discus our publicly available implementation of semantic email and outline research challenge inthis realm 
finding page on the web that are similar to a query page related page is an important component of modern search engine a variety of strategy have been proposed for answering related page query but comparative evaluation by user study is expensive especially when large strategy space must be searched e g when tuning parameter we present a technique for automatically evaluating strategy using web hierarchy such a open directory in place of user feedback we apply this evaluation methodology to a mix of document representation strategy including the use of text anchor text and link we discus the relative advantage and disadvantage of the various approach examined finally we describe how to efficiently construct a similarity index out of our chosen strategy and provide sample result from our index 
web link analysis ha been proved to provide significant enhancement to the precision of web search in practice the pagerank algorithm which is used in google search engine play an important role on improving the quality of it resuls by employing the explicit hyperlink structure among the web page the prestige of web page defined by pagerank is purely derived from surfer random walk on the web graph without textual content content consideration however in the practical sense user surfing behavior is far from random jumping in this paper we present a unified model for a more accurate page rank user s surfing is guided by a probabilistic model that is based on literal matching between connected page the result show that our proposed ranking algorithm do perform better than the original pagerank 
we discus the definition of key for xml document paying particular attention to the concept of a relative key which is commonly used in hierarchically structured document and scientific database 
providing knowledge worker with access to expert and community of practice is central to sharing expertise and crucial to organizational performance adaptation and even survival this paper cover ongoing research to develop an expert locator prototype a model based system for detecting expert and broader community of practice the underlying expertise model is extensible and support aggregation of evidence across diverse source the prototype is being used to locate critical expertise in key project area and current evaluation indicates it potential effectiveness 
cosine pivoted document length normalization ha reached a point of stability where many researcher indiscriminately apply a specific value of regardless of the collection our effort however demonstrate that applying this specific value without tuning for the document collection degrades average precision by a much a 
we present a framework in which probabilistic model for textual and visual information retrieval can be integrated seamlessly the framework facilitates searching for imagery using textual description and visual example simultaneously the underlying language model for text and gaussian mixture model for image have proven successful in various retrieval task 
we develop a method for predicting query performance by computing the relative entropy between a query language model and the corresponding collection language model the resulting clarity score measure the coherence of the language usage in document whose model are likely to generate the query we suggest that clarity score measure the ambiguity of a query with respect to a collection of document and show that they correlate positively with average precision in a variety of trec test set thus the clarity score may be used to identify ineffective query on average without relevance information we develop an algorithm for automatically setting the clarity score threshold between predicted poorly performing query and acceptable query and validate it using trec data in particular we compare the automatic threshold to optimum threshold and also check how frequently result a good are achieved in sampling experiment that randomly assign query to the two class 
the success of the semantic web crucially depends on the easy creation integration and use of semantic data for this purpose we consider an integration scenario that defies core assumption of current metadata construction method we describe a framework of metadata creation when web page are generated from a database and the database owner is cooperatively participating in the semantic web this lead u to the definition of ontology mapping rule by manual semantic annotation and the usage of the mapping rule and of web service for semantic query in order to create metadata the framework combine the presentation layer with the data description layer in contrast to conventional annotation which remains at the presentation layer therefore we refer to the framework a deep annotation we consider deep annotation a particularly valid because i web page generated from database outnumber static web page ii annotation of web page may be a very intuitive way to create semantic data from a database and iii data from database should not be materialized a rdf file it should remain where it can be handled most efficiently in it database 
we consider the problem of modeling annotated data data with multiple type where the instance of one type such a a caption serf a a description of the other type such a an image we describe three hierarchical probabilistic mixture model which aim to describe such data culminating in correspondence latent dirichlet allocation a latent variable model that is effective at modeling the joint distribution of both type and the conditional distribution of the annotation given the primary type we conduct experiment on the corel database of image and caption assessing performance in term of held out likelihood automatic annotation and text based image retrieval 
it is difficult to serialize an rdf graph a a humanly readable rdf xml document this paper describes the approach taken in jena in which a design pattern of guarded procedure invoked using top down recursive descent is used each procedure corresponds to a grammar rule the guard make the choice about the applicability of the production this approach is seen to correspond closely to the design of an ll k parser and a theoretical justification of this correspondence is found in universal algebra 
transitive retrieval and triangulation have been proposed a way to improve cross language retrieval quality when translation resource have poor lexical coverage we demonstrate that cross language retrieval is viable for european language with no translation resource at all that transitive retrieval without translation doe not suffer the drop off in retrieval quality sometimes reported for transitive retrieval with translation and that triangulation that combine multiple transitive run with no translation can boost performance over direct translation free retrieval 
in this paper we present a method based on document probe to quantify and diagnose topic structure distinguishing topic a monolithic structured or diffuse the method also yield a structure analysis that can be used directly to optimize filter classifier creation preliminary result illustrate the predictive value of the approach on trec reuters topic 
in the original pagerank algorithm for improving the ranking of search query result a single pagerank vector is computed using the link structure of the web to capture the relative importance of web page independent of any particular search query to yield more accurate search result we propose computing a set of pagerank vector biased using a set of representative topic to capture more accurately the notion of importance with respect to a particular topic by using these precomputed biased pagerank vector to generate query specific importance score for page at query time we show that we can generate more accurate ranking than with a single generic pagerank vector for ordinary keyword search query we compute the topic sensitive pagerank score for page satisfying the query using the topic of the query keywords for search done in context e g when the search query is performed by highlighting word in a web page we compute the topic sensitive pagerank score using the topic of the context in which the query appeared 
an empirical study ha been conducted investigating the relationship between the performance of an aspect based language model in term of perplexity and the corresponding information retrieval performance obtained it is observed on the corpus considered that the perplexity of the language model ha a systematic relationship with the achievable precision recall performance though it is not statistically significant 
this paper describes a question answering system that is designed to capitalize on the tremendous amount of data that is now available online most question answering system use a wide variety of linguistic resource we focus instead on the redundancy available in large corpus a an important resource we use this redundancy to simplify the query rewrite that we need to use and to support answer mining from returned snippet our system performs quite well given the simplicity of the technique being utilized experimental result show that question answering accuracy can be greatly improved by analyzing more and more matching passage simple passage ranking and n gram extraction technique work well in our system making it efficient to use with many backend retrieval engine 
if an e service approach to electronic commerce is to become widespread standardisation of ontology message content and message protocol will be necessary in this paper we present a lifecycle of a business to business e commerce interaction and show how the semantic web can support a service description language that can be used throughout this lifecycle by using daml we develop a service description language sufficiently expressive and flexible to be used not only in advertisement but also in matchmaking query negotiation proposal and agreement we also identify which operation must be carried out on this description language if the b b lifecycle is to be fully supported we do not propose specific standard protocol but instead argue that our operator are able to support a wide variety of interaction protocol and so will be fundamental irrespective of which protocol are finally adopted 
much attention ha been accorded to location based service and location tracking a necessary component in active trigger based lb application tracking the location of a large population of moving object requires very high update and query performance of the underlying spatial index in this paper we investigate the performance and scalability of three main memory based spatial indexing method under dynamic update and query load an r tree a zb tree and an array hashtable method by leveraging the locus performance evaluation testbed and the city simulator dynamic spatial data generator we are able to demonstrate the scalability of these method and determine the maximum population size supported by each method a useful parameter for capacity planning by wireless carrier 
this paper investigates the pre condition for successful combination of document representation formed from structural markup for the task of known item search a this task is very similar to work in meta search and data fusion we adapt several hypothesis from those research area and investigate them in this context to investigate these hypothesis we present a mixture based language model and also examine many of the current meta search algorithm we find that compatible output from system is important for successful combination of document representation we also demonstrate that combining low performing document representation can improve performance but not consistently we find that the technique best suited for this task are robust to the inclusion of poorly performing document representation we also explore the role of variance of result across system and it impact on the performance of fusion with the surprising result that the correct document have higher variance across document representation than highly ranking incorrect document 
many web information service utilize technique of information extraction ie to collect important fact from the web to create more advanced service one possible method is to discover thematic information from the collected fact through text classification however most conventional text classification technique rely on manual labelled corpus and are thus ill suited to cooperate with web information service with open domain in this work we present a system named liveclassifier that can automatically train classifiersthrough web corpus based on user defined topic hierarchy due to it flexibility and convenience liveclassifier can be easily adapted for various purpose new web information service can be created to fully exploit it human user can use it to create classifier for their personal application the effectiveness of classifier created by liveclassifier is well supportedby empirical evidence 
retrieval mechanism are frequently compared by computing the respective average score for some effectiveness metric across a common set of information need or topic with researcher concluding one method is superior based on those average since comparative retrieval system behavior is known to be highly variable across topic good experimental design requires that a sufficient number of topic be used in the test this paper us trec result to empirically derive error rate based on the number of topic used in a test and the observed difference in the average score the error rate quantify the likelihood that a different set of topic of the same size would lead to a different conclusion we directly compute error rate for topic set up to size and extrapolate those rate for larger topic set size the error rate found are larger than anticipated indicating researcher need to take care when concluding one method is better than another especially if few topic are used 
while being successful in providing keyword based access to web page commercial search portal still lack the ability to answer question expressed in a natural language we present a probabilistic approach to automated question answering on the web based on trainable pattern answer triangulation and semantic filtering in contrast to the other shallow approach our approach is entirely self learning it doe not require any manually created scoring and filtering rule while still performing comparably it also performs better than other fully trainable approach 
an important issue in the dissemination of time varying web data such a sport score and stock price is the maintenance of temporal coherency in the case of server adhering to the http protocol client need to frequently pull the data based on the dynamic of the data and a user s coherency requirement in contrast server that posse push capability maintain state information pertaining to client and push only those change that are of interest to a user these two canonical technique have complementary property with respect to the level of temporal coherency maintained communication overhead state space overhead and loss of coherency due to server failure in this paper we show how to combine push and pull based technique to achieve the best feature of both approach our combined technique tailor the dissemination of data from server to client based on the capability and load at server and proxy and client coherency requirement our experimental result demonstrate that such adaptive data dissemination is essential to meet diverse temporal coherency requirement to be resilient to failure and for the efficient and scalable utilization of server and network resource 
a problem facing many textbook author including one of the author of this paper is the inevitable delay between new advance in the subject area and their incorporation in a new paper edition of the textbook this mean that some textbook are quickly considered out of date particularly in active technological area such a the web even though the idea presented in the textbook are still valid and important to the community this paper describes our approach to building a companion website for the textbook hypermedia and the web an engineering approach we use bloom s taxonomy of educational objective to critically evaluate a number of authoring and presentation technique used in existing companion website and adapt these technique to create our own companion website using semantic web technology in order to overcome the identified weakness finally we discus a potential model of future companion website in the context of an e publishing e commerce semantic web service scenario 
content extraction signature ce enable selective disclosure of verifiable content provide privacy for blinded content and enable the signer to specify the content the document owner is allowed to extract or blind combined these property give what we call ce functionality in this paper we describe our work in developing custom transform algorithm to expand the functionality of an xml signature to include ce functionality in xml signature core validation we also describe a custom revocation mechanism and our implementation for non xml content where the custom transforms are dynamically loaded demonstrating that custom signing and verification is not constrained to a closed system through the use of dynamic loading we show that a verifier can still verify an xml signature compliant signature even though a custom signature wa produced 
the semantic web envisions a world wide web in which data is described with rich semantics and application can pose complex query to this point researcher have defined new language for specifying meaning for concept and developed technique for reasoning about them using rdf a the data model to flourish the semantic web need to be able to accommodate the huge amount of existing data and the application operating on them to achieve this we are faced with two problem first most of the world s data is available not in rdf but in xml xml and the application consuming it rely not only on the domain structure of the data but also on it document structure hence to provide interoperability between such source we must map between both their domain structure and their document structure second data management practitioner often prefer to exchange data through local point to point data translation rather than mapping to common mediated schema or ontology this paper describes the piazza system which address these challenge piazza offer a language for mediating between data source on the semantic web which map both the domain structure and document structure piazza also enables interoperation of xml data with rdf data that is accompanied by rich owl ontology mapping in piazza are provided at a local scale between small set of node and our query answering algorithm is able to chain set mapping together to obtain relevant data from across the piazza network we also describe an implemented scenario in piazza and the lesson we learned from it 
information retrieval using word sens is emerging a a good research challenge on semantic information retrieval in this paper we propose a new method using word sens in information retrieval root sense tagging method this method assigns coarse grained word sens defined in wordnet to query term and document term by unsupervised way using co occurrence information constructed automatically our sense tagger is crude but performs consistent disambiguation by considering only the single most informative word a evidence to disambiguate the target word we also allow multiple sense assignment to alleviate the problem caused by incorrect disambiguation experimental result on a large scale trec collection show that our approach to improve retrieval effectiveness is successful while most of the previous work failed to improve performance even on small text collection our method also show promising result when is combined with pseudo relevance feedback and state of the art retrieval function such a bm 
we address the problem of integrating object from a source taxonomy into a master taxonomy this problem is not only currently pervasive on the web but also important to the emerging semantic web a straightforward approach to automating this process would be to train a classifier for each category in the master taxonomy and then classify object from the source taxonomy into these category in this paper we attempt to use a powerful classification method support vector machine svm to attack this problem our key insight is that the availability of the source taxonomy data could be helpful to build better classifier in this scenario therefore it would be beneficial to do transductive learning rather than inductive learning i e learning to optimize classification performance on a particular set of test example noticing that the categorization of the master and source taxonomy often have some semantic overlap we propose a method cluster shrinkage c to further enhance the classification by exploiting such implicit knowledge our experiment with real world web data show substantial improvement in the performance of taxonomy integration 
we present result for automated text categorization of the reuters collection of news story our experiment use the entire one year collection of story and the entire subject index we divide the data into monthly group and provide an initial benchmark of text categorization performance on the complete collection experimental result show that efficient sparse feature implementation of linear method and decision tree using a global unstemmed dictionary can readily handle application of this size predictive performance is approximately a strong a the best result for the much smaller older reuters collection detailed result are provided over time period it is shown that a smaller time horizon doe not diminish predictive quality implying reduced demand for retraining when sample size is large 
this research explores the relationship between information seeking strategy i and information retrieval ir system design when people seek information they engage in a variety of i in order to search for specific item learn about the content of the database evaluate retrieved information and so on the theoretical foundation of the work are based on the information seeking episode model developed by belkin and the multi facet classification scheme of information behavior proposed by cool belkin the goal of this research is to construct and evaluate an interactive retrieval system which us different combination of ir technique to support different i example ir technique include comparison using exact and probabilistic matching algorithm summarization of information object using title snippet or abstract visualization technique such a list or classified result and navigation technique such a scrolling or following link by designing a retrieval system with diverse strategy in mind we can adaptively support multiple i permitting a user to move seamlessly from one strategy to another choosing instantiation of each support technique tailored to the specific i the research will be conducted in a series of four step develop an object oriented framework for representing basic ir technique design implement and evaluate system which support individual i such a browsing and searching specify an interaction structure for guiding and controlling sequence of different supporting technique design implement and evaluate a dynamically adaptive system supporting multiple i in comparison to a non adaptive baseline system 
a computationally enhanced message contains some embedded programmatic component that are interpreted and executed automatically upon receipt unlike ordinary text email or instant message they make possible a number of useful application in this paper we describe a general and flexible messaging system called shock that extends the functionality of prior computational email system by allowing xml encoded shock message to interact with an automatically created profile of a user these profile consist of information about the most common task user perform such a their web browsing behavior their conventional email usage etc since user are sensitive about such data the system is designed with privacy a a central design goal and employ a distributed peer to peer architecture to achieve it the system is largely implemented with commodity web technology and provides both a web interface a well a one that is tightly integrated with user ordinary email client with shock user can send highly targeted message without violating others privacy and engage in structured conversation appropriate to the context without disrupting their existing work practice we describe our implementation in detail the most useful novel application of the system and our experience with the system in a pilot field test 
for cross language information retrieval clir based on bilingual translation dictionary good performance depends upon lexical coverage in the dictionary this is especially true for language possessing few inter language cognate such a between japanese and english in this paper we describe a method for automatically creating and validating candidate japanese transliterated term of english word a phonetic english dictionary and a set of probabilistic mapping rule are used for automatically generating transliteration candidate a monolingual japanese corpus is then used for automatically validating the transliterated term we evaluate the usage of the extracted english japanese transliteration pair with japanese to english retrieval experiment over the clef bilingual test collection the use of our automatically derived extension to a bilingual translation dictionary improves average precision both before and after pseudo relevance feedback with gain ranging from to 
this paper explores the use of bayesian online classifier to classify text document empirical result indicate that these classifier are comparable with the best text classification system furthermore the online approach offer the advantage of continuous learning in the batch adaptive text filtering task 
a novel hardware assisted top doc hat component is disclosed hat is an optimized content indexing device based on a modified inverted index structure hat accommodates pattern of different length and support a varied posting list versus term count feature sustaining high reusability and efficiency the developed component can be used either a an internal slave component or a an external co processor and is efficient in resource demand a the component controller take only a minimal percentage of the target device space leaving the majority of the space to term and posting entry a very high speed integrated circuit vhsic hardware description language vhdl is used to model the hat system 
we propose a formal model of cross language information retrieval that doe not rely on either query translation or document translation our approach leverage recent advance in language modeling to directly estimate an accurate topic model in the target language starting with a query in the source language the model integrates popular technique of disambiguation and query expansion in a unified formal framework we describe how the topic model can be estimated with either a parallel corpus or a dictionary we test the framework by constructing chinese topic model from english query and using them in the clir task of trec the model achieves performance around of the strong mono lingual baseline in term of average precision in initial precision our model outperforms the mono lingual baseline by the main contribution of this work is the unified formal model which integrates technique that are essential for effective cross language retrieval 
we describe ongoing work on i i a system aimed at fostering opportunistic communication among user viewing or manipulating content on the web and in productivity application unlike previous work in which the url of web resource are used to group user visiting the same resource we present a more general framework for clustering work context to group user together that account for dynamic content and distributional property of web access which can limit the utility url based system in addition we describe a method for scaffolding asynchronous communication in the context of an ongoing task that take into account the ephemeral nature of the location of content on the web the technique we describe also nicely cover local file in progress in addition to publicly available web content we present the result of several evaluation that indicate system that use the technique we employ may be more useful than system that are strictly url based 
recently active behavior ha received attention in the xml field to automatically react to occurred event aside from proprietary approach for enriching xml with active behavior the w c standardized the document object model dom event module for the detection of event in xml document when using any of these approach however it is often impossible to decide which event to react upon because not a single event but a combination of multiple event i e a composite event determines a situation to react upon the paper present the first approach for detecting composite event in xml document by addressing the peculiarity of xml event which are caused by their hierarchical order in addition to their temporal order it also provides for the detection of satisfied multiplicity constraint defined by xml schema thereby the approach enables application operating on xml document to react to composite event which have richer semantics 
document clustering is useful in many information retrieval task document browsing organization and viewing of retrieval result generation of yahoo like hierarchy of document etc the general goal of clustering is to group data element such that the intra group similarity are high and the inter group similarity are low we present a clustering algorithm called cbc clustering by committee that is shown to produce higher quality cluster in document clustering task a compared to several well known clustering algorithm it initially discovers a set of tight cluster high intra group similarity called committee that are well scattered in the similarity space low inter group similarity the union of the committee is but a subset of all element the algorithm proceeds by assigning element to their most similar committee evaluating cluster quality ha always been a difficult task we present a new evaluation methodology that is based on the editing distance between output cluster and manually constructed class the answer key this evaluation measure is more intuitive and easier to interpret than previous evaluation measure 
link analysis ha shown great potential in improving the performance of web search pagerank and hit are two of the most popular algorithm most of the existing link analysis algorithm treat a web page a a single node in the web graph however in most case a web page contains multiple semantics and hence the web page might not be considered a the atomic node in this paper the web page is partitioned into block using the vision based page segmentation algorithm by extracting the page to block block to page relationship from link structure and page layout analysis we can construct a semantic graph over the www such that each node exactly represents a single semantic topic this graph can better describe the semantic structure of the web based on block level link analysis we proposed two new algorithm block level pagerank and block level hit whose performance we study extensively using web data 
multiple topic and varying length of web page are two negative factor significantly affecting the performance of web search in this paper we explore the use of page segmentation algorithm to partition web page into block and investigate how to take advantage of block level evidence to improve retrieval performance in the web context because of the special characteristic of web page different page segmentation method will have different impact on web search performance we compare four type of method including fixed length page segmentation dom based page segmentation vision based page segmentation and a combined method which integrates both semantic and fixed length property experiment on block level query expansion and retrieval are performed among the four approach the combined method achieves the best performance for web search our experimental result also show that such a semantic partitioning of web page effectively deal with the problem of multiple drifting topic and mixed length and thus ha great potential to boost up the performance of current web search engine 
the poster report on a project in which we are investigating method for breaking the human metadata generation bottleneck that plague digital library the research question is whether metadata element and value can be automatically generated from the content of educational resource and correctly assigned to mathematics and science educational material natural language processing and machine learning technique were implemented to automatically assign value of the gemgenerate metadata element set tofor learning resource provided by the gateway for education gem a service that offer web access to a wide range of educational material in a user study education professional evaluated the metadata assigned to learning resource by either automatic tagging or manual assignment result show minimal difference in the eye of the evaluator between automatically generated metadata and manually assigned metadata 
natural language nl ha evolved to facilitate human communication it enables the speaker to make the listener s mind wander among her experience and mental association roughly according to the intention of the speaker the speaker and the listener usually share experience and expectation and they use mostly the same unit and rule of a shared nl written language function similarly but in a le interactive way with fewer possibility for feedback both the symbol of nl i e word or morpheme and their arrangement are meaningful not with universal and precise meaning but similar enough among different speaker and accurate enough for the communication mostly to succeed nls are mostly very large system hundred of thousand of word and infinitely many possible utterance even inflection alone might produce huge number of form e g more than ten thousand distinct form out of every finnish verb entry nl processing for ir or any other purpose must cope with phenomenon like inflection and compounding synonymy polysemy ambiguity anaphora and head modifier relation among word and phrase language technology can neutralize much of the effect of these inconvenience inherent with nl but what kind of advantage could nl have redundant use of synonymous expression can effectively identify new concept multilingual parallel document may help in identifying their exact content nls typically carry connotation i e what is implied but not explicitly said e g attitude politeness vague association are easy to express in nl but not always in formal system e g a few year ago there wa an article about the rival of yeltsin i don t remember his name but he then went over to some region in siberia but what did the guy promise joke and humor belong to nls not to formal system are there any alternative for nl not really because any artificial and more precise formalism fail to adapt to new concept and they do not easily allow restructuring of previous idea one challenge for language technology is to find better solution for the above inconvenience in order to provide various ir document classification indexing and summarizing method with more accurate and adequate input data with more accurate input some of the more demanding task of ir can perhaps be solved 
this paper present a set of tool and technique for analyzing interaction of composite web service which are specified in bpel and communicate through asynchronous xml message we model the interaction of composite web service a conversation the global sequence of message exchanged by the web service a opposed to earlier work our tool set handle rich data manipulation via xpath expression this allows u to verify design at a more detailed level and check property about message content we present a framework where bpel specification of web service are translated to an intermediate representation followed by the translation of the intermediate representation to a verification language a an intermediate representation we use guarded automaton augmented with unbounded queue for incoming message where the guard are expressed a xpath expression a the target verification language we use promela input language of the model checker spin since spin model checker is a finite state verification tool we can only achieve partial verification by fixing the size of the input queue in the translation we propose the concept of synchronizability to address this problem we show that if a composite web service is synchronizable then it conversation set remains same when asynchronous communication is replaced with synchronous communication we give a set of sufficient condition that guarantee synchronizability and that can be checked statically based on our synchronizability result we show that a large class of composite web service with unbounded input queue can be completely verified using a finite state model checker such a spin 
despite traditional web caching technique redundant data is often transferred over http link these redundant transfer result from both resource modification and aliasing resource modification cause the data represented by a single uri to change often in transferring the new data some old data is retransmitted aliasing in contrast occurs when the same data is named by multiple uris often in the context of dynamic or advertising content traditional web caching technique index data by it name and thus often fail to recognize and take advantage of aliasing despite traditional web caching technique redundant data is often transferred over http link these redundant transfer result from both resource modification and aliasing resource modification cause the data represented by a single uri to change often in transferring the new data some old data is retransmitted aliasing in contrast occurs when the same data is named by multiple uris often in the context of dynamic or advertising content traditional web caching technique index data by it name and thus often fail to recognize and take advantage of aliasing 
we propose a bayesian extension to the ad hoc language model many smoothed estimator used for the multinomial query model in ad hoc language model including laplace and bayes smoothing are approximation to the bayesian predictive distribution in this paper we derive the full predictive distribution in a form amenable to implementation by classical ir model and then compare it to other currently used estimator in our experiment the proposed model outperforms bayes smoothing and it combination with linear interpolation smoothing outperforms all other estimator 
the web is increasingly becoming an important channel for conducting business disseminating information and communicating with people on a global scale more and more company organization and individual are publishing their information on the web with all this information publicly available naturally company and individual want to find useful information from these web page a an example company always want to know what their competitor are doing and what product and service they are offering knowing such information the company can learn from their competitor and or design countermeasure to improve their own competitiveness the ability to effectively find such business intelligence information is increasingly becoming crucial to the survival and growth of any company despite it importance little work ha been done in this area in this paper we propose a novel visualization technique to help the user find useful information from his her competitor web site easily and quickly it involves visualizing with the help of a clustering system the comparison of the user s web site and the competitor s web site to find similarity and difference between the site the visualization is such that with a single glance the user is able to see the key similarity and difference of the two site he she can then quickly focus on those interesting cluster and page to browse the detail experiment result and practical application show that the technique is effective 
the effectiveness of query in information retrieval can be improved through query expansion this technique automatically introduces additional query term that are statistically likely to match document on the intended topic however query expansion technique rely on fixed parameter our investigation of the effect of varying these parameter show that the strategy of using fixed value is questionable 
the social impact from the world wide web cannot be underestimated but technology used to build the web are also revolutionizing the sharing of business and government information within intranet in many way the lesson learned from the internet carry over directly to intranet but others do not apply in particular the social force that guide the development of intranet are quite different and the determination of a good answer for intranet search is quite different than on the internet in this paper we study the problem of intranet search our approach focus on the use of rank aggregation and allows u to examine the effect of different heuristic on ranking of search result 
recent work on incremental crawling ha enabled the indexed document collection of a search engine to be more synchronized with the changing world wide web however this synchronized collection is not immediately searchable because the keyword index is rebuilt from scratch le frequently than the collection can be refreshed an inverted index is usually used to index document crawled from the web complete index rebuild at high frequency is expensive previous work on incremental inverted index update have been restricted to adding and removing document updating the inverted index for previously indexed document that have changed ha not been addressed in this paper we propose an efficient method to update the inverted index for previously indexed document whose content have changed our method us the idea of landmark together with the diff algorithm to significantly reduce the number of posting in the inverted index that need to be updated our experiment verify that our landmark diff method result in significant saving in the number of update operation on the inverted index 
the information age ha brought with it the promise of unprecedented economic growth based on the efficiency made possible by new technology this same greater efficiency ha left society with le and le time to adapt to technological progress perhaps the greatest cost of this progress is the threat to privacy we all face from unconstrained exchange of our personal information in response to this threat the world wide web consortium ha introduced the platform for privacy preference p p to allow site to express policy in machine readable form and to expose these policy to site visitor however today p p doe not protect the privacy of individual nor doe it implementation empower community or group to negotiate and establish standard of behavior we propose a privacy architecture we call the social contract core scc designed to speed the establishment of new social contract needed to protect private data the goal of scc is to empower community speed the socialization of new technology and encourage the rapid access to and exchange of information addressing these issue is essential we feel to both liberty and economic prosperity in the information age 
the interactive query performance analyser qpa for information retrieval system is a web based tool for analysing and comparing the performance of individual query on top of a standard test collection it give an instant visualisation of the performance achieved in a given search topic by any user generated query in addition to experimental ir research qpa can be used in user training to demonstrate the characteristic of and compare difference between ir system and searching strategy the first prototype version and of the query performance analyser wa developed at the department of information study university of tampere to serve a a tool for rapid query performance analysis comparison and visualisation later it ha been applied to interactive optimisation of query the analyser ha served also in learning environment for ir the demonstration is based on the newest version of the query performance analyser v it is interfaced to a traditional boolean ir system trip and a probabilistic ir system inquery providing access to the trec collection and two finnish test collection version support multigraded relevance scale new type of performance visualisation and query conversion based on monoand multi lingual dictionary the motivation in developing the analyser is to emphasise the necessity of analysing the behaviour of individual query information retrieval experiment usually measure the average effectiveness of ir method developed the analysis of individual query is neglected although test result may contain individual test topic where general finding do not hold for the real user of an ir system the study of variation in result is even more important than average 
the computation of page importance in a huge dynamic graph ha recently attracted a lot of attention because of the web page importance or page rank is defined a the fixpoint of a matrix equation previous algorithm compute it off line and require the use of a lot of extra cpu a well a disk resource e g to store maintain and read the link matrix we introduce a new algorithm opic that work on line and us much le resource in particular it doe not require storing the link matrix it is on line in that it continuously refines it estimate of page importance while the web graph is visited thus it can be used to focus crawling to the most interesting page we prove the correctness of opic we present adaptive opic that also work on line but adapts dynamically to change of the web a variant of this algorithm is now used by xyleme we report on experiment with synthetic data in particular we study the convergence and adaptiveness of the algorithm for various scheduling strategy for the page to visit we also report on experiment based on crawl of significant portion of the web 
web search query log contain trace of user search modification one strategy user employ is deleting term presumably to obtain greater coverage it is useful to model and automate term deletion when arbitrary search are conjunctively matched against a small hand constructed collection such a a hand built hierarchy or collection of high quality page matched with key phrase query with no match can have word deleted till a match is obtained we provide algorithm which perform substantially better than the baseline in predicting which word should be deleted from a reformulated query for increasing query coverage in the context of web search on small high quality collection 
to improve performance in text categorization it is important to extract distinctive feature for each class this paper proposes topic difference factor analysis tdfa a a method to extract projection ax that reflect topic difference between two document set suppose all sentence vector that compose each document are projected onto projection ax tdfa obtains the ax that maximize the ratio between the document set a to the sum of squared projection by solving a generalized eigenvalue problem the ax are called topic difference factor tdf s by applying tdfa to the document set that belongs to a given class and a set of document that is misclassified a belonging to that class by an existent classifier we can obtain feature that take large value in the given class but small one in other class a well a feature that take large value in other class but small one in the given class a classifier wa constructed applying the above feature to complement the knn classifier a the result the micro averaged f measure for reuters improved from to 
the celebrated pagerank algorithm ha proved to be a very effective paradigm for ranking result of web search algorithm in this paper we refine this basic paradigm to take into account several evolving prominent feature of the web and propose several algorithmic innovation first we analyze feature of the rapidly growing frontier of the web namely the part of the web that crawler are unable to cover for one reason or another we analyze the effect of these page and find it to be significant we suggest way to improve the quality of ranking by modeling the growing presence of link rot on the web a more site and page fall out of maintenance finally we suggest new method of ranking that are motivated by the hierarchical structure of the web are more efficient than pagerank and may be more resistant to direct manipulation 
stemming can improve retrieval accuracy but stemmer are language specific character n gram tokenization achieves many of the benefit of stemming in a language independent way but it use incurs a performance penalty we demonstrate that selection of a single n gram a a pseudo stem for a word can be an effective and efficient language neutral approach for some language 
recent web search technique augment traditional text matching with a global notion of importance based on the linkage structure of the web such a in google s pagerank algorithm for more refined search this global notion of importance can be specialized to create personalized view of importance for example importance score can be biased according to a user specified set of initially interesting page computing and storing all possible personalized view in advance is impractical a is computing personalized view at query time since the computation of each view requires an iterative computation over the web graph we present new graph theoretical result and a new technique based on these result that encode personalized view a partial vector partial vector are shared across multiple personalized view and their computation and storage cost scale well with the number of view our approach enables incremental computation so that the construction of personalized view from partial vector is practical at query time we present efficient dynamic programming algorithm for computing partial vector an algorithm for constructing personalized view from partial vector and experimental result demonstrating the effectiveness and scalability of our technique 
we describe a method to define and use subwebs user defined neighborhood of the internet subwebs help improve search performance by inducing a topic specific page relevance bias over a collection of document subwebs may be automatically identified using a simple algorithm we describe and used to provide highly relevant topic specific information retrieval using subwebs in a help and support topic we see marked improvement in precision compared to generic search engine result 
we addressed two issue concerning the practical aspect of optimally scheduling web advertising proposed by langheinrich et al which scheduling maximizes the total number of click throughs for all banner advertisement one is the problem of multi impression in which two or more banner ad are impressed at the same time the other is inventory management which is important in order to prevent over selling and maximize revenue we propose efficient method which deal with these two issue 
structured method for query term replacement rely on separate estimate of term te of replacement probability statistically significantfrequency and document frequency to compute a weight for each query term this paper review prior work on structured query technique and introduces three new variant that leverage estima improvement in retrieval effectiveness are demonstrated for cross language retrieval and for retrieval based on optical character recognition when replacement probability are used to estimate both term frequency and document frequency 
many daily activity present information in the form of a stream of text and often people can benefit from additional information on the topic discussed tv broadcast news can be treated a one such stream of text in this paper we discus finding news article on the web that are relevant to news currently being broadcast we evaluated a variety of algorithm for this problem looking at the impact of inverse document frequency stemming compound history and query length on the relevance and coverage of news article returned in real time during a broadcast we also evaluated several postprocessing technique for improving the precision including reranking using additional term reranking by document similarity and filtering on document similarity for the best algorithm of the article found were relevant with at least of the article being on the exact topic of the broadcast in addition a relevant article wa found for at least of the topic 
we describe web a where a system for associating geography with web page web a where locates mention of place and determines the place each name refers to in addition it assigns to each page a geographic focus a locality that the page discus a a whole the tagging process is simple and fast aimed to be applied to large collection of web page and to facilitate a variety of location based application and data analysis geotagging involves arbitrating two type of ambiguity geo non geo and geo geo a geo non geo ambiguity occurs when a place name also ha a non geographic meaning such a a person name e g berlin or a common word turkey geo geo ambiguity arises when distinct place have the same name a in london england v london ontario an implementation of the tagger within the framework of the webfountain data mining system is described and evaluated on several corpus of real web page precision of up to on individual geotags is achieved we also evaluate the relative contribution of various heuristic the tagger employ and evaluate the focus finding algorithm using a corpus pretagged with locality showing that a many a of the focus reported are correct up to the country level 
document representation and indexing is a key problem for document analysis and processing such a clustering classification and retrieval conventionally latent semantic indexing lsi is considered effective in deriving such an indexing lsi essentially detects the most representative feature for document representation rather than the most discriminative feature therefore lsi might not be optimal in discriminating document with different semantics in this paper a novel algorithm called locality preserving indexing lpi is proposed for document indexing each document is represented by a vector with low dimensionality in contrast to lsi which discovers the global structure of the document space lpi discovers the local structure and obtains a compact document representation subspace that best detects the essential semantic structure we compare the proposed lpi approach with lsi on two standard database experimental result show that lpi provides better representation in the sense of semantic structure 
we study the caching of query result page in web search engine popular search engine receive million of query per day and efficient policy for caching query result may enable them to lower their response time and reduce their hardware requirement we present pdc probability driven cache a novel scheme tailored for caching search result that is based on a probabilistic model of search engine user we then use a trace of over seven million query submitted to the search engine altavista to evaluate pdc a well a traditional lru and slru based caching scheme the trace driven simulation show that pdc outperforms the other policy we also examine the prefetching of search result and demonstrate that prefetching can increase cache hit ratio by for large cache and can double the hit ratio of small cache when integrating prefetching into pdc we attain hit ratio of over 
we categorize the set of client communicating with a server on the web based on information that can be determined by the server the web server us the information to direct tailored action user with poor connectivity may choose not to stay at a web site if it take a long time to receive a page even if the web server at the site is not the bottleneck retaining such client may be of interest to a web site better connected client can receive enhanced representation of web page such a with higher quality image we explore a variety of consideration that could be used by a web server in characterizing a client once a client is characterized a poor or rich the server can deliver altered content alter how content is delivered alter policy and caching decision or decide when to redirect the client to a mirror site we also use network aware client clustering technique to provide a coarser level of client categorization and use it to categorize subsequent client from that cluster for which a client specific categorization is not available our result for client characterization and applicable server action are derived from real recent and diverse set of web server log our experiment demonstrate that a relatively simple characterization policy can classify poor client such that these client subsequently make the majority of badly performing request to a web server this policy is also stable in term of client staying in the same class for a large portion of the analysis period client clustering can significantly help in initially classifying client for which no previous information about the client is known we also show that different server action can be applied to a significant number of request sequence with poor performance 
multinomial naive bayes classifier have been widely used for the probabilistic text classification however their parameter estimation method sometimes generates inappropriate probability in this paper we propose a topic document model approach for naive bayes text classification where their parameter are estimated with an expectation from the training document experiment are conducted on reuters and newsgroup collection and our proposed approach obtained a significant improvement in performace over the conventional approach 
previous work show that a web page can be partitioned into multiple segment or block and often the importance of those block in a page is not equivalent also it ha been proven that differentiating noisy or unimportant block from page can facilitate web mining search and accessibility however no uniform approach and model ha been presented to measure the importance of different segment in web page through a user study we found that people do have a consistent view about the importance of block in web page in this paper we investigate how to find a model to automatically assign importance value to block in a web page we define the block importance estimation a a learning problem first we use a vision based page segmentation algorithm to partition a web page into semantic block with a hierarchical structure then spatial feature such a position and size and content feature such a the number of image and link are extracted to construct a feature vector for each block based on these feature learning algorithm are used to train a model to assign importance to different segment in the web page in our experiment the best model can achieve the performance with micro f and micro accuracy which is quite close to a person s view 
centralized resource description framework rdf repository have limitation both in their failure tolerance and in their scalability existing peer to peer p p rdf repository either cannot guarantee to find query result even if these result exist in the network or require up front definition of rdf schema and designation of super peer we present a scalable distributed rdf repository rdfpeers that store each triple at three place in a multi attribute addressable network by applying globally known hash function to it subject predicate and object thus all node know which node is responsible for storing triple value they are looking for and both exact match and range query can be efficiently routed to those node rdfpeers ha no single point of failure nor elevated peer and doe not require the prior definition of rdf schema query are guaranteed to find matched triple in the network if the triple exist in rdfpeers both the number of neighbor per node and the number of routing hop for inserting rdf triple and for resolving most query are logarithmic to the number of node in the network we further performed experiment that show that the triple storing load in rdfpeers differs by le than an order of magnitude between the most and the least loaded node for real world rdf data 
niche search engine offer an efficient alternative to traditional search engine when the result returned by general purpose search engine do not provide a sufficient degree of relevance by taking advantage of their domain of concentration they achieve higher relevance and offer enhanced feature we discus a new niche search engine ebizsearch based on the technology of citeseer and dedicated to e business and e business document we present the integration of citeseer in the framework of ebizsearch and the process necessary to tune the whole system towards the specific area of e business we also discus how using machine learning algorithm we generate metadata to make ebizsearch open archive compliant ebizsearch is a publicly available service and can be reached at 
organizing web search result into cluster facilitates user quick browsing through search result traditional clustering technique are inadequate since they don t generate cluster with highly readable name in this paper we reformalize the clustering problem a a salient phrase ranking problem given a query and the ranked list of document typically a list of title and snippet returned by a certain web search engine our method first extract and rank salient phrase a candidate cluster name based on a regression model learned from human labeled training data the document are assigned to relevant salient phrase to form candidate cluster and the final cluster are generated by merging these candidate cluster experimental result verify our method s feasibility and effectiveness 
sweetdeal is a rule based approach to representation of business contract that enables software agent to create evaluate negotiate and execute contract with substantial automation and modularity it build upon the situated courteous logic program knowledge representation in ruleml the emerging standard for semantic web xml rule here we newly extend the sweetdeal approach by also incorporating process knowledge description whose ontology are represented in daml oil emerging standard for semantic web ontology thereby enabling more complex contract with behavioral provision especially for handling exception condition e g late delivery or non payment that might arise during the execution of the contract this provides a foundation for representing and automating deal about service in particular about web service so a to help search select and compose them our system is also the first to combine emerging semantic web standard for knowledge representation of rule ruleml with ontology daml oil for a practical e business application domain and further to do so with process knowledge this also newly flesh out the evolving concept of semantic web service a prototype soon public is running 
breaking news often contains timely definition and description of current term organization and personality we utilize such web source to construct definition for such term previous work ha identified definition using hand crafted rule or supervised learning that construct rigid hard text pattern in contrast we demonstrate a new approach that us flexible soft matching pattern to characterize definition sentence our soft pattern are able to effectively accommodate the diversity of definition sentence structure exhibited in news we use pseudo relevance feedback to automatically label sentence for use in soft pattern generation the application of our unsupervised method significantly improves baseline system on both the standardized trec corpus a well a crawled online news article by and respectively in term of f measure when applied to a state of art definition generation system recently fielded in the trec definitional question answering task it improves the performance by 
in this paper we will present three browsing system that should save user s time the first us named entity and give a way to reduce search space by using a information visualization system the user can comprehend more easily the content of a corpus or a document named entity are highlighted for quick reading temporal and geographic representation give a global view of the result of a query all these browse and search help seem to be very useful nevertheless an evaluation would give more practical result 
this paper address the problem of extending an adaptive information filtering system to make decision about the novelty and redundancy of relevant document it argues that relevance and redundance should each be modelled explicitly and separately a set of five redundancy measure are proposed and evaluated in experiment with and without redundancy threshold the experimental result demonstrate that the cosine similarity metric and a redundancy measure based on a mixture of language model are both effective for identifying redundant document 
personal webservers have proven to be a popular mean of sharing file and peer collaboration unfortunately the transient availability and rapidly evolving content on such host render centralized crawl based search index stale and incomplete to address this problem we propose yousearch a distributed search application for personal webservers operating within a shared context e g a corporate intranet with yousearch search result are always fast fresh and complete property we show arise from an architecture that exploit both the extensive distributed resource available at the peer webservers in addition to a centralized repository of summarized network state yousearch extends the concept of a shared context within web community by enabling peer to aggregate into group and user to search over specific group in this paper we describe the challenge design implementation and experience with a successful intranet deployment of yousearch 
an adaptive information filtering system monitor a document stream to identify the document that match information need specified by user profile a the system filter it also refines it knowledge about the user s information need based on long term observation of the document stream and periodic feedback training data from the user low variance profile learning algorithm such a rocchio work well at the early stage of filtering when the system ha very few training data low bias profile learning algorithm such a logistic regression work well at the later stage of filtering when the system ha accumulated enough training data however an empirical system need to work well consistently at all stage of filtering process this paper address this problem by proposing a new technique to combine different text classification algorithm via a constrained maximum likelihood bayesian prior this technique provides a trade off between bias and variance and the combined classifier may achieve a consistent good performance at different stage of filtering we implemented the proposed technique to combine two complementary classification algorithm rocchio and logistic regression the new algorithm is shown to compare favorably with rocchio logistic regression and the best method in the trec and trec adaptive filtering track 
in this paper we propose a document clustering method that strives to achieve a high accuracy of document clustering and the capability of estimating the number of cluster in the document corpus i e the model selection capability to accurately cluster the given document corpus we employ a richer feature set to represent each document and use the gaussian mixture model gmm together with the expectation maximization em algorithm to conduct an initial document clustering from this initial result we identify a set of discriminative featuresfor each cluster and refine the initially obtained document cluster by voting on the cluster label of each document using this discriminative feature set this self refinement process of discriminative feature identification and cluster label voting is iteratively applied until the convergence of document cluster on the other hand the model selection capability is achieved by introducing randomness in the cluster initialization stage and then discovering a value c for the number of cluster n by which running the document clustering process for a fixed number of time yield sufficiently similar result performance evaluation exhibit clear superiority of the proposed method with it improved document clustering and model selection accuracy the evaluation also demonstrate how each feature a well a the cluster refinement process contribute to the document clustering accuracy 
this paper present a simple yet profound idea by thinking about the relationship between and within term and document we can generate a richer representation that encompasses aspect of web link analysis a well a text analysis technique from information retrieval this paper show one path to this unified representation and demonstrates the use of eigenvector calculation from web link analysis by stepping through a simple example 
query expansion ha long been suggested a an effective way to resolve the short query and word mismatching problem a number of query expansion method have been proposed in traditional information retrieval however these previous method do not take into account the specific characteristic of web searching in particular of the availability of large amount of user interaction information recorded in the web query log in this study we propose a new method for query expansion based on query log the central idea is to extract probabilistic correlation between query term and document term by analyzing query log these correlation are then used to select high quality expansion term for new query the experimental result show that our log based probabilistic query expansion method can greatly improve the search performance and ha several advantage over other existing method 
the overwhelming success of the web a a mechanism for facilitating information retrieval and for conducting business transaction ha ledto an increase in the deployment of complex enterprise application these application typically run on web application server which assume the burden of managing many task such a concurrency memory management database access etc required by these application the performance of an application server depends heavily on appropriate configuration configuration is a difficult and error prone task dueto the large number of configuration parameter and complex interaction between them we formulate the problem of finding an optimal configuration for a given application a a black box optimization problem we propose a smart hill climbing algorithm using idea of importance sampling and latin hypercube sampling lh the algorithm is efficient in both searching and random sampling it consists of estimating a local function and then hill climbing in the steepest descent direction the algorithm also learns from past search and restarts in a smart and selective fashion using the idea of importance sampling we have carried out extensive experiment with an on line brokerage application running in a websphere environment empirical result demonstrate that our algorithm is more efficient than and superior to traditional heuristic method 
this paper explores the possibility of using a disk based inverted file structure for collaborative filtering our hypothesis is that this allows for faster calculation of prediction and also that early termination heuristic may be used to further speed up the filtering process and perhaps even improve the quality of the prediction in an experiment on the eachmovie dataset this wa tested our result indicate that searching the inverted file structure is many time faster than general in memory vector search even for very large profile the continue termination heuristic produce the best ranked prediction in our experiment and quit is the top performer in term of speed 
web based search engine such a google and northernlight return document that are relevant to a user query not answer to user question we have developed an architecture that augments existing search engine so that they support natural language question answering the process entail five step query modulation document retrieval passage extraction phrase extraction and answer ranking in this paper we describe some probabilistic approach to the last three of these stage we show how our technique apply to a number of existing search engine and we also present result contrasting three different method for question answering our algorithm probabilistic phrase reranking ppr using proximity and question type feature achieves a total reciprocal document rank of on the trec corpus our technique have been implemented a a web accessible system called nsir 
this paper report on and discus a set of user experiment using the trec web interactive track protocol the focus is on comparing human and machine algorithm in term of performance in a topic distillation task we also investigated the effect of the search result layout in supporting the user effort we have demonstrated that machine can perform nearly a well a people on the topic distillation task given a system tailored to the task there is significant performance improvement and finally given a presentation that support the task there is strong user satisfaction 
prior research under a variety of condition ha shown the cori algorithm to be one of the most effective resource selection algorithm but the range of database size studied wa not large this paper show that the cori algorithm doe not do well in environment with a mix of small and very large database a new resource selection algorithm is proposed that us information about database size a well a database content we also show how to acquire database size estimate in uncooperative environment a an extension of the query based sampling used to acquire resource description experiment demonstrate that the database size estimate are more accurate for large database than estimate produced by a competing method the new resource ranking algorithm is always at least a effective a the cori algorithm and the new algorithm result in better document ranking than the cori algorithm 
link are one of the most important mean for navigation in the world wide web however the visualization of and the interaction with web link have been scarcely explored although link have severe implication on the appearance and usability of web page and the world wide web a such this paper present two study giving first insight of the effect of link visualization technique on reading habit and performance the first user study compare different highlighting technique for link marker and evaluates their effect on reading performance and user acceptance the second study examines link on demand link that appear when pressing a dedicated key and discus their possible effect on reading and browsing habit the finding of the conducted study imply that the standard appearance of link marker ha seriously underestimated effect on the usability of web page they can significantly reduce the readability of the text and alternative should be carefully considered for the design of future web browser 
we demonstrate an adaptive search system that work proactively to help searcher find relevant information the system observes searcher interaction us what it see to model information need and chooses additional query term the system watch for change in the topic of the search and selects retrieval strategy that reflect the extent to which the topic is seen to change 
this paper describes two study that looked at user ability to formulate visual query with a content based image retrieval system that us dominant image colour a the primary indexing key the first experiment examined user performance with two visual search tool a sketch tool and a structured browsing tool with different type of image query the result showed that while user were able to successfully search on the basis of colour and were able to formulate visual query their ability to do so wa affected by search task type search task type wa also shown to be related to search tool choice however the result of study two showed that while user were able to complete all of the task there wa evidence to suggest that a degree of compromise wa present in the user choice of image that wa largely due to problem relating to query formulation 
within five year our personal computer with terabyte disk drive will be able to store everything we read write hear and many of the image we see including video vannevar bush outlined such a system in his famous memex article for the last four year we have worked on mylifebits www mylifebits com http www mylifebits com a system to digitally store everything from one s life including book article personal financial record memorabilia email written correspondence photo time location taken telephone call video television program and web page visited we recently added content from personal device that automatically record photo and audio the project started with the capture of bell s content followed by an effort to explore the use of the sql database for storage and retrieval work ha continued along these line to extend content capture from every useful source e g a meeting capture system the second phase of the project includes the design of tool and link for annotation collection cluster analysis facet for characterizing the content creation of timeline and story and other inherent database related capability e g the ability to pivot on an event or photo or person to retrieve linked information ideally we would like to have a system that would read every document extract meta data e g dublin core and classify it using multiple ontology faceted classification or the relevant while such a system ha implication for future computing device and their user these system will only exist if we can effectively utilize the vast personal store although our system is exploratory the stuff i ve seen system demonstrates the utility and necessity of easy search and access to one s own data other research effort with similar goal relating to personal information include haystack lifestreams and the uk memory for life grand challenge there are serious research issue beyond the problem of making the information useful through rapid and easy retrieval the dear appy problem dear appy my application or platform or medium left me unreadable signed lost data is unsettling to archivist and computer professional and must be solved just navigating the stored life of individual would at first glance appear to take almost a lifetime to sift through while we are making progress in the capture of le traditionally archived content e g meeting phone call video automatic interpretation and index of voice are illusive mylifebits is currently focused on retrieval including the hopefully automatic addition of meta data e g document type identification high level knowledge while such data is essential for the archivist it is unclear how useful such meta data is to a one s own information without such higher level knowledge and concept the vast amount of raw bit may be completely unusable the most cited problem of personal archive is the control of the content including personal security together with joint ownership of content by other individual and organization in many corporation periodic expunging of document is the standard similarly the aspect of a person s life not available in public document is owned by the organization and all document may have to be tagged in such a way that it can be expunged if necessary when an individual is no longer part of the organization the hppa law in the u and even more stringent privacy law in other county have major implication for personal store 
the organization of html into a tag tree structure which is rendered by browser a roughly rectangular region with embedded text and href link greatly help surfer locate and click on link that best satisfy their information need can an automatic program emulate this human behavior and thereby learn to predict the relevance of an unseen href target page w r t an information need based on information limited to the href source page such a capability would be of great interest in focused crawling and resource discovery because it can fine tune the priority of unvisited url in the crawl frontier and reduce the number of irrelevant page which are fetched and discarded 
search algorithm in most current text retrieval system use index data structure extracted from the original text document in this paper we focus on reducing the size of the index by reducing the amount of space dedicated to store term frequency in experiment using trec ad hoc corpus and query set we show that it is possible to store the term frequency in only two bit without decreasing retrieval performance 
most existing clustering algorithm cluster highly related data object such a web page and web user separately the interrelation among different type of data object is either not considered or represented by a static feature space and treated in the same way a other attribute of the object in this paper we propose a novel clustering approach for clustering multi type interrelated data object recom reinforcement clustering of multi type interrelated data object under this approach relationship among data object are used to improve the cluster quality of interrelated data object through an iterative reinforcement clustering process at the same time the link structure derived from relationship of the interrelated data object is used to differentiate the importance of object and the learned importance is also used in the clustering process to further improve the clustering result experimental result show that the proposed approach not only effectively overcomes the problem of data sparseness caused by the high dimensional relationship space but also significantly improves the clustering accuracy 
when searching the www user often desire result restricted to a particular document category ideally a user would be able to filter result with a text classifier to minimize false positive result however current search engine allow only simple query modification to automate the process of generating effective query modification we introduce a sensitivity analysis based method for extracting rule from nonlinear support vector machine the proposed method allows the user to specify a desired precision while attempting to maximize the recall our method performs several level of dimensionality reduction and is vastly faster than searching the combination feature space moreover it is very effective on real world data 
in this paper we propose a novel document clustering method based on the non negative factorization of the term document matrix of the given document corpus in the latent semantic space derived by the non negative matrix factorization nmf each axis capture the base topic of a particular document cluster and each document is represented a an additive combination of the base topic the cluster membership of each document can be easily determined by finding the base topic the axis with which the document ha the largest projection value our experimental evaluation show that the proposed document clustering method surpasses the latent semantic indexing and the spectral clustering method not only in the easy and reliable derivation of document clustering result but also in document clustering accuracy 
the vision of the semantic web can only be realized through proliferation of well known ontology describing different domain to enable interoperability in the semantic web it will be necessary to break these ontology down into smaller well focused unit that may be reused currently three problem arise in that scenario firstly it is difficult to locate ontology to be reused thus leading to many ontology modeling the same thing secondly current tool do not provide mean for reusing existing ontology while building new ontology finally ontology are rarely static but are being adapted to changing requirement hence an infrastructure for management of ontology change taking into account dependency between ontology is needed in this paper we present such an infrastructure addressing the aforementioned problem 
although the owlweb ontology language add considerable expressive power to the semantic web it doe have expressive limitation particularly with respect to what can be said about property wepresent orl owl rule language a horn clause rule extension to owl that overcomes many of these limitation orl extends owl in a syntactically and semantically coherent manner the basic syntax for orl rule is an extension of the abstract syntax for owl dl and owllite orl rule are given formal meaning via an extension of the owldl model theoretic semantics orl rule are given an xml syntax basedon the owl xml presentation syntax and a mapping from orl rule to rdf graph is given based on the owl rdf xml exchange syntax wediscuss the expressive power of orl showing that the ontology consistency problem is undecidable provide several example of orlusage and discus how reasoning support for orl might be provided 
the integration of data produced and collected across autonomous heterogeneous web service is an increasingly important and challenging problem due to the lack of global identifier the same entity e g a product might have different textual representation across database textual data is also often noisy because of transcription error incomplete information and lack of standard format a fundamental task during data integration is matching of string that refer to the same entity in this paper we adopt the widely used and established cosine similarity metric from the information retrieval field in order to identify potential string match across web source we then use this similarity metric to characterize this key aspect of data integration a a join between relation on textual attribute where the similarity of match exceeds a specified threshold computing an exact answer to the text join can be expensive for query processing efficiency we propose a sampling based join approximation strategy for execution in a standard unmodified relational database management system rdbms since more and more web site are powered by rdbmss with a web based front end we implement the join inside an rdbms using sql query for scalability and robustness reason finally we present a detailed performance evaluation of an implementation of our algorithm within a commercial rdbms using real life data set our experimental result demonstrate the efficiency and accuracy of our technique 
this paper present a new protocol for certified email the protocol aim to combine security scalability easy implementation and viable deployment the protocol relies on a light on line trusted third party it can be implemented without any special software for the receiver beyond a standard email reader and web browser and doe not require any public key infrastructure 
the hypothesis that information on the web can be verified automatically with minimal user interaction will be tested by building and evaluating an interactive system in this paper verification is defined a a reasonable determination of the truth or correctness of a statement by examination research or comparison with similar text the system will contain module for reliability ranking query processing document retrieval and document clustering based on agreement the query processing and document retrieval component will use standard ir technique the reliability module will estimate the likelihood that a statement on the web can be trusted using standard developed by information scientist a well a linguistic aspect of the page and the link structure of associated web page the clustering module will cluster relevant document based on whether or not they agree or disagree with the statement to be verified relevant reference are discussed 
the intuition that different text classifier behave in qualitatively different way ha long motivated attempt to build a better metaclassifier via some combination of classifier we introduce a probabilistic method for combining classifier that considers the context sensitive reliability of contributing classifier the method harness reliability indicator variable that provide a valuable signal about the performance of classifier in different situation we provide background present procedure for building metaclassifiers that take into consideration both reliability indicator and classifier output and review a set of comparative study undertaken to evaluate the methodology 
we systematically investigate a new approach to estimating the parameter of language model for information retrieval called parsimonious language model parsimonious language model explicitly address the relation between level of language model that are typically used for smoothing a such they need fewer non zero parameter to describe the data we apply parsimonious model at three stage of the retrieval process at indexing time at search time at feedback time experimental result show that we are able to build model that are significantly smaller than standard model but that still perform at least a well a the standard approach 
the tremendous amount of data resulting from the regular usage of tool for automatic presentation recording demand for elaborate search functionality a detailed analysis of the according multimedia document is required to allow search at a very detailed level unfortunately the produced data differs significantly from traditional document in this demo we discus the problem appearing in the presentation retrieval scenario and introduce aofse a search engine to study and illustrate these problem a well a to develop and present according solution and new approach for this task 
large quantity of document in the internet and digital library are simply scanned and archived in image format many of which are packed in pdf file the word search tool provided by adobe reader acrobat doe not work for these imaged document in this paper we present a search engine to deal with this issue for imaged document in pdf file the experimental result show an encouraging performance 
the objective of this paper is to present a new technique for computing term weight for index term which lead to a new ranking mechanism referred to a set based model the component in our model are no longer term but termsets the novelty is that we compute term weight using a data mining technique called association rule which is time efficient and yet yield nice improvement in retrieval effectiveness the set based model function for computing the similarity between a document and a query considers the termset frequency in the document and it scarcity in the document collection experimental result show that our model improves the average precision of the answer set for all three collection evaluated for the trec collection our set based model led to a gain relative to the standard vector space model of in average precision curve and of in average precision for the top document like the vector space model the set based model ha time complexity that is linear in the number of document in the collection 
following the tradition of these acceptance talk i will begiving my thought on where our field is going any discussion ofthe future of information retrieval ir research however needsto be placed in the context of it history and relationship toother field although ir ha had a very strong relationship withlibrary and information science it relationship to computerscience c and it relative standing a a sub discipline of cshas been more dynamic ir is quite an old field and when a numberof c department were forming in the s it wa not uncommon forum faculty member to be pursuing research related to ir early acmcurriculum recommendation for c contained course on informationretrieval and encyclopedia described ir and database system asdifferent aspect of the same field by the s there were only a few ir researcher in csdepartments in the u s database system wa a separate andthriving field and many felt that ir had stagnated and waslargely irrelevant the truth in fact wa far from that the irresearch community wa a small but dedicated group of researchersin the u s and europe who were motivated by a desire to understandthe process of information retrieval and to build system thatwould help people find the right information in text database this wa and is a hard goal and led to different evaluationmetrics and methodology than the database community progress inthe field wa hampered by a lack of large scale testbeds and testswere limited to database containing at most a few hundred documentabstracts in the s ai boom ir wa still not a mainstream area despiteits focus on a human task involving natural language ir focused ona statistical approach to language rather than the much morepopular knowledge based approach the fact that ir conference mixpapers on effectiveness a measured by human judgment with papersmeasuring performance of file organization for large scale systemshas meant that ir ha always been difficult to classify into simplecategories such a system or ai that are often used in csdepartments since the early s just about everything ha changed large full text database were finally made available for experimentationthrough darpa funding and trec this ha had an enormous positiveimpact on the quantity and quality of ir research the advent ofthe web search engine ha validated the longstanding claim made byir researcher that simple query and ranking were the righttechniques for information access in a largely unstructuredinformation world what ha not changed is that there are stillrelatively few ir researcher in c department there are however many more people in c department doing ir relatedresearch which is just about the same thing conference indatabases machine learning computational linguistics and datamining publish a number of ir paper done by people who would notprimarily consider themselves a ir researcher given that there is an increasing diffusion of ir idea into thecs community it is worth stating what ir a a field of c hasaccomplished search engine have become the infrastructure for much ofinformation access in our society ir ha provided the basicresearch on the algorithm and data structure for these engine and continues to develop new capability such a cross lingualsearch distributed search question answering and topic detectionand tracking ir championed the statistical approach to language long beforeit wa accepted by other researcher working on languagetechnologies statistical nlp is now mainstream and result fromthat field are being used to improve ir system in questionanswering for example ir focused on evaluation a a research area and developed anevaluation methodology based on large standardized testbeds andcomparison with human judgment that ha been adopted byresearchers in a number of other language technology area ir because of it focus on measuring success based on humanjudgments ha always acknowledged the importance of the user andinteraction a a part of information access this led to a numberof contribution to the design of query and search interface andlearning technique based on user feedback although these achievement are important the long term goalsof the ir field have not yet been met what are those goal onepossibility that is often mentioned is the memex of vannevar bush another more recent statement of long term challenge wasmade in the report of the ir challenge workshop global information access satisfy human information needsthrough natural efficient interaction with an automated systemthat leverage world wide structured and unstructured data in anylanguage contextual retrieval combine search technology and knowledgeabout query and user context into a single framework in order toprovide the most appropriate answer for a user s informationneed these goal are in fact very similar to long term challengescoming out of other c field for example jim gray a turingaward winner from the database area mentioned in his address apersonal and world memex a long term goal for his field and c ingeneral ir s long term goal are clearly important long termgoals for the whole of c and achieving those goal will involveeveryone interested in the general area of information managementand retrieval rather than talking about what ir can do inisolation to progress towards it goal i would prefer to talkabout what ir can do in collaboration with other area there are many example of potential collaborative researchareas collaboration with researcher from the nlp and informationextraction community have been developing for some time in orderto study topic such a advanced question answering on the otherhand not enough ha been done to work with the database communityto develop probabilistic retrieval model for unstructured semi structured and structured data there have been a number ofattempts to combine ir and database functionality none of whichhas been particularly successful most recently some group havebeen working on combining ir search with xml document but what isneeded is a comprehensive examination of the issue and problem byteams from both area working together and the creation of newtestbeds that can be used to evaluate proposed model the time isright for such collaboration another example of where database ir and networking people canwork together is in the development of distributed heterogeneousinformation system this requires significant new research inareas like peer to peer architecture semantic heterogeneity automatic metadata generation and retrieval model if the information system described above are extended toinclude new data type such a video image sound and the wholerange of scientific data such a from the bioscience geoscience and astronomy then a broad range of new challenge are added thatneed to be tackled in collaboration with people who know aboutthese type of data there should also be more cooperation between the data mining ir and summarization community to tackle the core problem ofdefining what is new and interesting in stream of data these and other similar collaboration will the basis for thefuture development of the ir field we will continue to work onresearch problem that specifically interest u but this researchwill increasingly be in the context of larger effort ir conceptsand ir research will be an important part of the evolving mix of csexpertise that will be used to solve the grand challenge 
the problem of web cache consistency continues to be an important one current web cache use heuristic based policy for determining the freshness of cached object often forcing content provider to unnecessarily mark their content a uncacheable simply to retain control over it server driven invalidation ha been proposed a a mechanism for providing strong cache consistency for web object but it requires server to maintain per client state even for infrequently changing object we propose an alternative approach to strong cache consistency called monarch which doe not require server to maintain per client state in this work we focus on a new approach for evaluation of monarch in comparison with current practice and other cache consistency policy this approach us snapshot of content collected from real web site a input to a simulator result of the evaluation show monarch generates little more request traffic than an optimal cache coherency policy 
latent semantic indexing lsi is a well established and effective framework for conceptual information retrieval in traditional implementation of lsi the semantic structure of the collection is projected into the k dimensional space derived from a rank k approximation of the original term by document matrix this paper discus a new way to implement the lsi methodology based on polynomial filtering the new framework doe not rely on any matrix decomposition and therefore it computational cost and storage requirement are low relative to traditional implementation of lsi additionally it can be used a an effective information filtering technique when updating lsi model based on user feedback 
an important class of search on the world wide web ha the goal to find an entry page homepage of an organisation entry page search is quite different from ad hoc search indeed a plain ad hoc system performs disappointingly we explored three non content feature of web page page length number of incoming link and url form especially the url form proved to be a good predictor using url form prior we found over of all entry page at rank and up to in the top non content feature can easily be embedded in a language model framework a a prior probability 
in spite of the increase in the availability of mobile device in the last few year web information is not yet a accessible from pda or wap phone a it is from the desktop in this paper we propose a solution for supporting one of the most popular information discovery mechanism namely web directory navigation from mobile device our proposed solution consists of caching enough information on the device itself in order to conduct most of the navigation action locally with subsecond response time while intermittently communicating with the server to receive update and additional data requested by the user the cached information is captured in a directory capsule the directory capsule represents only the portion of the directory that is of interest to the user in a given context and is sufficiently rich and consistent to support the information need of the user in disconnected mode we define a novel subscription model specifically geared for web directory and for the special need of pda this subscription model enables user to specify the part of the directory that are of interest to them a well a the preferred granularity we describe a mechanism for keeping the directory capsule in sync over time with the web directory and user subscription request finally we present the pocket directory browser for palm powered computer that we have developed the pocket directory can be used to define view and manipulate the capsule that are stored on the palm we provide several usage example of our system on the open directory project one of the largest and most popular web directory 
page ranking is a fundamental step towards the construction of effective search engine for both generic horizontal and focused vertical search ranking scheme for horizontal search like the pagerank algorithm used by google operate on the topology of the graph regardless of the page content on the other hand the recent development of vertical portal vortals make it useful to adopt scoring system focussed on the topic and taking the page content into account in this paper we propose a general framework for web page scoring system wpss which incorporates and extends many of the relevant model proposed in the literature finally experimental result are given to ass the feature of the proposed scoring system with special emphasis on vertical search 
over the year software engineering researcher have suggested numerous technique for estimating development effort these technique have been classified mainly a algorithmic machine learning and expert judgement several study have compared the prediction accuracy of those technique with emphasis placed on linear regression stepwise regression and case based reasoning cbr to date no converging result have been obtained and we believe they may be influenced by the use of the same cbr configuration the objective of this paper is twofold first to describe the application of case based reasoning for estimating the effort for developing web hypermedia application second comparing the prediction accuracy of different cbr configuration using two web hypermedia datasets result show that for both datasets the best estimation were obtained with weighted euclidean distance using either one analogy dataset or analogy dataset we suggest therefore that case based reasoning is a candidate technique for effort estimation and with the aid of an automated environment can be applied to web hypermedia development effort prediction 
many common web search by their nature have a very small number of relevant document homepage and namedpage searching are known item search where there is only a single relevant document topic distillation is a special kind of topical relevance search where the user wish to find a few key web site rather than every relevant web page because these type of search are so common web search evaluation have come to focus on task where there are very few relevant document evaluation with few relevant document pose special challenge for current metric in particular the trec topic distillation evaluation is unable to distinguish most submitted run from each other 
due to the high number of inflectional variation of arabic word empirical result suggest that stemming is essential for arabic information retrieval however current light stemming algorithm do not extract the correct stem of irregular so called broken plural which constitute of arabic text and of plural although light stemming in particular ha led to improvement in information retrieval the effect of broken plural on the performance of information retrieval system ha not been examined we propose a light stemmer that incorporates a broken plural recognition component and evaluate it within the context of information retrieval our result show that identifying broken plural and reducing them to their correct stem doe result in a significant improvement in the performance of information retrieval system 
this paper describes seeker a platform for large scale text analytics and semtag an application written on the platform to perform automated semantic tagging of large corpus we apply semtag to a collection of approximately million web page and generate approximately million automatically disambiguated semantic tag published to the web a a label bureau providing metadata regarding the million annotation to our knowledge this is the largest scale semantic tagging effort to date we describe the seeker platform discus the architecture of the semtag application describe a new disambiguation algorithm specialized to support ontological disambiguation of large scale data evaluate the algorithm and present our final result with information about acquiring and making use of the semantic tag we argue that automated large scale semantic tagging of ambiguous content can bootstrap and accelerate the creation of the semantic web 
most test collection like trec and clef for experimental research in information retrieval apply binary relevance assessment this paper introduces a four point relevance scale and report the finding of a project in which trec and trec document pool on topic were reassessed the goal of the reassessment wa to build a subcollection of trec for experiment on highly relevant document and to learn about the assessment process a well a the characteristic of a multigraded relevance corpus relevance criterion were defined so that a distinction wa made between document rich in topical information relevant and highly relevant document and poor in topical information marginally relevant document it turned out that about of document assessed a relevant were regarded a marginal the characteristic of the relevance corpus and lesson learned from the reassessment project are discussed the need to develop more elaborated relevance assessment scheme is emphasized 
library have traditionally used manual image annotation for indexing and then later retrieving their image collection however manual image annotation is an expensive and labor intensive procedure and hence there ha been great interest in coming up with automatic way to retrieve image based on content here we propose an automatic approach to annotating and retrieving image based on a training set of image we assume that region in an image can be described using a small vocabulary of blob blob are generated from image feature using clustering given a training set of image with annotation we show that probabilistic model allow u to predict the probability of generating a word given the blob in an image this may be used to automatically annotate and retrieve image given a word a a query we show that relevance model allow u to derive these probability in a natural way experiment show that the annotation performance of this cross medium relevance model is almost six time a good in term of mean precision than a model based on word blob co occurrence model and twice a good a a state of the art model derived from machine translation our approach show the usefulness of using formal information retrieval model for the task of image annotation and retrieval 
the web ontology language owl defines three class of document lite dl and full all rdf xml document are owl full document some owl full document are also owl dl document and some owl dl document are also owl lite document this paper discus parsing and specie recognition that is the process of determining whether a given document fall into the owl lite dl or full class wedescribe two alternative approach to this task one based on abstract syntax tree the other on rdf triple and compare their key characteristic 
a key missing component in information retrieval system is self diagnostic test to establish whether the system can provide reasonable result for a given query on a document collection if we can measure property of a retrieved set of document which allow u to predict average precision we can automate the decision of whether to elicit relevance feedback or modify the retrieval system in other way we use meta data attached to document in the form of time stamp to measure the distribution of document retrieved in response to a query over the time domain to create a temporal profile for a query we define some useful feature over this temporal profile we find that using these temporal feature together with the content of the document retrieved we can improve the prediction of average precision for a query 
this paper describes an evaluation method based on term relevance set trels that measure an ir system s quality by examining the content of the retrieved result rather than by looking for pre specified relevant page trels consist of a list of term believed to be relevant for a particular query a well a a list of irrelevant term the proposed method doe not involve any document relevance judgment and a such is not adversely affected by change to the underlying collection therefore it can better scale to very large dynamic collection such a the web moreover this method can evaluate a system s effectiveness on an updatable live collection or on collection derived from different data source our experiment show that the proposed method is very highly correlated with official trec measure 
this paper proposes a flexible architecture for the creation of internet auction it allows the custom definition of the auction parameter and provides a decentralized control of the auction process auction policy are defined a law in the law governed interaction lgi paradigm each of these law specifies not only the auction algorithm itself e g open cry dutch etc but also how to handle the other parameter usually involved in the online auction such a certification auditioning and treatment of complaint lgi is used to enforce the rule established in the auction policy within the agent involved in the process after the agent find out about the action they interact in a peer to peer communication protocol reducing the role of the centralized auction room to an advertising registry and taking profit of the distributed nature of the internet to conduct the auction the paper present an example of an auction law illustrating the use of the proposed architecture 
this paper present an approach to develop bidding agent that participate in multiple alternative auction with the goal of obtaining an item at the lowest price the approach consists of a prediction method and a planning algorithm the prediction method exploit the history of past auction in order to build probability function capturing the belief that a bid of a given price may win a given auction the planning algorithm computes the lowest price such that by sequentially bidding in a subset of the relevant auction the agent can obtain the item at that price with an acceptable probability the approach address the case where the auction are for substitutable item with different value experimental result are reported showing that the approach increase the payoff of their user and the welfare of the market 
test collection model use case in way that facilitate evaluation of information retrieval system this paper describes the use of search guided relevance assessment to create a test collection for retrieval of spontaneous conversational speech approximately thematically coherent segment were manually identified in hour of oral history interview with individual automatic speech recognition result manually prepared summary controlled vocabulary indexing and name authority control are available for every segment those feature were leveraged by a team of four relevance assessor to identify topically relevant segment for topic developed from actual user request search guided assessment yielded sufficient inter annotator agreement to support formative evaluation during system development baseline result for ranked retrieval are presented to illustrate use of the collection 
this paper explores feature scoring and selection based on weight from linear classification model it investigates how these method combine with various learning model our comparative analysis includes three learning algorithm na ve bayes perceptron and support vector machine svm in combination with three feature weighting method odds ratio information gain and weight from linear model the linear svm and perceptron experiment show that feature selection using weight from linear svms yield better classification performance than other feature weighting method when combined with the three explored learning algorithm the result support the conjecture that it is the sophistication of the feature weighting method rather than it apparent compatibility with the learning algorithm that improves classification performance 
the web graph is a giant social network whose property have been measured and modeled extensively in recent year most such study concentrate on the graph structure alone and do not consider textual property of the node consequently web community have been characterized purely in term of graph structure and not on page content we propose that a topic taxonomy such a yahoo or the open directory provides a useful framework for understanding the structure of content based cluster and community in particular using a topic taxonomy and an automatic classifier we can measure the background distribution of broad topic on the web and analyze the capability of recent random walk algorithm to draw sample which follow such distribution in addition we can measure the probability that a page about one broad topic will link to another broad topic extending this experiment we can measure how quickly topic context is lost while walking randomly on the web graph estimate of this topic mixing distance may explain why a global pagerank is still meaningful in the context of broad query in general our measurement may prove valuable in the design of community specific crawler and link based ranking system 
this paper provides an extensive analysis of pre stored streaming medium workload focusing on the client interactive behavior we analyze four workload that fall into three different domain namely education entertainment video and entertainment audio our main goal are a to identify qualitative similarity and difference in the typical client behavior for the three workload class and b to provide data for generating realistic synthetic workload 
this paper describes a novel approach for obtaining semantic interoperability among data source in a bottom up semi automatic manner without relying on pre existing global semantic model we assume that large amount of data exist that have been organized and annotated according to local schema seeing semantics a a form of agreement our approach enables the participating data source to incrementally develop global agreement in an evolutionary and completely decentralized process that solely relies on pair wise local interaction participant provide translation between schema they are interested in and can learn about other translation by routing query gossiping to support the participant in assessing the semantic quality of the achieved agreement we develop a formal framework that take into account both syntactic and semantic criterion the assessment process is incremental and the quality rating are adjusted along with the operation of the system ultimately this process result in global agreement i e the semantics that all participant understand we discus strategy to efficiently find translation and provide result from a case study to justify our claim our approach applies to any system which provides a communication infrastructure existing website or database decentralized system p p system and offer the opportunity to study semantic interoperability a a global phenomenon in a network of information sharing party 
web search engine help user find useful information on the world wide web www however when the same query is submitted by different user typical search engine return the same result regardless of who submitted the query generally each user ha different information need for his her query therefore the search result should be adapted to user with different information need in this paper we first propose several approach to adapting search result according to each user s need for relevant information without any user effort and then verify the effectiveness of our proposed approach experimental result show that search system that adapt to each user s preference can be achieved by constructing user profile based on modified collaborative filtering with detailed analysis of user s browsing history in one day 
noun phrase in query are identified and classified into four type proper name dictionary phrase simple phrase and complex phrase a document ha a phrase if all content word in the phrase are within a window of a certain size the window size for different type of phrase are different and are determined using a decision tree phrase are more important than individual term consequently document in response to a query are ranked with matching phrase given a higher priority we utilize wordnet to disambiguate word sens of query term whenever the sense of a query term is determined it synonym hyponym word from it definition and it compound word are considered for possible addition to the query experimental result show that our approach yield between and improvement over the best known result on the trec and collection for short title only query without using web data 
we present a novel algorithm for the fast computation of pagerank a hyperlink based estimate of the importance of web page the original pagerank algorithm us the power method to compute successive iterates that converge to the principal eigenvector of the markov matrix representing the web link graph the algorithm presented here called quadratic extrapolation accelerates the convergence of the power method by periodically subtracting off estimate of the nonprincipal eigenvectors from the current iterate of the power method in quadratic extrapolation we take advantage of the fact that the first eigenvalue of a markov matrix is known to be to compute the nonprincipal eigenvectors using successive iterates of the power method empirically we show that using quadratic extrapolation speed up pagerank computation by on a web graph of million node with minimal overhead our contribution is useful to the pagerank community and the numerical linear algebra community in general a it is a fast method for determining the dominant eigenvector of a matrix that is too large for standard fast method to be practical 
recent research ha had some success using the length of time a user display a document in their web browser a implicit feedback for document preference however most study have been confined to specific search domain such a news and have not considered the effect of task on display time and the potential impact of this relationship on the effectiveness of display time a implicit feedback we describe the result of an intensive naturalistic study of the online information seeking behavior of seven subject during a fourteen week period throughout the study subject online information seeking activity were monitored with various piece of logging and evaluation software subject were asked to identify the task with which they were working classify the document that they viewed according to these task and evaluate the usefulness of the document result of a user centered analysis demonstrate no general direct relationship between display time and usefulness and that display time differ significantly according to specific task and according to specific user 
hunter gatherer is an interface that let web user carry out three main task collect component from within web page represent those component in a collection edit those component collection our research show that while the practice of making collection of content from within web page is common it is not frequent due in large part to poor interaction support in existing tool we engaged with user in task analysis a well a iterative design review in order to understand the interaction issue that are part of within web page collection making and to design an interaction that would support that process we report here on that design development a well a on the evaluation of the tool that evolved from that process and the future work stemming from these result in which our critical question is what happens to user perception and expectation of web based information their web based information management practice when they can treat this information a harvestable recontextualizable data rather than a fixed page 
we present an alternative technique for discovering aggregate usage profile from web access log the technique is based on clustering information need inferred from user browsing path browsing path are extracted from user access log information need is inferred from each browsing path by using the ostensive model the technique is evaluated in a document recommendation application we compare the performance of our technique against the well established transaction based technique proposed in based on an initial evaluation the result are encouraging 
we investigate the criterion used by online searcher when assessing the relevance of web page to information seeking task twenty four searcher were given three task each and indicated the feature of web page which they employed when deciding about the usefulness of the page these task were presented within the context of a simulated work task situation the result of this study provide a set of criterion used by searcher to decide about the utility of web page such criterion have implication for the design of system that use or recommend web page a well a to author of web page 
identifier resolution is presented a a way to link the physical world with virtual web resource in this paradigm designed to support nomadic user the user employ a handheld wirelessly connected sensor equipped device to read identifier associated with physical entity the identifier are resolved into virtual resource or action related to the physical entity a though the user clicked on a physical hyperlink we have integrated identifier resolution with the web so that it can be deployed a ubiquitously a the web in the infrastructure and on wirelessly connected handheld device we enable user to capture resolution service and application a web resource in their local context we use the web to invoke resolution service with a model of physical web form filling we propose a scheme for binding identifier to resource to promote service and application linking the physical and virtual world 
increasingly the world wide web is being viewed a a mean of creating web community rather than simply a a mean of publishing and delivering document and service in this research we develop the concept of federated information sharing community fisc and associated architecture that enables community centred information system to be constructed such system provide a way for organisation distributed workgroups and individual to build up a federated community based on their common interest over the world wide web to support community we develop capability that go beyond the generic retrieval of document to include the ability to retrieve people their interest and inter relationship we focus on providing social awareness in the large to help user understand the member within a community and the relationship between them who is working on what topic and who is working with whom within the fisc framework we provide a viewpoint retrieval service to enable a user to construct visual contextual view of the community from the perspective of any community member to evaluate these idea we develop test bed to compare individual component technology such a user and group profile construction and similarity matching and we develop prototype web network and citeseer community to explore the broader architecture and usage issue 
we discus the automatic generation of thematic lexicon by mean of term categorization a novel task employing technique from information retrieval ir and machine learning ml specifically we view the generation of such lexicon a an iterative process of learning previously unknown association between term and theme i e discipline or field of activity the process is iterative in that it generates for each ci in a set c c cm of theme a sequence li li lin of lexicon bootstrapping from an initial lexicon li and a set of text corpus thgr thgr thgr n given a input the method is inspired by text categorization the discipline concerned with labelling natural language text with label from a predefined set of theme or category however while text categorization deal with document represented a vector in a space of term term categorization deal dually with term represented a vector in a space of document and label term instead of document with theme a a learning device we adopt boosting since a it ha demonstrated state of the art effectiveness in a variety of text categorization application and b it naturally allows for a form of data cleaning thereby making the process of generating a thematic lexicon an iteration of generate and test step 
this paper present a novel statistical model for cross language information retrieval given a written query in the source language document in the target language are ranked by integrating probability computed by two statistical model a query translation model which generates most probable term by term translation of the query and a query document model which evaluates the likelihood of each document and translation integration of the two score is performed over the set of n most probable translation of the query experimental result with value n are presented on the italian english bilingual track data used in the clef and evaluation campaign 
youserv is a system that allows it user to pool existing desktop computing resource for high availability web hosting and file sharing by exploiting standard web and internet protocol e g http and dns youserv doe not require those who access youserv published content to install special purpose software because it requires minimal server side resource and administration youserv can be provided at a very low cost we describe the design implementation and a successful intranet deployment of the youserv system and compare it with several alternative 
in this paper we address the problem of searching schema database for semantically related schema we first give a method of finding semantic similarity between pair wise schema based on tokenization part of speech tagging word expansion and ontology matching we then address the problem of indexing the schema database through a semantic hash table matching schema in the database are found by hashing the query attribute and recording peak in the histogram of schema hit result indicated a improvement in search performance while maintaining high precision and recall 
in this paper we argue that cache consistency mechanism designed for stand alone proxy do not scale to the large number of proxy in a content distribution network and are not flexible enough to allow consistency guarantee to be tailored to object need to meet the twin challenge of scalability and flexibility we introduce the notion of cooperative consistency along with a mechanism called cooperative lease to achieve it by supporting dgr consistency semantics and by using a single lease for multiple proxy cooperative lease allows the notion of lease to be applied in a flexible scalable manner to cdns further the approach employ application level multicast to propagate server notification to proxy in a scalable manner we implement our approach in the apache web server and the squid proxy cache and demonstrate it efficacy using a detailed experimental evaluation our result show a factor of reduction in server message overhead and a reduction in server state space overhead when compared to original lease albeit at an increased inter proxy communication overhead 
resource description framework rdf is a general description technology that can be applied to many application domain redland is a flexible and efficient rdf system that complement this power and provides high level interface allowing instance of the model to be stored queried and manipulated in c perl python tcl java and other language it is implemented using an object based api providing several of the implementation class a module which can be added removed or replaced to allow different functionality or application specific optimisation the framework provides the core technology for developing new rdf application experimenting with implementation technique apis and representation 
in this paper we introduce an information theoretic method for estimating the usefulness of the hyperlink structure induced from the set of retrieved document we evaluate the effectiveness of this method in the context of an optimal bayesian decision mechanism which selects the most appropriate retrieval approach on a per query basis for two trec task the estimation of the hyperlink structure s usefulness is stable when we use different weighting scheme or when we employ sampling of document to reduce the computational overhead next we evaluate the effectiveness of the hyperlink structure s usefulness in a realistic setting by setting the threshold of a decision mechanism automatically our result show that improvement over the baseline are obtained 
for most web based application content are created dynamically based on the current state of a business such a product price and inventory stored in database system these application demand personalized content and track user behavior while maintaining application integrity many of such practice are not compatible with web acceleration solution consequently although many web acceleration solution have shown promising performance improvement and scalability architecting and engineering distributed enterprise web application to utilize available content delivery network remains a challenge in this paper we examine the challenge to accelerate j ee based enterprise web application we list obstacle and recommend some practice to transform typical database driven j ee application to cache friendly web application where web acceleration solution can be applied furthermore such transformation should be done without modification to the underlying application business logic and without sacrificing function that are essential to e commerce we take the j ee reference software the java petstore a a case study by using the proposed guideline we are able to cache more than of the content in the petstore and scale up the web site more than time 
real world application often require the classification of document under situation of small number of feature mi labeled document and rare positive example this paper investigates the robustness of three regularized linear classification method svm ridge regression and logistic regression under above situation we compare these method in term of their loss function and score distribution and establish the connection between their optimization problem and generalization error bound several set of controlled experiment on the reuters corpus are conducted to investigate the robustness of these method our result show that ridge regression seems to be the most promising candidate for rare class problem 
a wealth of information relevant for e commerce often appears intext form this includes specification and performance data sheetsof product financial statement product offering etc typicallythese type of product and financial data are published in tabularform the only separator between item in the table are whitespaces and line separator we will refer to such table a texttables due to the lack of structure in such table theinformation present is not readily queriable using traditionaldatabase query language like sql one way to make it amenable tostandard database querying technique is to extract the data itemsin the table and create a database out of the extracted data butextraction from text table pose difficulty due to theirregularity of the data in the column existing technique like and are based on finding fixedseparators between successive column however it is not alwayspossible to find fixed separator even if fixed separator existthey may not unambiguously separate column that have multiworditems another set of technique are based on regular expression the problem here are i they are difficult to construct and ii they depend on lexical similarity between column item note that by visual inspection a casual observer can correctlyassociate every item in a text table to it corresponding column this is because all the item belonging to a column appear clustered more closely to each other than to item in differentcolumns whereas such cluster can be clearly discerned by a humanobserver making them machine recognizable is the key to robustautomated extraction of data item from text based table clustering enables u to make association between item in acolumn based not merely on examining item in adjacent row butacross all the row in the table we have designed and implemented the cutex system forextracting data from irregular text table the input is a filecontaining only text table the output produced by cutex isan association between every item in a column note thatcutex doe not do table detection in text the innovativeaspect of cutex is it clustering based algorithm thatdrives the extraction process in cutex each line is brokendown into a set of token each token is a contiguous sequence ofnon white space character the center of any token in a cluster iscloser to the center of some other token in the same cluster inter cluster gap are gap between the extremal token in theclusters starting with an initial set of cluster adjacentclusters are merged into bigger cluster based on the inter clustergaps the algorithm terminates when no more cluster can be merged we have formalized the notion of a correct extraction and developeda syntactic characterization of table on which this algorithm willalways produce a correct extraction detail appear in anunique aspect of the algorithm is it robustness in the presence ofmisalignments precision of extraction can be improved by supplying the minimumseparation between column a a parameter such a separator isestimated by sampling a few input table the clustering algorithmdoes not merge adjacent cluster if the gap between them is largerthan this parameter value note though that the minimum column gapcannot be used a a fixed separator since doing so amount to doinglocalized determination making it brittle to misalignment cutex is implemented in java and is approximately about line of code the system automatically partition the set ofinput text table into directory containing correct and incorrectextractions at the end of an extraction the user can examine thedirectory containing incorrectly extracted table sample a few ofthem identify if it wa caused by an erroneous estimate of theminimum column gap re adjust the configuration parameter and starta new extraction on all these table successive iteration cangenerate a higher extraction yield the primary focus of the demonstration will be on illustratingthe robustness and the iterative process of improving theextraction yield of the clustering algorithm 
the larger amount of information on the web is stored in document database and is not indexed by general purpose search engine i e google and yahoo such information is dynamically generated through querying database which are referred to a hidden web database document returned in response to a user query are typically presented using template generated web page this paper proposes a novel approach that identifies web page template by analysing the textual content and the adjacent tag structure of a document in order to extract query related data preliminary result demonstrate that our approach effectively detects template and retrieves data with high recall and precision 
preview and overview of large heterogeneous information resource help user comprehend the scope of collection and focus on particular subset of interest for narrative document question of what happened where and when are natural point of entry building on our earlier work at the perseus project with detecting term place name and date we have exploited co occurrence of date and place name to detect and describe likely event in document collection we compare statistical measure for determining the relative significance of various event we have built interface that help user preview likely region of interest for a given range of space and time by plotting the distribution and relevance of various collocation user can also control the amount of collocation information in each view once particular collocation are selected the system can identify key phrase associated with each possible event to organize browsing of the document themselves 
an increasingly large amount of web application employ serviceobjects such a servlets to generate dynamic and personalizedcontent existing caching infrastructure are not well suited forcaching such content in mobile environment because ofdisconnection and weak connection one possible approach to thisproblem is to replicate web related application logic to clientdevices the challenge to this approach are to deal with clientdevices that exhibit huge divergence in resource availability tosupport application that have different data sharing and coherencyrequirements and to accommodate the same application underdifferent deployment environment the replet system target these challenge it us client server and application capability and preference information cpi to direct the replication of service object to client device from the selection of a device for replication and populating thedevice with client specific data to choosing an appropriatereplica to serve a given request and maintaining the desired stateconsistency among replica the replet system exploit on devicereplication to enable client serverand application specificcost metric for replica invocation and synchronization we haveimplemented a prototype in the context of servlet based webapplications our experiment and simulation result demonstrate theviability and significant benefit of cpi driven on device serviceobject replication 
this poster session examines a probabilistic approach to distributed information retrieval using a logistic regression algorithm for estimation of collection relevance the algorithm is compared to other method for distributed search using test collection developed for distributed search evaluation 
while monochrome unformatted text and richly colored graphical content are both capable of conveying a message well designed graphical content ha the potential for better engaging the human sensory system it is our contention that the author of an audio presentation should be afforded the benefit of judiciously exploiting the human aural perceptual ability to deliver content in a more compelling concise and realistic manner while contemporary streaming medium player and voice browser share the ability to render content non textually neither technology is currently capable of rendering three dimensional medium the contribution described in this paper are proposed d audio extension to smil and a server based framework able to receive a request and on demand process such a smil file and dynamically create the multiple simultaneous audio object spatialize them in d space multiplex them into a single stereo audio and prepare it for transmission over an audio stream to a mobile device to the knowledge of the author this is the first reported solution for delivering and rendering on a commercially available wireless handheld device a rich d audio listening experience a described by a markup language naturally in addition to mobile device this solution also work with desktop streaming medium player 
the mantra of every experienced web application developer is the same thou shalt separate business logic from display ironically almost all template engine allow violation of this separation principle which is the very impetus for html template engine development this situation is due mostly to a lack of formal definition of separation and fear that enforcing separation emasculates a template s power i show that not only is strict separation a worthy design principle but that we can enforce separation while providing a potent template engine i demonstrate my stringtemplate engine used to build jguru com and other commercial site at work solving some nontrivial generational task my goal is to formalize the study of template engine thus providing a common nomenclature a mean of classifying template generational power and a way to leverage interesting result from formal language theory i classify three type of restricted template analogous to chomsky s type grammar class and formally define separation including the rule that embody separation because this paper provides a clear definition of model view separation template engine designer may no longer blindly claim enforcement of separation moreover given theoretical argument and empirical evidence programmer no longer have an excuse to entangle model and view 
we suggest a way for locating duplicate and plagiarism in a text collection using an r measure which is the normalized sum of the length of all suffix of the text repeated in other document of the collection the r measure can be effectively computed using the suffix array data structure additionally the computation procedure can be improved to locate the set of duplicate or plagiarised document we applied the technique to several standard text collection and found that they contained a significant number of duplicate and plagiarised document another reformulation of the method lead to an algorithm that can be applied to supervised multi class categorization we illustrate the approach using the recently available reuters corpus volume rcv the result show that the method outperforms svm at multi class categorization and interestingly that result correlate strongly with compression based method 
passage retrieval is an important component common to many question answering system because most evaluation of question answering system focus on end to end performance comparison of common component becomes difficult to address this shortcoming we present a quantitative evaluation of various passage retrieval algorithm for question answering implemented in a framework called pauchok we present three important finding boolean querying scheme perform well in the question answering task the performance difference between various passage retrieval algorithm vary with the choice of document retriever which suggests significant interaction between document retrieval and passage retrieval the best algorithm in our evaluation employ density based measure for scoring query term our result reveal future direction for passage retrieval and question answering 
one of the major problem in question answering qa is that the query are either too brief or often do not contain most relevant term in the target corpus in order to overcome this problem our earlier work integrates external knowledge extracted from the web and wordnet to perform event based qa on the trec task this paper extends our approach to perform event based qa by uncovering the structure within the external knowledge the knowledge structure loosely model different facet of qa event and is used in conjunction with successive constraint relaxation algorithm to achieve effective qa our result obtained on trec qa corpus indicate that the new approach is more effective and able to attain a confidence weighted score of above 
when searching large hypertext document collection it is often possible that there are too many result available for ambiguous query query refinement is an interactive process of query modification that can be used to narrow down the scope of search result we propose a new method for automatically generating refinement or related term to query by mining anchor text for a large hypertext document collection we show that the usage of anchor text a a basis for query refinement produce high quality refinement suggestion that are significantly better in term of perceived usefulness compared to refinement that are derived using the document content furthermore our study suggests that anchor text refinement can also be used to augment traditional query refinement algorithm based on query log since they typically differ in coverage and produce different refinement our result are based on experiment on an anchor text collection of a large corporate intranet 
the empirical investigation of the effectiveness of information retrieval ir system requires a test collection a set of query topic and a set of relevance judgment made by human assessor for each query previous experiment show that difference in human relevance assessment do not affect the relative performance of retrieval system based on this observation we propose and evaluate a new approach to replace the human relevance judgment by an automatic method ranking of retrieval system with our methodology correlate positively and significantly with that of human based evaluation in the experiment we assume a web like imperfect environment the indexing information for all document is available for ranking but some document may not be available for retrieval such condition can be due to document deletion or network problem our method of simulating imperfect environment can be used for web search engine assessment and in estimating the effect of network condition e g network unreliability on ir system performance 
we introduce an emergent collaborative filing system in such a system an individual is allowed to organize a subset of document in a repository into a personal hierarchy and share the hierarchy with others the system generates a consensus hierarchy from all user personal hierarchy which provides a full common and emergent view of all document we believe that collaborative filing help translate personal tacit knowledge into sharable structure which help the user a well a community of which he or she is a part our filing system is suitable for any document from text to multimedia file initial result on an experimental website show promise for a knowledge task involving extensive document retrieval hierarchy are not only used frequently but are also effective in identifying high quality document one surprising finding is how often subject use others personal hierarchy and upon close examination social network play a key role a well 
text classifier that give probability estimate are more readily applicable in a variety of scenario for example rather than choosing one set decision threshold they can be used in a bayesian risk model to issue a run time decision which minimizes a user specified cost function dynamically chosen at prediction time however the quality of the probability estimate is crucial we review a variety of standard approach to converting score and poor probability estimate from text classifier to high quality estimate and introduce new model motivated by the intuition that the empirical score distribution for the extremely irrelevant hard to discriminate and obviously relevant item are often significantly different finally we analyze the experimental performance of these model over the output of two text classifier the analysis demonstrates that one of these model is theoretically attractive introducing few new parameter while increasing flexibility computationally efficient and empirically preferable 
with the huge amount of information available electronically there is an increasing demand for automatic text summarization system the use of machine learning technique for this task allows one to adapt summary to the user need and to the corpus characteristic these desirable property have motivated an increasing amount of work in this field over the last few year most approach attempt to generate summary by extracting sentence segment and adopt the supervised learning paradigm which requires to label document at the text span level this is a costly process which put strong limitation on the applicability of these method we investigate here the use of semi supervised algorithm for summarization these technique make use of few labeled data together with a larger amount of unlabeled data we propose new semi supervised algorithm for training classification model for text summarization we analyze their performance on two data set the reuters news wire corpus and the computation and language cmp lg collection of tipster summac we perform comparison with a baseline non learning system and a reference trainable summarizer system 
we extend the applicability of impact transformation which is a technique for adjusting the term weight assigned to document so a to boost the effectiveness of retrieval when short query are applied to large document collection in conjunction with technique called quantization and thresholding impact transformation allows improved query execution rate compared to traditional vector space similarity computation a the number of arithmetic operation can be reduced the transformation also facilitates a new dynamic query pruning heuristic we give result based upon the trec web data that show the combination of these various technique to yield highly competitive retrieval in term of both effectiveness and efficiency for both short and long query 
much attention ha been paid to the relative effectiveness of interactive query expansion versus automatic query expansion although interactive query expansion ha the potential to be an effective mean of improving a search in this paper we show that on average human searcher are le likely than system to make good expansion decision to enable good expansion decision searcher must have adequate instruction on how to use interactive query expansion functionality we show that simple instruction on using interactive query expansion do not necessarily help searcher make good expansion decision and discus difficulty found in making query expansion decision 
the world wide web is emerging not only a an infrastructure for data but also for a broader variety of resource that are increasingly being made available a web service relevant current standard like uddi wsdl and soap are in their fledgling year and form the basis of making web service a workable and broadly adopted technology however realizing the fuller scope of the promise of web service and associated service oriented architecture will requite further technological advance in the area of service interoperation service discovery service composition and process orchestration semantics especially a supported by the use of ontology and related semantic web technology are likely to provide better qualitative and scalable solution to these requirement just a semantic annotation of data in the semantic web is the first critical step to better search integration and analytics over heterogeneous data semantic annotation of web service is an equally critical first step to achieving the above promise our approach is to work with existing web service technology and combine them with idea from the semantic web to create a better framework for web service discovery and composition in this paper we present mwsaf meteor s web service annotation framework a framework for semi automatically marking up web service description with ontology we have developed algorithm to match and annotate wsdl file with relevant ontology we use domain ontology to categorize web service into domain an empirical study of our approach is presented to help evaluate it performance 
we address the problem of integrating object from a source taxonomy into a master taxonomy this problem is not only currently pervasive on the web but also important to the emerging semantic web a straightforward approach to automating this process would be to learn a classifier that can classify object from the source taxonomy into category of the master taxonomy the key insight is that the availability of the source taxonomy data could be helpful to build better classifier for the master taxonomy if their categorization have some semantic overlap in this paper we propose a new approach co bootstrapping to enhance the classification by exploiting such implicit knowledge our experiment with real world web data show substantial improvement in the performance of taxonomy integration 
our demonstration show the hierarchy system working on a locally run search engine hierarchy are dynamically generated from the retrieved document and visualised on the menu when a user selects a term from the hierarchy the document linked to the term are listed and the term is then added to the initial query to rerun a search through the demonstration we illustrate how hierarchical presentation of expansion term is achieved and how our approach support user to articulate their information need using the hierarchy 
this paper proposes a novel unified and systematic approach to combine collaborative and content based filtering for ranking and user preference prediction the framework incorporates all available information by coupling together multiple learning problem and using a suitable kernel or similarity function between user item pair we propose and evaluate an on line algorithm jrank that generalizes perceptron learning using this framework and show significant improvement over other approach 
query by melody is the problem of retrieving musical performance from melody retrieval of real performance is complicated due to the large number of variation in performing a melody and the presence of colored accompaniment noise we describe a simple yet effective probabilistic model for this task we describe a generative model that is rich enough to capture the spectral and temporal variation of musical performance and allows for tractable melody retrieval while most of previous study on music retrieval from melody were performed with either symbolic e g midi data or with monophonic single instrument performance we performed experiment in retrieving live and studio recording of opera that contain a leading vocalist and rich instrumental accompaniment our result show that the probabilistic approach we propose is effective and can be scaled to massive datasets 
in this paper we describe a cross document summarizer xdox designed specifically to summarize large document set document and more such set of document are typically obtained from routing or filtering system run against a continuous stream of data such a a newswire xdox work by identifying the most salient theme within the set at the granularity level that is regulated by the user and composing an extraction summary which reflects these main theme in the current version xdox is not optimized to produce a summary based on a few unrelated document indeed such summary are best obtained simply by concatenating summary of individual document we show example of summary obtained in our test a well a from our participation in the first document understanding conference duc 
model theoretic semantics is a formal account of the interpretation of legitimate expression of a language it is increasingly being used to provide web markup language with well defined semantics but a discussion of it role and limitation for the semantic web ha not yet received a coherent and detailed treatment this paper take the first step towards such a treatment the major result is an introductory explication of key idea that are usually only implicit in existing account of semantics for the web reference to more detailed account of these idea are also provided the benefit of this explication is increased awareness among web user of some important issue inherent in using model theoretic semantics for web markup language 
although text categorization is a burgeoning area of ir research readily available test collection in this field are surprisingly scarce we describe a methodology and system named accio for automatically acquiring labeled datasets for text categorization from the world wide web by capitalizing on the body of knowledge encoded in the structure of existing hierarchical directory such a the open directory we define parameter of category that make it possible to acquire numerous datasets with desired property which in turn allow better control over categorization experiment in particular we develop metric that estimate the difficulty of a dataset by examining the host directory structure these metric are shown to be good predictor of categorization accuracy that can be achieved on a dataset and serve a efficient heuristic for generating datasets subject to user s requirement a large collection of automatically generated datasets are made available for other researcher to use 
this paper focus on the optimization of the navigation through voluminous subsumption hierarchy of topic employed by portal catalog like netscape open directory odp we advocate for the use of labeling scheme for modeling these hierarchy in order to efficiently answer query such a subsumption check descendant ancestor or nearest common ancestor which usually require costly transitive closure computation we first give a qualitative comparison of three main family of scheme namely bit vector prefix and interval based scheme we then show that two labeling scheme are good candidate for an efficient implementation of label querying using standard relational dbms namely the dewey prefix scheme and an interval scheme by agrawal borgida and jagadish we compare their storage and query evaluation performance for the odp hierarchy using the postgresql engine 
internet search engine and comparison shopping have recently begun implementing a paid placement strategy where some content provider are given prominent positioning in return for a placement fee this bias generates placement revenue but creates a disutility to user thus reducing user based revenue we formulate the search engine design problem a a tradeoff between these two type of revenue we demonstrate that the optimal placement strategy depends on the relative benefit to provider and disutilities to user of paid placement we compute the optimal placement fee characterize the optimal bias level and analyze sensitivity of the placement strategy to various factor in the optimal paid placement strategy the placement revenue are set below the monopoly level due to it negative impact on advertising revenue an increase in the search engine s quality of service allows it to improve profit from paid placement moving it closer to the ideal however an increase in the value per user motivates the gatekeeper to increase market share by reducing further it reliance on paid placement and fraction of paying provider 
metadata for the world wide web is important but metadata for peer to peer p p network is absolutely crucial in this paper we discus the open source project edutella which build upon metadata standard defined for the www and aim to provide an rdf based metadata infrastructure for p p application building on the recently announced jxta framework we describe the goal and main service this infrastructure will provide and the architecture to connect edutella peer based on exchange of rdf metadata a the query service is one of the core service of edutella upon which other service are built we specify in detail the edutella common data model ecdm a basis for the edutella query exchange language rdf qel i and format implementing distributed query over the edutella network finally we shortly discus registration and mediation service and introduce the prototype and application scenario for our current edutella aware peer 
since many arabic document are available only in print automating retrieval from collection of scanned arabic document image using optical character recognition ocr is an interesting problem arabic combine rich morphology with a writing system that present unique challenge to ocr system these factor must be considered when selecting term for automatic indexing in this paper alternative choice of indexing term are explored using both an existing electronic text collection and a newly developed collection built from image of actual printed arabic document character n gram or lightly stemmed word were found to typically yield near optimal retrieval effectiveness and combining both type of term resulted in robust performance across a broad range of condition 
we review here the result of one of the experiment performed at the reliable information access ria workshop hosted by mitre corporation and the northeast regional research center nrrc the experiment concentrate on query expansion using relevance feedback and explores the behaviour of several information retrieval system using variable number of relevant document 
bilingual dictionary have been commonly used for query translation in cross language information retrieval clir however we are faced with the problem of translation selection several recent study suggested the utilization of term co occurrence in this selection this paper present two extension to improve them first we extend the basic co occurrence model by adding a decaying factor that decrease the mutual information when the distance between the term increase second we incorporate a triple translation model in which syntactic dependence relation represented a triple are integrated our evaluation on translation accuracy show that translating triple a unit is more precise than a word by word translation our clir experiment show that the addition of the decaying factor lead to substantial improvement of the basic co occurrence model and the triple translation model brings further improvement 
wide area database replication technology and the availability of content delivery network allow web application to be hosted and served from powerful data center this form of application support requires a complete web application suite to be distributed along with the database replica a major advantage of this approach is that dynamic content is served from location closer to user leading into reduced network latency and fast response time however this is achieved at the expense of overhead due to a invalidation of cached dynamic content in the edge cache and b synchronization of database replica in the data center these have adverse effect on the freshness of delivered content in this paper we propose a freshness driven adaptive dynamic content caching which monitor the system status and adjusts caching policy to provide content freshness guarantee the proposed technique ha been intensively evaluated to validate it effectiveness the experimental result show that the freshness driven adaptive dynamic content caching technique consistently provides good content freshness furthermore even a web site that enables dynamic content caching can further benefit from our solution which improves content freshness up to time especially under heavy user request traffic and long network latency condition our approach also provides better scalability and significantly reduced response time up to in the experiment 
we present a probabilistic model for a document corpus that combine many of the desirable feature of previous model the model is called gap for gamma poisson the distribution of the first and last random variable gap is a factor model that is it give an approximate factorization of the document term matrix into a product of matrix and x these factor have strictly non negative term gap is a generative probabilistic model that assigns finite probability to document in a corpus it can be computed with an efficient and simple em recurrence for a suitable choice of parameter the gap factorization maximizes independence between the factor so it can be used a an independent component algorithm adapted to document data the form of the gap model is empirically a well a analytically motivated it give very accurate result a a probabilistic model measured via perplexity and a a retrieval model the gap model project document and term into a low dimensional space of theme and model text a passage of term on the same theme 
we describe an empirical evaluation of the utility of thumbnail preview in web search result result page were constructed to show text only summary thumbnail preview only or the combination of text summary and thumbnail preview we found that in the combination case user were able to make more accurate decision about the potential relevance of result than in either of the other version with hardly any increase in speed of processing the page a a whole 
recent work ha demonstrated that the assessment of pairwise object similarity can be approached in an axiomatic manner using information theory we extend this concept specifically to document similarity and test the effectiveness of an information theoretic measure for pairwise document similarity we adapt query retrieval to rate the quality of document similarity measure and demonstrate that our proposed information theoretic measure for document similarity yield statistically significant improvement over other popular measure of similarity 
previous research on cluster based retrieval ha been inconclusive a to whether it doe bring improved retrieval effectiveness over document based retrieval recent development in the language modeling approach to ir have motivated u to re examine this problem within this new retrieval framework we propose two new model for cluster based retrieval and evaluate them on several trec collection we show that cluster based retrieval can perform consistently across collection of realistic size and significant improvement over document based retrieval can be obtained in a fully automatic manner and without relevance information provided by human 
while electronic music archive are gaining popularity access to and navigation within these archive is usually limited to text based query or manually predefined genre category browsing we present a system that automatically organizes a music collection according to the perceived sound similarity resembling genre or style of music audio signal are processed according to psychoacoustic model to obtain a time invariant representation of it characteristic subsequent clustering provides an intuitive interface where similar piece of music are grouped together on a map display 
we propose several context based method for text categorization one method a small modification to the ppm compression based model which is known to significantly degrade compression performance counter intuitively ha the opposite effect on categorization performance another method called c measure simply count the presence of higher order character context and outperforms all other approach investigated 
document centric xml collection contain text rich document marked up with xml tag the tag add lightweight semantics to the text querying such collection call for a hybrid query language the text rich nature of the document suggest a content oriented ir approach while the mark up allows user to add structural constraint to their ir query we will show how evidence for relevancy from different source help to answer such hybrid query we evaluate our method using the inex test set and show that structural hint in hybrid query help to improve retrieval effectiveness 
in this paper we propose a new language model namely a title language model for information retrieval different from the traditional language model used for retrieval we define the conditional probability p q d a the probability of using query q a the title for document d we adopted the statistical translation model learned from the title and document pair in the collection to compute the probability p q d to avoid the sparse data problem we propose two new smoothing method in the experiment with four different trec document collection the title language model for information retrieval with the new smoothing method outperforms both the traditional language model and the vector space model for ir significantly 
web service web accessible program and device are a key application area for the semantic web with the proliferation of web service and the evolution towards the semantic web come the opportunity to automate various web service task our objective is to enable markup and automated reasoning technology to describe simulate compose test and verify composition of web service we take a our starting point the daml s daml oil ontology for describing the capability of web service we define the semantics for a relevant subset of daml s in term of a first order logical language with the semantics in hand we encode our service description in a petri net formalism and provide decision procedure for web service simulation verification and composition we also provide an analysis of the complexity of these task under different restriction to the daml s composite service we can describe finally we present an implementation of our analysis technique this implementation take a input a daml s description of a web service automatically generates a petri net and performs the desired analysis such a tool ha broad applicability both a a back end to existing manual web service composition tool and a a stand alone tool for web service developer 
topic detection and tracking tdt task are evaluated using a cost function the standard tdt cost function assumes a constant probability of relevance p rel across all topic in practice p rel varies widely across topic we argue using both theoretical and experimental evidence that the cost function should be modified to account for the varying p rel 
multi level annotation of image is a promising solution to enable more effective semantic image retrieval by using various keywords at different semantic level in this paper we propose a multi level approach to annotate the semantics of natural scene by using both the dominant image component and the relevant semantic concept in contrast to the well known image based and region based approach we use the salient object a the dominant image component to achieve automatic image annotation at the content level by using the salient object for image content representation a novel image classification technique is developed to achieve automatic image annotation at the concept level to detect the salient object automatically a set of detection function are learned from the labeled image region by using support vector machine svm classifier with an automatic scheme for searching the optimal model parameter to generate the semantic concept finite mixture model are used to approximate the class distribution of the relevant salient object an adaptive em algorithm ha been proposed to determine the optimal model structure and model parameter simultaneously we have also demonstrated that our algorithm are very effective to enable multi level annotation of natural scene in a large scale dataset 
this paper present a document representation improvement technique named the relevance feedback accumulation rfa algorithm using prior relevance feedback assessment and a data mining measure called support this algorithm improves document representation and generates higher quality index at the same time the algorithm is efficient and scalable suited for retrieval system managing large document collection the result of the preliminary evaluation reveal that the rfa algorithm is able to reduce the index dimensionality while improving retrieval effectiveness 
this paper describes a new paradigm for modeling traffic level on the world wide web www using a method of entropy maximization this traffic is subject to the conservation condition of a circulation flow in the entire www an aggregation of the www or a subgraph of the www such a an intranet or extranet we specifically apply the primal and dual solution of this model to the static ranking of web site the first of these us an imputed measure of total traffic through a web page the second provides an analogy of local temperature allowing u to quantify the hotness of a page 
semantic video classification ha become an active research topic to enable more effective video retrieval and knowledge discovery from large scale video database however most existing technique for classifier training require a large number of hand labeled sample to learn correctly to address this problem we have proposed a semi supervised framework to achieve incremental classifier training by integrating a limited number of labeled sample with a large number of unlabeled sample specifically this emi supervised framework includes a modeling the semantic video concept by using the finite mixture model to approximate the class distribution of the relevant salient object b developing an adaptive em algorithm to integrate the unlabeled sample to achieve parameter estimation and model selection simultaneously the experimental result in a certain domain of medical video are also provided 
the structure of the web is increasingly being used to improve organization search and analysis of information on the web for example google us the text in citing document document that link to the target document for search we analyze the relative utility of document text and the text in citing document near the citation for classification and description result show that the text in citing document when available often ha greater discriminative and descriptive power than the text in the target document itself the combination of evidence from a document and citing document can improve on either information source alone moreover by ranking word and phrase in the citing document according to expected entropy loss we are able to accurately name cluster of web page even with very few positive example our result confirm quantify and extend previous research using web structure in these area introducing new method for classification and description of page 
test collection for the filtering track in trec have typically used either past set of relevance judgment or categorized collection such a reuters corpus volume or ohsumed because filtering system need relevance judgment during the experiment for training and adaptation for trec we constructed an entirely new set of search topic for the reuters corpus for measuring filtering system our method for building the topic involved multiple iteration of feedback from assessor and fusion of result from multiple search system using different search algorithm we also developed a second set of inexpensive topic based on category in the document collection we found that the initial judgment made for the experiment were sufficient subsequent pooled judging changed system ranking very little we also found that system performed very differently on the category topic than on the assessor built topic 
the fast spreading of electronic business to business procurement system ha led to the development of new standard for the exchange of electronic product catalog e catalog e catalog contain various information about product essential is price information price are used for buying decision and following order transaction while simple price model are often sufficient for the description of indirect good e g office supply other good and line of business make higher demand in this paper we examine what price information is contained in commercial xml standard for the exchange of product catalog data for that purpose we bring the different implicit price model of the examined catalog standard together and provide a generalized model 
user of the world wide web are not only confronted by an immense overabundance of information but also by a plethora of tool for searching for the web page that suit their information need web search engine differ widely in interface feature coverage of the web ranking method delivery of advertising and more in this paper we present a method for comparing search engine automatically based on how they rank known item search result because the engine perform their search on overlapping but different subset of the web collected at different point in time evaluation of search engine pose significant challenge to the traditional information retrieval methodology our method us known item searching comparing the relative rank of the item in the search engine ranking our approach automatically construct known item query using query log analysis and automatically construct the result via analysis of editor comment from the odp open directory project additionally we present our comparison on five lycos netscape fast google hotbot well known search service and find that some service perform known item search better than others but the majority are statistically equivalent 
empirical study of information retrieval method show that good retrieval performance is closely related to the use of various retrieval heuristic such a tf idf weighting one basic research question is thus what exactly are these necessary heuristic that seem to cause good retrieval performance in this paper we present a formal study of retrieval heuristic we formally define a set of basic desirable constraint that any reasonable retrieval function should satisfy and check these constraint on a variety of representative retrieval function we find that none of these retrieval function satisfies all the constraint unconditionally empirical result show that when a constraint is not satisfied it often indicates non optimality of the method and when a constraint is satisfied only for a certain range of parameter value it performance tends to be poor when the parameter is out of the range in general we find that the empirical performance of a retrieval formula is tightly related to how well it satisfies these constraint thus the proposed constraint provide a good explanation of many empirical observation and make it possible to evaluate any existing or new retrieval formula analytically 
one of the key benefit of xml is it ability to represent a mix of structured and unstructured text data although current xml query language such a xpath and xquery can express rich query over structured data they can only express very rudimentary query over text data we thus propose texquery which is a powerful full text search extension to xquery texquery provides a rich set of fully composable full text search primitive such a boolean connective phrase matching proximity distance stemming and thesaurus texquery also enables user to seamlessly query over both structured and text data by embedding texquery primitive in xquery and vice versa finally texquery support a flexible scoring construct that can be used toscore query result based on full text predicate texquery is the precursor ofthe full text language extension to xpath and xquery currently being developed by the w c 
this work proposes a model for video retrieval based upon the inference network model the document network is constructed using video metadata encoded using mpeg and capture information pertaining to the structural aspect video breakdown into shot and scene conceptual aspect video scene and shot content and contextual aspect context information about the position of conceptual content within the document the retrieval process a exploit the distribution of evidence among the shot to perform ranking of different level of granularity b address the idea that evidence may be inherited during evaluation and c exploit the contextual information to perform constrained query 
amilcare is a tool for adaptive information extraction ie designed for supporting active annotation of document for the semantic web sw it can be used either for unsupervised document annotation or a a support for human annotation amilcare is portable to new application domain without any knowledge of ie a it just requires user to annotate a small training corpus with the information to be extracted it is based on lp a supervised learning strategy for ie able to cope with different text type from newspaper like text to rigidly formatted web page and even a mixture of them adaptation start with the definition of a tag set for annotation possibly organized a an ontology then user have to manually annotate a small training corpus amilcare provides a default mouse based interface called melita where annotation are inserted by first selecting a tag from the ontology and then identifying the text area to annotate with the mouse differently from similar annotation tool melita actively support training corpus annotation while user annotate text amilcare run in the background learning how to reproduce the inserted annotation induced rule are silently applied to new text and their result are compared with the user annotation when it rule reach a user defined level of accuracy melita present new text with a preliminary annotation derived by the rule application in this case user have just to correct mistake and add missing annotation user correction are inputted back to the learner for retraining this technique focus the slow and expensive user activity on uncovered case avoiding requiring annotating case where a satisfying effectiveness is already reached moreover validating extracted information is a much simpler task than tagging bare text and also le error prone speeding up the process considerably at the end of the corpus annotation process the system is trained and the application can be delivered mnm and ontomat annotizer are two annotation tool adopting amilcare s learner in this demo we simulate the annotation of a small corpus and we show how and when amilcare is able to support user in the annotation process focusing on the way the user can control the tool s proactivity and intrusivity we will also quantify such support with data derived from a number of experiment on corpus we will focus on training corpus size and correctness of suggestion when the corpus is increased 
we describe an evaluation of result set filtering technique for providing ultra high precision in the task of presenting related news for general web query in this task the negative user experience generated by retrieving non relevant document ha a much worse impact than not retrieving relevant one we adapt cost based metric from the document filtering domain to this result filtering problem in order to explicitly examine the tradeoff between missing relevant document and retrieving non relevant one a large manual evaluation of three simple threshold filter show that the basic approach of counting matching title term outperforms also incorporating selected abstract term based on part of speech or higher level linguistic structure simultaneously leveraging these cost based metric allows u to explicitly determine what other task would benefit from these alternative technique 
this paper report on theoretical investigation about the assumption underlying the inverse document frequency idf we show that an intuitive idf based probability function for the probability of a term being informative assumes disjoint document event by assuming document to be independent rather than disjoint we arrive at a poisson based probability of being informative the framework is useful for understanding and deciding the parameter estimation and combination in probabilistic retrieval model 
recently a widespread interest ha emerged in using ontology on the web resource description framework schema rdfs is a basic tool that enables user to define vocabulary structure and constraint for expressing meta data about web resource however it includes no provision for formal semantics and it expressivity is not sufficient for full fledged ontological modeling and reasoning in this paper we will show how rdfs can be extended to include a more expressive knowledge representation language that in turn would enrich it with the required additional expressivity and the semantics of that language we do this by describing the ontology language ontology inference layer oil a an extension of rdfs an important advantage to our approach is that it ensures maximal sharing of meta data on the web even partial interpretation of an oil ontology by le semantically aware processor will yield a correct partial interpretation of the meta data 
chemoinformatics is the generic name for the technique used to represent store and process information about the two dimensional d and three dimensional d structure of chemical molecule chemoinformatics ha attracted much recent prominence a a result of development in the method that are used to synthesize new molecule and then to test them for biological activity these development have resulted in a massive increase in the amount of structural and biological information that is available to support discovery programme in the pharmaceutical and agrochemical industry chemoinformatics may appear to be far removed from information retrieval ir and there are indeed many significant difference most notably in the use of graph representation to encode chemical molecule rather than the string that are used to encode text however there are also many similarity between the two field and this paper will exemplify some of these relationship the most obvious area of similarity is in the principal type of database search that are carried out with both application domain making extensive use of exact match partial match and best match searching procedure in the ir context these are known item searching boolean searching and ranked output searching in the chemical context these are structure searching substructure searching and similarity searching in ir there is a natural distinction between an initial ranked output search and one in which relevance feedback can be employed where the keywords in the query statement are assigned weight based on their differential occurrence in known relevant and known nonrelevant document in the chemoinformatics technique called substructural analysis substructural fragment are assigned weight based on their occurrence in molecule that do posse and molecule that do not posse some desired biological activity the analogy between relevance and biological activity ha also resulted in the development of measure to quantify the effectiveness of chemical searching procedure that are based on the standard ir concept of recall and precision analogy such a these have provided the basis for some of the chemoinformatics research carried out in sheffield the starting point wa the recognition that technique applicable to document represented by keywords might also be applicable to molecule represented by substructural fragment this led directly to the introduction of similarity searching something that is now a standard tool in chemoinformatics software system in particular it use for virtual screening i e the ranking of a database in order of decreasing probability of activity so a to maximize the cost effectiveness of biological testing measure of inter molecular structural similarity also lie at the heart of system for clustering chemical database just a ir ha the cluster hypothesis similar document tend to be relevant to the same request a a basis for document clustering so the similar property principle similar molecule tend to have similar property ha led to clustering becoming a well established tool for the organization of large chemical database more recently we have applied another ir technique the use of data fusion to combine different ranking of a database to chemoinformatics and again found that it is equally applicable in this new domain the many similarity between ir and chemoinformatics that have already been identified suggest that chemoinformatics is a domain of which ir researcher should be aware when considering the applicability of new technique that they have developed 
a novel maximal figure of merit mfom learning approach to text categorization is proposed different from the conventional technique the proposed mfom method attempt to integrate any performance metric of interest e g accuracy recall precision or f measure into the design of any classifier the corresponding classifier parameter are learned by optimizing an overall objective function of interest to solve this highly nonlinear optimization problem we use a generalized probabilistic descent algorithm the mfom learning framework is evaluated on the reuters task with lsi based feature extraction and a binary tree classifier experimental result indicate that the mfom classifier give improved f and enhanced robustness over the conventional one it also outperforms the popular svm method in micro averaging f other extension to design discriminative multiple category mfom classifier for application scenario with new performance metric could be envisioned too 
most information retrieval technology are designed to facilitate information discovery however much knowledge work involves finding and re using previously seen information we describe the design and evaluation of a system called stuff i ve seen si that facilitates information re use this is accomplished in two way first the system provides a unified index of information that a person ha seen whether it wa seen a email web page document appointment etc second because the information ha been seen before rich contextual cue can be used in the search interface the system ha been used internally by more than employee we report on both qualitative and quantitative aspect of system use initial finding show that time and people are important retrieval cue user find information more easily using si and use other search tool le frequently after installation 
web search engine provide a large scale text document retrieval service by processing huge inverted file index inverted file index allow fast query resolution and good memory utilization since their d gap representation can be effectively and efficiently compressed by using variable length encoding method this paper proposes and evaluates some algorithm aimed to find an assignment of the document identifier which minimizes the average value of d gap thus enhancing the effectiveness of traditional compression method we ran several test over the google contest collection in order to validate the technique proposed the experiment demonstrated the scalability and effectiveness of our algorithm using the proposed algorithm we were able to sensibly improve up to the compression ratio of several encoding scheme 
this poster report upon the ongoing effort being made to establish trec like and other comprehensive evaluation paradigm within the music ir mir and music digital library mdl research community the proposed research task are based upon expert opinion garnered from member of the information retrieval ir mdl and mir community with regard to the construction and implementation of scientifically valid evaluation framework 
continuous query are query for which response given to user must be continuously updated a the source of interest get updated such query occur for instance during on line decision making e g traffic flow control weather monitoring etc the problem of keeping the response current reduces to the problem of deciding how often to visit a source to determine if and how it ha been modified in order to update earlier response accordingly on the surface this seems to be similar to the crawling problem since crawler attempt to keep index up to date a page change and user pose search query we show that this is not the case both due to the inherent difference between the nature of the two problem a well a the performance metric we propose develop and evaluate a novel multi phase continuous adaptive monitoring cam solution to the problem of maintaining the currency of query result some of the important phase are the tracking phase in which change to an initially identified set of relevant page are tracked from the observed change characteristic of these page a probabilistic model of their change behavior is formulated and weight are assigned to page to denote their importance for the current query during the next phase the resource allocation phase based on these statistic resource needed to continuously monitor these page for change are allocated given these resource allocation the scheduling phase produce an optimal achievable schedule for the monitoring task an experimental evaluation of our approach compared to prior approach for crawling dynamic web page show the effectiveness of cam for monitoring dynamic change for example by monitoring just of the page change cam is able to return of the changed information to the user the experiment also produce some interesting observation pertaining to the difference between the two problem of crawling to build an index and the problem of change tracking to respond to continuous query 
the ability to determine what day to day activity such a cooking pasta taking a pill or watching a video a person is performing is of interest in many application domain a system that can do this requires model of the activity of interest but model construction doe not scale well human must specify low level detail such a segmentation and feature selection of sensor data and high level structure such a spatio temporal relation between state of the model for each and every activity a a result previous practical activity recognition system have been content to model a tiny fraction of the thousand of human activity that are potentially useful to detect in this paper we present an approach to sensing and modeling activity that scale to a much larger class of activity than before we show how a new class of sensor based on radio frequency identification rfid tag can directly yield semantic term that describe the state of the physical world these sensor allow u to formulate activity model by translating labeled activity such a cooking pasta into probabilistic collection of object term such a pot given this view of activity model a text translation we show how to mine definition of activity in an unsupervised manner from the web we have used our technique to mine definition for over activity we experimentally validate our approach using data gathered from actual human activity a well a simulated data 
in this paper we propose a new type of web browser called the comparative web browser cwb which concurrently present multiple web page in a way that enables the content of the web page to be automatically synchronized the ability to view multiple web page at one time is useful when we wish to make a comparison on the web such a when we compare similar product or news article from different newspaper the cwb is characterized by automatic content based retrieval of passage from another web page based on a passage of the web page the user is reading and automatic transformation of a user s behavior scrolling clicking or moving backward or forward on a web page into a series of behavior on the other web page the cwb try to concurrently present similar passage from different web page and for this purpose our cwb automatically navigates web page that contain passage similar to those of the initial web page furthermore we propose an enhancement to the cwb which enables it to use linkage information to find related document based on link structure 
this article describes the development of a free test collection for chinese text categorization a novel retrieval based approach wa developed to detect duplicate and label inconsistency in this corpus and in reuters for comparison the method wa able to detect certain type of similar and or duplicated document that were overlooked by an alternative repetition based method experiment showed that effectiveness wa not affected by the confusing document 
peer to peer information sharing environment are increasingly gaining acceptance on the internet a they provide an infrastructure in which the desired information can be located and downloaded while preserving the anonymity of both requestors and provider a recent experience with p p environment such a gnutella show anonymity open the door to possible misuse and abuse by resource provider exploiting the network a a way to spread tampered with resource including malicious program such a trojan horse and virus in this paper we propose an approach to p p security where servents can keep track and share with others information about the reputation of their peer reputation sharing is based on a distributed polling algorithm by which resource requestors can ass the reliability of perspective provider before initiating the download the approach nicely complement the existing p p protocol and ha a limited impact on current implementation furthermore it keep the current level of anonymity of requestors and provider a well a that of the party sharing their view on others reputation 
we present a new tool for gathering textual information according to a query text on arbitrary web site specified by an information seeking user this tool is helpful in any knowledge intensive area it technology is based on the vector space model with optimized feature definition 
organizing web search result into a hierarchy of topic and sub topic facilitates browsing the collection and locating result of interest in this paper we propose a new hierarchical monothetic clustering algorithm to build a topic hierarchy for a collection of search result retrieved in response to a query at every level of the hierarchy the new algorithm progressively identifies topic in a way that maximizes the coverage while maintaining distinctiveness of the topic we refer the proposed algorithm to a discover evaluating the quality of a topic hierarchy is a non trivial task the ultimate test being user judgment we use several objective measure such a coverage and reach time for an empirical comparison of the proposed algorithm with two other monothetic clustering algorithm to demonstrate it superiority even though our algorithm is slightly more computationally intensive than one of the algorithm it generates better hierarchy our user study also show that the proposed algorithm is superior to the other algorithm a a summarizing and browsing tool 
collaborative filtering cf system have been researched for over a decade a a tool to deal with information overload at the heart of these system are the algorithm which generate the prediction and recommendation in this article we empirically demonstrate that two of the most acclaimed cf recommendation algorithm have flaw that result in a dramatically unacceptable user experience in response we introduce a new belief distribution algorithm that overcomes these flaw and provides substantially richer user modeling the belief distribution algorithm retains the quality of nearest neighbor algorithm which have performed well in the past yet produce prediction of belief distribution across rating value rather than a point rating value in addition we illustrate how the exclusive use of the mean absolute error metric ha concealed these flaw for so long and we propose the use of a modified precision metric for more accurately evaluating the user experience 
in the selective dissemination of information or publish subscribe paradigm client subscribe to a server with continuous query or profile that express their information need client can also publish document to server whenever a document is published the continuous query satisfying this document are found and notification are sent to appropriate client this paper deal with the filtering problem that need to be solved effciently by each server given a database of continuous query db and a document d find all query q db that match d we present data structure and indexing algorithm that enable u to solve the filtering problem efficiently for large database of query expressed in the model awp which is based on named attribute with value of type text and word proximity operator 
web application are becoming increasingly popular for mobile wireless pda however web browsing on these system can be quite slow an alternative approach is handheld thin client computing in which the web browser and associated application logic run on a server which then sends simple screen update to thepda for display to ass the viability of this thin client approach we compare the web browsing performance of thin client against fat client that run the web browser locally on a pda our result show that thin client can provide better web browsing performance compared to fat client both in term of speed and ability to correctly display web content surprisingly thin client are faster even when having to send more data over the network we characterize and analyze different design choice in various thin client system and explain why these approach can yield superior web browsing performance on mobile wireless pda 
both ranking function and user query are very important factor affecting a search engine s performance prior research ha looked at how to improve ad hoc retrieval performance for existing query while tuning the ranking function or modify and expand user query using a fixed ranking scheme using blind feedback however almost no research ha looked at how to combine ranking function tuning and blind feedback together to improve ad hoc retrieval performance in this paper we look at the performance improvement for ad hoc retrieval from a more integrated point of view by combining the merit of both technique in particular we argue that the ranking function should be tuned first using user provided query before applying the blind feedback technique the intuition is that highly tuned ranking offer more high quality document at the top of the hit list thus offer a stronger baseline for blind feedback we verify this integrated model in a large scale heterogeneous collection and the experimental result show that combining ranking function tuning and blind feedback can improve search performance by almost over the baseline okapi system 
the is a new layer of the internet that enables semantic representation of the content of existing web page using common ontology human user sketch out the most important fact in model that act a intelligent whiteboards once model are broadcasted to the internet new and intelligent search engine ambient intelligent device and agent would be able to exploit this knowledge network the main idea of semtalk is to empower end user to contribute to the semantic web by offering an easy to use based graphical editor to create rdf like schema and workflow since the modeled data is found by microsoft s smarttags user can benefit from these semantic web a part of their daily work with other microsoft office product such a word excel or outlook semtalk s graphically configurable meta model also extends the functionality of the visio modeling tool because it make it easy to configure visio to different modeling world such a business engineering and case methodology but also to these feature can be applied to any other visio drawing ontology project department wide information modeling at the credit suisse bank main emphasis wa on linguistic standardization of term based on a common central glossary local knowledge management team were able to develop specialized model for their decentralized department a part of the knowledge management process local glossary were continually carried over into a common shared model business process management project distributed process modeling of the bausparkasse deutscher ring a german financial institution several group of student from the technical university fh brandenburg explored how to develop and apply an industry specific semantic web to business process modeling 
the web graph meaning the graph induced by web page a node and their hyperlink a directed edge ha become a fascinating object of study for many people physicist sociologist mathematician computer scientist and information retrieval specialist recent result range from theoretical e g model for the graph semi external algorithm to experimental e g new insight regarding the rate of change of page new data on the distribution of degree to practical e g improvement in crawling technology recent result range from theoretical e g model for the graph semi external algorithm to experimental e g new insight regarding the rate of change of page new data on the distribution of degree to practical e g improvement in crawling technology the goal of this talk is to convey an introduction to the state of the art in this area and to sketch the current issue in collecting representing analyzing and modeling this graph although graph analytic method are essential tool in the web ir arsenal they are well known to the sigir community and will not be discussed here in any detail instead we will explore some challenge and opportunity for using ir method and technique in the exploration of the web graph in particular in dealing with legitimate and spam perturbation of the natural process of birth and death of node and link and conversely the challenge and opportunity of using graph method in support of ir on the web and in the enterprise 
although information retrieval research ha always been concerned with improving the effectiveness of search in some application such a information analysis a more specific requirement exists for high accuracy retrieval this mean that achieving high precision in the top document rank is paramount in this paper we present work aimed at achieving high accuracy in ad hoc document retrieval by incorporating approach from question answering qa we focus on getting the first relevant result a high a possible in the ranked list and argue that traditional precision and recall are not appropriate measure for evaluatin this task we instead use the mean reciprocal rank mrr of the first relevant result we evaluate three different method for modifying query to achieve high accuracy the experiment done on trec data provide support for the approach of using mrr and incorporating qa technique for getting high accuracy in ad hoc retrieval task 
collaborative filtering identifies information interest of a particular user based on the information provided by other similar user the memory based approach for collaborative filtering e g pearson correlation coefficient approach identify the similarity between two user by comparing their rating on a set of item in these approach different item are weighted either equally or by some predefined function the impact of rating discrepancy among different user ha not been taken into consideration for example an item that is highly favored by most user should have a smaller impact on the user similarity than an item for which different type of user tend to give different rating even though simple weighting method such a variance weighting try to address this problem empirical study have shown that they are ineffective in improving the performance of collaborative filtering in this paper we present an optimization algorithm to automatically compute the weight for different item based on their rating from training user more specifically the new weighting scheme will create a clustered distribution for user vector in the item space by bringing user of similar interest closer and separating user of different interest more distant empirical study over two datasets have shown that our new weighting scheme substantially improves the performance of the pearson correlation coefficient method for collaborative filtering 
the ability to accurately identify the network traffic associated with different p p application is important to a broad range of network operation including application specific traffic engineering capacity planning provisioning service differentiation etc however traditional traffic to higher level application mapping technique such a default server tcp or udp network port baseddisambiguation is highly inaccurate for some p p application in this paper we provide an efficient approach for identifying the p p application traffic through application level signature we firstidentify the application level signature by examining some available documentation and packet level trace we then utilize the identified signature to develop online filter that can efficiently and accurately track the p p traffic even on high speed network link we examine the performance of our application level identification approach using five popular p p protocol our measurement show thatour technique achieves le than false positive and false negative ratio in most case we also show that our approach only requires the examination of the very first few packet le than packet to identify a p p connection which make our approach highly scalable our technique can significantly improve the p p traffic volume estimate over what pure network port based approach provide for instance we were able to identify time a much traffic for the popular kazaa p p protocol compared to the traditional port based approach 
network and server centric computing paradigm are quickly returning to being the dominant method by which we use computer web application are so prevalent that the role of a pc today ha been largely reduced to a terminal for running a client or viewer such a a web browser implementers of network centric application typically rely on the limited capability of html employing proprietary plug in or transmitting the binary image of an entire application that will be executed on the client alternatively implementers can develop without regard for remote use requiring user who wish to run such application on a remote server to rely on a system that creates a virtual frame buffer on the server and transmits a copy of it raster image to the local client we review some of the problem that these current approach pose and show how they can be solved by developing a distributed user interface toolkit a distributed user interface toolkit applies technique to the high level component of a toolkit that are similar to those used at a low level in the x window system a an example of this approach we present remotejfc a working distributed user interface toolkit that make it possible to develop thin client application using a distributed version of the java foundation class 
there exist many portal server that support the construction of my portal that is portal that allow the user to have one or more personal page composed of a number of personalizable service the main drawback of current portal server is their lack of generality and adaptability this paper present the design of mypersonalizer a j ee based framework for engineering my portal the framework is structured according to the model view controller and layer architectural pattern providing generic adaptable model and controller layer that implement the typical use case of a my portal mypersonalizer allows for a good separation of role in the development team graphical designer without programming skill develop the portal view by writing jsp page while software engineer implement service plugins and specify framework configuration 
the rapid growth of the web ha been noted and tracked extensively recent study have however documented the dual phenomenon web page have small half life and thus the web exhibit rapid death a well consequently page creator are faced with an increasingly burdensome task of keeping link up to date and many are falling behind in addition to just individual page collection of page or even entire neighborhood of the web exhibit significant decay rendering them le effective a information resource such neighborhood are identified only by frustrated searcher seeking a way out of these stale neighborhood back to more up to date section of the web measuring the decay of a page purely on the basis of dead link on the page is too naive to reflect this frustration in this paper we formalize a strong notion of a decay measure and present algorithm for computing it efficiently we explore this measure by presenting a number of validation and use it to identify interesting artifact on today s web we then describe a number of application of such a measure to search engine web page maintainer ontologists and individual user 
although originally designed for large scale electronic publishing xml play an increasingly important role in the exchange of data on the web in fact it is expected that xml will become the lingua franca of the web eventually replacing html not surprisingly there ha been a great deal of interest on xml both in industry and in academia nevertheless to date no comprehensive study on the xml web i e the subset of the web made of xml document only nor on it content ha been made this paper is the first attempt at describing the xml web and the document contained in it our result are drawn from a sample of a repository of the publicly available xml document on the web consisting of about document our result show that despite it short history xml already permeates the web both in term of generic domain and geographically also our result about the content of the xml web provide valuable input for the design of algorithm tool and system that use xml in one form or another 
in this paper we describe research which could lead to a novel approach to gathering an overview of a document in a foreign language the research explores how much of the meaning of a document could be represented using image by researching the ability of subject to derive the search term that might have been used to return a set of image from an image library the google image search engine wa used to retrieve the image for this experiment which us english throughout the result were analysed with respect to a previous paper exploring ability to recognise concrete object in hierarchy it wa found that there is a tendency to use one particular level of categorization 
two important architectural choice underlie the success of the web numerous independently operated server speak a common protocol and a single type of client the web browser provides point and click access to the content and service on these decentralized server however because html marries content and presentation into a single representation end user are often stuck with inappropriate choice made by the web site designer of how to work with and view the content rdf metadata on the semantic web doe not have this limitation user can gain direct access to information and control over how it is presented this principle form the basis for our semantic web browser an end user application that automatically locates metadata and assembles point and click interface from a combination of relevant information ontological specification and presentation knowledge all described in rdf and retrieved dynamically from the semantic web because data and service are accessed directly through a standalone client and not through a central point of access e g a portal new content and service can be consumed a soon a they become available in this way we take advantage of an important sociological force that encourages the production of new semantic web content while remaining faithful to the decentralized nature of the web 
recommender system have emerged in the past several year a an effective way to help people cope with the problem of information overload one application in which they have become particularly common is in e commerce where recommendation of item can often help a customer find what she is interested in and therefore can help drive sale unscrupulous producer in the never ending quest for market penetration may find it profitable to shill recommender system by lying to the system in order to have their product recommended more often than those of their competitor this paper explores four open question that may affect the effectiveness of such shilling attack which recommender algorithm is being used whether the application is producing recommendation or prediction how detectable the attack are by the operator of the system and what the property are of the item being attacked the question are explored experimentally on a large data set of movie rating taken together the result of the paper suggest that new way must be used to evaluate and detect shilling attack on recommender system 
a lexical signature of a web page is often sufficient for finding the page even if it url ha changed we conduct a large scale empirical study of eight method for generating lexical signature including phelps and wilensky s original proposal pw and seven of our own variation we examine their performance on the web and on a trec data set evaluating their ability both to uniquely identify the original document and to locate other relevant document if the original is lost lexical signature chosen to minimize document frequency df are good at unique identification but poor at finding relevant document pw work well on the relatively small trec data set but act almost identically to df on the web which contains billion of document term frequency based lexical signature tf are very easy to compute and often perform well but are highly dependent on the ranking system of the search engine used in general tfidf based method and hybrid method which combine df with tf or tfidf seem to be the most promising candidate for generating effective lexical signature 
this paper describes the process of constructing a markup language for maritime information from the starting point of ontology building ontology construction from source material in the maritime information domain is outlined the structure of the markup language is described in term of xml schema and dtds a prototype application that us the markup language is also described 
this paper describes method for service selection and service access for mobile sensor enhanced web client such a wireless camera or wireless pda with sensor device attached the client announce their data creating capability in produce header sent to server server respond with form that match these capability client fill in these form with sensor data a well a text or file data the resultant system enables client to access dynamically discovered service spontaneously a their user engage in everyday nomadic activity 
due to the increasing availability and use of digital video data on the web video caching will be an important performance factor in the future www we propose an architecture of a video proxy cache that integrates modern multimedia and communication standard especially we describe feature of the mpeg and mpeg multimedia standard that can be helpful for a video proxy cache qbix support real time adaptation in the compressed and in the decompressed domain it us adaptation to improve the cache replacement strategy in the proxy but also to realize medium gateway functionality driven by the client terminal capability 
this demonstration utilizes a geographic information system interface to display multilingual news document in time and space by extracting place name from text and matching them to a multilingual multi script gazetteer which identifies the latitude and longitude of the location 
this paper proposes an example based phrase translation method in a chinese to english cross language information retrieval clir system the method can generate much more accurate query translation than dictionary based and common mt based method and then improves the retrieval performance of our clir system 
researcher in web engineering have regularly noted that existing web application development environment provide little support for managing the evolution of web application key limitation of web development environment include line oriented change model that inadequately represent web document semantics and in ability to model change to link structure or the set of object making up the webapplication developer may find it difficult to grasp how theoverall structure of the web application ha changed over time and may respond by using ad hoc solution that lead to problem of maintain ability quality and reliability web application are software artifact and a such can benefit from advanced version control and software configuration management scm technology from software engineering we have modified an integrated development environment to manage the evolution and maintenance of web application the resulting environment is distinguished by itsfine grained version control framework fine grained web contentchange management and product versioning configuration management in which a web project can be organized at the logical level and itsstructure and component are versioned in a fine grained manner aswell this paper describes the motivation for this environment a well a it user interface feature and implementation 
aliasing occurs in web transaction when request containing different url elicit reply containing identical data payload conventional cache associate stored data with url and can therefore suffer redundant payload transfer due to aliasing and other cause existing research literature however say little about the prevalence of aliasing in user initiated transaction or about redundant payload transfer in conventional web cache hierarchy this paper quantifies the extent of aliasing and the performance impact of url indexed cache management using a large client trace from webtv network fewer than of reply payload are aliased referenced via multiple url but over of successful transaction involve aliased payload aliased payload account for under of the trace s working set size sum of payload size but over of byte transferred for the webtv workload roughly of payload transfer to browser cache and of payload transfer to a shared proxy are redundant assuming infinite capacity conventional cache our analysis of a large proxy trace from compaq corporation yield similar result url indexed caching doe not entirely explain the large number of redundant proxy to browser payload transfer previously reported in the webtv system we consider other possible cause of redundant transfer e g reply metadata and browser cache management policy and discus a simple hop by hop protocol extension that completely eliminates all redundant transfer regardless of cause 
we show how to interoperate semantically and inferentially between the leading semantic web approach to rule ruleml logic program and ontology owl daml oil description logic via analyzing their expressive intersection to do so we define a new intermediate knowledge representation kr contained within this intersection description logic program dlp and the closely related description horn logic dhl which is an expressive fragment of first order logic fol dlp provides a significant degree of expressiveness substantially greater than the rdf schema fragment of description logic we show how to perform dlp fusion the bidirectional translation of premise and inference including typical kind of query from the dlp fragment of dl to lp and vice versa from the dlp fragment of lp to dl in particular this translation enables one to build rule on top of ontology it enables the rule kr to have access to dl ontological definition for vocabulary primitive e g predicate and individual constant used by the rule conversely the dlp fusion technique likewise enables one to build ontology on top of rule it enables ontological definition to be supplemented by rule or imported into dl from rule it also enables available efficient lp inferencing algorithm implementation to be exploited for reasoning over large scale dl ontology 
this paper describes the patent retrieval task in the fourth ntcir workshop and the test collection produced in this task we perform the invalidity search task in which each participant group search a patent collection for the patent that can invalidate the demand in an existing claim we also perform the automatic patent map generation task in which the patent associated with a specific topic are organized in a multi dimensional matrix 
recent increase in the number of search engine on the web and the availability of meta search engine that can query multiple search engine make it important to find effective method for combining result coming from different source in this paper we introduce novel method for reranking in a meta search environment based on expert agreement and content of the snippet we also introduce an objective way of evaluating different method for ranking search result that is based upon implicit user judgement we incorporated our method and two variation of commonly used merging method in our meta search engine mearf and carried out an experimental study using log accumulated over a period of twelve month our experiment show that the choice of the method used for merging the output produced by different search engine play a significant role in the overall quality of the search result in almost all case examined result produced by some of the new method introduced were consistently better than the one produced by traditional method commonly used in various meta search engine these observation suggest that the proposed method can offer a relatively inexpensive way of improving the meta search experience over existing method 
in this paper we study the problem of finding most topical named entity among all entity in a document which we refer to a focused named entity recognition we show that these focused named entity are useful for many natural language processing application such a document summarization search result ranking and entity detection and tracking we propose a statistical model for focused named entity recognition by converting it into a classification problem we then study the impact of various linguistic feature and compare a number of classification algorithm from experiment on an annotated chinese news corpus we demonstrate that the proposed method can achieve near human level accuracy 
the emerging edge service architecture promise to improve the availability and performance of web service by replicating server at geographically distributed site a key challenge in such system is data replication and consistency so that edge server code can manipulate shared data without incurring the availability and performance penalty that would be incurred by accessing a traditional centralized database this paper explores using a distributed object architecture to build an edge service system for an e commerce application an online bookstore represented by the tpc w benchmark we take advantage of application specific semantics to design distributed object to manage a specific subset of shared information using simple and effective consistency model our experimental result show that by slightly relaxing consistency within individual distributed object we can build an edge service system that is highly available and efficient for example in one experiment we find that our object based edge server system provides a factor of five improvement in response time over a traditional centralized cluster architecture and a factor of nine improvement over an edge service system that distributes code but retains a centralized database 
digital medium audio video can be difficult to search and share in a personal way souvenir is a software system that offer user a flexible and comprehensive way to use their handwritten or text note to retrieve and share specific medium moment user can take note on a variety of device such a the paper based crosspad the palm pilot and standard keyboard device souvenir segment handwritten note into an effective medium index without the need for handwriting recognition user can use their note to create hyperlink to random access medium stored in a digital library souvenir also ha web publishing and email capability to enable anyone to access or email medium moment directly from a web page souvenir annotation capture information that can not be easily inferred by automatic medium indexing tool 
this short paper present a light weight technique to merge result list obtained from querying different database the motivation for such a technique is a general purpose search engine for palm o based pda 
query have specific property and may need individualized method and parameter to optimize retrieval length is one property we look at how two word query may attain higher precision by re ranking using word co occurrence evidence in retrieved document co occurrence within document context is not sufficient but window context including sentence context evidence can provide precision improvement at low recall region of to using initial retrieval result and positively affect pseudo relevance feedback 
many museum and library archive are digitizing their large collection of handwritten historical manuscript to enable public access to them these collection are only available in image format and require expensive manual annotation work for access to them current handwriting recognizers have word error rate in excess of and therefore cannot be used for such material we describe two statistical model for retrieval in large collection of handwritten manuscript given a text query both use a set of transcribed page image to learn a joint probability distribution between feature computed from word image and their transcription the model can then be used to retrieve unlabeled image of handwritten document given a text query we show experiment with a training set of transcribed page and a test set of handwritten page image from the george washington collection experiment show that the precision at document is about to depending on the model to the best of our knowledge this is the first automatic retrieval system for historical manuscript using text query without manual transcription of the original corpus 
in this paper we present hearsay a system for browsing hypertext web document via audio the hearsay system is based on our novel approach to automatically creating audio browsable content from hypertext web document it combine two key technology automatic partitioning of web document through tightly coupled structural and semantic analysis which transforms raw html document into semantic structure so a to facilitate audio browsing and voicexml an already standardized technology which we adopt to represent voice dialog automatically created from the xml output of partitioning this paper describes the software component of hearsay and present an initial system evaluation 
table is a commonly used presentation scheme especially for describing relational information however table understanding remains an open problem in this paper we consider the problem of table detection in web document it potential application include web mining knowledge management and web content summarization and delivery to narrow bandwidth device we describe a machine learning based approach to classify each given table entity a either genuine or non genuine various feature reflecting the layout a well a content characteristic of table are studied in order to facilitate the training and evaluation of our table classifier we designed a novel web document table ground truthing protocol and used it to build a large table ground truth database the database consists of html file collected from hundred of different web site and contains leaf table element out of which are genuine table experiment were conducted using the cross validation method and an f measure of wa achieved 
the openvxi is a portable open source based toolkit that interprets the voicexml dialog markup language it is designed to serve a a framework for system integrator and platform vendor who want to incorporate voicexml into their platform a first version of the toolkit wa released in the winter of with a second version released in september of a number of company and individual have adopted the toolkit for their platform in this paper we discus the architecture of the toolkit the architectural issue involved with implementing a framework for voicexml performance result with the openvxi and future direction for the toolkit 
protein secondary structure prediction is an important step towards understanding the relation between protein sequence and structure however most current prediction method use feature difficult for biologist to interpret in this paper we present a new method that applies information retrieval technique to solve the problem we extract a context sensitive biological vocabulary for protein sequence and apply text classification method to predict protein secondary structure experimental result show that our method performs comparably to the state of art method furthermore the context sensitive vocabulary can serve a a useful tool to discover meaningful regular expression pattern for protein structure 
recent study show that a majority of web page access are referred by search engine in this paper we study the widespread use of web search engine and it impact on the ecology of the web in particular we study how much impact search engine have on the popularity evolution of web page for example given that search engine return currently popular page at the top of search result are we somehow penalizing newly created page that are not very well known yet are popular page getting even more popular and new page completely ignored we first show that this unfortunate trend indeed exists on the web through an experimental study based on real web data we then analytically estimate how much longer it take for a new page to attract a large number of web user when search engine return only popular page at the top of search result our result show that search engine can have an immensely worrisome impact on the discovery of new web page 
the quality of translation resource is arguably the most important factor affecting the performance of a cross language information retrieval system while many investigation have explored the use of query expansion technique to combat error induced by translation no study ha yet examined the effectiveness of these technique across resource of varying quality this paper present result using parallel corpus and bilingual wordlists that have been deliberately degraded prior to query translation across different language translingual resource and degree of resource degradation pre translation query expansion is tremendously effective in several instance pre translation expansion result in better performance when no translation are available than when an uncompromised resource is used without pre translation expansion we also demonstrate that post translation expansion using relevance feedback can confer modest performance gain measuring the efficacy of these technique with resource of different quality suggests an explanation for the conflicting report that have appeared in the literature 
the web service world consists of loosely coupled distributed system which adapt to ad hoc change by the use of service description that enable opportunistic service discovery at present these service description are semantically impoverished being concerned with describing the functional signature of the service rather than characterising their meaning in the semantic web community the daml service effort attempt to rectify this by providing a more expressive way of describing web service using ontology however this approach doe not separate the domain neutral communicative intent of a message considered in term of speech act from it domain specific content unlike similar development from the multi agent system community in this paper we describe our experience of designing and building an ontologically motivated web service system for situational awareness and information triage in a simulated humanitarian aid scenario in particular we discus the merit of using technique from the multi agent system community for separating the intentional force of message from their content and the implementation of these technique within the daml service model 
current search technology work in a one size fit all fashion therefore the answer to a query is independent of specific user information need in this paper we describe a novel ranking technique for personalized search servicesthat combine content based and community based evidence the community based information is used in order to provide context for query andis influenced by the current interaction of the user with the service ouralgorithm is evaluated using data derived from an actual service available on the web an online bookstore we show that the quality of content based ranking strategy can be improved by the use of communityinformation a another evidential source of relevance in our experiment the improvement reach up to in term of average precision 
in this paper we present the autocat system for product classification autocat us a vector space model modified to consider product attribute unavailable in traditional document classification we present key feature of our user interface developed to assist user with evaluating and editing the output of the classification algorithm finally we present observation about the use of this technology in the field 
security remains a major roadblock to universal acceptance of the web for many kind of transaction especially since the recent sharp increase in remotely exploitable vulnerability have been attributed to web application bug many verification tool are discovering previously unknown vulnerability in legacy c program raising hope that the same success can be achieved with web application in this paper we describe a sound and holistic approach to ensuring web application security viewing web application vulnerability a a secure information flow problem we created a lattice based static analysis algorithm derived from type system and typestate and addressed it soundness during the analysis section of code considered vulnerable are instrumented with runtime guard thus securing web application in the absence of user intervention with sufficient annotation runtime overhead can be reduced to zero we also created a tool named webssari web application security by static analysis and runtime inspection to test our algorithm and used it to verify open source web application project on sourceforge net which were selected to represent project of different maturity popularity and scale contained vulnerability after notifying the developer acknowledged our finding and stated their plan to provide patch our statistic also show that static analysis reduced potential runtime overhead by 
the ability of the web to share data regardless of geographical location raise a new issue called remote authoring with the internet and web browser being independent of hardware it becomes possible to build web enabled database application many approach are provided to integrate database into the web environment which use the web s protocol i e http to transfer the data between client and server however those method are affected by the http shortfall with regard to remote authoring this paper introduces and discus a new methodology for remote authoring of database which is based on the webdav protocol it is a seamless and effective methodology for accessing and authoring database particularly in that it naturally benefit from the webdav advantage such a metadata and access control these feature establish a standard way of accessing database metadata and increase the database security while speeding up the database connection 
it is increasingly common for user to interact with the web using a number of different alias this trend is a double edged sword on one hand it is a fundamental building block in approach to online privacy on the other hand there are economic and social consequence to allowing each user an arbitrary number of free alias thus there is great interest in understanding the fundamental issue in obscuring the identity behind alias however most work in the area ha focused on linking alias through analysis of lower level property of interaction such a network route we show that alias that actively post text on the web can be linked together through analysis of that text we study a large number of user posting on bulletin board and develop algorithm to anti alias those user we can with a high degree of success identify when two alias belong to the same individual our result show that such technique are surprisingly effective leading u to conclude that guaranteeing privacy among alias that post actively requires mechanism that do not yet exist 
application level web security refers to vulnerability inherent in the code of a web application itself irrespective of the technology in which it is implemented or the security of the web server back end database on which it is built in the last few month application level vulnerability have been exploited with serious consequence hacker have tricked e commerce site into shipping good for no charge user name and password have been harvested and condential information such a address and credit card number ha been leaked in this paper we investigate new tool and technique which address the problem of application level web security we i describe a scalable structuring mechanism facilitating the abstraction of security policy from large web application developed in heterogenous multi platform environment ii present a tool which assist programmer develop secure application which are resilient to a wide range of common attack and iii report result and experience arising from our implementation of these technique 
industry professional and everyday user of the internet have long accepted that due to both the size and growth of this ubiquitous repository new tool are needed to assist with the finding and extraction of very specific resource relevant to a user s task previously this definition of relevance ha been based on the extremely generic matching between resource and query term but recently the emphasis is shifting towards a more personalised model based on the relevance of a particular resource for one specific user we introduce a prototype tt fetch which adopts this concept within an information seeking environment specifically designed to provide user with the mean to better describe a problem s he doesn t understand 
named entity tagging comprises the sub task of identifying a text span and classifying it but this view ignores the relationship between the entity and the world spatial and temporal entity ground event in space time and this relationship is vital for application such a question answering and event tracking there is much recent work regarding the temporal dimension setzer and gaizauskas mani and wilson but no detailed study of the spatial dimension i propose to investigate how spatial named entity which are often referentially ambiguous can be automatically resolved with respect to an extensional coordinate model toponym resolution to this end various information source including linguistic cue pattern co occurrence information discourse positional information world knowledge such a size and population a well a minimality heuristic leidner et al will be combined in a supervised machine learning regime the major contribution of this research project will be a corpus of text manually annotated for spatial named entity with their model correlate a a training and evaluation resource a novel method to spatially ground toponym in text and a component based evaluation based on this new reference corpus 
two popular link based webpage ranking algorithm are i pagerank and ii hit hypertext induced topic selection hit make the crucial distinction of hub and authority and computes them in a mutually reinforcing way pagerank considers the hyperlink weight normalization and the equilibrium distribution of random surfer a the citation score we generalize and combine these key concept into a unified framework in which we prove that ranking produced by pagerank and hit are both highly correlated with the ranking by in degree and out degree 
in the past few year a number of constraint language for xml document ha been proposed they are cumulatively called schema language or validation language and they comprise among others dtd xml schema relax ng schematron dsd xlinkit one major point of discrimination among schema language is the support of co constraint or co occurrence constraint e g requiring that attribute a is present if and only if attribute b is or is not presentin the same element although there is no way in xml schema to express these requirement they are in fact frequently used in many xml document type usually only expressed in plain human readable text and validated by mean of special code module by the relevant application in this paper we propose schemapath a light extension of xml schema to handle conditional constraint on xml document two new construct have been added to xml schema condition based on xpath pattern on type assignment for element and attribute and a new simple type xsd error for the direct expression of negative constraint e g it is prohibited for attribute a to be present if attribute b is also present a proof of concept implementation is provided a web interface is publicly accessible for experiment and assessment of the real expressiveness of the proposed extension 
in this poster we describe a novel session based search engine which put the search in context the search engine ha a number of session based feature including expansion of the current query with user query history and clickthrough data title and summary of clicked web page in the same search session and the session boundary recognition through temporal closeness and probabilistic similarity between query term in addition the search engine visualizes the rank change of web page a different query are submitted in the same search session to help the user reformulate the query 
a machine learning methodology for the disambiguation of acronym sens is presented which start from an acronym sense dictionary training data is automatically extracted from downloaded document identified from the result of search engine query leave one out cross validation on document with acronym form achieves accuracy and f 
we seek to gain improved insight into how web search engine shouldcope with the evolving web in an attempt to provide user with themost up to date result possible for this purpose we collectedweekly snapshot of some web site over the course of one year and measured the evolution of content and link structure our measurement focus on aspect of potential interest to search engine designer the evolution of link structure over time the rate ofcreation of new page and new distinct content on the web and the rate of change of the content of existing page under search centric measure of degree of change our finding indicate a rapid turnover rate of web page i e high rate of birth and death coupled with an even higher rate ofturnover in the hyperlink that connect them for page that persistover time we found that perhaps surprisingly the degree of contentshift a measured using tf idf cosine distance doe not appear to beconsistently correlated with the frequency of contentupdating despite this apparent non correlation the rate of content shift of a given page is likely to remain consistent over time that is page that change a great deal in one week will likely change by a similarly large degree in the following week conversely page that experience little change will continue to experience little change we conclude the paper with a discussion of the potential implication ofour result for the design of effective web search engine 
in this paper we consider the possibility of altering the pagerank of web page from an administrator s point of view through the modification of the pagerank equation it is shown that this problem can be solved using the traditional quadratic programming technique in addition it is shown that the number of parameter can be reduced by clustering web page together through simple clustering technique this problem can be formulated and solved using quadratic programming technique it is demonstrated experimentally on a relatively large web data set viz the wt g that it is possible to modify the pageranks of the web page through the proposed method using a set of linear constraint it is also shown that the pagerank of other page may be affected and that the quality of the result depends on the clustering technique used it is shown that our result compared well with those obtained by a hit based method 
information retrieval using meta data can be traced back to the early age of ir where document are represented by the controlled vocabulary in this paper we explore the usage of meta data information under the framework of language model we present a new language model that is able to take advantage of the category information for document to improve the retrieval accuracy we compare the new language model with the traditional language model over the trec dataset where the collection information for document is obtained using the k mean clustering method the new language model outperforms the traditional language model which verifies our statement 
we propose new feature and algorithm for automating web page classification task such a content recommendation and ad blocking we show that the automated classification of web page can be much improved if instead of looking at their textual content we consider each link s url and the visual placement of those link on a referring page these feature are unusual rather than being scalar measurement like word count they are tree structured describing the position of the item in a tree we develop a model and algorithm for machine learning using such tree structured feature we apply our method in automated tool for recognizing and blocking web advertisement and for recommending interesting news story to a reader experiment show that our algorithm are both faster and more accurate than those based on the text content of web document 
for many year i have wanted to give a talk like this look backon our subject identify the high and perhaps low point consider what worked what did not work and speculate a littleabout the future now that i at last have the opportunity to givesuch a talk the realisation ha dawned just how difficult it is todo justice to the topic the only way out of this difficulty for mei to emphasise that this is a personal account based on myinvolvement with the field since and that error of omissionand commission are not deliberate but simply due to lack ofknowledge and time on my part to talk of landmark is easy but to say what they are in ir isnot they come in various shape and size event publication experiment idea etc in the course of this presentation i shallbe judiciously mixing all of these however the emphasis will beon idea and their subsequent modelling and testing throughexperimentation the interaction between theory and experiment willbe a recurring theme i will try and associate these developmentswith key individual thereby running the risk of ignoring some iapologise for this in advance the pre history of our subject can be traced back to the work inthe th century perhaps even further but i will pick it up atthe middle of the last century th starting with the work ofrobert fairthorne and vannevar bush this early work emphasised thepossibility of using mechanical device to store and retrieveinformation of course the foundation of modern informationretrieval were properly laid after with the pioneering work ofcleverdon salton sparck jones and others this work gave rise toa strong experimental methodology for the evaluation of theoreticalideas which ha been sustained to this day it ha been a hallmarkof ir research that theory is developed in the context ofexperimentation there is no doubt that many discipline arejealous of the success of trec ir research ha thrown up a number of successful model thesemodels have been based on some often unstated assumption orhypotheses i will attempt to identify some of the underlyingideas giving credit where is due that led to the fruitfulexploration of retrieval model this will include system orientedas well a user oriented idea especially those concerned with themeasurement of retrieval performance ir ha been fortunate in that the subject ha grown through theactive collaboration between computer scientist and informationscientists this ha meant that traditional approach to thestorage and retrieval of information emanating from the libraryworld for example have always strongly influenced newdevelopments this tension between manual human process andautomatic computer based process in ir ha always been fruitful even now with the evolution of idea about meta data and ontologiesneeded to enhance web retrieval the debate about controlledvocabularies versus automatic indexing is relevant issue ofscalability are particularly important here one of the strength that have emerged in our subject is thatmany of our model can be deployed independently of medium ormodality for example retrieving image or audio sequence can behandled in way similar to those used to retrieve text data thishas proved to be great boon to ir the development of web retrievalthrough the deployment of various kind of search engine ha beenbased on the considerable early work in ir although detailing thespecific influence is not easy it is clear that the underlyingmathematical and statistical model in ir have been ubiquitous inapplication the extreme difficulty encountered in making nlp workfor ir forced researcher to develop powerful statistical probabilistic geometrical and logical technique to complementlinguistic one this is now paying off because of the similardifficulties encountered in other medium having given some account of how we got here i will spend alittle time talking about where we go from here how do we extractthe message from the bottle 
discriminative model have been preferred over generative model in many machine learning problem in the recent past owing to some of their attractive theoretical property in this paper we explore the applicability of discriminative classifier for ir we have compared the performance of two popular discriminative model namely the maximum entropy model and support vector machine with that of language modeling the state of the art generative model for ir our experiment on ad hoc retrieval indicate that although maximum entropy is significantly worse than language model support vector machine are on par with language model we argue that the main reason to prefer svms over language model is their ability to learn arbitrary feature automatically a demonstrated by our experiment on the home page finding task of trec 
we present a novel sequential clustering algorithm which is motivated by the information bottleneck ib method in contrast to the agglomerative ib algorithm the new sequential sib approach is guaranteed to converge to a local maximum of the information with time and space complexity typically linear in the data size information a required by the original ib principle moreover the time and space complexity are significantly improved we apply this algorithm to unsupervised document classification in our evaluation on small and medium size corpus the sib is found to be consistently superior to all the other clustering method we examine typically by a significant margin moreover the sib result are comparable to those obtained by a supervised naive bayes classifier finally we propose a simple procedure for trading cluster s recall to gain higher precision and show how this approach can extract cluster which match the existing topic of the corpus almost perfectly 
peer to peer file sharing network are currently receiving much attention a a mean of sharing and distributing information however a recent experience show the anonymous open nature of these network offer an almost ideal environment for the spread of self replicating inauthentic file we describe an algorithm to decrease the number of downloads of inauthentic file in a peer to peer file sharing network that assigns each peer a unique global trust value based on the peer s history of uploads we present a distributed and secure method to compute global trust value based on power iteration by having peer use these global trust value to choose the peer from whom they download the network effectively identifies malicious peer and isolates them from the network in simulation this reputation system called eigentrust ha been shown to significantly decrease the number of inauthentic file on the network even under a variety of condition where malicious peer cooperate in an attempt to deliberately subvert the system 
in this poster we will present the result of effort we have undertaken to conduct evaluation of a qa system in a real world environment and to understand the nature of the dimension on which user evaluate qa system when given full reign to comment on whatever dimension they deem important 
studying web graph is often difcult due to their large size recently several proposal have been published about various technique that allow to store a web graph in memory in a limited space exploiting the inner redundancy of the web the webgraph framework is a suite of code algorithm and tool that aim at making it easy to manipulate large web graph this paper present the compression technique used in webgraph which are centred around referentiation and intervalisation which in turn are dual to each other webgraph can compress the webbase graph mnodes glinks in a little a bit per link and it transposed version in a little a bit per link 
the fluid document project ha developed various research prototype that show that powerful annotation technique based on animated typographical change can help reader utilize annotation more effectively our recently developed fluid open hypermedia prototype support the authoring and browsing of fluid annotation on third party web page this prototype is an extension of the arakne environment an open hypermedia application that can augment web page with externally stored hypermedia structure this paper describes how various web standard including dom cs xlink xpointer and rdf can be used and extended to support fluid annotation 
this paper give an overview of the evaluation method used for the web retrieval task in the third ntcir workshop which is currently in progress in the web retrieval task we try to ass the retrieval effectiveness of each web search engine system using a common data set and attempt to build a re usable test collection suitable for evaluating web search engine system with these objective we have built gigabyte and gigabyte document set mainly gathered from the jp domain relevance judgment is performed on the retrieved document which are written in japanese or english 
arabic a highly inflected language requires good stemming for effective information retrieval yet no standard approach to stem ming ha emerged we developed several light stemmer based on heuristic and a statistical stemmer based on co occurrence for arabic retrieval we compared the retrieval effectiveness of our stemmer and of a morphological analyzer on the trec data the best light stemmer wa more effective for cross lan guage retrieval than a morphological stemmer which tried to find the root for each word a repartitioning process consisting of vowel removal followed by clustering using co occurrence analy si pro duced stem class which were better than no stemming or very light stemming but still inferior to good light stemming or mor phological analysis 
the world wide web contains a number of source code archive program are usually classified into various category within the archive by hand we report on experiment for automatic classification of source code into these category we examined a number of factor that affect classification accuracy weighting feature by expected entropy loss make a significant improvement in classification accuracy we show a support vector machine can be trained to classify source code with a high degree of accuracy we feel these result show promise for software reuse 
new event detection is a challenging task that still offer scope for great improvement after year of effort in this paper we show how performance on new event detection ned can be improved by the use of text classification technique a well a by using named entity in a new way we explore modification to the document representation in a vector space based ned system we also show that addressing named entity preferentially is useful only in certain situation a combination of all the above result in a multi stage ned system that performs much better than baseline single stage ned system 
the simplicity of http wa a major factor in the success of the web however a both the protocol and it us have evolved http ha grown complex this complexity result in numerous problem including confused implementors interoperability failure difficulty in extending the protocol and a long specification without much documented rationale many of the problem with http can be traced to unfortunate choice about fundamental definition and model this paper analyzes the current http protocol design showing how it fails in certain case and how to improve these fundamental some problem with http can be fixed simply by adopting new model and terminology allowing u to think more clearly about implementation and extension other problem require explicit but compatible protocol change 
compression reduces both the size of index and the time needed to evaluate query in this paper we revisit the compression of inverted list of document posting that store the position and frequency of indexed term considering two approach to improving retrieval efficiency better implementation and better choice of integer compression scheme first we propose several simple optimisation to well known integer compression scheme and show experimentally that these lead to significant reduction in time second we explore the impact of choice of compression scheme on retrieval efficiency in experiment on large collection of data we show two surprising result use of simple byte aligned code half the query evaluation time compared to the most compact golomb rice bitwise compression scheme and even when an index fit entirely in memory byte aligned code result in faster query evaluation than doe an uncompressed index emphasising that the cost of transferring data from memory to the cpu cache is le for an appropriately compressed index than for an uncompressed index moreover byte aligned scheme have only a modest space overhead the most compact scheme result in index that are around of the size of the collection while a byte aligned scheme is around we conclude that fast byte aligned code should be used to store integer in inverted list 
in this paper we propose a new data clustering method called concept factorization that model each concept a a linear combination of the data point and each data point a a linear combination of the concept with this model the data clustering task is accomplished by computing the two set of linear coefficient and this linear coefficient computation is carried out by finding the non negative solution that minimizes the reconstruction error of the data point the cluster label of each data point can be easily derived from the obtained linear coefficient this method differs from the method of clustering based on non negative matrix factorization nmf citexu in that it can be applied to data containing negative value and the method can be implemented in the kernel space our experimental result show that the proposed data clustering method and it variation performs best among algorithm and their variation that we have evaluated on both tdt and reuters corpus in addition to it good performance the new method also ha the merit in it easy and reliable derivation of the clustering result 
term based representation of document have found wide spread use in information retrieval however one of the main shortcoming of such method is that they largely disregard lexical semantics and a a consequence are not sufficiently robust with respect to variation in word usage in this paper we investigate the use of concept based document representation to supplement wordor phrase based feature the utilized concept are automatically extracted from document via probabilistic latent semantic analysis we propose to use adaboost to optimally combine weak hypothesis based on both type of feature experimental result on standard benchmark confirm the validity of our approach showing that adaboost achieves consistent improvement by including additional semantic feature in the learned ensemble 
recent advance in information retrieval over hyperlinked corpus have convincingly demonstrated that link carry le noisy information than text we investigate the feasibility of applying link based method in new application domain the specific application we consider is to partition author into opposite camp within a given topic in the context of newsgroups a typical newsgroup posting consists of one or more quoted line from another posting followed by the opinion of the author this social behavior give rise to a network in which the vertex are individual and the link represent responded to relationship an interesting characteristic of many newsgroups is that people more frequently respond to a message when they disagree than when they agree this behavior is in sharp contrast to the www link graph where linkage is an indicator of agreement or common interest by analyzing the graph structure of the response we are able to effectively classify people into opposite camp in contrast method based on statistical analysis of text yield low accuracy on such datasets because the vocabulary used by the two side tends to be largely identical and many newsgroup posting consist of relatively few word of text 
this paper follows a formal approach to information retrieval based on statistical language model by introducing some simple reformulations of the basic language modeling approach we introduce the notion of importance of a query term the importance of a query term is an unknown parameter that explicitly model which of the query term are generated from the relevant document the important term and which are not the unimportant term the new language modeling approach is shown to explain a number of practical fact of today s information retrieval system that are not very well explained by the current state of information retrieval theory including stop word mandatory term coordination level ranking and retrieval using phrase 
topic tracking is complicated when the story in the stream occur in multiple language typically researcher have trained only english topic model because the training story have been provided in english in tracking non english test story are then machine translated into english to compare them with the topic model we propose a native language hypothesis stating that comparison would be more effective in the original language of the story we first test and support the hypothesis for story link detection for topic tracking the hypothesis implies that it should be preferable to build separate language specific topic model for each language in the stream we compare different method of incrementally building such native language topic model 
html document composed of frame can be difficult to write correctly we demonstrate a technique that can be used by author manually creating html document or by document editor to verify that complex frame construction exhibit the intended behavior when browsed the method is based on model checking an automated program verification technique and on temporal logic specification of expected frame behavior we show how to model the html frame source a a cobweb protocol related to the trellis model of hypermedia document we show how to convert the cobweb protocol to input for a model checker and discus several way for author to create the necessary behavior specification our solution allows web document to be built containing a large number of frame and content page interacting in complex way we expect such web structure to be more useful in literary hypermedia than for web site used a interface to organizational information or database 
digital information is increasingly more and more important to enable interaction and transaction on the internet on the other hand leakage of sensitive information can have harmful effect for people enterprise and government this paper focus on the problem of dealing with timed release of confidential information and simplifying it access once public it is a common issue in the industry government and day to day life we introduce the hp time vault service based on the emerging identifier based encryption ibe cryptography schema ibe public encryption key specify the disclosure time these key are used to encrypt confidential information an independent time server generates and publishes ibe decryption key correspondent to the current time at predefined interval we discus the advantage of this approach against current approach based on traditional cryptography a web service based prototype is described a a proof of concept 
push technology i e the ability of sending relevant information to client in reaction to new event is a fundamental aspect of modern information system xml is rapidly emerging a the widely adopted standard for information exchange and representation and hence several xml based protocol have been defined and are the object of investigation at w c and throughout commercial organization in this paper we propose the new concept of active xml rule for pushing reactive service to xml enabled repository rule operate on xml document and deliver information to interested remote user in reaction to update event occurring at the repository site the proposed mechanism assumes the availability of xml repository supporting a standard xml query language such a xquery that is being developed by the w c for the implementation of the reactive component it capitalizes on the use of standard dom event and of the soap interchange standard to enable the remote installation of active rule a simple protocol is proposed for subscribing and unsubscribing remote rule 
a person reading a book need to build an understanding based on the available textual material a a result of a survey of user reading behaviour and of existing e book user interface we found that most of these interface provide poor support for the actual process of reading and comprehension in particular there is generally minimal support for understanding the overall structure or contextual structure and the narrative structure of a book we propose adapting topic tracking and detection technique to discover the narrative thread within a book and hence it narrative structure the contextual and narrative structure will be presented to the user through purpose designed visualisation which will be integrated and linked within a newly developed e book browser we have chosen to use the bible a our test corpus a it ha a rich narrative structure and relatively complex contextual structure evaluation of the interface and it component will be done through field study involving actual reader of the bible to ass the effectiveness of the user interface in enhancing a user s experience 
we present an xml programming language designed for the implementation of web service xl is portable and fully compliant with w c standard such a xquery xml protocol and xml schema one of the key feature of xl is that it allows programmer to concentrate on the logic of their application xl provides high level and declarative construct for action which are typically carried out in the implementation of a web service e g logging error handling retry of action workload management event etc issue such a performance tuning e g caching horizontal partitioning etc should be carried out automatically by an implementation of the language this way the productivity of the programmer the ability of evolution of the program and the chance to achieve good performance are substantially enhanced 
topic detection and tracking tdt research explores the development of algorithm to detect novel event and track their development over time for online report development of these method requires careful evaluation and analysis traditional reductive method of evaluation only represent some of the available information of algorithm behaviour we describe a visualisation tool for topic tracking which make it easy to analysis and compare the temporal behaviour of tracking algorithm 
the process driven composition of web service is emerging a a promising approach to integrate business application within and across organizational boundary in this approach individual web service are federated into composite web service whose business logic is expressed a a process model the task of this process model are essentially invocation to functionality offered by the underlying component service usually several component service are able to execute a given task although with different level of pricing and quality in this paper we advocate that the selection of component service should be carried out during the execution of a composite service rather than at design time in addition this selection should consider multiple criterion e g price duration reliability and it should take into account global constraint and preference set by the user e g budget constraint accordingly the paper proposes a global planning approach to optimally select component service during the execution of a composite service service selection is formulated a an optimization problem which can be solved using efficient linear programming method experimental result show that this global planning approach outperforms approach in which the component service are selected individually for each task in a composite service 
previous research in novelty detection ha focused on the task of finding novel material given a set or stream of document on a certain topic this study investigates the more difficult two part task defined by the trec novelty track given a topic and a group of document relevant to that topic find the relevant sentence from the document and find the novel sentence from the collection of relevant sentence our research show that the former step appears to be the more difficult part of this task and that the performance of novelty measure is very sensitive to the presence of non relevant sentence 
dynamic web content provides u with time sensitive and continuously changing data to glean up to date information user need to regularly browse collect and analyze this web content without proper tool support this information management task is tedious time consuming and error prone especially when the quantity of the dynamic web content is large when many information management service are needed to analyze it and when underlying service network are not completely reliable this paper describes a multi level lifecycle design time and run time coordination mechanism that enables rapid efficient development and execution of information management application that are especially useful for processing dynamic web content such a coordination mechanism brings dynamism to coordinating independent distributed information management service dynamic parallelism spawn merges multiple execution service branch based on available data and dynamic run time reconfiguration coordinate service execution to overcome faulty service and bottleneck these feature enable information management application to be more efficient in handling content and format change in web resource and enable the application to be evolved and adapted to process dynamic web content 
textual communication in message board is analyzed for classifying web community we present a communication content based generalization of an existing business oriented classification of web community using keygraph a method for visualizing the co occurrence relation between word and word cluster in text here the text in a message board is analyzed with keygraph and the structure obtained is shown to reflect the essence of the content flow the relation of this content flow with participant interest is then formalized three structure feature of relation between participant and word determining the type of the community are shown to be computed and visualized centralization context coherence and creative decision this help in surveying the essence of a community e g whether the community creates useful knowledge how easy it is to join the community and whether why the community is good for making commercial advertisement 
we present a unified framework for simultaneously solving both the pooling problem the construction of efficient document pool for the evaluation of retrieval system and metasearch the fusion of ranked list returned by retrieval system in order to increase performance the implementation is based on the hedge algorithm for online learning which ha the advantage of convergence to bounded error rate approaching the performance of the best linear combination of the underlying system the choice of a loss function closely related to the average precision measure of system performance ensures that the judged document set performs well both in constructing a metasearch list and a a pool for the accurate evaluation of retrieval system our experimental result on trec data demonstrate excellent performance in all measure evaluation of system retrieval of relevant document and generation of metasearch list 
a web site are now ordinary product it is necessary to explicit the notion of quality of a web site the quality of a site may belinked to the easiness of accessibility and also to other criterion such a the fact that the site is up to date and coherent this last quality is difficult to insure because site may be updated very frequently may have many author may be partially generated and inthis context proof reading is very difficult the same piece of information may be found in different occurrence but also in data ormeta data leading to the need for consistency checking in this paper we make a parallel between program and web site we present some example of semantic constraint that one would like to specify constraint between the meaning of category and sub category in a thematic directory consistency between the organization chart and the rest of the site in an academic site we present quickly the natural semantics a way to specify the semantics of programming language that inspires ourworks natural semantics itself come from both an operational semantics and from logic programming and it implementation us prolog then we propose a specification language for semantic constraint in web site that in conjunction with the well known make program permit to generate some site verification tool by compiling the specification into prolog code we apply our method to alarge xml document which is the scientific part of our instituteactivity report tracking error or inconsistency and alsoconstructing some indicator that can be used by the management of theinstitute 
this paper examines whether the cranfield evaluation methodology is robust to gross violation of the completeness assumption i e the assumption that all relevant document within a test collection have been identified and are present in the collection we show that current evaluation measure are not robust to substantially incomplete relevance judgment a new measure is introduced that is both highly correlated with existing measure when complete judgment are available and more robust to incomplete judgment set this finding suggests that substantially larger or dynamic test collection built using current pooling practice should be viable laboratory tool despite the fact that the relevance information will be incomplete and imperfect 
traditionally when one want to learn about a particular topic one read a book or a survey paper with the rapid expansion of the web learning in depth knowledge about a topic from the web is becoming increasingly important and popular this is also due to the web s convenience and it richness of information in many case learning from the web may even be essential because in our fast changing world emerging topic appear constantly and rapidly there is often not enough time for someone to write a book on such topic to learn such emerging topic one can resort to research paper however research paper are often hard to understand by non researcher and few research paper cover every aspect of the topic in contrast many web page often contain intuitive description of the topic to find such web page one typically us a search engine however current search technique are not designed for in depth learning top ranking page from a search engine may not contain any description of the topic even if they do the description is usually incomplete since it is unlikely that the owner of the page ha good knowledge of every aspect of the topic in this paper we attempt a novel and challenging task mining topic specific knowledge on the web our goal is to help people learn in depth knowledge of a topic systematically on the web the proposed technique first identify those sub topic or salient concept of the topic and then find and organize those informative page containing definition and description of the topic and sub topic just like those in a book experimental result using topic show that the proposed technique are highly effective 
vinci is a local area service oriented architecture soa designed for rapid development and management of robust web application based on xml document exchange vinci is designed to complement and interoperate with wide area soas such a soap uddi and net this paper present the vinci architecture the rationale behind it design and an evaluation of it performance specifically we show how system architected with vinci are developed quickly scaled effortlessly and easily moved from prototype to production 
the web provides a global platform for knowledge sharing however several shortcoming still arise from the absence of personalization and collaboration in web search more effective retrieval technique could be provided by mean of transforming explicit knowledge into implicit knowledge the approach presented in this paper is based on a peer to peer architecture and aim at complementing classical web search in term of personalized ranking list these local ranking can be accumulated and evaluated in order to supplement the process of knowledge generation by building virtual knowledge community furthermore the aggregation of ranking list can be used to identify topic a well a community of interest together with social aspect for community support a framework for congenial web search is defined 
the paper proposes and empirically motivates an integration of supervised learning with unsupervised learning to deal with human bias in summarization in particular we explore the use of probabilistic decision tree within the clustering framework to account for the variation a well a regularity in human created summary 
document clustering ha long been an important problem in information retrieval in this paper we present a new clustering algorithm asi which us explicitly modeling of the subspace structure associated with each cluster asi simultaneously performs data reduction and subspace identification via an iterative alternating optimization procedure motivated from the optimization procedure we then provide a novel method to determine the number of cluster we also discus the connection of asi with various existential clustering approach finally extensive experimental result on real data set show the effectiveness of asi algorithm 
use of semantic content is one of the major issue which need to be addressed for improving image retrieval effectiveness we present a new approach to classify image based on the combination of image processing technique and hybrid neural network multiple keywords are assigned to an image to represent it main content i e semantic content image are divided into a number of region and colour and texture feature are extracted the first classifier a self organising map som cluster similar image based on the extracted feature then region of the representative image of these cluster were labeled and used to train the second classifier composed of several support vector machine svms initial experiment on the accuracy of keyword assignment for a small vocabulary are reported 
answer pattern have been shown to improve the perfor mance of open domain factoid qa system their use however requires either constructing the pattern manually or developing algorithm for learning them automatically we present here a simpler approach that extends the technique of language modeling to create answer model these are language model trained on the correct answer to training question we show how they fit naturally into a probabilis tic model for answer passage retrieval and demonstrate their effectiveness on the trec qa corpus 
most of the work on xml query and search ha stemmed from the publishing and database community mostly for the need of business application recently the information retrieval community began investigating the xml search issue to answer information discovery need following this trend we present here an approach where information need can be expressed in an approximate manner a piece of xml document or xml fragment of the same nature a the document that are being searched we present an extension of the vector space model for searching xml collection via xml fragment and ranking result by relevance we describe how we have extended a full text search engine to comply with this model the value of the proposed method is demonstrated by the relative high precision of our system which wa among the top performer in the recent inex workshop our result indicate that certain query are more appropriate than others for the extended vector space model specifically query with relatively specific context but vague information need are best situated to reap the benefit of this model finally our result show that one method may not fit all type of query and that it could be worthwhile to use different solution for different application 
in this paper we introduce the fractal summarization model based on the fractal theory in fractal summarization the important information is captured from the source text by exploring the hierarchical structure and salient feature of the document a condensed version of the document that is informatively close to the original is produced iteratively using the contractive transformation in the fractal theory user evaluation ha shown that fractal summarization outperforms traditional summarization 
the world wide web is evolving into a medium that will soon make it possible for conceiving and implementing situation aware service a situation aware or situated web application is one that render the user with an experience content interaction and presentation that is so tailored to his her current situation this requires the fact and opinion regarding the context to be communicated to the server by mean of a profile which is then applied against the description of the application object at the server in order to generate the required experience this paper discus a profile view of the situated web architecture and analyzes the key technology and capability that enable them we conclude that trusted framework wherein rich vocabulary describing user and their context application and document along with rule for processing them are critical element of such architecture 
contextual retrieval is a critical technique for facilitating many important application such a mobile search personalized search pc troubleshooting etc despite of it importance there is no comprehensive retrieval model to describe the contextual retrieval process we observed that incompatible context noisy context and incomplete query are several important issue commonly existing in contextual retrieval application however these issue have not been previously explored and discussed in this paper we propose probabilistic model to address these problem our study clearly show that query log is the key to build effective contextual retrieval model we also conduct a case study in the pc troubleshooting domain to testify the performance of the proposed model and experimental result show that the model can achieve very good retrieval precision 
a form of advertisement which is becoming very popular on the web is based on electronic coupon e coupon distribution e coupon are the digital analogue of paper coupon which are used to provide customer with discount or gift in order to incentive the purchase of some product nowadays the potential of digital coupon ha not been fully exploited on the web this is mostly due to the lack of efficient technique to handle the generation and distribution of e coupon in this paper we discus model and protocol for e coupon satisfying a number of security requirement our protocol is lightweight and preserve the privacy of the user since it doe not require any registration phase 
a directed network of people connected by rating or trust score and a model for propagating those trust score is a fundamental building block in many of today s most successful e commerce and recommendation system we develop a framework of trust propagation scheme each of which may be appropriate in certain circumstance and evaluate the scheme on a large trust network consisting of k trust score expressed among k people we show that a small number of expressed trust distrust per individual allows u to predict trust between any two people in the system with high accuracy our work appears to be the first to incorporate distrust in a computational trust propagation setting 
the paper describes a user study examining method for improving user query specifically interactive and automatic query expansion and advanced search option the user study includes subjective and objective evaluation of the effect of the above method and a comparison between the real and perceived effect 
we present a non traditional retrieval problem we call subtopic retrieval the subtopic retrieval problem is concerned with finding document that cover many different subtopics of a query topic in such a problem the utility of a document in a ranking is dependent on other document in the ranking violating the assumption of independent relevance which is assumed in most traditional retrieval method subtopic retrieval pose challenge for evaluating performance a well a for developing effective algorithm we propose a framework for evaluating subtopic retrieval which generalizes the traditional precision and recall metric by accounting for intrinsic topic difficulty a well a redundancy in document we propose and systematically evaluate several method for performing subtopic retrieval using statistical language model and a maximal marginal relevance mmr ranking strategy a mixture model combined with query likelihood relevance ranking is shown to modestly outperform a baseline relevance ranking on a data set used in the trec interactive track 
applying belief revision logic to model adaptive information retrieval is appealing since it provides a rigorous theoretical foundation to model partiality and uncertainty inherent in any information retrieval ir process in particular a retrieval context can be formalised a a belief set and the formalised context is used to disambiguate vague user query belief revision logic also provides a robust computational mechanism to revise an ir system s belief about the user changing information need in addition information flow is proposed a a text mining method to automatically acquire the initial ir context the advantage of a belief based irsystem is that it ir behaviour is more predictable and explanatory however computational efficiency is often a concern when the belief revision formalism are applied to large real life application this paper describes our belief based adaptive ir system which is underpinned by an efficient belief revision mechanism our initial experiment show that the belief based symbolic ir model is more effective than a classical quantitative ir model to our best knowledge this is the first successful empirical evaluation of a logic based ir model based on large ir benchmark collection 
annotea is a web based shared annotation system based on a general purpose open resource description framework rdf infrastructure where annotation are modeled a a class of metadata annotation are viewed a statement made by an author about a web document annotation are external to the document and can be stored in one or more annotation server one of the goal of this project ha been to re use a much existing w c technology a possible we have reached it mostly by combining rdf with xpointer xlink and http we have also implemented an instance of our system using the amaya editor browser and a generic rdf database accessible through an apache http server in this implementation the merging of annotation with document take place within the client the paper present the overall design of annotea and describes some of the issue we have faced and how we have solved them 
in this paper we present an evaluation of technique that are designed to encourage web searcher to interact more with the result of a web search two specific technique are examined the presentation of sentence that highly match the searcher s query and the use of implicit evidence implicit evidence is evidence captured from the searcher s interaction with the retrieval result and is used to automatically update the display our evaluation concentrate on the effectiveness and subject perception of these technique the result show with statistical significance that the technique are effective and efficient for information seeking 
word sense ambiguity is recognized a having a detrimental effect on the precision of information retrieval system in general and web search system in particular due to the sparse nature of the query involved despite continued research into the application of automated word sense disambiguation the question remains a to whether le than accurate automated word sense disambiguation can lead to improvement in retrieval effectiveness in this study we explore the development and subsequent evaluation of a statistical word sense disambiguation system which demonstrates increased precision from a sense based vector space retrieval model over traditional tf idf technique 
the world wide web wa originally developed a a shared writable hypertext medium a facility that is still widely needed we have recently developed a web based management reporting system for a legal firm in an attempt to improve the efficiency and management of their overall business process this paper share our experience in relating the firm s specific writing and issue tracking task to existing web open hypermedia and semantic web research and describes why we chose to develop a new solution a set of open hypermedia component collectively called the management reporting system rather than employ an existing system 
we present a new web middleware architecture that allows user to customize their view of the web for optimal interaction and system operation when using non traditional resource limited client machine such a wireless pda personal digital assistant web stream customizers wsc are dynamically deployable software module and can be strategically located between client and server to achieve improvement in performance reliability or security an important design feature is that customizers provide two point of control in the communication path between client and server supporting adaptive system based and content based customization our architecture exploit http s proxy capability allowing customizers to be seamlessly integrated with the basic web transaction model we describe the wsc architecture and implementation and illustrate it use with three non trivial adaptive customizer application that we have built we show that the overhead in our implementation is small and tolerable and is outweighed by the benefit that customizers provide 
thesaurus ha been widely used in many application including information retrieval natural language processing and question answering in this paper we propose a novel approach to automatically constructing a domain specific thesaurus from the web using link structure information the proposed approach is able to identify new term and reflect the latest relationship between term a the web evolves first a set of high quality and representative website of a specific domain is selected after filtering out navigational link link analysis is applied to each website to obtain it content structure finally the thesaurus is constructed by merging the content structure of the selected website the experimental result on automatic query expansion based on our constructed thesaurus show improvement in search precision compared to the baseline 
xml retrieval is a departure from standard document retrieval in which each individual xml element ranging from italicized word or phrase to full blown article is a potentially retrievable unit the distribution of xml element length is unlike what we usually observe in standard document collection prompting u to revisit the issue of document length normalization we perform a comparative analysis of arbitrary element versus relevant element and show the importance of length a a parameter for xml retrieval within the language modeling framework we investigate a range of technique that deal with length either directly or indirectly we observe a length bias introduced by the amount of smoothing and show the importance of extreme length prior for xml retrieval we also show that simply removing shorter element from the index by introducing a cut off value doe not create an appropriate document length normalization even after increasing the minimal size of xml element occurring in the index the importance of an extreme length bias remains 
in response to a query a search engine return a ranked list of document if the query is about a popular topic i e it match many document then the returned list is usually too long to view fully study show that user usually look at only the top to result however we can exploit the fact that the best target for popular topic are usually linked to by enthusiast in the same domain in this paper we propose a novel ranking scheme for popular topic that place the most authoritative page on the query topic at the top of the ranking our algorithm operates on a special index of expert document these are a subset of the page on the www identified a directory of link to non affiliated source on specific topic result are ranked based on the match between the query and relevant descriptive text for hyperlink on expert page pointing to a given result page we present a prototype search engine that implement our ranking scheme and discus it performance with a relatively small million page expert index our algorithm wa able to perform comparably on popular query with the best of the mainstream search engine 
we present a web search interface designed to encourage user to interact more fully with the result of a web search wrapping around a major commercial search engine the system combine three main feature real time query biased web document summarisation the presentation of sentence highly relevant to the searcher s query and evidence captured from searcher interaction with the retrieval result 
reagent are remotely executing agent that customize web browsing for non standard client a reagent is essentially a one shot mobile agent that act a an extension of a client dynamically launched by the client to run on it behalf at a remote more advantageous location reagent simplify the use of mobile agent technology by transparently handling data migration and run time network communication and provide a general interface for programmer to more easily implement their application specific customizing logic this is made possible by the identification of useful remote behavior i e common pattern of action that exploit the ability to process and communicate remotely example of such behavior are transformer monitor cachers and collators in this paper we identify a set ofuseful reagent behavior for interacting with web service via astandard browser describe how to program and use reagent and show that the overhead of using reagent is low and outweighed by it benefit 
we propose two new tool to address the evolution of hyperlinked corpus first we define time graph to extend the traditional notion of an evolving directed graph capturing link creation a a point phenomenon in time second we develop definition and algorithm for time dense community tracking to crystallize the notion of community evolution we develop these tool in the context of blogspace the space of weblogs or blog our study involves approximately k link among k blog we create a time graph on these blog by an automatic analysis of their internal time stamp we then study the evolution of connected component structure and microscopic community structure in this time graph we show that blogspace underwent a transition behavior around the end of and ha been rapidly expanding over the past year not just in metric of scale but also in metric of community structure and connectedness this expansion show no sign of abating although measure of connectedness must plateau within two year by randomizing link destination in blogspace but retaining source and timestamps we introduce a concept of randomized blogspace herein we observe similar evolution of a giant component but no corresponding increase in community structure having demonstrated the formation of micro community over time we then turn to the ongoing activity within active community we extend recent work of kleinberg to discover dense period of bursty intra community link creation 
a scalable approach to trust negotiation is required in web service environment that have large and dynamic requester population we introduce trust serv a model driven trust negotiation framework for web service the framework employ a model for trust negotiation that is based on state machine extended with security abstraction our policy model support lifecycle management an important trait in the dynamic environment that characterize web service in particular we provide a set of change operation to modify policy and migration strategy that permit ongoing negotiation to be migrated to new policy without being disrupted experimental result show the performance benefit of these strategy the proposed approach ha been implemented a a container centric mechanism that is transparent to the web service and to the developer of web service simplifying web service development and management a well a enabling scalable deployment 
we propose a self supervised word segmentation technique for chinese information retrieval this method combine the advantage of traditional dictionary based approach with character based approach while overcoming many of their shortcoming experiment on trec data show comparable performance to both the dictionary based and the character based approach however our method is language independent and unsupervised which provides a promising avenue for constructing accurate multilingual information retrieval system that are flexible and adaptive 
the web s hyperlink are notoriously brittle and break whenever a resource migrates one solution to this problem is a transparent resource migration mechanism which separate a resource s location from it identity and help provide referential integrity however although several such mechanism have been designed they have not been widely adopted due largely to a lack of compliance with current web standard in addition these mechanism must be updated manually whenever a resource migrates limiting their effectiveness for large web site recently however new web protocol such a webdav web distributed authoring and versioning have emerged which extend the http protocol and provide a new level of control over web resource in this paper we show how we have used these protocol in the design of a new resource migration protocol rmp which enables transparent resource migration across standard web server the rmp work with a new resource migration mechanism we have developed called the resource locator service rls and is fully backwards compatible with the web s architecture enabling all web server and all web content to be involved in the migration process we describe the protocol and the new rls in full together with a prototype implementation and demonstration application that we have developed the paper concludes by presenting performance data taken from the prototype that show how the rls will scale well beyond the size of today s web 
we present an algorithm for translating xslt program into sql our context is that of virtual xml publishing in which a single xml view is defined from a relational database and subsequently queried with xslt program each xslt program is translated into a single sql query and run entirely in the database engine our translation work for a large fragment of xslt which we define that includes descendant ancestor axis recursive template mode parameter and aggregate we put considerable effort in generating correct and efficient sql query and describe several optimization technique to achieve this efficiency we have tested our system on all sql query of the tpc h database benchmark which we represented in xslt and then translated back to sql using our translator 
in this poster we report on the effect of pseudo relevance feedback prf for a cross language image retrieval task using a test collection typically prf ha been shown to improve retrieval performance in previous clir experiment based on average precision at a fixed rank however our experiment have shown that query in which no relevant document are returned also increase because query reformulation for cross language is likely to be harder than with monolingual searching a great deal of user dissatisfaction would be associated with this scenario we propose that an additional effectiveness measure based on failed query may better reflect user satisfaction than average precision alone 
in this poster we present an overview of the technique we used to develop and evaluate a text categorisation system to automatically classify racist text detecting racism is difficult because the presence of indicator word is insufficient to indicate racist text unlike some other text classification task support vector machine svm are used to automatically categorise web page based on whether or not they are racist different interpretation of what constitutes a term are taken and in this poster we look at three representation of a web page within an svm bag of word bigram and part of speech tag 
multimedia scheduling model provide a rich variety of tool for managing the synchronization of medium like video and audio but generally have an inflexible model for time itself in contrast modern animation model in the computer graphic community generally lack tool for synchronization and structural time but allow for a flexible concept of time including variable pacing acceleration and deceleration and other tool useful for controlling and adapting animation behavior multimedia author have been forced to choose one set of feature over the others limiting the range of presentation they can create some programming model addressed some of these problem but provided no declarative mean for author and authoring tool to leverage the functionality this paper describes a new model incorporated into smil that combine the strength of scheduling model with the flexible time manipulation of animation model the implication of this integration are discussed with respect to scheduling and structured time drawing upon experience with smil timing and synchronization and the integration with xhtml 
in information retrieval it is well known that the complexity of processing boolean query depends on the size of the intermediate result which could be huge and are typically on disk even though the size of the final result may be quite small in the case of inverted file the most time consuming operation is the merging or intersection of the list of occurrence we propose the keyword tree k tree and forest efficient structure to handle boolean query in keyword based information retrieval extensive simulation show that k tree is order of magnitude faster i e far fewer i o s for boolean query than the usual approach of merging the list of occurrence and incurs only a small overhead for single keyword query the k tree can be efficiently parallelized a well the construction cost of k tree is comparable to the cost of building inverted file 
structured document rich information need and detailed information about user are becoming more pervasive within everyday computing usage application such a question answering reading tutor and xml retrieval demand more robust retrieval on richly annotated document in order to effectively serve these application the community will need a better understanding of the combination of evidence in this work i propose that the use of simple generative probabilistic model will be an effective framework for these problem statistical language model which are a special case of generative probabilistic model have been used extensively within recent information retrieval research their flexibility ha been very effective in adapting to numerous task and problem i propose to extend the statistical language modeling framework to handle rich information need and document with structural and linguistic annotation much of the prior work on combination of evidence ha had few well studied theoretical contribution so i also propose to develop a sounder theoretical basis which give more predictable result 
within the initiative for the evaluation of xml retrieval inex a number of metric to evaluate the effectiveness of content oriented xml retrieval approach were developed although these metric provide a solution towards addressing the problem of overlapping result element they do not consider the problem of overlapping reference component within the recall base thus leading to skewed effectiveness score we propose alternative metric that aim to provide a solution to both overlap issue 
this paper present a method for admission control and request scheduling for multiply tiered e commerce web site achieving both stable behavior during overload and improved response time our method externally observes execution cost of request online distinguishing different request type and performs overload protection and preferential scheduling using relatively simple measurement and a straight forward control mechanism unlike previous proposal which require extensive change to the server or operating system our method requires no modification to the host o s web server application server or database since our method is external it can be implemented in a proxy we present such an implementation called gatekeeper using it with standard software component on the linux operating system we evaluate the proxy using the industry standard tpc w workload generator in a typical three tiered e commerce environment we show consistent performance during overload and throughput increase of up to percent response time improves by up to a factor of with only a percent penalty to large job 
in earlier work we proposed a way for a web server to detect connectivity information about client accessing it in order to take tailored action for a client request this paper describes the design implementation and evaluation of such a working system a web site ha a strong incentive to reduce the time to glass to retain user who may otherwise lose interest and leave the site we have performed a measurement study from multiple client site around the world with various level of connectivity to the internet communicating with modified apache web server under our control the result show that client can be classified in a correct and stable manner and that user perceived latency can be reduced via tailored action our measurement show that classification and determination of server action are done without significant overhead on the web server we explore a variety of modified action ranging from selecting a lower quality version of the resource to altering the manner of content delivery by studying numerous performance related factor in a single unified framework and examining both individual action a well a combination of action our modified web server implementation show the efficacy of various server action 
peer to peer p p system are very large computer network where peer collaborate to provide a common service providing large scale information retrieval ir e g for searching the word wide web is an appealing application for p p system the research community ha presented several proposal for p p ir however so far the concept of p p and of ir have been intermingled in this paper we propose an architecture to structure p p ir system we difierentiate between concept belonging to the construction and maintenance of a p p overlay network and those belonging to ir furthermore we distinguish basic p p ir concept which are likely to be needed in all p p ir system and advanced p p ir concept that rather depend on the avor of the system this decomposition of the p p retrieval process is an important step towards a structured implementation of such system furthermore it allows a systematic sharing of method and resource needed to perform retrieval the next generation of global information retrieval system will combine these distributed resource in new way to provide more e cient web search keywords peer to peer information retrieval p p ir architecture key based routing kbr p p web search 
richly interlinked machine understandable data constitute the basis for the semantic web we provide a framework cream that allows for creation of metadata while the annotation mode of cream allows to create metadata for existing web page the authoring mode let author create metadata almost for free while putting together the content of a page a a particularity of our framework cream allows to create relational metadata i e metadata that instantiate interrelated definition of class in a domain ontology rather than a comparatively rigid template like schema asm dublin core we discus some of the requirement one ha to meet when developing such an ontology based framework e g the integration of a metadata crawler inference service document management and a meta ontology and describe it implementation viz ont o mat a component based ontology driven web page authoring and annotation tool 
term dependence is a natural consequence of language use it successful representation ha been a long standing goal for information retrieval research we present a methodology for the construction of a concept hierarchy that take into account the three basic dimension of term dependence we also introduce a document evaluation function that allows the use of the concept hierarchy a a user profile for information filtering initial experimental result indicate that this is a promising approach for incorporating term dependence in the way document are filtered 
data referring to cultural calendar such a the widespread gregorian date but also date after the chinese hebrew or islamic calendar a well a data referring to professional calendar like fiscal year or teaching term are omnipresent on the web formalism such a xml schema have acknowledged this by offering a rather extensive set of gregorian date and time a basic data type this article introduces into catts the calendar and time type system catts go far beyond predefined date and time type after the gregorian calendar a supported by xml schema catts first give rise to declaratively specify more or le complex cultural or professional calendar including specificity such a leap second leap year and time zone catts further offer a tool for the static type checking of data typed after calendar s defined in catts catts finally offer a language for declaratively expressing and a solver for efficiently solving temporal constraint referring to calendar s expressed in catts catts complement data modeling and reasoning method designed for generic semantic web application such a rdf or owl with method specific to the particular application domain of calendar and time 
the growth of the web ha posed new challenge for information retrieval ir most of the current system are based on traditional model which have been developed for atomic and independent document and are not adapted to the web a promising research orientation consists of studying the impact of the web structure on indexing the hyperdocument model presented in this article is based on essential aspect of information comprehension content composition and linear non linear reading 
this paper compare document blind feedback and passage blind feedback in information retrieval ir based on the work during the nrrc reliable information access summer workshop the analysis of our experimental result show overall consistency on the performance impact of using passage and document for blind feedback however it is observed that the behavior of passage blind feedback compared to document blind feedback is both system dependent and topic dependent the relationship between the performance impact of passage blind feedback and the number of feedback term and the topic s average relevant document length respectively are examined to illustrate these dependency 
web link analysis ha proven to be a significant enhancement for quality based web search most existing link can be classified into two category intra type link e g web hyperlink which represent the relationship of data object within a homogeneous data type web page and inter type link e g user browsing log which represent the relationship of data object across different data type user and web page unfortunately most link analysis research only considers one type of link in this paper we propose a unified link analysis framework called link fusion which considers both the interand intratype link structure among multiple type inter related data object and brings order to object in each data type at the same time the pagerank and hit algorithm are shown to be special case of our unified link analysis framework experiment on an instantiation of the framework that make use of the user data and web page extracted from a proxy log show that our proposed algorithm could improve the search effectiveness over the hit and directhit algorithm by and respectively 
in this paper we propose a new approach for topic distillation on world wide web topic distillation is to find quality document related to the user query topic our approach is based on bharat s topic distillation algorithm we present the analysis of hyperlink graph structure using hierarchy concept tree to solve the mixed hub problem that is also remained in the bharat s algorithm for assigning better weight to hyperlink which point to relevant document among hyperlink in a document we try to find the relationship in document connected by hyperlink using content analysis and we assign weight to hyperlink based on the relationship we evaluated this algorithm using topic on wt g corpus and obtained improved result 
wireless access with mobile or handheld device is a promising addition to the www and traditional electronic business mobile device provide convenience and portable access to the huge information space on the internet without requiring user to be stationary with network connection however the limited screen size narrow network bandwidth small memory capacity and low computing power are the shortcoming of handheld device loading and visualizing large document on handheld device become impossible the limited resolution restricts the amount of information to be displayed the download time is intolerably long in this paper we introduce the fractal summarization model for document summarization on handheld device fractal summarization is developed based on the fractal theory it generates a brief skeleton of summary at the first stage and the detail of the summary on different level of the document are generated on demand of user such interactive summarization reduces the computation load in comparing with the generation of the entire summary in one batch by the traditional automatic summarization which is ideal for wireless access three tier architecture with the middle tier conducting the major computation is also discussed visualization of summary on handheld device is also investigated 
we investigate how user interact with the result page of a www search engine using eye tracking the goal is to gain insight into how user browse the presented abstract and how they select link for further exploration such understanding is valuable for improved interface design a well a for more accurate interpretation of implicit feedback e g clickthrough for machine learning the following present initial result focusing on the amount of time spent viewing the presented abstract the total number of abstract viewed a well a measure of how thoroughly searcher evaluate their result set 
the exponential growth of data demand scalable infrastructure capable of indexing and searching rich content such a text music and image a promising direction is to combine information re trieval with peer to peer technology for scalability fault tolerance and low administration cost one pioneering work along this di rection is psearch psearch place document onto a peer topeer overlay network according to semantic vector produced using latent semantic indexing lsi the search cost for a query is reduced since document related to the query are likely to be co located on a small number of node unfortunately because of it reliance on lsi psearch also inherits the limitation of lsi when the corpus is large and heterogeneous lsi s retrieval quality is inferior to method such a okapi the singular value decomposition svd used in lsi is unscalable in term of both memory consumption and computation time this paper address the above limitation of lsi and make the following contribution to reduce the cost of svd we reduce the size of it input matrix through document clustering and term selection our method retains the retrieval quality of lsi but is several order of magnitude more efficient through extensive experimentation we found that proper normalization of semantic vector for term and document improves recall by to further improve retrieval quality we use low dimensional subvectors of semantic vector to cluster document in the overlay and then use okapi to guide the search and document selection 
this paper evaluates an extraction based approach to answering definitional question our system extracted useful linguistic construct called linguistic feature from raw text using information extraction tool and formulated answer based on such feature the feature employed include appositives copula structured pattern relation proposition and raw sentence the feature were ranked based on feature type and similarity to a question profile redundant feature were detected using a simple heuristic based strategy the approach achieved state of the art performance at the trec qa evaluation component analysis of the system wa carried out using an automatic scoring function called rouge lin and hovy major finding include answer using linguistic feature are significantly better than those using raw sentence the most useful feature are appositives and copula question profile a a mean of modeling user interest can significantly improve system performance the rouge score are closely correlated with subjective evaluation result indicating the suitability of using rouge for evaluating definitional qa system 
many collaborative music recommender system cmrs have succeeded in capturing the similarity among user or item based on rating however they have rarely considered about the available information from the multimedia such a genre let alone audio feature from the medium stream such information is valuable and can be used to solve several problem in r in this paper we design a cmrs based on audio feature of the multimedia stream in the cmrs we provide recommendation service by our proposed method where a clustering technique is used to integrate the audio feature of music into the collaborative filtering cf framework in hope of achieving better performance experiment are carried out to demonstrate that our approach is feasible 
in a categorized information space predicting user information need at the category level can facilitate personalization caching and other topic oriented service this paper present a two phase model to predict the category of a user s next access based on previous access phase generates a snapshot of a user s preference among category based on a temporal and frequency analysis of the user s access history phase us the computed preference to make prediction at different category granularity several alternative for each phase are evaluated using the rating behavior of on line raters a the form of access considered the result show that a method based on re access pattern and frequency analysis of a user s whole history ha the best prediction quality even over a path based method markov model that us the combined history of all user 
four statistical visual feature index are proposed slm shot length mean the average length of each shot in a video sld shot length deviation the standard deviation of shot length for a video onm object number mean the average number of object per frame of the video and ond object number deviation the standard deviation of the number of object per frame across the video each of these index provides a unique perspective on video content a novel video retrieval interface ha been developed a a platform to examine our assumption that the new index facilitate some video retrieval task initial feedback is promising and formal experiment are planned 
we describe an efficient robust method for selecting and optimizing term for a classification or filtering task term are extracted from positive example in training data based on several alternative term selection algorithm then combined additively after a simple term score normalization step to produce a merged and ranked master term vector the score threshold for the master vector is set via beta gamma regulation over all the available training data the process avoids para meter calibration and protracted training it also result in compact profile for run time evaluation of test new document result on trec filtering task datasets demonstrate substantial improvement over trec median result and rival both idealized ir based result and optimized and expensive svm based classifier in general effectiveness 
the ability to find table and extract information from them is a necessary component of data mining question answering and other information retrieval task document often contain table in order to communicate densely packed multi dimensional information table do this by employing layout pattern to efficiently indicate field and record in two dimensional form their rich combination of formatting and content present difficulty for traditional language modeling technique however this paper present the use of conditional random field crfs for table extraction and compare them with hidden markov model hmms unlike hmms crfs support the use of many rich and overlapping layout and language feature and a a result they perform significantly better we show experimental result on plain text government statistical report in which table are located with f and their constituent line are classified into table related category with accuracy we also discus future work on undirected graphical model for segmenting column finding cell and classifying them a data cell or label cell 
much work in information retrieval focus on using a model of document and query to derive retrieval algorithm model based development is a useful alternative to heuristic development because in a model the assumption are explicit and can be examined and refined independent of the particular retrieval algorithm we explore the explicit assumption underlying the na ve framework by performing computational analysis of actual corpus and query to devise a generative document model that closely match text our thesis is that a model so developed will be more accurate than existing model and thus more useful in retrieval a well a other application we test this by learning from a corpus the best document model we find the learned model better predicts the existence of text data and ha improved performance on certain ir task 
in this paper we present two way to improve the precision of hit based algorithm on web document first by analyzing the limitation of current hit based algorithm we propose a new weighted hit based method that assigns appropriate weight to in link of root document then we combine content analysis with hit based algorithm and study the effect of four representative relevance scoring method vsm okapi tl and cdr using a set of broad topic query our experimental result show that our weighted hit based method performs significantly better than bharat s improved hit algorithm when we combine our weighted hit based method or bharat s hit algorithm with any of the four relevance scoring method the combined method are only marginally better than our weighted hit based method between the four relevance scoring method there is no significant quality difference when they are combined with a hit based algorithm 
we seek insight into latent semantic indexing by establishing a method to identify the optimal number of factor in the reduced matrix for representing a keyword this method is demonstrated empirically by duplicating all document containing a term t and inserting new document in the database that replace t with t by examining the number of time term t is identified for a search on term t precision using differing range of dimension we find that lower ranked dimension identify related term and higher ranked dimension discriminate between the synonym 
web page classification is much more difficult than pure text classification due to a large variety of noisy information embedded in web page in this paper we propose a new web page classification algorithm based on web summarization for improving the accuracy we first give empirical evidence that ideal web page summary generated by human editor can indeed improve the performance of web page classification algorithm we then propose a new web summarization based classification algorithm and evaluate it along with several other state of the art text summarization algorithm on the looksmart web directory experimental result show that our proposed summarization based classification algorithm achieves an approximately improvement a compared to pure text based classification algorithm we further introduce an ensemble classifier using the improved summarization algorithm and show that it achieves about improvement over pure text based method 
it is widely recognized that distributed denial of service ddos attack can disrupt electronic commerce and cause large revenue loss however effective defense continue to be mostly unavailable we describe and evaluate vipnet a novel value added network service for protecting e commerce and other transaction based site from ddos attack in vipnet e merchant pay internet service provider isps to carry the packet of the e merchant best client called vip in a privileged class of service co protected from congestion whether malicious or not in the regular co vipnet reward vip with not only better quality of service but also greater availability because vip right are clientand server specific cannot be forged are usage limited and are only replenished after successful client transaction e g purchase it is impractical for attacker to mount and sustain ddos attack against an e merchant s vip vipnet can be deployed incrementally and doe not require universal adoption experiment demonstrate vipnet s benefit 
in this poster we incorporate user query history a context information to improve the retrieval performance in interactive retrieval experiment using the trec data show that incorporating such context information indeed consistently improves the retrieval performance in both average precision and precision at document 
xml is the w c standard document format for writing and exchanging information on the web rdf is the w c standard model for describing the semantics and reasoning about information on the web unfortunately rdf and xml although very close to each other are based on two different paradigm we argue that in order to lead the semantic web to it full potential the syntax and the semantics of information need to work together to this end we develop a model theoretic semantics for the xml xquery and xpath data model which provides a unified model for both xml and rdf this unified model can serve a the basis for web application that deal with both data and semantics we illustrate the use of this model on a concrete information integration scenario our approach enables each side of the fence to benefit from the other notably we show how the rdf world can take advantage of xml query language and how the xml world can take advantage of the reasoning capability available for rdf 
this poster describes initial work exploring a relatively unexamined area of data fusion fusing the result of retrieval system whose collection have no overlap between them many of the effective meta search data fusion strategy gain much of their success from exploiting document overlap across the source system being merged when the intersection of the collection is the empty set the strategy generally degrade to a simpler form in order to address such situation two strategy were examined re ranking of merged result using a locally run search on the text fragment returned by the source search engine and re ranking based on cross document similarity again using text fragment presented in the retrieved list result from experiment which go beyond previous work indicate that both strategy improve fusion effectiveness 
an important trend in web information processing is the support of multimedia retrieval however the most prevailing paradigm for multimedia retrieval content based retrieval cbr is a rather conservative one whose performance depends on a set of specifically defined low level feature and a carefully chosen sample object in this paper an aggressive search mechanism called octopus is proposed which address the retrieval of multi modality data using multifaceted knowledge in particular octopus promotes a novel scenario in which the user supply seed object of arbitrary modality a the hint of his information need and receives a set of multi modality object satisfying his need the foundation of octopus is a multifaceted knowledge base constructed on a layered graph model lgm which describes the relevance between medium object from various perspective link analysis based retrieval algorithm is proposed based on the lgm a unique relevance feedback technique is developed to update the knowledge base by learning from user behavior and to enhance the retrieval performance in a progressive manner a prototype implementing the proposed approach ha been developed to demonstrate it feasibility and capability through illustrative example 
xml messaging is at the heart of web service providing the flexibility required for their deployment composition and maintenance yet current approach to web service development hide the messaging layer behind java or c apis preventing the application to get direct access to the underlying xml information to address this problem we advocate the use of a native xml language namely xquery a an integral part of the web service development infrastructure the main contribution of the paper is a binding between wsdl the web service description language and xquery the approach enables the use of xquery for both web service deployment and composition we present a simple command line tool that can be used to automatically deploy a web service from a given xquery module and extend the xquery language itself with a statement for accessing one or more web service the binding provides tight coupling between wsdl and xquery yielding additional benefit notably the ability to use wsdl a an interface language for xquery and the ability to perform static typing on xquery program that include web service call last but not least the proposal requires only minimal change to the existing infrastructure we report on our experience implementing this approach in the galax xquery processor 
we formulate and propose the template detection problem and suggest a practical solution for it based on counting frequent item set we show that the use of template is pervasive on the web we describe three principle which characterize the assumption made by hypertext information retrieval ir and data mining dm system and show that template are a major source of violation of these principle a a consequence basic pure implementation of simple search algorithm coupled with template detection and elimination show surprising increase in precision at all level of recall 
subject or prepositional content ha been the focus of most classification research genre or style on the other hand is a different and important property of text and automatic text genre classification is becoming important for classification and retrieval purpose a well a for some natural language processing research in this paper we present a method for automatic genre classification that is based on statistically selected feature obtained from both subject classified and genre classified training data the experimental result show that the proposed method outperforms a direct application of a statistical learner often used for subject classification we also observe that the deviation formula and discrimination formula using document frequency ratio also work a expected we conjecture that this dual feature set approach can be generalized to improve the performance of subject classification a well 
real world application of text categorization often require a system to deal with ten of thousand of category defined over a large taxonomy this paper address the problem with respect to a set of popular algorithm in text categorization including support vector machine k nearest neighbor ridge regression linear least square fit and logistic regression by providing a formal analysis of the computational complexity of each classification method followed by an investigation on the usage of different classifier in a hierarchical setting of categorization we show how the scalability of a method depends on the topology of the hierarchy and the category distribution in addition we are able to obtain tight bound for the complexity by using the power law to approximate category distribution over a hierarchy experiment with knn and svm classifier on the ohsumed corpus are reported on a concrete example 
this paper present a new dependence language modeling approach to information retrieval the approach extends the basic language modeling approach based on unigram by relaxing the independence assumption we integrate the linkage of a query a a hidden variable which express the term dependency within the query a an acyclic planar undirected graph we then assume that a query is generated from a document in two stage the linkage is generated first and then each term is generated in turn depending on other related term according to the linkage we also present a smoothing method for model parameter estimation and an approach to learning the linkage of a sentence in an unsupervised manner the new approach is compared to the classical probabilistic retrieval model and the previously proposed language model with and without taking into account term dependency result show that our model achieves substantial and significant improvement on trec collection 
the fundamental difference between standard information retrieval and xml retrieval is the unit of retrieval in traditional ir the unit of retrieval is fixed it is the complete document in xml retrieval every xml element in a document is a retrievable unit this make xml retrieval more difficult besides being relevant a retrieved unit should be neither too large nor too small the research presented here a comparative analysis of two approach to xml retrieval aim to shed light on which xml element should be retrieved the experimental evaluation us data from the initiative for the evaluation of xml retrieval inex 
the corpus analysis method in chinese keyword extraction look on the corpus a a single sample of language stochastic process but the distribution of keywords in the whole corpus and in each document are very different from each other the extraction based on global statistical information only can get significant keywords in the whole corpus max duplicated string contain the local significant keywords in each document in this paper we designed an efficient algorithm to extract the max duplicated string by building pat tree for the document so that the keywords can be picked out from the max duplicated string by their sig value in the corpus 
term weighting method have been shown to give significant increase in information retrieval performance the presence of pronomial reference in document reduces the term frequency of associated word with a consequent effect on term weight and information retrieval behaviour this investigation explores the impact on information retrieval performance of broad coverage automatic pronoun resolution result indicate that this approach ha potential to improve both precision at fixed cutoff level and average precision 
the scalable vector graphic format svg is already substantially improving graphic delivery on the web but some important issue still remain to be addressed in particular svg doe not support client side adaption of document to different viewing condition such a varying screen size style preference or different device capability based on our earlier work we show how svg can be extended with constraint based specification of document layout to augment it with adaptive capability the core of our proposal is to include one way constraint into svg which offer more expressiveness than the previously suggested class of linear constraint and at the same time require substantially le computational effort 
accurate cross language information retrieval requires that query term be correctly translated several new technique to improve the translation of out of vocabulary term in english chinese cross language information retrieval have been developed however these require query and a document collection to enable translation disambiguation although effective they involve much processing and searching of the web at query time and may not be practical in a production web search engine in this work we consider what task maybe carried out beforehand the goal being to reduce the processing required at query time we have successfully developed new technique to extract and translate out of vocabulary term using the web and add them into a translation dictionary prior to query time 
providing video on demand vod service over the internet in a scalable way is a challenging problem in this paper we propose p cast an architecture that us a peer to peer approach to cooperatively stream video using patching technique while only relying on unicast connection among peer we address the following two key technical issue in p cast constructing an application overlay appropriate for streaming and providing continuous stream playback without glitch in the face of disruption from an early departing client our simulation experiment show that p cast can serve many more client than traditional client server unicast service and that it generally out performs multicast based patching if client can cache more than of a stream s initial portion we handle disruption by delaying the start of playback and applying the shifted forwarding technique a threshold on the length of time during which arriving client are served in a single session in p cast serf a a knob to adjust the balance between the scalability and the client viewing quality in p cast 
soboroff nicholas and cahan recently proposed a method for evaluating the performance of retrieval system without relevance judgment they demonstrated that the system evaluation produced by their methodology are correlated with actual evaluation using relevance judgment in the trec competition in this work we propose an explanation for this phenomenon we devise a simple measure for quantifying the similarity of retrieval system by assessing the similarity of their retrieved result then given a collection of retrieval system and their retrieved result we use this measure to ass the average similarity of a system to the other system in the collection we demonstrate that evaluating retrieval system according to average similarity yield result quite similar to the methodology proposed by soboroff et al and we further demonstrate that these two technique are in fact highly correlated thus the technique are effectively evaluating and ranking retrieval system by popularity a opposed to performance 
user make journey through the web web travel encompasses the task of orientation and navigation the environment and the purpose of the journey the ease of travel it mobility varies from page to page and site to site for visually impaired user in particular mobility is reduced the object that support travel are inaccessible or missing altogether web development tool need to include support to increase mobility we present a framework for finding and classifying travel object within web page the evaluation carried out ha shown that this framework support a systematic and consistent method for assessing travel upon the web we propose that such a framework can provide the foundation for a semi automated tool for the support of travel upon the web 
we have developed a method for recommending item that combine content and collaborative data under a single probabilistic framework we benchmark our algorithm against a na ve bayes classifier on the cold start problem where we wish to recommend item that no one in the community ha yet rated we systematically explore three testing methodology using a publicly available data set and explain how these method apply to specific real world application we advocate heuristic recommenders when benchmarking to give competent baseline performance we introduce a new performance metric the croc curve and demonstrate empirically that the various component of our testing strategy combine to obtain deeper understanding of the performance characteristic of recommender system though the emphasis of our testing is on cold start recommending our method for recommending and evaluation are general 
model schema language msl is an attempt to formalize some of the core idea in xml schema the benefit of a formal description are that it is both concise and precise msl ha already proved helpful in work on the design of xml query we expect that similar technique can be used to extend msl to include most or all xml schema 
multimodal interface are becoming increasingly ubiquitous with the advent of mobile device accessibility consideration and novel software technology that combine diverse interaction medium in addition to improving access and delivery capability such interface enable flexible and personalized dialog with website much like a conversation between human in this paper we present a software framework for multimodal web interaction management that support mixed initiative dialog between user and website a mixed initiative dialog is one where the user and the website take turn changing the flow of interaction the framework support the functional specification and realization of such dialog using staging transformation a theory for representing and reasoning about dialog based on partial input it support multiple interaction interface and offer sessioning caching and co ordination function through the use of an interaction manager two case study are presented to illustrate the promise of this approach 
since cdn simulation are known to be highly memory intensive in this paper we argue the need for reducing the memory requirement of such simulation we propose a novel memory efficient data structure that store cache state for a small subset of popular object accurately and us approximation for storing the state for the remaining object since popular object receive a large fraction of the request while le frequently accessed object consume much of the memory space this approach yield large memory saving and reduces error we use bloom filter to store approximate state and show that careful choice of parameter can substantially reduce the probability of error due to approximation we implement our technique into a user library for constructing proxy cache in cdn simulator our experimental result show up to an order of magnitude reduction in memory requirement of cdn simulation while incurring a error 
although interactive query reformulation ha been actively studied in the laboratory little is known about the actual behavior of web searcher who are offered terminological feedback along with their search result we analyze log session for two group of user interacting with variant of the altavista search engine a baseline group given no terminological feedback and a feedback group to whom twelve refinement term are offered along with the search result we examine uptake refinement effectiveness condition of use and refinement type preference although our measure of overall session success show no difference between outcome for the two group we find evidence that a subset of those user presented with terminological feedback do make effective use of it on a continuing basis 
a online document collection continue to expand both on the web and in proprietary environment the need for duplicate detection becomes more critical the goal of this work is to facilitate a investigation into the phenomenon of near duplicate and b algorithmic approach to minimizing it negative effect on search result harnessing the expertise of both client user and professional searcher we establish principled method to generate a test collection for identifying and handling inexact duplicate document 
we compare standard global ir searching with user centric localized technique to address the database selection problem we conduct a series of experiment to compare the retrieval effectiveness of three separate search mode applied to a hierarchically structured data environment of textual database representation the data environment is represented a a tree like directory containing over unique database and over total leaf node our search mode consist of varying degree of browse and search from a global search at the root node to a refined search at a sub node using dynamically calculated inverse document frequency idf to score candidate database for probable relevance our finding indicate that a browse and search approach that relies upon localized searching from sub node is capable of producing the most effective result 
query ambiguity is a generally recognized problem particularly in web environment where query are commonly only one or two word in length in this study we explore one technique that find commonly occurring pattern of part of speech near a one word query and allows them to be transformed into clarification question we use a technique derived from statistical language modeling to show that the clarification query will reduce ambiguity much of the time and often quite substantially 
this paper evaluates the xlink format in comparison with other linking format the comparison is based on xspect an implementation of xlink xspect handle transformation between an open hypermedia format ohif and xlink and the paper discus this isomorphic transformation and generalises it to include another open hypermedia format fohm the xspect system based on xslt and javascript provides user with an interface to browse and merge linkbases xspect support navigational hypermedia in the form of link inserted on the fly into web page a well a guided tour presented a svg xspect ha two implementation one server side and one running on the client both implementation provide the user with an interface for the creation of annotation the main result of the paper is a critique of xlink xlink is shown to be a format well suited for navigational hypermedia but lacking in more advanced construct more problematic are the issue regarding large scale use such a evaluating validity and credibility of linkbases and ensuring general support for a format a flexible a xlink 
this work evaluates a few search strategy for arabic monolingual and cross lingual retrieval using the trec arabic corpus a the test bed the release by nist in of an arabic corpus of nearly k document with both monolingual and cross lingual query and relevance judgment ha been a new enabler for empirical study experimental result show that spelling normalization and stemming can significantly improve arabic monolingual retrieval character tri gram from stem improved retrieval modestly on the test corpus but the improvement is not statistically significant to further improve retrieval we propose a novel thesaurus based technique different from existing approach to thesaurus based retrieval ours formulates word synonym a probabilistic term translation that can be automatically derived from a parallel corpus retrieval result show that the thesaurus can significantly improve arabic monolingual retrieval for cross lingual retrieval clir we found that spelling normalization and stemming have little impact 
pseudo feedback is a commonly used technique to improve information retrieval performance it assumes a few top ranked document to be relevant and learns from them to improve the retrieval accuracy a serious problem is that the performance is often very sensitive to the number of pseudo feedback document in this poster we address this problem in a language modeling framework we propose a novel two stage mixture model which is le sensitive to the number of pseudo feedback document than an effective existing feedback model the new model can tolerate a more flexible setting of the number of pseudo feedback document without the danger of losing much retrieval accuracy 
web accessibility is an important goal however most approach to it attainment are based on unrealistic economic model in which web content developer are required to spend too much for which they receive too little we believe this situation is due in part to the overly narrow definition given both to those who stand to benefit from enhanced access to the web and what is meant by this enhanced access in this paper we take a broader view discussing a complementary approach that cost developer le and provides greater advantage to a larger community of user while we have quite specific aim in our technical work we hope it can also serve a an example of how the technical conversation regarding web accessibility can move beyond the narrow confines of limited adaptation for small population 
observation from a unique investigation of failure analysis of information retrieval ir research engine are presented the reliable information access ria workshop invited seven leading ir research group to supply both their system and their expert to an effort to analyze why their system fail on some topic and whether the failure are due to system flaw approach flaw or the topic itself there were surprising result from this cross system failure analysis one is that despite system retrieving very different document the major cause of failure for any particular topic wa almost always the same across all system another is that relationship between aspect of a topic are not especially important for state of the art system the system are failing at a much more basic level where the top retrieved document are not reflecting some aspect at all 
syntactic information potentially play a much more important role in question answering than it doe in information retrieval although many people have used syntactic evidence in question answering there haven t been many detailed experiment reported in the literature the aim of the experiment described in this paper is to study the impact of a particular approach for using syntactic information on question answering effectiveness our result indicate that a combination of syntactic information with heuristic for ranking potential answer can perform better than the ranking heuristic on their own 
in this paper we describe the concept of federated information sharing community fisc and associated architecture which provide a way for organisation distributed workgroups and individual to build up a federated community based on their common interest over the world wide web to support community we develop capability that go beyond the generic retrieval of document to include the ability to retrieve people their interest and inter relationship we focus on providing social awareness in the large to help user understand the member within a community and the relationship between them within the fisc framework we provide viewpoint retrieval to enable a user to construct visual contextual view of the community from the perspective of any community member to evaluate these idea we develop test bed to compare individual component technology such a user and group profile construction and similarity matching and we develop prototype to explore the broader architecture and usage issue 
abstract to exploit the similarity information hidden in the hyperlinkstructure of the web this paper introduces algorithmsscalable to graph with billion of vertex on a distributedarchitecture the similarity of multi step neighborhood ofvertices are numerically evaluated by similarity function includingsimrank a recursive refinement of cocitation psimrank a novel variant with better theoretical characteristic and the jaccard coe cient extended to multi stepneighborhoods our 
it is not easy to tokenize agglutinative language like japanese and chinese into word many ir system start with a dictionary based morphology program like chasen unfortunately dictionary cannot cover all possible word unknown word such a proper noun are important for ir this paper proposes a statistical dictionary free method for selecting index string based on recent work on adaptive language modeling 
in general term the evaluation of a summary depends on how close it is to the chief point in the source text this begets the question a to what are the chief point in the source text and how is this information used in itself in identifying the source text this is crucially important when we discus automatic evaluation of summary so the question of main point is the source text typically this would be around a nucleus of keywords however the salience the frequency and the relationship of the text with other text in the collection of these keywords is perhaps are important text categorisation using neural network explicates these point well and also ha a practical impact 
a program that make an existing website look like a database is called a wrapper wrapper learning is the problem of learning website wrapper from example we present a wrapper learning system called wl that can exploit several different representation of a document example of such different representation include dom level and token level representation a well a two dimensional geometric view of the rendered page for tabular data and representation of the visual appearance of text asm it will be rendered additionally the learning system is modular and can be easily adapted to new domain and task the learning system described is part of an industrial strength wrapper management system that is in active use at whizbang lab controlled experiment show that the learner ha broader coverage and a faster learning rate than earlier wrapper learning system 
hypermedia system and more specifically open hypermedia system oh provide a rich set of implementation of different hypertext flavor such a navigational hypertext spatial hypertext or taxonomic hypertext additionally these system offer component based modular architecture and address interoperability between hypertext domain despite multiple effort of integrating web client a widespread adoption of oh technology by web developer ha not taken place in this paper it is argued that web service which offer a component model for web application can be integrated in oh an architectural integration is proposed a step by step process is outlined and an example of integration is provided this very approach is aimed to benefit both world the web community with new rich hypermedia functionality that extends the current navigational hypermedia and the oh community by opening it tool and platform to the many developer group of the web community 
time based medium centric web presentation can be described declaratively in the xml world through the development of language such a smil it is difficult however to fully integrate them in a complete document transformation processing chain in order to achieve the desired processing of data driven time based medium centric presentation the text flow based formatting vocabulary used by style language such a xsl cs and dsssl need to be extended the paper present a selection of use case which are used to derive a list of requirement for a multimedia style and transformation formatting vocabulary the boundary of applicability of existing text based formatting model for medium centric transformation are analyzed the paper then discus the advantage and disadvantage of a fully fledged time based multimedia formatting model finally the discussion is illustrated by describing the key property of the example multimedia formatting vocabulary currently implemented in the back end of our cuypers multimedia transformation engine 
okapi bm scoring of anchor text surrogate document ha been shown to facilitate effective ranking in navigational search task over web data we hypothesize that even better ranking can be achieved in certain important case particularly when anchor score must be fused with content score by avoiding length normalisation and by reducing the attentuation of score associated with high tf preliminary result are presented 
internet server selection mechanism attempt to optimize subject to a variety of constraint the distribution of client request to a geographically and topologically diverse pool of server research on server selection ha thus far focused primarily on technique for choosing a server from a group administered by single entity like a content distribution network provider in a federated multi provider computing system however selection must occur over distributed server set deployed by the participating provider without the benefit of the full information available in the single provider case intelligent server set selection algorithm will require a model of the expected performance client would receive from a candidate server set in this paper we study whether the complex policy and dynamic of intelligent server selection can be effectively modeled in order to predict client performance for server set we introduce a novel server set distance metric and use it in a measurement study of several million server selection transaction to develop simple model of existing server selection scheme we then evaluate these model in term of their ability to accurately predict performance for a second larger set of distributed client we show that our model are able to predict performance within m for over of the observed sample our analysis demonstrates that although existing deployment use a variety of complex and dynamic server selection criterion most of which are proprietary these scheme can be modeled with surprising accuracy 
in contrast to traditional document retrieval a web page a a whole is not a good information unit to search because it often contains multiple topic and a lot of irrelevant information from navigation decoration and interaction part of the page in this paper we propose a vision based page segmentation vip algorithm to detect the semantic content structure in a web page compared with simple dom based segmentation method our page segmentation scheme utilizes useful visual cue to obtain a better partition of a page at the semantic level by using our vip algorithm to assist the selection of query expansion term in pseudo relevance feedback in web information retrieval we achieve performance improvement on web track dataset 
ontology play a prominent role on the semantic web they make possible the widespread publication of machine understandable data opening myriad opportunity for automated information processing however because of the semantic web s distributed nature data on it will inevitably come from many different ontology information processing across ontology is not possible without knowing the semantic mapping between their element manually finding such mapping is tedious error prone and clearly not possible at the web scale hence the development of tool to assist in the ontology mapping process is crucial to the success of the semantic web we describe glue a system that employ machine learning technique to find such mapping given two ontology for each concept in one ontology glue find the most similar concept in the other ontology we give well founded probabilistic definition to several practical similarity measure and show that glue can work with all of them this is in contrast to most existing approach which deal with a single similarity measure another key feature of glue is that it us multiple learning strategy each of which exploit a different type of information either in the data instance or in the taxonomic structure of the ontology to further improve matching accuracy we extend glue to incorporate commonsense knowledge and domain constraint into the matching process for this purpose we show that relaxation labeling a well known constraint optimization technique used in computer vision and other field can be adapted to work efficiently in our context our approach is thus distinguished in that it work with a variety of well defined similarity notion and that it efficiently incorporates multiple type of knowledge we describe a set of experiment on several real world domain and show that glue proposes highly accurate semantic mapping 
capitalizing on the intuitive underlying assumption of language modelling for ad hoc retrieval we present a novel approach that is capable of injecting the user s context of the document collection into the retrieval process the preliminary finding from the evaluation undertaken suggest that improved ir performance is possible under certain circumstance this motivates further investigation to determine the extent and significance of this improved performance 
the simplicity of the basic client server model of web service led quickly to it widespread adoption but also to scalability and performance problem the technological response to these problem ha been the development of technology for the creation of surrogate for web server starting with simple proxy cache and reverse proxy and leading more recently to the development of content distribution network surrogate technology based on cache have proved quite successful in reducing the load due to delivery of cacheable content html file and image but in general they cannot replicate service that are implemented through the execution of program at the server a full service surrogate is a technology that is designed to address this issue directly because it is a copy or mirror of the server that is created managed and updated automatically one of the central issue in the creation of full service surrogate is portability of interpreted content and the representation of metadata necessary to support execution in this paper we describe the portable channel representation pcr which is an extensible markup language resource definition framework encoded data model developed to enable full service surrogate and we discus the implication of the increasing importance of executable web service 
in this paper we present a framework and a system that extract event relevant to a query from a collection c of document and place such event along a timeline each event is represented by a sentence extracted from c based on the assumption that important event are widely cited in many document for a period of time within which these event are of interest in our experiment we used query that are event type earthquake and person name e g george bush evaluation wa performed using g leader name a query comparison made by human evaluator between manually and system generated timeline showed that although manually generated timeline are on average more preferable system generated timeline are sometimes judged to be better than manually constructed one 
the web pose itself a the largest data repository ever available in the history of humankind major effort have been made in order to provide efficient access to relevant information within this huge repository of data although several technique have been developed to the problem of web data extraction their use is still not spread mostly because of the need for high human intervention and the low quality of the extraction result in this paper we present a domain oriented approach to web data extraction and discus it application to automatically extracting news from web site our approach is based on a highly efficient tree structure analysis that produce very effective result we have tested our approach with several important brazilian on line news site and achieved very precise result correctly extracting of the news in a set of page distributed among different site 
the success of the semantic web depends on the availability of ontology a well a on the proliferation of web page annotated with metadata conforming to these ontology thus a crucial question is where to acquire these metadata from in this paper wepropose pankow pattern based annotation through knowledge on theweb a method which employ an unsupervised pattern based approach to categorize instance with regard to an ontology the approach is evaluated against the manual annotation of two human subject the approach is implemented in ontomat an annotation tool for the semantic web and show very promising result 
list question answering qa offer a unique challenge in effectively and efficiently locating a complete set of distinct answer from huge corpus or the web in trec the median average f performance of list qa system wa only this paper exploit the wealth of freely available text and link structure on the web to seek complete answer to list question we employ natural language parsing web page classification and clustering to find reliable list answer we also study the effectiveness of web page classification on both the recall and uniqueness of answer for web based list qa 
it is crucial for cross language information retrieval clir system to deal with the translation of unknown query due to that real query might be short the purpose of this paper is to investigate the feasibility of exploiting the web a the corpus source to translate unknown query for clir we propose an online translation approach to determine effective translation for unknown query term via mining of bilingual search result page obtained from web search engine this approach can alleviate the problem of the lack of large bilingual corpus translate many unknown query term provide flexible query specification and extract semantically close translation to benefit clir task especially for cross language web search 
this paper describes a dynamic service reconfiguration model where the proxy is composed of a chain of service object called mobilets pronounced a mo be let which can be deployed onto the network actively this model offer flexibility because the chain of mobilets can be dynamically reconfigured to adapt to the vigorous change in the characteristic of the wireless environment without interrupting the service provision for other mobile node furthermore mobilets can also be migrated to a new proxy server when the mobile node move to a different network domain we have realized the dynamic service reconfiguration model by crafting it design into a programmable infrastructure that form the baseline architecture of the webpads short for web proxy for actively deployable service system 
web page often contain clutter such a pop up ad unnecessary image and extraneous link around the body of an article that distracts a user from actual content extraction of useful and relevant content from web page ha many application including cell phone and pda browsing speech rendering for the visually impaired and text summarization most approach to removing clutter or making content more readable involve changing font size or removing html and data component such a image which take away from a webpage s inherent look and feel unlike content reformatting which aim to reproduce the entire webpage in a more convenient form our solution directly address content extraction we have developed a framework that employ easily extensible set of technique that incorporate advantage of previous work on content extraction our key insight is to work with the dom tree rather than with raw html markup we have implemented our approach in a publicly available web proxy to extract content from html web page 
automatically generated html a produced by wysiwyg program typically contains much repetitive and unnecessary markup thispaper identifies aspect of such html that may be altered whileleaving a semantically equivalent document and proposes technique to achieve optimizing modification these technique include attribute re arrangement via dynamic programming the use of style class and dead coderemoval these technique produce document a small a of original size the size decrease obtained are still significant when the technique are used in combination with conventional text based compression 
crawling the web is deceptively simple the basic algorithm is a fetch a page b parse it to extract all linked url c for all the url not seen before repeat a c however the size of the web estimated at over billion page and it rate of change estimated at per week move this plan from a trivial programming exercise to a serious algorithmic and system design challenge indeed these two factor alone imply that for a reasonably fresh and complete crawl of the web step a must be executed about a thousand time per second and thus the membership test c must be done well over ten thousand time per second against a set too large to store in main memory this requires a distributed architecture which further complicates the membership test a crucial way to speed up the test is to cache that is to store in main memory a dynamic subset of the seen url the main goal of this paper is to carefully investigate several url caching technique for web crawling we consider both practical algorithm random replacement static cache lru and clock and theoretical limit clairvoyant caching and infinite cache we performed about simulation using these algorithm with various cache size using actual log data extracted from a massive day web crawl that issued over one billion http request our main conclusion is that caching is very effective in our setup a cache of roughly entry can achieve a hit rate of almost interestingly this cache size fall at a critical point a substantially smaller cache is much le effective while a substantially larger cache brings little additional benefit we conjecture that such critical point are inherent to our problem and venture an explanation for this phenomenon 
collaborative filtering cf is valuable in e commerce and for direct recommendation for music movie news etc but today s system have several disadvantage including privacy risk a we move toward ubiquitous computing there is a great potential for individual to share all kind of information about place and thing to do see and buy but the privacy risk are severe in this paper we describe a new method for collaborative filtering which protects the privacy of individual data the method is based on a probabilistic factor analysis model privacy protection is provided by a peer to peer protocol which is described elsewhere but outlined in this paper the factor analysis approach handle missing data without requiring default value for them we give several experiment that suggest that this is most accurate method for cf to date the new algorithm ha other advantage in speed and storage over previous algorithm finally we suggest application of the approach to other kind of statistical analysis of survey or questionaire data 
the optimal setting of retrieval parameter often depend on both the document collection and the query and are usually found through empirical tuning in this paper we propose a family of two stage language model for information retrieval that explicitly capture the different influence of the query and document collection on the optimal setting of retrieval parameter a a special case we present a two stage smoothing method that allows u to estimate the smoothing parameter completely automatically in the first stage the document language model is smoothed using a dirichlet prior with the collection language model a the reference model in the second stage the smoothed document language model is further interpolated with a query background language model we propose a leave one out method for estimating the dirichlet parameter of the first stage and the use of document mixture model for estimating the interpolation parameter of the second stage evaluation on five different database and four type of query indicates that the two stage smoothing method with the proposed parameter estimation method consistently give retrieval performance that is close to or better than the best result achieved using a single smoothing method and exhaustive parameter search on the test data 
query length in best match information retrieval ir system is well known to be positively related to effectiveness in the ir task when measured in experimental non interactive environment however in operational interactive ir system query length is quite typically very short on the order of two to three word we report on a study which tested the effectiveness of a particular query elicitation technique in increasing initial searcher query length and which tested the effectiveness of query elicited using this technique and the relationship in general between query length and search effectiveness in interactive ir result show that the specific technique result in longer query than a standard query elicitation technique that this technique is indeed usable that the technique result in increased user satisfaction with the search and that query length is positively correlated with user satisfaction with the search 
we measure the wt g test collection used in the trec and trec web track with common measure used in the web topology community in order to see if wt g look like the web this is not an idle question characteristic of the web such a power law relationship diameter and connected component have all been observed within the scope of general web crawl constructed by blindly following link in contrast wt g wa carved out from a larger crawl specifically to be a web search test collection within the reach of university researcher doe such a collection retain the property of the larger web in the case of wt g yes 
forming test collection relevance judgment from the pooled output of multiple retrieval system ha become the standard process for creating resource such a the trec clef and ntcir test collection this paper present a series of experiment examining three different way of building test collection where no system pooling is used first a collection formation technique combining manual feedback and multiple system is adapted to work with a single retrieval system second an existing method based on pooling the output of multiple manual search is re examined testing a wider range of searcher and retrieval system than ha been examined before third a new approach is explored where the ranked output of a single automatic search on a single retrieval system is assessed for relevance no pooling whatsoever using established technique for evaluating the quality of relevance judgment in all three case test collection are formed that are a good a trec 
a novel method for simultaneous keyphrase extraction and generic text summarization is proposed by modeling text document a weighted undirected and weighted bipartite graph spectral graph clustering algorithm are useed for partitioning sentence of the document into topical group with sentence link prior being exploited to enhance clustering quality within each topical group saliency score for keyphrases and sentence are generated based on a mutual reinforcement principle the keyphrases and sentence are then ranked according to their saliency score and selected for inclusion in the top keyphrase list and summary of the document the idea of building a hierarchy of summary for document capturing different level of granularity is also briefly discussed our method is illustrated using several example from news article news broadcast transcript and web document 
rdf based p p network have a number of advantage compared with simpler p p network such a napster gnutella or with approach based on distributed index such a can and chord rdf based p p network allow complex and extendable description of resource instead of fixed and limited one and they provide complex query facility against these metadata instead of simple keyword based search in previous paper we have described the edutella infrastructure and different kind of edutella peer implementing such an rdf based p p network in this paper we will discus these rdf based p p network a a specific example of a new type of p p network schema based p p network and describe the use of super peer based topology for these network super peer based network can provide better scalability than broadcast based network and do provide perfect support for inhomogeneous schema based network which support different metadata schema and ontology crucial for the semantic web furthermore a we will show in this paper they are able to support sophisticated routing and clustering strategy based on the metadata schema attribute and ontology used especially helpful in this context is the rdf functionality to uniquely identify schema attribute and ontology the resulting routing index can be built using dynamic frequency counting algorithm and support local mediation and transformation rule and we will sketch some first idea for implementing these advanced functionality a well 
in this study we show experimental result on using independent component analysis ica and the self organizing map som in document analysis our document are segment of spoken dialogue carried out over the telephone in a customer service transcribed into text the task is to analyze the topic of the discussion and to group the discussion into meaningful subset the quality of the grouping is studied by comparing to a manual topical classification of the document 
in this poster we present a model of the flow of information among bioinformatics resource in the context of a specific scientific problem combining task analysis with traditional qualitative research we determined the extent to which the bioinformatics analysis process could be automated the model represents a semi automated process involving fourteen distinct data processing step and form the framework for an interface to bioinformatics information 
this paper provides an objective evaluation of the performance impact of binary xml encoding using a fast stream based xquery processor a our representative application instead of proposing one binary format and comparing it against standard xml parser we investigate the individual effect of several binary encoding technique that are shared by many proposal our goal is to provide a deeper understanding of the performance impact of binary xml encoding in order to clarify the ongoing and often contentious debate over their merit particularly in the domain of high performance xml stream processing 
this paper present a novel domain independent text segmentation method which identifies the boundary of topic change in long text document and or text stream the method consists of three component a a preprocessing step we eliminate the document dependent stop word a well a the generic stop word before the sentence similarity is computed this step assist in the discrimination of the sentence semantic information then the cohesion information of sentence in a document or a text stream is captured with a sentence distance matrix with each entry corresponding to the similarity between a sentence pair the distance matrix can be represented with a gray scale image thus a text segmentation problem is converted into an image segmentation problem we apply the anisotropic diffusion technique to the image representation of the distance matrix to enhance the semantic cohesion of sentence topical group a well a sharpen topical boundary at last the dynamic programming technique is adapted to find the optimal topical boundary and provide a zoom in and zoom out mechanism for topic access by segmenting text in variable number of sentence topical group our approach involves no domain specific training and it can be applied to text in a variety of domain the experimental result show that our approach is effective in text segmentation and outperforms several state of the art method 
we review a query log of hundred of million of query that constitute the total query traffic for an entire week of a general purpose commercial web search service previously query log have been studied from a single cumulative view in contrast our analysis show change in popularity and uniqueness of topically categorized query across the hour of the day we examine query traffic on an hourly basis by matching it against list of query that have been topically pre categorized by human editor this represents of the query traffic we show that query traffic from particular topical category differs both from the query stream a a whole and from other category this analysis provides valuable insight for improving retrieval effectiveness and efficiency it is also relevant to the development of enhanced query disambiguation routing and caching algorithm 
dividing web page into fragment ha been shown to provide significant benefit for both content generation and caching in order for a web site to use fragment based content generation however good method are needed for dividing web page into fragment manual fragmentation of web page is expensive error prone and unscalable this paper proposes a novel scheme to automatically detect and flag fragment that are cost effective cache unit in web site serving dynamic content we consider the fragment to be interesting if they are shared among multiple document or they have different lifetime or personalization characteristic our approach ha three unique feature first we propose a hierarchical and fragment aware model of the dynamic web page and a data structure that is compact and effective for fragment detection second we present an efficient algorithm to detect maximal fragment that are shared among multiple document third we develop a practical algorithm that effectively detects fragment based on their lifetime and personalization characteristic we evaluate the proposed scheme through a series of experiment showing the benefit and cost of the algorithm we also study the impact of adopting the fragment detected by our system on disk space utilization and network bandwidth consumption 
there have been significant advance in cross language information retrieval clir in recent year one of the major remaining reason that clir doe not perform a well a monolingual retrieval is the presence of out of vocabulary oov term previous work ha either relied on manual intervention or ha only been partially successful in solving this problem we use a method that extends earlier work in this area by augmenting this with statistical analysis and corpus based translation disambiguation to dynamically discover translation of oov term the method can be applied to both chinese english and english chinese clir correctly extracting translation of oov term from the web automatically and thus is a significant improvement on earlier work 
the heterogeneous web exacerbates ir problem and short user query make them worse the content of web document are not enough to find good answer document link information and url information compensates for the insufficiency of content information however static combination of multiple evidence may lower the retrieval performance we need different strategy to find target document according to a query type we can classify user query a three category the topic relevance task the homepage finding task and the service finding task in this paper a user query classification scheme is proposed this scheme us the difference of distribution mutual information the usage rate a anchor text and the po information for the classification after we classified a user query we apply different algorithm and information for the better result for the topic relevance task we emphasize the content information on the other hand for the homepage finding task we emphasize the link information and the url information we could get the best performance when our proposed classification method with the okapi scoring algorithm wa used 
web application are becoming increasingly popular for mobile wireless system however wireless network can have high packet loss rate which can degrade web browsing performance on wireless system an alternative approach is wireless thin client computing in which the web browser run on a remote thin server with a more reliable wired connection to the internet a mobile client then maintains a connection to the thin server to receive display update over the lossy wireless network to ass the viability of this thin client approach we compare the web browsing performance of thin client against fat client that run the web browser locally in lossy wireless network our result show that thin client can operate quite effectively over lossy network compared to fat client running web browser locally our result show surprisingly that thin client can be faster and more resilient on web application over lossy wireless lan despite having to send more data over the network we characterize and analyze different design choice in various thin client system and explain why these approach can yield superior web browsing performance in lossy wireless network 
in this poster we describe an experiment exploring the effectiveness of a pen based text input device for use in query construction standard trec query were written recognised and subsequently retrieved upon comparison between retrieval effectiveness based on the recognised writing and a typed text baseline were made on average effectiveness wa of the baseline other statistic on the quality and nature of recognition are also reported 
information filtering ha made considerable progress in recent year the predominant approach are content based method and collaborative method researcher have largely concentrated on either of the two approach since a principled unifying framework is still lacking this paper suggests that both approach can be combined under a hierarchical bayesian framework individual content based user profile are generated and collaboration between various user model is achieved via a common learned prior distribution however it turn out that a parametric distribution e g gaussian is too restrictive to describe such a common learned prior distribution we thus introduce a nonparametric common prior which is a sample generated from a dirichlet process which assumes the role of a hyper prior we describe effective mean to learn this nonparametric distribution and apply it to learn user information need the resultant algorithm is simple and understandable and offer a principled solution to combine content based filtering and collaborative filtering within our framework we are now able to interpret various existing technique from a unifying point of view finally we demonstrate the empirical success of the proposed information filtering method 
variability and diverseness among incoming request to a service hosted on a finite capacity resource necessitates sophisticated request admission control technique for providing guaranteed quality of service qos we propose in this paper a service time based online admission control methodology for maximizing profit of a service provider the proposed methodology chooses a subset of incoming request such that the revenue of the provider is maximized admission control decision in our proposed system is based upon an estimate of the service time of the request qos bound prediction of arrival and service time of request to come in the short term future and reward associated with servicing a request within it qos bound effectiveness of the proposed admission control methodology is demonstrated using experiment with a content based messaging middleware service 
previous work on understanding user web search behavior ha focused on how people search and what they are searching for but not why they are searching in this paper we describe a framework for understanding the underlying goal of user search and our experience in using the framework to manually classify query from a web search engine our analysis suggests that so called navigational search are le prevalent than generally believed while a previously unexplored resource seeking goal may account for a large fraction of web search we also illustrate how this knowledge of user search goal might be used to improve future web search engine 
in this paper we present a mobile streaming medium cdn content delivery network architecture in which content segmentation request routing pre fetch scheduling and session handoff are controlled by smil synchronized multimedia integrated language modification in this architecture mobile client simply follow modified smil file downloaded from a streaming portal server these modification enable multimedia content to be delivered to the mobile client from the best surrogate in the cdn the key component of this architecture are content segmentation with smil modification on demand rewriting of url in smil pre fetch scheduling based on timing information derived from smil smil update by soap simple object access protocol messaging for session handoff due to client mobility we also introduce qos control with a network agent called an rtp monitoring agent to enable appropriate control of medium quality based on both network congestion and radio link condition the current status of our prototyping on a mobile qos testbed mobiq is reported in this paper we are currently designing the soap based apis application programmable interface needed for the mobile streaming medium cdn and building the cdn over the current testbed 
an important objective of the semantic web is to make electronic commerce interaction more flexible and automated to achieve this standardization of ontology message content and message protocol will be necessary in this paper we investigate how semantic and web service technology can be used to support service advertisement and discovery in e commerce in particular we describe the design and implementation of a service matchmaking prototype which us a daml s based ontology and a description logic reasoner to compare ontology based service description we also present the result of initial experiment testing the performance of this prototype implementation in a realistic agent based e commerce scenario 
current e book browser provide minimal support for comprehending the organization narrative structure and theme of large complex book in order to build an understanding of such book reader should be provided with user interface that present and relate the organizational narrative and thematic structure we propose adapting information retrieval technique for the purpose of discovering these structure and sketch three distinctive visualization for presenting these structure to the e book reader these visualization are presented within an initial design for an e book browser 
topic tracking and information filtering are model of interactive task but their evaluation are generally done in a way that doe not reflect likely usage the model either force frequent judgment or disallow any at all assume the user is always available to make a judgment and do not allow for user fatigue in this study we extend the evaluation framework for topic tracking to incorporate those more realistic issue we demonstrate that tracking can be done in a realistic interactive setting with minimal impact on tracking cost and with substantial reduction in required interaction 
an information retrieval method is proposed using a hierarchical dirichlet process a a prior on the parameter of a set of multinomial distribution the resulting method naturally includes a number of feature found in other popular method specifically tf idf like term weighting and document length normalisation are recovered the new method is compared with okapi bm and the twenty one model on trec data and is shown to give better performance 
despite the connotation of the word browsing and surfing web usage often follows routine pattern of access however few mechanism exist to assist user with these routine task bookmark or portal site must be maintained manually and are insensitive to the user s browsing context to fill this void we designed and implemented the montage system a web montage is an ensemble of link and content fused into a single view such a coalesced view can be presented to the user whenever he or she open the browser or return to the start page we pose a number of hypothesis about how user would interact with such a system and test these hypothesis with a fielded user study our finding support some design decision such a using browsing context to tailor the montage raise question about others and point the way toward future work 
this paper address the problem of merging result obtained from different database and search engine in a distributed information retrieval environment the prior research on this problem either assumed the exchange of statistic necessary for normalizing score cooperative solution or is heuristic both approach have disadvantage we show that the problem in uncooperative environment is simpler when viewed a a component of a distributed ir system that us query based sampling to create resource description document sampled for creating resource description can also be used to create a sample centralized index and this index is a source of training data for adaptive result merging algorithm a variety of experiment demonstrate that this new approach is more effective than a well known alternative and that it allows query by query tuning of the result merging function 
the semantic web is vitally dependent on a formal meaning for the construct of it language for semantic web language to work well together their formal meaning must employ a common view or thesis of representation otherwise it will not be possible to reconcile document written in different language the thesis of representation underlying rdf and rdfs is particularly troublesome in this regard a it ha several unusual aspect both semantic and syntactic a more standard thesis of representation would result in the ability to reuse existing result and tool in the semantic web 
approach to increase training example to hopefully improve classification effectiveness are proposed in this work the approach were verified by use of two chinese collection classified by two top performing classifier 
we describe a language called abet that allows rapid conversion of on line human readable bilingual dictionary to machine readable form 
for the manual semantic markup of document to become wide spread usersmust be able to express annotation that conform to ontology orschemas that have shared meaning however a typical user is unlikelyto be familiar with the detail of the term a defined by the ontology author in addition the idea to be expressed may not fit perfectly within a pre defined ontology the ideal tool should help user find apartial formalization that closely follows the ontology where possiblebut deviate from the formal representation where needed we describe animplemented approach to help user create semi structured semantic annotation for a document according to an extensible owl ontology in our approach user enter a short sentence in free text to describe allor part of a document and the system present a set of potential paraphrase of the sentence that are generated from valid expression inthe ontology from which the user chooses the closest match we use a combination of off the shelf parsing tool and breadth first search of expression in the ontology to help user create valid annotation starting from free text the user can also define new term to augmentthe ontology so the potential match can improve over time 
the understanding of semantic web document is built upon ontology that define concept and relationship of data hence the correctness of ontology is vital ontology reasoner such a racer and fact have been developed to reason ontology with a high degree of automation however complex ontology related property may not be expressible within the current web ontology language consequently they may not be checkable by racer and fact we propose to use the software engineering technique and tool i e z eve and alloy analyzer to complement the ontology tool for checking semantic web document in this approach z eve is first applied to remove trivial syntax and type error of the ontology next racer is used to identify any ontological inconsistency whose origin can be traced by alloy analyzer finally z eve is used again to express complex ontology related property and reveal error beyond the modeling capability of the current web ontology language we have successfully applied this approach to checking a set of military plan ontology 
this article proposes a technique for correcting chinese ocr error to support retrieval of scanned document the technique us a completely automatic technique no manually constructed lexicon or confusion resource to identify both keywords and confusable term improved retrieval effectiveness on a single term query experiment is demonstrated 
more and more resource are becoming available on the web and there is a growing need for infrastructure that based on advertised description are able to semantically match demand with supply we formalize general property a matchmaker should have then we present a matchmaking facilitator compliant with desired property the system embeds a neoclassic reasoner whose structural subsumption algorithm ha been modified to allow match categorization into potential and partial and ranking of match within category experiment carried out show the good correspondence between user and system ranking 
in this paper we present an incremental transformation framework called incxslt this framework ha been experimented for the xslt language defined at the world wide web consortium for the currently available tool designing the xml content and the transformation sheet is an inefficient a tedious and an error prone experience incremental transformation processor such a incxslt represent a better alternative to help in the design of both the content and the transformation sheet we believe that such framework are a first step toward fully interactive transformation based authoring environment 
when processing raw document in information retrieval ir system a term weighting scheme is used to calculate the importance of each term which occurs in a document however most term weighting scheme assume that a term is independent of the other term term dependency is an indispensable consequence of language use therefore this assumption can make the information of a document being lost in this paper we propose new approach to refine term weight of document using term dependency discovered from a set of document then we evaluate our method with two experiment based on the vector space model and the language model 
this paper present an approach to bilingual lexicon extraction from comparable corpus and evaluation on cross language information retrieval we explore a bi directional extraction of bilingual terminology primarily from comparable corpus a combined statistic based and linguistics based model to select best translation candidate to phrasal translation is proposed evaluation using a large test collection for japanese english revealed the proposed combination of bi directional comparable corpus bilingual dictionary and transliteration augmented with linguistics based pruning to be highly effective in cross language information retrieval 
the goal of collaborative filtering is to make recommendation for a test user by utilizing the rating information of user who share interest similar to the test user because rating are determined not only by user interest but also the rating habit of user it is important to normalize rating of different user to the same scale in this paper we compare two different normalization strategy for user rating namely the gaussian normalization method and the decoupling normalization method particularly we incorporated these two rating normalization method into two collaborative filtering algorithm and evaluated their effectiveness on the eachmovie dataset the experiment result have shown that the decoupling method for rating normalization is more effective than the gaussian normalization method in improving the performance of collaborative filtering algorithm 
search engine need to evaluate query extremely fast a challenging task given the vast quantity of data being indexed a significant proportion of the query posed to search engine involve phrase in this paper we consider how phrase query can be efficiently supported with low disk overhead previous research ha shown that phrase query can be rapidly evaluated using nextword index but these index are twice a large a conventional inverted file we propose a combination of nextword index with inverted file a a solution to this problem our experiment show that combined use of an auxiliary nextword index and a conventional inverted file allow evaluation of phrase query in half the time required to evaluate such query with an inverted file alone and the space overhead is only of the size of the inverted file further time saving are available with only slight increase in disk requirement 
the web contains a wealth of product review but sifting through them is a daunting task ideally an opinion mining tool would process a set of search result for a given item generating a list of product attribute quality feature etc and aggregating opinion about each of them poor mixed good we begin by identifying the unique property of this problem and develop a method for automatically distinguishing between positive and negative review our classifier draw on information retrieval technique for feature extraction and scoring and the result for various metric and heuristic vary depending on the testing situation the best method work a well a or better than traditional machine learning when operating on individual sentence collected from web search performance is limited due to noise and ambiguity but in the context of a complete web based tool and aided by a simple method for grouping sentence into attribute the result are qualitatively quite useful 
delivering web page to mobile phone or personal digital assistant ha become possible with the latest wireless technology however mobile device have very small screen size and memory capacity converting web page for delivery to a mobile device is an exciting new problem in this paper we propose to use a ranking algorithm similar to google s pagerank algorithm to rank the content object within a web page this allows the extraction of only important part of web page for delivery to mobile device experiment show that the new method is effective in experiment on page from randomly selected website the system needed to extract and deliver only of the object in a web page in order to provide of a viewer s desired viewing content this provides significant saving in the wireless traffic and downloading time while providing a satisfactory reading experience on the mobile device 
today most large company maintain virtual private network vpns to connect their remote location into a single secure network vpns can be quite large covering more than location and in most case use standard internet protocol and service such vpns are implemented using a diverse set of technology such a frame relay mpls or ipsec to achieve the goal of privacy and performance isolation from the public internet using vpns to distribute live content ha recently received tremendous interest for example a vpn could be used to broadcast a ceo employee town hall meeting to distribute this type of content economically without overloading the network the deployment of streaming cache or splitter is most likely required in this paper we address the problem of optimally placing such streaming splitter or cache to broadcast to a given set of vpn endpoint under the constraint typically found within a vpn in particular we introduce an efficient algorithm with complexity o v v being the number of router in the vpn this guarantee the optimal cache placement if interception is used for redirection we prove that the general problem is np hard and introduce multiple heuristic for efficient and robust cache placement suitable under different constraint at the expense of increased implementation complexity each heuristic solution provides additional saving in the number of cache required we evaluate proposed solution using extensive simulation in particular we show our flow based solution is very close to the optimal 
this paper present a search architecture that combine classical search technique with spread activation technique applied to a semantic model of a given domain given an ontology weight are assigned to link based on certain property of the ontology so that they measure the strength of the relation spread activation technique are used to find related concept in the ontology given an initial set of concept and corresponding initial activation value these initial value are obtained from the result of classical search applied to the data associated with the concept in the ontology two test case were implemented with very positive result it wa also observed that the proposed hybrid spread activation combining the symbolic and the sub symbolic approach achieved better result when compared to each of the approach alone 
the paper study two type of event that often overload web site to a point when their service are degraded or disrupted entirely flash event fe and denial of service attack do the former are created by legitimate request and the latter contain malicious request whose goal is to subvert the normal operation of the site we study the property of both type of event with a special attention to characteristic that distinguish the two identifying these characteristic allows a formulation of a strategy for web site to quickly discard malicious request we also show that some content distribution network cdns may not provide the desired level of protection to web site against flash event we therefore propose an enhancement to cdns that offer better protection and use trace driven simulation to study the effect of our enhancement on cdns and web site 
we present a question answering qa system which learns how to detect and rank answer passage by analyzing question and their answer qa pair provided a training data we built our system in only a few person month using off the shelf component a part of speech tagger a shallow parser a lexical network and a few well known supervised learning algorithm in contrast many of the top trec qa system are large group effort using customized ontology question classifier and highly tuned ranking function our ease of deployment arises from using generic trainable algorithm that exploit simple feature extractor on qa pair with trec qa data our system achieves mean reciprocal rank mrr that compare favorably with the best score in recent year and generalizes from one corpus to another our key technique is to recover from the question fragment of what might have been posed a a structured query had a suitable schema been available comprises selector token that are likely to appear almost unchanged in an answer passage the other fragment contains question token which give clue about the answer type and are expected to be replaced in the answer passage by token which specialize or instantiate the desired answer type selector are like constant in where clause in relational query and answer type are like column name we present new algorithm for locating selector and answer type clue and using them in scoring passage with respect to a question 
this paper introduces a framework for modeling and specifying the global behavior of e service composition under this framework peer individual e service communicate through asynchronous message and each peer maintains a queue for incoming message a global watcher keep track of message a they occur we propose and study a central notion of a conversation which is a sequence of class of message observed by the watcher we consider the case where the peer are represented by mealy machine finite state machine with input and output the set of conversation exhibit unexpected behavior for example there exists a composite e service based on mealy peer whose set of conversation is not context free and not regular the set of conversation is always context sensitive one cause for this is the queuing of message we introduce an operator prepone that simulates queue delay from a global perspective and show that the set of conversation of each mealy e service is closed under prepone we illustrate that the global prepone fails to completely capture the queue delay effect and refine prepone to a local version on conversation seen by individual peer on the other hand mealy implementation of a composite e service will always generate conversation whose projection are consistent with individual e service we use projection join to reflect such situation however there are still mealy peer whose set of conversation is not the local prepone and projection join closure of any regular language therefore we propose conversation specification a a formalism to define the conversation allowed by an e service composition we give two technical result concerning the interplay between the local behavior of mealy peer and the global behavior of their composition one result show that for each regular language it local prepone and projection join closure corresponds to the set of conversation by some mealy peer effectively constructed from the second result give a condition on the shape of a composition which guarantee that the set of conversation that can be realized is the local prepone and projection join closure of a regular language 
the implicit query iq prototype is a system which automatically generates context sensitive search based on a user s current computing activity in the demo we show iq running when user are reading or composing email query are automatically generated by analyzing the email message and result are presented in a small pane adjacent to the current window to provide peripheral awareness of related information 
xml is poised to take the world wide web to the next level of innovation xml data large or small with or without associated schema will be exchanged between increasing number of application running on diverse device efficient storage and transportation of such data is an important issue we have designed a system called millau and a series of algorithm for efficient encoding and representation of xml structure in this paper we describe some of the newer algorithm and apis in our system for compression of xml structure and data our compression algorithm in addition to separating structure and text for compression take advantage of the associated schema if available in compressing the structure we also quantify xml document and their schema with the purpose of defining a decision logic to apply the appropriate compression algorithm for a document or a set of document following a particular schema our system also defines a programming model corresponding to xml document object model and simple api for xml stream and document in compressed form our experiment have shown significant performance gain of our algorithm and apis we describe some of these result in this paper we also describe some web application based on our system 
clarity in semantics and a rich formalization of this semantics are important requirement for ontology designed to be deployed in large scale open distributed system such a the envisioned semantic web this is especially important for the description of web service which should enable complex task involving multiple agent a one of the first initiative of the semantic webcommunity for describing web service owl s attracts a lot of interest even though it is still under development we identify problematic aspect of owl s and suggest enhancement through alignment to a foundational ontology another contribution of ourwork is the core ontology of service that try to fill the epistemological gap between the foundational ontology and owl s it can be reused to align other web service description language a well finally we demonstrate the applicability of our work byaligning owl s standard example called congobuy 
today there is a plethora of data accessible via the internet the web ha greatly simplified the process of searching for accessing and sharing information however a considerable amount of internet distributed data still go unnoticed and unutilized particularly in the case of frequently updated internet distributed database in this paper we give an overview of webformulate a web based visual continual query system that address the problem associated with formulating temporal ad hoc analysis over network of heterogeneous frequently updated data source the main distinction between this system and existing internet facility to retrieve information and assimilate it into computation is that webformulate provides the necessary facility to perform continual query developing and maintaining dynamic link such that web based computation and report automatically maintain themselves a further distinction is that this system is specifically designed for user of spreadsheet level ability rather than professional programmer 
hierarchy provide a mean of organizing summarizing and accessing information we describe a method for automatically generating hierarchy from small collection of text and then apply this technique to summarizing the document retrieved by a search engine 
we present a semantic web application that we callcs aktive space the application exploit a wide range of semantically heterogeneousand distributed content relating to computer science research in theuk this content is gathered on a continuous basis using a variety of method including harvesting and scraping a well a adopting a range model for content acquisition the content currently comprises aroundten million rdf triple and we have developed storage retrieval andmaintenance method to support it management the content is mediated through an ontology constructed for the application domainand incorporates component from other published ontology c aktive spacesupports the exploration of pattern and implication inherent in the content and exploit a variety of visualisation and multi dimensional representation knowledge service supported in the applicationinclude investigating community of practice who is working researching or publishing with whom this work illustrates a number ofsubstantial challenge for the semantic web these include problem of referential integrity tractable inference and interaction support wereview our approach to these issue and discus relevant related work 
web search engine employ multiple so called crawler to maintain local copy of web page but these web page are frequently updated by their owner and therefore the crawler must regularly revisit the web page to maintain the freshness of their local copy in this paper we propose a two part scheme to optimize this crawling process one goal might be the minimization of the average level of staleness over all web page and the scheme we propose can solve this problem alternatively the same basic scheme could be used to minimize a possibly more important search engine embarrassment level metric the frequency with which a client make a search engine query and then click on a returned url only to find that the result is incorrect the first part our scheme determines the nearly optimal crawling frequency a well a the theoretically optimal time to crawl each web page it doe so within an extremely general stochastic framework one which support a wide range of complex update pattern found in practice it us technique from probability theory and the theory of resource allocation problem which are highly computationally efficient crucial for practicality because the size of the problem in the web environment is immense the second part employ these crawling frequency and ideal crawl time a input and creates an optimal achievable schedule for the crawler our solution based on network flow theory is exact a well a highly efficient an analysis of the update pattern from a highly accessed and highly dynamic web site is used to gain some insight into the property of page update in practice then based on this analysis we perform a set of detailed simulation experiment to demonstrate the quality and speed of our approach 
this paper present a transaction time http server called ttapache that support document versioning a document often consists of a main file formatted in html or xml and several included file such a image and stylesheets a change to any of the file associated with a document creates a new version of that document to construct a document version history snapshot of the document s file are obtained over time transaction time are associated with each file version to record the version s lifetime the transaction time is the system time of the edit that created the version accounting for transaction time is essential to supporting audit query that delve into past document version and differential query that pinpoint difference between two version ttapache performs automatic versioning when a document is read thereby removing the burden of versioning from document author since some version may be created but never read ttapache distinguishes between known and assumed version of a document ttapache ha a simple query language to retrieve desired version a browser can request a specific version or the entire history of a document query can also rewrite link and reference to point to current or past version over time the version history of a document continually grows to free space some version can be vacuumed vacuuming a version however change the semantics of request for that version this paper present several policy for vacuuming version and strategy for accounting for vacuumed version in query 
in this paper we describe an approach to combining text and visual feature from mpeg description of video a video retrieval process is aligned to a text retrieval process based on the tf idf vector space model via clustering of low level visual feature our assumption is that shot within the same cluster are not only similar visually but also semantically to a certain extent our experiment on the trecvid and trecvid collection show that adding extra meaning to a shot based on the shot from the same cluster is useful when each video in a collection contains a high proportion of similar shot for example in documentary 
given the experimental nature of information retrieval progress critically depends on analyzing the error made by existing retrieval approach and understanding their limitation our research explores various hypothesized reason for hard topic in trec ad hoc task and show that the bad performance is partially due to the existence of highly distracting sub collection that can dominate the overall performance 
xml repository are now a widespread mean for storing and exchanging information on the web a these repository become increasingly used in dynamic application such a e commerce there is a rapidly growing need for a mechanism to incorporate reactive functionality in an xml setting event condition action eca rule are a technology from active database and are a natural method for supporting suchfunctionality eca rule can be used for activity such a automatically enforcing document constraint maintaining repository statistic and facilitating publish subscribe application an important question associated with the use of a eca rule is how to statically predict their run time behaviour in this paper we define a language for eca rule on xml repository we then investigate method for analysing the behaviour of a set of eca rule a task which ha added complexity in this xml setting compared with conventional active database 
we present a new method and system for performing the new event detection task i e in one or multiple stream of news story all story on a previously unseen new event are marked the method is based on an incremental tf idf model our extension include generation of source specific model similarity score normalization based on document specific average similarity score normalization based on source pair specific average term reweighting based on inverse event frequency and segmentation of the document we also report on extension that did not improve result the system performs very well on tdt and tdt test data and scored second in the tdt evaluation 
latent dirichlet allocation lda is a fully generative approach to language modelling which overcomes the inconsistent generative semantics of probabilistic latent semantic indexing plsi this paper show that plsi is a maximum a posteriori estimated lda model under a uniform dirichlet prior therefore the perceived shortcoming of plsi can be resolved and elucidated within the lda framework 
in peer to peer network finding the appropriate answer for an information request such a the answer to a query for rdf s data depends on selecting the right peer in the network we hereinvestigate how social metaphor can be exploited effectively andefficiently to solve this task to this end we define a method for query routing remindin that let i peer observewhich query are successfully answered by other peer ii memorizes this observation and iii subsequently us this information in order to select peer to forward request to remindin ha been implemented for the swap peer to peer platformas well a for a simulation environment we have used the simulation environment in order to investigate how successfulvariations of remindin are and how they compare to baseline strategy in term of number of message forwarded in the networkand statement appropriately retrieved 
an under explored question in cross language information retrieval clir is to what degree the performance of clir method depends on the availability of high quality translation resource for particular domain to address this issue we evaluate several competitive clir method with different training corpus on test document in the medical domain our result show severe performance degradation when using a general purpose training corpus or a commercial machine translation system systran versus a domain specific training corpus a related unexplored question is whether we can improve clir performance by systematically analyzing training resource and optimally matching them to target collection we start exploring this problem by suggesting a simple criterion for automatically matching training resource to target corpus by using cosine similarity between training and target corpus a resource weight we obtained an average of improvement over using all resource with no weight the same metric yield of the performance obtained when an oracle chooses the optimal resource every time 
question classification is very important for question answering this paper present our research work on automatic question classification through machine learning approach we have experimented with five machine learning algorithm nearest neighbor nn naive bayes nb decision tree dt sparse network of winnow snow and support vector machine svm using two kind of feature bag of word and bag of ngrams the experiment result show that with only surface text feature the svm outperforms the other four method for this task further we propose to use a special kernel function called the tree kernel to enable the svm to take advantage of the syntactic structure of question we describe how the tree kernel can be computed efficiently by dynamic programming the performance of our approach is promising when tested on the question from the trec qa track 
current approach to information retrieval rely on the creativity of individual to develop new algorithm in this investigation the use of genetic algorithm ga and genetic programming gp to learn ir algorithm is examined document structure weighting is a technique whereby different part of a document title abstract etc contribute unevenly to the overall document weight during ranking near optimal weight can be learned with a ga doing so show a statistically significant relative improvement in map for vector space inner product and croft s probabilistic ranking but no improvement for bm two application of this approach are suggested offline learning and relevance feedback in a second set of experiment a new ranking function wa learned using gp this new function yield a statistically significant relative improvement on unseen query tested on the training document portability test to different collection not used in training demonstrate the performance of the new function exceeds vector space and probability and slightly exceeds bm learning weight for this new function is proposed the application of genetic learning to stemming and thesaurus construction is discussed stemming rule such a those of the porter algorithm are candidate for gp learning whereas synonym set are candidate for ga learning 
in this poster we describe an investigation of topic similarity measure we elicit assessment on the similarity of pair of topic from subject and use these a a benchmark to ass how well each measure performs the measure have the potential to form the basis of a predictive technique for adaptive search system the result of our evaluation show that measure based on the level of correlation between topic concord most with general subject perception of search topic similarity 
this study is the first to evaluate the performance benefit of using the recently proposed tcp splice kernel service in web proxy server previous study show that splicing client and server tcp connection in the ip layer improves the throughput of proxy server like firewall and content router by reducing the data transfer overhead in a web proxy server data transfer overhead represent a relatively large fraction of the request processing overhead in particular when content is not cacheable or the proxy cache is memory based the study is conducted with a socket level implementation of tcp splice compared to ip level implementation socket level implementation make possible the splicing of connection with different tcp characteristic and improve response time by reducing recovery delay after a packet loss the experimental evaluation is focused on http request type for which the proxy can fully exploit the tcp splice service which are the request for non cacheabl content and ssl tunneling the experimental testbed includes an emulated wan environment and benchmark application for http web client web server and web proxy running on aix r machine our experiment demonstrate that tcp splice enables reduction in cpu utilization of of the cpu depending on file size and request rate larger relative reduction are observed when tunneling ssl connection in particular for small file transfer response time are also reduced by up to sec 
in recent year statistical language model are being proposed a alternative to the vector space model viewing document a language sample introduces the issue of defining a joint probability distribution over the term the present paper model a document a the result of a markov process it argues that this process is ergodic which is theoretically plausible and easy to verify in practice the theoretical result is that the joint distribution can be easily obtained this can also be applied for search resolution other than the document level we verified this in an experiment on query expansion demonstrating both the validity and the practicability of the method this hold a promise for general language model 
a technical infrastructure for storing querying and managing rdfdata is a key element in the current semantic web development system like jena sesame or the ic forth rdf suite are widelyused for building semantic web application currently none ofthese system support the integrated querying of distributed rdf repository we consider this a major shortcoming since the semanticweb is distributed by nature in this paper we present an architecture for querying distributed rdf repository by extending the existing sesame system we discus the implication of our architectureand propose an index structure a well a algorithm forquery processing and optimization in such a distributed context 
current web search engine generally impose link analysis based re ranking on web page retrieval however the same technique when applied directly to small web search such a intranet and site search cannot achieve the same performance because their link structure are different from the global web in this paper we propose an approach to constructing implicit link by mining user access pattern and then apply a modified pagerank algorithm to re rank web page for small web search our experimental result indicate that the proposed method outperforms content based method by explicit link based pagerank by and directhit by respectively 
this paper present an algorithm to generate possible variant for biomedical term the algorithm give each variant it generation probability representing it plausibility which is potentially useful for query and dictionary expansion the probabilistic rule for generating variant are automatically learned from raw text using an existing abbreviation extraction technique our method therefore requires no linguistic knowledge or labor intensive natural language resource we conducted an experiment using medline abstract for rule induction and abstract for testing the result indicate that our method will significantly increase the number of retrieved document for long biomedical term 
a a large and complex application platform the world wide web is capable of delivering a broad range of sophisticated application however many web application go through rapid development phase with extremely short turnaround time making it difficult to eliminate vulnerability here we analyze the design of web application security assessment mechanism in order to identify poor coding practice that render web application vulnerable to attack such a sql injection and cross site scripting we describe the use of a number of software testing technique including dynamic analysis black box testing fault injection and behavior monitoring and suggest mechanism for applying these technique to web application real world situation are used to test a tool we named the web application vulnerability and error scanner wave an open source project available at http wave sourceforge net and to compare it with other tool our result show that wave is a feasible platform for assessing web application security 
the world wide web is opening up access to document and data for scholar however it ha not yet impacted on one of the primary activity in research assessing new finding in the light of current knowledge and debating it with colleague the claimaker system us a directed graph model with similarity to hypertext in which new idea are published a node which other contributor can build on or challenge in a variety of way by linking to them node and link have semantic structure to facilitate the provision of specialist service for interrogating and visualizing the emerging network by way of example this paper is grounded in a claimaker model to illustrate how new claim can be described in this structured way 
this paper describes an automatic content indexing system for news program with a special emphasis on it segmentation process the process can successfully segment an entire news program into topic centered news story the primary tool is a linguistic topic segmentation algorithm experiment show that the resulting speech based segment are fairly accurate and scene change point supplied by an external video processor can be of help in improving segmentation effectiveness 
we analyzes category score algorithm for k nn classifier found in the literature including majority voting algorithm mva simple sum algorithm ssa mva and ssa are two mainly used algorithm to estimate score for candidate category in k nn classifier system based on the hypothesis that utilization of internal relation between document and category could improve system performance two new weighting score model concept based weighting cbw score model and term independence based weighting ibw score model are proposed our experimental result confirm our hypothesis and show that in the term of precision average ibw and cbw are better than the other score model while ssa is higher than mva according to macro average f cbw performs best rocchio based algorithm rba always performs worst 
many tool have been developed to help user query extract and integrate data from web page generated dynamically from database i e from the hidden web a key prerequisite for such tool is to obtain the schema of the attribute of the retrieved data in this paper we describe a system called dela which reconstructs part of a hidden back end web database it doe this by sending query through html form automatically generating regular expression wrapper to extract data object from the result page and restoring the retrieved data into an annotated labelled table the whole process need no human involvement and prof to be fast le than one minute for wrapper induction for each site and accurate over correctness for data extraction and around correctness for label assignment 
we present earchivarius an interactive system for accessing collection of electronic mail the system combine search clustering visualization and time based visualization of email message and people who send or received the message 
a new measure anchormap is introduced to evaluate how close two document retrieval ranking are to each other it is shown that anchormap score when run on a set of initial ranked document list from different system are very highly correlated with categorization of topic a easy or hard and separately are highly correlated with those topic on which blind feedback work in another experiment anchormap is used to compare the initial ranked document list from a single system against the ranked document list from that system after blind feedback again high anchormap value are highly correlated with both topic difficulty and successful application of blind feedback both experiment are example of using property of a topic which are independent of relevance information to predict the actual performance of ir system on the topic initial experiment to attempt to improve retrieval performance based upon anchormap failed the cause for failure are discussed 
current ontological specification for semantically describing property of web service are limited to their static interface description normally for proving property of service composition mapping input output parameter and specifying the pre post condition are found to be sufficient however these property are assertion only on the initial and final state of the service respectively they do not help in specifying verifying ongoing behaviour of an individual service or a composed system we propose a framework for enriching semantic service description with two compositional assertion assumption and commitment that facilitate reasoning about service composition and verification of their integration the technique is based on interval temporal logic itl a sound formalism for specifying and proving temporal property of system our approach utilizes the recently proposed semantic web rule language 
reflecting the rapid growth in the utilization of large test collection for information retrieval since the s extensive comparative experiment have been performed to explore the effectiveness of various retrieval model however most collection were intended for retrieving newspaper article and technical abstract in this paper we describe the process of producing a test collection for patent retrieval the ntcir patent retrieval collection which includes two year of japanese patent application and topic produced by professional patent searcher we also report experimental result obtained by using this collection to re examine the effectiveness of existing retrieval model in the context of patent retrieval the relative superiority among existing retrieval model did not significantly differ depending on the document genre that is patent and newspaper article issue related to patent retrieval are also discussed 
this paper address the problem of automatically structuring heterogenous document collection by using clustering method in contrast to traditional clustering we study restrictive method and ensemble based meta method that may decide to leave out some document rather than assigning them to inappropriate cluster with low confidence these technique result in higher cluster purity better overall accuracy and make unsupervised self organization more robust our comprehensive experimental study on three different real world data collection demonstrate these benefit the proposed method seem particularly suitable for automatically substructuring personal email folder or personal web directory that are populated by focused crawler and they can be combined with supervised classification technique 
collaborative filtering ha been very successful in both research and application such a information filtering and e commerce the k nearest neighbor knn method is a popular way for it realization it key technique is to find k nearest neighbor for a given user to predict his interest however this method suffers from two fundamental problem sparsity and scalability in this paper we present our solution for these two problem we adopt two technique a matrix conversion method for similarity measure and an instance selection method and then we present an improved collaborative filtering algorithm based on these two method in contrast with existing collaborative algorithm our method show it satisfactory accuracy and performance 
in the semantic web architecture web ontology language arebuilt on top of rdf s however serious difficulty have arisen when trying to layer expressive ontology language like owl on top of rdf schema although these problem can be avoided owl andthe whole semantic web architecture becomes much more complex than it should be in this paper a possible simplification of thesemantic web architecture is suggested which ha several import antadvantages with respect to the layering currently accepted by the w c ontology working group 
content based music genre classification is a fundamental component of music information retrieval system and ha been gaining importance and enjoying a growing amount of attention with the emergence of digital music on the internet currently little work ha been done on automatic music genre classification and in addition the reported classification accuracy are relatively low this paper proposes a new feature extraction method for music genre classification dwchs dwchs stand for daubechies wavelet coefficient histogram dwchs capture the local and global information of music signal simultaneously by computing histogram on their daubechies wavelet coefficient effectiveness of this new feature and of previously studied feature are compared using various machine learning classification algorithm including support vector machine and linear discriminant analysis it is demonstrated that the use of dwchs significantly improves the accuracy of music genre classification 
integrity constraint are an essential part of modern schema definition language they are useful for semantic specification update consistency control query optimization etc in this paper we propose ucm a model of integrity constraint for xml that is both simple and expressive because it relies on a single notion of key and foreign key the ucm model is easy to use and make formal reasoning possible because it relies on a powerful type system the ucm model is expressive capturing in a single framework the constraint found in relational database object oriented schema and xml document type definition we study the problem of consistency of ucm constraint the interaction between constraint and subtyping and algorithm for implementing these constraint 
we will present a novel two step fuzzy translation technique for cross lingual spelling variant in the first stage transformation rule are applied to source word to render them more similar to their target language equivalent the rule are generated automatically using translation dictionary a source data in the second stage the intermediate form obtained in the first stage are translated into a target language using fuzzy matching the effectiveness of the technique wa evaluated empirically using five source language and english a a target language the target word list contained english word with the correct equivalent for the source word among them the source word were translated using the two step fuzzy translation technique and the result were compared with those of plain fuzzy matching based translation the combined technique performed better sometimes considerably better than fuzzy matching alone 
this paper describes progress towards a general framework for incorporating multimodal cue into a trainable system for automatically annotating user defined semantic concept in broadcast video model of arbitrary concept are constructed by building classifier in a score space defined by a pre deployed set of multimodal model result show annotation for user defined concept both in and outside the pre deployed set is competitive with our best video only model on the trec video corpus an interesting side result show speech only model give performance comparable to our best video only model for detecting visual concept such a outdoors face and cityscape 
this paper report on the user centered design methodology and technique used for the elicitation of user requirement and how these requirement informed the first phase of the user interface design for a cross language information retrieval system we describe a set of factor involved in analysis of the data collected and finally discus the implication for user interface design based on the finding 
xml ha become one of the core technology for contemporary business application especially web based application to facilitate processing of diverse xml data we propose an extensible integrated xml processing architecture the xml virtual machine xvm which connects xml data with their behavior at the same time the xvm is also a framework for developing and deploying xml based application using component based technique the xvm support arbitrary granularity and provides a high degree of modularity and reusability xvm component are dynamically loaded and composed during xml data processing using the xvm both client side and server side xml application can be developed and deployed in an integrated way we also present an xml application container built on top of the xvm along with several sample application to demonstrate the applicability of the xvm framework 
a user acquire or gain access to an increasingly diverse range of web access client web application are adapting their user interface to support multiple modality on multiple client type user experience can be enhanced by client with differing capability combining to provide a distributed user interface to application indeed user will be frustrated if their interaction with application is limited to one client at a time this paper discus the requirement for coordinating web interaction across an aggregation of client we present a framework for multi device browsing that provides both coordinated navigation between web resource and coordinated interaction between variant or representation of those resource once instantiated in the client the framework protects the application from some of the complexity of client aggregation we show how a small number of enhancement to the xforms and xml event vocabulary can facilitate coordination between client and provide an appropriate level of control to application we also describe a novel proxy which consolidates http request from aggregation of client and reduces the burden that multi client browsing place on the application 
this paper introduces a rule based context dependent word clustering method with the rule derived from various domain database and the word text orthographic property besides significant dimensionality reduction our experiment show that such rule based word clustering improves by the overall accuracy of extracting bibliographic field from reference and by on average the class specific performance on the line classification of document header 
the web ha established itself a the dominant medium for doing electronic commerce realizing that it global reach provides significant market and business opportunity service provider both large and small are advertising their service on the web a number of them operate their own web site promoting their service at length while others are merely listed in a referral site aggregating all of the provider into a queriable service directory make it easy for customer to locate the one most suited for his her need yellowpager is a tool for creating service directory by mining web source service directory created by yellowpager have several merit compared to those generated by existing practice which typically require participation by service provider e g verizon s superyellowpages com firstly the information content will be rich secondly since the process is automated and repeatable the content can always be kept current finally the same process can be readily adapted to different domain yellowpager build service directory by mining the web through a combination of keyword based search engine web agent text classifier and novel extraction algorithm the extraction is driven by a service ontology consisting of a taxonomy of service concept and their associated attribute such a name and address and type description for the attribute in addition the ontology also associate an extractor function with each attribute applying the function to a web page will identify all the occurrence of the attribute in that page yellowpager s mining algorithm consists of a training step followed by classification and extraction step in the training step a classifier is trained to identify web page relevant to the service of interest the classification step proceeds by doing a search for the particular service of interest using a keyword based web search engine and retrieves all the matching web page from these page the relevant one are identified using the classifier the final step is extraction of attribute value associated with the service from these page each web page is parsed into a dom tree and the extractor function are applied all of the attribute corresponding to a service provider are then correctly aggregated this can pose difficulty especially in the presence of multiple service provider in a page using a novel concept of scoring and conflict resolution to prevent erroneous association of attribute with service provider entity in the page the algorithm aggregate all the attribute occurrence correctly the extractor function may not be complete in the sense that it cannot always identify all the attribute in a page by exploiting the regularity of the sequence in which attribute occurr in referral page the mining algorithm automatically learns generalized pattern to locate attribute that the extractor function miss the distinguishing aspect of yellowpager s extraction algorithm are i it is unsupervised and ii the attribute value in the page are extracted independent of any page specific relationship that may exist among the markup tag yellowpager ha been used by a large pet food producer to build a directory of veterinarian service provider in the united state the resulting database wa found to be much larger and richer than that found in vetquest vetworld and the super yellow page yellowpager is implemented in java and is interfaced to rainbow a library utility in c that is used for classification the tool will demonstrate the creation of a service directory for any service domain by mining web source 
indexing and ranking are two key factor for efficient and effective xml information retrieval inappropriate indexing may result in false negative and false positive and improper ranking may lead to low precision in this paper we propose a configurable xml information retrieval system in which user can configure appropriate index type for xml tag and text content based on user index configuration the system transforms xml structure into a compact tree representation ctree and index xml text content to support xml ranking we propose the concept of weighted term frequency and inverted element frequency where the weight of a term depends on it frequency and location within an xml element a well a it popularity among similar element in an xml dataset we evaluate the effectiveness of our system through extensive experiment on the inex dataset and content and structure ca topic the experimental result reveal that our system ha significantly high precision at low recall region and achieves the highest average precision a compared with official inex submission using the strict evaluation metric 
we describe a new family of topic ranking algorithm for multi labeled document the motivation for the algorithm stem from recent advance in online learning algorithm the algorithm we present are simple to implement and are time and memory efficient we evaluate the algorithm on the reuters corpus and the new corpus released by reuters in on both corpus the algorithm we present outperform adaptation to topic ranking of rocchio s algorithm and the perceptron algorithm we also outline the formal analysis of the algorithm in the mistake bound model to our knowledge this work is the first to report performance result with the entire new reuters corpus 
the main conclusion from the metric based evaluation of video retrieval system at trec s video track is that non interactive image retrieval from general collection using visual information only is not yet feasible we show how a detailed analysis of retrieval result looking beyond mean average precision map score on topical relevance give significant insight in the main problem with the visual part of the retrieval model under study such an analytical approach prof an important addition to standard evaluation measure 
web community are web virtual broadcasting space where people can freely discus anything while such community function a discussion board they have even greater value a large repository of archived information in order to unlock the value of this resource we need an effective mean for searching archived discussion thread unfortunately the technique that have proven successful for searching document collection and the web are not ideally suited to the task of searching archived community discussion in this paper we explore the problem of creating an effective ranking function to predict the most relevant message to query in community search we extract a set of predictive feature from the thread tree of newsgroup message a well a feature of message author and lexical distribution within a message thread our final result indicate that when using linear regression with this feature set our search system achieved a performance improvement compared to our baseline system 
mobile device have already been widely used to access the web however because most available web page are designed for desktop pc in mind it is inconvenient to browse these large web page on a mobile device with a small screen in this paper we propose a new browsing convention to facilitate navigation and reading on a small form factor device a web page is organized into a two level hierarchy with a thumbnail representation at the top level for providing a global view and index to a set of sub page at the bottom level for detail information a page adaptation technique is also developed to analyze the structure of an existing web page and split it into small and logically related unit that fit into the screen of a mobile device for a web page not suitable for splitting auto positioning or scrolling by block is used to assist the browsing a an alterative our experimental result show that our proposed browsing convention and developed page adaptation scheme greatly improve the user s browsing experience on a device with a small display 
this demonstration will describe how timber a native xml database system ha been extended with the capability to answer xml style structured query e g xquery with embedded ir style keyword based non boolean condition with the original structured query processing engine and the ir extension built into the system timber is well suited for efficiently and effectively processing query with both structural and textual content constraint 
this paper investigates the level of metadata accuracy required for image filter to be valuable to user access to large digital image and video collection is hampered by ambiguous and incomplete metadata attributed to imagery though improvement are constantly made in the automatic derivation of semantic feature concept such a indoor outdoor face and cityscape it is unclear how good these improvement should be and under what circumstance they are effective this paper explores the relationship between metadata accuracy and effectiveness of retrieval using an amateur photo collection documentary video and news video the accuracy of the feature classification is varied from performance typical of automated classification today to ideal performance taken from manually generated truth data result establish an accuracy threshold at which semantic feature can be useful and empirically quantify the collection size when filtering first show it effectiveness 
in traditional peer to peer search network operation focus on properly labeled file such a music or video and the actual search is often limited to text tag the explosive growth of available multimedia document in recent year call for more flexible search capability namely search by content most content based search algorithm are computationally intensive making them inappropriate for a peer to peer environment in this paper we discus a content based music retrieval algorithm that can be decomposed and parallelized efficiently we present a peer to peer architecture for such a system that make use of spare resource among subscriber with protocol that dynamically redistribute load in order to maximize throughput and minimize inconvenience to subscriber our framework can be extended beyond the music retrieval domain and adapted to other scenario where resource pooling is desired a long a the underlying algorithm satisfies certain condition 
in this paper we describe a news story gisting system that generates a word short summary of a news story this system us a machine learning technique to combine linguistic statistical and positional information in order to generate an appropriate summary we also present the result of an automatic evaluation of this system with respect to the performance of other baseline summarisers using the new rouge evaluation metric 
manually querying search engine in order to accumulate a large bodyof factual information is a tedious error prone process of piecemealsearch search engine retrieve and rank potentially relevantdocuments for human perusal but do not extract fact assessconfidence or fuse information from multiple document this paperintroduces knowitall a system that aim to automate the tedious process ofextracting large collection of fact from the web in an autonomous domain independent and scalable manner the paper describes preliminary experiment in which an instance of knowitall running for four day on a single machine wa able to automatically extract fact knowitall associate a probability with each fact enabling it to trade off precision and recall the paper analyzes knowitall s architecture and report on lesson learned for the design of large scale information extraction system 
collaborative filtering aim at learning predictive model of user preference interest or behavior from community data i e a database of available user preference in this paper we describe a new model based algorithm designed for this task which is based on a generalization of probabilistic latent semantic analysis to continuous valued response variable more specifically we assume that the observed user rating can be modeled a a mixture of user community or interest group where user may participate probabilistically in one or more group each community is characterized by a gaussian distribution on the normalized rating for each item the normalization of rating is performed in a user specific manner to account for variation in absolute shift and variance of rating experiment on the eachmovie data set show that the proposed approach compare favorably with other collaborative filtering technique 
using our question answering system question from the trec evaluation were executed over a series of web data collection with the size of the collection increasing from gigabyte up to nearly a terabyte 
there is enormous amount of multilingual document from various source and possibly from different country describing a single event or a set of related event it is desirable to construct text mining method that can compare and highlight similarity and difference of those multilingual document we discus our ongoing research that seek to model a pair of multilingual document a a weighted bipartite graph with the edge weight computed by mean of machine translation we use spectral method to identify dense subgraphs of the weighted bipartite graph which can be considered a corresponding to sentence that correlate well in textual content we illustrate our approach using english and german text 
we present a principled methodology for filtering news story by formal measure of information novelty and show how the technique can be usedto custom tailor news feed based on information that a user ha already reviewed we review method for analyzing novelty and then describe newsjunkie a system that personalizes news for user by identifying the novelty of story in the context of story they have already reviewed newsjunkie employ novelty analysis algorithm that represent article a word and named entity the algorithm analyze inter andintra document dynamic by considering how information evolves over timefrom article to article a well a within individual article we review the result of a user study undertaken to gauge the value of the approachover legacy time based review of newsfeeds and also to compare the performance of alternate distance metric that are used to estimate the dissimilarity between candidate new article and set of previously reviewed article 
this paper proposes an algorithm to hierarchically cluster document each cluster is actually a cluster of document and an associated cluster of word thus a document word co cluster note that the vector model for document creates the document word matrix of which every co cluster is a submatrix one would intuitively expect a submatrix made up of high value to be a good document cluster with the corresponding word cluster containing it most distinctive feature our algorithm look to exploit this we have defined matrix density and our algorithm basically us matrix density consideration in it working the algorithm is a partitional agglomerative algorithm the partitioning step involves the identification of dense submatrices so that the respective row set partition the row set of the complete matrix the hierarchical agglomerative step involves merging the most similar submatrices until we are down to the required number of cluster if we want a flat clustering or until we have just the single complete matrix left if we are interested in a hierarchical arrangement of document it also generates apt label for each cluster or hierarchy node the similarity measure between cluster that we use here for the merging cleverly us the fact that the cluster here are co cluster and is a key point of difference from existing agglomerative algorithm we will refer to the proposed algorithm a rpsa rowset partitioning and submatrix agglomeration we have compared it a a clustering algorithm with spherical k mean and spectral graph partitioning we have also evaluated some hierarchy generated by the algorithm 
information retrieval system evaluation is complicated by the need for manually assessed relevance judgment large manually built directory on the web open the door to new evaluation procedure by assuming that web page are the known relevant item for query that exactly match their title we use the odp open directory project and looksmart directory for system evaluation we test our approach with a sample from a log of ten million web query and show that such an evaluation is unbiased in term of the directory used stable with respect to the query set selected and correlated with a reasonably large manual evaluation 
we describe an extensible markup language xml based methodology for web data extraction that extends beyond simple screen scraping an ideal data extraction process can digest target web database that are visible only a hypertext markup language html page and create a local replica of those database a a result what is needed is more than a web crawler and set of web site wrapper a comprehensive data extraction process must deal with such obstacle a session identifier html form client side javascript incompatible datasets and vocabulary and missing and conflicting data proper data extraction also requires solid data validation and error recovery to handle data extraction failure our andes software framework help solve these problem and provides a platform for building a production quality web data extraction process key aspect of andes are that it us xml technology for data extraction including extensible html and extensible stylesheet language transformation and provides access to the deep web 
we present set an architecture for efficient search in peer to peer network building upon idea drawn from machine learning and social network theory the key idea is to arrange participating site in a topic segmented overlay topology in which most connection are short distance connecting pair of site with similar content topically focused set of site are then joined together into a single network by long distance link query are matched and routed to only the topically closest region we discus a variety of design issue and tradeoff that an implementor of set would face we show that set is efficient in network traffic and query processing load 
this paper present a new two phase pattern pp discovery technique for information extraction pp consists of orthographic pattern discovery opd and semantic pattern discovery spd where the opd determines the structural feature from an identified region of a document and the spd discovers a dominant semantic pattern for the region via inference apposition and analogy then the discovered pattern is applied back into the region to extract required data item through pattern matching we evaluated pp using data item and obtained effective result 
this paper present a systematic study of the property of a large number of web site hosted by a major isp to our knowledge ours is the first comprehensive study of a large server farm that contains thousand of commercial web site we also perform a simulation analysis to estimate potential performance benefit of content delivery network cdns for these web site we make several interesting observation about the current usage of web technology and web site performance characteristic first compared with previous client workload study the web server farm workload contains a much higher degree of uncacheable response and response that require mandatory cache validation a significant reason for this is that cookie use is prevalent among our population especially among more popular site however we found an indication of wide spread indiscriminate usage of cooky which unnecessarily impedes the use of many content delivery optimization we also found that most web site do not utilize the cache control feature ofthe http protocol resulting in suboptimal performance moreover the implicit expiration time in client cache for response is constrained by the maximum value allowed in the squid proxy finally our simulation result indicate that most web site benefit from the use of a cdn the amount of the benefit depends on site popularity and somewhat surprisingly a cdn may increase the peak to average request ratio at the origin server because the cdn can decrease the average request rate more than the peak request rate 
we propose a novel named entity matching model which considers both semantic and phonetic clue the matching is formulated a an optimization problem one major component is a phonetic matching model which exploit similarity at the phoneme level we investigate three learning algorithm for obtaining the similarity information of basic phoneme unit based on training example by applying this proposed named entity matching model we also develop a mining framework for discovering new unseen named entity translation from online daily web news this framework harvest comparable news in different language using an existing bilingual dictionary it is able to discover new name translation not found in the dictionary 
recent observation through experiment that we have performed incurrent third generation wireless network have revealed that the achieved throughput over wireless link varies widely depending on the application in particular the throughput achieved by file transfer application ftp and web browsing application http are quite different the throughput achieved over a http session is much lower than that achieved over an ftp session the reason for the lower http throughput is that the http protocol is affected by the large round trip time rtt across wireless link http transfer require multiple tcp connection and dns lookup before a http page can be displayed each tcp connection requires several rtts to fully open the tcp send window and each dns lookup requires several rtts before resolving the domain name to ip mapping these tcp dns rtts significantly degrade the performance of http over wireless link to overcome these problem we have developed session level optimization technique to enhance http download mechanism these technique a minimize the number of dns lookup over the wireless link and b minimize the number of tcp connection opened by the browser these optimization bridge the mismatch caused by wireless link between application level protocol such a http and transport level protocol such astcp our solution do not require any client side software and can be deployed transparently on a service provider network toprovide decrease in end to end user perceived latency and increase in data throughput across wireless link for http session 
mobile knowledge seeker often need access to information on the web during a meeting or on the road while away from their desktop a common practice today is to use pervasive device such a personal digital assistant or mobile phone however these device have inherent constraint e g slow communication form factor which often make information discovery task impractical in this paper we present a new focused search approach specifically oriented for the mode of work and the constraint dictated by pervasive device it combine focused search within specific topic with encapsulation of topic specific information in a persistent repository one key characteristic of these persistent repository is that their footprint is small enough to fit on local device and yet they are rich enough to support many information discovery task in disconnected mode more specifically we suggest a representation for topic specific information based on knowledge agent base that comprise all the information necessary to access information about a topic under the form of key concept and key web page and assist in the full search process from query formulation assistance to result scanning on the device itself the key contribution of our work is the coupling of focused search with encapsulated knowledge representation making information discovery from pervasive device practical a well a efficient we describe our model in detail and demonstrate it aspect through sample scenario 
this paper present the design and user evaluation of smartback a feature that complement the standard back button by enabling user to jump directly to key page in their navigation session making common navigation activity more efficient defining key page wa informed by the finding of a user study that involved detailed monitoring of web usage and analysis of web browsing in term of navigation trail the page accessible through smartback are determined automatically based on the structure of the user s navigation trail or page association with specific user s activity such a search or browsing bookmarked site we discus implementation decision and present result of a usability study in which we deployed the smartback prototype and monitored usage for a month in both corporate and home setting the result show that the feature brings qualitative improvement to the browsing experience of individual who use it 
in a federated digital library system it is too expensive to query every accessible library resource selection is the task to decide to which library a query should be routed most existing resource selection algorithm compute a library ranking in a heuristic way in contrast the decision theoretic framework dtf follows a different approach on a better theoretic foundation it computes a selection which minimises the overall cost e g retrieval quality time money of the distributed retrieval for estimating retrieval quality the recall precision function is proposed in this paper we introduce two new method the first one computes the empirical distribution of the probability of relevance from a small library sample and assumes it to be representative for the whole library the second method assumes that the indexing weight follow a normal distribution leading to a normal distribution for the document score furthermore we present the first evaluation of dtf by comparing this theoretical approach with the heuristical state of the art system cori here we find that dtf outperforms cori in most case 
