unsupervisedlearningalgorithmshavebeenderivedforseveralstatistical model of english grammar but their computational complexity make applying them to large data set intractable this paper present a probabilistic model of english grammar that is much simpler than conventional model but which admits an e cient em training algorithm the model is based upon grammatical bigram i e syntactic relationship between pair of word we present the result of experiment that quantify the representational adequacy of the grammatical bigram model it ability to generalize from labelled data and it ability to induce syntactic structure from large amount of raw text 
we introduce a new notion related to auction and mechanism design called fair imposition in this setting a center wish to fairly and efficiently allocate task among a set of agent not knowing their cost structure a in the study of auction the main obstacle to overcome is the self interest of the agent which will in general cause them to hide their true cost unlike the auction setting however here the center ha the power to impose arbitrary behavior on the agent and furthermore wish to distribute the cost a fairly a possible among them we define the problem precisely present solution criterion for these problem the central of which is called k efficiency and present both positive result in the form of concrete protocol and negative result in the form of impossibility theorem concerning these criterion 
we present an approach that significantly enhances the capability of traditional image mosaicing the key observation is that a a camera move it sens each scene point multiple time we rigidly attach to the camera an optical filter with spatially varying property so that multiple measurement are obtained for each scene point under different optical setting fusing the data captured in the multiple image yield an image mosaic that includes additional information about the scene this information can come in the form of extended dynamic range high spectral quality or enhancement to other dimension of imaging we refer to this approach a generalized mosaicing the approach wa tested using a filter with spatially varying transmittance and a standard bit black white video camera to achieve image mosaicing with dynamic range comparable to imaging with a bit camera in another experiment we attached a spatially varying spectral filter to the same camera to obtain mosaic that represent the spectral distribution rather than the usual rgb measurement of each scene point we also discus how generalized mosaicing can be used to explore other imaging dimension 
in a previous work we presented systemltree a multivariate tree that combine adecision tree with a linear discriminant bymeans of constructive induction we haveshown that it performs quite well in term ofaccuracy and learning time in comparisonwith other multivariate system like lmdt oc and cart in this work we extend theprevious work by using two new discriminantfunctions a quadratic discriminant and a logistic discriminant using the same architectureas ltree we 
dynamic texture are sequence of image of moving scene that exhibit certain stationarity property in time these include sea wave smoke foliage whirlwind but also talking face traffic scene etc we present a novel characterization of dynamic texture that pose the problem of modelling learning recognizing and synthesizing dynamic texture on a firm analytical footing we borrow tool from system identification to capture the essence of dynamic texture we do so by learning i e identifying model that are optimal in the sense of maximum likelihood or minimum prediction error variance for the special case of second order stationary process we identify the model in closed form once learned a model ha predictive power and can be used for extrapolating synthetic sequence to infinite length with negligible computational cost we present experimental evidence that within our framework even low dimensional model can capture very complex visual phenomenon 
in a number of discipline directional data provides a fundamental source of information a novel framework for isotropic and anisotropic diffusion of direction is presented in this paper the framework can be applied both to regularize directional data and to obtain multi scale representation of it the basic idea is to apply and extend result from the theory of harmonic map in liquid crystal this theory deal with the regularization of vectorial data while satisfying the unit norm constraint of directional data we show the corresponding variational and partial differential equation formulation for isotropic diffusion obtained from an l norm and edge preserving diffusion obtained from an l norm in contrast with previous approach the framework is valid for direction in any dimension support non smooth data and give both isotropic and anisotropic formulation we present a number of theoretical result open question and example for gradient vector optical flow and color image 
diagrammatic reasoning comprises phenomenon that range from the so called free ride e g almost immediate understanding of visually perceived relationship to convention about token such reasoning must involve cognitive process that are highly perceptual in content in the domain of mathematical proof where diagram have had a long history we have an opportunity to investigate in detail and in a controlled setting the various perceptual device and cognitive process that facilitate diagrammatically based argument this paper continues recent work by examining two kind of diagrammatic proof called category and by jamnik et al the first being one in which generalization of a diagram instance is implied and the second being one in which an infinite completion is represented by an ellipsis we provide explanation of why these proof work a semantics for ellipsis and conjecture about the underlying cognitive process that seem to resonate with such proof 
substantial data support a temporal difference td model of dopamine da neuron activity in which the cell provide a global error signal for reinforcement learning however in certain circumstance da activity seems anomalous under the td model responding to non rewarding stimulus we address these anomaly by suggesting that da cell multiplex information about reward bonus including sutton s exploration bonus and ng et al s non distorting shaping bonus we interpret this 
in this paper we consider the problem of cyclic schedule such a arise in manufacturing we introduce a new formulation of this problem that is a very simple modification of a standard job shop scheduling formulation and which enables u to use existing constraint reasoning technique to generate cyclic schedule we present evidence for the effectiveness of this formulation and describe extension for handling multiple capacity resource and for recovering from break in cyclic schedule 
the notion of a virtual sensor for optimal d reconstruction is introduced instead of planar perspective image that collect many ray at a fixed viewpoint omnivergent camera collect a small number of ray at many different viewpoint the resulting d manifold of ray are arranged into two multiple perspective image for stereo reconstruction we call such image omnivergent image and the process of reconstructing the scene from such image omnivergent stereo this procedure is shown to produce d scene model with minimal reconstruction error due to the fact that for any point in the d scene two ray with maximum vergence angle can be found in the omnivergent image furthermore omnivergent image are shown to have horizontal epipolar line enabling the application of traditional stereo matching algorithm without modification three type of omnivergent virtual sensor are presented spherical omnivergent camera center strip camera and dual strip camera 
market mechanism play a central role in ai a a coordination tool in multiagent system and a an application area for algorithm design mechanism where buyer are directly cleared with seller and thus do not require an external liquidity provider are highly desirable for electronic marketplace for several reason in this paper we study the inherent complexity of and design algorithm for clearing auction and reverse auction with multiple indistinguishable unit for sale we consider setting where bidder express their preference via price quantity curve and setting where the bid are price quantity pair we show that market with piecewise linear supply demand curve and non discriminatory pricing can always be cleared in polynomial time surprisingly if discriminatory pricing is used to clear the market the problem becomes complete even for step function curve if the price quantity curve are all linear then in most variant the problem admits a poly time solution even for discriminatory pricing when bidder express their preference with price quantity pair the problem is complete but solvable in pseudo polynomial time with free disposal the problem admits a poly time approximation scheme but no such approximation scheme is possible without free disposal we also present pseudo polynomial algorithm for xor bid and ofs bid and analyze the approximability 
we present a novel approach to measuring similarity between shape and exploit it for object recognition in our framework the measurement of similarity is preceded by solving for correspondence between point on the two shape using the correspondence to estimate an aligning transform in order to solve the correspondence problem we attach a descriptor the shape context to each point the shape context at a reference point capture the distribution of the remaining point relative to it thus offering a globally discriminative characterization corresponding point on two similar shape will have similar shape context enabling u to solve for correspondence a an optimal assignment problem given the point correspondence we estimate the transformation that best aligns the two shape regularized thin plate spline provide a fle xible class of transformation map for this purpose dissimilarity between two shape is computed a a sum of matching error between correspondingpoints together with a term measuring the magnitude of the aligning transform we treat recognition in a nearest neighbor classification framework result are presented for silhouette trademark handwritten digit and the coil dataset 
this paper is about new statistic and new efficientalgorithms for a form of mixture modelthat learns filamentary structure suchmodels are important in several area of scientificdata analysis but in this paper ourmain example is identification of large scalestructure among galaxy we describe softwarewhich can extract the position of sphericaland line shaped cluster from data aboutthe location of object such a galaxy wedo so by fitting a particular type of 
incremental search technique find optimal solution to ser y of similar search task much faster than is possible by solving each search task from scratch while researcher have developed incremental version of uninformed search method we develop an incremental version of a the first search of lifelong planning a is the same a that of a but all subsequent search are much faster because it reuses th ose part of the previous search tree that are identical to the new search tree we then present experimental result that demonstrate the advanta ge of lifelong planning a for simple route planning task overview artificial intelligence ha investigated knowledge based search technique that allow one to solve search task in large domain most of the research o n these method ha studied how to solve one shot search problem however search is often a repetitive process where one need to solve a series of similar search task for example because the actual situation turn out to be slightly different from the one ini tially assumed or because the situation change over time an example for route planning task are changing traffic condition thus one need to replan for the new situation for example if one always want to display the least time consuming route from the airport to the conference center on a web page in these situation most search method replan from scratch that is solve the search problem independently incremental search technique share with case based planning plan adaptation repair based planning and lea rning search control knowledge the property that they find solution to series of similar sea rch task much faster than is possible by solving each search task from scratch incremental search technique however differ from the other technique in that the quality of their solution is guaranteed to be a good a the quality of the solution obtained by replanning from scratch although incremental search method are not widely known in artificial intelligence and control different researcher have developed incrementa l search version of uninformed search method in the algorithm literature an overview ca n be found in fmsn we on the other hand develop an incremental version of a thus combining idea from the algorithm literature and the artificial intelligence l iterature we call the algorithm lifelong planning a lpa in analogy to lifelong learning thr because it reuses 
we propose a method to gradually expand the move to consider at the node of game search tree the algorithm begin with an iterative deepening search using the minimal set of move and if the search doe not succeed iteratively widens the set of possible move performing a complete iterative deepening search after each widening when d esigning carefully the different set of possible move the a lgorithm can save some time in the game of go tree search a shown by experimental result 
it ha been shown that the receptive field of simple cell in v can be explained by assuming optimal encoding provided that an extra constraint of sparseness is added this finding suggests that there is a r eason independent of optimal representation for sparseness however this work used an ad hoc model for the noise here i show that if a biologically more plausible noise model describing neuron a poisson process is used sparseness doe not have to be added a a constraint thus i conclude that sparseness is not a feature that evolution ha str iven for but is simply the result of the evolutionary pressure towards an op timal representation 
i show that the equational treatment of ellipsis proposed in dalrymple et al can further be viewed a modeling the effect of parallelism on semantic interpretation i illustrate this claim by showing that the account straightforwardly extends to a general treatment of sloppy identity on the one hand and to deaccented focus on the other i also briefly discus the result obtained in a prototype implementation 
the mapping between d body pose and d shadow is fundamentally many to many and defeat regression method even with windowed context we show how to learn a function between path in the two system resolving ambiguity by integrating information over the entire length of a sequence the basis of this function is a configurable and dynamical manifold that summarizes the target system s behavior this manifold can be modeled from data with a hidden markov model having special topological property that we obtain via entropy minimization inference is then a matter of solving for the geodesic on the manifold that best explains the evidence in the cue sequence we give a closed form maximum a posteriori solution for geodesic through the learned density space thereby obtaining optimal path over the dynamical manifold these method give a completely general way to perform inference over time series in vision they support analysis recognition classification and synthesis of behavior in linear time we demonstrate with a prototype that infers d from monocular monochromatic sequence e g back subtraction without using any articulatory body model the framework readily accommodates multiple camera and other source of evidence such a optical flow or feature tracking 
the shape of an object may be estimated by observing the shadow on it surface we present a method that is robust with respect to a conservative classification of shadow region assuming that a conservative estimate of the object shape is available we analyze image of the object illuminated with known point light source taken from known camera location we adjust our surface estimate using the shadow region to produce a refinement that is still a conservative estimate a proof of correctness is provided no assumption about the object topology are made although any tangent plane discontinuity over the object s surface are supposed to be detectable an implementation and some experimental result are presented 
recommender system have been revolutionizing the way shopper and information seeker find what they want we will study some of the tremendous success and spectacular failure of recommenders in e commerce to understand the cause of the success or failure we will leverage that understanding into a set of principle for successfully applying recommenders to business problem finally we will study the economic and social force that are shaping the evolution of recommenders and peer into the crystal ball to glimpse the direction the technology will be going in the future 
we introduce a new type of self organizing map som to navigate in the semantic space of large text collection we propose a hyperbolic som hsom based on a regular tesselation of the hyperbolic plane which is a non euclidean space characterized by constant negative gaussian curvature the exponentially increasing size of a neighborhood around a point in hyperbolic space provides more freedom to map the complex information space arising from language into spatial relation we describe experiment showing that the hsom can successfully be applied to text categorization task and yield result comparable to other state of the art method 
we propose a bayesian framework for representing and recognizing local image motion in term of two primitive model translation and motion discontinuity motion discontinuity are represented using a non linear generative model that explicitly encodes the orientation of the boundary the velocity on either side the motion of the occluding edge over time and the appearance disappearance of pixel at the boundary we represent the posterior distribution over the model parameter given the image data using discrete sample this distribution is propagated over time using the condensation algorithm to efficiently represent such a high dimensional space we initialize sample using the response of a low level motion discontinuity detector 
we introduce a novel distributional clustering algorithm t hat explicitly maximizes the mutual information per cluster between the data and given category this algorithm can be considered a a bottom up hard version of the recently introduced information bottleneck method we relate the mutual information between cluster and category to the bayesian classification error which provides another motivation for using the obtained cluster s a feature the algorithm is compared with the top down soft version of the information bottleneck method and a relationship between the hard and soft result is established we demonstrate the alg orithm on the newsgroupsdata set for a subset of two news group we achieve compression by order of magnitude loosing only of the original mutual information 
quantitative data on the speed with which animal acquire behavioral response during classical conditioning experiment should provide strong constraint on model of learning however most model have simply ignored these data the few that have attempted to address them have failed by at least an order of magnitude we discus key data on the speed of acquisition and show how to account for them using a statistically sound model of learning in which differential reliability of stimulus play a crucial role 
in this article we propose and analyze a class of actor critic algorithm these are two time scale algorithm in which the critic us temporal difference learning with a linearly parameterized approximation architecture and the actor is updated in an approximate gradient direction based on information provided by the critic we show that the feature for the critic should ideally span a subspace prescribed by the choice of parameterization of the actor we study actor critic algorithm for markov decision process with polish state and action space we state and prove two result regarding their convergence 
a model for the formation of situation concept is described a characteristic of this form of concept formation is that it doe not require instructive feedback this render it suitable for concept formation by autonomous agent it is experimentally demonstrated that situation concept constructed independently by several agent can convey useful information between agent through a learned system of communication a relation wa found between the development of the learned system of communication and the duration of the situation 
intelligent scissors and intelligent paint are complemen tary interactive image segmentation tool that allow a user to quickly and accurately select object of interest this demonstration provides a mean for participant to experi ence the dynamic nature of these tool introduction 
many visual matching algorithm can be describedin term of the feature and the inter feature distanceor metric the most commonly used metric is thesum of squared difference ssd which is valid froma maximum likelihood perspective when the real noisedistribution is gaussian based on real noise distributionsmeasured from international test set we havefound experimentally that the gaussian noise distributionassumption is often invalid this implies thatother metric which have 
we describe data mining technique designed to address the problem of selecting prospective customer from a large pool of candidate these technique cover a number of different scenario namely whether the marketing researcher have demographic information on the current customer or the general market population or people with propensity to become customer we also present a novel approach to the problem by exploiting the availability of a data sample from the general market population finally we describe an on line lead management and delivery system that us the mining approach described in this paper for insurance agent to obtain qualified customer lead 
this article present a phase computational learning model and application a a demonstration a system ha been built called chime for computer human interacting musical entity in phase of training recurrent back propagation train the machine to reproduce jazz melody the recurrent network is expanded and is further trained in phase with a reinforcement learning algorithm and a critique produced by a set of basic rule for jazz improvisation after each phase chime can interactively improvise with a human in real time 
variation in illumination can have a dramatic effect on the appearance of an object in an image in this paper we propose how to deal with illumination variation in eigenspace method we demonstrate that the eigenimages obtained by a training set under a single illumination condition ambient light can be used for recognition of object taken under different illumination condition the major idea is to incorporate a set of gradient based lter bank into the eigenspace recognition framework this can be achieved since the eigenimage coeficients are invariant for linearly ltered image input and eigenimages to achieve further illumination insensitivity we devised a robust procedure for coeficient recovery the proposed approach ha been extensively evaluated on a set of image and the result were compared to other approach 
in many application large volume of time sensitive textual information require triage rapid approximate prioritization for subsequent action in this paper we explore the use of prospective indication of the importance of a time sensitive document for the purpose of producing better document filtering or ranking by prospective we mean importance that could be assessed by action that occur in the future for example a news story may be assessed retrospectively a being important based on event that occurred after the story appeared such a a stock price plummeting or the issuance of many follow up story if a system could anticipate prospectively such occurrence it could provide a timely indication of importance clearly perfect prescience is impossible however sometimes there is sufficient correlation between the content of an information item and the event that occur subsequently we describe a process for creating and evaluating approximate information triage procedure that are based on prospective indication unlike many information retrieval application for which document labeling is a laborious manual process for many prospective criterion it is possible to build very large labeled training corpus automatically such corpus can be used to train text classification procedure that will predict the prospective importance of each document this paper illustrates the process with two case study demonstrating the ability to predict whether a news story will be followed by many very similar news story and also whether the stock price of one or more company associated with a news story will move significantly following the appearance of that story we conclude by discussing how the comprehensibility of the learned classifier can be critical to success 
abstract a framework is developed for the design and analysis of single viewpoint catadioptric camera that use two or more mirror the use of multiple mirror permit folding of the optic which lead to more compact camera design than one that use a single mirror a dictionary of camera design that use two conic mirror is presented we show that any folded system that us conic mirror ha a geo metrically equivalent system that us a single conic mir ror this result make it easy to determine the scene to image mapping of a conic folded system in addition we discus the optical benefit of using folded system a an example we choose a camera design from our dictio nary and optimize it parameter via optical simulation this design is used to construct a compact video camera that provides a hemispherical field of view 
having access to massive amount of data doe not necessarily imply that inductionalgorithms must use them all sample often provide the same accuracy with far lesscomputational cost however the correct sample size is rarely obvious we analyzemethods for progressive sampling starting with small sample and progressively increasingthem a long a model accuracy improves we show that a simple geometricsampling schedule is efficient in an asymptotic sense we then explore the notion 
traditional theory of child language acquisition center around theexistence of a language acquisition device which is specifically tunedfor learning a particular class of language more recent proposalssuggest that language acquisition is assisted by the evolution oflanguages towards form that are easily learnable in this paper we evolve combinatorial language which can be learned by a simplerecurrent network quickly and from relatively few example additionally we evolve 
many imaging system seek a good interpretation of the scene presented i e a plausible perhaps optimal mapping from aspect of the scene to real world object this paper address the issue of nding such likely mapping efficiently in general an interpretation policy specie when to apply which imaging operator which can range from low level edge detector and region grower through highlevel token combination rule and expectation driven objectdetectors given the cost of these operator and the distribution of possible image we can determine both the expected cost and expected accuracy of any such policy our task is to nd a maximally effective policy typically one with sufcient accuracy whose cost is minimal we explore this framework in several context including the eigenface approach to face recognition our result show in particular that policy which select the operator that maximize information gain per unit cost work more effectively than other policy including one that at each stage simply try to establish the putative most likely interpretation 
i present a simple variation of importance sampling that explicitly search for important region in the target distribution i prove that the technique yield unbiased estimate and show empirically it can reduce the variance of standard monte carlo estimator this is achieved by concentrating sample in more significant region of the sample space 
belief propagation bp wa only supposed to work for tree like network but work surprisingly well in many application involving network with loop including turbo code however there ha been little understanding of the algorithm or the nature of the solution it find for general graph we show that bp can only converge to a stationary point of an approximate free energy known a the bethe free energy in statistical physic this result characterizes bp fix ed point and make connection with variational approach to approximate inference more importantly our analysis let u build on the progress made in statistical physic since bethe s approximation wa introduced in kikuchi and others have shown how to construct more accurate free energy approximation of which bethe s approximation is the simplest exploiting the insight from our analysis we derive generalized belief propagation gbp version of these kikuchi approximation these new message passing algorithm can be significantly more accurate than ordinary bp at an adjustable increase in complexity we illustrate such a new gbp algorithm on a grid markov network and show that it give much more accurate marginal probability than those found using ordinary bp 
extracting rule from rbfs is not a trivial task because of nonlinear function or high input dimensionality in such case some of the hidden unit of the rbf network have a tendency to be shared across several output class or even may not contribute 
factorization using singular value decomposition svd is often used for recovering d shape and motion from feature correspondence across multiple view svd is powerful at finding the global solution to the associated least square error minimization problem however this is the correct error to minimize only when the x and y positional error in the feature are uncorrelated and identically distributed but this is rarely the case in real data uncertainty in feature position depends on the underlying spatial intensity structure in the image which ha strong directionality to it hence the proper measure to minimize is covariance weighted squared error or the mahalanobis distance in this paper we describe a new approach to covariance weighted factorization which can factor noisy feature correspondence with high degree of directional uncertainty into structure and motion our approach is based on transforming the raw data into a covariance weighted data space where the component of noise in the different direction are uncorrelated and identically distributed applying svd to the transformed data now minimizes a meaningful objective function we empirically show that our new algorithm give good result for varying degree of directional uncertainty in particular we show that unlike other svd based factorization algorithm our method doe not degrade with increase in directionality of uncertainty even in the extreme when only normal flow data is available it thus provides a unified approach for treating corner like point together with point along linear structure in the image 
in this paper we present a novel approach to robust skeleton extraction we use undirected graph to model connectivity of the skeleton point the graph topology remains unchanged throughout the skeleton computation which greatly reduces sensitivity of the skeleton to noise in the shape outline furthermore this representation naturally defines an ordering of the point along the skeleton the process of skeleton extraction can be formulated a energy minimization in this framework we provide an iterative snake like algorithm for the skeleton estimation using distance transform fixed topology skeleton are useful if the global shape of the object is known ahead of time such a for people silhouette hand outline medical structure image of letter and digit small change in the object outline should be either ignored or detected and analyzed but they do not change the general structure of the underlying skeleton example application include tracking object recognition and shape analysis 
for many application it is important that agent be not only correct but also comprehensible to human user typically people have tried to make agent behavior and reasoning understandable by adding post hoc special purpose explanation system with often disappointing result here i instead take the comprehensibility of agent behavior a a central agent design consideration from the start i describe an agent architecture the expressivator that support comprehensibility on top of a behavior based framework using four technical innovation structuring the agent s behavior according to the sign and signifier it is intended to communicate allowing the agent to keep track of it impression on the user with sign management using behavioral transition to explain the reason for agent behavior and expressing behavioral interrelationship directly using meta level control 
we consider a group of bayesian learner whose interaction with the environment and other agent allow them to improve their model of the dependency among various factor that have influence on their interaction with the environment effective collaboration can improve the performance of isolated individual learner we present a mechanism to pool together the knowledge of many modeler in the domain each of whom may have only partial access to the environment the application domain used in 
we present a formal characterization and semantic representation for a number of credulous inference relation based on the notion of an epistemic state it is shown in particular that credulous inference can be naturally represented in term of expectation see gardenfors and makinson we describe also the relationship between credulous and usual skeptical nonmonotonic inference and show how they can facilitate each other 
we have assembled a standalone movable system that can capture long sequence of omnidirectional image up to image at hz and a resolution of the goal of this system is to reconstruct complex large environment such a an entire floor of a building from the captured image only in this paper we address the important issue of how to calibrate such a system our method us image of the environment to calibrate the camera without the use of any special calibration pattern knowledge of camera motion or knowledge of scene geometry it us the consistency of pairwise tracked point feature across a sequence based on the characteristic of catadioptric imaging we also show how the projection equation for this catadioptric camera can be formulated to be equivalent to that of a typical rectilinear perspective camera with just a simple transformation 
catadioptric system are realization of omnidirectional vision through mirror lens combination design preserving the uniqueness of an effective viewpoint have recently gained attraction we present here a novel approach for estimating the intrinsic parameter of a well known catadioptric system consisting of a paraboloid mirror and an orthographic lens we introduce the geometry of catadioptric line projection and we show that the vanishing point lie on a conic section which encodes the entire calibration information projection of two set of parallel line suffice for intrinsic calibration from one view a well a for metric rectification of a plane our approach overcomes limitation of existing manual calibration method and wa successfully tested on the task of back warping real image image onto virtual plane 
in this work a new approach to fully automatic color image segmentation called jseg is presented first color in the image are quantized to several representing class that can be used to differentiate region in the image then image pixel color are replaced by their corresponding color class label thus forming a class map of the image a criterion for good segmentation using this class map is proposed applying the criterion to local window in the class map result in the j image in which high and low value correspond to possible region boundary and region center respectively a region growing method is then used to segment the image based on the multi scale j image experiment show that jseg provides good segmentation result on a variety of image 
determining and setting maximal revenue expectation or other business performance target whether it is for regional company division or individual customer can have profound financial implication operational technique are changed staffing level are altered and management attention is re focused all in the name of expectation in practice these expectation are often derived in an ad hoc manner to address this unsupervised task we combine nearest neighbor method and classical statistical method and derive a new solution to the classical econometric task of frontier analysis we apply our methodology to two real world business problem in verizon a major telecommunication provider in the united state more specifically in the print yellow page division verizon information service identifying under marketed customer for targeted upselling campaign and focused sale attention and benchmarking regional directory division to incent performance improvement our analysis uncovers some commercially useful aspect of these domain and by conservative estimate can increase revenue by several million dollar in each domain 
we present an extensive experimental study of consequence finding algorithm based on kernel resolution using both a trie based and a novel zbdd based implementation which us zerosuppressed binary decision diagram to concisely store and process very large clause set our study considers both the full prime implicate task and application of consequence finding for restricted target language in abduction model based and faulttree diagnosis and polynomially bounded knowledge compilation we show that the zbdd implementation can push consequence finding to a new limit solving problem which generate over clause 
many interesting problem such a powergrids network switch and traffic flow thatare candidate for solving with reinforcementlearning rl also have property that makedistributed solution desirable we proposean algorithm for distributed reinforcementlearning based on distributing the representationof the value function across node eachnode in the system only ha the ability tosense state locally choose action locally andreceive reward locally the goal of 
in this paper we use a logic based system description for a simple non logic functional language to examine the way in which a diagnosis system can use it system description to improve debugging performance the key concept is that the notion of expression replacement which is the basis for repairing a program can also serve a a fundamental heuristic for searching the source of an error we formally define replacement in term of fault mode explicitly define a replacement order and use the replacement heuristic for finding diagnosis finally we incorporate the use of multiple test case and discus their use in discriminating between diagnosis 
we introduce a finite difference expansion for closely space d camera in projective vision and use it to derive differential analogue of the finite displacement projective matching tensor and constraint the result are simpler more general and easier t o use than astrom heyden s time derivative based continuous time matching constraint we suggest how to use the formalism f or tensor tracking propagation of matching relation agai nst a fixed base image along an image sequence we relate this to non linear tensor estimator and show how unwrapping the optimization loop along the sequence allows simple linear point update estimate to converge rapidly to statistically near optim al nearconsistent tensor estimate a the sequence proceeds we also give guideline a to when difference expansion is likely to be worthwhile a compared to a discrete approach 
much work in computer science ha adopted competitive analysis a a tool for decision making under uncertainty in this work we extend competitive analysis to the context of multi agent system unlike classical competitive analysis where the behavior of an agent s environment is taken to be arbitrary we consider the case where an agent s environment consists of other agent these agent will usually obey some minimal rationality constraint this lead to the definition of rational competitive analysis we introduce the concept of rational competitive analysis and initiate the study of competitive analysis for multi agent system we also discus the application of rational competitive analysis to the context of bidding game a well a to the classical one way trading problem 
we present a method for the recognition of walking people in monocular image sequence based on the extraction of coordinate of specific point location on the body the method work by a comparison of sequence of recorded coordinate with a library of sequence from different individual the comparison is based on the evaluation of view invariant and calibration independent view consistency constraint these constraint are function of corresponding image coordinate in two view and are satisfied whenever the two view are projected from the same three dimensional d object by evaluating the view consistency constraint for each pair of frame in a sequence of a walking person and a stored sequence we obtain a matrix of consistency value that ideally are zero whenever the pair of image depict the same d posture the method is virtually parameter free and computes a consistency residual between a pair of sequence that can be used a a distance for clustering and classification using interactively extracted data we present experimental result that are superior to those of previously published algorithm both in term of performance and generality key word structure from motion calibration object recognition 
in theory the winnow multiplicative update ha certain advantage over the perceptron additive update when there are many irrelevant attribute recently there ha been much effort on enhancing the perceptron algorithm by using regularization leading to a class of linear c lassification method called support vector machine similarly it is al so possible to apply the regularization idea to the winnow algorithm which give method we call regularized winnow we show that the resulting method compare with the basic winnow in a similar way that a support vector machine compare with the perceptron we investigate algorithmic issue and learning property of the derived method some experimental result will also be provided to illustrate different metho d 
a simple algorithm is described that computes the radiometric response function of an imaging system from image of an arbitrary scene taken using different exposure the exposure is varied by changing either the aperture setting or the shutter speed the algorithm doe not require precise estimate of the exposure used rough estimate of the ratio of the exposure e g fnumber setting on an inexpensive lens are sufficient for accurate recovery of the response function a well a the actual exposure ratio the computed response function is used to fuse the multiple image into a single high dynamic range radiance image robustness is tested using a variety of scene and camera a well a noisy synthetic image generated using randomly selected response curve automatic rejection of image area that have large vignetting effect or temporal scene variation make the algorithm applicable to not just photographic but also video camera code for the algorithm and several result are publicly available at http www c columbia edu cave 
this paper present an introduction to reinforcement learning and relational reinforcement learning at a level to be understood by student and researcher with different background it give an overview of the fundamental principle and technique of reinforcement learning without involving a rigorous deduction of the mathematics involved through the use of an example application then relational reinforcement learning is presented a a combination of reinforcement learning with relational learning it advantage such a the possibility of using structural representation making abstraction from specific goal pursued and exploiting the result of previous learning phase are discussed 
extending a dierential total least square method for range flow estimation we present an iterative regularisation approach to compute dense range flow field we demonstrate how this algorithm can be used to detect motion discontinuity this can can be used to segment the data into independently moving region the dierent type of aperture problem encountered are discussed our regularisation scheme then take the various type of flow vector and combine them into a smooth flow field within the previously segmented region a quantitative performance analysis is presented on both synthetic and real data the proposed algorithm is also applied to range data from castor oil plant obtained with the biris laser range sensor to study the d motion of plant leaf 
we rst show how to represent sharp posterior probability distribu tions using real valued coe cients on broadly tuned basis function then we show how the precise time of spike can be used to con vey the real valued coe cients on the basis function quickly and accurately finally we describe a simple simulation in which spik ing neuron learn to model an image sequence by tting a dynamic generative model 
shopbots are agent that automatically search the internet to obtain information about price and other attribute of good and service they herald a future in which autonomous agent profoundly influence electronic market in this study a simple economic model is proposed and analyzed which is intended to quan tify some of the likely impact of a proliferation of shopbots and other economically motivated software agent in addition this paper report on simulation of pricebots adaptive pricesetting agent which firm may well implement to combat or even take advantage of the grow ing community of shopbots this study form part of a larger research program that aim to provide insight into the impact of agent tech nology on the nascent information economy 
context is used in many nlp system a an indicator of a term s syntactic and semantic function the accuracy of the system is dependent on the quality and quantity of contextual information available to describe each term however the quantity variable is no longer fixed by limited corpus resource given fixed training time and computational resource it make sense for system to invest time in extracting high quality contextual information from a fixed corpus however with an effectively limitless quantity of text available extraction rate and representation size need to be considered we use thesaurus extraction with a range of context extracting tool to demonstrate the interaction between context quantity time and size on a corpus of million word 
the structured policy iteration spi algorithm boutilier et al construct structured solution to markov decision problem mdps it find the optimal value function by treating all state which have the same value under a particular policy a a single aggregate state here we show that this approach can also be applied locally in a structured version of prioritized sweeping a model based reinforcement learning algorithm that attempt to focus the learning agent s limited computational resource on state where the model is most likely to be incorrect in structured prioritized sweeping we maintain a structured value function and update the value of aggregate state this increase the scope of local value function change and the rate at which they propagate across the state space which make learning more efficient particularly in larger problem 
we introduce two mechanism for scaling computation in the framework of temporal reasoning the first one address abstraction at the methodological level operator are defined that engender flexible switching between different granularity of temporal representation structure the second one account for abstraction at the interface level of a temporal reasoning engine various generalization of temporal relation are introduced that approximate more fine grained representation by abstracting away irrelevant detail 
we describe how d affine measurement may be computed from a single perspective view of a scene given only minimal geometric information determined from the image this minimal information is typically the vanishing line of a reference plane and a vanishing point for a direction not parallel to the plane it is shown that affine scene structure may then be determined from the image without knowledge of the camera s internal calibration e g focal length nor of the explicit relation between camera and world pose in particular we show how to i compute the distance between plane parallel to the reference plane up to a common scale factor semi ii compute area and length ratio on any plane parallel to the reference plane semi iii determine the camera s location simple geometric derivation are given for these result we also develop an algebraic representation which unifies the three type of measurement and amongst other advantage permit a first order error propagation analysis to be performed associating an uncertainty with each measurement we demonstrate the technique for a variety of application including height measurement in forensic image and d graphical modelling from single image 
integrating the splitting rule into a saturation based theorem prover may be highly beneficial for solving certain class of fist order problem the use of splitting in the context of saturation based theorem proving based on explicit case analysis a implemented in spa employ backtracking which is difficult to implement a it affect design of the whole system here we present a quot cheap quot and efficient technique for implementing splitting that doe not use backtracking 
many domain are naturally organized in an abstraction hierarchy or taxonomy where the instance in nearby class in the taxonomy are similar in this paper we provide a general probabilistic framework for clust ering data into a set of class organized a a taxonomy where each class is associated with a probabilistic model from which the data wa generated the clustering algorithm simultaneously optimizes three thing the assignment of data instance to cluster the model associated with the cluster and the struc ture of the abstraction hierarchy a unique feature of our approach is that it utiliz e global optimization algorithm for both of the last two step reducing the sensi tivity to noise and the propensity to local maximum that are characteristic of algor ithms such a hierarchical agglomerative clustering that only take local step we provide a theoretical analysis for our algorithm showing that it converges to a lo cal maximum of the joint likelihood of model and data we present experimental result on synthetic data and on real data in the domain of gene expression and text 
we develop a pairwise classification framework for face recognition in which a c class face recognition problem is divided into a set of c c two class problem such a problem decomposition not only lead to a set of simpler classification problem to be solved thereby increasing overall classification accuracy but also provides a framework for independent feature selection for each pair of class a simple feature ranking strategy is used to select a small subset of the feature for each pair of class furthermore we evaluate two classification method under the pairwise comparison framework the bayes classifier and the adaboost experiment on a large face database with face image of individual indicate that feature are enough to achieve a relatively high recognition accuracy which demonstrates the effectiveness of the pairwise recognition framework 
classification and regression are fundamental data mining technique the goal of such technique is to build predictor based on a training dataset and use them to predict the property of new data for a wide range of technique combining predictor built on sample from the training dataset provides lower error rate faster construction or both than a predictor built from the entire training dataset this provides a natural parallelization strategy in which predictor based on sample are built independently and hence concurrently we discus the performance implication for two subclass those in which predictor are independent and those in which knowing a set of predictor reduces the difficulty of finding a new one 
we present a simple procedure for synthesising novel view using two or more basis image a input it is possible for the user to interactively adjust the viewpoint and for the corresponding image to be computed and rendered in real time rather than employing a d model our method is based on the linear relation which exist between image taken with an affine camera we show how the combination of view proposed by ullman and basri can be appropriately parameterised when a sequence of five or more image is available this is achieved by fitting polynomial model to the coefficient of the combination where the latter are function of the unknown camera parameter we discus an alternative approach direct image interpolation and argue that our method is preferable when there is a large difference in orientation between the original gaze direction we show the result of applying the parameterisation to a fixating camera using both simulated and real input our observation are relevant to several application including visualisation animation and low bandwidth communication 
we suggest a nonparametric framework for unsupervised learning of projection model in term of density estimation on quantized sample space the objective is not to optimally reconstruct the data but instead the quantizer is chosen to optimally reconstruct the density of the data for the resulting quantizing density estimator qde we present a general method for parameter estimation and model selection we show how projection set which correspond to traditional unsupervised method like vector quantization or pca appear in the new framework for a principal component quantizer we present result on synthetic and realworld data which show that the qde can improve the generalization of the kernel density estimator although it estimate is based on significantly lower dimensional projection index of the data 
a theory of shape is important for ai both for recognition and description of object and for reasoning about the possible behaviour of ob jects theory of shape may be loosely clas sified a either volume based or outline based we present a theory of the latter type ini tially confined to two dimensional outline we represent outline by mean of string over an alphabet of seven qualitative curvature type and give a regular grammar which generates the string corresponding to possible outline we use subset of the curvature type alphabet to characterise cognitively salient subclass of outline with corresponding regular subgrammars and use decusping smoothing and merg ing operator to simplify outline for represen tation at coarser granularity we give an algo rithm for deriving the curvature sequence of an outline using only local information obtained a the outline is traversed finally we indicate how more detailed including quantitative in formation can be incorporated into the theory 
story generation is experiencing a revival despite disappointing preliminary result from the preceding three decade one of the principle reason for previous inadequacy wa the low level of writing quality which resulted from the excessive focus of story grammar on plot design although these system leveraged narrative theory via corpus analysis they failed to thoroughly extend those analysis to all relevant linguistic level the end result wa narrative that were recognizable a story but whose prose quality wa unsatisfactory however the blame for poor writing quality cannot be laid squarely at the foot of story grammar a natural language generation ha to date not fielded system capable of faithfully reproducing either the variety or complexity of naturally occurring story this paper present the author architecture for accomplishing precisely that task the storybook implementation of a narrative prose generator and a brief description of a formal evaluation of the story it produce 
we address the problem of integrating document from different source into a master catalog this problem is pervasive in web marketplace and portal current technology for automating this process consists of building a classifier that us the categorization of document in the master catalog to construct a model for predicting the category of unknown document our key insight is that many of the data source have their own categorization and classification accuracy can be improved by factoring in the implicit information in these source categorization we show how a naive bayes classification can be enhanced to incorporate the similarity information present in source catalog our analysis and empirical evaluation show substantial improvement in the accuracy of catalog integration 
we present a corpus based study of the sequential ordering among premodifiers in noun phrase this information is important for the fluency of generated text in practical application we propose and evaluate three approach to identify sequential order among premodifiers direct evidence transitive closure and clustering our implemented system can make over of such ordering decision correctly a evaluated on a large previously unseen test corpus 
ockham s razor is a powerful rule that ha impregnated western logical method of challenging problem and ha fostered the development of science in this essay i discus briefly the historical origin of this epistemological rule during the middle age i suggest that ockham s razor may represent a cognitive ability which ha been applied by several thinker since before medieval time furthermore i suggest that the application of ockham s razor ha had an important role both in the development and rejection of new revolutionary theory 
these are node type univariate v multivariate branching factor two or more grouping of class into two if the tree is binary error impurity measure and the method for minimization to find the best split vector and threshold we then propose a new decision tree induction method that we name linear discriminant tree ldt which us the best combination of these criterion in term of accuracy simplicity and learning time this tree induction method can be univariate or multivariate the method ha a supervised outer optimization layer for converting a k class problem into a sequence of two class problem and each two class problem is solved analytically using fisher s linear discriminant analysis lda on twenty datasets from the uci repository we compare the linear discriminant tree with the univariate decision tree method c and c multivariate decision tree method cart oc quest neural tree and lmdt our proposed linear discriminant tree learn fast are accurate and the tree generated are small 
we introduce two abstraction mechanism for streamlining the process of semantic interpretation configurational description of dependency graph increase the linguistic generality of interpretation schema while interfacing them to lexical and conceptual inheritance hierarchy reduces the amount and complexity of semantic specification 
kernel method like support vector machine have successfully been used for text categorization a standard choice of kernel function ha been the inner product between the vector space representation of two document in analogy with classical information retrieval ir approach latent semantic indexing lsi ha been successfully used for ir purpose a a technique for capturing semantic relation between term and inserting them into the similarity measure between two document one of it main drawback in ir is it computational cost in this paper we describe how the lsi approach can be implemented in a kernel defined feature space we provide experimental result demonstrating that the approach can significantly improve performance and that it doe not impair it 
a lightweight rule induction method is describedthat generates compact disjunctivenormal form dnf rule each class hasan equal numberofunweighted rule a newexample is classified by applying all rule andassigning the example to the class with themost satisfied rule the induction methodattempts to minimize the training error withno pruning an overall design is specifiedby setting limit on the size and numberof rule during training case are adaptivelyweighted 
we present a monte carlo algorithm for learning to act in partially observable markov decision process pomdps with real valued state and action space our approach us importance sampling for representing belief and monte carlo approximation for belief propagation a reinforcement learning algorithm value iteration is employed to learn value function over belief state finally a samplebased version of nearest neighbor is used to generalize across state initial empirical result suggest that our approach work well in practical application 
gaussian mixture or so called radial basis function network fordensity estimation provide a natural counterpart to sigmoidal neuralnetworks for function tting and approximation in both case it is possible to give simple expression for the iterative improvementof performance a component of the network are introducedone at a time in particular for mixture density estimation we showthat a k component mixture estimated by maximum likelihood orby an iterative likelihood 
we present a general framework for discriminative estimation based on the maximum entropy principle and it extension all calculation involve distribution over structure and or parameter rather than specific setting and reduce to relative entropy projection this hold even when the data is not separable within the chosen parametric class in the context of anomaly detection rather than classification or when the label in the training set are uncertain or incomplete support vector machine are naturally subsumed under this class and we provide several extension we are also able to estimate exactly and efficiently discriminative distribution over tree structure of class conditional model within this framework preliminary experimental result are indicative of the potential in these technique 
given the ranked list of document returned by multiple search engine in response to a given query the problem ofmetasearchis to combine these list in a way which optimizes the performance of the combination this paper make three contribution to the problem of metasearch we describe and investigate a metasearch model based on an optimal democratic voting procedure the borda count we describe and investigate a metasearch model based on bayesian inference and we describe and investigate a model for obtaining upper bound on the performance of metasearch algorithm our experimental result show that metasearch algorithm based on the borda and bayesian model usually outperform the best input system and are competitive with and often outperform existing metasearch strategy finally our initial upper bound demonstrate that there is much to learn about the limit of the performance of metasearch 
the spatial distribution of gray level intensity in an image can be naturally modeled using markov random field mrf model we develop and investigate the performance of face detection algorithm derived from mrf consideration for enhanced detection the mrf model are defined for every permutation of site index in the image we find the optimal permutation that provides maximum discriminatory power to identify face from nonfaces the methodology presented here is a generalization of the face detection algorithm in where a most discriminating markov chain model wa used the mrf model successfully detect face in a number of test image in real time 
we describe an iterative algorithm for building vector mach ines used in classification task the algorithm build on idea from sup port vector machine boosting and generalized additive model the algorithm can be used with various continuously differential function t hat bound the discrete classification loss and is very simple to impl ement we test the proposed algorithm with two different loss function on synthetic and natural data we also describe a norm penalized version of the algorithm for the exponential loss function used in adaboost the performance of the algorithm on natural data is comparable to support vecto r machine while typically it running time is shorter than of svm 
when constructing a classier the probability of correct classi cation of future data point should be maximized in the currentpaper this desideratum is translated in a very direct way into anoptimization problem which is solved using method from convexoptimization we also show how to exploit mercer kernel inthis setting to obtain nonlinear decision boundary a worst casebound on the probability of misclassication of future data is obtainedexplicitly 
the standard reinforcement learning view of the involvement of neuromodulatory system in instrumental conditioning includes a rather straightforward conception of motivation a prediction of sum future reward competition between action is based on the motivating characteristic of their consequent state in this sense substantial careful experiment reviewed in dickinson amp balleine into the neurobiology and psychology of motivation show that this view is incomplete in many case 
this paper introduces multiple instance regression a variant of multiple regression in which each data point may be described by more than one vector of value for the independent variable the goal of this work are to understand the computational complexity of the multiple instance regression task and develop an ecien t algorithm that is superior to ordinary multiple regression when applied to multiple instance data set 
we propose a novel method for the analysis of sequential data that exhibit an inherent mode switching in particular the data might be a non stationary time series from a dynamical system that switch between multiple operating mode unlike other approach our method process the data incrementally and without any training of internal parameter we use an hmm with a dynamically changing number of state and an on line variant of the viterbi algorithm that performs an unsupervised segmentation and classication of the data on the y i e the method is able to process incoming data in real time the main idea of the approach is to track and segment change of the probability density of the data in a sliding window on the incoming data stream the usefulness of the algorithm is demonstrated by an application to a switching dynamical system 
the vicinal risk minimization principle establishes a bridge between generative model and method derived from the structural risk minimization principle such a support vector machine or statistical regularization we explain how vrm provides a framework which integrates a number of existing algorithm such a parzen window support vector machine ridge regression constrained logistic classifier and tangent prop we then show how the approach implies new algorithm for solving problem usually associated with generative model new algorithm are described for dealing with pattern recognit ion problem with very different pattern distribution and dealing with unlabeled data preliminary empirical result are presented 
for ambiguous sentence traditional semantics construction produce large number of higher order formula which must then be reduced individually underspecified version can produce compact description of all reading but it is not known how to perform reduction on these description we show how to do this using reduction constraint in the constraint language for structure clls 
this article deal with optical law that must be considered when using underwater camera both theoretical and experimental point of view are described and it is shown that relationship between air and water calibration can be found 
we present an algorithm that induces a class of model with thin junction tree model that are characterized by an upper bound on the size of the maximal clique of their triangulated graph by ensurin g that the junction tree is thin inference in our model remains tract able throughout the learning process this allows both an efficient implemen tation of an iterative scaling parameter estimation algorithm and al so ensures that inference can be performed efficiently with the final model w e illustrate the approach with application in handwritten digit recogn ition and dna splice site detection 
metaquery also known a metapattem is a datamining tool useful for learning rule in volving more than one relation in the database a metaquery is a template or a second order proposition in a language l that describes the type of pattern to be discovered this tool ha already been successfully applied to several real world application in this paper we advance the state of the art in metaqueries research in several way first we analyze the related computational problem and classify it a np hard with a tractable sub set that is quite immediately evident second we argue that the notion of support for metaqueries where support is intuitively some indi cation to the relevance of the rule to be discovered is not adequately defined in the liter ature and propose our own definition third we propose some efficient algorithm for com puting support and present preliminary experi mental result that indicate that our algorithm are indeed quite useful 
a few year ago pearl s probability propagationalgorithmin graph with cycle wa shown to produce excellent result for error correcting turbodecoding ever since we have wondered whether iterative probability propagation could be used successfully for machine learning a a first step in this direction we study iterative inference and learning in the simple factor analyzer network a two layer densely connected network that model bottom layer sensory input a a linear combination of top layer factor plus independent gaussian sensor noise the number of bottom up top down iteration needed to exactly infer the factor given a network and an input scale with the number of factor in the top layer in online learning this iterative procedure must be reinitialized upon each pattern presentation and so learning becomes prohibitively slow in big network such a those used for face recognition and for large scale model of the cortex we show that probabilitypropagation in a factor analyzer usually take just a few iteration to achieve a low inference error even in network with sensor and factor we derive an expression for the algorithm s fixed point and provide an eigenvalue condition for globalconvergence we also show how iterativeinference can be used to do online learning and give result on using this method to do online dimensionalityreduction for the purpose of recognizing pixelimages of face using a dimensional subspace this work suggests that iterative probability propagation in densely connected network may lead to a broad class of useful algorithm for machine learning 
using statistical mechanic result i calculate learning curve average generalization error for gaussian process gps and bayesian neural network nns used for regression applying the result to learning a teacher defined by a two layer network i can directly compare gp and bayesian nn learning i find that a gp in general requires training example to learn input feature of order is the input dimension whereas a nn can learn the task with order the number of adjustable weight training example since a gp can be considered a an infinite nn the result show that even in the bayesian approach it is important to limit the complexity of the learning machine the theoretical finding are confirmed in simulation with analytical gp learning and a nn mean field algorithm 
this paper introduces a new probabilistic framework for space carving in this framework each voxel is assigned a probability which is computed by comparing the likelihood for the voxel existing and not existing this new framework avoids many of the difficulty associated with the original space carving algorithm specifically it doe not need a global threshold parameter and it guarantee that no hole will be carved in the model this paper also proposes that a voxel based thick texture is a realistic and efficient representation for scene which contain dominant plane the algorithm is tested using both real and synthetic data and both qualitative and quantitative result are presented 
support vector machine have met with significant success in numerous real worldlearning task however like most machine learning algorithm they are generally appliedusing a randomly selected training set classified in advance in many setting we also havethe option of using pool based active learning instead of using a randomly selected trainingset the learner ha access to a pool of unlabeled instance and can request the label for somenumber of them we introduce a new 
we present a new learning algorithm for pattern recognition inspired by a recent upper bound on leave one out error jaakkola and haussler proved for support vector machine svms vapnik the new approach directly minimizes the expression given by the bound in an attempt to minimize leave one out error this give a convex optimization problem which construct a sparse linear classifier in feature space using the kernel technique a such the algorithm posse many of the same property a svms the main novelty of the algorithm is that apart from the choice of kernel it is parameterless the selection of the number of training error is inherent in the algorithm and not chosen by an extra free parameter a in svms first experiment using the method on benchmark datasets from the uci repository show result similar to svms which have been tuned to have the best choice of parameter 
abstract classifier system are now viewed disappointing because of their problem such a the rule strength v rule set performance problem and the credit assignment problem in order to solve the problem we have developed a hybrid classifier system gls generalization learning system in designing gls we view cs a model free learning in pomdps and take a hybrid approach to finding the best generalization given the total number of rule gls us the policy improvement procedure by jaakkola et al for an locally optimal stochastic policy when a set of rule condition is given gls us ga to search for the best set of rule condition 
compilation to boolean satisfiability ha become a powerful paradigm for solving ai problem however domain that require metric reasoning cannot be compiled efficiently to sat even if they would otherwise benefit from compilation we address this problem by introducing the lcnf representation which combine propositional logic with metric constraint we present lpsat an engine which solves lcnf problem by interleaving call to an incremental simplex algorithm with systematic satisfaction method we describe a compiler which convert metric resource planning problem into lcnf for processing by lpsat the experimental section of the paper explores several optimization to lpsat including learning from constraint failure and randomized cutoff 
we introduce a framework for finding preference information to derive desired conclusion in nonmonotonic reasoning a new abductive framework called preference abduction enables u to infer an appropriate set of priority to explain the given observation skeptically thereby resolving the multiple extension problem in the answer set semantics for extended logic program preference abduction is also combined with a usual form of abduction in abductive logic programming and ha application such a specification of rule preference in legal reasoning and preference view update the issue of learning abducibles and priority is also discussed in which abduction to a particular cause is equivalent to abduction to preference 
reinforcement learning agent attempt to learn and construct a decision policy which maximises some reward signal in turn this policy is directly derived from long term value estimate of state action pair in environment with real valued state space however it is impossible to enumerate the value of every state action pair necessitating the use of a function approximator in order to infer state action value from similar state typically function approximators require many parameter 
data mining research ha long concentrated on the five main area clustering association discovery classification forecasting and sequential pattern web data mining project are concerned mainly with text mining user segmentation forecasting web usage and analyzing user clickstream pattern we present a new type of web usage mining called funnel analysis or funnel report mining a funnel report is a study of the retention behavior among a series of page or site for example of all hit on the home page of www msn com what percentage of those are followed by hit to moneycentral msn com what percentage of www msn com hit are followed by moneycentral msn com and then www msnbc com what are the most interesting funnel starting with www msn com where doe the greatest drop off rate occur after a user ha hit msnbc funnel report are extremely useful in e business because they give product planner an idea of how usable and well structured their site is from our experience performing web usage mining for the msn network of site funnel report are requested even more than user segmentation analysis site affiliation study and classification exercise in this paper we define a framework for funnel analysis and provide a tree based solution we have been using successfully to extract all relevant funnel using only one scan of the data file 
fourteen indicator that measure the frequency of lexico syntactic phenomenon linguistically related to aspectual class are applied to aspectual classification this group of indicator is shown to improve classification performance for two aspectual distinction stativity and completedness i e telicity over unrestricted set of verb from two corpus several of these indicator have not previously been discovered to correlate with aspect 
preliminary work by the author made use of the so called manhattan world assumption about the scene statistic of city and indoor scene this assumption stated that such scene were built on a cartesian grid which led to regularity in the image edge gradient statistic in this paper we explore the general applicability of this assumption and show that surprisingly it hold in a large variety of le structured environment including rural scene this enables u from a single image to determine the orientation of the viewer relative to the scene structure and also to detect target object which are not aligned with the grid these inference are performed using a bayesian model with probability distribution e g on the image gradient statistic learnt from real data 
abstract locating expertise source in a community of interest or practice is a critical need for distributed organization operating in knowledge intensive business sector this is specially true in those one that deal with innovation activites which have to manage knowledge about the creation of new knowledge finding fastly a suitable expert and knowing how to reach him or her can be seen a a way to gain advantage and speed up organizational knowledge creation and learning usually expertise location is done through the use of personal social or knowledge network and involves aspect such a trust and reputation however and specially in distributed organization relying on communication technology for cooperation each member of a community is just aware of it own personal social or knowledge network this make difficult to get to know other potential expert in the community which may pertain to other member network netexpert is an agent based expertise location system that replicates the process of social and knowledge network building at a community or organization level in so doing it is able to connect several network and put into contact expertise that otherwise would remain hidden keywords software agent multiagent system assistant agent expertise location localize expertise social 
stimulus array are inevitably presented at different position on the retina in visual task even those that nominally require fixation in par ticular this applies to many perceptual learning task we show that per ceptual inference or discrimination in the face of positional variance ha a structurally different quality from inference about fixed position stimulus involving a particular quadratic non linearity rather than a purely lin ear discrimination we show the advantage taking this non linearity into account ha for discrimination and suggest it a a role for recurrent con nections in area v by demonstrating the superior discrimination perfor mance of a recurrent network we propose that learning the feedforward and recurrent neural connection for these task corresponds to the fast and slow component of learning observed in perceptual learning task 
certain planning system that deal with quantitative time constraint have used an underlying simple temporal problem solver to ensure temporal consistency of plan however many application involve process of uncertain duration whose timing cannot be controlled by the execution agent these case require more complex notion of temporal feasibility in previous work various controllability property such a weak strong and dynamic controllability have been defined the most interesting and useful controllability property the dynamic one ha ironically proved to be the most difficult to analyze in this paper we resolve the complexity issue for dynamic controllability unexpectedly the problem turn out to be tractable we also show how to efficiently execute network whose status ha been verified 
we propose a casa labelling method of the tf representation which is based on the periodicity of the speech related to the voicing a local voicing index is estimated in four subbands after demodulation of the signal this index is used a a reliability measure for both pitch identification and speech recognition first this model allows robust f identification thanks to the voicing index which is a consistent reliability measure associated to the f measure since the task is to recognise speech corrupted with additive noise the periodicity is specific to the signal in our model the evaluation of speech reliability is not direct it also depends on a priori knowledge about the relationship between snr and the voicing index the goal is to assign to each tf region a probability clean enough to feed a multistream recogniser only adapted to clean data the model is able to localise region where the target speech dominates over the background noise this probability is evaluated according to a function established a priori the snr feature mapping and the choice of a snr decision threshold this is adapted to a new multistream recognition approach since the previous probability serve to weight the stream posterior 
the most prevalent experimental methodology for comparing the effectiveness of information retrieval system requires a test collection composed of a set of document a set of query topic and a set of relevance judgment indicating which document are relevant to which topic it is well known that relevance judgment are not infallible but recent retrospective investigation into result from the text retrieval conference trec ha shown that difference in human judgment of relevance do not affect the relative measured performance of retrieval system based on this result we propose and describe the initial result of a new evaluation methodology which replaces human relevance judgment with a randomly selected mapping of document to topic which we refer to aspseudo relevance judgment ranking of system with our methodology correlate positively with official trec ranking although the performance of the top system is not predicted well the correlation are stable over a variety of pool depth and sampling technique with improvement such a methodology could be useful in evaluating system such a world wide web search engine where the set of document change too often to make traditional collection construction technique practical 
this paper examines the problem of reconstructing a voxelized representation of d space from a series of image an iterative algorithm is used to find the scene model which jointly explains all the observed image by determining which region of space is responsible for each of the observation the current approach formulates the problem a one of optimization over estimate of these responsibility the process converges to a distribution of responsibility which accurately reflects the constraint provided by the observation the position and shape of both solid and transparent object and the uncertainty which remains reconstruction is robust and gracefully represents regio n of space in which there is little certainty about the exact structure due to limited non existent or contradicting d ata rendered image of voxel space recovered from synthetic and real observation image are shown 
we describe an interactive system which support the exploration of argument generated from bayesian network in particular we consider key feature which support interactive behaviour an attentional mechanism which update the activation of concept a the interaction progress a set of exploratory response and a set of probabilistic pattern and an argument grammar which support the generation of natural language argument from bayesian network a preliminary evaluation ass the effect of our exploratory response on user s belief 
recurrent neural network of analog unit are computer for realvaluedfunctions we study the time complexity of real computationin general recurrent neural network these have sigmoidal linear and product unit of unlimited order a node and no restrictionson the weight for network operating in discrete time we exhibit a family of function with arbitrarily high complexity and we derive almost tight bound on the time required to computethese function thus evidence is 
we propose a novel co training method for statistical parsing the algorithm take a input a small corpus sentence annotated with parse tree a dictionary of possible lexicalized structure for each word in the training set and a large pool of unlabeled text the algorithm iteratively label the entire data set with parse tree using empirical result based on parsing the wall street journal corpus we show that training a statistical parser on the combined labeled and unlabeled data strongly out performs training only on the labeled data 
the problem of establishing correspondence between image taken from different viewpoint is fundamental in computer vision we propose an algorithm which is capable of handling larger change in viewpoint than classical correlation based technique optimal performance for the algorithm is achieved for textured object which are locally planar in at least one direction the algorithm work by computing affinely invariant fourier feature from intensity profile in each image the intensity profile are extracted from the image data between randomly selected pair of image interest point using a voting scheme pair of interest point are matched across image by comparing vector of fourier feature outlier among the match are rejected in two stage a fast stage using novel view consistency constraint and a second slower stage using ransac and fundamental matrix computation in order to demonstrate the quality of the result the algorithm is tested on several different image pair 
independent component analysis of natural image lead to emergence of simple cell property i e linear filter that resemble wavelet or gabor function in this paper we extend ica to explain further property of v cell first we decompose natural image into independent subspace instead of scalar component this model lead to emergence of phase and shift invariant feature similar to those in v complex cell second we define a topography between the linear component obtained 
this paper investigates the multiple view geometry of smooth surface and a plane where the plane provides a planar homography mapping between the view innovation are made in three area first new solution are given for the computation of epipolar and trifocal geometry for this type of scene in particular it is shown that the epipole may be determined from bitangents between the homography registered occluding contour and a new minimal solution is given for computing the trifocal tensor second algorithm are demonstrated for automatically estimating the fundamental matrix and trifocal tensor from image of such scene third a method is developed for estimating camera matrix for a sequence of image of these scene these three area are combined in a freehand scanner application where d texture mapped graphical model of smooth object are acquired directly from a video sequence of the object and plane 
since kimball parsing preference principle such a right association ra and minimal attachment ma are often formulated with respect to constituent tree we present preference principle based on derivation tree within the framework of ltags we argue they remedy some shortcoming of the former approach and account for widely accepted heuristic e g argument modifier idiom 
we investigate the effect of speech recognition error on a system for the unsupervised nearly synchronous clustering of broadcast news story using the tdt topic detection and tracking corpus two question are addressed are speech recognition error detrimental to the performance of the system can a background collection of contemporaneous clean text improve performance we investigate both the large cluster and small cluster limit 
this paper present a new approach to image based guidance of a needle or surgical tool during percutaneous procedure the method is based on visual servoing it requires no prior calibration or registration the technique provides highly precise d alignment of the tool with respect to an anatomic target by taking advantage of projective geometry and projective invariant this can be achieved in a fixed number of iteration in addition the approach estimate the required insertion depth experiment include automatic d alignment and insertion of a needle held by a medical robot into a pig kidney under x ray fluoroscopy 
in this paper we explore two quantitative approach to the modelling of counterfactual reasoning a linear and a noisy or model based on information contained in conceptual dependency network empirical data is acquired in a study and the fit of the model compared to it we conclude by considering the appropriateness of non parametric approach to counterfactual reasoning and examining the prospect for other parametric approach in the future 
this paper we discus an adaptive method ofhandling fragmented user utterance to a speech basedmultimodal dialogue system inserted silent pausesbetween fragment present the following problem doe the current silence indicate that the user hascompleted her utterance or is the silence just a pausebetween two fragment so that the system should waitfor more input our system incrementally classifiesuser utterance a either closing more input isunlikely to come or non closing 
in this paper we describe inca an adaptive advisable assistant for crisis response the system let user guide the search towardparticular schedule by giving highlevel operational advice about the solutionsdesired because trace of user interactionsprovide information regarding theuser s preference among schedule incacan draw on machine learning techniquesto construct user model that reflect thesepreferences we characterize the modelingtask a that of learning a 
this paper present an approach to object detection which is based on recent work in statistical model for texture synthesis and recognition heeger and bergen de bonet and viola zhu et al simoncelli and portilla our method follows the texture recognition work of de bonet and viola we use feature vector which capture the joint occurrence of local feature at multiple resolution the distribution of feature vector for a set of training image of an object class is estimated by clustering the data and then forming a mixture of gaussian model the mixture model is further refined by determining which cluster are the most discriminative for the class and retaining only those cluster after the model is learned test image are classified by computing the likelihood of their feature vector with respect to the model we present promising result in applying our technique to face detection and car detection 
this paper is concerned with using a semantic hierarchy to estimate the frequency with which a word sense appears a a given argument of a verb assuming the data is not sense disambiguated the standard approach is to split the count for any noun appearing in the data equally among the alternative sens of the noun this can lead to inaccurate estimate we describe a reestimation process which us the accumulated count of hypernym of the alternative sens in order to redistribute the count in order to choose a hypernym for each alternative sense we employ a novel technique which us a x test to measure the homogeneity of set of concept in the hierarchy 
this paper describes a new layered brain architecture for simulated autonomous and semi autonomous creature that inhabit graphical world the main feature of the brain is it division into distinct system which communicate through common access to an internal mental blackboard the brain wa designed to encourage experimentation with various system and architecture it ha so far proven flexible enough to accommodate research advancing in a number of different direction by a small team of researcher 
we present a tool for the annotation of xmlencoded multi modal language corpus nonhierarchical data is supported by mean of standoff annotation we define base level and suprabase level element and theory independent markables for multi modal annotation and apply them to a cospecification annotation scheme we also describe how arbitrary annotation scheme can be represented in term of these element apart from theoretical consideration however the development of a fast robust and highly usable annotation tool wa a major objective of the work presented 
we analyze the effect of perturbation on the estimation of depth from defocus dfd implemented by changing the focus setting e g axially moving the sensor the analysis yield the optimal change of focus setting and the spatial frequency for which estimation is most robust for stable estimation at all spatial frequency the change in focus setting should be le than twice the depth of field for the most robust estimation in the highest spatial frequency the axial interval should be equal to the depth of field 
propagating constraint is the main feature of anyconstraint solver this is thus of prime importanceto manage constraint propagation a efficiently aspossible justifying the use of the best algorithm but the ease of integration is also one of the concernswhen implementing an algorithm in a constraintsolver this paper focus on ac whichis the simplest arc consistency algorithm knownso far we propose two refinement that preserveas much a possible the ease of 
theory of object recognition often assume that only one representation scheme is used within one visual processing pathway versatility of the visual system come from having multiple visual processing pathway each specialized in a different category of object we propose a theoretically simpler alternative capable of explaining the same set of data and more a single primary visual processing pathway loosely modular is assumed memory module are attached to site along this pathway object identity decision is made independently at each site a site s response time is a monotonic decreasing function of it confidence regarding it decision an observer s response is the first arriving response from any site the effective representation s of such a system determined empirically can appear to be specialized for different task and stimulus consistent with recent clinical and functional imaging finding this however merely reflects a decision being made at it appropriate level of abstraction the system itself is intrinsically flexible and adaptive 
several machine learning algorithm have recently been used for text categorization and filtering in particular boosting method such a adaboost have shown good performance applied to real text data however most of existing boosting algorithm are based on classifier that use binary valued feature thus they do not fully make use of the weight information provided by standard term weighting method in this paper we present a boosting based learning method for text filtering that us naive bayes classifier a a weak learner the use of naive bayes allows the boosting algorithm to utilize term frequency information while maintaining probabilistically accurate confidence ratio applied to trec and trec filtering track document the proposed method obtained a significant improvement in lf lf f and f measure compared to the best result submitted by other trec entry 
we describe a distributed modular architecture for platform independent natural language system it feature automatic interface generation and self organization adaptive and non adaptive voting mechanism are used for integrating discrete module the architecture is suitable for rapid prototyping and product delivery 
new method to generate hard random problem instance have driven progress on algorithm for deduction and constraint satisfaction recently achlioptas et al aaai introduced a new generator based on latin square that creates only satisfiable problem and so can be used to accurately test incomplete one sided solver we investigate how this and other generator are biased away from the uniform distribution of satisfiable problem and show how they can be improved by imposing a balance condition more generally we show that the generator is one member of a family of related model that generate distribution ranging from one that are everywhere tractable to one that exhibit a sharp hardness threshold we also discus the critical role of the problem encoding in the performance of both systematic and local search solver 
we present a novel algorithm that creates document vector with reduced dimensionality this work wa motivated by an application characterizing relationship among document in a collection our algorithm yielded inter document similarity with an average precision up to higher than that of singular value decomposition svd used for latent semantic indexing the best performance wa achieved with dimensional reduction rate that were higher than svd on average our algorithm creates basis vector for a reduced space by iteratively scaling vector and computing eigenvectors unlike svd it break the symmetry of document and term to capture information more evenly across document we also discus correlation with a probabilistic model and evaluate a method for selecting the dimensionality using log likelihood estimation 
in many case self calibration is not able to yield a unique solution for the d reconstruction of a scene this is due to the occurrence of critical motion sequence if this is the case an ambiguity is left on the reconstruction in this paper it is derived under which condition correct novel view can be generated from ambiguous reconstruction the problem is r st approached from a theoretical point of view it is proven that novel view are correct a long a the inclusion of the new view in the sequence yield the same ambiguity on the reconstruction the problem is therefore much related to the problem of critical motion sequence since the virtual camera can be arbitrarily moved within the smallest critical motion set that contains the recovered camera motion without distortion becoming visible based on these result a practical measure for the expected ambiguity on a novel view based on the recovered structure and motion is derived a an application a viewer wa built that indicates if a specic novel view can be trusted or not by changing the background color 
we consider the problem of how an agent creates a discrete spatial representation from it continuous interaction with the environment such representation will be the minimal one that explains the experience of the agent in the environment in this 
ensemble method have recently garnered a great deal of attention in the machine learning community technique such a boosting and bagging have proven to be highly effective but require repeated resampling of the training data making them inappropriate in a data mining context the method presented in this paper take advantage of plentiful data building separate classifier on sequential chunk of training point these classifier are combined into a fixed size ensemble using a heuristic replacement strategy the result is a fast algorithm for large scale or streaming data that classifies a well a a single decision tree built on all the data requires approximately constant memory and adjusts quickly to concept drift 
many cachine learning application require classifier that minimize an asymmetric cost function rather than the misclassification rate and several recent paper have addressed this problem however these paper have either applied no statistical testing or have applied statistical method that are not appropriate for the cost sensititve setting without good statistical method it is difficult to tell whether these new cost sensastive method are better than existing method that ignore cost and it is also difficult to tell whether one cost sensitive method is better than another to rectify this problem this paper present two statistical method for the cost sensitive setting the first construct a confidence interval for the expected cost of a single classifier the second construct a confidence interval for the expected difference in cost of two classifier in both case the basic idea is to separate the problem of estimating the probability of each cell in the confusion matrix which is independent of the cost matrix from the problem of computing the expected cost we show experimentally that these bootstrap test work better than applying standard z test based on the normal distribution 
a bayesian framework for deformable pattern classification wa proposed by k w cheung et al with promising result for isolated handwritten character recognition it performance however degrades significantly when it is applied to detect deformable pattern in complex scene where the amount of outlier due to other neighboring object or the background is usually large also the fact that the associated evidence measure doe not penalize model resting on white space result in a high false alarm rate another bayesian framework for deformable pattern detection is proposed the framework posse the intrinsic property of matching with only part of an image segmentation and it associated evidence measure can penalize white space implicitly however limited data exploration capability is the major trade off by properly combining the two framework a new matching algorithm called bidirectional matching is proposed this combined approach posse the advantage of the two framework and give robust result for non rigid shape extraction to evaluate the performance of the proposed approach we have applied it to shape based handwritten word retrieval using a subset of the bb dataset in the cedar database we can achieve a recall rate of and a precision rate of 
extracting rule from rbfs is not a trivial task because of nonlinear function or high input dimensionality in such case some of the hidden unit of the rbf network have a tendency to be shared across several output class or even may not contribute 
we study a population decoding paradigm in which the maximum likelihood inference is based on an unfaithful decoding model umli this is usually the case for neural population decoding because the encoding process of the brain is not exactly known or because a simplified decoding model is preferred for saving computational cost we calculate the decoding error of umli and show an example of an unfaithful model which neglect the neuronal correlation the perf ormance of umli is compared with that of the maximum likelihood inference based on a faithful model and that of the center of mass decoding method it turn out that umli ha advantage of decreasing the computational complexity remarkablely and maintaining a high level decoding accuracy at the same time 
we study the recognition of surface made from different material such a concrete rug marble or leather on the basis of their textural appearance such natural texture arise from spatial variation of two surface attribute reflectance and surface normal in this paper we provide a unified model to address both these aspect of natural texture the main idea is to construct a vocabulary of prototype tiny surface patch with associated local geometric and photometric property we call these d textons example might be ridge groove spot or stripe or combination thereof associated with each texton is an appearance vector which characterizes the local irradiance distribution represented a a set of linear gaussian derivative filter output under different lighting and viewing condition given a large collection of image of different material a clustering approach is used to acquire a small on the order of d texton vocabulary given a few to image of any material it can be characterized using these textons we demonstrate the application of this representation for recognition of the material viewed under novel lighting and viewing condition 
we study using reinforcement learning in dynamic environment such environment may contain many dynamic object which make optimal planning hard one way of using information about all dynamic object is to expand the state description but this result in a high dimensional policy space our approach is to instantiate information about dynamic object in the model of the environment and to replan using model based reinforcement learning whenever this information change furthermore our 
point set obtained from computer vision technique are often noisy and non uniform we present a new method of surface reconstruction that can handle such data set using anisotropic basis function our reconstruction algorithm draw upon the work in variational implicit surface for constructing smooth and seamless d surface implicit function are often formulated a a sum of weighted basis function that are radially symmetric using radially symmetric basis function inherently assumes however that the surface to be reconstructed is everywhere locally symmetric such an assumption is true only at planar region and hence reconstruction using isotropic basis is insufficient to recover object that exhibit sharp feature we preserve sharp feature using anisotropic basis that allow the surface to vary locally the reconstructed surface is sharper along edge and at corner point we determine the direction of anisotropy at a point by performing principal component analysis of the data point in a small neighborhood the resulting field of principle direction across the surface is smoothed through tensor filtering we have applied the anisotropic basis function to reconstruct surface from noisy synthetic d data and from real range data obtained from space carving 
this paper focus on the analysis and prediction of so called defined a turn where a user of a spoken dialogue system first becomes aware that the system ha made a speech recognition error we describe statistical comparison of feature of these aware site in a train timetable spoken dialogue corpus which reveal significant prosodic difference between such turn compared with turn that correct speech recognition error a well a with normal turn that are neither aware site nor correction we then present machine learning result in which we show how prosodic feature in combination with other automatically available feature can predict whether or not a user turn wa a normal turn a correction and or an aware site 
in this paper we present a novel approach to estimateand analyze d fluid structure and motion ofclouds from multi spectrum d cloud image sequence accurate cloud top structure and motion are very importantfor a host of meteorological and climate application however due to the extremely complex natureof cloud fluid motion classical nonrigid motion analysismethods will be insufficient to solve this particularproblem in this paper two spectrum of satellite cloudimages are utilized 
this paper present an algorithm that match interest point detected on a pair of grey level image taken from arbitrary point of view first matching hypothesis are generated using a similarity measure of the interest point hypothesis are confirmed using local group of interest paint group match are based on a measure defined on an affine transformation estimate and on a correlation coefficient computed on the intensity of the interest point once a reliable match ha been determined for a given interest point and the corresponding local group new group match are found by propagating the estimated affine transformation the algorithm ha been widely tested under various image transformation it provides dense match and is very robust to outlier i e interest point generated by noise or present in only one image because of occlusion or non overlap 
a family of axially symmetric mirror shape are proposed for panoramic imaging these shapeskeep the resolution in the image invariant to change in elevation in the scene in other word this family of shape achievessolid angle pixel density invariance an analysis of range finding using two coaxial axially symmetric resolution invariant mirror in a coaxial pair is presented the resolution invariance property of these mirror mean that when the image captured is unwarped the resultant image will not suffer from variable image quality this problem is present in unwarped image captured with any mirror shape not designed for resolution invariance the resolution invariance of these mirror is especially important in the case of stereo panoramic mirror where one view of the scene is captured within the other and thus will have lower resolution it is necessary to clearly identify two view of an object to undertake range finding the proposed mirror shape will be useful for mobile robotics and machine vision other application area such a visual sensing for control of traffic light at street intersection could be considered 
a method for upgrading a projective reconstruction to metric is presented the reconstruction is first transformed by considering cheirality so that the convex hull of the set of camera projection centre is the same a in the metric counterpart the method then proceeds iteratively and starting from such a reconstruction is a necessary condition for many iterative calibration algorithm to converge the result show that in practice it is also most often sufficient provided that the minimised objective function is a geometrically meaningful quantity the method ha been found extremely reliable for both large and small reconstruction in a large number of experiment on real data when subjected to the common degeneracy of little or no rotation between the view the method still yield a very reasonable member of the family of possible solution furthermore the method is very fast and therefore suitable for the purpose of viewing reconstruction 
this demonstration will show describe the construction and application of cross domain information server using feature of the standard z information retrieval protocol z the system is currently being used to build and search distributed index for database with disparate structured data sgml and xml we use the z explain database to determine the database and index of a given server then use the z scan facility to extract the content of the index this information is used to build collection document that can be retrieved using probabilistic retrieval algorithm 
face image are subject to change in view and illumination such change cause data distribution to be highly nonlinear and complex in the image space it is desirable to learn a nonlinear mapping from the image space to a low dimensional space such that the distribution becomes simpler tighter and therefore more predictable for better modeling of face in this paper we present a kernel machine based approach for learning such nonlinear mapping the aim is to provide an effective view based representation for multiview face detection and pose estimation assuming that the view is partitioned into a number of distinct range one nonlinear view subspace is learned for each range of view from a set of example face image of that view range by using kernel principal component analysis kpca projection of the data onto the view subspace are then computed a view based nonlinear feature multi view face detection and pose estimation are performed by classifying a face into one of the facial view or into the nonface class by using a multi class kernel support vector classifier ksvc experimental result show that fusion of evidence from multiviews can produce better result than using the result from a single view and that our approach yield high detection and low false alarm rate in face detection and good accuracy in pose estimation in comparison with the linear counterpart composed of linear principal component analysis pca feature extraction and fisher linear discriminant based classification fldc 
one of the most powerful and widely acceptedanalytical formalism for modeling biologicaland physical system is that of thepartial dierential equation pde establishingan acceptable pde model for a dynamicsystem occupies a major portion of thework of the mathematical modeler there aretwo main aspect to this activity first anappropriate structure ha to be determinedfor the equation involved the model identication problem second acceptably accuratevalues for 
we consider the problem of how an agent creates a discrete spatial representation from it continuous interaction with the environment such representation will be the minimal one that explains the experience of the agent in the environment in this 
extracting rule from rbfs is not a trivial task because of nonlinear function or high input dimensionality in such case some of the hidden unit of the rbf network have a tendency to be shared across several output class or even may not contribute 
identification of outlier can lead to the discovery of unexpected interesting and useful knowledge existing method are designed for detecting spatial outlier in multidimensional geometric data set where a distance metric is available in this paper we focus on detecting spatial outlier in graph structured data set we define statistical test analyze the statistical foundation underlying our approach design several fast algorithm to detect spatial outlier and provide a cost model for outlier detection procedure in addition we provide experimental result from the application of our algorithm on a minneapolis st paul twin city traffic dataset to show their effectiveness and usefulness 
many problem are difficult to adequately explore until a prototype exists in order to elicit user feedback one such problem is a system that automatically categorizes and manages email due to a myriad of user interface issue a prototype is necessary to determine what technique and technology are effective in the email domain this paper describes the implementation of an add in for microsoft outlook tm that intends to address two problem with email help manage the inbox by automatically classifying email based on user folder and to aid in search and retrieval by providing a list of email relevant to the selected item this add in represents a first step in an experimental system for the study of other issue related to information management the system ha been set up to allow experimentation with other classification algorithm and the source code is available online in an effort to promote further experimentation 
this paper present a restricted version of set local multi component tag weir which retains the strong generative capacity of tree local multi component tag i e produce the same derived structure but ha a greater derivational generative capacity i e can derive those structure in more way this formalism is then applied a a framework for integrating dependency and constituency based linguistic representation 
abstract we present a variational bayesian method for model selection over family of kernel classiers like support vector machine or gaussian process the algorithm need no user interaction and is able to adapt a large number of kernel parameter to given data without having to sacrice training case for validation this open the possibility to use sophisticated family of kernel in situation where the small standard kernel class are clearly inappropriate we relate the method to other work done on gaussian process and clarify the relation between support vector machine and certain gaussian process model 
we investigate the problem of classifying individual based on estimated density functionsfor each individual given labelled histogram characterizing red blood cell rbc fordifferent individual the learning problem is to build a classifier which can classify newunlabelled histogram into normal and iron deficient class thus the problem is similarto conventional classification in that there is labelled training data but different in thatthe underlying measurement are not 
abstract data we o er a simple paradigm for tting mod we o er a simple paradigm for tting parametric el parametric and non parametric to noisy data model which solves these two problem this is which resolve some of the problem associated with done by considering each point on the parametric classic mse algorithm this is done by consider model a a possible source for each data point the ing each point on the model a a possible source for model is also extended to non parametric model each data point and give good result even for curve with strong the paradigm also allows to solve problem which discontinuity are not de ned in the classical mse approach such we show result of the method for line seg a tting a segment a opposed to a line it is ments circle and general curve we also show shown to be non biased and to achieve excellent result using gaussian and uniform noise model result for general curve even in the presence of strong discontinuity previous work result are shown for a number of tting prob lem including line circle segment and gen there are many paper written on using least eral curve contaminated by gaussian and uniform square technique in order to t parameter to a noise noisy model and on using di erent numerical tech niques and linear approximation in order to do the 
the theoretical study of the range concatenation grammar rcg formalism ha revealed many attractive property which may be used in nlp in particular range concatenation language rcl can be parsed in polynomial time and many classical grammatical formalism can be translated into equivalent rcgs without increasing their worst case parsing time complexity for example after translation into an equivalent rcg any tree adjoining grammar can be parsed in o n time in this paper we study a parsing technique whose purpose is to improve the practical efficiency of rcl parser the non deterministic parsing choice of the main parser for a language l are directed by a guide which us the shared derivation forest output by a prior rcl parser for a suitable superset of l the result of a practical evaluation of this method on a wide coverage english grammar are given 
test collection have traditionally been used by information retrieval researcher to improve their retrieval strategy to be viable a a laboratory tool a collection must reliably rank different retrieval variant according to their true effectiveness in particular the relative effectiveness of two retrieval strategy should be insensitive to modest change in the relevant document set since individual relevance assessment are known to vary widely the test collection developed in the trec workshop have become the collection of choice in the retrieval research community to verify their reliability nist investigated the effect change in the relevance assessment have on the evaluation of retrieval result very high correlation were found among the ranking of system produced using different relevance judgment set the high correlation indicate that the comparative evaluation of retrieval performance is stable despite substantial difference in relevance judgment and thus reaffirm the use of the trec collection a laboratory tool 
for motion picture special effect it is often necessary to take a source image of an actor segment the actor from the unwanted background and then composite over a new background the standard approach requires the unwanted background to be a blue screen while this technique is capable of handling area where the foreground blend into the background the physical requirement present many practical problem this paper present an algorithm that requires minimal human interaction to segment motion picture resolution image and image sequence we show that it can be used not only to segment badly lit or noisy bluescreen image but also to segment actor where the background is more varied 
this paper investigates whether a machine can automatically learn the task of finding within a large collection of candidate response the answer to question the learning process consists of inspecting a collection of answered question and characterizing the relation between question and answer with a statistical model for the purpose of learning this relation we propose two source of data usenet faq document and customer service call center dialogue from a large retail company we will show that the task of answer finding differs from both document retrieval and tradition question answering presenting challenge different from those found in these problem the central aim of this work is to discover through theoretical and empirical investigation those statistical technique best suited to the answer finding problem 
this paper proposes a hidden markov model hmm and an hmm based chunk tagger from which a named entity ne recognition ner system is built to recognize and classify name time and numerical quantity through the hmm our system is able to apply and integrate four type of internal and external evidence simple deterministic internal feature of the word such a capitalization and digitalization internal semantic feature of important trigger internal gazetteer feature external macro context feature in this way the ner problem can be resolved effectively evaluation of our system on muc and muc english ne task achieves f measure of and respectively it show that the performance is significantly better than reported by any other machine learning system moreover the performance is even consistently better than those based on handcrafted rule 
a method of assisting a user in finding the required document effectively is proposed a user being informed which document are worth examining can browse in a digital library dl in a linear fashion computational evaluation were carried out and a dl and it navigator are designed and constructed 
learning curve for gaussian process regression are well understood when the student model happens to match the teacher true data generation process i derive approximation to the learning curve for the more generic case of mismatched model and find very rich behaviour for large input space dimensionality where the result become exact there are universal studentindependent plateau in the learning curve with transition in between that can exhibit arbitrarily many over fitting maximum in lower dimension plateau also appear and the asymptotic decay of the learning curve becomes strongly student dependent all prediction are confirmed by simulation 
we present a formal framework for treating both incomplete information in the initial database and possible failure during an agent s execution of a course of action these two aspect of uncertainty are formalized by two different notion of probability we introduce also a concept of expected probability which is obtained by combining the two previous notion expected probability account for the probability of a sentence on the hypothesis that the sequence of action needed to make it true might have failed expected probability lead to the possibility of comparing course of action and verifying which is more safe 
we treat the problem of edge detection a one of statistical inference local edge cue implemented by lters provide information about the likely position of edge which can be used a input to higher level model dieren t edge cue can be evaluated by the statistical eectiv ene of their corresponding lters evaluated on a dataset of pre segmented image we use information theoretic measure to determine the eectiv ene of a variety of dieren t edge detector working at multiple scale on black and white and colour image our result give quantative measure for the advantage of multi level processing for the use of chromaticity in addition to greyscale and for the relative eectiv ene of dieren t detector proceeding computer vision and pattern recognition cvpr fort collins colorado 
pattern based machine translation is one of the machine translation method which performs syntactic analysis and structure transfer at the same time using bilingual pattern pbmt is used to expand the length of pattern up to sentence length in order to reduce ambiguity in translation but it brought out the problem of rapidly increased pattern we propose a model which shortens the length of pattern to phrase length and reduces ambiguity in translation by using two level translation pattern selection method in the first level the proper translation pattern are selected by using a hybrid method of exact example matching and semantic constraint by thesaurus in the second level the most natural translation pattern for the verb phrase is selected among the selected translation pattern category by using statistic information of the target language by using this proposed model we could shorten the length of pattern without raising the ambiguity in translation 
in this paper we propose adding long term grammatical information in a whole sentence maximun entropy language model wsme in order to improve the performance of the model the grammatical information wa added to the wsme model a feature and were obtained from a stochastic context free grammar finally experiment using a part of the penn treebank corpus were carried out and significant improvement were acheived 
the majority of existing language generation system have a pipeline architecture which offer efficient sequential execution of module but doe not allow decision about text content to be revised in later stage however a exemplified in this paper in some case choosing appropriate content can depend on text length and formatting which in a pipeline architecture are determined after content planning is completed unlike pipeline interleaved and revision based architecture can deal with such dependency but tend to be more expensive computationally since our system need to generate acceptable hypertext explanation reliably and quickly the pipeline architecture wa modified instead to allow additional content to be requested in later stage of the generation process if necessary 
we present a java based framework swami shared wisdom through the amalgamation of many interpretation for building and studying collaborative ltering system swami consists of three component a prediction engine an evaluation system and a visualization component the prediction engine provides a common interface for implementing dierent prediction algorithm the evaluation system provides a standardized testing methodology and metric for analyzing the accuracy and run time performance of prediction algorithm the visualization component suggests how graphical representation can inform the development and analysis of prediction algorithm we demonstrate swami on the eachmovie data set by comparing three prediction algorithm a traditional pearson correlation based method support vector machine and a new accurate and scalable correlation based method based on clustering technique 
the paper describes a branch and bound scheme that us heuristic generated mechanically by the mini bucket approximation this scheme is presented and evaluated for optimization task such a finding the most probable explanation mpe in bayesian network the mini bucket scheme yield monotonic heuristic of varying strength which cause different amount of pruning allowing a controlled tradeoff between preprocessing and search the resulting branch and bound with mini bucket heuristic bbmb is evaluated using random network probabilistic decoding and medical diagnosis network result show that the bbmb scheme overcomes the memory explosion of bucket elimination allowing a gradual tradeoff of space for time and of time for accuracy 
we consider the problem of how an agent creates a discrete spatial representation from it continuous interaction with the environment such representation will be the minimal one that explains the experience of the agent in the environment in this 
we study the low level problem of predicting pixel intensity after subpixel image translation this is a basic subroutine for image warping and super resolution and it ha a critical influence on the accuracy of subpixel matching by image correlation rather than using traditional frequency space filtering theory or ad hoc interpolators such a spline we take an empirical approach finding optimal subpixel interpolation filter by direct numerical optimization over a large set of training example the training set is generated by subsampling larger image at different translation using subsamplers that mimic the spatial response function of real pixel we argue that this give realistic result and design filter of various different parametric form under traditional and robust prediction error metric we systematically study the performance of the resulting filter paying particular attention to the influence of the underlying image sampling regime and the effect of aliasing jaggies we summarize the result and give practical advice for obtaining subpixel accuracy 
this paper study the role of two mechanism memory and balance to exploit the arm race resulting from predator prey interaction when solving a given problem memory ensures that individual are not only well adapted to the current member of the opposite population but also to earlier generation of opponent a balanced co evolution on the other hand adapts the speed of evolution i e the reproduction rate to the performance of a population it lead to a steady progress in both population indirectly a balanced co evolution avoids a premature loss of genetic diversity this in turn diminishes the need for a long memory span the current paper show how both mechanism can be incorporated in coevolutionary genetic algorithm cgas empirical result support the importance of and interaction between both mechanism 
conventional vision system and algorithm assume the camera to have a single viewpoint however sensor need not always maintain a single viewpoint for instance an incorrectly aligned system could cause non single viewpoint also system could be designed to specifically deviate from a single viewpoint to trade off image characteristic such a resolution and field of view in these case the locus of viewpoint form what is called a caustic in this paper we present an in depth analysis of caustic of catadioptric camera with conic reflector property of caustic with respect to field of view and resolution are presented finally we present way to calibrate conic catadioptric system and estimate their caustic from known camera motion 
we present a methodology for achieving cooperation between already existing theorem provers employing different proof paradigm and or different search control and using dif ferent but related logic cooperation between the provers is achieved by periodically inter changing clause which are selected by so called referee by employing referee both on the side of a sending prover and a receiving prover the communication is both successand demanddriven which result in a rather small commu nication overhead and synergetical effect we report on experiment regarding the coop eration of the provers spa setheo and discount in domain of the tptp library and with problem stemming from an application in software component retrieval the experiment show significant improvement in the number of problem solved a well a in the solution time 
in proc of ieee int l conf on computer vision vancouver canada vision based motion capturing of hand articulation is a challenging task since the hand present a motion of high degree of freedom model based approach could be taken to approach this problem by searching in a high dimensional hand state space and matching projection of a hand model and image observation however it is highly inefficient due to the curse of dimensionality fortunately natural hand articulation is highly constrained which largely reduces the dimensionality of hand state space this paper present a model based method to capture hand articulation by learning hand natural constraint our study show that natural hand articulation lie in a lower dimensional configuration space characterized by a union of linear manifold spanned by a set of basis configuration by integrating hand motion constraint an efficient articulated motion capturing algorithm is proposed based on sequential monte carlo technique our experiment show that this algorithm is robust and accurate for tracking natural hand movement this algorithm is easy to extend to other articulated motion capturing task 
the image point in two image satisfy epipolar constraint however not all set of point satisfying epipolar constra int correspond to any real geometry because there can exist no camera and scene point projecting to given image point such that all image point have positive depth using the cheirality theory due to hartley and previous work on oriented projective geometry we give necessary and sufficient condition for an image point set to correspond to any real geometry for image from conventional camera this condition is simple and given in term of epipolar line and epipoles surprisingly this is not sufficient for central panoramic camera apart from giving the insight to epipolar geometry among the application are reducing the search space and ruling out impossible match in stereo and ruling out impossible solution for a fundamental matrix computed from seven point 
the paper present a novel technique of constrained independent component analysis cica to introduce constraint into the classical ica and solve the constrained optimization problem by using lagrange multiplier method this paper show that cica can be used to order the resulted independent component in a specific manner and normalize the demixing matrix in the signal separation procedure it can systematically eliminate the ica s indeterminacy on permutation and dilation the experiment 
by thinking of each state in a hidden markov model a corresponding to some spatial region of a fictitious topology space it is possible to naturally define neighbouring state a those which are connected in that space the transition matrix can then be constrained to allow transition only between neighbour this mean that all valid state sequence correspond to connected path in the topology space i show how such constrained hmms can learn to discover underlying structure in complex sequence of high dimensional data and apply them to the problem of recovering mouth movement from acoustic in continuous speech latent variable model for structured sequence data structured time series are generated by system whose underlying state variable change in a continuous way but whose state to output mapping are highly nonlinear many to one and not smooth probabilistic unsupervised learning for such sequence requires model with two essential feature latent hidden variable and topology in those variable hidden markov model hmms can be thought of a dynamic generalization of discrete state static data model such a gaussian mixture or a discrete state version of linear dynamical system ldss which are themselves dynamic generalization of continuous latent variable model such a factor analysis while both hmms and ldss provide probabilistic latent variable model for time series both have important limitation traditional hmms have a very powerful model of the relationship between the underlying state and the associated observation because each state store a private distribution over the output variable this mean that any change in the hidden state can cause arbitrarily complex change in the output distribution however it is extremely difficult to capture reasonable dynamic on the discrete latent variable because in principle any state is reachable from any other state at any time step and the next state depends only on the current state ldss on the other hand have an extremely impoverished representation of the output a a function of the latent variable since this transformation is restricted to be global and linear but it is somewhat easier to capture state dynamic since the state is a multidimensional vector of continuous variable on which a matrix flo w is acting this enforces some continuity of the latent variable across time constrained hidden markov model address the modeling of state dynamic by building some topology into the hidden state representation the essential idea is to constrain the transition parameter of a conventional hmm so that the discretevalued hidden state evolves in a structured way in particular below i consider parameter restriction which constrain the state to evolve a a discretized version of a continuous multivariate variable i e so that it inscribes only connected path in some space this lends a physical interpretation to the discrete state trajectory in an hmm a standard trick in traditional speech application of hmms is to use left to right transition matrix which are a special case of the type of constraint investigated in this paper however leftto right bakis hmms force state trajectory that are inherently one dimensional and uni directional whereas here i also consider higher dimensional topology and free omni directional motion 
statistic based colour constancy algorithm work well aslong a there are many colour in a scene they fail however when theencountering scene comprise few surface in contrast physic basedalgorithms based on an understanding of physical process such ashighlights and interreflection are theoretically able to solve for colourconstancy even when there are a few a two surface in a scene unfortunately physic based theory rarely work outside the lab in thispaper we 
the proliferation of hypertext and the popularity of kleinberg s hit algorithm have brought about an increased inter est in link analysis while hit and it older relative from the bibliometrics literature provide a method for finding au thoritative source on a particular topic they do not allow individual user to inject their own opinion about what sourc e are authoritative this paper present a technique that inc orporates user feedback by adjusting the measure of authority to match an individual s internal notion of what source are important by lifting the authority of a few user specifie d source the eigenvectors of the entire link matrix are realigned resulting in a computationally cheap method that is much more rich than simple spreading activation we present experimental result based on a database of about one million reference collected a part of the cora on line index of the computer science literature 
this paper present several technique for performing automatic coreference annotation and performance result for each of them to demonstrate that they can be applied to real world data we have built a simple question answering system which us the technique a system using coreference is compared to a baseline system with the result that the addition of the coreference annotation improves performance 
this paper describes an algorithm for finding face within an image the basis of the algorithm is to run an observation window at all possible position scale and orientation within the image a non linear support vector machine is used to determine whether or not a face is contained within the observation window the non linear support vector machine operates by comparing the input patch to a set of support vector which can be thought of a face and anti face template each support vector is scored by some nonlinear function against the observation window and if the resulting sum is over some threshold a face is indicated because of the huge search space that is considered it is imperative to investigate way to speed up the support vector machine within this paper we suggest a method of speeding up the non linear support vector machine a set of reduced set vector rv s are calculated from the support vector by considering the rv s sequentially and if at any point a face is deemed too unlikely to cease the sequential evaluation obviating the need to evaluate the remaining rv s the idea being that we only need to apply a subset of the rv s to eliminate thing that are obviously not a face thus reducing the computation the key then is to explore the rv s in the right order and a method for this is proposed 
the medial surface of a volumetric object is of signicant interest for shape analysis however it numerical computation can be subtle method based on voronoi technique preserve the object s topology but heuristic pruning measure are introduced to remove unwanted face approach based on euclidean distance function can localize medial surface point accurately but often at the cost of altering the object s topology in this paper we introduce a new algorithm for computing medial surface which address these concern the method is robust and accurate ha low computational complexity and preserve topology the key idea is to measure the net outward flux of a vector eld per unit volume and to detect location where a conservation of energy principle is violated this is done in conjunction with a thinning process applied in a cubic lattice we illustrate the approach with example of medial surface of synthetic object and complex anatomical structure obtained from medical image 
the task of multi camera surveillance is to reconstruct the path taken by all moving object that are temporarily visible from multiple non overlapping camera we present a bayesian formalization of this task where the optimal solution is the set of object path with the highest posterior probability given the observed data we show how to eciently approximate the maximum a posteriori solution by linear programming and present initial experimental result multi camera surveillance 
capturing surface appearance is important for a large number of application appearance of real world surface is difcult to model a it varies with the direction of illumination a well a the direction from which it is viewed consequently measurement of thebrdf bidirectional reectance distribution function have been important in addition many application require measuring how the entire surface reects light i e spatially varying brdf measurement are important a well for compactness we refer to a spatially varying brdf a a btf bidirectional texture function measurement of brdf and or btf typically require signicant resource in time and equipment in this work a device for brdf btf measurement is presented that is compact economical and convenient the device us the approach of curved mirror to remove the need for hemispherical positioning of the camera and illumination source instead simple planar translation of optical component are used to vary the illumination direction and to scan the surface furthermore the measurement process is fast because the device enables simultaneous measurement of multiple viewing direction 
active appearance model aams have been shown to be useful for interpreting image of deformable object here we place the aam matching algorithm in a statistical framework allowing extra constraint to be applied this enables the model to be combined with other method of object location we demonstrate how user interaction can be used to guide the search and give result of experiment showing the effect of constraint on the performance of model matching 
prior knowledge about video structure can be used both a a mean to improve the performance of content analysis and to extract feature tha t allow semantic classification we introduce statistical model fo r two important component of this structure shot duration and activi ty and demonstrate the usefulness of these model by introducing a bayesian formulation for the shot segmentation problem the new formul ations is shown to extend standard thresholding method in an adaptive an d intuitive way leading to improved segmentation accuracy 
we formulate structure from motion a a bayesian inference problem and use a markov chain monte carlo sampler to sample the posterior on this problem this result in a method that can identify both small and large tracker error and yield reconstruction that are stable in the presence of these error furthermore the method give detailed information on the range of ambiguity in structure given a particular dataset and requires no special geometric formulation to cope with degenerate situation motion segmentation is obtained by a layer of discrete variable associating a point with an object we demonstrate a sampler that successfully sample an approximation to the marginal on this domain producing a relatively unambiguous segmentation 
this paper proposes an approach to classification of adjacent segment of a time series a being either of k class we use a hierarchical model that consists of a feature extraction stage and a generative classifier which is built on top of these feature such two stage approach are often used in signal and image processing the novel part of our work is that we link these stage probabilistically by using a latent feature space to use one joint model is a bayesian requirement which ha the advantage to fuse information according to it certainty the classifier is implemented a hidden markov model with gaussian and multinomial observation distribution defined on a suitably chosen representation of autoregressive model the markov dependency is motivated by the assumption that successive classification will be correlated inference is done with markov chain monte carlo mcmc technique we apply the proposed approach to synthetic data and to classification of eeg that wa recorded while the subject performed different cognitive task all experiment show that using a latent feature space result in a significant improvement in generalization accuracy hence we expect that this idea generalizes well to other hierarchical model 
image rectification is the process of warping a pair of stereo image in order to align the epipolar line with the scan line of the image once a pair of image is rectified stereo matching can be implemented in an efficient manner given the epipolar geometry it is straightforward to define a rectifying transformation however many transformation will lead to unwanted image distortion in this paper we present a novel method for stereo rectification that determines the transformation that minimizes the effect of resampling that can impede stereo matching the effect we seek to minimize are the loss of pixel due to under sampling and the creation of new pixel due to over sampling to minimize these effect we parameterize the family of rectification transformation and solve for the one that minimizes the change in local area integrated over the area of the image 
when several agent learn concurrently the payoff received by an agent is dependent on the behavior of the other agent a the other agent learn the reward of one agent becomes non stationary this make learning in multiagent systemsmore difficult than single agent learning a few method how ever are known to guarantee convergence to equilibrium in the limit in such system in this paper we experimentally study one such technique the minimax q in a competitive domain and prove it equivalence with another well known method for competitive domain we study the rate of convergence of minimax q and investigate possible way for increasing the same we also present a variant of the algorithm minimax sarsa and prove it convergence to minimax q value under appropriate condition finally we show that this new algorithm performs better than simple minimax q in a general sum domain a well 
abstract we introduce the problem of discovering functional determinacies that result from rolling up data to a higher abstraction leveldependency rud an example rud is the probability that two file in the same directory have the same file extension is greater than a specific number we show the applicability of ruds for olap and data mining we consider the problem of mining ruds that satisfy specified support and confidence threshold this problem is np hard in the number of attribute we give an algorithm for this problem experimental result show that the algorithm us linear time in the number of tuples of the input database 
the goal of this paper is to argue the need to approach the personalization issue in web application from the very beginning in the application s development cycle since personalization is a critical aspect in many popular domain such a e commerce it important enough that it should be dealt with through a design view rather than only an implementation view which discus mechanism rather than design option we present different scenario of personalization covering most existing application since our design approach is based on the object oriented hypermedia design method we briefly introduce it emphasizing the way in which we build web application model a object oriented view of conceptual model we show how we specify personalized web application by refining view according to user profile or preference we show that an object oriented approach allows maximizing reuse in these specification we discus some implementation aspect and compare our work with related approach and present some concluding remark 
the mutual information of two random variable i and j with joint probability t ij is commonly used in learning bayesian net a well a in many other field the chance t ij are usually estimated by the empirical sampling frequency n ij n leading to a point estimate i n ij n for the mutual information to answer question like is i n ij n consistent with zero or what is the probability that the true mutual information is much larger than the point estimate one ha to go beyond the point estimate in the bayesian framework one can answer these question by utilizing a second order prior distribution p t comprising prior information about t from the prior p t one can compute the posterior p t n from which the distribution p i n of the mutual information can be calculated we derive reliable and quickly computable approximation for p i n we concentrate on the mean variance skewness and kurtosis and non informative prior for the mean we also give an exact expression numerical issue and the range of validity are discussed 
keyphrases are an important mean of document summarization clustering and topic search only a small minority of document have author assigned keyphrases and manually assigning keyphrases to existing document is very laborious therefore it is highly desirable to automate the keyphrase extraction process this paper show that a simple procedure for keyphrase extraction based on the naive bayes learning scheme performs comparably to the state of the art it go on to explain how this procedure s performance can be boosted by automatically tailoring the extraction process to the particular document collection at hand result on a large collection of technical report in computer science show that the quality of the extracted keyphrases improves significantly when domain specific information is exploited 
the problem of multiple global comparison infamilies of biological sequence ha been wellstudied fewer algorithm have been developedfor identifying local consensus patternsor motif in biological sequence these twoimportant problem have different biologicalconstraints and consequently different computationalapproaches the difficulty of findingthe biologically meaningful motif resultsfrom the variation among motif base the alignment of motif position site 
we study property of popular near uniform dirichlet prior for learning undersampled probability distribution on discrete nonmetric space and show that they lead to disastrous result however an occam style phase space argument expands the prior into their infinite mixture and resolve most of the observed problem this lead to a surprisingly good estimator of entropy of discrete distribution 
this paper present a novel way of examining the accuracy of the evaluation measure commonly used in information retrieval experiment it validates several of the rule of thumb experimenter use such a the number of query needed for a good experiment is at least and is better while challenging other belief such a the common evaluation measure are equally reliable a an example we show that precision at document ha about twice the average error rate a average precision ha these result can help information retrieval researcher design experiment that provide a desired level of confidence in their result in particular we suggest researcher using web measure such a precision at document will need to use many more than query or will have to require two method to have a very large difference in evaluation score before concluding that the two method are actually different 
typically commercial web search engine provide very little feedback to the user concerning how a particular query is processed and interpreted specifically they apply key query transformation without the user knowledge although these transformation have a pronounced effect on query result user have very few resource for recognizing their existence and understanding their practical importance we conducted a user study to gain a better understanding of user knowledge of and reaction to the operation of several query transformation that web search engine automatically employ additionally we developed and evaluated transparent query a software system designed to provide user with lightweight feedback about opaque query transformation the result of the study suggest that user do indeed have difficulty understanding the operation of query transformation without additional assistance finally although transparency is helpful and valuable interface that allow direct control of query transformation might ultimately be more helpful for end user 
this paper discus theoretical and experimental aspect of gradient based approach to the direct optimization of policy performance in controlled s we introduce a like algorithm for estimating an approximation to the gradient of the average reward a a function of the parameter of a stochastic policy the algorithm s chief advantage are that it requires only a single sample path of the underlying markov chain it us only one free parameter which ha a natural interpretation in term of bias variance trade off and it requires no knowledge of the underlying state we prove convergence of and show how the gradient estimate produced by can be used in a conjugate gradient procedure to find local optimum of the average reward 
in the perception of gait timing is everything specifically the relative timing of the individual motion in a gait and when event occur periodically a they do in a gait then relative timing is equivalent to phase the importance of phase in gait appears in the medical psychology and computer vision literature the video phase locked loop vpll is a novel system that perceives gait is sensitive to the phase of the component motion of the gait and is model free vplls provide a mechanism to perform two critical task in gait perception frequency entrainment and phase locking a vpll can lock on oscillation in pixel that arise because of oscillatory motion in doing so the vpll match it internal oscillator to the oscillation in pixel intensity thus performing frequency entrainment phase locking occurs a individual phased locked loop at each pixel site lock simultaneously the abundance of data extracted by the vpll make gait recognition possible in this demonstration we show a vpll operating in realtime the vpll lock to oscillation in the gait of a person walking on a treadmill and also detects translational motion a the vpll system extract phase information in the form of a phasor configuration the configuration is also displayed in real time best provides an excellent introduction to phaselocked loop their application to vplls is found in boyd figure a show a superimposed frame an image sequence of an oscillatory motion a person walking on a treadmill the vpll process the sequence locking on the oscillation figure b show the result a the magnitude of the oscillation a determined by the vpll while we have chosen to display the magnitude signal the vpll also capture frequency and phase information there is ample information derived by a vpll to recognize various oscillatory motion we use procrustes shape analysis to perform recognition related task such a averaging and matching the phase pattern that emerge from the vpll our demonstration includes a real time display of the captured phase pattern the variation in phase that arise from different motion is evident a the phase config a 
in an effort to develop measure of discourse level management strategy this study examines a measure of the degree to which decision making interaction consist of sequence of utterance function that are linked in a decision making routine the measure is applied to dyadic interaction elicited in both face to face and computer mediated environment with systematic variation of task complexity and message window size every utterance in the interaction is coded according to a system that identifies decision making function and other routine function of utterance markov analysis of the coded utterance make it possible to measure the relative frequency with which sequence of and utterance trace a path in a markov model of the decision routine these proportion suggest that interaction in all condition adhere to the model although we find greater conformity in the computer mediated environment which is probably due to increased processing and attentional demand for greater efficiency the result suggest that measure based on markov analysis of coded interaction can provide useful measure for comparing discourse level property for correlating discourse feature with other textual feature and for analysis of discourse management strategy 
in the field of spatial reasoning point to point relation have been thoroughly examined but only little attention ha been payed to the modeling of path relation we propose a computational model that extends the existing referential semantics for point to point relation to path relation on the linguistic side we present some research on german path preposition a well a result on their english counterpart this analysis of path preposition is used to extract a semantic model for path relation on the geometric side we examine the characteristic of trajectory and propose a computational method to find an appropriate path relation for a given situation finally we show how our finding on the linguistic and the geometric side can be brought together to form a consistent model 
abstract this paper oers a novel detection method which work well even in the case of a complicated image collection for instance a frontal face under a large class of linear transformation it wa also successfully applied to detect d object under dierent view call the class of image which should be detected a multi template the detection problem is solved by sequentially applying very simple lters or detector which are designed to yield small result on the multi template hence anti face and large result on random natural image this is achieved by making use of a simple probabilistic assumption on the distribution of natural image which is borne out well in practice and by using a simple implicit representation of the multi template only image which passed the threshold test imposed by the rst detector are examined by the second detector etc the detector have the added bonus that they act independently so that their false alarm are uncorrelated this result in a percentage of false alarm which exponentially decrease in the number of detector this in turn lead to a very fast detection algorithm usually requiring n operation to classify an n pixel image where also the algorithm requires no training loop the suggested algorithm s performance favorably compare to the wellknown eigenface and support vector machine based algorithm and it is substantially faster d vernon ed eccv lncs pp 
there is a lot of research on formalization of intention the common idea of these theory is to interprete intention a an unary modal operator in kripkean semantics these theory suffer from the side effect problem seriously we introduce an alternative approach by establishing a nonclassical logic of intention this logic is based on a novel non kripkean semantics which embodies some cognitive feature we show that this logic doe provide a formal specification and a decidable inference mechanism of intention consequence all and only the instance of sideeffects except one in absorbent form are forbidden in the logic 
the information bottleneck methodis an unsupervised non parametric data organization technique given a joint distribution this method construct a new variable that extract partition or cluster over the value of that are informative about in a recent paper we introduced a general principled framework for multivariate extension of the information b ottleneck method that allows u to consider multiple system of data partition th at are inter related in this paper we present a new family of simple agglomerative algorithm to construct such system of inter related cluster we analyze t he behavior of these algorithm and apply them to several real life datasets 
this paper introduces a new mult iview reconstruction problem called approximate n view stereo the goal of this problem is to recover a one parameter family of volume that are increasingly tighter supersets of an unknown arbitrarily shaped d scene by studying d shape that reproduce the input photograph up to a special image transformation called a shuffle transformation we prove that these shape can be organized hierarchically into nested supersets of the scene and they can be computed using a simple algorithm called approximate space carving that is provably correct for arbitrary discrete scene i e for unknown arbitrarily shaped lambertian scene that are defined by a finite set of voxels and are viewed from n arbitrarily distributed viewpoint inside or around them the approach is specifically designed to attack practical reconstruction problem including recovering shape from image with inaccurate calibration information and building coarse scene model from multiple view 
abstract this paper ha been prompted by observation of some anomaly in the performance of the standard imaging model pin hole thin lens and gaussian thick lens in the context of composing omnifocus image and estimating depth map from a sequence of image a closer examination of the model revealed that they a sume a position of the aperture that con icts with the design of many available lens we have shown in this paper that the imaging geometry and photometric property of an image are signi cantly in uenced by the position of the aperture this is con rmed by the discrepancy between observed mapping and those pre dicted by the model we have therefore concluded that the current imaging model do not adequately represent practical imaging system we have proposed a new imaging model which overcomes these de ciencies and have given the associated mapping the impact of this model on some common imaging scenario is described along with experimental veri cation of the better perfor mance of the model on three real lens 
we provide a natural gradient method that represents the steepestdescent direction based on the underlying structure of the parameterspace although gradient method cannot make large changesin the value of the parameter we show that the natural gradientis moving toward choosing a greedy optimal action rather thanjust a better action these greedy optimal action are those thatwould be chosen under one improvement step of policy iterationwith approximate compatible value 
this article describes the construction and performance of granska a surface oriented system for grammar checking of swedish text with the use of carefully constructed error detection rule the system can detect and suggest correction for a number of grammatical error in swedish text in this article we specifically focus on how erroneously split compound and noun phrase disagreement are handled in the rule the system combine probabilistic and rule based method to achieve high efficiency and robustness this is a necessary prerequisite for a grammar checker that will be used in real time in direct interaction with user we hope to show that the granska system with higher efficiency can achieve the same or better result than system that use rule based parsing alone part of this work were presented at nodalida domeij et al in this article another grammar checker for swedish is presented this grammar checker called granska ha been developed at kth for about four year we will first present the structure of granska and then in more detail describe four important part of the system the part of speech tagging module the construction of error detection rule the algorithm for rule matching and the generation of error correction finally we describe the performance of the tagging error detection and np recognition 
numerical function approximation over a boolean domain is a classical problem with wide application to data modeling task and various form of learning a great many function approximation algorithm have been devised over the year because the goal is to produce an approximating function that ha low expected error algorithm are typically guided by error reduction this guiding force to reduce error can bias the algorithm in a detrimental manner we illustrate this bias and then propose an alternative approach based on a notion of value unification 
a lot of recent research ha focused on method of modeling web user and on efficient way to initialize and manage user model in this paper we present a new user modeling technique relying on a temporal graph based data model for semistructured information our technique can be used to filter www information and to make the web experience personalized for the individual user moreover such graph based data model also offer appropriate query language which allow user defined query either over the whole web site or over the personalized user site view 
the success of the world wide web measured in term of the number of it user and of the resulting traffic increase is only commensurate to the patience required when sitting in front of one s computer waiting for a document to be down loaded if one could identify the typical access pattern for a set of document on a web server the server could use or extend the existing protocol to accordingly pre fetch or push document to the browser and proxy server in this paper we present and evaluate a strategy for making web server pushier which document is to be pushed is determined by a set of association rule mined from a sample of the access log of the web server once a rule of the form document a document b ha been identified and selected the web server decides to push document if document is requested the strategy is individual user oriented while not ignoring the aggregate perspective we evaluate the effectiveness and the cost of such a strategy for two architecture a two tier web server web browser architecture and a three tier web server proxy server web browser architecture we consider different setting in the architecture a well a refinement of the strategy taking into account the size of the document 
in this paper we construct a new concept description language intended for representing dynamic and intensional knowledge the most important feature distinguishing this language from it predecessor in the literature is that it allows application of modal operator to all kind of syntactic term concept role and formula moreover the language may contain both local i e state dependent and global i e state independent concept role and object all this provides u with the most complete and natural mean for reflecting the dynamic and intensional behaviour of application domain we construct a satisfiability checking mosaic type algorithm for this language based on alc in i arbitrary multimodal frame ii frame with universal accessibility relation for knowledge and iii frame with transitive symmetrical and euclidean relation for belief on the other hand it is shown that the satisfaction problem becomes undecidable if the underlying frame are arbitrary linear order or the language contains the common knowledge operator for n agent 
the partition function for a boltzmann machine can be boundedfrom above and below we can use this to bound the mean andthe correlation for network with small weight the value ofthese statistic can be restricted to non trivial region i e a subsetof gamma experimental result show that reasonable boundingoccurs for weight size where mean field expansion generally givegood result 
we present a general architecture for efficient and deterministic morphological analysis based on memory based learning and apply it to morphological analysis of dutch the system make direct mapping from letter in context to rich category that encode morphological boundary syntactic class label and spelling change both precision and recall of labeled morpheme are over on held out dictionary test word and estimated to be over in free text 
localization is one of the most important capability for autonomous mobile agent markov localization ml applied to dense range image ha proven to be an effective technique but it computational and storage requirement put a large burden on robot system and make it difficult to update the map dynamically in this paper we introduce a new technique based on correlation of a sensor scan with the map that is several order of magnitude more efficient than ml cbml correlation based ml permit video rate localization using dense range scan dynamic map update and a more precise error model than ml in this paper we present the basic method of cbml and validate it efficiency and correctness in a series of experiment on an implemented mobile robot base 
in image matching application such a tracking and stereo matching it is common to use the sum of squareddiflerences ssd measure to determine the best match for an image template however this measure is sensitive to outlier and is not robust to template variation we describe a robust measure and eficient search strategy for template matching with a binary or greyscale template using a maximum likelihood formulation in addition to subpixel localization and uncertainty estimation these technique allow optimal feature selection based on minimizing the localization uncertainty we examine the use of these technique for object recognition stereo matching feature selection and tracking 
we introduce the mixture of gaussian process mgp model which is useful for application in which the optimal bandwidth of a map is input dependent the mgp is derived from the mixture of expert model and can also be used for modeling general conditional probability density we discus how gaussian process in particular in form of gaussian process classi cation the support vector machine and the mgp model can be used for quantifying the dependency in graphical model 
in this paper we shall focus here on mixturesof factor analyzer from the perspectiveof a method for model based density estimationfrom high dimensional data and hencefor the clustering of such data this modelenables a normal mixture model to be ttedto high dimensional data the number of freeparameters is controlled through the dimensionof the latent factor space by workingin this reduced space it allows an interpolationin model complexity from isotropic tofull 
we study distributional similarity measure for the purpose of improving probability estimation for unseen cooccurrences our contribution are three fold an empirical comparison of a broad range of measure a classification of similarity function based on the information that they incorporate and the introduction of a novel function that is superior at evaluating potential proxy distribution 
this paper present the measurement of object reflectance from color image we exploit the gaussian scalespace paradigm to define a framework for the robust measurement of object reflectance from color image illumination and geometrical invariant property are derived from a physical reflectance model based on the kubelka munk theory imaging condition are assumed to be white illumination and matte dull object or general object respective ly summarized by shadow highlight illumination intensity illumination color 
this paper is concerned with extendingneural network to multi instance learning in multi instance learning each example corresponds to a set of tuples in asingle relation furthermore example are classied a positive if at least onetuple i e at least one attribute value pair satises certain condition if none ofthe tuples satisfy the requirement the example is classied a negative we willstudy how to extend standard neural network and backpropagation to multiinstance 
we propose a mixed language query disambiguation approach by using co occurrence information from monolingual data only a mixed language query consists of word in a primary language and a secondary language our method translates the query into monolingual query in either language two novel feature for disambiguation namely contextual word voting and best contextual word are introduced and compared to a baseline feature the nearest neighbor average query translation accuracy for the two feature are and compared to the baseline accuracy of 
despite the wide application of bilinear problem to problem both in computer vision and in other field their behaviour under the effect of noise is still poorly understood in this paper we show analytically that marginal distribution on the solution component of a bilinear problem can be bimodal even with gaussian measurement error we demonstrate and compare three different method of estimating the covariance of a solution we show that the hessian at the mode substantially underestimate covariance many problem in computer vision can be posed a bilinear problem i e one must find a solution to a set of equation of the form ck ij gijkaibj 
information retrieval query often result in a large number of document found to be relevant these document are usually sorted by relevance not by an analysis of what the user meant if the document collection contains many document on one of those meaning it is hard to find other document we present a technique called conceptual grouping that automatically distinguishes between different meaning of a user query given a document collection by analysing a word co occurrence network of a text database we are able to form group of word related to the query grouped by semantic coherence these group are used to reorganise the result according to what the user ha meant by his query testing show that this automated technique can improve precision help user find what they need more easily and give them a semantic overview of the document collection 
in this paper a robust pattern recognition system using an appearance based representation of colour image is described standard appearance based approach are not robust to outlier occlusion or segmentation error the approach proposed here relies on robust m estimator involving non quadratic and possibly non convex energy function to deal with the minimisation of non convex function in a deterministic framework we introduce an estimation scheme relying on m estimator used in continuation from convex function to hard redescending nonconvex estimator at each step of the robust estimation scheme the non quadratic criterion is minimized using the half quadratic theory this lead to a weighted least square algorithm which is easy to implement the proposed robust estimation scheme doe not require any user interaction because all necessary parameter are previously estimated the method is illustrated on a road sign recognition application experiment show significant improvement with respect to standard estimation scheme 
a large portion of real world data is stored in commercial relational database system in contrast most statistical learning method work only with flat data representation thus to apply these method we are forced to convert our data into a flat form thereby losing much of the relational structure present in our database this paper build on the recent work on probabilistic relational model prms and describes how to learn them from database prms allow the property of an object to depend probabilistically both on other property of that object and on property of related object although prms are significantly more expressive than standard model such a bayesian network we show how to extend well known statistical method for learning bayesian network to learn these model we describe both parameter estimation and structure learning the automatic induction of the dependency structure in a model moreover we show how the learning procedure can exploit standard database retrieval technique for efficient learning from large datasets we present experimental result on both real and synthetic relational database 
we describe a class of probabilistic model that we call credibility network using parse tree a internal representation of image credibility network are able to perform segmentation and recog nition simultaneously removing the need for ad hoc segmentation heuristic promising result in the problem of segmenting hand written digit were obtained 
evidence from neurophysiological and psychological study is coming together to shed light on how we represent and recognize object this review describes evidence supporting two major hypothesis the first is that object are represented in a mosaic like form in which object are encoded by combination of complex reusable feature rather than two dimensional template or three dimensional model the second hypothesis is that transform invariant representation of object are learnt through experience and that this learning is affected by the temporal sequence in which different view of the object are seen a well a by their physical appearance 
a learning account for the problem of object recognitionis developed within the pac probably approximatelycorrect model of learnability the proposedapproach make no assumption on the distribution ofthe observed object but quanties success relative toits past experience most importantly the success oflearning an object representation is naturally tied tothe ability to represent it a a function of some intermediaterepresentations extracted from the image we evaluate this 
in this paper we present our project for realizing virtualized reality in a large scale space such a a soccer stadium concert hall etc we have developed an extended system of cmu d room in an efficient way so that d digitization of a large scale space can practically realized we place progressive scan camera around a large space e g m m with m height the video from those camera are digitized into pc that are connectedvia ethernet a camera calibration technique for a large scale space that is accurate enoughto apply cv based algorithm is developed we also present a method to omit the calibration process with an idea of projective grid space the recent result and future work are presented for demonstrating the significance of the project a a cv based practical application 
in this paper we describe a logic based ai architecturebased on brook subsumption architecture we axiomatizeeach of the layer of control in his system separatelyand use independent theorem provers to deriveeach layer s output given it input we implementthe subsumption of lower layer by higher layer usingcircumscription to make assumption in lower levelsand nonmonotonically retract them when higher levelscome up with some new conclusion we give formalsemantics to our 
statistic based classifier in natural language are developed typically by assuming a generative model for the data estimating it parameter from training data and then using bayes rule to obtain a classifier for many problem the assumption made by the generative model are evidently wrong leaving open the question of why these approach work this paper present a learning theory account of the major statistical approach to learning in natural language a class of linear statistical query lsq hypothesis is defined and learning with it is shown to exhibit some robustness property many statistical learner used in natural language including naive bayes markov model and maximum entropy model are shown to be lsq hypothesis explaining the robustness of these predictor even when the underlying probabilistic assumption do not hold this coherent view of when and why learning approach work in this context may help to develop better learning method and an understanding of the role of learning in natural language inference 
extracting rule from rbfs is not a trivial task because of nonlinear function or high input dimensionality in such case some of the hidden unit of the rbf network have a tendency to be shared across several output class or even may not contribute 
we describe a learning based method for low level vision problem estimating scene from image we generate a synthetic world of scene and their corresponding rendered image modeling their relationship with a markov network bayesian belief propagation allows u to efficiently find a local maximum of the posterior probability for the scene given an image we call this approach vista vision by image scene training we apply vista to the super resolution problem estimating high frequency detail from a low resolution image showing good result to illustrate the potential breadth of the technique we also apply it in two other problem domain both simplified we learn to distinguish shading from reflectance variation in a single image under particular lighting condition for the motion estimation problem in a blob world we show figure ground discrimination solution of the aperture problem and filling in arising from application of the same probabilistic machinery 
in this paper we present and compare various single word based alignment model for statistical machine translation we discus the five ibm alignment model the hidden markov alignment model smoothing technique and various modification we present different method to combine alignment a evaluation criterion we use the quality of the resulting viterbi alignment compared to a manually produced reference alignment we show that model with a first order dependence and a fertility model lead to significantly better result than the simple model ibm or ibm which are not able to go beyond zero order dependency 
a number of experimental study e g geldard and sher rick kolers and von gr unau have suggested that interpretation of event can override direct sensory evi dence for example for some sequence of perceptual event of short duration the interpretation of individual event in the sequence depends on the characteristic of the sequence a a whole this backwards referral in time in which later event influence the perception of earlier event is difficult to account for within a serial model of cognition without incor porating implausible delay basically delaying sensory expe rience until all the data is in dennett and kinsbourne have proposed the multiple draft theory a a way of modelling such cognitive process the multiple draft theory is based on a paral lel distributed view of cognition in which large number of process work independently on multiple interpretation of data simultaneously these are the multiple draft eventu ally a single draft may become dominant but no draft is ever entirely safe from revision 
mobile internet technology such a wap are important for pervasive anytime anywhere computing although much progress ha been made in term of technological innovation many of mobile internet system are difficult to use lack flexibility and robustness they give a poor user experience evaluation and theoretical analysis of usability combined with innovative design can achieve significant improvement in user performance and satisfaction using such multidisciplinary method explains the negative reaction to wap and more constructively suggest way of developing more effective and efficient device and service 
swiftfile is an intelligent assistant that help user organize their e mail into folder swiftfile us a text classifier to predict where each new message is likely to be filed by the user and provides shortcut button to quickly file message into one of it predicted folder one of the challenge faced by swiftfile is that the user s mail filing habit are constantly changing user are frequently creating deleting and rearranging folder to meet their current filing need in this paper we discus the importance of incremental learning in swiftfile we present several criterion for judging how well incremental learning algorithm adapt to quickly changing data and evaluate swiftfile s classifier using these criterion we find that swiftfile s classifier is surprisingly responsive and doe not require the extensive training that is often assumed in most learning system 
in this work we introduce an information theoretic based correction term to the likelihood ratio classification method for multiple class under certain condition the term is sufficient for optimally correcting the difference between the true and estimated likelihood ratio and we analyze this in the gaussian case we find that the new correction term significantly improves the classification result when tested on medium vocabulary speech recognition task moreover the addition of this term make the class comparison analogous to an intransitive game and we therefore use several tournament like strategy to deal with this issue we find that further small improvement are obtained by using an appropriate tournament lastly we find that intransitivity appears to be a good measure of classification confidence 
in this paper we address the problem of extracting key piece of information from voicemail message such a the identity and phone number of the caller this task differs from the named entity task in that the information we are interested in is a subset of the named entity in the message and consequently the need to pick the correct subset make the problem more difficult also the caller s identity may include information that is not typically associated with a named entity in this work we present three information extraction method one based on hand crafted rule one based on maximum entropy tagging and one based on probabilistic transducer induction we evaluate their performance on both manually transcribed message and on the output of a speech recognition system 
this paper describes an approach to machine translation that place linguistic information at it foundation the difficulty of translation from english to japanese is illustrated with data that show the influence of various linguistic contextual factor next a method for natural language transfer is presented that integrates translation example represented a typed feature structure with source target index with linguistic rule and constraint the method ha been implemented and the result of an evaluation are presented 
the image irradiance of a three dimensional object is known to be the function of three component the distribution of light source the shape and reflectance of a real object surface in the past recovering the shape and reflectance of an object surface from the recorded image brightness ha been intensively investigated on the other hand there ha been little progress in recovering illumination from the knowledge of the shape and reflectance of a real object in this paper we propose a new method for estimating the illumination distribution of a real scene from image brightness observed on a real object surface in that scene more specifically we recover the illumination distribution of the scene from a radiance distribution inside shadow cast by an object of known shape onto another object surface of known shape and reflectance by using the occlusion information of incoming light we are able to reliably estimate the illumination distribution of a real scene even in a complex illumination environment 
given a set of d point that we know lie on thesurface of an object we can define many possible surfacesthat pas through all of these point even whenwe consider only surface triangulation there are stillan exponential number of valid triangulation that allfit the data each triangulation will produce a differentfaceted surface connecting the point our goal is to overcome this ambiguity and find theparticular surface that is closest to the true object surface we do not know 
digital watermark have been proposed a a methodfor discouraging illicit copying and distribution ofcopyrighted material and to create secure digital imagelibraries by adding to image copyright and userrightinformation using a robust digital watermark todetect and trace copyright violation ha therefore lotof interest this paper describes an approach to embeddinga digital watermark using the fourier transform the paper also address the difficult problemof oblivious watermark 
a new fully automated shape learning method is presented it is based on clustering a set of trainingshapes in the original shape space defined by the coordinate of the contour point and performinga procrustes analysis on each cluster to obtain cluster prototype average object and statisticalinformation about intra cluster shape variation the main difference from previously reported methodsis that the training set is first automatically clustered and those shape considered to be 
the incremental algorithm introduced in dale and reiter for producing distinguishing description doe not always generate a minimal description in this paper i show that when generalised to set of individual and disjunctive property this approach might generate unnecessarily long and ambiguous and or epistemically redundant description i then present an alternative constraint based algorithm and show that it build on existing related algorithm in that i it produce minimal description for set of individual using positive negative and disjunctive property ii it straightforwardly generalises to n ary relation and iii it is integrated with surface realisation 
natural language generation from flat semantics is an np complete problem this make it necessary to develop algorithm that run with reasonable efficiency in practice despite the high worst case complexity we show how to convert tag generation problem into dependency parsing problem which is useful because optimization in recent dependency parser based on constraint programming tackle exactly the combinatorics that make generation hard indeed initial experiment display promising runtimes 
this paper present a generalized associative memory model which store a collection of tuples whose component are set rather than scalar it is shown that all library pattern are stored stably on the other hand spurious memory may develop application of this model to storage and retrieval of naturally arising generalized sequence in bioinformatics are presented the model is shown to work well for detection of novel generalized sequence against a large database of stored sequence and for removal of noisy black pixel in a probe image against a very large set of stored image 
in this work we examine evidence combination mechanism for classifying multimedia information in particular we examine linear and dempster shafer method of evidence combination in the context of identifying personal image on the world wide web an automatic web search engine named diogenes search the web for personal image and combine different piece of evidence for identification the source of evidence consist of input from face detection recognition and text html analysis module a degree of uncertainty is involved with both of these source diogenes automatically determines the uncertainty locally for each retrieval and us this information to set a relative significance for each evidence to our knowledge diogenes is the first image search engine using dempster shafer evidence combination based on automatic object recognition and dynamic local uncertainty assessment in our experiment diogenes comfortably outperformed some well known commercial and research prototype image search engine for celebrity image query 
several geometric active contour model have been proposed for segmentation in computer vision and image analysis the essential idea is to evolve a curve in d or a surface in d under constraint from image force so that it cling to feature of interest in an intensity image recent variation on this theme take into account property of enclosed region and allow for multiple curve or surface to be simultaneously represented however it is still unclear how to apply these technique to image of narrow elongated structure such a blood vessel where intensity contrast may be low and reliable region statistic cannot be computed to address this problem we derive the gradient flow which maximize the rate of increase of flux of an appropriate vector field through a curve in d or a surface in d the key idea is to exploit the direction of the vector field along with it magnitude the calculation lead to a simple and elegant interpretation which is essentially parameter free and ha the same form in both dimension we illustrate it advantage with several level set based segmentation of d and d angiography image of blood vessel 
we present an improvement of noviko s perceptron convergencetheorem reinterpreting this mistake bound a a margin dependentsparsity guarantee allows u to give a pacstyle generalisation errorbound for the classier learned by the perceptron learning algorithm the bound value crucially depends on the margin a supportvector machine would achieve on the same data set using the samekernel ironically the bound yield better guarantee than are currentlyavailable for the support 
abstract we introduce a fast multiscale algorithm for image segmentation our algorithm us modern numeric technique to nd an approximate solution to normal ized cut measure in time that is linear in the size of the image with only a few dozen operation per pixel in just one pas the algorithm provides a complete hi erarchical decomposition of the image into segment the algorithm detects the segment by applying a pro ce of recursive coarsening in which the same mini mization problem is represented with fewer and fewer variable producing an irregular pyramid during this coarsening process we may compute additional inter nal statistic of the emerging segment and use these statistic to facilitate the segmentation process once the pyramid is completed it is scanned from the top down to associate pixel close to the boundary of seg ments with the appropriate segment the algorithm is inspired by algebraic multigrid amg solver of min imization problem of heat or electric network we demonstrate the algorithm by applying it to real im age 
we show how to use a sampling method to find sparsely clad people in static image people are modeled a an assembly of nine cylindrical segment segment are found using an em algorithm and then assembled into hypothesis incrementally using a learned likelihood model each assembly step pass on a set of sample of it likelihood to the next this yield effective pruning of the space of hypothesis the collection of available nine segment hypothesis is then represented by a set of equivalence class which yield an efficient pruning process the posterior for the number of people is obtained from the class representative people are counted quite accurately in image of real scene using an map estimate we show the method allows top down a well a bottom up reasoning while the method can be overwhelmed by very large number of segment we show that this problem can be avoided by quite simple pruning step 
abstract we introduce a method of feature selection for support vector machine the method is based upon finding those feature which minimize bound on the leave one out error this search can be efficiently performed via gradient descent the resulting algorithm are shown to be superior to some standard feature selection algorithm on both toy data and real life problem of face recognition pedestrian detection and analyzing dna microarray data 
in this paper we present the feature of a question answering q a system that had unparalleled performance in the trec evaluation we explain the accuracy of our system through the unique characteristic of it architecture usage of a wide coverage answer type taxonomy repeated passage retrieval lexico semantic feedback loop extraction of the answer based on machine learning technique and answer caching experimental result show the effect of each feature on the overall performance of the q a system and lead to general conclusion about q a from large text collection 
current vision system are designed to perform in clear weather needle to say in any outdoor application there is no escape from bad weather ultimately computer vision system must include mechanism that enable them to function even if somewhat le reliably in the presence of haze fog rain hail and snow we begin by studying the visual manifestation of different weather condition for this we draw on what is already known about atmospheric optic next we identify effect caused by bad weather that can be turned to our advantage since the atmosphere modulates the information carried from a scene point to the observer it can be viewed a a mechanism of visual information coding based on this observation we develop model and method for recovering pertinent scene property such a threedimensional structure from image taken under poor weather condition 
abstract a robust integrative algorithm is presented for computing the position of the focus of expansion or axis of rotation the singular point in optical flow field such a those generated by self motion measurement are shown of a fully parallel cmos analog vlsi motion sensor array which computes the direction of local motion sign of optical flow at each pixel and can directly implement this algorithm the flow field singular point is computed in real time with a power consumption of le than computation of the singular point for more general flow field requires measure of field expansion and rotation which it is shown can also be computed in real time hardware again using only the sign of the optical flow field these measure along with the location of the singular point provide robust real time self motion information for the visual guidance of a moving platform such a a robot 
we consider the interaction between edge and intensity distribution in semi open image neighborhood surrounding them locally this amount to a kind of figure ground problem and we analyze the case of smooth figure occluding arbitrary background technique from differential topology permit a classification into what we call fold the side of an edge from a smooth object and cut the arbitrary background intuitively cut arise when an arbitrary scene is cut from view by an occluder the condition take the form of transversality between an edge tangent map and a shading flow field and example are included 
we consider the problem of how an agent creates a discrete spatial representation from it continuous interaction with the environment such representation will be the minimal one that explains the experience of the agent in the environment in this 
the use of pipeline is an important technique in contemporary hardwaredesign particularly at the level of register transfer logic rtl earlierformal analysis e g using the acl theorem prover showed correctnessof pipelined floating point rtl this paper extends that workby considering a notion of a conditional pipeline essentially the result ofsharing hardware among several distinct pipeline we have employed apipeline tool written in acl but completely unverified 
realistic application of nearest neighborclassiers suer from capacity related problem the size of today s data warehousesrenders loading the entire data into the mainmemory impossible moreover comparingeach object with million of stored examplescan be prohibitively expensive some researchershave therefore developed methodsthat replace large set of example with theirrepresentative subset in this paper we suggestan approach that selects three very smallgroups of 
thresholding strategy in automated text categorization are an underexplored area of research this paper present an examination of the effect of thresholding strategy on the performance of a classifier under various condition using k nearest neighbor knn a the classifier and five evaluation benchmark collection a the testbets three common thresholding method were investigated including rank based thresholding rcut proportion based assignment pcut and score based local optimization scut in addition new variant of these method are proposed to overcome significant problem in the existing approach experimental result show that the choice of thresholding strategy can significantly influence the performance of knn and that the optimal strategy may vary by application scut is potentially better for fine tuning but risk overfitting pcut cope better with rare category and exhibit a smoother trade off in recall versus precision but is not suitable for online decision making rcut is most natural for online response but is too coarse grained for global or local optimization rtcut a new method combining the strength of category ranking and scoring outperforms both pcut and rcut significantly 
in the world wide web myriad of hyperlink connect document and page to create an unprecedented highly complex graph structure the web graph this paper present a novel approach to learning probabilistic model of the web which can be used to make reliable prediction about connectivity and information content of web document the proposed method is a probabilistic dimension reduction technique which recasts and unites latent semantic analysis and kleinberg s hub and authori tie algorithm in a statistical setting this is meant to be a first step towards the development of a statistical foundation for web related information technology although this paper doe not focus on a particular application a variety of algorithm operating in the web internet environment can take advantage of the presented technique including search engine web crawler and information agent system 
most real world data is stored in relational form in contrast most statistical learning method work with flat data representation forcing u to convert our data into a form that loses much of the relational structure the recently introduced framework of probabilistic relational model prms allows u to represent probabilistic model over multiple entity that utilize the relation between them in this paper we propose the use of probabilistic model not only for the attribute in a relational model but for the relational structure itself we propose two mechanism for modeling structural uncertainty reference uncertainty and existence uncertainty we describe the appropriate condition for using each model and present learning algorithm for each we present experimental result showing that the learned model can be used to predict relational structure and moreover the observed relational structure can be used to provide better prediction for the attribute in the model 
a fundamental difficulty faced by group of agent that work together is how to efficiently coordinate their effort this paper present technique that allow heterogeneous agent to more efficiently solve coordination problem by acquiring procedural knowledge in particular each agent autonomously learns coordinated procedure that reflect her contribution towards successful past joint behavior empirical result validate the significant benefit of coordinated procedure 
inverse document frequency idf is a popular measure of a word s importance the idf invariably appears in a host of heuristic measure used in information retrieval however so far the idf ha itself been a heuristic in this paper we show idf to be optimal in a principled sense we show that idf is the optimal weight of a word with respect to minimization of a kullback leibler distance suitably generalized to nonnegative function which need not be probability distribution this optimization problem is closely related to maximum entropy problem we show that the idf is the optimal weight associated with a word feature in an information retrieval setting where we treat each document a the query that retrieves itself that is idf is optimal for document self retrieval 
an important issue in applying svms to speech recognition is the ability to classify variable length sequence this paper present extension to a standard scheme for handling this variable length data the fisher score a more discriminatory mapping is introduced based on the likelihood ratio the score space de ned by this mapping avoids some of the problem with the fisher score it also allows the discriminative power of the generative model to be directly incorporated into the 
we describe a new system for estimating road shape ahead of a vehicle for the purpose of driver assistance the method utilises a single on board colour camera together with inertial and velocity information to estimate both the position of the host car with respect to the lane it is following and also the width and curvature of the lane ahead at distance of up to metre the system s image processing extract a variety of different style of lane marking from road imagery and is able to compensate for a range of lighting condition road shape and car position are estimated using a particle filter the system which run at frame per second ha been applied with some success to several hour worth of data captured from highway under varying imaging condition any of the target detected by the radar system present an obstacle to the vehicle s motion and to alert the driver if such condition arise in this paper we present a vision algorithm which is able to estimate the vehicle s position bearing and lateral offset with respect to the centre of it current highway lane together with the pitch of the camera and width of the lane the curvature and rate of change of curvature of the lane up to metre ahead is also estimated to coincide with the field of view of the radar obstacle detection system the system ha been demonstrated on several hour worth of data captured from the test vehicle and is found to perform stably in a range of road and weather condition we review related work below before giving detail of the algorithm and some example of it operation 
in this paper we propose a general framework for distributed boosting intended for efficient integrating specialized classifier learned over very large and distributed homogeneous database that cannot be merged at a single location our distributed boosting algorithm can also be used a a parallel classification technique where a massive database that cannot fit into main computer memory is partitioned into disjoint subset for a more efficient analysis in the proposed method at each boosting round the classifier are first learned from disjoint datasets and then exchanged amongst the site finally the classifier are combined into a weighted voting ensemble on each disjoint data set the ensemble that is applied to an unseen test set represents an ensemble of ensemble built on all distributed site in experiment performed on four large data set the proposed distributed boosting method achieved classification accuracy comparable or even slightly better than the standard boosting algorithm while requiring le memory and le computational time in addition the communication overhead of the distributed boosting algorithm is very small making it a viable alternative to the standard boosting for large scale database 
when an unknown object with lambertian reflectance is viewedorthographically there is an implicit ambiguity in determining it d structure we show that the object s visible surface f x y is indistinguishable from a generalized ba relief transformation of the object s geometry bar f x y equal f x y x y and a corresponding transformation on the object s albedo for each image of the object illuminated by an arbitrary number of distant light source there exists anidentical image of the transformed object illuminated by similarlytransformed light source this result hold both for theilluminated region of the object a well a those in cast andattached shadow furthermore neither small motion of the object nor of the viewer will resolve the ambiguity in determining theflattening or scaling of the object s surface implication of this ambiguity on structure recovery and shape representation are discussed 
we introduce a new categorial formalism based on intuitionistic linear logic this formalism which derives from current type logical grammar is abstract in the sense that both syntax and semantics are handled by the same set of primitive a a consequence the formalism is reversible and provides different computational paradigm that may be freely composed together 
we propose a general approach for estimating the parameter of latentvariable probability model to maximize conditional likelihoodand discriminant criterion unlike joint likelihood these objectivesare better suited for classification and regression the approachutilizes and extends the previously introduced cem framework conditional expectation maximization which reformulates emto handle the conditional likelihood case we generalize the cemalgorithm to estimate any mixture of 
the eikonal equation and variant of it are of significant interest for problem in computer vision and image processing it is the basis for continuous version of mathematical morphology stereo shape from shading and for recent dynamic theory of shape it numerical simulation can be delicate owing to the formation of singularity in the evolving front and is typically based on level set method however there are more classical approach rooted in hamiltonian physic which have received little consideration in computer vision in this paper we first introduce a new algorithm for simulating the eikonal equation which offer a number of computational and conceptual advantage over the earlier method when it come to shock tracking next we introduce a very efficient algorithm for shock detection where the key idea is to measure the net outward flux of a vector field per unit volume and to detect location where a conservation of energy principle is violated we illustrate the approach with several numerical example including skeleton of complex d and d shape 
we are developing a system to extract geodetic texturedcad model from thousand of initially uncontrolled close range ground and aerial image of urbanscenes here we describe one component of the system which operates after the imagery ha been controlledor geo referenced this fully automatic componentdetects significant vertical facade in the scene then extrudes them to meet an inferred triangulatedterrain and procedurally generated roof polygon thealgorithm then estimate for 
most of the current work on corpus annotation is concentrated on morphemics lexical semantics and sentence structure however it becomes more and more obvious that attention should and can be also paid to phenomenon that reflect the link between a sentence and it context i e the discourse anchoring of utterance if conceived in this way an annotated corpus can be used a a resource for linguistic research not only within the limit of the sentence but also with regard to discourse pattern thus the application of the research to issue of information retrieval and extraction may be made more effective also application in new domain become feasible be it to serve for inner linguistic and literary aim such a text segmentation specification of topic of part of a discourse or for other discipline 
we present an algorithm that sample the hypothesis space of kernelclassifiers given a uniform prior over normalised weight vectorsand a likelihood based on a model of label noise lead to a piecewiseconstant posterior that can be sampled by the kernel gibbssampler kg the kg is a markov chain monte carlo methodthat chooses a random direction in parameter space and samplesfrom the resulting piecewise constant density along the line chosen the kg can be used a an analytical 
the nonnegative boltzmann machine nnbm is a recurrent neural network model that can describe multimodal nonnegative data application of maximum likelihood estimation to this model give a learning rule that is analogous to the binary boltzmann machine we examine the utility of the mean field approximation for the nnbm and describe how monte carlo sampling technique can be used to learn the parameter of the nnbm reflective slice sampling is particularly well suited for this distribution and can efficiently be implemented to sample the distribution we illustrate learning of the nnbm on a translationally invariant distribution a well a on a generative model for image of human face 
the support vector machine svm is a state of the art techniquefor regression and classication combining excellent generalisationproperties with a sparse kernel representation however it doessuer from a number of disadvantage notably the absence of probabilisticoutputs the requirement to estimate a trade o parameterand the need to utilise mercer kernel function in this paper weintroduce the relevance vector machine rvm a bayesian treatmentof a generalised linear 
the symmetric alldiff constraint is a particular case of the all diff constraint a case in which variable and value are defined from the same set that is every variable represents an element c of s and it value represent the element of s that are compatible with c this constraint requires that all the value taken by the variable are different similar to the classical all diff constraint and that if the variable representing the element i is assigned to the value representing the element j then the variable representing the element j is assigned to the value representing the element this constraint is present in many real world problem such sport scheduling where it express match between team in this paper we show how to compute the arc consistency of this constraint in o n m m i d i where n is the number of involved variable and d i the domain of the variable i we also propose a filtering algorithm of le complexity o m 
in this paper we propose a novel method for nonlinear non gaussian on line estimation the algorithm consists of a particle filter that us an unscented kalman filter ukf to generate the importance proposal distribution the ukf allows the particle filter to incorporate the latest observation into a prior updating routine in addition the ukf generates proposal distribution that match the true posterior more closely and also ha the capability of generating heavier tailed distribution 
the self organizing map is a very popular unsupervisedneural network model for the analysisof high dimensional input data a it istypically found in information retrieval application however the interpretation of themap requires much manual effort especially asfar a the analysis of the learned feature andthe characteristic of identified cluster is concerned in this paper we present our novel labelsom method which based on the featureslearned by the map automatically selects 
a general framework for update based planning is presented we first give a new family of dependence based update operator that are wellsuited to the representation of simple action and we identify the complexity of query entailment from an updated belief base then we introduce conditional nondeterministic and concurrent update so a to encode the corresponding type of action effect plan verification and existence are expressed in this update based framework 
the possibility for data mining from large text collection are virtually untapped text express a vast rich range of information but encodes this information in a form that is difficult to decipher automatically perhaps for this reason there ha been little work in text data mining to date and most people who have talked about it have either conflated it with information access or have not made use of text directly to discover heretofore unknown information in this paper i will first define data mining information access and corpus based computational linguistics and then discus the relationship of these to text data mining the intent behind these contrast is to draw attention to exciting new kind of problem for computational linguist i describe example of what i consider to be real text data mining effort and briefly outline recent idea about how to pursue exploratory data analysis over text 
we offer a computational analysis of the resolution of ellipsis in certain case of dialogue clarification we show that this go beyond standard technique used in anaphora and ellipsis resolution and requires operation on highly structured linguistically heterogeneous representation we characterize these operation and the representation on which they operate we offer an analysis couched in a version of head driven phrase structure grammar combined with a theory of information state is in dialogue we sketch an algorithm for the process of utterance integration in i which lead to grounding or clarification 
we explore the relation between classical probabilistic model of information retrieval and the emerging language modeling approach it ha long been recognized that the primary obstacle to effective performance of classical model is the need to estimate arelevance model probability of word in the relevant class we propose a novel technique for estimating these probability using the query alone we demonstrate that our technique can produce highly accurate relevance model addressing important notion of synonymy and polysemy our experiment show relevance model outperforming baseline language modeling system on trec retrieval and tdt tracking task the main contribution of this work is an effective formal method for estimating a relevance model with no training data 
by representing image and image prototype by linear subspace spanned by tangent vector derivative of an image with respect to translation rotation etc impressive invariance to known type of uniform distortion can be built into feedforward discriminator we describe a new probability model that can jointly cluster data and learn mixture of nonuniform smooth deformation field our field are based on low frequency wavelet so they use very few parameter to model a wide range of smooth deformation unlike e g factor analysis which us a large number of parameter to model deformation in spirit our idea are most similar to the idea of separating content from style published by tenenbaum and freeman however our model do not need labeled data for training and thus allow for unsupervised separation of appearance from deformation we give result on handwritten digit recognition and face recognition 
description logic are formalism for the representation of and reasoning about conceptual knowledge on an abstract level concrete domain allow the integration of description logic reasoning with reasoning about concrete object such a number time interval or spatial region the importance of this combined approach especially for building real world application is widely accepted however the complexity of reasoning with concrete domain ha never been formally analyzed and efficient algorithm have not been developed this paper close the gap by providing a tight bound for the complexity of reasoning with concrete domain and presenting optimal algorithm 
we consider the problem of how an agent creates a discrete spatial representation from it continuous interaction with the environment such representation will be the minimal one that explains the experience of the agent in the environment in this 
the statistic of range image from natural environmentsis a largely unexplored field of research itclosely relates to the statistical modeling of the scenegeometry in natural environment and the modelingof optical natural image we have used a d laserrange finder to collect range image from mixed forestscenes the image are here analyzed with respect todifferent statistic 
sqca is an implemented technique for the semi quantitative comparative analysis of dynamical system it is both able to deal with incompletely specified model and make precise prediction by exploiting semiquantitative information in the form of numerical bound on the variable and function occuring in the model the technique ha a solid mathematical foundation which facilitates proof of correctness and convergence property 
a feature of data mining that distinguishes it from quot classical quot machine learning ml and statistical modeling sm is scale the community seems to agree on this yet progress to this point ha been limited we present a methodology that address scale in a novel fashion that ha the potential for revolutionizing the field while the methodology applies most directly to flat row by column data set we believe that it can be adapted to other representation our approach to the problem is not 
the paper report on experiment in which autonomous visually grounded agent bootstrap an ontology and a shared lexicon without prior design nor other form of human intervention the agent do so while playing a particular languagegame called the guessing game we show that synonymy and polysemy arise a emergent property in the language but also that there are tendency to dampen it so a to make the language more coherent and thus more optimal from the viewpoint of communicative success cognitive complexity and learnability 
gradient based policy search is an alternative to value fu nction based method for reinforcement learning in non markovian domain one apparent drawback of policy search is it requirement that all action be on policy that is that there be no explicit exploration in this paper we provide a method for using importance sampling to allow any wellbehaved directed exploration policy during learning we show both theoretically and experimentally that using this method can achieve dramatic performance improvement 
we study the problem of automatically generating an integrated schema for xml dtds introducing a novel view inference approach we show that the set of view and source description can be automatically derived introduction the problem of information integration ha become significant a the growing number of information source on the internet information integration system provide user with an integrated schema of underlying source the integrated schema is designed by hand and a mapping between the integrated schema and the source schema is needed for the system to answer query a xml bray paoli sperberg mcqueen ha become a new standard for representation and exchange of data on the internet in this article we consider the problem of automatically generating an integrated schema for different xml dtds with similar document type architecture xml with a dtd is self descriptive and provides a semistructured data model these property render that dtds defining similar document type have structural and naming similarity given a collection of source dtds we propose a view inference approach which automatically derives the set of integrated view and source description 
source separation or computational auditory scene analysis attempt to extract individual acoustic object from input which contains a mixture of sound from different source altered by the acoustic environment unmixing algorithm such a ica and it extension recover source by reweighting multiple observation sequence and thus cannot operate when only a single observation signal is available i present a technique called refiltering which recovers source by a nonstationary reweighting masking of frequency sub band from a single recording and argue for the application of statistical algorithm to learning this masking function i present result of a simple factorial hmm system which learns on recording of single speaker and can then separate mixture using only one observation signal by computing the masking function and then refiltering 
we introduce the notion of kernel alignment a measure of similarity between two kernel function or between a kernel and a target function this quantity capture the degree of agreement between a kernel and a given learning task and ha very natural interpretation in machine learning leading also to simple algorithm for model selection and learning we analyse it theoretical property proving that it is sharply concentrated around it expected value and we discus it relation with 
a linear method for computing a projective reconstruction from a large number of image is presented and then evaluated the method us planar homographies between view to linearize the resecting of the camera constraint based on the fundamental matrix trifocal tensor or quadrifocal tensor are used to derive relationship between the position vector of all the camera at once the resulting set of equation are solved using a svd the algorithm is computationally efficient a it is linear in the number of matched point used a key feature of the algorithm is that all of the image are processed simultaneously a in the sturmtriggs factorization method but it differs in not requiring that all point be visible in all view an additional advantage is that it work with any mixture of line and point correspondence through the constraint these impose on the multilinear tensor experiment on both synthetic and real data confirm the method s utility 
recent interpretation of the adaboost algorithm view it a performinga gradient descent on a potential function simply changingthe potential function allows one to create new algorithm relatedto adaboost however these new algorithm are generallynot known to have the formal boosting property this paper examinesthe question of which potential function lead to new algorithmsthat are booster the two main result are general setsof condition on the potential one set implies 
there are now numerous agent application that track interest of thousand of user in situation where change occur continuously shim et al suggested that such agent can be made efficient by merging commonality in their activity however past algorithm cannot merge more than or concurrent activity we develop technique so that a large number of concurrent activity typically over can be partitioned into component group of activity of small size e g to so that each component s activity can be merged using previously developed algorithm e g shim et al we first formalize the problem and show that finding optimal partition is nphard we then develop three algorithm greedy based and bab branch and bound based and bab are both guaranteed to compute optimal solution greedy on the other hand us heuristic and typically find suboptimal solution we implemented all three algorithm we experimentally show that the greedy algorithm find partition whose cost are at most worse than that found by based and bab however greedy is able to handle over thousand concurrent request very fast while the other two method are much slower and able to handle only request hence greedy appears to be the best 
this dissertation examines the use of partial programming a a mean of designing agent for large markov decision problem in this approach a programmer specifies only that which they know to be correct and the system then learns the rest from experience using reinforcement learning in contrast to previous low level language for partial programming this dissertation present alisp a lisp based high level partial programming language alisp allows the programmer to constrain the policy considered by a learning process and to express his or her prior knowledge in a concise manner optimally completing a partial alisp program is shown to be equivalent to solving a semi markov decision problem smdp under a finite memory use condition online learning algorithm for alisp are proved to converge to an optimal solution of the smdp and thus to an optimal completion of the partial program this dissertation then present method for exploiting the modularity allows an agent to ignore aspect of it current state that are irrelevant to it current decision and therefore speed up reinforcement learning by decomposing representation of the value of action along subroutine boundary optimality i e optimality among all policy consistent with the partial program these method are demonstrated on two simulated taxi task function approximation a method for representing the value of action allows reinforcement learning to be applied to problem where exact method are intractable soft shaping is a method for guiding an agent toward a solution without constraining the search space both can be integrated with alisp alisp with function approximation and reward shaping is successfully applied on a difficult continuous variant of the simulated taxi task together the method presented in this work comprise a system for agent design that allows the programmer to specify what they know hint at what they suspect using soft shaping and leave unspecified that which they don t know the system then optimally completes the program through experience and take advantage of the hierarchical structure of the specified program to speed learning 
we propose a novel probabilistic framework for semantic video indexing we define probabilistic multimedia object multijects to map low level medium feature to high level semantic label a graphical network of such multijects multinet capture scene context by discovering intra frame a well a inter frame dependency relation between the concept the main contribution is a novel application of a factor graph framework to model this network we model relation between semantic concept in term of their co occurrence a well a the temporal dependency between these concept within video shot using the sum product algorithm for approximate or exact inference in these factor graph multinets we attempt to correct error made during isolated concept detection by forcing high level constraint this result in a significant improvement in the overall detection performance 
we study the special form that the general multi image tensor formalism take under the plane parallax decomposition including matching tensor and constraint closure and depth recovery relation and inter tensor consistency constraint plane parallax alignment greatly simplifies the algebra and uncovers the underlying geometric content we relate plane parallax to the geometry of translating calibrated camera and introduce a new par allax factorizing projective reconstruction method based on this initial plane parallax alignment reduces the problem to a single rank one factorization of a matrix of rescaled parallax into a vector of projection centre and a vector o f projective height above the reference plane the method extends to d line represented by viapoints and d plane represented by homographies 
we present a probabilistic method for fusion of image producedby multiple sensor the approach is based on an image formationmodel in which the sensor image are noisy locally linear functionsof an underlying true scene a bayesian framework then providesfor maximum likelihood or maximum a posteriori estimate of thetrue scene from the sensor image maximum likelihood estimatesof the parameter of the image formation model involve local second order image statistic and 
in this paper we present a novel method for automatically and efficiently generating stereoscopic mosaic by seamless registration of optical data collected by a video camera mounted on an airborne platform that undergoes dominant translational motion there are four critical point discussed in this paper using a parallel perspective representation a pair of geometrically registered stereo mosaic can be constructed before we explicitly recover any d information under rather general motion a prism parallel ray interpolation for stereo mosaicing technique is proposed to make stereo mosaic seamless in the presence of motion parallax and for rather arbitrary scene a fast prism algorithm is presented and issue on stitching point selection and occlusion handling are discussed the epipolar geometry of parallel perspective stereo mosaic generated under constrained dof motion is formulated which show optimal baseline easy search for correspondence and constant depth resolution the proposed method for the generation of stereo mosaic and then the reconstruction of a d map are efficient in both computation and storage experimental result on long video sequence are given 
stereoscopic vision ha a fundamental role both for animal and human nonetheless in the computer vision literature there is limited reference to biological model related to stereoscopic vision and in particular to the functional property and the organization of binocular information within the visual cortex in this paper a simple stereo technique based on a space variant mapping of the image data and a multi layered cortical stereoscopic representation mimicking the neural organization of the early stage of the human visual system is proposed radial disparity computed from a stereo pair is used to map the relative depth with respect to the fixation point a set of experiment demonstrating the applicability of the devised technique is also presented 
both document clustering and word clustering are well studied problem most existing algorithm cluster document and word separately but not simultaneously in this paper we present the novel idea of modeling the document collection a a bipartite graph between document and word using which the simultaneous clustering problem can be posed a a bipartite graph partitioning problem to solve the partitioning problem we use a new spectral co clustering algorithm that us the second left and right singular vector of an appropriately scaled word document matrix to yield good bipartitionings the spectral algorithm enjoys some optimality property it can be shown that the singular vector solve a real relaxation to the np complete graph bipartitioning problem we present experimental result to verify that the resulting co clustering algorithm work well in practice 
many communicative behavior in the animal kingdom consist of performing and recognizing specialized pattern of oscillatory motion here we present an approach to the representation and recognition of these oscillatory motion based on the categorical organization of a simple sinusoidal model having very specific and limited parameter value this characterization is used to specify the type and layout of computation for recognizing the pattern result of the method are demonstrated with 
co training is a weakly supervised learning paradigm in which the redundancy of the learning task is captured by training two classifier using separate view of the same data this enables bootstrapping from a small set of labeled training data via a large set of unlabeled data this study examines the learning behavior of co training on natural language processing task that typically require large number of training instance to achieve usable performance level using base noun phrase bracketing a a case study we find that co training reduces by the difference in error between co trained classifier and fully supervised classifier trained on a labeled version of all available data however degradation in the quality of the bootstrapped data arises a an obstacle to further improvement to address this we propose a moderately supervised variant of cotraining in which a human corrects the mistake made during automatic labeling our analysis suggests that corrected co training and similar moderately supervised method may help cotraining scale to large natural language learning task 
neocortical circuit are dominated by massive excitatory feedback more than eighty percent of the synapsis made by excitatory cortical neuron are onto other excitatory cortical neuron why is there such massive recurrent excitation in the neocortex and what is it role in cortical computation recent neurophysiological experiment have shown that the plasticity of recurrent neocortical synapsis is governed by a temporally asymmetric hebbian learning rule we describe how such a rule may allow the cortex to modify recurrent synapsis for prediction of input sequence the goal is to predict the next cortical input from the recent past based on previous experience of similar input sequence we show that a temporal difference learning rule for prediction used in conjunction with dendritic back propagating action potential reproduces the temporally asymmetric hebbian plasticity observed physiologically biophysical simulation demonstrate that a network of cortical neuron can learn to predict moving stimulus and develop direction selective response a a consequence of learning the space time response property of model neuron are shown to be similar to those of direction selective cell in alert monkey v 
a novel algorithm for motion segmentation is proposed the algorithm us the fact that shape of an object with homogeneous motion is represented a dimensional linear space thus motion segmentation is done a the decomposition of shape space of multiple object into a set of dimensional subspace the decomposition is realized using the discriminant analysis of orthogonal projection matrix of shape space since only discriminant analysis of ddata is needed this analysis is quite simple the algorithm based on the analysis is robust for data with noise and outlier because the analysis can extract useful information for motion segmentation while rejecting useless one the implementation result show that the proposed method is robust and efficient enough to do online task for real scene 
case based reasoning cbr is concerned with solving new problem by adapting solution that worked for similar problem in the past year of experience in building and fielding cbr system have shown that the rase approach is not free from problem it ha been realized that the knowledge engineering effort required for designing many real world easebases can be prohibitively high based on the wide spread use of database and powerful machine learning method some cbr researcher have been investigating the possibility of designing casebases automatically this paper proposes a flexible model for the automatic discovery of abstract case from data base based on the lattice machine it also proposes an efficient and effective algorithm for retrieving such case besides the known benefit associated with abstract case the main advantage of this approach are that the discovery process is fully automated no knowledge engineering cost 
standard presentation system consisting of a laptop connected to a projector suffer from two problem the projected image appears distorted keystoned unless the projector is precisely aligned to the projection screen the speaker is forced to interact with the computer rather than the audience this paper show how the addition of an uncalibrated camera aimed at the screen solves both problem although the location orientation and optical parameter of the camera and projector are unknown the projector camera system calibrates itself by exploiting the homography between the projected slide and the camera image significant improvement are possible over passively calibrating system since the projector actively manipulates the environment by placing feature point into the scene for instance using a low resolution x camera we can achieve an accuracy of pixel in a x presentation slide the camera projector system infers model for the projector to camera and projector to screen mapping in order to provide two major benefit first image sent to the projector are pre warped in such a way that the distortion induced by the arbitrary projector screen geometry are precisely negated this enables projector to be mounted anywhere in the environment for instance at the side of the room where the speaker is le likely to cast shadow on the screen and where the projector doe not occlude the audience s view second the system detects the position of the user s laser pointer dot in the camera image at hz allowing the laser pointer to emulate the pointing action of a mouse this enables the user to activate virtual button in the presentation such a next slide and draw on the projected image the camera assisted presentation system requires no special hardware aside from the cheap camera 
we describe a novel method for coping with ungrammatical input based on the use of chart like data structure which permit anytime processing priority is given to deep syntactic analysis should this fail the best partial analysis are selected according to a shortest path algorithm and assembled in a robust processing phase the method ha been applied in a speech translation project with large hpsg grammar 
the problem of establishing correspondence and measuring the similarity of a pair of planar curve arises in many application in computer vision and pattern recognition this paper present a new method for comparing planar curve and for performing matching at sub sampling resolution the analysis of the algorithm a well a it structural property are described the performanceof the new technique applied to the problem of signature verification is shown and compared with the performance of the well known dynamic time warping algorithm of different method proposed in the pattern recognition literature that could be applied to this problem some of the method require the extraction of a prototype that summarizes the mean behavior of the example and a measure of the deviation from this prototype that exists in the training set some other method compute the similarity of the curve based on a time distortion function that best aligns the curve a measure of this similarity is later on used for classification 
hidden markov model hmms are a powerful probabilistic tool for modeling sequential data and have been applied with success to many text relatedtasks suchas part of speechtagging text segmentation and information extraction in these case the observation are usually modeled a multinomial distribution over a discrete vocabulary and the hmm parameter are set to maximize the likelihood of the observation this paper present a new markovian sequence model closely related to hmms that allows observation to be represented a arbitrary overlapping feature such a word capitalization formatting part of speech and defines the conditional probability of state sequence given observation sequence it doe this by using the maximumentropyframeworkto fit a set of exponential model that represent the probability of a state given an observation and the previous state we present positive experimental result on the segmentation of faq s 
we analyze economic efficiency and equilibrium property in decentralized task allocation problem involving hierarchical dependency and resource contention we bound the inefficiency of a type of approximate equilibrium in proportion to the number of agent and the bidding parameter in a particular market protocol this protocol converges to an approximate equilibrium with respect to all agent except those which may acquire unneeded input we introduce a decommitment phase to allow such agent to decommit from their input contract experiment indicate that the augmented market protocol produce highly efficient allocation on average 
an easy way of translating query in one language to the other for cross language information retrieval ir is to use a simple bilingual dictionary because of the general purpose nature of such dictionary however this simple method yield a severe translation ambiguity problem this paper describes the degree to which this problem arises in korean english cross language ir and suggests a relatively simple yet effective method for disambiguation using mutual information statistic obtained only from the target document collection in this method mutual information is used not only to select the best candidate but also to assign a weight to query term in the target language our experimental result based on the trec collection show that this method can achieve up to of the monolingual retrieval case and of the manual disambiguation case 
grammatical relationship grs form an important level of natural language processing but different set of grs are useful for different purpose therefore one may often only have time to obtain a small training corpus with the desired gr annotation to boost the performance from using such a small training corpus on a transformation rule learner we use existing system that find related type of annotation 
we introduce a new model of selectional preference induction unlike previous approach we provide a stochastic generation model for the word that appear a argument of a predicate more specifically we define a hidden markov model with the general shape of a given semantic class hierarchy this model ha a number of attractive feature among them that selectional preference can be seen a distribution over word initial result are promising however unsupervised parameter estimation ha proven problematic a central problem is word sense ambiguity in the training corpus we describe attempt to modify the forward backward algorithm an em algorithm to handle such disambiguation although these attempt were unsuccessful at improving performance we believe they give insight into the nature of the bottleneck and into the behavior of the em algorithm 
work in simultaneous localisation and map building slam for mobile robot ha focused on the simplified case in which a robot is considered to move in two dimension on a ground plane while this is often a good approximation a large number of real world application require robot to move around terrain which ha significant slope and undulation and it is desirable that these robot too should be able to estimate their location by building map of natural feature in this paper we describe a real time ekf based slam system permitting unconstrained d localisation and in particular develop model for the motion of a wheeled robot in the presence of unknown slope variation in a fully automatic implementation our robot observes visual point feature using fixating stereo vision and build a sparse map on the fly combining this visual measurement with information from odometry and a roll pitch accelerometer sensor the robot performs accurate repeatable localisation while traversing an undulating course 
the hierarchical hidden markov model hhmm is a generalization of the hidden markov model hmm that model sequence with structure at many length time scale fst unfortunately the original inference algorithm is rather complicated and take time where is the length of the sequence making it impractical for many domain in this paper we show how hhmms are a special kind of dynamic bayesian network dbn and thereby derive a much simpler inference algorithm which only take time furthermore by drawing the connection between hhmms and dbns we enable the application of many standard approximation technique to further speed up inferenc e 
the error correcting output coding ecoc approach to classifier design decomposes a multi class problem into a set of complementary two class problem we show how to apply the ecoc concept to automatic face verification which is inherently a two class problem the output of the binary classifier defines the ecoc feature space in which it is easier to separate transformed pattern representing client and impostor we propose two different combining strategy a the matching score for face verification the fir st us the fir st order minkowski metric and requires a threshold to be set the second is a kernel based method and ha no parameter to set the proposed method exhibit better performance on the well known xm vt data set compared with previous reported result 
we describe a novel viewpoint lighting ambiguity which we call the kgbr this ambiguity assumes orthographic projection or an affine camera and us lambertian reflectance function including cast attached shadow and multiple light source a kgbr transform alters the geometry by a three dimensional affine transformation and albedo property of object if two object are related by a kgbr transform then for any viewpoint and lighting of the first object there exists a corresponding viewpoint and lighting of the second object so that the image are identical up to an affine transformation the generalized ba relief gbr ambiguity is obtained a a special case of the kgbr we describe generic viewpoint and lighting assumption and show that either or both resolve this ambiguity by biasing towards object with planar geometry in proceeding international conference on computer vision vancouver british columbia 
of the various problem that natural language processing ha revealed polysemy is probably the most frustrating people deal with polysemy so easily that potential abiguities are overlooked whereas computer must work hard to do far le well a linguistic approach generally involves a parser a lexicon and some ad hoc rule for using linguistic context to identify the context appropriate sense a statistical approach generally involves the use of word co occurrence statistic to create a semantic hyperspace where each word regardless of it polysemy is represented a a single vector each approach ha strength and limitation some combination is often proposed various possibility will be discussed in term of their psychological plausibility computational linguistics is generally considered to be the branch of engineering that us computer to do useful thing with linguistic signal but it can also be viewed a an extended test of computational theory of human cognition it is this latter perspective that psychologist find most interesting language provides a critical test for the hypothesis that physical symbol system are adequate to perform all human cognitive function a yet no adequate system for natural language processing ha approached human level of performance 
sensitivity to variation in illumination is a fundamental and challenging problem in face recognition in this paper we describe a new method based on symmetric shape from shading ssfs to develop a face recognition system that is robust to change in illumination the basic idea of this approach is to use the ssfs algorithm a a tool to obtain a prototype image which is illumination normalized it ha been shown that the ssfs algorithm ha a unique point wise solution but it is still difficult to recover accurate shape information given a single real face image with complex shape and varying albedo in stead we utilize the fact that all face share a similar shape making the direct computation of the prototype image from a given face image feasible finally to demonstrate the efficacy of our method we have applied it to several publicly available face database 
the proliferation of hypertext and the popularityof kleinberg s hit algorithm havebrought about an increased interest in linkanalysis while hit and it older relativesfrom the bibliometrics provide a method forfinding authoritative source on a particulartopic they do not allow individual user toinject their own opinion on what source areauthoritative this paper present a techniquefor learning a user s internal model ofauthority we present experimental result 
this paper present the permitted relation between two rectangle whose side are parallel to the ax of some orthogonal basis in a dimensional euclidean space elaborating rectangle algebra just like interval algebra it defines the concept of convexity a well a the one of weak preconvexity and strong preconvexity it introduces afterwards the fundamental operation of intersection composition and inversion and demonstrates that the concept of weak preconvexity is preserved by the operation of composition whereas the concept of strong preconvexity is preserved by the operation of intersection finally fitting the propagation technique conceived to solve interval network it show that the polynomial path consistency algorithm is a decision method for the problem of proving the consistency of strongly preconvex rectangle network 
dynamic control task are good candidate for the application of reinforcement learning technique however many of these task inherently have continuous state or action variable this can cause problem for traditional reinforcement learning algorithm which assume discrete state and action in this paper we introduce an algorithm that safely approximates the value function for continuous state control task and that learns quickly from a small amount of data we give experimental result using this algorithm to learn policy for both a simulated task and also for a real robot operating in an unaltered environment the algorithm work well in a traditional learning setting and demonstrates extremely good learning when bootstrapped with a small amount of human provided data 
algorithm for feature selection fall into two broad category wrapper use thelearning algorithm itself to evaluate the usefulness of feature while filter evaluatefeatures according to heuristic based on general characteristic of the data for applicationto large database filter have proven to be more practical than wrappersbecause they are much faster however most existing filter algorithm only work withdiscrete classification problem this paper describes a fast 
in this paper we propose a novel method called local nonnegative matrix factorization lnmf for learning spatially localized part based subspace representation of visual pattern an objective function is defined to impose localization constraint in addition to the non negativity constraint in the standard nmf this give a set of base which not only allows a non subtractive part based representation of image but also manifest localized feature an algorithm is presented for the learning of such basis component experimental result are presented to compare lnmf with the nmf and pca method for face representation and recognition which demonstrates advantage of lnmf 
partially observable markov decision process pomdps provide a coherent mathematical framework for planning under uncertainty when the state of the system cannot be fully observed however the problem of finding an exact pomdp solution is intractable computing such solution requires the manipulation of a piecewise linear convex value function which specifies a value for each possible belief state this value function can be represented by a set of vector each one with dimension equal to the size of the state space in nontrivial problem however these vector are too large for such a representation to be feasible preventing the use of exact pomdp algorithm we propose an approximation scheme where each vector is represented a a linear combination of basis function to provide a compact approximation to the value function we also show that this representation can be exploited to allow for efficient computation in approximate value and policy iteration algorithm in the context of factored pomdps where the transition model is specified using a dynamic bayesian network 
this paper describes a method for structuring a robot motor learning task by designing a suitably parameterized policy we show that a simple search algorithm along with biologically motivated constraint offer an effective mean for motor skill acquisition the framework make use of the robot counterpart to several element found in human motor learning imitation equilibrium point control motor program and synergy we demonstrate that through learning coordinated behavior emerges from initial crude knowledge about a difficult robot weightlifting task 
chow and liu introduced an algorithm for fitting a multivariate distribution with a tree i e a density model that assumes that there are only pairwise dependency between variable and that the graph of these dependency is a spanning tree the original algorithm is quadratic in the dimesion of the domain and linear in the number of data point that define the target distribution p this paper show that for sparse discrete data fitting a tree distribution can be done in time and memory that is jointly subquadratic in the number of variable and the size of the data set the new algorithm called the accl algorithm take advantage of the sparsity of the data to accelerate the computation of pairwise marginals and the sorting of the resulting mutual information achieving speed ups of up to order of magnitude in the experiment 
a component based generic agent architecture for multi attribute integrative negotiation is introduced and it application is described in a prototype system for negotiation about car developed in co operation with among others dutch telecom kpn the approach can be characterised a co operative one to one multi criterion negotiation in which the privacy of both party is protected a much a possible 
a latent variable generative model with finite noise is used to describe several different algorithm for independent component analysis ica in particular the fixed point ica algorithm is shown to be equivalent to the expectationmaximization algorithm for maximum likelihood under certain constraint allowing the condition for global convergence to be elucidated the algorithm can also be explained by their generic behavior near a singular point where the size of the optimal generative base vanishes an expansion of the likelihood about this singular point indicates the role of higher order correlation in determining the feature discovered by ica the application and convergence of these algorithm are demonstrated on the learning of edge feature a the independent component of natural image 
in many optimization and decision problem the objective function can be expressed a a linear combination of competing criterion the weight of which specify the relative importance of the criterion for the user we consider the problem of learning such a subjective function from preference judgment collected from trace of user interaction we propose a new algorithm for that task based on the theory of support vector machine one advantage of the algorithm is that prior knowledge about the domain can easily be included to constrain the solution we demonstrate the algorithm in a route recommendation system that adapts to the driver s route preference we present experimental result on real user that show that the algorithm performs well in practice 
a novel algorithm is proposed to learn pattern similarity for texture image retrieval similar pattern in different texture class are grouped into a cluster in the feature space each cluster is isolated from others by an enclosed boundary which is represented by several support vector and their weight obtained from a statistical learning algorithm called support vector machine svm the signed distance of a pattern to the boundary is used to measure it similarity furthermore the pattern of different class within each cluster are separated by several sub boundary which are also learned by the svms the signed distance of the similar pattern to a particular sub boundary associated with the query image are used for ranking these pattern experimental result on the brodatz texture database indicate that the new method performs significantly better than the traditional euclidean distance based approach 
principal component analysis pca is a commonly applied technique for dimensionalityreduction pca implicitly minimizes a squared loss function which maybe inappropriate for data that is not real valued such a binary valued data thispaper draw on idea from the exponential family generalized linear model andbregman distance to give a generalization of pca to loss function which weargue are better suited to other data type we describe algorithm for minimizingthe loss 
this paper present a unified probabilistic framework for denoising and dereverberation of speech signal the framework transforms the denoising and dereverberation problem into bayes optimal signal estimation the key idea is to use a strong speech model that is pre trained on a large data set of clean speech computational efficiency is achieved by using variational em working in the frequency domain and employing conjugate prior the framework cover both single and multiple microphone we apply this approach to noisy reverberant speech signal and get result substantially better than standard method 
we present a probabilistic generative model for timing deviationsin expressive music performance the structure of the proposedmodel is equivalent to a switching state space model we formulatetwo well known music recognition problem namely tempotracking and automatic transcription rhythm quantization a lteringand maximum a posteriori map state estimation task 
object such a event action and propo sitions can all serve a antecedent of auaphoric ex pressions we are not aware of any reliability resultsfor this type of annotation but the lt seg gt elementcan be used to identify the antecedent in this typeof anaphora if desired the annotator could use asecond attribute type to specify the type of objectintroduced by the lt seg element type would havevalues event prop and action 
we describe a set of supervised machine learning experiment centering on the construction of statistical model of wh question these model which are built from shallow linguistic feature of question are employed to predict target variable which represent a user s informational goal we report on different aspect of the predictive performance of our model including the influence of various training and testing factor on predictive performance and examine the relationship among the target variable 
we present a rule based shallow parser compiler which allows to generate a robust shallow parser for any language even in the absence of training data by resorting to a very limited number of rule which aim at identifying constituent boundary we contrast our approach to other approach used for shallow parsing i e finite state and probabilistic method we present an evaluation of our tool for english penn treebank and for french newspaper corpus lemonde for several task np chunking deeper parsing 
dominance constraint are logical description of tree that are widely used in computational linguistics their general satisfiability problem is known to be np complete here we identify the natural fragment of normal dominance constraint and show that it satisfiability problem is in deterministic polynomial time 
previous biophysical modeling work showed that nonlinear interactionsamong nearby synapsis located on active dendritic tree canprovide a large boost in the memory capacity of a cell mel a b the aim of our present work is to quantify this boost byestimating the capacity of a neuron model with passive dendriticintegration where input are combined linearly across theentire cell followed by a single global threshold and an activedendrite model in which a 
we present a generative distributional model for the unsupervised induction of natural language syntax which explicitly model constituent yield and context parameter search with em produce higher quality analysis than previously exhibited by unsupervised system giving the best published un supervised parsing result on the atis corpus experiment on penn treebank sentence of comparable length show an even higher f of on non trivial bracket we compare distributionally induced and actual part of speech tag a input data and examine extension to the basic model we discus error made by the system compare the system to previous model and discus upper bound lower bound and stability for this task 
the task of causal structure discovery from empirical data is a fundamental problem in many area experimental data is crucial for accomplishing this task however experiment are typically expensive and must be selected with great care this paper us active learning to determine the experiment that are most informative towards uncovering the underlying structure we formalize the causal learning task a that of learning the structure of a causal bayesian network we consider an active learner that is allowed to conduct experiment where it intervenes in the domain by setting the value of certain variable we provide a theoretical framework for the active learning problem and an algorithm that actively chooses the experiment to perform based on the model learned so far experimental result show that active learning can substantially reduce the number of observation required to determine the structure of a domain 
it is necessary to have a large annotated corpus to build a statistical parser acquisition of such a corpus is costly and time consuming this paper present a method to reduce this demand using active learning which selects what sample to annotate instead of annotating blindly the whole training corpus sample selection for annotation is based upon representativeness and usefulness a model based distance is proposed to measure the difference of two sentence and their most likely parse tree based on this distance the active learning process analyzes the sample distribution by clustering and calculates the density of each sample to quantify it representativeness further more a sentence is deemed a useful if the existing model is highly uncertain about it par where uncertainty is measured by various entropy based score experiment are carried out in the shallow semantic parser of an air travel dialog system our result show that for about the same parsing accuracy we only need to annotate a third of the sample a compared to the usual random selection method 
building a bilingual dictionary for transfer in a machine translation system is conventionally done by hand and is very time consuming in order to overcome this bottleneck we propose a new mechanism for lexical transfer which is simple and suitable for learning from bilingual corpus it exploit a vector space model developed in information retrieval research we present a preliminary result from our computational experiment 
much excitement ha been generated by the recent success of stochastic local search procedure at finding satisfying assignment to large formula many of the problem on which these method have been effective are non boolean in that they are most naturally formulated in term of variable with domain size greater than two to tackle such a problem with a boolean procedure the problem is first reformulated a an equivalent boolean problem this paper introduces and study the alternative of extending a boolean stochastic local search procedure to operate directly on non boolean problem it then compare the non boolean representation to three boolean representation and present experimental evidence that the non boolean method is often superior for problem with large domain size 
almost two decade ago hopeld showed that network ofhighly reduced model neuron can exhibit multiple attractingxedpoints thus providing a substrate for associative memory it is stillnot clear however whether realistic neuronal network can supportmultiple attractor the main diculty is that neuronal networksin vivo exhibit a stable background state at lowring rate typicallya few hz embedding attractor is easy doing so withoutdestabilizing the background is 
a collective intelligence coin is a set of interacting reinforcement learning rl algorithm designed in an automated fashion so that their collective behavior optimizes a global utility function we summarize the theory of coin then present experiment using that theory to design coin to control internet traffic routing these experiment indicate that coin outperform all previously investigated rl based shortest path routing algorithm 
we present two measure for comparingcorpora based on information theorystatistics such a gain ratio a wellas simple term class frequency count 
we address the problem of motion flow estimation for a scene with multiple moving object observed from a possibly moving camera we take a input a possibly sparse noisy velocity field a obtained from local matching produce a set of motion boundary and identify pixel with different velocity in overlapping layer for a fixed observer these overlapping layer capture occlusion information for a moving observer further processing is required to segment independent object and infer structure unlike previous approach which generate layer by iteratively fitting data to a set of predefined parameter we instead find boundary first then infer region and address occlusion overlap relationship all computational step use a common framework oftensors to represent velocity information together with saliency confidence and uncertainty communication between site is performed by convolution like tensorvoting the scheme is non iterative and the only free parameter is the scale related to neighborhood size we illustrate the approach with result obtained from synthetic sequence and from real image the quantitative result compare favorably with those of other method especially in the presence of occlusion 
abstract in many rule induction algorithm it is relatively common that a large number of rule is generated this complexity may harm the comprehensibility of the model without improving it predictive performance for aiding automatic knowledge acquisition and knowledge discovery in classification domain we propose a framework to post process boolean rule obtained from rule induction algorithm this process generates probabilistic rule set with fewer rule and premise while maintaining comparable classification accuracy 
this paper describes new and improved technique which help a unification based parser to process input efficiently and robustly in combination these method result in a speed up in parsing time of more than an order of magnitude the method are correct in the sense that none of them rule out legal rule application 
we investigate the application of bayesian network markov random field and mixture model to the problem of query answering for transaction data set we formulate two version of the querying problem the query selectivity estimation i e finding exact count for tuples in a data set and the query generalization problem i e computing the probability that a tuple will occur in new data we show that frequent itemsets are useful for reducing the original data to a compressed representation and introduce a method to store them using an adtree data structure in an extension of our earlier work on this topic we propose several new scheme for query answering based on the compressed representation that avoid direct scan of the data at query time experimental result on real world transaction data set provide insight into various tradeoff involving the offline time for model building the online time for query answering the memory footprint of the compressed data and the accuracy of the estimate provided to the query 
answer validation is an emerging topic in question answering where open domain system are often required to rank huge amount of candidate answer we present a novel approach to answer validation based on the intuition that the amount of implicit knowledge which connects an answer to a question can be quantitatively estimated by exploiting the redundancy of web information experiment carried out on the trec judged answer collection show that the approach achieves a high level of performance success rate the simplicity and the efficiency of this approach make it suitable to be used a a module in question answering system 
string similarity metric are used for several purpose in text processing one task is the extraction of cognate from bilingual text in this paper three approach to the automatic generation of language dependent string matching function are presented 
this paper present part of an on going project to integrate perception attention drive emotion behavior arbitration and expressive act for a robot designed to interact socially with human we present the design of a visual attention system based on a model of human visual search behavior from wolfe the attention system integrates perception motion detection color saliency and face popouts with habituation effect and influence from the robot s motivational and behavioral state to create a context dependent attention activation map this activation map is used to direct eye movement and to satiate the drive of the motivational system 
we propose a new stereo method for d navigation in a dynamic environment such a road without depth search and metric camera calibration conventionally there is an effective stereo method based on the constraint that an observer move on the ground plane namely the gp constraint although the gp constraint is often violated in an outdoor environment due to the movement of the observer i e camera vibration and inclination it can be updated using some feature on the ground plane only if the stereo camera are weakly calibrated however the conventional stereo method ha several drawback first it is rather difficult to solve for the general epipolar geometry summarized in the fundamental matrix second although the updated gp constraint often becomes imperfect it lack an effective contrivance for the noise reduction third there is no measure to ass the danger of detected obstacle to solve these problem we develop a domain specific stereo method which utilizes various attribute of road we introduce the pseudo projective camera model that provides a good approximation to the projective camera in road scene and accordingly define the linear epipolar geometry for a pair of pseudo projective camera furthermore we show that extraction of two parallel line lying on the road is effective to update the gp constraint overcome the noise issue and even to estimate the degree of danger through experiment we demonstrate that our method is efficient and applicable to a variety of outdoor scene 
in automatic speech recognition asr enabled application for medical dictation corpus of literal transcription of speech are critical for training both speaker independent and speaker adapted acoustic model obtaining these transcription is both costly and time consuming non literal transcription on the other hand are easy to obtain because they are generated in the normal course of a medical transcription operation this paper present a method of automatically generating text that can take the place of literal transcription for training acoustic and language model atrs is an automatic transcription reconstruction system that can produce near literal transcription with almost no human labor we will show that i adapted acoustic model trained on atrs data perform a well a or better than adapted acoustic model trained on literal transcription a measured by recognition accuracy and ii language model trained on atrs data have lower perplexity than language model trained on non literal data 
constructing good model for chemical carcinogenesis wa identified in ijcai a providing a substantial challenge to knowledge discovery program attention wa drawn to a comparative exercise which called for prediction on the outcome of rodent carcinogenicity bioassay this the predictive toxicology evaluation or pte challenge wa seen to provide ai program with an opportunity to participate in an enterprise of scientific merit and a yardstick for comparison against strong competition here we provide an assessment of the machine learning ml submission made model submitted are assessed on their accuracy in comparison to model developed with expert collaboration and their explanatory value for toxicology the principal finding were a using structural information available from a standard modelling package layman devised feature and outcome of established biological test result from mlderived model were at least a good a those with expert derived technique this wa surprising b the combined use of structural and biological feature by ml derived model wa unusual and suggested new avenue for toxicology modelling this wa also unexpected and c significant effort wa required to interpret the output of even the most symbolic of ml derived model much of this could have been alleviated with measure for converting the result into a more toxicology friendly form a it stand their absence is sufficient to prevent a whole hearted acceptance of these promising method by toxicologist this suggests that ml technique have been able to respond not fully but nevertheless substantially to the pte challenge 
in this paper we present part of speech tagger based on hidden markov model which adopt a le strict markov assumption to consider rich context in model whose parameter are very specific like lexicalized one sparse data problem is very serious and also conditional probability tend to be estimated unreliably to overcome data sparseness a simplified version of the well known back off smoothing method is used to mitigate unreliable estimation problem our model assume joint independence instead of conditional independence because joint probability have the same degree of estimation reliability in experiment for the brown corpus model with rich context achieve relatively high accuracy and some model assuming joint independence show better result than the corresponding hmms 
in order to select a good hypothesis language or model from a collection of possible model one ha to ass the generalization performance of the hypothesis which is returned by a learner that is bound to use that model the paper deal with a new and very efficient way of assessing this generalization performance we present an analysis which characterizes the expected generalization error of the hypothesis with least training error in term of the distribution of error rate of the hypothesis in the model this distribution can be estimated very efficiently from the data which immediately lead to an efficient model selection algorithm the analysis predicts learning curve with a very high precision and thus contributes to a better understanding of why and when over fitting occurs we present empirical study controlled experiment on boolean decision tree and a large scale text categorization problem which show that the model selection algorithm lead to error rate which are often a low a those obtained by fold cross validation sometimes even lower however the algorithm is much more efficient because the learner doe not have to be invoiced at all and thus solves model selection problem with a many a a thousand relevant attribute and example 
topic segmentation is an important initial step in many text based task a hierarchical representation of a text topic is useful in retrieval and allows judging relevancy at different level of detail this short paper describes research on generic algorithm for topic detection and segmentation that are applicable on text of heterogeneous type and domain 
capturing real motion from video sequence is a powerful method for automatic building of facial articulation model in this paper we propose an explanation based facial motion tracking algorithm based on a piecewise bezier volume deformation model pbvd the pbvd is a suitable model both for the synthesis and the analysis of facial image it is linear and independent of the facial mesh structure with this model basic facial movement or action unit are interactively defined by changing the magnitude of these action unit animated facial image are generated the magnitude of these action unit can also be computed from real video sequence using a model based tracking algorithm however in order to customize the articulation model for a particular face the predefined pbvd action unit need to be adaptively modified in this paper we first briefly introduce the pbvd model and it application in facial animation then a multiresolution pbvd based motion tracking algorithm is presented finally we describe an explanation based tracking algorithm that take the predefined action unit a the initial articulation model and adaptively improves them during the tracking process to obtain a more realistic articulation model experimental result on pbvd based animation model based tracking and explanation based tracking are shown in this paper 
this work deal with the recovery of illusory linear clue from perspectively skewed document with the purpose of using them for rectification the computational approach proposed implement the perceptual organization principle implicitly used in textual layout the numerous example provided show that the method is robust and viewpoint and scale invariant in this paper we will be dealing with the problem of finding illusory clue of the type a and c without assumption on the type of document font or camera rotation and orientation other than the presence of some organized text text had been designed long before perception study but with the unspoken principle of perceptual saliency see e g in mind give any organised text even foreign and unintelligible to u and word line and paragraph structure pop out preattentively the approach presented in this paper is strongly based on a computational implementation of these perceptual organization principle and will be shown to be robust fast and general 
this paper introduces new learning algorithm for natural language processing based on the perceptron algorithm we show how the algorithm can be efficiently applied to exponential sized representation of parse tree such a the all subtrees dop representation described by bod or a representation tracking all sub fragment of a tagged sentence we give experimental result showing significant improvement on two task parsing wall street journal text and named entity extraction from web data 
query expansion is an effective relevance feedback technique for improving performance in information retrieval in general query expansion method select term from the complete content of relevant document one problem with this approach is that expansion term unrelated to document relevance can be introduced into the modified query due to their presence in the relevant document and distribution in the document collection motivated by the hypothesis that query expansion term should only be sought from the most relevant area of a document this investigation explores the use of document summary in query expansion the investigation explores the use of both context independent standard summary and query biased summary experimental result using the okapi bm probabilistic retrieval model with the trec ad hoc retrieval task show that query expansion using document summary can be considerably more effective than using full document expansion the paper also present a novel approach to term selection that separate the choice of relevant document from the selection of a pool of potential expansion term again this technique is shown to be more effective that standard method 
many people are now routinely building grammar based language model for interactive spoken language application these language model are typically ad hoc semantic grammar which ignore many standard linguistic constraint in particular grammatical agreement we describe a series of experiment in which we took three cfg based language model from non trivial implemented system and in each case contrasted the performance of a version which included agreement constraint against a version which ignored them our finding suggest that inclusion of agreement constraint significantly improves performance in term of both word error rate and semantic error rate 
in this paper we consider the problem of recovering the d motion and shape of an arbitrarily moving arbitrarilyshaped curve from multiple synchronized video stream acquired from distinct and known point in space by studying the d motion and shape constraint provided by the input video stream we show that shape and motion recovery is equivalent to the problem of recovering the differential property of the spatio temporal curve manifold that describes the curve s trace in space time and a local analytical description of this manifold can be computed directly from the spatio temporalvolumes definedby the input video stream our experimental result suggest that this manifold basedapproachto joint shape and motion estimation yield shape estimate of higher accuracy that those obtained from stereo alone allows accurate recovery of d curve motion and provides significant robustness against image noise and camera calibration error 
we look at distributed representation of structure with variable binding that is natural for neural net and allows traditional symbolic representation and processing the representation support learning from example this is demonstrated by taking several instance of the mother of relation implying the parent of relation by encoding them into a mapping vector and by showing that the mapping vector map new instance of mother of into parent of 
in this paper we address the problem of estimating and analyzing the motion in image sequence that involve fluid phenomenon in this context standard motion estimation technique are not well adapted and more dedicated approach have to be designed in this prospect we propose to estimate in a joint and cooperative way a dense motion field and a peculiar parametric representation of the flow the parametric model issue from an extension of rankine vortex model and includes a laminar flow field dense and parametric field are estimated by minimizing a robust global objective function thanks to a specific alternate scheme the method ha been validated on different kind of meteorological image sequence 
previous work addressing the issue of word distribution in document ha shown the importance of word repetitiveness a an indicator of the word contentbearing characteristic in this paper we propose a simple method using a measure of the tendency of word to repeat within a document to separate the word with similar document frequency but different topic discriminating characteristic 
we study the impact of richer syntactic dependency on the performance of the structured language model slm along three dimension parsing accuracy lp lr perplexity ppl and word error rate wer n best re scoring we show that our model achieve an improvement in lp lr ppl and or wer over the reported baseline result using the slm on the upenn treebank and wall street journal wsj corpus respectively analysis of parsing performance show correlation between the quality of the parser a measured by precision recall and the language model performance ppl and wer a remarkable fact is that the enriched slm outperforms the baseline gram model in term of wer by when used in isolation a a second pas n best re scoring language model 
this paper present a new method for the simultaneous estimation of lighting direction and shape from shading the method estimate the shape and the lighting direction using a two step iterative process we assume an initial possibly incorrect estimate of the lighting position a stiff deformable model is then fitted to the image assuming this lighting position next a least square estimate of the lighting position is derived from the model using the levenberg marquart method the two step model fitting and lighting position estimation are iterated once the light direction ha converged to a stable solution the deformable model stiffness is lowered and the model fit accurately given the lighting model in addition we show how the method can be used with either orthographic or perspective projection assumption in a variety of experiment on real and synthetic data the method is robust to error both to the initial light position and shape estimate 
we consider the problem of how an agent creates a discrete spatial representation from it continuous interaction with the environment such representation will be the minimal one that explains the experience of the agent in the environment in this paper we take the spatial semantic hierarchy a the agent s target spatial representation and use a circumscriptive theory to specify the minimal model associated with this representation we provide a logic program to calculate the model of the proposed theory we also illustrate how the different level of the representation assume different spatial property about both the environment and the action performed by the agent these spatial property play the role of filter the agent applies in order to distinguish the different environment state it ha visited 
we describe a neurally inspired unsupervised learning algorithm that build a non linear generative model for pair of face image from the same individual individual are then recognized by finding the highest relative probability pair among all pair that consist of a test image and an image whose identity is known our method compare favorably with other method in the literature the generative model consists of a single layer of rate coded non linear feature detector and it ha the property that given a data vector the true posterior probability distribution over the feature detector activity can be inferred rapidly without iteration or approximation the weight of the feature detector are learned by comparing the correlation of pixel intensity and feature activation in two phase when the network is observing real data and when it is observing reconstruction of real data generated from the feature activation 
abstract a novel approach for real time skin segmentation in video sequence is described the approach enables reliable skin segmentation despite wide variation in illumination during tracking an explicit second order markov model is used to predict evolution of the skin color hsv histogram over time histogram are dynamically updated based on feedback from the current segmentation and based on prediction of the markov model the evolution of the skin color distribution at each frame is parameterized by translation scaling and rotation in color space consequent change in geometric parameterization of the distribution are propagated by warping and re sampling the histogram the parameter of the discrete time dynamic markov model are estimated using maximum likelihood estimation and also evolve over time quantitative evaluation of the method wa conducted on labeled ground truth video sequence taken from popular movie 
log linear model provide a statistically sound framework for stochastic unification based grammar subgs and stochastic version of other kind of grammar we describe two computationally tractable way of estimating the parameter of such grammar from a training corpus of syntactic analysis and apply these to estimate a stochastic version of lexical functional grammar 
in this paper we derive a minimal set of sufficient constraint in order for number to constitute a trifocal tensor it is shown that in general eight nonlinear algebraic constraint are enough this result is in accordance with the theoretically expected number of eight independent constraint and novel since the to date known set of sufficient constraint contain at least condition up to now research and formulation of constraint for the trifocal tensor ha concentrated mainly on the correlation slice and ha produced set of constraint that are neither minimal nor independent we show that by turning attention from correlation to homographic slice simple geometric consideration yield the desired result having the minimal set of constraint is important for constrained estimation of the tensor a well a for deepening the understanding of the multiple view relation that are valid in the projective framework 
this paper present an algorithm for text summarization using the thematic hierarchy of a text the algorithm is intended to generate a one page summary for the user thereby enabling the user to skim large volume of an electronic book on a computer display the algorithm first detects the thematic hierarchy of a source text with lexical cohesion measured by term repetition then it identifies boundary sentence at which a topic of appropriate grading probably start finally it generates a structured summary indicating the outline of the thematic hierarchy this paper mainly describes and evaluates the part for boundary sentence identification in the algorithm and then briefly discus the readability of one page summary 
we present a method to recover scene deteriorated by superposition of transparent and semi reflected contribution a appear in reflection off window separating the superimposed contribution from the image in which either contribution is in focus is based on mutual blurring and subtraction of the perturbing component this procedure requires the defocus blur kernel to be known the use of uncalibrated kernel had previously led to contaminated result we propose a method for self calibration of the blur kernel from the raw image themselves the kernel are sought to minimize the mutual information of the recovered layer this relaxes the need for prior knowledge on the optical transfer function experimental result are presented 
the committee approach ha been proposed for reducing modeluncertainty and improving generalization performance the advantageof committee depends on the performance of individualmembers and the correlational structure of error betweenmembers this paper present an input grouping technique for designing a heterogeneous committee with this technique all inputvariables are first grouped based on their mutual information statisticallysimilar variable are assigned to 
non compositional expression present a special challenge to nlp application we present a method for automatic identification of non compositional expression using their statistical property in a text corpus our method is based on the hypothesis that when a phrase is non composition it mutual information differs significantly from the mutual information of phrase obtained by substituting one of the word in the phrase with a similar word 
many collection of data do not come packagedin a form amenable to the ready applicationof machine learning technique nevertheless there ha been only limited researchon the problem of preparing raw data forlearning perhaps because widespread di erencesbetween domain make generalizationdi cult this paper focus on one commonclass of raw data in which the entitiesof interest actually comprise collectionsof smaller piece of homologous data wepresent a technique 
we consider the problem of how an agent creates a discrete spatial representation from it continuous interaction with the environment such representation will be the minimal one that explains the experience of the agent in the environment in this 
decision making in multiagent setting requires significant computational resource agent need to model each other to decide how to coordinate this sometimes may require solving nested model of many other agent and may be impractical to perform in an acceptable time in this paper we investigate way in which the agent can be equipped with flexible decision making procedure to allow multiagent decision making under time pressure one of the technique we implemented us iterative deepening algorithm guided by performance profile generated offline when the interaction involves many agent the algorithm iteratively enhances the quality of coordinated decision making by incrementally adding the level of nesting considered but with the additional penalty of increased running time to identify the appropriate scope of modeling online we use the concept of urgency which represents cost of delaying decision we validate our framework with experiment in a simulated anti air defense domain the contribution of our framework is that it endows our autonomous agent with flexibility to cope with time pressure in complex multiagent setting 
this paper present a real time auditory and visual tracking of multiple object for humanoid under real world environment real time processing is crucial for sensorimotor task in tracking and multiple object tracking is crucial for real world application multiple sound source tracking need perception of a mixture of sound and cancellation of motor noise caused by body movement however it real time processing ha not been reported yet real time tracking is attained by fusing information obtained by sound source localization multiple face recognition speaker tracking focus of attention control and motor control auditory stream with sound source direction are extracted by active audition system with motor noise cancellation capability from khz sampling sound visual stream with face id and d position are extracted by combining skincolor extraction correlation based matching and multiple scale image generation from a single camera these auditory and visual stream are associated by comparing the spatial location and associated stream are used to control focus of attention auditory visual and association processing are performed asynchronously on different pc s connected by tcp ip network the resulting system implemented on an upper torso humanoid can track multiple object with the delay of msec which is forced by visual tracking and network latency 
a cortical model for motion in depth selectivity of complex cell in the visual cortex is proposed the model is based on a time extension of the phase based technique for disparity estimation we consider the computation of the total temporal derivative of the time varying disparity through the combination of the response of disparity energy unit to take into account the physiological plausibility the model is based on the combination of binocular cell characterized by difierent ocular dominance index the resulting cortical unit of the model show a sharp selectivity for motion indepth that ha been compared with that reported in the literature for real cortical cell 
this paper present a unied bias variance decomposition that is applicable to squared loss zero one loss variable misclassication cost and other loss function the unied decomposition shed light on a number of signican t issue the relation between some of the previously proposed decomposition for zero one loss and the original one for squared loss the relation between bias variance and schapire et al s notion of margin and the nature of the trade o between bias and variance in classication while the biasvariance behavior of zero one loss and variable misclassication cost is quite dieren t from that of squared loss this dierence derives directly from the dieren t denitions of loss we have applied the proposed decomposition to decision tree learning instancebased learning and boosting on a large suite of benchmark data set and made several signican t observation 
we describe a method for improving the classification of short text string using a combination of labeled training data plus a secondary corpus of unlabeled but related longer document we show that such unlabeled background knowledge can greatly decrease error rate particularly if the number of example or the size of the string in the training set is small this is particularly useful when labeling text is a labor intensive job and when there is a large amount of information available about a particular problem on the world wide web our approach view the task a one of information integration using whirl a tool that combine database functionality with technique from the information retrieval literature 
this paper proposes a method for packing feature structure which automatically collapse equivalent part of lexical phrasal feature structure of hpsg into a single packed feature structure this method avoids redundant repetition of unification of those part preliminary experiment show that this method can significantly improve a unification speed in parsing 
perspective distortion occlusion and specular reflection are challenging problem in shape from stereo in this paper we review one recently published area based stereo matching algorithm bhat and nayar designed to be robust in these case although the algorithm is an important contribution to stereo matching we show that it coefficient ha a low discriminatory power which lead to a significant number of multiple best match in order to cope with this drawback we introduce a new normalized ordinal correlation coefficient experiment showing the behavior of the proposed coefficient are performed on various datasets including real data with ground truth the new coefficient reduces the occurrence of multiple best match to almost zero per cent it also show a more robust and equally accurate behavior these benefit are achieved at almost no additional computational cost 
a novel method of incorporating shape information into the image segmentation process is presented we introduce a representation for deformable shape and define a probability distribution over the variance of a set of training shape the segmentation process embeds an initial curve a the zero level set of a higher dimensional surface and evolves the surface such that the zero level set converges on the boundary of the object to be segmented at each step of the surface evolution we estimate the maximum a posteriori map position and shape of the object in the image based on the prior shape information and the image information we then evolve the surface globally towards the map estimate and locally based on image gradient and curvature result are demonstrated on synthetic data and medical imagery in d and d 
in this paper we will treat input selection for a radial basis function rbf like classifier within a bayesian framework we approximate the a posteriori distribution over both model coecients and input subset by sample drawn with gibbs update and reversible jump move using some public datasets we compare the classification accuracy of the method with a conventional ard scheme these datasets are also used to infer the a posteriori probability of different input subset 
many real world text contain table in order to process these text correctly and extract the information contained within the table it is important to identify the presence and structure of table in this paper we present a new approach that learns to recognize table in free text including the boundary row and column of table when tested on wall street journal news document our learning approach outperforms a deterministic table recognition algorithm that identifies table recognition algorithm that identifies table based on a fixed set of condition our learning approach is also more flexible and easily adaptable to text in different domain with different table characteristic 
sparse principal component analysis s pca is a novel framework for learning a linear orthonormal basis representation for structure intrinsic to an ensemble of image s pca is based on the discovery that natural image exhibit structure in a low dimensional subspace in a sparse scale dependent form the s pca basis optimizes an objective function which trade off correlation among output coefficient for sparsity in the description of basis vector element this objective function is minimized by a simple robust and highly scalable adaptation algorithm consisting of successive planar rotation of pair of basis vector the formulation of s pca is novel in that multi scale representation emerge for a variety of ensemble including face image image from outdoor scene and a database of optical flow vector representing a motion class 
decision tree induced from stored case are increasingly used to guide case retrieval in case based reasoning cbr system for fault diagnosis and product recommendation in this paper we refer to such a decision tree a an identification tree when a often in practice each of the fault to be identified or available product is represented by a single case in the case library we evaluate common splitting criterion for decision tree in the special case of identification tree we present simplified version of those that are most effective in reducing the average path length of an identification tree or equivalently the average number of question asked when the tree is used for problem solving we also identify condition in which no such reduction is possible with any splitting criterion 
we present an empirical analysis of optimization technique devised to speed up the so called tbox classification supported by description logic system which have to deal with very large knowledge base e g containing more than concept introduction axiom these technique are integrated into the race architecture which implement a tbox and abox reasoner for the description logic alcnhr the described technique consist of adaption of previously known a well a new optimization technique for efficiently coping with these kind of very large knowledge base the empirical result presented in this paper are based on experience with an ontology for the unified medical language system and demonstrate a considerable runtime improvement they also indicate that appropriate description logic system based on sound and complete algorithm can be particularly useful for very large knowledge base 
a new exemplar based probabilistic paradigm for visual tracking is presented probabilistic mechanism are attractive because they handle fusion of information especially temporal fusion in a principled manner exemplar are selected representative of raw training data used here to represent probabilistic mixture distribution of object configuration their use avoids tedious hand construction of object model and problem with change of topology using exemplar in place of a parameterized model pose several challenge addressed here with what we call the metric mixture m approach the m model ha several valuable property principally it provides alternative to standard learning algorithm by allowing the use of metric that are not embedded in a vector space secondly it us a noise model that is learned from training data lastly it eliminates any need for an assumption of probabilistic pixelwise independence experiment demonstrate the effectiveness of the m model in two domain tracking walking people using chamfer distance on binary edge image and tracking mouth movement by mean of a shuffle distance 
in this paper we analyze the performance of clusteringmethods on the task of constructing communitymodels for the user of large web site 
we present an exemplar based object recognition system which is capable of on line learning of representation of scene and object from image sequence local appearance feature are used in a tracking framework to find keyframes of the input sequence during learning the representation of the stored sequence which are used for recognition of novel image consists only of the appearance feature in these key frame and contains no further a priori assumption about the underlying sequence the system is able to create sparse and extendable representation and show good recognition performance in a variety of viewing condition for database of natural and synthetic image sequence 
this long abstract ha been accepted for a full oral presentation at nip ful l paper to appear research supported in part by nih grant ey and nsf grant dm g wahba x lin f gao and d xiang and nih grant ey and ey r klein and b klein the bias variance tradeoff and the randomized gacv grace wahba xiwu lin and fangyu gao 
in this paper we present a machine learning approachto question answering the task is answeringfactual question where the answer are to be foundin document in a large text database we trainedour system on question from the remedia corpus a well a trec development question we then evaluated our system on question ofthe trec question answering task although ourlearning approach only us feature we are ableto achieve quite competitive accuracy the 
this paper present a novel modeling algorithm that is capable of simultaneously recovering correct shape geometry a well a it unknown topology from arbitrarily complicated datasets our algorithm start from a simple seed model of genus zero that can be arbitrarily initiated by user within any dataset the deformable behavior of our model is governed by a locally defined objective function associated with each vertex of the model through the numerical computation of function optimization our algorithm can adaptively subdivide the model geometry automatically detect self collision of the model properly modify it topology because of the occurrence of self collision continuously evolve the model towards the object boundary and reduce fitting error and improve fitting quality via global subdivision commonly used mesh optimization technique are employed throughout the geometric deformation and topological variation in order to ensure the model both locally smooth and globally well conditioned we have applied our algorithm to various real synthetic range data a well a volumetric image data in order to empirically verify and validate it usefulness based on our experiment the new modeling algorithm prof to be very powerful and extremely valuable for shape recovery in computer vision reverse engineering in computer graphic and iso surface extraction in visualization 
in this paper we study the problem of recovering the dshape reflectance and non rigid motion of a dynamic dscene because these property are completely unknown our approach us multiple view to build a piecewisecontinuousgeometric and radiometric representation of thescene s trace in space time basic primitive of this representationis the dynamic surfel which encodes the instantaneouslocal shape reflectance and motion of a smallregion in the scene and enables 
abstract wepresentapowerfulmeta clusteringtechniquecallediterativedouble clustering idc the idc method is a natural extension of the recentdoubleclustering dc methodofslonimandtishbythatexhibited impressive performance on text categorization task usingsyntheticallygenerateddataweempiricallyflndthatwheneverthe dcprocedureissuccessfulinrecoveringsomeofthestructurehidden in the data the extended idc procedure can incrementally compute a signiflcantly more accurate classiflcation idc is especially advantageous when the data exhibit high attribute noise our simulation resultsalso show theefiectiveness ofidcin text categorization problem surprisingly this unsupervised procedure can be competitive with a supervised svm trained with a small training set finally weproposeasimpleandnaturalextensionofidcforsemi supervised and transductive learning where we are given both labeled and unlabeled example 
for many kdd operation such a nearest neighbor search distance based clustering and outlier detection there is an underlying kgr d data space in which each tuple object is represented a a point in the space in the presence of differing scale variability correlation and or outlier we may get unintuitive result if an inappropriate space is used the fundamental question that this paper address is what then is an appropriate space we propose using a robust space transformation called the donoho stahel estimator in the first half of the paper we show the key property of the estimator of particular importance to kdd application involving database is the stability property which say that in spite of frequent update the estimator doe not a change much b lose it usefulness or c require re computation in the second half we focus on the computation of the estimator for high dimensional database we develop randomized algorithm and evaluate how well they perform empirically the novel algorithm we develop called the hybrid random algorithm is in most case at least an order of magnitude faster than the fixed angle and subsampling algorithm 
we describe two new algorithm for two frame structure from motion from tracked point feature one is the first fast algorithm for computing an exact least square estimate it exploit our observation that the rotationally invariant least square error can be written in a simple form that depends just on the motion the other is essentially a accurate a the least square estimate and is more efficient probably faster and potentially more robust than previous algorithm of comparable accuracy we also analyze theoretically the accuracy of the optical flow approximation to the least square error 
the approach of multi perspective enterprise modelling is now more commonly accepted and used in practice a a way to manage knowledge than ever before however the concept of applying multiple modelling language to describe the same domain may still sound frightening to many in addition to the cost time and complexity involved problem such a knowledge sharing between multiple model and achieving and maintaining integrity between them are also important we argue that multi perspective enterprise modelling is helpful and in some situation necessary this paper give example of how formal method such a logical language can provide assistance in making such an approach more appealing and transparent we suggest that the mpm approach is valuable in representing understanding and analysing a complex domain but that much automated support is needed key word knowledge management knowledge sharing multi perspective modelling business modelling bsdm enterprise modelling process modelling knowledge based support tool business process re engineering role activity and communication diagram 
this paper describes and extensively evaluates a system for the automatic routing of submitted paper to reviewer and area committee without the need for any human annotation from the reviewer or the program chair routing is based on a profile of previous writing obtainable on line for the reviewer pool a generally stable and reusable resource that requires no manual adaptation for new submission stream the paper explores a wide set of variation and extension on the core model and achieves system accuracy approaching that of several human judge on the same task 
this paper explains why well known discretizationmethods such a entropy basedand ten bin work well for naive bayesianclassifiers with continuous variable regardlessof their complexity these methodsusually assume that discretized variableshave dirichlet prior since perfect aggregation hold for dirichlets we can show that generally a wide variety of discretizationmethods can perform well with insignificantdifference we identify situation where discretization 
this paper proposes new result in the fieldof software assistant helping user of interactivetools in the task of automatically performingrepetitive task we propose an innovativeintegration of such an assistant intoan interactive programming environment inthis context learning to recognize situation inwhich repetitive task occur is difficult becauselanguages describing user action are complexand because fast learning is mandatory toachieve this goal we propose an 
boolean linear program blps are ubiquitous in ai satisfiability testing planning with resource constraint and winner determination in combinatorial auction are all example of this type of problem although increasingly well informed by work in or current ai research ha tended to focus on specialized algorithm for each type of blp task and ha only loosely patterned new algorithm on effective method from other task in this paper we introduce a single general purpose local search procedure that can be simultaneously applied to the entire range of blp problem without modification although one might suspect that a general purpose algorithm might not perform a well a specialized algorithm we find that this is not the case here our experiment show that our generic algorithm simultaneously achieves performance comparable with the state of the art in satisfiability search and winner determination in combinatorial auction two very different blp problem our algorithm is simple and combine an old idea from or with recent idea from ai 
in this paper we present an analysis of st ylistic variation that us a factor analytic technique to group the variable responsible for the bulk of the linguistic variation found in a corpus of pharmaceutica l leaflet two main factor of variation were found and analysed in more detail they also were compared with other two analysis 
maximum margin classifier such a support vector machine svms critically depends upon the convex hull of the trainingsamples of each class a they implicitly search for the minimumdistance between the convex hull we propose extrapolated vectormachines xvms which rely on extrapolation outside theseconvex hull xvms improve svm generalization very significantlyon the mnist ocr data they share similarity with thefisher discriminant maximize the inter class margin 
we present a comprehensive treatment of d object tracking by posing it a a nonlinear state estimation problem the measurement are derived using the output of shape encoded filter s the nonlinear state estimation is performed by solving the zakai equation and we use the branching particle propagation method for computing the solution the unnormalized conditional density for the solution to the zakai equation is realized by the weight of the particle we fir st sample a set of particle approximating the initial distribution of the state vector conditioned on the observation where each particle encodes the set of geometric parameter of the object the weight of the particle represents geometric and temporal fit which is computed bottom up from the raw image using a shape encodedfilter the particle branch so that the mean numberof offspring is proportional to the weight time update is handled by employing a second order motion model combined with local stochastic search to minimize the prediction error the prediction adjustmentsuggested by system identification theory is empirically verified to contribute to global stability the amount of diffusion is effectively adjusted using a kalman updating of the covariance matrix we have successfully applied this method to human head tracking where we estimate head motion and compute structure using simple head and facial feature model 
this work take place in the context of hierarchical stochastic model for the resolution of discrete inverse problem from low level vision some of these model lie on the node of a quad tree which lead to non iterative inference procedure nevertheless if they circumvent the algorithmic drawback of grid based model computational load and or great dependence on the initialization they admit modeling shortcoming cumbersome and somehow artificial we investigate a new hierarchical stochastic model which take benefit from both the spatial and the hierarchical prior modeling the independence graph is based on a tree which ha been pollarded with the node at the coarsest resolution exhibiting a grid based interaction structure for this class of model we address the critical problem of parameter estimation to this end we derive an em algorithm on the hybrid structure which mix an exact em algorithm on each subtrees and a low cost gibbsian em algorithm on the coarse spatial grid experiment on a synthetic image and on multi spectral satellite image are reported 
om this paper explores the direct motion estimation problem assuming that video rate depth information is available from either stereo camera or other sensor we use these depth measurement in the traditional linear brightness constraint equation and we introduce a new depth constraint equation a a result estimation of certain type of motion such a translation in depth and rotation out of the image plane becomes more robust we derive linear brightness and depth change constraint equation that govern the velocity field in d for both perspective and orthographic camera projection model these constraint are integrated jointly over image region according to a rigidbody motion model yielding a single linear system to robustly track d object pose result are shown for tracking the pose of face in sequence of synthetic and real image for a color version of this paper and for video result sequence see http www interval com paper 
today it is quite common for web page content to include an advertisement since advertiser often want totarget their message to people with certain demographic attribute the anonymity of internet user pose a specialproblem for them the purpose of the present research is to find an effective way to infer demographic information e g gender age or income about people who use the internet but for whom demographic information is nototherwise available our hope is to build a 
a family of resource bounded paraconsistent inference relation is introduced these relation are based on s entailment an inference relation logically weaker than classical entailment and parametrized by a set s of variable their property are investigated especially from the computational complexity point of view among the strong feature of our framework is the fact that tractability is ensured each time s is bounded and that binary connective behave in a classical manner moreover our family is large enough to include both s inference the standard inference relation based on the selection of consistent subbase and some additional form of paraconsistent reasoning a specific case 
content based image retrieval cbir present special challenge in term of how image data is indexed accessed and how end system are evaluated this paper discus the design of a cbir system that us global colour a the primary indexing key and a user centered evaluation of the system visual search tool the result indicate that user are able to make use of a range of visual search tool and that different tool are used at different point in the search process the result also show that the provision of a structured navigation and browsing tool can support image retrieval particularly in situation in which the user doe not have a target image in mind the result are discussed in term of their implication for the design of visual search tool and their implication for the use of user centered evaluation for cbir system 
in this article we propose a new reinforcement learning rl method based on an actor critic architecture the actor andthe critic are approximated by normalized gaussian network ngnet which are network of local linear regression unit thengnet is trained by the on line em algorithm proposed in our previouspaper we apply our rl method to the task of swinging upand stabilizing a single pendulum and the task of balancing a doublependulum near the upright position the experimental 
we present a statistical model for organizing image collection which integrates semantic information provided by associated text and visual information provided by image feature the model is very promising for information retrieval task such a database browsing and searching for image based on text and or image feature furthermore since the model learns relationship between text and image feature it can be used for novel application such a associating word with picture and unsupervised learning for object recognition 
latent semantic indexing lsi dramatically reduces the dimension of the document space by mapping it into a space spanned by conceptual index empirically the number of concept that can represent the document are far fewer than the great variety of word in the textual representation although this almost obviates the problem of lexical matching the mapping incurs a high computational cost compared to document parsing indexing query matching and updating this paper show how lsi is based on a unitary transformation for which there are computationally more attractive alternative this is exemplified by the haar transform which is memory efficient and can be computed in linear to sublinear time the principle advantage of lsi are thus preserved while the computational cost are drastically reduced 
we propose a new model for representing and revising belief structure which relies on a notion of partial language splitting and tolerates some amount of inconsistency while retaining classical logic the model preserve an agent s ability to answer query in a coherent way using belnap s four valued logic axiom analogous to the agm axiom hold for this new model the distinction between implicit and explicit belief is represented and psychologically plausible computationally tractable procedure for query answering and belief base revision are obtained 
we describe a model of document citationthat learns to identify hub and authoritiesin a set of linked document such a pagesretrieved from the world wide web or papersretrieved from a research paper archive unlikethe popular hit algorithm which relieson dubious statistical assumption ourmodel provides probabilistic estimate thathave clear semantics we also find thatin general the identified authoritative documentscorrespond better to human intuition 
we introduce joint feature distribution a general statistical framework for feature based multi image matching that explicitly model the joint probability distribution of corresponding feature across several image conditioning on feature position in some of the image give well localized distribution for their correspondent in the others and hence tight likelihood region for correspondence search we apply the framework in the simplest case of gaussian like distribution over the direct sum affine image and tensor product projective image of the image coordinate this produce probabilistic correspondence model that generalize the geometric multi image matching constraint roughly speaking by a form of model averaging over them these very simple method predict accurate correspondence likelihood region for any scene geometry including planar and near planar scene without illconditioning or explicit model selection small amount of distortion and non rigidity are also tolerated we develop the theory for any number of affine or projective image explain it relationship to matching tensor and give result for an initial implementation 
the gram model at the same model size our analysis show that the high performance of the acm lie in the asymmetry of the model 
the goal of many unsupervised learning procedure is to bring twoprobability distribution into alignment generative model suchas gaussian mixture and boltzmann machine can be cast in thislight a can recoding model such a ica and projection pursuit 
this paper present a new methodology for evaluating the quality of motion estimation and stereo correspondence algorithm motivated by application such a novel view generation and motion compensated compression we suggest that the ability to predict new view or frame is a natural metric for evaluating such algorithm our new metric ha several advantage over comparing algorithm output to true motion or depth first of all it doe not require the knowledge of ground truth data which may be difficult or laborious to obtain second it more closely match the ultimate requirement of the application which are typically tolerant of error in uniform color region but very sensitive to isolated pixel error or disocclusion error in the paper we develop a number of error metric based on this paradigm including forward and inverse prediction error residual motion error and local motion compensated prediction error we show result on a number of widely used motion and stereo sequence many of which do not have associated ground truth data 
in adaptive boosting several weak learner trained sequentiallyare combined to boost the overall algorithm performance recentlyadaptive boosting method for classification problem havebeen derived a gradient descent algorithm this formulation justifieskey element and parameter in the method all chosen tooptimize a single common objective function we propose an analogousformulation for adaptive boosting of regression problem utilizing a novel objective function that lead to a 
domain specific search engine are becoming increasingly popular because they offer increased accuracy and extra feature not possible with general web wide search engine unfortunately they are also difficult and time consuming to maintain this 
in this paper we report result of an investigation into englishjapanese cross language information retrieval clir comparing a number of query translation method result from experiment using the standard bmir j japanese collection suggest that full machine translation mt can outperform popular dictionary based query translation method and further that in this context mt is largely robust to query with little linguistic structure 
this paper describes an active character recognitionmethodology henceforth referred to a acr wepresentinthis paper a method that us an active heuristic functionsimilar to the one used by a search algorithm that adaptivelydetermines the length of the feature vector a well asthe feature themselves used to classify an input pattern acr adapts to factor such a the quality of the inputpattern it intrinsic similarity and difference from patternsof other class it is being 
a non metric pan tilt stereo head consists of a weakly calibrated stereo rig mounted on a pan tilt mechanism it is called non metric since neither the kinematics of the mechanism nor camera calibration are required the lie group of projective rotation homographies of projective space corresponding to pure rotation is an original formalism to model the geometry of such a pan tilt system a rodrigues alike formula a well a a minimal parameterization of projective rotation are introduced based on this the practical part devise a numerical optimization technique for accurately estimating projective rotation from point correspondence only this procedure recovers sufficient geometry to operate the system the experiment validate and evaluate the proposed approach on real image data they show the weak calibration image prediction and homing of a non metric pan tilt head 
under normal viewing condition human find it easy to distinguish between object made out of different material such a plastic metal or paper untextured material such a these have different surface reflectance property including lightness and gloss with single isolated image and unknown illumination condition the task of estimating surface reflectance is highly underconstrained because many combination of reflection and illumination are consistent with a given image in order to work out how human estimate surface reflectance property we asked subject to match the appearance of isolated sphere taken out of their original context we found that subject were able to perform the task accurately and reliably without contextual information to specify the illumination the sphere were rendered under a variety of artificial illumination such a a single point light source and a number of photographically captured real world illumination from both indoor and outdoor scene subject performed more accurately for stimulus viewed under real world pattern of illumination than under artificial illumination suggesting that subject use stored assumption about the regularity of real world illumination to solve the ill posed problem 
the work of the russian mathematician vladimir vapnik at t lab enables u to go back to the root of theoretical statistic leaving behind fisher s parameter in favor of the general approach started in the s by glivenko cantelli kolmogorov nowadays it ha become possible to model million of event described by thousand of variable within a reasonable time for a specific application the srm approach work with a family of model and calibrates the family of model to a point which is the best compromise between accuracy and robustness it also measure the complexity of the model using vc dimension which is not plagued by number of parameter hence model for large event described by several parameter can be generalized this open up great prospect in numerous field like customer relationship management network optimization risk management manufacturing yield management and a number of other data rich problem 
we improve the promising colour by correlation method for computational colour constancy by modifying it to work in a three dimensional colour space the previous version of the algorithm us only the chromaticity of the input and thus cannot make use of the information inherent in the pixel brightness which previous work suggests is useful we develop the algorithm for the mondrian world matte surface the mondrian world with fluorescent surface and the mondrian world with specularities we test the new algorithm on synthetic data and on a data set of carefully calibrated image we find that on the synthetic data the new algorithm significantly out performs all other colour constancy algorithm in the case of image data the result are also promising the new algorithm doe significantly better than it chromaticity counter part and it performance approach that of the best algorithm since the research into the method is still young we are hopeful that the performance gap between the real and synthetic case can be narrowed 
we present work on the automatic generation of short indicative informative abstract of scientific and technical article the indicative part of the abstract identifies the topic of the document while the informative part of the abstract elaborate some topic according to the reader s interest by motivating the topic describing entity and defining concept we have defined our method of automatic abstracting by studying a corpus professional abstract the method also considers the reader s interest a essential in the process of abstracting 
the goal of path analysis is to understand visitor navigation of a web site the fundamental analysis component is a path a path is a finite sequence of element typically representing url or group of url a full path is an abstraction of a visit or a session which can contain attribute described below subpaths represent interesting subsequence of the full path path analysis provides user configurable extraction filtering preprocessing noise reduction descriptive statistic and detailed analysis of three basic specific object element sub path and couple of element in each case list of frequent object subject to particular filtering and sorting are available we call the corresponding interactive tool element path and couple analyzer we also allow in depth exploration of individual element path and couple element explorer investigates composition and convergence of traffic through an element and allows conditioning based on the number of preceding succeeding step path explorer visualizes in and out flow of a path and attrition rate along the path couple explorer present distinct path connecting couple element along with measure of their association and some additional statistic 
we present in this article a system which improves a dem the method reconstructs all the facade of the building and then corrects the initial dem by deleting the point of the roof which pas over the boundary defined by the facade we correct the shape of the building with the initial photograph to have sharp contour the proposed approach doe not use any a priori information about the orientation of the facade and the shape of the building we present result with synthetic and real image 
many large markov decision process mdps can be represented compactly using a structured representation such a a dynamic bayesian network unfortunately the compact representation doe not help standard mdp algorithm because the value function for the mdp doe not retain the structure of the process description we argue that in many such mdps structure is approximately retained that is the value function are nearly additive closely approximated by a linear function over factor associated with small subset of problem feature based on this idea we present a convergent approximate value determination algorithm for structured mdps the algorithm maintains an additive value function alternating dynamic programming step with step that project the result back into the restricted space of additive function we show that both the dynamic programming and the projection step can be computed efficiently despite the fact that the number of state is exponential in the number of state variable 
the main aim of this paper is to analyse the effect of applying pronominal anaphora resolution to question answering qa system for this task a complete qa system ha been implemented system evaluation measure performance improvement obtained when information that is referenced anaphorically in document is not ignored 
this paper establishes a link between uncalibrated stereo vision and the motion of rigid and articulated body the variation in the projective reconstruction of a dynamic scene over time allows an uncalibrated stereo rig to be used a a faithful motion capturing device we introduce an original theoretical framework projective kinematics which allows rigid and articulated motion to be represented within the transformation group of projective space corresponding projective velocity are defined in the tangent space most importantly these projective motion inherit the lie group structure of the displacement group these theoretical result lead immediately to non metric formulation of visual serving tracking motion capturing and motion synthesis system that no longer require the metric geometry of a stereo camera or of the articulated body to be known we report on such a non metric formulation of a visual serving system and present simulated experimental result 
we investigate the following data mining problem from computational chemistry from a large data set of compound find those that bind to a target molecule in a few iteration of biological testing a possible in each iteration a comparatively small batch of compound is screened for binding to the target we apply active learning technique for selecting the successive batch one selection strategy pick unlabeled example closest to the maximum margin hyperplane another produce many weight vector by running perceptrons over multiple permutation of the data each weight vector vote with it prediction and we pick the unlabeled example for which the prediction is most evenly split between and for a third selection strategy note that each unlabeled example bisects the version space of consistent weight vector we estimate the volume on both side of the split by bouncing a billiard through the version space and select unlabeled example that cause the most even split of the version space we demonstrate that on two data set provided by dupont pharmaceutical that all three selection strategy perform comparably well and are much better than selecting random batch for testing 
lbr is a lazy semi naive bayesian classifier learning technique designed to alleviate the attribute interdependence problem of naive bayesian classification to classify a test example it creates a conjunctive rule that selects a most appropriate subset of training example and induces a local naive bayesian classifier using this subset lbr can significantly improve the performance of the naive bayesian classifier a bias and variance analysis of lbr reveals that it significantly reduces the 
image alignment is one of the most important task incomputer vision in this paper we explicitly model spatialillumination variation by low order polynomial functionsin an energy minimization framework data constraintsfor the alignment and illumination parameter are derivedfrom the first order taylor approximation of a generalizedbrightness assumption we formulate the parameterestimation problem in a weighted least square frameworkby using the influence function from robust 
the human visual system encodes the chromatic signal conveyedby the three type of retinal cone photoreceptors in an opponentfashion this color opponency ha been shown to constitute anefficient encoding by spectral decorrelation of the receptor signal we analyze the spatial and chromatic structure of natural scene bydecomposing the spectral image into a set of linear basis functionssuch that they constitute a representation with minimal redundancy independent component 
a decentralized multiagent system comprises agent who act autonomously based on local knowledge achieving coordination in such a system is nontrivial hut is essential in most application where disjointed or incoherent behavior would be undesirable coordination in decentralized system is a richer phenomenon than previously believed in particular five major attribute are crucial the extent of the local knowledge and choice of the member agent the extent of their shared knowledge the level of their inertia and the level of precision of the required coordination interestingly precision and inertia turn out to control the coordination process they define different region within each of which the other attribute relate nicely with coordination but among which their relationship are altered or even reversed based on our study we propose simple design rule to obtain coordinated behavior in decentralized multiagent system 
we present a method to learn object class model from unlabeled and unsegmented cluttered scene for the purpose of visual object recognition we focus on a particular type of model where object are represented a flexible constellation of rigid part feature the variability within a class is represented by a joint probability density function pdf on the shape of the constellation and the output of part detector in a first stage the method automatically identifies distinctive part in the training set by applying a clustering algorithm to pattern selected by an interest operator it then learns the statistical shape model using expectation maximization the method achieves very good classification result on human face and rear view of car 
we present a parsing system built from a handwritten lexicon and grammar and trained on a selection of the brown corpus on the sentence it can parse the parser performs a well a purely corpus based parser it advantage lie in the fact that it syntactic analysis readily support semantic interpretation moreover the system s hand written foundation allows for a more fully lexicalized probabilistic model i e one sensitive to co occurrence of lexical head of phrase constituent 
dynamic contour or snake provide an effective method for tracking complex moving object for segmentation and recognition task but have difficulty tracking occluding boundary on cluttered background to compensate for this shortcoming dynamic contour often rely on detailed object shape or motion model to distinguish between the boundary of the tracked object and other boundary in the background in this paper we present a complementary approach to detailed object model we impr ove the discriminative power of the local image measurement that drive the tracking process we describe a new robust external energy term for dynamic contour that can track occluding boundary without detailed object model we show how our image model improves tracking in cluttered scene and describe how a fi ne grained image segmentation mask is created directly from the local ima ge measurement used for tracking tracking boundary tracking visual feature in a series of image is an important task both for vision based control and for rotoscoping application dynamic contour ka and related active tracking technique are well suited for both application because they combine simple light weight object model with rapid update dynamic contour track boundary by minimizing the sum of an external force from a local image measure and an internal force from a shape dynamic model a dynamic contour track the indicated boundary by fi nding the shape that minimizes the combined external and inter nal force the external force dri f the dynamic contour according to the current image appearance the internal force increase the spatial and temporal continuity of the tracked boundary dynamic contour usually employ a simple image con 
in this paper we outline a theory of referential accessibility called vein theory vt we show how vt address the problem of left satellite currently a problem for stack based model and show that vt can be used to significantly reduce the search space for antecedent we also show that vt provides a better model for determining domain of referential accessibility and discus how vt can be used to address various issue of structural ambiguity 
this paper present an unsupervised learning algorithm that can derive the probabilistic dependence structure of part of an object a moving human body in our example automatically from unlabeled data the distinguished part of this work is that it is based on unlabeled data i e the training feature include both useful foreground part and background clutter and the correspondence between the part and detected feature are unknown we use decomposable triangulated graph to depict the probabilistic independence of part but the unsupervised technique is not limited to this type of graph in the new approach labeling of the data part assignment is taken a hidden variable and the em algorithm is applied a greedy algorithm is developed to select part and to search for the optimal structure based on the differential entropy of these variable the success of our algorithm is demonstrated by applying it to generate model of human motion automatically from unlabeled real image sequence 
this paper present a work we have done on the motion detection in the context of an outdoor traffic scene for visual surveillance purpose our motion detection algorithm is based both on background subtraction and three frame difference we propose quite innovative solution for denoising blob filling and shadow detection without exploiting any a priori knowledge actually method presented here have been fully setup only for the former technique target sequence is made of bit grey level still image taken at fps this application work off line at fps on a mhz pentium iii computer 
user waiting time for information on the www may be reduced by pre sending document they are likely to request albeit at a possible expense of additional transmission cost in this paper we describe a prediction model which anticipates the document a user is likely to request next and present a decision theoretic approach for pre sending document based on the prediction made by this model we introduce two evaluation method which measure the immediate and the eventual benefit of pre sending a document we use these evaluation method to compare the performance of our decision theoretic policy to that of a naive pre sending policy and to identify the domain parameter configuration for which each of these policy provides a clear overall benefit to the user 
the www is the most important resource for external business information this paper present a tool called insyder an information assistant for finding and analysis business information from the www insyder is a system using different agent for crawling the web evaluating and visualising the result these agent the used visualisation and a first summary of user study are presented 
stochastic fluctuation of voltage gated ion channel generate current and voltage noise in neuronal membrane this noise may be a critical determinant of the efficacy of information processing within neural system using monte carlo simulation we carry out a systematic investigation of the relationship between channel kinetics and the resulting membrane voltage noise using a stochastic markov version of the mainen sejnowski model of dendritic excitability in cortical neuron our simulation show that kinetic parameter which lead to an increase in membrane excitability increasing channel density decreasing temperature also lead to an increase in the magnitude of the sub threshold voltage noise noise also increase a the membrane is depolarized from rest towards threshold this suggests that channel fluctuation may interfere with a neuron s ability to function a an integrator of it syn aptic input and may limit the reliability and precision of neural informatio n processing 
in this paper we present a thorough evaluation of a corpus resource for portuguese cetemp blico a million word newspaper corpus free for r d in portuguese processing we provide information that should be useful to those using the resource and to considerable improvement for later version in addition we think that the procedure presented can be of interest for the larger nlp community since corpus evaluation and description is unfortunately not a common exercise 
the problem of controlling the capacity of decision tree is considered for the case where the decision node implement linear threshold function in addition to the standard early stopping and pruning procedure we implement a strategy based on the margin of the decision boundary at the node the approach is motivated by bound on generalization error obtained in term of the margin of the individual classifier experimental result are given which demonstrate that considerable advantage can be derived from using the margin information the same strategy is applied to the problem of transduction where the position of the testing point are revealed to the training algorithm this information is used to generate an alternative training criterion motivated by transductive theory in the transductive case the result are not a encouraging suggesting that little if any consistent advantage is culled from using the unlabelled data in the proposed fashion this conclusion doe not contradict theoretical result but leaf open the theoretical and practical question of whether more effective use can be made of the additional information 
the similarity join is an important operation for mining high dimensional feature space given two data set the similarity join computes all tuples x y that are within a distance egr one of the most efficient algorithm for processing similarity join is the multidimensional spatial join msj by koudas and sevcik in our previous work pursued for the two dimensional case we found however that msj ha several performance shortcoming in term of cpu and i o cost a well a memory requirement therefore msj is not generally applicable to high dimensional data in this paper we propose a new algorithm named generic external space sweep ge ge introduces a modest rate of data replication to reduce the number of expensive distance computation we present a new cost model for replication an i o model and an inexpensive method for duplicate removal the principal component of our algorithm is a highly flexible replication engine our analytical model predicts a tremendous reduction of the number of expensive distance computation by several order of magnitude in comparison to msj factor in addition the memory requirement of ge are shown to be lower by several order of magnitude furthermore the i o cost of our algorithm is by factor better independent from the fact whether replication occurs or not our analytical result are confirmed by a large series of simulation and experiment with synthetic and real high dimensional data set 
qualitative and quantitative representation of space in general and motion in particular have their typical field of application which are unified in an autonomously moving robot interacting with human being therefore it is necessary to make some consideration on both approach when dealing with such a robot this paper present quantitative and qualitative representation of locomotion and algorithm to deal with them this work wa applied to the navigation of a semi autonomous wheelchair along route in network of corridor 
collaborative filtering is a technique for recommending document to user based on how similar their taste are to other user if two user tend to agree on what they like the system will recommend the same document to them the generalized vector space model of information retrieval represents a document by a vector of it similarity to all other document the process of collaborative filtering is nearly identical to the process of retrieval using gvsm in a matrix of user rating using this observation a model for filtering collaboratively using document content is possible 
we report the result of a study on topic spotting in conversational speech using a machine learning approach we build classifier that accept an audio file of conversational human speech a input and output an estimate of the topic being discussed our methodology make use of a well known corpus of transcribed and topic labeled speech the switchboard corpus and involves an interesting double use of the boostexter learning algorithm our work is distinguished from previous effort in topic spotting by our explicit study of the effect of dialogue length on classifier performance and by our use of off the shelf speech recognition technology one of our main result is the identification of a single classifier with good performance relative to our classifier space across all subdialogue length 
in this paper brill s rule based po tagger is tested and adapted for hungarian it i s s hown that t he present system doe not obtain a high accuracy for hungarian a it doe for english and other germanic language because of the structural difference between these language hungarian un like english ha rich morphology is agglutinative with some inflectional characteristic and h a fairly free word order the tagger ha the greatest difficulty with part of speech belonging to open class because of their complicated morphological structure it is shown that the accuracy of tagging can be increased from approximately to by simply changing the rule generating mechanism namely the lexical template in the lexical training module 
the problem of neural coding is to understand how sequence ofaction potential spike are related to sensory stimulus motor output or ultimately thought and intention one clear questionis whether the same coding rule are used by dierent neuron orby corresponding neuron in dierent individual we present aquantitative formulation of this problem using idea from informationtheory and apply this approach to the analysis of experimentsin the y visual system we nd 
in this paper we present a procedure for organizing real world scene along semantic ax the approach is based on the output energy of linear discriminant filter that take into account or not spatial information we introduce three semantic ax along which picture are ordered the main semantic axis computes the degree of naturalness of a scene then urban picture are evaluated according to their degree of verticalness and natural scene according to their degree of openness we observe the emergence of typical scene category such a beach mountain skyscraper city center etc along the ax 
domain specific search engine are becoming increasingly popular because they offer increased accuracy and extra feature not possible with general web wide search engine unfortunately they are also difficult and time consuming to maintain this 
this paper report on our work and resultsframing signal processing algorithm optimizationas a machine learning task a singlesignal processing algorithm can be representedby many different but mathematicallyequivalent formula when these formulasare implemented in actual code they havevery different running time signal processingoptimization is concerned with finding aformula that implement the algorithm a efficientlyas possible unfortunately a correctmapping 
clickstream data collected at any web site site centric data is inherently incomplete since it doe not capture user browsing behavior across site user centric data hence model learned from such data may be subject to limitation the nature of which ha not been well studied understanding the limitation is particularly important since most current personalization technique are based on site centric data only in this paper we empirically examine the implication of learning from incomplete data in the context of two specific problem a predicting if the remainder of any given session will result in a purchase and b predicting if a given user will make a purchase at any future session for each of these problem we present new algorithm for fast and accurate data preprocessing of clickstream data based on a comprehensive experiment on user level clickstream data gathered from user browsing behavior we demonstrate that model built on user centric data outperform model built on site centric data for both prediction task 
predictive model developed by applying data mining technique are used to improve forecasting accuracy in the airline business in order to maximize the revenue on a flight the number of seat available for sale is typically higher than the physical seat capacity overbooking to optimize the overbooking rate an accurate estimation of the number of no show passenger passenger who hold a valid booking but do not appear at the gate to board for the flight is essential currently no show on future flight are estimated from the number of no show on historical flight averaged on booking class level in this work classification tree and logistic regression model are applied to estimate the probability that an individual passenger turn out to be a no show passenger information stored in the reservation system of the airline is either directly used a explanatory variable or used to create attribute that have an impact on the probability of a passenger to be a no show the total number of no show in each booking class or on the total flight is then obtained by accumulating the individual no show probability over the entity of interest we show that this forecasting approach is more accurate than the currently used method in addition the selected model lead to a deepened insight into passenger behavior 
we describe a lightweight learning method that induces an ensemble of decision rule solution for regression problem instead of direct prediction of a continuous output variable the method discretizes the variable by k mean clustering and solves the resultant classification problem prediction on new example are made by averaging the mean value of class with vote that are close in number to the most likely class we provide experimental evidence that this indirect approach can often yield strong result for many application generally outperforming direct approach such a regression tree and rivaling bagged regression tree 
optimization method based on iterative scheme can be divided into two class linesearch method and trust region method while linesearch technique are commonly found in various vision application not much attention is paid to trust region method motivated by the fact that linesearch method can be considered a special case of trust region method we propose to apply trust region method to visual tracking problem our approach integrates trust region method with the kullback leibler distance to track a rigid or non rigid object in real time if not limited by the speed of a camera the algorithm can achieve frame rate above fps to justify our method a variety of experiment comparison are carried out for the trust region tracker and a linesearch based mean shift tracker with same initial condition the experimental result support our conjecture that a trust region tracker should perform superiorly to a linesearch one 
we present result of machine learning experiment designed to identify user correction of speech recognition error in a corpus collected from a train information spoken dialogue system we investigate the predictive power of feature automatically computable from the prosody of the turn the speech recognition process experimental condition and the dialogue history our best performing feature reduce classification error from baseline of to 
many web page have implicit structure in this paper we show the feasibility of automatically extracting data from web page by using approximate matching technique this can be applied to generate automatic wrapper or to notify display web page difference web page change monitoring etc 
in this paper we show how we can learn to select good word for a document title we view the problem of selecting good title word for a document a a variant of an information retrieval problem each title word is treated a a document and selection of appropriate title word a finding relevant document based on our training collection consisting of document and title pair we learn the document representation for all the title word and apply these learned representation to select appropriate title word over test document compared to other learning approach namely k nearest neighbor approach a na ve bayesian approach and a variant of a machine translation model we find that our approach is significantly better a indicated by the f metric 
this paper present a practical technique for modelbased d hand tracking an anatomically accurate hand model is built from truncated quadric this allows for the generation of d profile of the model using elegant tool from projective geometry and for an efficient method to handle self occlusion the pose of the hand model is estimated with an unscented kalman filter ukf which minimizes the geometric error between the profile and edge extracted from the image the use of the ukf permit higher frame rate than more sophisticated estimation method such a particle filtering whilst providing higher accuracy than the extended kalman filter the system is easily scalable from single to multiple view and from rigid to articulated model first experiment on real data using one and two camera demonstrate the quality of the proposed method for tracking a dof hand model 
curve evolution scheme for segmentation implemented with level set method have become an important approach in computer vision previous work ha modeled evolving contour which are curve in d or surface in d our objective is to explore recent mathematical work enabling the evolution of manifold of higher co dimension we consider d curve in d codimension two for the application of automatically segmenting blood vessel in volumetric magnetic resonance angiography mra image this paper describes the theoretical foundation of our system curve then provides segmentation result compared against segmentation obtained interactively by a neurosurgeon segmentation of bronchus in lung computed tomography ct scan are also presented the new experiment comparison to manual segmentation and sample comparison to the use of a codimension one regularization force are the primary contribution of this report 
we consider the problem of how an agent creates a discrete spatial representation from it continuous interaction with the environment such representation will be the minimal one that explains the experience of the agent in the environment in this 
in this paper parametric statistical modelling of distribution of colour camera data is discussed a review is provided with some analysis of the property of some common model which a re generally based on an assumption of i ndependence of the c hromaticity and intensity c omponents of colour data result of an empirical comparison of t he performance of various model are also reviewed these result indicate that such model are not appropriate for situation other than highly controlled environment in particular they perform poorly for daylight imagery here a modification to existing statistical colour model is proposed an d the resultant new model are assessed using the same methodology a for the previous result this simple modification which is based on the inclusion of an ambient term in the underlying physical model is s hown to have a major impact on the performance of the model in le constrained daylight environment 
previous research ha shown that the plausibility of an adjective noun combination is correlated with it corpus co occurrence frequency in this paper we estimate the co occurrence frequency of adjective noun pair that fail to occur in a million word corpus using smoothing technique and compare them to human plausibility rating both class based smoothing and distance weighted averaging yield frequency estimate that are significant predictor of rated plausibility which provides independent evidence for the validity of these smoothing technique 
this paper describes an algorithm for generating compact d model of indoor environment with mobile robot our algorithm employ the expectation maximization algorithm to fit a lowcomplexity planar model to d data collected by range finder and a panoramic camera the complexity of the model is determined during model fitting by incrementally adding and removing surface in a final post processing step measurement are converted into polygon and projected onto the surface model where possible empirical result obtained with a mobile robot illustrate that high resolution model can be acquired in reasonable time 
in the missing data approach to improving the robustness of automatic speech recognition to added noise an initial process identifies spectraltemporal region which are dominated by the speech source the remaining region are considered to be missing in this paper we develop a connectionist approach to the problem of adapting speech recognition to the missing data case using recurrent neural network in contrast to method based on hidden markov model rnns allow u to make use of long term time constraint and to make the problem of classification with incomplete data and imputing missing value interact we report encouraging result on an isolated digit recognition task 
broad coverage corpus annotated with semantic role or argument structure information are becoming available for the first time statistical system have been trained to automatically label semantic role from the output of statistical parser on unannotated text in this paper we quantify the effect of parser accuracy on these system performance and examine the question of whether a flatter chunked representation of the input can be a effective for the purpose of semantic role identification 
abstract spike triggered averaging technique are effective for linear characteri zation of neural response but neuron exhibit important nonlinear be haviors such a gain control that are not captured by such analysis we describe a spike triggered covariance method for retrieving suppres sive component of the gain control signal in a neuron the method in simulation and on salamander retinal ganglion cell data analysis of physiological data reveals meaningful suppressive ax and explains interesting nonlinearities we expect this method to be applica ble to other sensory area and modality white noise analysis ha emerged a a powerful technique for characterizing response prop erties of spiking neuron a sequence of stimulus are drawn randomly from an ensemble and presented in rapid succession and one examines the subset that elicit action potential spike triggered stimulus ensemble can provide information about the neuron s response characteristic in the most widely used form of this analysis one estimate an excitatory linear kernel by computing the spike triggered average sta that is the mean stimulus that elicited a spike e g under the assumption that spike are generated by a poisson process with instantaneous rate determined by linear projection onto a kernel fol lowed by a static nonlinearity the sta provides an unbiased estimate of this kernel recently a number of author have developed interesting extension of white noise anal ysis some have examined spike triggered average in a reduced linear subspace of input stimulus e g others have recovered excitatory subspace by computing the spike triggered covariance stc followed by an eigenvector analysis to determine the subspace ax e g sensory neuron exhibit striking nonlinear behavior that are not explained by fundamen tally linear mechanism for example the response of a neuron typically saturates for large amplitude stimulus the response to the optimal stimulus is often suppressed by the presence of a non optimal mask e g and the kernel recovered from sta analysis may change shape a a function of stimulus amplitude e g iors can be attributed to gain control e g in which neural response are suppressively modulated by a gain signal derived from the stimulus derlying mechanism and time scale associated with such gain control are current topic of research the basic functional property appear to be ubiquitous occurring throughout the nervous system howard hughes medical inst center for neural science edu new york university eero 
we present hemp a novel learning algorithm designed to operate in the domain of markov process and markov decision process hemp learns to perform a 
this paper introduces a foundation for inductive learning based on the use of higher order logic for knowledge representation in particular the paper i provides a systematic individual a term approach to knowledge representation for inductive learning and demonstrates the utility of type and higher order construct for this purpose ii introduces a systematic way to construct predicate for use in induced definition and iii widens the applicability of decision tree algorithm beyond the usual attribute value setting to the classification of individual with complex internal structure the paper contains several illustrative application the effectiveness of the approach is demonstrated by applying the decision tree learning system to two benchmark problem 
i describe a framework for interpreting support vector machine svms a maximum a posteriori map solution to inferenceproblems with gaussian process prior this can provide intuitiveguidelines for choosing a good svm kernel it can also assign by evidence maximization optimal value to parameter such asthe noise level c which cannot be determined unambiguously fromproperties of the map solution alone such a cross validation error i illustrate this using a simple 
in this paper we present a method that can automatically evaluate performance of different term weighting scheme in information retrieval without resorting to precision recall based on human relevance judgment specifically the problem is given two document term matrix generated from two different term weighting scheme can we tell which term weighting scheme will performance better than the other we propose a meta scoring function which take a input the document term matrix generated by some term weighting scheme and computes a goodness score from the document term matrix in our experiment we found out that this score is highly correlated with the precision recall measurement for all the collection and term weighting schema we tried thus we conclude that our meta scoring function can be a substitute for the precision recall measurement that need relevance judgment of human subject furthermore this meta scoring function is not limited only to text information retrieval can be applied to field such a image and dna retrieval 
hierarchical learning machine are non regular and non identifiable statistical model whose true parameter set are analytic set with singularity using algebraic analysis we rigorously prove that the stochastic complexity of a non identifiable learning machine is asymptotically equal to log n m log log n const where n is the number of training sample moreover we show that the rational number and the integer m can be algorithmically calculated using resolution of singularity in algebraic geometry also we obtain inequality d and m d where d is the number of parameter 
traditionally coreference is resolved by satisfying a combination of salience syntactic semantic and discourse constraint the acquisition of such knowledge is time consuming difficult and error prone therefore we present a knowledge minimalist methodology of mining coreference rule from annotated text corpus semantic consistency evidence which is a form of knowledge required by coreference is easily retrieved from wordnet additional consistency knowledge is discovered by a meta bootstrapping algorithm applied to unlabeled text 
best first chart parsing utilises a figure of merit fom to efficiently guide a parse by first attending to those edge judged better in the past it ha usually been static this paper will show that with some extra information a parser can compensate for fom flaw which otherwise slow it down our result are faster than the prior best by a factor of and the speedup is won with no significant decrease in parser accuracy 
we describe a computer system that provides a real time musicalaccompaniment for a live soloist in a piece of non improvisedmusic for soloist and accompaniment a bayesian network is developedthat represents the joint distribution on the time at whichthe solo and accompaniment note are played relating the twoparts through a layer of hidden variable the network is first constructedusing the rhythmic information contained in the musicalscore the network is then trained to 
abstract this paper discus the use of unlabeled example for the problem of named entity classification a large number of rule is needed for coverage of the domain suggesting that a fairly large number of labeled example should be required to train a classifier however we show that the use of unlabeled data can reduce the requirement for supervision to just simple seed rule the approach gain leverage from natural redundancy in the data for many named entity instance both the spelling of the name and the context inwhich it appears are sufficient to determine it type we present two algorithm the first method us a similar algorithm to that of yarowsky with modification motivated by blum and mitchell the second algorithm extends idea from boosting algorithm designed for supervised learning task to the framework suggested by blum and mitchell 
a framework for modeling and recognition of temporal activity is proposed the modeling of set of exemplar activity is achieved by parameterizing their representation in the form of principal component recognition of spatio temporal variant of modeled activity is achieved by parameterizing the search in the space of admissible transformation that the activity can undergo experiment on recognition of articulated and deformable object motion from image motion parameter are presented 
this paper describes a novel view based learning algorithm for d object recognition from d image using a network of linear unit the snow learning architecture is a sparse network of linear function over a pre defined or incrementally learned feature space and is specifically tailored for learning in the presence of a very large number of feature we use pixel based and edge based representation in large scale object recognition experiment in which the performance of snow is compared with that of support vector machine svms and nearest neighbor using the object in the columbia image object database coil experimental result show that the snow based method outperforms the svm based system in term of recognition rate and the computational cost involved in learning most importantly snow s performance degrades more gracefully when the training data contains fewer view the empirical result also provide insight into practical and theoretical consideration on view based method for d object recognition 
we consider the registration of sequence of image where the observed scene is entirely non rigid for example a camera flying over water a panning shot of a field of sunflower in the wind or footage of a crowd applauding at a sport event in these case it is not possible to impose the constraint that world point have similar colour in successive view so existing registration technique cannot be applied indeed the relationship between a point s colour in successive frame is essentially a random process however by treating the sequence of image a a set of sample from a multidimensionalstochastic time series we can learn a stochastic model e g an ar model of the random process which generated the sequence of image with a static camera this stochastic model can be used to extend the sequence arbitrarily in time driving the model with random noise result in an infinitely varying sequence of image which always look like the short input sequence in this way we can create videotextures which can play forever without repetition with a moving camera the image generation process comprisestwo component astochastic componentgenerated by the videotexture and a parametric component due to the camera motion for example a camera rotation induces a relationship between successive image which is modelled by a point perspective transformation or homography human observer can easily separate the camera motion from the stochastic element the key observation for an automatic implementation is that without image registration the time series analysis must work harder to model the combined stochastic and parametric image generation specifically the learned model will require more component or more coefficient to achieve the same expressive power a for the static scene with the correct registration the model will be more compact therefore by searching for the registration parameter which result in the most parsimoniousstochastic model we can register sequence where there is only stochastic rigidity the paper describes an implementation of this scheme and show result on a number of example sequence 
we expect a variety of autonomous system from rover to life support system to play a critical role in the success of future space mission the crew and ground support personnel will want to control and be informed by these system at varying level of detail depending on the situation moreover these system will need to operate safely in the presence of people and cooperate with them effectively we call such autonomous system human centered in contrast with traditional black box autonomous system our goal is to design a framework for human centered autonomous system that enables user to interact with these system at whatever level of control is most appropriate whenever they so choose but minimize the necessity for such interaction this paper discus on going research at the nasa ames research center and the johnson space center in developing human centered autonomous system that can be used for space mission 
visual inspection of neuron suggests that dendritic orientation may be determined both by internal constraint e g membrane tension and by external vector field e g neurotrophic gradient for example basal dendrite of pyramidal cell appear nicely fan out this regular orientation is hard to justify completely with a general t endency to grow straight given the z igzags observed experimentally instead dendrite could a favor a fixed external direction or b repel from t heir own soma to investigate these possibility quantitatively reconstructed h ippocampal cell were subjected to bayesian analysis the statistical model combined linearly factor a and b a well a the tendency to grow straight for all morphological class b wa found to be significantly positive and consistently greater than a in addition when dendrite were artificially re oriented according to this model the resulting structure closely resembled real morphology these result suggest that somatodendritic repulsion may play a role in determining dendritic orientation since hippocampal cell are very densely packed and their dendritic tree highly overlap the repulsion must be ce llspecific we discus possible mechanism underlying such specificity 
operation and unit of memory in many application this large computational demand may be prohibitive here we suggest an approach to reduce the computational effort relying on the relatively small dimension denoted o f the partial kl basis that is usually needed we propose an algorithm that doe not require to store the entire set of input image before proceeding to the calculation of the kl basis rather it take the image in small block and update the required kl basis sequentially 
in many practical learning scenario there isa small amount of labeled data along witha large pool of unlabeled data many supervisedlearning algorithm have been developedand extensively studied we presenta new quot co training quot strategy for using unlabeleddata to improve the performanceof standard supervised learning algorithm unlike much of the prior work such a theco training procedure of blum and mitchell we do not assume there are two redundantviews both of 
the paper introduces logan h a system for learning first order function free horn expression from interpretation the system is based on an algorithm that learns by asking question and that wa proved correct in previous work the current paper show how the algorithm can be implemented in a practical system and introduces a new algorithm based on it that avoids interaction and learns from example only the logan h system implement these algorithm and add several facility and optimization that allow efficient application in a wide range of problem a one of the important ingredient the system includes several fast procedure for solving the subsumption problem an np complete problem that need to be solved many time during the learning process we describe qualitative and quantitative experiment in several domain the experiment demonstrate that the system can deal with varied problem large amount of data and that it achieves good classification accuracy 
we present a new technique for time series analysis based on dynamicprobabilistic network in this approach the observed dataare modeled in term of unobserved mutually independent factor a in the recently introduced technique of independent factor analysis ifa however unlike in ifa the factor are not i i d eachfactor ha it own temporal statistical characteristic we derive afamily of em algorithm that learn the structure of the underlyingfactors and their relation to the 
we propose a new classification for multi agent learning algorithm with each league of player characterized by both their possible strategy and possible belief using this classification we review the optimality of existing algorithm including the case of interleague play we propose an incremental improvement to the existing algorithm that seems to achieve average payoff that are at least the nash equilibrium payoff in the longrun against fair opponent 
we describe a unifying method for proving relative loss bound for online linear threshold classification algorithm such a the perceptron and the winnow algorithm for classification problem the discrete loss is used i e the total number of prediction mistake we introduce a continuous loss function called the linear hinge loss that can be employed to derive the update of the algorithm we first prove bound w r t the linear hinge loss and then convert them to the discrete loss we introduce a notion of average margin of a set of example we show how relative loss bound based on the linear hinge loss can be converted to relative loss bound i t o the discrete loss using the average margin 
coil challenge wa a supervised learning contest that attracted entry the author of entry later wrote explanation of their work this paper discus these report and reach three main conclusion first naive bayesian classifier remain competitive in practice they were used by both the winning entry and the next best entry second identifying feature interaction correctly is important for maximizing predictive accuracy this wa the difference between the winning classifier and all others third and most important too many researcher and practitioner in data mining do not appreciate properly the issue of statistical significance and the danger of overfitting given a dataset such a the one for the coil contest it is pointless to apply a very complicated learning algorithm or to perform a very time consuming model search in either ease one is likely to overfit the training data and to fool oneself in estimating predictive accuracy and in discovering useful correlation 
we describe an extension of the markov decision process model in which a continuous time dimension is included in the state space this allows for the representation and exact solution of a wide range of problem in which transition or reward vary over time we examine problem based on route planning with public transportation and telescope observation scheduling 
the retrieval of stored image matching an input configuration is an important form of content based retrieval exhaustive processing i e retrieval of the best solution of configuration similarity query is in general exponential and fast search for sub optimal solution is the only way to deal with the vast and ever increasing amount of multimedia information in several real time application in this paper we discus the utilization of hill climbing heuristic that can provide very good result within limited processing time we propose several heuristic which differ on the way that they search through the solution space and identify the best one depending on the query and image characteristic finally we develop new algorithm that take advantage of the specific structure of the problem to improve performance 
abstract this paper address the issue of reducing the storage requirement on instance based learning algorithm algorithm proposed by other research use heuristic to prune instance of the training set or modify the instance themselves to achieve a reduced set of instance our work present an alternative way we propose to induce a reduced set of partially dened instance with evolutionary algorithm experiment were performed with gale our ne grained parallel evolutionary algorithm and other well known reduction technique on several datasets result suggest that evolutionary algorithm are competitive and robust for inducing set of partially dened instance achieving better reduction rate in storage requirement without loss in generalization accuracy 
support vector machine svms have shown great potential in numerous visual learning and pattern recognition problem the optimal decision surface of a svm is constructed from it support vector which are conventionally determined by solving a quadratic programming qp problem however solving a large optimization problem is challenging since it is computationally intensive and the memory requirement grows with square of the training vector in this paper we propose a geometric method to extract a small superset of support vector which we call guard vector to construct the optimal decision surface specifically the guard vector are found by solving a set of linear programming problem experimental result on synthetic and real data set show that the proposed method is more efficient than conventional method using qps and requires much le memory 
many neural system extend their dynamic range by adaptation we examine the timescales of adaptation in the context of dynamically modulated rapidly varying stimulus and demonstrate in the fly v isual system that adaptation to the statistical ensemble of the stimulus dynamically maximizes information transmission about the time dependent stimulus further while the rate response ha long transient the ad aptation take place on timescales consistent with optimal variance estimation 
window size and shape selection is a difficult problem in area based stereo we propose an algorithm which chooses an appropriate window shape by optimizing over a large class of compact window we call them compact because their ratio of perimeter to area tends to be small we believe that this is the first window matching algorithm which can explicitly construct non rectangular window efficient optimization over the compact window class is achieved via the minimum ratio cycle algorithm in practice it take time linear in the size of the largest window in our class still the straightforward approach to find the optimal window for each pixel disparity pair is too slow we develop pruning heuristic which give practically the same result while reducing running time from minute to second our experiment show that unlike fixed window algorithm our method avoids blurring disparity boundary a well a construct large window in low textured area the algorithm ha few parameter which are easy to choose and the same parameter work well for different image pair 
it is well known that naive bayes can only represent linearly separable function in binary domain but the learnability of general augmented naive bayes is open little work is done on the learnability of bayesian network in nominal domain a general case of binary domain this paper explores the learnability of augmented naive bayes in nominal domain we introduce a complexity measure for nominal function and prove upper bound of the learnability of augmented naive bayes in term of 
introductionindexing and searching for digital information especially imagesand video are becoming important issue because of theremarkable increase of digital document content based imageretrieval technique is receiving widespread research interest and many image retrieval system such a qbic photobook and visualseek have been developedso far a lot of effort have been put on image feature extractionin term of colour texture shape and spatial relation i e 
this paper proposes a method for incrementally understanding user utterance whose semantic boundary are not known and responding in real time even before boundary are determined it is an integrated parsing and discourse processing method that update the partial result of understanding word by word enabling response based on the partial result this method incrementally find plausible sequence of utterance that play crucial role in the task execution of dialogue and utilizes beam search to deal with the ambiguity of boundary a well a syntactic and semantic ambiguity the result of a preliminary experiment demonstrate that this method understands user utterance better than an understanding method that assumes pause to be semantic boundary 
we propose index to measure the difficulty of the named entity ne task by looking at test corpus based on expression inside and outside the ne these index are intended to estimate the difficulty of each task without actually using an ne system and to be unbiased towards a specific system the value of the index are compared with the system performance in japanese document we also discus the difference between ne class with the index and show useful clue which will make it easier to recognize ne 
we present an approach tolanguage specific query basedsampling which given a singledocument in a target language can find many more example ofdocuments in that language byautomatically constructing queriesto access such document on theworld wide web we propose a numberof method for building searchqueries to quickly obtain documentsin the target language they performaccurately and efficiently forbuilding a corpus of document intagalog starting from a single 
we address the problem of integrating shading and multi frame stereo cue within the framework ofoptimization in the innite dimensional space of piecewise smooth surface cue integration then reducesto the determination of region where prior assumption on the reectance of the surface can be enforced and result in a novel re formulation of the correspondence problem a a segmentation quot or grouping quot task in general our formulation combine stereo and shading and therefore allows de 
we demonstrate multiagent control of modular self reconfigurable msr robot for object manipulation task and show how it provides a useful programming abstraction such robot consist of many module that can move relative to each other and change their connectivity thereby changing the robot s overall shape to suit different task we illustrate this approach through simulation experiment of the telecube msr robot system 
a the web and e business have proliferated the practice of using customer facing knowledge base to augment customer service and support operation ha increased this can be a very efficient scalable and cost effective way to share knowledge the effectiveness and cost saving are proportional to the utility of the information within the knowledge base and inversely proportional to the amount of labor required in maintaining the knowledge to address this issue we have developed an algorithm and methodology to increase the utility of the information within a knowledge base while greatly reducing the labor required in this paper we describe an implementation of an algorithm and methodology for comparing a knowledge base to a set of problem ticket to determine which category and subcategories are not well addressed within the knowledge base we utilize text clustering on problem ticket text to determine a set of problem category we then compare each knowledge base solution document to each problem category centroid using a cosine distance metric the distance between the closest solution document and the corresponding centroid becomes the basis of that problem category s knowledge gap our claim is that this gap metric serf a a useful method for quickly and automatically determining which problem category have no relevant solution in a knowledge base we have implemented our approach and we present the result of performing a knowledge gap analysis on a set of support center problem ticket 
recently statistical machine translation model have begun to take advantage of higher level linguistic structure such a syntactic dependency underlying these model is an assumption about the directness of translational correspondence between sentence in the two language however the extent to which this assumption is valid and useful is not well understood in this paper we present an empirical study that quantifies the degree to which syntactic dependency are preserved when par are projected directly from english to chinese our result show that although the direct correspondence assumption is often too restrictive a small set of principled elementary linguistic transformation can boost the quality of the projected chinese par by relative to the unimproved baseline 
this paper proposes a method for event tracking on broadcast news story based on distinction between a topic and an event a topic and an event are identified using a simple criterion called domain dependency of word how greatly a word feature a given set of data the method wa tested on the tdt corpus which ha been developed by the tdt pilot study and the result can be regarded a promising the usefulness of the method 
finding a solution to the frame problem that is robust in the presence of action with indirect effect ha proven to be a difficult task example that feature the instantaneous propagation of interacting indirect effect are particularly taxing this article show that an already widely known predicate calculus formalism namely the event calculus can handle such example with only minor enhancement 
the nearest neighbor technique is a simple and appealing method to address classificationproblems it relies on the assumption of locally constant class conditionalprobabilities this assumption becomes invalid in high dimension with a finitenumber of example due to the curse of dimensionality severe bias can be introducedunder these condition when using the nearest neighbor rule the employmentof a local adaptive metric becomes crucial in order to keep class conditional 
this paper present a novel approach for multisensory information fusion in the bayesian inference framework specifically under the maximum entropy principle a formula is derived for estimating the joint probability of multisensory signal the formula us appropriate mapping function to reflect the dependency among multisensory signal selection of the mapping is guided by the maximum mutual information criterion in addition an algorithm is proposed for linear mapping of gaussian random variable experiment on simulated gaussian data and video audio signal have been carried out preliminary result demonstrate that the proposed method can significantly improve the recognition accuracy for this type of task 
tom m mitchell is author of the textbook machine learning mcgraw hill president of the american association for artificial intelligence and a member of the national research council s computer science and telecommunication board he is vice president and chief scientist at whizbang lab and is currently on a two year leave of absence from carnegie mellon university where he is the fredkin professor of learning and ai in the school of computer science and founding director of cmu s center for automated learning and discovery mitchell s research interest span many area of machine learning theory and practice his current work at whizbang lab involves developing machine learning method for extracting information from text for example whizbang ha developed the world s largest database of job opening by training it software to automatically locate and extract detailed information from job posting on corporate web site see www flipdog com 
we propose a novel approach for building finite memory predic tive model similar in spirit to variable memory length markov model vlmms the model are constructed by first transforming the block structure of the training sequence into a spatial structure of point in a unit hypercube such that the longer is the common suffix shared by any two block the closer lie their point representation such a transfor mation embodies a markov assumption block with long common suffix are likely to produce similar continuation finding a set of predicti on context is formulated a a resource allocation problem solved by vector quantizing the spatial block representation we compare our model with both the classical and variable memory length markov model on three data set with different memory and stochastic component our model have a superior performance yet their construction is fully aut omatic which is shown to be problematic in the case of vlmms 
developing automated agent that intelligently perform complex real world task is time consuming and expensive the most expensive part of developing these intelligent task performance agent involves extracting knowledge from human expert and encoding it into a form useable by automated agent machine learning from a sufficiently rich and focused knowledge source can significantly reduce the cost of developing intelligent performance agent by automating the knowledge acquisition and encoding process potential knowledge source include instruction from human expert experiment performed in the task environment and observation of an expert performing the task observation is particularly well suited to learning hierarchical performance knowledge for task that require realistic human like behavior our learning by observation system called knomic knowledge mimic extract knowledge from observation of an expert performing a task and generalizes this knowledge into rule that an agent can use to perform the same task learning performance knowledge by observation is more efficient than hand coding the knowledge in a number of way knowledge can be encoded directly from the expert without the need for a knowledge engineer to act a an intermediary also the expert only need to demonstrate the task rather than organize and communicate all the relevant information this paper will describe the knowledge required for task performance describe how this knowledge is learned by knomic and report on our effort to learn performance knowledge in the tactical air combat domain and the computer game quake ii 
in this paper we address the problem of segmenting foreground region corresponding to a group of people given model of their appearance that were initialized before occlusion we present a general framework that us maximum likelihood estimation to estimate the best arrangement for people in term of d translation that yield a segmentation for the foreground region given the segmentation result we conduct occlusion reasoning to recover relative depth information and we show how to utilize this depth information in the same segmentation framework we also present a more practical solution for the segmentation problem that is online to avoid searching an exponential space of hypothesis the person model is based on segmenting the body into region in order to spatially localize the color feature corresponding to the way people are dressed modeling these region involves modeling their appearance color distribution a well a their spatial distribution with respect to the body we use a non parametric approach based on kernel density estimation to represent the color distribution of each region and therefore we do not restrict the clothing to be of uniform color instead it can be any mixture of color and or pattern we also present a method to automatically initialize these model and learn them before the occlusion 
we propose to scale learning algorithm to arbitrarily large database by the following method first derive an upper bound for the learner s loss a a function of the number of example used in each step of the algorithm then use this to minimize each step s number of example while guaranteeing that the model produced doe not differ significantly from the one that would be obtained with infinite data we apply the method to k mean clustering and empirically observe it speedup relative 
we introduce a tensor hijk and it dual hijk which represent the d projective mapping of point across three projection view the tensor hijk is a generalization of the well known d collineation matrix homography matrix and it concatenates two homography matrix to represent the joint mapping across three view the dual tensor hijk concatenates two dual homography matrix mapping of line space and is responsible for representing the mapping associated with moving point along straight line path i e hijk can be recovered from line of sight measurement only 
this paper present an open domain textual question answering system that us several feedback loop to enhance it performance these feedback loop combine in a new way statistical result with syntactic semantic or pragmatic information derived from text and lexical database the paper present the contribution of each feedback loop to the overall performance of human assessed precise answer 
three method for improving the generalization performance of linear support vector machine are proposed and this in the case that some dimension in the data can be considered irrelevant for the pattern recognition task at hand in contrast to other method the generalization improvement is not obtained by modifying the system but by modifying the training set itself the first proposed method guarantee faultless performance and can be applied in the case of a small number of irrelevant dimension the two other proposed method are approximation of this method for the case of larger number of irrelevant dimension in order to test these approximation the method are applied to a real world application d object recognition without segmentation in this case the image of the object contain a background that should be made irrelevant for the recognition task the approximative method that us a pair of black white background a training set give rise to excellent result we are able to report on correct recognition rate of unseen view of d object placed on a wide variety of cluttered background that never are worse than for an set size of object this is compared to the performance of svms without this training method in this case the performance can drop below on specific background 
it is well known that under noisy condition we can hear speech much more clearly when we read the speaker s lip this suggests the utility of audio visual information for the task of speech enhancement we propose a method to exploit audio visual cue to enable speech separation under non stationary noise and with a single microphone we revise and extend hmm based speech enhancement technique in which signal and noise model are factorially combined to incorporate visual lip information 
this paper we look at the relationship between thedesign of agent and the design of aa for an intelligent system we present guideline to help agent designer build agent thatare easier to use in system with aa intelligent agentsmany of the recent exciting development in artificial intelligence ai have centered around the concept of an agent anagent is an autonomous entity that sens it environment andacts intelligently and pro actively towards it goal an 
to control the walking gait of a four legged robot we present a novel neuromorphic vlsi chip that coordinate the relative phasing of the robot s leg similar to how spinal central pattern generato r are believed to control vertebrate locomotion the chip control the leg movement by driving motor with time varying voltage which are the output of a small network of coupled oscillator the characte ristics of the chip s output voltage depend on a set of input parameter t he relationship between input parameter and output voltage can be computed analytically for an idealized system in practice however this ideal relationship is only approximately true due to transistor mi match and offset fine tuning of the chip s input parameter is done auto matically by the robotic system using an unsupervised support vector sv learning algorithm introduced recently the learning requires o nly that the description of the desired output is given the machine lear n from unlabeled example how to set the parameter to the chip in order to obtain a desired motor behavior 
in text classification most technique use bag of word to represent document the main problem is to identify what word are best suited to classify the document in such a way a to discriminate between them feature selection technique are then needed to identify these word the feature selection method presented in this paper is rather simple and computationally efficient it combine a well known feature selection criterion the information gain and a new algorithm that selects and add a feature to a bag of word if it doe not occur too often with the feature already in a small set composed of the best feature selected so far for their high information gain in brief it try to avoid considering feature whose discrimination capability is sufficiently covered by already selected feature reducing in size the set of the feature used to characterize the document set this paper present this feature selection method and it result and how we have predetermined some of it parameter through experimentation 
computing the least common subsumer lcs in description logic is an inference task first introduced for sublanguages of classic roughly speaking the lcs of a set of concept description is the most specific concept description that subsumes all of the input description a such the lcs allows to extract the commonality from given concept description a task essential for several application like e g inductive learning information retrieval or the bottom up construction of kr knowledge base previous work on the lcs ha concentrated on description logic that either allow for number restriction or for existential restriction many application however require to combine these constructor in this work we present an lcs algorithm for the description logic alen which allows for both constructor a well a concept conjunction primitive negation and value restriction the proof of correctness of our lcs algorithm is based on an appropriate structural characterization of subsumption in alen also introduced in this paper 
this paper revisits the classical neuroscience paradigm of hebbian learning we find that a necessary requirement for effective associative memory learning is that the efficacy of the incoming synapsis should be uncorrelated this requirement is difficult to achieve in a robust manner by hebbian synaptic learning since it depends on network level information effective learning can yet be obtained by a neuronal process that maintains a zero sum of the incoming synaptic efficacy this 
we describe a case study in which a memory based learning algorithm is trained to simultaneously chunk sentence and assign grammatical function tag to these chunk we compare the algorithm s performance on this parsing task with varying training set size yielding learning curve and different input representation in particular we compare input consisting of word only a variant that includes word form information for low frequency word gold standard po only and combination of these the word based shallow parser display an apparently log linear increase in performance and surpasses the flatter po based curve at about sentence of training data the low frequency variant performs even better and the combination is best comparative experiment with a real po tagger produce lower result we argue that we might not need an explicit intermediate po tagging step for parsing when a sufficient amount of training material is available and word form information is used for low frequency word 
in this paper we present committee a new multi class learning algorithm related to the winnow family of algorithm committee is an algorithm for combining the prediction of a set of sub expert s in the online mistake bounded model of learning a sub expert is a special type of attribute that predicts with a distribution over a finite number of class committee learns a linear function of sub expert and us t his function to make class prediction we provide bound for committee that show it performs well when the target can be represented by a few relevant sub expert we also show how committee can be used to solve more traditional problem composed of attribute this lead to a natural extension that learns on multi class problem that contain bo th traditional attribute and sub expert in this paper we present a new multi class learning algorit hm called committee committee learns a k class target function by combining information from a large set of sub expert a sub expert is a special type of attribute that predicts wit h a distribution over the target class the target space of function are linear max funct ion we define these a function that take a linear combination of sub expert prediction an d return the class with maximum value it may be useful to think of the sub expert a individ ual classifying function that are attempting to predict the target function even though t he individual sub expert may not be perfect committee attempt to learn a linear max function that represents the target function in truth this picture is not quite accurate the r eason we call them sub expert and not expert is because even though a individual sub expert might be poor at prediction it may be useful when used in a linear max function for example some sub expert might be used to add constant weight to the linear max function the algorithm is analyzed for the on line mistake bounded model of learning lit this is a useful model for a type of incremental learning where an algorithm can use feedback about it current hypothesis to improve it performance in this model the algorithm go through a series of learning trial a trial is composed of th ree step first the algorithm 
problem solving method provide reusable architecture and component for implementing the reasoning part of knowledge based system the unified problem solving method development language upml ha been developed to describe and implement such architecture and component and to facilitate their semiautomatic reuse and adaptation in a nutshell upml is a framework for developing knowledge intensive reasoning system based on library of generic problem solving component the paper describes the component architectural constraint development guideline and tool provided by upml our focus is hereby on the meta ontology that ha been developed to formalize the architectural structure and element of upml 
information sharing is important for different goal such a sharing reputation of seller among potential buyer load balancing solving technical problem etc in the short run providing information a a response to query is often unbeneficial in the long run mechanism that enable beneficial stable strategy for information exchange can be found this paper present such mechanism and specifies under which condition it is beneficial to the agent to answer query we analyze a model of repeated encounter in which two agent ask each other queriesover time we present different strategy that enable information exchange and compare them according to the expected utility for the agent and the condition required for the cooperative equilibrium to exist 
this paper present a computational model for perceptual organization a figure ground segregation network is proposed based on a novel boundary pair representation the system solves the figure ground segregation problem through temporal evolution gestalt like grouping rule are incorporated by modulating connection which determines the temporal behavior and thus the perception of the system the result are then fed to a surface completion module based on local diffusion different perceptual phenomenon such a modal and a modal completion virtual contour grouping and shape decomposition are explained by the model with a fixed set of parameter computationally the system eliminates combinatorial optimization which is common to many existing computational approach it also account for more example that are consistent with psychological experiment in addition the boundary pair representation is consistent with well known onand off center cell response and thus biologically more plausible 
method to avoid overfitting fall into two broad category data oriented using separate data for validation and representation oriented penalizing complexity in the model both have limitation that are hard to overcome we argue that fully adequate model evaluation is only possible if the search process by which model are obtained is also taken into account to this end we recently proposed a method for process oriented evaluation p e and successfully applied it to rule induction domingo b however for the sake of simplicity this treatment made a number of rather artificial assumption in this paper the assumption are removed and a simple formula for error estimation is obtained empirical trial show the new better founded form of poe to be a accurate a the previous one while further reducing theory size 
research in machine learning statistic and related field ha produced a wide variety of algorithm for classification however most of these algorithm assume that all error have the same cost which is seldom the case in kdd problem individually making each classification learner costsensitive is laborious and often non trivial in this paper we propose a principled method for making an arbitrary classifier cost sensitive by wrapping a cost minimizing procedure around it this 
estimation of rigid body motion parameter in computer vision is normally performed from image correspondence between two coordinate frame a large number of method and algorithm have been proposed based on that such correspondence are known unfortunately the establishment of correspondence is often time consuming and in many case impossible in this paper we propose a novel correspondenceless motion estimation algorithm based on the cross matrix for a comparative study we also implemented a correspondenceless motion estimation algorithm based on the scatter matrix experimental result have demonstrated that our method is more accurate and robust than the scatter matrix based algorithm 
abstract a the identity fraud in our society is reaching unprecedentedproportions and a there is an increasingemphasis on the emerging automatic positive personalidentification application identification basedon biometrics in general and fingerprint in particular is receiving a lot of attention in both researchand industrial community there are two majorshortcomings of the traditional approach to fingerprintrepresentation for a significant fraction ofpopulation the 
rightnow web is an integrated software package for web based customer service that ha at it core a database of answer to frequently asked question faq one major design goal is to facilitate end user interaction with this dynamic document collection i e make it a easy and efficient a possible for user to browse the collection and locate desired information to this end we perform several type of analysis on the session tracking database that record user navigation history first using both explicit and implicit measure of user satisfaction we infer a solved count representing the average utility of an faq second using the user navigation pattern we construct a link matrix representing connection between faq the technique of building up the link matrix and using it to advise user on related information amount to a form of the swarm intelligence method of finding optimal path both solved count and the link matrix are continuously updated a user interact with the site furthermore they are periodically aged to emphasize recent activity the synergistic combination of these technique allows user to learn from the database in a more effective manner a evidenced by usage statistic 
we build upon our work on iterated phan 
introductionanaphora resolution is a complicated problem in natural language processing and hasattracted the attention of many researcher the approach developed traditional from purely syntactic one to highly semantic and pragmatic one alternative statistic uncertainty reasoning etc or knowledge poor offer only approximatesolutions the paper is an introduction to anaphora resolution offering a brief survey of the majorworks in the field basic notion and 
we present several method for the estimation of relative pose between plane and camera based on projection of set of coplanar feature in image while such method exist for simple case especially one plane seen in one or several view the aim of this paper is to propose solution for multi plane multi view situation possibly with little overlap we propose a factorization based method for the general case of plane seen in view a mechanism for computing missing data i e when one or several of the plane are not visible in one or several of the image is described finally a bundle adjustment procedure is developed that optimizes camera and plane pose a well a camera calibration 
in automated negotiation system consisting of self interested agent contract have traditionally been binding leveled commitment contract i e contract where each party can decommit by paying a predetermined penalty were recently shown to improve pareto efficiency even if agent rationally decommit in nash equilibrium using inflated threshold on how good their outside offer must be before they decommit this paper operationalizes the four leveled commitment contracting protocol by presenting algorithm for using them algorithm are presented for computing the nash equilibrium decommitting threshold and decommitting probability given the contract price and the penalty existence and uniqueness of the equilibrium are analyzed algorithm are also presented for optimizing the contract itself price and penalty existence and uniqueness of the optimum are analyzed using the algorithm we offer a contract optimization service on the web a part of mediator our next generation electronic commerce server finally the algorithm are generalized to contract involving more than two agent 
we formulate face alignment a a model based parameter estimation problem in this paper first we work within a framework that combine two separate subspace model to represent frontal face pattern and pose change independently the combined unified nonlinear model represents varying pose face with a complex manifold then we use a feature based similarity measure fbsm to evaluate image difference in term of pose and match unknown pose face with the model image using a combined 
in kernel based learning the data is mapped to a kernel feature space ofa dimension that corresponds to the number of training data point inpractice however the data form a smaller submanifold in feature space a fact that ha been used e g by reduced set technique for svms wepropose a new mathematical construction that permit to adapt to the intrinsicdimension and to find an orthonormal basis of this submanifold 
abstract non negative matrix factorization nmf ha previously been shown to be a useful decomposition for multivariate data two different multiplicative algorithm for nmf are analyzed they differ only slightly in the multiplicative factor used in the update rule one algorithm can be shown to minimize the conventional least square error while the other minimizes the generalized kullback leibler divergence the monotonic convergence of both algorithm can be proven using an auxiliary function analogous to that used for proving convergence of the expectationmaximization algorithm the algorithm can also be interpreted a diagonally rescaled gradient descent where the rescaling factor is optimally chosen to ensure convergence 
this paper address the problem of estimating the epipolar geometry from apparent contour in two special case under weak perspective and for circular motion an appropriate parametrization of the fundamental matrix is introduced for both case a well a suitable cost function for the estimation of the epipoles the algorithm used in the affine approximation proved to be robust and accurate under several condition the circular motion case turned out to be much more difficult but for a wide baseline the method introduced here is successful for small viewing angle the technique is too sensitive to noise to be used in practice nevertheless circular motion with small baseline can be well modeled by an affine camera system and this approximation should be used in this circumstance 
this paper proposes a new scheme for multi image projective reconstruction based on a projective grid space the projective grid space is defined by two basis view and the fundamental matrix relating these view given fundamental matrix relating other view to each of the two basis view this projective grid space can be related to any view in the projective grid space a a general space that is related to all image a projective shape can be reconstructed from all the image of weakly calibrated camera the projective reconstruction is one way to reduce the effort of the calibration because it doe not need euclid metric information but rather only correspondence of several point between the image for demonstrating the effectiveness of the proposed projective grid definition we modify the voxel coloring algorithm for the projective voxel scheme the quality of the virtual view image re synthesized from the projective shape demonstrates the effectiveness of our proposed scheme for projective reconstruction from a large number of image 
we derive an equivalence between adaboost and the dual of a convex optimization problem showing that the only difference between minimizing the exponential loss used by adaboost and maximum likelihood for exponential model is that the latter requires the model to be normalized to form a conditional probability distribution over label in addition to establishing a simple and easily understood connection between the two method this framework enables u to derive new regularization procedure for boosting that directly correspond to penalized maximum likelihood experiment on uci datasets support our theoretical analysis and give additional insight into the relationship between boosting and logistic regression 
we describe new algorithm for multi frame structurefrom motion from tracked point feature thealgorithms are essentially linear and give accuraciessimilar to those of a maximum likelihood estimate for the common situation where the calibration is xedand approximately known we experimentally comparethe fully projective version of our algorithm to mixedprojective euclidean strategy our theoretical resultsclarify the natureofdominant plane compensationand the e ect of calibration 
this paper proposes a novel hypergraph skeletal representation for d shape based on a formal derivation of the generic structure of it medial axis by classifying each skeletal point by it order of contact we show that generically the medial axis consists of ve type of point which are then organized into sheet curve and point i sheet manifold with boundary which are the locus of bitangent sphere with regular tangency a two type of curve ii the intersection curve of three sheet and the locus of center of tri tangent sphere a and iii the boundary of sheet which are the locus of center of sphere whose radius equal the larger principal curvature i e higher order contact a point and two type of point iv center of quad tangent sphere a and v center of sphere with one regular tangency and one higher order tangency a a the geometry of the d medial axis thus consists of sheet a bounded by one type of curve a on their free end which corresponds to ridge on the surface and attached to two other sheet at another type of curve a which support a generalized cylinder description the a curve can only end in a a point where they must meet an a curve the a curve meet together in four at an a point this formal result lead to a compact representation for d shape referred to a the medial axis hypergraph representation consisting of node a and a a point ank notation mean n distinct k fold tangency of the sphere of contact a explained in the text 
the national aeronautics and space administration nasa along with member of the aircraft industry recently developed technology for a new supersonic aircraft one of the technological area considered for this aircraft is the use of video camera and image processing equipment to aid the pilot in detecting other aircraft in the sky the detection technique should provide high detection probability for obstacle that can vary from sub pixel to a few pixel in size while maintaining a low false alarm probability in the presence of noise and severe background clutter furthermore the detection algorithm must be able to report such obstacle in a timely fashion imposing severe constraint on their execution time this paper describes approach to detect airborne obstacle on collision course and crossing trajectory in video image captured from an airborne aircraft in both case the approach consist of an image processing stage to identify possible obstacle followed by a tracking stage to distinguish between true obstacle and image clutter based on their behavior the crossing target detection algorithm wa also implemented on a pipelined architecture from datacube and run in real time both algorithm have been successfully tested on flight test conducted by nasa 
although connectionist model have provided insight into the nature of perception and motor control connectionist account of higher cognition seldom go beyond an implementation of traditional symbol processing theory we describe a connectionist constraint satisfaction model of how people solve anagram problem the model exploit statistic of english orthography but also address the interplay of subsymbol ic and symbolic computation by a mechanism that extract approximate symbolic representation partial ordering of letter from subsym bolic structure and injects the extracted representation back into the model to assist in the solution of the anagram we show the computational benefit of this extraction injection process and discus it relationshi p to conscious mental process and working memory we also account for experimental data concerning the difficulty of anagram solution based on the orthographic structure of the anagram string and the target word historically the mind ha been viewed from two opposing computational perspective the symbolic perspective view the mind a a symbolic information processing engine according to this perspective cognition operates on representation that en code logical relationship among discrete symbolic element such a stack and structu red tree and cognition involves basic operation such a mean end analysis and best fi rst search in contrast the subsymbolic perspective view the mind a performing statistical inference and involves basic operation such a constraint satisfaction search the d ata structure on which these operation take place are numerical vector in some domain of cognition significant progress ha been made throug h analysis from one computational perspective or the other the thesis of our work is t hat many of these domain might be understood more completely by focusing on the interplay of subsymbolic and symbolic information processing consider the higher cognitive domain of problem solving at an abstract level of description problem solving task can r eadily be formalized in term of symbolic representation and operation however the n eurobiological hardware that underlies human cognition appears to be subsymbolic representation are noisy and graded and the brain operates and adapts in a continuous fashion that is difficult to characterize in discrete symbolic term at some level between the computational level of the task description and the implementation level of human neurobiology the symbolic and subsymbolic account must come into contact with one another we focus on this point of contact by proposing mechanism by which symbolic representation can modulate subsymbolic processing and mechanism by which subsymbolic representation 
newly available high quality inexpensive digital camera enable cheap and easy construction of omnidirectional camera we address several important issue for creating real time panorama including user assisted focusing devignetting and computer controlled brightness contrast and white balance we demonstrate remarkable improvement in panoramic image quality using method requiring very little cpu usage 
in this paper we propose an unsupervised method for discovering inference rule from text such a x is author of y ap x wrote y x solved y ap x found a solution to y and x caused y ap y is triggered by x inference rule are extremely important in many field such a natural language processing information retrieval and artificial intelligence in general our algorithm is based on an extended version of harris distributional hypothesis which state that word that occurred in the same context tend to be similar instead of using this hypothesis on word we apply it to path in the dependency tree of a parsed corpus 
combinatorial auction i e auction where bidder can bid on combination of item tend to lead to more efficient allocation than traditional auction in multi item auction where the agent valuation of the item are not additive however determining the winner so a to maximize revenue is np complete we present a search algorithm for optimal winner determination experiment are shown on several bid distribution the algorithm allows combinatorial auction to scale up to significantly larger number of item and bid than prior approach to optimal winner determination by capitalizing on the fact that the space of bid is necessarily sparsely populated in practice we do this via provably sufficient selective generation of child in the search and by using a method for fast child generation heuristic that are accurate and optimized for speed and four method for preprocessing the search space 
lp is a covering algorithm for adaptive information extraction from text ie it induces symbolic rule that insert sgml tag into text by learning from example found in a user defined tagged corpus training is performed in two step initially a set of tagging rule is learned then additional rule are induced to correct mistake and imprecision in tagging induction is performed by bottom up generalization of example in the training corpus shallow knowledge about natural language processing nlp is used in the generalization process the algorithm ha a considerable success story from a scientific point of view experiment report excellent result with respect to the current state of the art on two publicly available corpus from an application point of view a successful industrial ie tool ha been based on lp real world application have been developed and license have been released to external company for building other application this paper present lp experimental result and application and discus the role of shallow nlp in rule induction 
given a d object and some measurement for point in this object it is desired to find the d location of the object a new model based pose estimator from stereo pair based on linear programming lp is presented in the presence of outlier the new lp estimator provides better result than maximum likelihood estimator such a weighted least square and is usually almost a good a robust estimator such a least median of square lmeds in the presence of noise the new lp estimator provides better result than robust estimator such a lmeds and is slightly inferior to maximum likelihood estimator such a weighted least square in the presence of noise and outlier especially for wide angle stereo the new estimator provides the best result the lp estimator is based on correspondence of a point to convex polyhedron each point corresponds to a unique polyhedron which represents it uncertainty in d a computed from the stereo pair polyhedron can also be computed for d data point by using a priori depth boundary the lp estimator is a single phase no separate outlier rejection phase estimator solved by single iteration no re weighting and always converges to the global minimum of it error function the estimator can be extended to include random sampling and re weighting within the standard frame work of a linear program 
this paper present a unifying probabilistic framework for clustering individual or systemsinto group when the available data measurement are not multivariate vector of fixeddimensionality for example one might have data from a set of medical patient wherefor each patient one ha different number of time series observation each time series ofdifferent length we propose a general model based framework for clustering heterogeneousdata type of this form we discus a general 
the paper describes the application of a probabilisticlocalization technique for indoor mobilerobot navigation based on a qualitative topological representation of the environment from the characterization of the uncertaintiesrelated to motor behavior and landmark recognition a markov based localization system hasbeen developed which maintains and update aprobability distribution for all landmark containedin a topological map a reliable measureof successful localization 
in this paper we examine the problem of estimating the parameter of a multinomial distribution over a large number of discrete outcome most of which do not appear in the training data we analyze this problem from a bayesian perspective and develop a hierarchical prior that incorporates the assumption that the observed outcome constitute only a small subset of the possible outcome we show how to efficiently perform exact inference with this form of hierarchical prior and compare our method to standard approach and demonstrate it merit category algorithm and architecture presentation preference none this paper wa not submitted elsewhere nor will be submitted during nip review period 
in a language generation system a content planner embodies one or more plan that are usually hand crafted sometimes through manual analysis of target text in this paper we present a system that we developed to automatically learn element of a plan and the ordering constraint among them a training data we use semantically annotated transcript of domain expert performing the task our system is designed to mimic given the large degree of variation in the spoken language of the transcript we developed a novel algorithm to find parallel between transcript based on technique used in computational genomics our proposed methodology wa evaluated two fold the learning and generalization capability were quantitatively evaluated using cross validation obtaining a level of accuracy of a qualitative evaluation is also provided 
a central issue in principal component analysis pca is choosing the number of principal component to be retained by interpreting pca a density estimation we show how to use bayesian model selection to estimate the true dimensionality of the data the resulting e timate is simple to compute yet guaranteed to pick the correct dimensionality given enough data the estimate involves an integral over the steifel manifold of frame which is difficult to compute exactly but after cho osing an appropriate parameterization and applying laplace s meth od an accurate and practical estimator is obtained in simulation i t is convincingly better than cross validation and other proposed algorithm s plus it run much faster 
the complete set of measurement that could ever be used by a stereo algorithm is the plenoptic function or light field we give a concise characterization of when the lightfield of a lambertian scene uniquely determines it shape and conversely when stereo is inherently ambiguous we show that stereo computed from the complete light field is ambiguous if and only if the scene is radiating light of a constant intensity and color over an extended region 
we present a new chart parsing method for lambek grammar inspired by a method for d tree grammar parsing the formula of a lambek sequent are firstly converted into rule of an indexed grammar formalism which are used in an earley style predictive chart algorithm the method is non polynomial but performs well for practical purpose much better than previous chart method for lambek grammar 
this paper describes a lexicon organized around systematic polysemy a set of word sens that are related in systematic and predictable way the lexicon is derived by a fully automatic extraction method which utilizes a clustering technique called tree cut we compare our lexicon to wordnet cousin and the inter annotator disagreement observed between wordnet semcor and dso corpus 
many process in biology from the regulation of gene expression in bacteria to memory in the brain involve switch constructed from network of biochemical reaction crucial molecule are present in small number raising question about noise and stability analysis of noise in simple reaction scheme indicates that switch stable for year and switchable in millisecond can be built from fewer than one hundred molecule prospect for direct test of this prediction a well a implication are discussed 
we present a class of approximate inference algorithm for graphical model of the qmr dt type we give convergence rate for these algorithm and for the jaakkola and jordan algorithm and verify these theoretical prediction empirically we also present empirical result on the difficult qmr dt network problem obtaining performance of the new algorithm roughly comparable to the jaakkola and jordan algorithm 
qualitative probabilistic network represent probabilistic influence between variable due to the level of representation detail provided knowledge about influence that hold only in specific context cannot be expressed the result computed from a qualitative network a a consequence can be quite weak and uninformative we extend the basic formalism of qualitative probabilistic network by providing for the inclusion of context specific information about influence and show that exploiting this information upon inference ha the ability to forestall unnecessarily weak result 
we analyze the computational complexity of causal relationship in pearl s structural model where we focus on causality between variable event causality and probabilistic causality in particular we analyze the complexity of the sophisticated notion of weak and actual causality by halpern and pearl in the course of this we also prove an open conjecture by halpern and pearl and establish other semantic result to our knowledge no complexity aspect of causal relationship have been considered so far and our result shed light on this issue 
the motion of a non rigid scene over time imposes more constraint on it structure than those derived from image at a single time instant alone an algorithm is presented for simultaneously recovering dense scene shape and scene flow i e the instantaneous d motion at every point in the scene the algorithm operates by carving away hexels or point in the d space of all possible shape and flow that are inconsistent with the image captured at either time instant or across time the recovered shape is demonstrated to be more accurate than that recovered using image at a single time instant application of the combined scene shape and flow include motion capture for animation retiming of video and non rigid motion analysis 
in this paper we consider the problem of active learning in trigonometric polynomial network and give a necessary and sufficient condition of sample point to provide the optimal generalization capability by analyzing the condition from the functional analytic point of view we clarify the mechanism of achieving the optimal generalization capability we also show that a set of training example satisfying the condition doe not only provide the optimal generalization but also reduces the computational complexity and memory required for the calculation of learning result finally example of sample point satisfying the condition are given and computer simulation are performed to demonstrate the effectiveness of the proposed active learning method 
competition in the wireless telecommunication industry is rampant to maintain profitability wireless carrier must control churn the loss of subscriber who switch from one carrier to another we explore statistical technique for churn prediction and based on these prediction an optimal policy for identifying customer to whom incentive should be offered to increase retention our experiment are based on a data base of nearly u s domestic subscriber and includes information about their usage billing credit application and complaint history we show that under a wide variety of assumption concerning the cost of intervention and the retention rate resulting from intervention churn prediction and remediation can yield significant saving to a carrier we also show the importance of a data representation crafted by domain expert competition in the wireless telecommunication industry is rampant a many a seven competing carrier operate in each market the industry is extremely dynamic with new service technology and carrier constantly altering the landscape carrier announce new rate and incentive weekly hoping to entice new subscriber and to lure subscriber away from the competition the extent of rivalry is reflected in the deluge of advertisement for wireless service in the daily newspaper and other mass medium the united state had million wireless subscriber in roughly of the population some market are further developed for example the subscription rate in finland is industry forecast are for a u s penetration rate of by although there is significant room for growth in most market the industry growth rate is declining and competition is rising consequently it ha become crucial for wireless carrier to control churn the loss of customer who switch from one carrier to another at present domestic monthly churn rate are of the customer base at an average cost of to acquire a subscriber churn cost the industry nearly billion in the total annual loss rose to nearly billion when lost monthly revenue from subscriber cancellation is considered luna it cost roughly five time a much to sign on a new subscriber a to retain an existing one consequently for a carrier with million subscriber reducing the monthly churn rate from to would yield an increase in annual earnings of at least million and an increase in shareholder value of approximately million estimate are even higher when lost monthly revenue is considered see fowlkes madan andrew jensen luna the goal of our research is to evaluate the benefit of predicting churn using technique from statistical machine learning we designed model that predict the probability mozer m c w olniewicz r grime d b johnson e kaushansky h churn reduction in the wireless industry in s a solla t k leen k r mueller ed advance in neural information processing system pp cambridge ma mit press 
in this paper we examine the learning behavior of a heuristic threshold setting approach to information filtering in particular we study how different initial threshold setting and different updating parameter setting affect threshold learning the result on one of the trec news database indicate that learning allows recovery from the inevitable nonoptimality of the initial condition and a greater willingness to learn expressed by a deliberate lowering of the score threshold in the learning stage doe eventually lead to a higher performance in spite of the expected initial performance penalty 
image rectification is the process of applying a pair of dimensional projective transforms or homographies to a pair of image whose epipolar geometry is known so that epipolar line in the original image map to horizontally aligned line in the transformed image we propose anovel technique for image rectification based on geometrically well defined criterion such that image distortion due to rectification is minimized this is achieved by decomposing each homography into a specialized 
this paper make two contribution it provides an operational definition of textons the putative elementary unit of texture perception and an algorithm for partitioning the image into disjoint region of coherent bright ness and texture where boundary of region are defined by peak in contour orientation energy and difference in texton density across the contour julesz introduced the term texton analogous to a phoneme in speech recognition but did not provide an operational definition for gray level image here we re invent textons a frequently co occurring combination of oriented linear filter output these can be learned using a k mean approach by mapping each pixel to it nearest texton the image can be analyzed into texton channel each of which is a point set where discrete technique such a voronoi diagram become applicable local histogram of texton frequency can be used with a x test for significant difference to find texture boundary natural image contain both textured and untextured region so we combine this cue with that of the presence of peak of contour energy derived from output of oddand even symmetric oriented gaussian derivative filter each of these cue ha a domain of applicability so to facilitate cue combination we introduce a gating operator based on a statistical test for isotropy of delaunay neighbor having obtained a local measure of how likely two nearby pixel are to belong to the same region we use the spectral graph theoretic framework of normalized cut to find partition of the image into region of coherent texture and brightness experimental result on a wide range of image are shown 
in repeated general sum game an agent using a quot best response quot strategy maximizes it own payoff assuming it behavior ha no effect on it opponent this notion of best response requires some degree of learning to determine the fixed opponent behavior 
we examine linear program lp approach to boosting and demonstrate their efficient solution using lpboost a column generation simplex method we prove that minimizing the soft margin error function equivalent to solving an lp directly optimizes a generalization error bound lpboost can be used to solve any boosting lp by iteratively optimizing the dual classification cost in a restricted lp and dynamically generating weak learner to make new lp column unlike gradient boosting algorithm lpboost converges finitely to a global solution using well defined stopping criterion computationally lpboost find very sparse solution a good a or better than those found by adaboost using comparable computation 
it is widely recognized that the proliferation of annotation scheme run counter to the need to re use language resource and that standard for linguistic annotation are becoming increasingly mandatory to answer this need we have developed a representation framework comprised of an abstract model for a variety of different annotation type e g morpho syntactic tagging syntactic annotation co reference annotation etc which can be instantiated in different way depending on the annotator s approach and goal in this paper we provide an overview of our representation framework and demonstrate it applicability to syntactic annotation we show how the framework can contribute to comparative evaluation and merging of parser output and diverse syntactic annotation scheme 
we present an architecture for the integration of shallow and deep nlp component which is aimed at flexible combination of different language technology for a range of practical current and future application in particular we describe the integration of a high level hpsg parsing system with different high performance shallow component ranging from named entity recognition to chunk parsing and shallow clause recognition the nlp component enrich a representation of natural language text with layer of new xml meta information using a single shared data structure called the text chart we describe detail of the integration method and show how information extraction and language checking application for realworld german text benefit from a deep grammatical analysis 
the hr program form concept and make conjecture in domain of pure mathematics and us theorem prover otter and model generator mace to prove or disprove the conjecture hr measure property of concept and ass the theorem and proof involving them to estimate the interestingness of each concept and employ a best first search this approach ha led hr to the discovery of interesting new mathematics and enables it to build theory from just the axiom of finite algebra 
we consider the problem of how an agent creates a discrete spatial representation from it continuous interaction with the environment such representation will be the minimal one that explains the experience of the agent in the environment in this 
this article present a dependency parsing scheme using an extended finite state approach the parser augments input representation with channel so that link representing syntactic dependency relation among word can be accommodated and iterates on the input a number of time to arrive at a fixed point intermediate configuration violating various constraint of projective dependency representation such a no crossing link and no independent item except sentential head are filtered via finite state filter we have applied the parser to dependency parsing of turkish 
role limiting approach using explicit theory of problem solving have been successful for acquiring knowledge from domain expert however most system using this approach do not support acquiring procedural knowledge only instance and type information approach using interdependency among different piece of knowledge have been successful for acquiring procedural knowledge but these approach usually do not provide all the support that domain expert require we show how the two approach can be combined in such a way that each benefit from information provided by the other we extend the role limiting approach with a knowledge acquisition tool that dynamically generates question for the user based on the problem solving method this allows a more flexible interaction pattern when user add knowledge this tool generates expectation for the procedural knowledge that is to be added when these procedure are refined new expectation are created from interdependencymodels that in turn refine the information used by the system the implemented ka tool provides broader support than previously implemented system preliminary evaluation in a travel planning domain show that user who are not programmer can with little training specify executable procedural knowledge to customize an intelligent system 
in this paper we discus work on the use of diffusion tensor mri for inter subject brain matching a multiresolution elastic matching a lgorithm for s patial normalisation of d image data ha been adapted for use with diffusion tensor data the hope is that by exploitation of the added information contained in the diffusion tensor image improved anatomical match can be found particularly in white matter r egions of the brain result show that by matching on the diffusion tensor alone anisotropic region of the brain white matter are aligned better than if the match is computed on standard structural data however there is a cost of some accuracy in the alignment of prominent feature in more conventional structural mri data such a s pd t and t weighted imagery if both type of data to d rive the matching process prominent feature in both image can be aligned simultaneously the motivation for this work lie in the characterisation of t he distribution of brain image taken from population group 
combinatorial auction where bidder can bid on bundle of item can lead to more economical allocation but determining the winner is np complete and inapproximable we present cabob a sophisticated search algorithm for the problem it us decomposition technique upper and lower bounding also across component elaborate and dynamically chosen bid ordering heuristic and a host of structural observation experiment against cplex show that cabob is usually faster never drastically slower and in many case drastically faster we also uncover interesting aspect of the problem itself first the problem with short bid that were hard for the first generation of specialized algorithm are easy second almost all of the cat distribution are easy and become easier with more bid third we test a number of random restart strategy and show that they do not help on this problem because the run time distribution doe not have a heavy tail at least not for cabob 
we investigate a probabilistic framework for automatic speechrecognition based on the intrinsic geometric property of curve in particular we analyze the setting in which two variable onecontinuous x one discrete s evolve jointly in time we supposethat the vector x trace out a smooth multidimensional curveand that the variable s evolves stochastically a a function of thearc length traversed along this curve since arc length doe notdepend on the rate at which a curve is 
after the pioneering work of the bacon system the study in the field of scientific discoveryhas been directed to the discovery ofmore plausible law equation to represent thefirst principle underlying objective system the state of the art ha only succeeded ina weak sense that the soundness the reproducibilityand the mathematical admissibilityof the candidate hold within the experimentalmeasurements the plausibility shouldbe checked for various object and or 
this paper present a computational paradigm called data driven markov chain monte carlo ddmcmc for image segmentation in the bayesian statistical framework the paper contributes to image segmentation in three aspect firstly it design effective and well balanced markov chain dynamic to explore the solution space and make the split and merge process reversible at a middle level vision formulation thus it achieves globally optimal solution independent of initial segmentation secondly instead of computing a single maximum a posteriori solution it proposes a mathematical principle for computing multiple distinct solution to incorporates intrinsic ambiguity in image segmentation a k adventurer algorithm is proposed for extracting distinct multiple solution from the markov chain sequence thirdly it utilizes data driven bottom up technique such a clustering and edge detection to compute importance proposal probability which effectively drive the markov chain dynamic and achieve tremendous speedup in comparison to traditional jump diffusion method thus ddm cmc paradigm provides a unifying framework where the role of existing segmentation algorithm such a edge detection clustering region growing split merge snake region competition are revealed a either realizing markov chain dynamic or computing importance proposal probability we report some result on color and grey level image segmentation in this paper and refer to a detailed report and a web site for extensive discussion 
word sense disambiguation wsd is the process of distinguishing between different sens of a word in general the disambiguation rule differ for different word for this reason the automatic construction of disambiguation rule is highly desirable one way to achieve this aim is by applying machine learning technique to training data containing the various sens of the ambiguous word in the work presented here the decision tree learning algorithm c is applied on a corpus of financial news article instead of concentrating on a small set of ambiguous word a done in most of the related previous work all content word of the examined corpus are disambiguated furthermore the effectiveness of word sense disambiguation for different part of speech noun and verb is examined empirically 
in this paper we propose a method for generating realistic shadow of virtual object inserted into a real video sequence our aim is to improve and extend the work of sato sato and ikeuchi which is based on a static camera to the case of a video sequence this extension consists of several procedure calibration of a moving video camera and a graphic camera removing false shadow occurring due to a shortcoming of the static camera approach for the estimation of an illumination distribution and so on the calibration of the moving camera is solved by camera self calibration and with it we designed a flexible graphic world coordinate system embedding technique called match move we also show that the shortcoming of the previous static camera approach is overcome by using information from video sequence finally we present the experimental result of a real video sequence 
finding optimal solution for job shop scheduling problem requires high computational effort especially under consideration of uncertainty and frequent replanning in contrast to computational solution domain expert are often able to derive good local dispatching heuristic by looking at typical problem instance they can be efficiently applied by looking at few relevant feature however these rule are usually not optimal especially in complex decision situation here we describe an approach that try to combine both world a neural network based agent autonomously optimizes it local dispatching policy with respect to a global optimization goal defined for the overall plant on two benchmark scheduling problem we show both learning and generalization ability of the proposed approach 
the aim of this work is to index image in domain specific database using color computed from the object of interest only instead of the whole image the main problem in this task is the segmentation of the region of interest from the background viewing segmentation a a figure ground segregation problem lead to a new approach eliminating the background leaf the figure or object of interest to find possible object color we fir st find background color and eliminate them we then use an edge image at an appropriate scale to eliminate those part of the image which are not in focus and do not contain contain significant structure the edge information is combined with the color based background elimination to produce object figure region we test our approach on a database of bird image we show that in of bird image tested the segmentation is sufficient to determine the color of the bird correctly for retrieval purpose we also show that our approach provides improved retrieval performance 
we call data weakly labeled if it ha no exact label but rather a numerical indication of correctness of the label guessed by the learning algorithm a situation commonly encountered in problem of reinforcement learning the term emphasizes similarity of our approach to the known technique of solving unsupervised and transductive problem in this paper we present an on line algorithm that cast the problem a a multi arm bandit with hidden state and solves it iteratively within the expectation maximization framework the hidden state is represented by a parameterized probability distribution over state tied to the reward the parameterization is formally justified allowing for smooth blending between likelihoodand reward based cost 
an on line recursive algorithm for training support vector machine one vector at a time is presented adiabatic increment retain the kuhntucker condition on all previously seen training data in a number of step each computed analytically the incremental procedure is reversible and decremental unlearning offer an efficient method to exactly evaluate leave one out generalization performance interpretation of decremental unlearning in feature space shed light on the relationship between generalization and geometry of the data 
discovering significant pattern that exist implicitly in huge spatial database is an important computational task a common approach to this problem is to use cluster analysis we propose a novel approach to clustering based on the deterministic analysis of random walk on a weighted graph generated from the data our approach can decompose the data into arbitrarily shaped cluster of different size and density overcoming noise and outlier that may blur the natural decomposition of the data the method requires only o n log n time and one of it variant need only constant space 
a comprehensive novel multi view dynamic face model is presented in this paper to address two challenging problem in face recognition and facial analysis modelling face with large pose variation and modelling face dynamically in video sequence the model consists of a sparse d shape model learnt from d image a shape and pose free texture model and an affine geometrical model model fitting is performed by optimising a global fitting criterion on the overall face appearance while it change across view and over time a local fitting criterion on a set of landmark and a temporal fitting criterion between successive frame in a video sequence by temporally estimating the model parameter over a sequence input the identity and geometrical information of a face is extracted separately the former is crucial to face recognition and facial analysis the latter is used to aid tracking and aligning face we demonstrate the result of successfully applying this model on face with large variation of pose and expression over time 
singularity are ubiquitous in the parameter space of hierarchicalmodels such a multilayer perceptrons at singularity the fisherinformation matrix degenerate and the cram er rao paradigmdoes no more hold implying that the classical model selection theorysuch a aic and mdl cannot be applied it is important tostudy the relation between the generalization error and the trainingerror at singularity the present paper demonstrates a methodof analyzing these error both for 
this paper present new complexity result for propositional closed world reasoning cwr from tractable knowledge base kb both basic cwr generalized cwr extended generalized cwr careful cwr and extended cwr equivalent to circumscription are considered the focus is laid on tractable kb belonging to target class for exact compilation function blake formula dnfs disjunction of horn formula and disjunction of renamable horn formula the complexity of inference is identified for all the form of cwr listed above for each of them new tractable fragment are exhibited our result suggest knowledge compilation a a valuable approach to deal with the complexity of cwr in some situation 
choosing the best lexeme to realize a meaning in natural language generation is a hard task we investigate different tree based stochastic model for lexical choice because of the difficulty of obtaining a sense tagged corpus we generalize the notion of synonymy we show that a tree based model can achieve a word bag based accuracy of representing an improvement over the baseline 
logistic unit in therst hidden layer of a feedforward neural networkcompute the relative probability of a data point under twogaussians this lead u to consider substituting other densitymodels we present an architecture for performing discriminativelearning of hidden markov model using a network of many smallhmm s experiment on speech data show it to be superior to thestandard method of discriminatively training hmm s 
we present a simple approach to combining scene and auto calibration constraint for the calibration of camera from single view and stereo pair calibration constraint are provided by imaged scene structure such a vanishing point of orthogonal direction or rectified plane in addition constraint are available from the nature of the camera and the motion between view we formulate these constraint in term of the geometry of the imaged absolute conic and it relationship to pole polar pair and the imaged circular point of plane three significant advantage result first constraint from scene feature camera characteristic and auto calibration constraint providelinear equation in the element of the image of the absolute conic this mean that constraint may easily be combined and their solution is straightforward second the degeneracy that occur when constraint are not independent may be easily identified lastly the constraint from scene plane and image plane may be treated uniformly example of various case of constraint combination and degeneracy a well a computational technique are presented 
the multiple instance learning model ha received much attention recently with a primary application area being that of drug activity prediction most prior work on multiple instance learning ha been for concept learning yet for drug activity prediction the label is a real valued affinity measurement giving the binding strength we present extension of k nearest neighbor k nn citation knn and the diverse density algorithm for the real valued setting and study their performance on boolean and real valued data we also provide a method for generating chemically realistic artificial data 
in this paper we extend the notion of affine shape introduced by sparr from finite point set to curve the extension make it possible to reconstruct d curve up to projective transformation from a number of their dprojections we also extend the bundle adjustment technique from point feature to curve the first step of the curve reconstruction algorithm is based on affine shape is independent of choice of coordinate robust doe not rely on any preselected parameter and work for an arbitrary number of image in particular this mean that a solution is given to the aperture problem of finding point correspondence between curve the second step take advantage of any knowledge of measurement error in the image this is possible by extending the bundle adjustment technique to curve finally experiment are performed on both synthetic and real data to show the performance and applicability of the algorithm 
optimal missing data estimation algorithm including deblurring and denoising are designed to restore image captured from large ccd sensor array using a butting technique where to column of data are missed at the butting edge we developed a consistency method with separable deblurring to estimate the missing data this method convert an ill posed restoration problem into a well posed one by making few assumption based on regularization theory under the condition that no noise is inserted and the separable blur kernel is exactly known the consistency method can deblur the original image and at the same time estimate the missing column s exactly however this algorithm becomes unstable when large noise is inserted or inaccurate estimation of the blur kernel is made conditioning analysis is used to quantify the amount of ill condition of the blur kernel when the assumption are relaxed to different level which provides a solid measurement on how stable the system will remain knowing the signal to noise ratio and the inaccuracy of the blur kernel estimation experimental result from different approach are compared 
the curse of dimensionality give rise to prohibitive computational requirement that render infeasible the exact solution of large scale stochastic control problem we study an efficient method based on linear programming for approximating solution to such problem 
this paper explores several approach for articulatedpose estimation assuming that video rate depth information is available from either stereo camera or other sensor we use these depth measurement in the traditional linear brightness constraint equation a well a in a depth constraint equation to capture the joint constraint we combine the brightness and depth constraint with twist mathematics we address several important issue in the formation of the constraint equation including updating the body rotation matrix without using a first order matrix approximation and removing the coupling between the rotation and translation update the resulting constraint equation are linear on a modified parameter set after solving these linear constraint there is a single closedform non linear transformation to return the update to the original pose parameter we show result for tracking body pose in oblique view of synthetic walking sequence and in moving camera view of synthetic jumping jack sequence we also show result for tracking body pose in side view of a real walking sequence 
the problem of reinforcement learning in a non markov environment is explored using a dynamic bayesian network where conditional independence assumption between random variable are compactly represented by network parameter the parameter are learned on line and approximation are used to perform inference and to compute the optimal value function the relative effect of inference and value function approximation on the quality of the final policy are investigated by learning to solve a moderately difficult driving task the two value function approximation linear and quadratic were found to perform similarly but the quadratic model wa more sensitive to initialization both performed below the level of human performance on the task the dynamic bayesian network performed comparably to a model using a localist hidden state representation while requiring exponentially fewer parameter 
animal data on delayed reward conditioning experiment show astriking property the data for different time interval collapsesinto a single curve when the data is scaled by the time interval 
hierarchical reinforcement learning promise to be the key to scaling reinforcement learning method to large complex real world problem many theoretical model have been proposed but so far there ha been little in the way of empirical work published to demonstrate these claim in this paper we begin to fill this void by by demonstrating the application of the rl top hierarchical reinforcement learning system to the problem of learning to control an aircraft in a ight simulator we 
bagging and boosting are well known ensemble learning method they combine multiple learned base model with the aim of improving generalization performance to date they have been used primarily in batch mode i e they require multiple pass through the training data in previous work we presented online bagging and boosting algorithm that only require one pas through the training data and presented experimental result on some relatively small datasets through additional experiment on a variety of larger synthetic and real datasets this paper demonstrates that our online version perform comparably to their batch counterpart in term of classification accuracy we also demonstrate the substantial reduction in running time we obtain with our online algorithm because they require fewer pass through the training data 
if you have planned to achieve one particular goal in a stochastic delayed reward problem and then someone asks about a different goal what should you do what if you need to be ready to quickly supply an answer for any possible goal this paper show that by using a new kind of automaton caily generated abstract action hierarchy that with n state preparing for all of n possible goal can be much much cheaper than n time the work of preparing for one goal in goal based markov decision problem it is usual to generate a policy x mapping state to action and a value function j x mapping state to an estimate of minimum expected cost to goal starting at x in this paper we will use the terminology that a multipolicy x y for all state pair x y map a state x to the first action it should take in order to reach y with expected minimum cost and a multi valuefunction j x y is a definition of this minimum cost building these object quickly and with little memory is the main purpose of this paper but a secondary result is a natural automatic way to create a set of parsimonious yet powerful abstractactions for mdps the paper concludes with a set of empirical result on increasingly large mdps 
to verify hardware design by model checking circuit specification are commonly expressed in the temporal logic ctl automatic conversion of english to ctl requires the definition of an appropriately restricted subset of english we show how the limited semantic expressibility of ctl can be exploited to derive a hierarchy of subset our strategy avoids potential difficulty with approach that take existing computational semantic analysis of english a their starting point such a the need to ensure that all sentence in the subset posse a ctl translation 
we describe a simple active learning heuristic which greatly enhances the generalization behavior of support vector machine svms on several practical document classification task we observe a number of benefit the most surprising of which is that a svm trained on a wellchosen subset of the available corpus frequently performs better than one trained on all available data the heuristic for choosing this subset is simple to compute and make no use of information about the test set given that the training time of svms depends heavily on the training set size our heuristic not only offer better performance with fewer data it frequently doe so in le time than the naive approach of training on all available data 
many researcher have explored method for hierarchical reinforcement learning rl with temporal abstraction in which abstract action are defined that can perform many primitive action before terminating however little is known about learning with state abstraction in which aspect of the state space are ignored in previous work we developed the maxq method for hierarchical rl in this paper we define five condition under which state abstraction can be combined with the maxq value function decomposition we prove that the maxq q learning algorithm converges under these condition and show experimentally that state abstraction is important for the successful application of maxq q learning 
this paper describes how a machine learning named entity recognizer ner on upper case text can be improved by using a mixed case ner and some unlabeled text the mixed case ner can be used to tag some unlabeled mixed case text which are then used a additional training material for the upper case ner we show that this approach reduces the performance gap between the mixed case ner and the upper case ner substantially by for muc and for muc named entity test data our method is thus useful in improving the accuracy of ners on upper case text such a transcribed text from automatic speech recognizers where case information is missing 
in this paper we present an algorithm for robust absolute position estimation in natural terrain based on landmark extracted from dense surface our landmark are constructed by concatenating pose dependent oriented surface point with pose invariant surface signature into a single feature vector this definition of landmark allows a priori pose information to be used to constrain the search for landmark match the first step in our algorithm is to extract landmark from stable and salient surface patch these landmark are then stored in a closest point search structure with which landmark are matched eficiently using available pose constraint and invariant value finalb an iterative pose estimation algorithm based on least median square is wrapped around landmark matching to eliminate outlier and estimate absolute position to validate our algorithm we show hundred of absolute position estimation result from three different natural scene these result show that our algorithm can incorporate constraint on position and attitude for eficient landmark matching and match small and dense scene surface patch to large and coarse model surface 
hidden markov model hmms are increasingly being used in computer vision for application such a gesture analysis action recognition from video and illumination modeling their use involves an off line learning step that is used a a basis for on line decision making i e a stationarity assumption on the model parameter but realworld application are often non stationary in nature this lead to the need for a dynamic mechanism to learn and update the model topology a well a it parameter this paper present a new framework for hmm topology and parameter estimation in an online dynamic fashion the topology and parameter estimation is posed a a model selection problem with an mdl prior online modification to the topology are made possible by incorporating a state splitting criterion to demonstrate the potential of the algorithm the background modeling problem is considered theoretical validation and real experiment are presented 
the engineering of computer vision system that meet application specific computational and accuracy requirement is crucial to the deployment of real life computer vision system this paper illustrates how past work on a systematic engineering methodology for vision system performance characterization can be used to develop a real time people detection and zooming system to meet given application requirement we illustrate that by judiciously choosing the system module and performing a 
in literature the mediator architecture ha been proposed for taking information from distributed heterogeneous and often dynamic source and making them work together a a whole in this paper we propose a distributed case based approach for the main problem of a mediator i e rewriting query according to mediator s schema according to this approach we use a case memory a mediator s schema therefore such a schema is not static a in other system but is dynamically updated through the cooperation with information source and other mediator strongly influenced by the query submitted by a consumer from the analysis of different cooperation strategy arises that it is more efficient and effective for a mediator to directly cooperate with information source when the source are few otherwise it is more efficient to cooperate with other mediator 
probabilistic dfa inference is the problem of inducing a stochastic regular grammar from a positive sample of an unknown language the alergia algorithm is one of the most successful approach to this problem in the present work we review this algorithm and explain why it generalization criterion a state merging operation is purely local this characteristic lead to the conclusion that there is no explicit way to bound the divergence between the distribution de ned by the solution and the 
this paper describes a framework for learning probabilistic model of object and scene and for exploiting these model for tracking complex deformable or articulated object in image sequence we focus on the probabilistic tracking of people and learn model of how they appear and move in image in particular we learn the likelihood of observing various spatial and temporal filter response corresponding to edge ridge and motion difference given a model of the person similarly we learn probability distribution over filter response for general scene that define a likelihood of observing the filter response for arbitrary background we then derive a probabilistic model for tracking that exploit the ratio between the likelihood that image pixel corresponding to the foreground person were generated by an actual person or by some unknown background the paper extends previous work on learning image statistic and combine it with bayesian tracking using particle filtering by combining multiple image cue and by using learned likelihood model we demonstrate improved robustness and accuracy when tracking complex object such a people in monocular image sequence with cluttered scene and a moving camera 
we propose two novel method for reducing dimension in training polynomial network we consider the class of polynomial network whose output is the weighted sum of a basis of monomials our first method for dimension reduction eliminates redundancy in the training process using an implicit matrix structure we derive iterative method that converge quickly a second method for dimension reduction involves a novel application of random dimension reduction to feature space the combination of these algorithm produce a method for training polynomial network on large data set with decreased computation over traditional method and model complexity reduction and control 
we discus an information theoretic approach for categorizing and modeling dynamic process the approach can learn a compact and informative statistic which summarizes past state to predict future observation furthermore the uncertainty of the prediction is characterized nonparametrically by a joint density over the learned statistic and present observation we discus the application of the technique to both noise driven dynamical system and random process sampled from a density which is conditioned on the past in the first case we show result in which both the dynamic of random walk and the statistic of the driving noise are captured in the second case we present result in which a summarizing statistic is learned on noisy random telegraph wave with differing dependency on past state in both case the algorithm yield a principled approach for discriminating process with differing dynamic and or dependency the method is grounded in idea from information theory and nonparametric statistic 
the paper describes the application of k mean a standard clustering technique to the task of inducing semantic class for german verb using probability distribution over verb subcategorisation frame we obtained an intuitively plausible clustering of verb into class the automatic clustering wa evaluated against independently motivated hand constructed semantic verb class a series of post hoc cluster analysis explored the influence of specific frame and frame group on the coherence of the verb class and supported the tight connection between the syntactic behaviour of the verb and their lexical meaning component 
data visualization and feature selection method are proposedbased on the joint mutual information and ica the visualizationmethods can find many good d projection for high dimensionaldata interpretation which cannot be easily found by the other existingmethods the new variable selection method is found to bebetter in eliminating redundancy in the input than other methodsbased on simple mutual information the efficacy of the methodsis illustrated on a radar signal analysis 
we investigate the use of linear and nonlinear principal manifold for learning low dimensional representation for visual recognition three technique principal component analysis pca independent component analysis ica and nonlinear pca nlpca are examined and tested in a visual recognition experiment using a large gallery of facial image from the feret database we compare the recognition performance of a nearest neighbor matching rule with each principal manifold representation to that of a maximum a posteriori map matching rule using a bayesian similarity measure derived from probabilistic subspace and demonstrate the superiority of the latter 
data independent sample bound are known to grossly overestimate the amount of data needed for most individual problem instance this ha led to significant recent interest in sequential algorithm which also give precise guarantee about the quality of result but determine the amount of data needed based on characteristic of the actual problem instance at hand and thus need significantly fewer example in this paper we present a practical sequential sampling algorithm which a is 
the snow sparse network of winnow architecture hasrecently been successful applied to a number of natural languageprocessing nlp problem in this paper we proposelarge margin version of the winnow algorithm which weargue can potentially enhance the performance of basic winnow and hence the snow architecture we demonstratethat the resulting method achieve performance comparablewith support vector machine for text categorization application we also explain why both large 
novelty detection involves modeling the normal behaviour of a systemhence enabling detection of any divergence from normality ithas potential application in many area such a detection of machinedamage or highlighting abnormal feature in medical data one approach is to build a hypothesis estimating the support ofthe normal data i e constructing a function which is positive in theregion where the data is located and negative elsewhere recentlykernel method have been proposed 
in this paper the hopfield neural network with delay hnnd is studied from the standpoint of regarding it a an optimized computational model two general updating rule for network with delay gurd are given based on hopfield type neural network with delay for optimization problem and characterized dynamic threshold it is proved that in any sequence of updating rule mode the gurd monotonously converges to a stable state of the network the diagonal element of the connection matrix are shown to have an important influence on the convergence process and they represent the relationship of the local maximum value of the energy function with the stable state of the network all ordinary dhnn algorithm are instance of gurd it can be shown that the convergence condition of gurd may be relaxed in the context of application for instance the condition of nonnegative diagonal element of the connection matrix can be removed from the original convergence theorem new updating rule mode and restrictive condition can guarantee the network to achieve a local maximum of the energy function 
we examine a psychophysical law that describes the inuence ofstimulus and context on perception according to this law choiceprobability ratio factorize into component independently controlledby stimulus and context it ha been argued that this patternof result is incompatible with feedback model of perception in this paper we examine this claim using neural network modelsdened via stochastic dierential equation we show that the lawis related to a condition named channel 
to account for the variability of object appearance due to difference in illumination attention ha recently been focused on representing the set of image for all possible lighting condition approach that address this problem have primarily focused on lighting difference for diffuse reflection using the lambertian model however specular reflection can additionally present considerable disparity in appearance we present a method for representing illumination appearance for both diffuse and specular reflection for object of uniform surface roughness using four photometric image this approach us separation of reflection component extract surface reflectance and roughness and produce arbitrary lighting image without explicit computation of surface shape experimental result demonstrate the validity of the proposed method for constructing diffuse and specular appearance 
the curse of dimensionality is severe when modeling high dimensional discrete data the number of possible combination of the variable explodes exponentially in this paper we propose a new architecture for modeling high dimensional data that requires resource parameter and computation that grow only at most a the square of the number of variable using a multi layer neural network to represent the joint distribution of the variable a the product of conditional distribution the neural network can be interpreted a a graphical model without hidden random variable but in which the conditional distribution are tied through thehiddenunits theconnectivityoftheneuralnetworkcanbe prunedby using dependency test between the variable experiment on modeling the distribution of several discrete data set show statistically significant improvement over other method such a naive bayes and comparable bayesian network and show that significant improvement can be obtained by pruning the network 
we consider noisy euclidean traveling salesman problem in the plane which are random combinatorial problem with underlying structure gibbs sampling is used to compute average trajectory which estimate the underlying structure common to all instance this procedure requires identifying the exact relationship between permutation and tour in a learning setting the average trajectory is used a a model to construct solution to new instance sampled from the same source experimental result show that the average trajectory can in fact estimate the underlying structure and that overtting eects occur if the trajectory adapts too closely to a single instance 
o disambiguate the paper from the projectedimage at previous time step finally the two fold ambiguityin the paper orientation ha to be resolved see figure detected quot top quot edgeclipboard projected edge removed detectededgesfigure paper detection the camera see the paperand the projected image from the previous step theorientation of the paper is ambiguous without the clipboard to solve these problem we have developed a paperdetection algorithm that take 
we study fragment of allen s algebra that contain a basic relation distinct from the equality relation we prove that such a fragment is either npcomplete or else contained in some already known tractable subalgebra we obtain this result by giving a new uniform description of known maximal tractable subalgebras and then systematically using an algebraic technique for description of maximal subalgebras with a given property this approach avoids the need for extensive computerassisted search 
we present a new method to automatically merge lexicon that employ different incompatible po category such incompatibility have hindered effort to combine lexicon to maximize coverage with reasonable human effort given an original lexicon our method is able to merge lexeme from an additional lexicon into the original lexicon converting lexeme from the additional lexicon with about precision this level of precision is achieved with the aid of a device we introduce called an anti lexicon which neatly summarizes all the essential information we need about the co occurrence of tag and lemma our model is intuitive fast easy to implement and doe not require heavy computational resource nor training corpus lemma i tag 
the computation of optical flow is a well studied topic in biological and computational vision however the existence of multiple motion in dynamic imagery due to occlusion or even transparency still raise challenging question in this paper we propose an approach for the detection and characterization of occlusion and transparency we propose a theoretical framework for both type of multiple motion which explicitly show the difference between occlusion and transparency in the frequency domain then we employ an em algorithm for the computation of one or two image velocity and a simple test for the detection of occlusion our approach differs from other em approach which blindly assume the superposition of two model in the spatial domain without providing with a separate formal model for occlusion we test and compare the characterization performance on synthetic and real data 
in human sentence processing cognitive load can be defined many way this report considers a definition of cognitive load in term of the total probability of structural option that have been disconfirmed at some point in a sentence the surprisal of word wi given it prefix wo i on a phrase structural language model these load can be efficiently calculated using a probabilistic earley parser stolcke which is interpreted a generating prediction about reading time on a word by word basis under grammatical assumption supported by corpus frequency data the operation of stolcke s probabilistic earley parser correctly predicts processing phenomenon associated with garden path structural ambiguity and with the subject object relative asymmetry 
this paper describes articulatory adaptation in humanto human communication a computational model of such adaption by human communicator and the application of this work intended primarily for the design of communication aid but with potential application to multimodal communicative agent background communicator must adapt their strategy not only in response to the communicative act performed by other interlocutor but also in consideration of their own mean for the perception and articulation of act of communication the impact of the latter the focus of our current research is especially relevant to individual who are affected by expressive communication disorder past research ha addressed adaptation by a communicative agent to uncertainty with respect to it own percept paek and horvitz in contrast our work concern adaptation by a communicator to uncertainty with respect to his or her own embodiment and the affordances of that embodiment for performing act of communication and while past research concern the design of communicative agent for use in dialogue system our work concern the development of communicative agent for use in computational simulation individual with expressive communication disorder must adapt their strategy in response to constraint arising from physical disorder which can impair the individual s ability to perform certain act of communication and can have intermittent or chronic effect e g neuromuscular dysfunction resulting in spasticity atonicity or excessive fatigue for instance the intelligibility of an individual s speech or gesture might vary over the course of an interaction from somewhat intelligible to not at all when functional communication is not possible clinical intervention possibly in the form of a voice output communication aid a type of augmentative and alternative communication aac device can provide interlocutor with an additional aided mode of communication in the form of synthesized speech and while these device can be useful they are slow and extremely costly to use in term of physical exertion in addition the interface of these device often compete with or subvert the communicator s adaptive strategy for instance an interlocutor anticipating her next conversational turn must look down at the display of the device in order to compose a spoken utterance precisely at the point where eye gaze is important function in regulating turn taking such communicator must also adapt to their interlocutor whose level of familiarity with these device can signal which communicative strategy are likely to be fruitful a familiar partner is likely to understand a strategy of using abbreviation or gesture which is effort saving but requires a pre existing shared understanding while an unfamiliar partner is not an poorly chosen strategy can result in great expenditure of effort to signal and to repair any resulting misunderstanding often more effort than would have been exerted had a more costly strategy been chosen baljko b so in addition to perceptual uncertainty aided communicator also face articulatory uncertainty that is uncertainty about which mode of communication aided or unaided are most appropriate the strategy for mode use must be chosen with care a conservative choice can lead to unnecessary exertion while a risky choice can lead to the breakdown of communication compounding the difficulty the choice of strategy depends on communicator s uncertain perception of his own possibly shifting expressive capability and of the common ground accumulating with his communication partner 
we present a generalization of frequent itemsets allowing for the notion of error in the itemset definition we motivate the problem and present an efficient algorithm that identifies error tolerant frequent cluster of item in transactional data customer purchase data web browsing data text etc the algorithm exploit sparseness of the underlying data to find large group of item that are correlated over database record row the notion of transaction coverage allows u to extend the algorithm and view it a a fast clustering algorithm for discovering segment of similar transaction in binary sparse data we evaluate the new algorithm on three real world application clustering high dimensional data query selectivity estimation and collaborative filtering result show that the algorithm consistently uncovers structure in large sparse database that other traditional clustering algorithm fail to find 
the paper address the problem of class based image based recognition and rendering with varying illumination the rendering problem is defined a follows given a single input image of an object and a sample of image with varying illumination condition of other object of the same general class re render the input image to simulate new illumination condition the class based recognition problem is similarly defined given a single image of an object in a database of im age of other object some of them are multiply sampled under varying illumination identify match any novel image of that object under varying illumination with the single image of that object in the database we focus on lambertian surface class and in particu lar the class of human face the key result in our approach is based on a definition of an illumination invariant signature im age which enables an analytic generation of the image space with varying illumination we show that a small database of object in our experiment a few a two object is suffi cient for generating the image space with varying illumination of any new object of the class from a single input image of that object in many case the recognition result outperform by far conventional method and the re rendering is of remark able quality considering the size of the database of example image and the mild pre process required for making the algo rithm work 
we describe a text categorization approach that is based on a combination of feature distributional cluster with a support vector machine svm classifier our feature selection approach employ distributional clustering of word via the recently introducedinformation bottleneck method which generates a more efficientword clusterrepresentation of document combined with the classification power of an svm this method yield high performance text categorization that can outperform other recent method in term of categorization accuracy and representation efficiency comparing the accuracy of our method with other technique we observe significant dependency of the result on the data set we discus the potential reason for this dependency 
there is general consensus that context can be a rich source of information about an object s identity location and scale however the issue of how to formalize contextual influence is still largely open here we introduce a simple probabilistic framework for modeling the relationship between context and object property we represent global context information in term of the spatial layout of spectral component the resulting scheme serf a an effective procedure for context driven focus of attention and scale selection on real world scene based on a simple holistic analysis of an image the scheme is able to accurately predict object location and size 
video telephony could be considerably enhanced by provision of a tracking system that allows freedom of movement to the speaker while maintaining a well framed image for transmission over limited bandwidth already commercial multi microphone system exist which track speaker direction in order to reject background noise stereo sound and vision are complementary modality in that sound is good for initialisation where vision is expensive whereas vision is good for localisation where sound is le precise using generative probabilistic model and particle filtering we show that stereo sound and vision can indeed be fused effectively to make a system more capable than with either modality on it own 
many problem in computer vision require estimation of both model parameter and boundary which limit the usefulness of standard estimation technique from statistic exampleproblems include surface reconstructionfrom range data estimation of parametric motion model fitting circular or elliptic arc to edgel data and many others this paperintroducesa new estimationtechnique called the domain bounding m estimator which is a generalization of ordinary m estimatorscombiningerror measure on model parameter and boundary in a joint robust objective function minimization of the objective function given a rough initialization yield simultaneous estimate of parameter and boundary the dbm estimator ha been applied to estimating line segment surface and the symmetry transformation between two edgel chain it is unaffected by outlier and prevents boundary estimate from crossing even small magnitude discontinuity 
this paper describes a clustering algorithm for vector quantizers using a stochastic association model it offer a new simple and powerful softmax adaptation rule the adaptation process is the same a the on line k mean clustering method except for adding random fluctuation in the distortion error evaluation process simulation result demonstrate that the new algorithm can achieve efficient adaptation a high a the neural gas algorithm which is reported a one of the most efficient clustering method it is a key to add uncorrelated random fluctuation in the similarity evaluation process for each reference vector for hardware implementation of this process we propose a nanostructure whose operation is described by a single electron circuit it positively us fluctuation in quantum mechanical tunneling process 
magnetic resonance imaging mri of the brain followed by automated segmentation of the corpus callosum cc in midsagittal section have important application in both clinical neurology and neurocognitive research since the size and shape of the cc are shown to be correlated to sex age neurodegenerative disease and various lateralized behavior in man moreover whole head multispectral d mri recording enable voxel based tissue classification and estimation of total brain volume in addition to cc morphometric parameter we propose a new algorithm that us both multispectral mri measurement intensity value and prior information about shape cc template to segment cc in midsagittal slice with very little user interaction the algorithm ha been tested on a sample of subject scanned with multispectral d mri collected for a study of dyslexia with very good agreement between the manually traced true cc outline and the detected cc outline we conclude that the proposed method for cc segmentation is promising for clinical use when multispectral mr image are recorded 
we present a novel efficient initialization free approach to the problem of epipolar geometry estimation by formulating it a one of hyperplane inference from a sparse and noisy point set in an d space given a set of noisy point correspondence in two image a obtained from two view of a static scene without correspondence even in the presence of moving object our method pull out inlier match while rejecting outlier unlike most method which optimize certain objective function our approach doe not involve initialization or any search in the parameter space and therefore is free of the problem of local optimum or poor convergence since no search is involved it is unnecessary to impose simplifying assumption such a affine camera or local planar homography to the scene being analyzed for reducing the search complexity subject to the general epipolar constraint only we detect wrong match by establishing salient extremalities via a naval approach d tensor voting the input set of match is first transformed into a sparse and discrete d point set dense tensor kernel are then applied to vote for the most salient hyperplane normal and intercept that capture all inliers inherent in the input with this filtered set of match the normalized eight point algorithm suffices for the accurate estimation of the fundamental matrix by using efficient data structure and locality our method is both time and space efficient despite the higher dimensionality we demonstrate the general usefulness of our method using example image pair i for aerial image analysis ii with widely different view and iii from non static d scene e g basketball game in an indoor stadium each example contains a considerable amount of wrong match 
we generalize a recent formalism to describe the dynamic of supervised learning in layered neural network in the regime where data recycling is inevitable to the case of noisy teacher our theory generates reliable prediction for the evolution in time of trainingand generalization error and extends the class of mathematically solvable learning process in large neural network to those situation where overfitting can occur 
many study have been made in the past for optimization using covariance matrix of feature point we first describe how to compute the covariance matrix of a feature point from the gray level by integrating existing method then we experimentally examine if thus computed covariance matrix really reflect the accuracy of the feature point to test this we do subpixel template matching and compute the homography and the fundamental matrix our conclusion is rather surprising pointing out important element often overlooked 
neuron receive excitatory input via both fast ampa and slownmda type receptor wend that neuron receiving input vianmda receptor can have two stable membrane state which areinput dependent action potential can only be initiated from thehigher voltage state similar observation have been made in severalbrain area which might be explained by our model the interactionsbetween the two kind of input lead u to suggest thatsome neuron may operate in state disabled 
current alternative for language modeling are statistical technique based on large amount of training data and hand crafted context free or finite state grammar that are difficult to build and maintain one way to address the problem of the grammar based approach is to compile recognition grammar from grammar written in a more expressive formalism while theoretically straight forward the compilation process can exceed memory and time bound and might not always result in accurate and efficient speech recognition we will describe and evaluate two approach to this compilation problem we will also describe and evaluate additional technique to reduce the structural ambiguity of the language model 
we present a new learning architecture the decision directed acyclic graph ddag which is used to combine many two class classifier into a multiclass classifier for an n class problem the ddag contains n n classifier one for each pair of class we present a vc analysis of the case when the node classifier are hyperplanes the resulting bound on the test error depends on n and on the margin achieved at the node but not on the dimension of the space this motivates an algorithm 
this paper present a linear algorithm for the simultaneous computation of d point and camera position from multiple perspective view based on having four point on a reference plane visible in all view the reconstruction and camera recovery is achieved in a single step by finding the null space of a matrix using singular value decomposition unlike factorization algorithm the presented algorithm doe not require all point to be visible in all view by simultaneously reconstructing point and view the numerically stabilizing effect of having wide spread camera with large mutual baseline is exploited experimental result are presented for both finite and infinite reference plane an especially interesting application of this method is the reconstruction of architectural scene with the reference plane taken a the plane at infinity which is visible via three orthogonal vanishing point this is demonstrated by reconstructing the outside and inside courtyard of a building on the basis of view in one single svd 
we analyze the asymptotic behavior of autoregressive neural network ar nn process using technique from markov chain and non linear time series analysis it is shown that standard ar nns without shortcut connection are asymptotically stationary if linear shortcut connection are allowed only the shortcut weight determine whether the overall system is stationary hence standard condition for linear ar process can be used 
we have developed a system for detecting and tracking human face and eye in an unstructured environment we adopt a biologically plausible retinally connected neural network architecture and integrate it with an active vision system while the active vision system track the object moving in real time and the neural network detects the face and eye location from the video stream at a slower rate this paper provides a systematic way of creating and selecting example for training the network by exploring the link between theory and practice experimental result on real sequence of image from a space varying sensor depicts the performance of the system 
we model hippocampal place cell and head direction cell by combining allothetic visual and idiothetic proprioceptive s timuli visual input provided by a video camera on a miniature robot is preprocessed by a set of gabor filter on node of a log polar retinotopic graph unsupervised hebbian learning is employed to incrementally build a population of localized overlapping place field place cell serv e a basis function for reinforcement learning experimental result fo r goal oriented navigation of a mobile robot are presented 
we show that the set of all flow field in a sequenceof frame imaging a rigid scene resides in a lowdimensionallinear subspace based on this observation we develop a method for simultaneous estimationof optical flow across multiple frame which us thesesubspace constraint the multi frame subspace constraintsare strong constraint and replace commonlyused heuristic constraint such a spatial or temporalsmoothness the subspace constraint are geometricallymeaningful and are 
we present an approach to reward maximizationin a non stationary mobile robot environment the approach work within therealistic constraint of limited local sensingand limited a priori knowledge of the environment it is based on the use of augmentedmarkov model amms a general modelingtool we have developed amms are essentiallymarkov chain having additional statisticsassociated with state and state transition we have developed an algorithm thatconstructs amms 
research in feature selection ha paid littleattention to unsupervised learning in thispaper we follow the guideline suggested inprevious work by gennari and present someempirical result in incremental learning ofprobabilistic concept hierarchy we identifydierent type of feature selection and justifythe use of method that run in parallel withlearning and individually select a dierent setof feature for each node in the hierarchy weuse a very simple and inexpensive 
in this paper we use mutual information to characterize the distributionsof phonetic and speaker channel information in a timefrequencyspace the mutual information mi between the phoneticlabel and one feature and the joint mutual information jmi between the phonetic label and two or three feature are estimated the miller s bias formula for entropy and mutual information estimatesare extended to include higher order term the mi andthe jmi for speaker channel recognition 
a parametric camera model and calibration procedure are developed for an outdoor active camera system with pan tilt and zoom control unlike traditional method active camera motion play a key role in the calibration process and no special laboratory setup are required intrinsic parameter are estimated automatically by fitting parametric model to the optic flow induced by rotating and zooming no knowledge of d scene structure is needed extrinsic parameter are calculated by actively rotating the camera to sight a sparse set of surveyed landmark over a virtual hemispherical field of view yielding a well conditioned pose estimation problem 
this paper present autodj a system for automatically generating music play list based on one or more seed song selected by a user autodj us gaus sian process regression to learn a user preference function over song this function take music metadata a input this paper further introduces kernel meta training which is a method of learning a gaussian process kernel from a distribution of function that generates the learned function for playlist gen eration autodj learns a kernel from a large set of album this learned kernel is shown to be more effective at predicting user playlist than a reasonable hand designed kernel 
camera pose estimation is the problem of determining the position and orientation of an internally calibrated camera from known d reference point and their image we briefly survey several existing method for pose estimation then introduce four new linear algorithm the first three give a unique linear solution from four point by svd null space estimation they are based on resultant matrix the method is the raw resultant matrix and the and method are compressed version of this obtained by gaussian elimination with pivoting on constant entry the final method return the four intrinsic solution to the pose from point problem it is based on eigendecomposition of a matrix one advantage of all these method is that they are simple to implement in particular the matrix entry are simple function of the input data numerical experiment are given comparing the performance of the new algorithm with several existing algebraic and linear method 
in principle the recovery and reconstruction of a d object from it d view projection require the parameterisation of it shape structure and surface reflectance property explicit representation and recovery of such d information is notoriously difficult to achieve alternatively a linear combination of d view can be used which requires the establishment of dense correspondence between view this in general is difficult to compute and necessarily expensive in this paper we examine the use of affine and local feature based transformation in establishing correspondence between very large pose variation in doing so we utilise a generic view template a generic d surface model and kernel pca for modelling shape and texture nonlinearities across view the ability of both approach to reconstruct and recover face from any d image are evaluated and compared 
we study novel aspect of multi agent qlearningin a model market in which twoidentical competing quot pricebots quot strategicallyprice a commodity two fundamentallydifferent solution are observed an exact stationary solution with zero bellman errorconsisting of symmetric policy anda non stationary broken symmetry pseudosolution with small but non zero bellmanerror this quot pseudo convergent quot asymmetricsolution ha no analog in ordinary qlearning we calculate analytically 
many application dealing with textual information require classification of word into semantic class or concept however manually constructing semantic class is a tedious task in this paper we present an algorithm unicon for unsupervised induction of concept some advantage of unicon over previous approach include the ability to classify word with low frequency count the ability to cluster a large number of element in a high dimensional space and the ability to classify previously unknown word into existing cluster furthermore since the algorithm is unsupervised a set of concept may be constructed for any corpus 
a serious problem in learning probabilistic model is the presence of hidden variable these variable are not observed yet interact with several of the observed variable a such they induce seemingly complex dependency among the latter in recent year much attention ha been devoted to the development of algorithm for learning parameter and in some case structure in the presence of hidden variable in this paper we address the related problem of detecting hidden variable that interact with the observed variable this problem is of interest both for improving our understanding of the domain and a a preliminary step that guide the learning procedure towards promising model a very natural approach is to search for structural signature of hidden variable substructure in the learned network that tend to suggest the presence of a hidden variable we make this basic idea concrete and show how to integrate it with structure search algorithm we evaluate this method on several synthetic and real life datasets and show that it performs surprisingly well 
spoken dialogue manager have benefited from using stochastic planner such a markov decision process mdps however so far mdps do not handle well noisy and ambiguous speech utterance we use a partially observable markov decision process pomdp style approach to generate dialogue strategy by inverting the notion of dialogue state the state represents the user s intention rather than the system state we demonstrate that under the same noisy condition a pomdp dialogue manager make fewer mistake than an mdp dialogue manager furthermore a the quality of speech recognition degrades the pomdp dialogue manager automatically adjusts the policy 
a new class of support vector machine svm that is applicable to sequential pattern recognition such a speech recognition is developed by incorporating an idea of non linear time alignment into the kernel function since the time alignment operation of sequential pattern is embedded in the new kernel function standard svm training and classication algorithm can be employed without further modications the proposed svm dtak svm is evaluated in speaker dependent speech recognition experiment of hand segmented phoneme recognition preliminary experimental result show comparable recognition performance with hidden markov model hmms 
scene flow is the d motion field of point in the world given n n image sequence gathered with a n eye stereo camera or n calibrated camera we present a novel system which integrates d scene flow and structure recovery in order to complement each other s performance we do not assume rigidity of the scene motion thus allowing for non rigid motion in the scene in our work image are segmented into small region we assume that each small region is undergoing similar motion represented by a d affine model nonlinear motion model fitting based on both optical flow constraint and stereo constraint is then carried over each image region in order to simultaneously estimate d motion correspondence and structure to ensure the robustness several regularization constraint are also introduced a recursive algorithm is designed to incorporate the local and regularization constraint experimental result on both synthetic and real data demonstrate the effectiveness of our integrated d motion and structure analysis scheme 
in this paper we develop the method of bounding the generalization error of a classifier in term of it margin distribution which wa i ntroduced in the recent paper of bartlett and schapire freund bartlet t and lee the theory of gaussian and empirical process allow u to prove the margin type inequality for the most general functional class the complexity of the class being measured via the so called gaussian complexity function a a simple application of our result we obtain the b ounds of schapire freund bartlett and lee for the generalization e rror of boosting we also substantially improve the result of bartlett o n bounding the generalization error of neural network in term of norm of the weight of neuron furthermore under additional assumption on the complexity of the class of hypothesis we provide some tighter bound which in the case of boosting improve the result of schapire freund bartlett and lee 
computer perception of biological motion is key to developing convenient and powerful human computer inter face successful body tracking algorithm have been developed however initialization is done by hand we propose a method for detecting a moving human body and for labeling it part automatically it is based on maximizing the joint probability density function pdf of the position and velocity of the body part the pdf is estimated from training data dynamic programming is used for calculating efficiently the best global labeling on an approximation of the pdf the computational cost is on the order of n where n is the number of feature detected we explore the performance of our method with experiment carried on a variety of periodic and non periodic body motion viewed monocularly for a total of approximately frame point marker were strapped to the joint of the subject for facilitating image analysis we find an average of labeling error the experiment also suggest a high degree of viewpoint invariance 
introductionwe demonstrate a self calibrating system that employ uncalibratedcameras and microportable projector to createnovel interactive display and presentation three benefitsof our system are detailed in the following section automatic keystone correctionthe image generated by an off center projector appearsdistorted figure left using an uncalibrated camerapointed at the presentation screen our system automaticallyrecovers the projector to screen homography 
named entity phrase are some of the most difficult phrase to translate because new phrase can appear from nowhere and because many are domain specific not to be found in bilingual dictionary we present a novel algorithm for translating named entity phrase using easily obtainable monolingual and bilingual resource we report on the application and evaluation of this algorithm in translating arabic named entity to english we also compare our result with the result obtained from human translation and a commercial system for the same task 
agent based computing represents an exciting new synthesis both for artificial intelligence ai and more generally computer science it ha the potential to significantly improve the theory and the practice of modelling designing and implementing complex system yet to date there ha been little systematic analysis of what make an agent such an appealing and powerful conceptual model moreover even le effort ha been devoted to exploring the inherent disadvantage that stem from adopting an agent oriented view here both set of issue are explored the standpoint of this analysis is the role of agent based software in solving complex real world problem in particular it will be argued that the development of robust and scalable software system requires autonomous agent that can complete their objective while situated in a dynamic and uncertain environment that can engage in rich high level social interaction and that can operate within flexible organisational structure 
this paper proposes a method for obtaining surface orientationsof transparent object using polarization in highlight since the highlight the specular component of reflectionlight from object is observed only near the speculardirection it appears merely limited part on an objectsurface in order to obtain orientation of a whole objectsurface we employ a spherical extended light source thispaper report it experimental apparatus a shape recoveryalgorithm and it performance 
a procedure for the parameterization of surface mesh of object with spherical topology is presented the generation of such a parameterization ha been formulated and solved a a large constrained optimization problem by brechb hler but the convergence of this algorithm becomes unstable for object mesh consisting of several thousand vertex we propose a new more stable algorithm to overcome this problem using multi resolution mesh 
this paper describes a method of translating a predicate argument structure of a verb into that of an equivalent verb which is a core component of the dictionary based paraphrasing our method grasp several usage of a headword and those of the def head a a form of their case frame and aligns those case frame which mean the acquisition of word sense disambiguation rule and the detection of the appropriate equivalent and case marker transformation 
we consider the problem of reconstructing a temporal discrete sequenceof multidimensional real vector when part of the data is missing underthe assumption that the sequence wa generated by a continuous process a particular case of this problem is multivariate regression whichis very difficult when the underlying mapping is one to many we proposean algorithm based on a joint probability model of the variablesof interest implemented using a nonlinear latent variable model eachpoint 
while paraphrasing is critical both for interpretation and generation of natural language current system use manual or semi automatic method to collect paraphrase we present an unsupervised learning algorithm for identification of paraphrase from a corpus of multiple english translation of the same source text our approach yield phrasal and single word lexical paraphrase a well a syntactic paraphrase 
in this paper we address the problem of structure and motion recovery from two view of a scene containing plane i e set of coplanar point most of the existing work do only exploit this constraint in a sub optimal manner we propose to parameterize the structure of such scene with plane and point on plane and derive the mle maximum likelihood estimator using a minimal parameterization based on d entity the result is the estimation of camera motion and d structure in projective space that minimizes reprojection error while satisfying the piecewise planarity we propose a quasi linear estimator that provides reliable initialization value for plane equation experimental result show that the reconstruction is of clearly superior quality compared to traditional method based only on point even if the scene is not perfectly piecewise planar 
we address an open and hitherto neglected problem in computer vision how to reconstruct the geometry of object with arbitrary and possibly anisotropic bidirectional reflectance distribution function brdfs present reconstruction technique whether stereo vision structure from motion laser range finding etc make explicit or implicit assumption about the brdf here we introduce two method that were developed by re examining the underlying image formation process the method make no assumption about the object s shape the presence or absence of shadowing or the nature of the brdf which may vary over the surface the first method take advantage of helmholtz reciprocity while the second method exploit the fact that the radiance along a ray of light is constant in particular the first method us stereo pair of image in which point light source are co located at the center of projection of the stereo camera the second method is based on double covering a scene s incident light field the depth of surface point are estimated using a large collection of image in which the viewpoint remains fixed and a point light source illuminates the object result from our implementation lend empirical support to both technique 
traditional anomaly detection technique focus on detecting anomaly in new data after training on normal or clean data in this paper we present a technique for detecting anomaly without training on normal data we present a method for detecting anomaly within a data set that contains a large number of normal element and relatively few anomaly we present a mixture model for explaining the presence of anomaly in the data motivated by the model the approach us machine learning 
the reinforcement learning problem can be decomposed into two parallel type of inference i estimating the parameter of a model for the underlying process ii determining behavior which maximizes return under the estimated model following dearden friedman and andre it is proposed that the learning process estimate online the full posterior distribution over model to determine behavior a hypothesis is sampled from this distribution and the greedy policy with respect to the hypothesis is obtained by dynamic programming by using a different hypothesis for each trial appropriate exploratory and exploitative behavior is obtained this bayesian method always converges to the optimal policy for a stationary process with discrete state 
this paper investigates the potential for projecting linguistic annotation including part of speech tag and base noun phrase bracketings from one language to another via automatically word aligned parallel corpus first experiment ass the accuracy of unmodified direct transfer of tag and bracket from the source language english to the target language french and chinese both for noisy machine aligned sentence and for clean hand aligned sentence performance is then substantially boosted over both of these baseline by using training technique optimized for very noisy data yielding core french part of speech tag accuracy and french bracketing f measure for stand alone monolingual tool trained without the need for any human annotated data in the given language 
a very simple model of two reciprocally connected attractor neural network is studied analytically in situation similar to those encountered in delay match to sample task with intervening stimulus and in task of memory guided attention the model qualitatively reproduces many of the experimental data on these type of task and provides a framework for the understanding of the experimental observation in the context of the attractor neural network scenario 
massive transaction data set are recorded in a routine manner in telecommunication retail commerce and web site management in this paper we address the problem of inferring predictive individual proflles from such historical transaction data we describe a generative mixture model for count data and use an an approximate bayesian estimation framework that efiectively combine an individual s speciflc history with more general population pattern we use a large real world retail transaction data set to illustrate how these proflles consistently outperform non mixture and non bayesian technique in predicting customer behavior in out of sample data 
the goal of this paper is to present an appropriate method for the segmentation of line at intersection x junction and branch t junction which can be regarded a local region where line occur at multiple orientation a novel representation called orientation space is proposed which is derived by adding the orientation axis to the abscissa and the ordinate of the image the orientation space representation is constructed by treating the orientation parameter to which gabor filter can be tuned a a continuous variable the problem of segmenting line at multiple orientation is dealt with by thresholding d image in the orientation space and then detecting the connected component therein in this way x junction and t junction can be separated effectively curve grouping can also be accomplished the segmentation of mathematically modeled x t and l junction is demonstrated and analyzed the sensitivity limit of the method are also discussed experimental result using both synthesized and real image show the method to be effective for junction segmentation and curve grouping 
this paper present a novel statistical model for automatic identification of english basenp it us two step the n best part of speech po tagging and basenp identification given the n best po sequence unlike the other approach where the two step are separated we integrate them into a unified statistical framework our model also integrates lexical information finally viterbi algorithm is applied to make global search in the entire sentence allowing u to obtain linear complexity for the entire process compared with other method using the same testing set our approach achieves in precision and in recall the result is comparable with or better than the previously reported result 
in this paper we describe a new method for medical image registration the registration is formulated a a minimization problem involving robust estimator we propose an efficient hierarchical optimization framework which is both multiresolution and multigrid an anatomical segmentation of the cortex is introduced in the adaptive partitioning of the volume on which the multigrid minimization is based this allows to limit the estimation to the area of interest to accelerate the algorithm 
some issue in marker le tracking of human body motion are addressed extended kalman filter have commonly been applied to kinematic variable to combine prediction consistent with plausible motion with the incoming stream of visual measurement kalman filtering is applicable only when the underlying distribution is approximately gaussian often this assumption prof remarkably robust there are two pervasive circumstance under which the gaussianity assumption can break down the first is kinematic singularity and the second is at joint end stop failure of kalman filtering under these circumstance is illustrated the non gaussian nature of the distribution is demonstrated experimentally by mean of monte carlo simulation random simulation particle filtering or condensation prof to provide a robust alternative algorithm for tracking that can also deal with these difficult condition 
we propose a novel method for temporally and spatially corresponding moving object by automatically learning the relevance of the object appearance feature to the task of discrimination efficient correspondence is achieved by enforcing temporal consistency of the relevance for a particular object relevance are learned using a technique we have termed differential discriminative diagnosis an agent is assigned to each moving object in the scene the agent posse the basic capability to decide whether or not an object in the scene is the one it represents each agent customizes itself to the object by mean of differential discriminative diagnosis a the object persists in the scene we explain this correspondence scheme a applied to the task of corresponding moving people in a surveillance system 
the first step prior to data mining is often to merge database from different source entry in these database or description retrieved using information extraction may use significantly different vocabulary so one often need to determine whether similar description refer to the same item or to different item e g people or good string edit distance is an elegant way of defining the degree of similarity between entry and can be efficiently computed using dynamic programming ristad and yianilos however in order to achieve reasonable accuracy most real problem require the use of extended set of edit rule with associated cost that are tuned specifically to each data set we present a flexible approach to string edit distance which can be automatically tuned to different data set and can use synonym dictionary dynamic programming is used to calculate the edit distance between a pair of string based on a set of string edit rule including a new edit rule that allows word and phrase to be deleted or substituted a genetic algorithm is used to learn cost corresponding to each edit rule based on a small set of labeled training data deleting content le word like method and substituting synonym such a ibuprofen for motrin significantly increase the algorithm s accuracy from to on a difficult sample medical data set when cost are correctly tuned this string edit based matching tool is easily adapted for a variety of different case when one need to recognize which text string from different information source refer to the same item such a a person address medical procedure or product 
real image a realistic image can be generated how ever almost all of the ibr approach are classi ed a a photometric image based rendering pibr con 
fisher linear discriminant analysis lda is a classical multivariatetechnique both for dimension reduction and classication the data vectorsare transformed into a low dimensional subspace such that the classcentroids are spread out a much a possible in this subspace lda worksas a simple prototype classier the resulting decision boundary arelinear however in many application the linear boundary do not adequatelyseparate the class and the possibility of modeling more 
nearest neighbor is a well known algorithmextensively studied by the pattern recognitionand machine learning community andwidely exploited in case based reasoningapplications the notion of metric is centralto nearest neighbor s working and differentfeature weighting metric have been proposedin order to increase it performance in this work we present an original probabilitybased metric i e a metric for classificationtasks that relies on estimate ofthe posterior 
arabic is the language of million of people all over the world y et a publicly available grammatically tagged corpus of arabic still doe not exist in this paper i describe some of the initial finding in the development of an arabic part of speech tagger for this tagger i have c ompiled a tagset containing tag that is derived from traditional arabic grammatical theory i have used this tagger to manually tag a c orpus and i have e xtracted a lexicon from this corpus because arabic is a morphologically complex language some preprocessing is necessary before automatic tagging can take place the result i have obtained so far highlight some of the characteristic of the arabic language that need to be addressed to improve tagging 
nearest neighbor classification assumes locally constant class conditional probability this assumption becomes invalid in high dimension with finite sample due to the curse of dimensionality severe bias can be introduced under these condition when using the nearest neighbor rule we propose a locally adaptive nearest neighbor classification method to try to minimize bias we use a chi squared distance analysis to compute a flexible metric for producing neighborhood that are elongated 
we propose a model of appearance and a matching method which combine global model in which a few parameter control global appearance with local elastic or optical flow based method in which deformation is described by many local parameter together with some regularisation constraint we use an active appearance model aam a the global model which can match a statistical model of appearance to a new image rapidly however the amount of variation allowed is constrained by the mode of the model which may be too restrictive for instance when insufficient training example are available or the number of mode is deliberately truncated for efficiency or memory conservation to compensate for this after global aam convergence we allow further local model deformation driven by local aams around each model node this is analogous to optical flow or demon method of non linear image registration we describe the technique in detail and demonstrate that allowing this extra freedom can improve the accuracy of object location with only a modest increase in search time we show the combined method is more accurate than either pure local or pure global model search 
in this work we propose a generalisation of the notion of association rule in the context of flat transaction to that of a composite association rule in the context of a structured directed graph such a the world wide web the technique proposed aim at finding pattern in che user behaviour when traversing such a hypertext system we redefine the concept of confidence and support for composite association rule and two algorithm to mine such rule are proposed extensive experiment with random data were conducted and the result show that in spite of the worst case complexity analysis which indicates exponential behaviour in practice the algorithm complexity measured in the number of iteration performed is linear in the number of node traversed 
experimental data show that biological synapsis behave quite differently from the symbolic synapsis in common artificial neural network model biological synapsis are dynamic i e their weight change on a short time scale by several hundred percent in dependence of the past input to the synapse in this article we explore the consequence that these synaptic dynamic entail for the computational power of feedforward neural network we show that gradient descent suffices to approximate a given quadratic filter by a rather small neural system with dynamic synapsis we also compare our network model to artificial neural network designed for time series processing our numerical result are complemented by theoretical analysis which show that even with just a single hidden layer such network can approximate a surprisingly large large class of nonlinear filter all filter that can be characterized by volterra series this result is robust with regard to various change in the model for synaptic dynamic 
this paper address the problem of reliablyestimating the coe cients of the parameterized image variety piv associated with the set of weak perspectiveimages of a rigid scene with application in image basedrendering exploiting the fact that the constraint deningthe piv are linear in it coe cients and bilinear in the imagedata the estimation procedure is cast in the error invariablesframework and solved using the method proposedin for this type of problem the 
new functionals for parameter model selection of support vector machine are introduced based on the concept of the span of support vector and rescaling of the feature space it is shown that usin g these functionals one can both predict the best choice of parameter of the model and the relative quality of performance for any value of parameter 
in binocular stereo matching point in left and right image are matched according to feature that characterize each point and identify pair of point when one try to use multiple feature a difficult problem is which feature or combination of feature to use moreover feature are difficult to crossnormalize and so comparison must take into account not only their output but also their distribution their output for different parameter we present a new approach that us geometric constraint on the matching surface to select optimal feature or combination of feature from multiscale edge and intensity feature the approach requires the cyclopean coordinate system to set mutually exclusive matching choice to obtain the matching surface we solve a global optimization problem on an energy functional that model occlusion discontinuity and interepipolar line interaction 
this paper describes named entity ne extraction based on a maximum entropy m e model and transformation rule there are two type of named entity when focusing on the relationship between morpheme and ne a defined in the ne task of the irex competition held in each ne consists of one or more morpheme or includes a substring of a morpheme we extract the former type of ne by using the m e model we then extract the latter type of ne by applying transformation rule to the text 
we present an algorithm which compensates for the mismatchesbetween characteristic of real world problem and assumption ofindependent component analysis algorithm to provide additionalinformation to the ica network we incorporate top down selectiveattention an mlp classier is added to the separated signalchannel and the error of the classier is backpropagated to theica network this backpropagation process result in estimationof expected ica output signal for the top down 
machine learning research ha been very successful at producing powerful broadlyapplicable classification learner however many practical learning problem do not fit the classification framework well and a a result the initial phase of suitably formulating the problem and incorporating the relevant domain knowledge can be very difficult and time consuming here we propose a framework to systematize and speed this process based on the notion of version space algebra we extend the notion of version space beyond concept learning and propose that carefully tailored version space for complex application can be built by composing simpler restricted version space we illustrate our approach with smartedit a programming by demonstration application for repetitive text editing that us version space algebra to guide a search over text editing action sequence we demonstrate the system on a suite of repetitive text editing problem and present experimental result showing it effectiveness in learning from a small number of example 
a new method for real time tracking of non rigid objectsseen from a moving cameraisproposed the centralcomputational module is based on the mean shiftiterations and find the most probable target position inthe current frame the dissimilarity between the targetmodel it color distribution and the target candidatesis expressed by a metric derivedfrom the bhattacharyyacoefficient the theoretical analysis of the approachshows that it relates to the bayesian framework whileproviding a 
bioinformatics is the study of information flow in biology interest in the field ha exploded in the last year with the emergence of technique for large scale experimental data collection including genome sequencing gene expression analysis protein interaction detection high throughput structure determination and others these technique in the context of a large online published literature have created relatively large data set at least by biological standard that are not possible to analyze manually there is therefore a critical need for method to analyze these data and reduce them to new knowledge the principle challenge to the field include the great diversity of data type and question that are asked of the data and the communication difficulty that can exist between expert in biology and expert in machine learning in this talk i will provide an introduction to the major biological question that are being addressed why they are important and how the field is trying to address them with technical approach 
most formulation of reinforcement learning depend on a single reinforcement reward value to guide the search for the optimal policy solution if observation of this reward is rare or expensive converging to a solution can be impractically slow one way to exploit additional domain knowledge is to use more readily available but related quantity a secondary reinforcer to guide the search through the space of all policy we propose a method to augment policy gradient reinforcement learning algorithm by using prior domain knowledge to estimate desired relative level of a set of secondary reinforcement quantity rl can then be applied to determine a policy which will establish these level the primary reinforcement reward is then sampled to calculate a gradient for each secondary reinforcer in the direction of increased primary reward these gradient are used to improve the estimate of relative secondary value and the process iterates until reward is maximized we prove that the algorithm converges to a local optimum in secondary reward space and that the rate of convergence of the performance gradient estimate in secondary reward space is independent of the size of the state space experimental result demonstrate that the algorithm can converge many order of magnitude faster than standard policy gradient formulation 
the cambridge university multimedia document retrieval demo system is a web based application that allows the user to query a database of automatically generated transcript of radio broadcast that are available on line the paper describes how speech recognition and information retrieval technique are combined in this system and show how the user can interact with it 
this paper present nonstationary markovian model and their application to recognitionof string of token such a zip code in the u mailstream unlike traditionalapproaches where digit are simply recognized in isolation the novelty of our approachlies in the manner in which recognition score or probability value along with domainspecific knowledge about the frequency distribution of various combination of digit areall integrated into one unified model the domain specific 
many planning domain require a richer notion of time in which action can overlap and have different duration the key to fast performance in classical planner e g graphplan ipp and blackbox ha been the use of a disjunctive representation with powerful mutual exclusion reasoning this paper present tgp a new algorithm for temporal planning tgp operates by incrementally expanding a compact planning graph representation that handle action of differing duration the key to tgp performance is tight mutual exclusion reasoning which is based on an expressive language for bounding mutexes and includes mutexes between action and proposition our experiment demonstrate that mutual exclusion reasoning remains valuable in a rich temporal setting 
in this paper we show that given two homography matrix for two plane in space there is a linear algorithm for the rotation and translation between the two camera the focal length of the two camera and the plane equation in the space using the estimate a an initial guess we can further optimize the solution by minimizing the difference between observation and reprojections experimental result are shown we also provide a discussion about the relationship between this approach and the kruppa equation 
reducing language model lm size is a critical issue when applying a lm to realistic application which have memory constraint in this paper three measure are studied for the purpose of lm pruning they are probability rank and entropy we evaluated the performance of the three pruning criterion in a real application of chinese text input in term of character error rate cer we first present an empirical comparison showing that rank performs the best in most case we also show that the high performance of rank lie in it strong correlation with error rate we then present a novel method of combining two criterion in model pruning experimental result show that the combined criterion consistently lead to smaller model than the model pruned using either of the criterion separately at the same cer 
spectral analysis provides a powerful mean of estimating the perspective pose of texture plane unfortunately one of the problem that restricts the utility of the method is the need to set the size of the spectral window for texture plane viewed under extreme perspective distortion the spectral frequency density may vary rapidly across the image plane if the size of the window is mismatched to the underlying texture distribution then the estimated frequency spectrum may become severely defocussed this in turn limit the accuracy of perspective pose estimation the aim in this paper is to describe an adaptive method for setting the size of the spectral window we provide an analysis which show that there is a window size that minimises the degree of defocusing the minimum is located through an analysis of the spectral covariance matrix we experiment with the new method on both synthetic and real world imagery this demonstrates that the method provides accurate pose angle estimate even when the slant angle is large we also provide a comparison of the accuracy of perspective pose estimation that result both from our adaptive scale method and with one of fixed scale 
state abstraction is of central importance in reinforcementlearning and markov decision process this paper study the case of variable resolutionstate abstraction for continuous state deterministicdynamic control problem in which near optimalpolicies are required we describe variable resolutionpolicy and value function representationsbased on kuhn triangulation embedded in a kdtree we then consider top down approach tochoosing which cell to split in order to generateimproved 
this paper argues that two apparently distinct mode of generalizing con cepts abstracting rule and computing similarity to exemplar should both be seen a special case of a more general bayesian learning frame work bayes explains the specific working of these two mode which rule are abstracted how similarity is measured a well a why gener alization should appear rule or similarity based in different situation this analysis also suggests why the rule similarity distinction even if not computationally fundamental may still be useful at the algorithmic level a part of a principled approximation to fully bayesian learning 
software design is the hardest part of creating intelligent agent therefore agent architecture should be optimized a design tool this paper present an architectural synthesis between the three layer architecture which dominate autonomous robotics and virtual reality and a more agent oriented approach to viewing behavior module we provide an approach behavior oriented design bod for rapid maintainable development we demonstrate our approach by modeling primate learning 
an ideal retrieval system should retrieve image that satisfy the user s need and should therefore measure image similarity in a manner consistent with human s perception however existing computational similarity measure are not perceptually consistent this paper proposes an approach of improving retrieval performance by improving the perceptual consistency of computational similarity measure for texture based on relevance feedback judgment 
tangential hand velocity profile of rapid human arm movement often appear a sequence of several bell shaped acceleration deceleration phase called submovements or movement unit this suggests how the nervous system might efficiently control a motor plant in the presence of noise and feedback delay another critical observation is that stochasticity in a motor control problem make the optimal control policy essentially different from the optimal control policy for the deterministic case we use a simplified dynamic model of an arm and address rapid aimed arm movement we use reinforcement learning a a tool to approximate the optimal policy in the presence of noise and feedback delay using a simplified model we show that multiple submovements emerge a an optimal policy in the presence of noise and feedback delay the optimal policy in this situation is to drive the arm s end point close to the target by one fast submovement and then apply a few slow submovements to accurately drive the arm s end point into the target region in our simulation the controller sometimes generates corrective submovements before the initial fast submovement is completed much like the predictive correction observed in a number of psychophysical experiment 
abstract the discovery of association rule is one of the classic problem of data mining typically it is done over well structured data such a dat aba in this paper we present a method of discovery of association rule in semi structured data namely in a set of conceptual graph the method is based on conceptual clustering of the data and constructing of a conceptual hierarchy a feature of the method is the possibility of using different le vels of generalization 
we are beginning to make use of technology that intervenes in the content of the communication language processing ha indeed a large practical potential if we take into account multiple modality of communication multimodality refers to the perception of different co ordinated medium used in delivering a message but also to the combination of various attitude in relation to communication and information access e g goal oriented and exploration oriented in the paper reference is made to some prototype developed at irst conceived for cultural tourism in a recent one the specificity is the combination of two form of navigation taking place at the same time one in information space the other in the physical space some challenge for the future are discussed toward the end 
previous research ha shown that citation and hypertext link can be usefully combined with document content to improve retrieval link can be used in many way e g link topology can be used to identify important page anchor text can be used to augment the text of cited page and activation can be spread to linked page this paper introduces a probabilistic model that integrates content matching and these three us of link information in a single unified framework experiment with a web collection show benefit for link information especially for general query 
this paper conduct an experiment to investigate the effect of a physical constraint on a subject s viewpoint when using spoken language to navigate a robot in addition a robot navigation environment named spondia ii ha been developed for the experiment with an actual autonomous mobile robot it is well known that the meaning of an utterance such a a demonstrative pronoun depends on the viewpoint of the speaker or the hearer in a conversation between people the primary factor in determining viewpoint is the physical constraint that are mediated by their body movement this paper note that these physical constraint also have an effect on viewpoint even when people instruct a robot furthermore it is argued that the utterance process also would greatly improve if the robot were able to comprehend the constraint 
we present a new model based bundle adjustment algorithm to recover the d model of a scene object from a sequence of image with unknown motion instead of representing scene object by a collection of isolated d feature usually point our algorithm us a surface controlled by a small set of parameter compared with previous model based approach our approach ha the following advantage first instead of using the model space a a regularizer we directly use it a our search space thus resulting in a more elegant formulation with fewer unknown and fewer equation second our algorithm automatically associate tracked point with their correct location on the surface thereby eliminating the need for a prior d to d association third regarding face modeling we use a very small set of face metric meaningful deformation to parameterize the face geometry resulting in a smaller search space and a better posed system experiment with both synthetic and real data show that this new algorithm is faster more accurate and more stable than existing one 
this paper proposes a description of german word order including phenomenon considered a complex such a scrambling partial vp fronting and verbal pied piping our description relates a syntactic dependency structure directly to a topological hierarchy without resorting to movement or similar mechanism 
unsupervised learning algorithm are designed to extract structurefrom data sample reliable and robust inference requires a guaranteethat extracted structure are typical for the data source i e similar structure have to be infered from a second sample set of thesame data source the overtting phenomenon in maximum entropybased annealing algorithm is exemplarily studied for a classof histogram clustering model bernstein s inequality for largedeviations is used to determine the 
by using mirror reflection of a scene stereo image can be captured with a single camera catadioptricstereo single camera stereo provides both geometric and radiometric advantage over traditional two camera stereo in this paper we discus the geometryand calibration of catadioptric stereo with two planar mirror and show how the relative orientation the epipolar geometry and the estimation of the focal length are constrained by planar motion in addition we have implementeda real time system which demonstrates the viability of stereo with mirror a an alternative to traditional two camera stereo 
among the local consistency technique used in the resolution of constraint satisfaction problem csps path consistency pc ha received a great deal of attention a constraint graph g is pc if for any valuation of a pair of variable that satisfy the constraint in g between them one can find value for the intermediate variable on any other path in g between those variable so that all the constraint along that path are satisfied on complete graph montanari showed that pc hold if and only if each path of length two is pc by convention it is therefore said that a csp is pc if the completion of it constraint graph is pc in this paper we show that montanari s theorem extends to triangulated graph one can therefore enforce pc on sparse graph by triangulating instead of completing them the advantage is that with triangulation much le universal constraint need to be added we then compare the pruning capacity of the two approach we show that when the constraint are convex the pruning capacity of pc on triangulated graph and their completion are identical on the common edge furthermore our experiment show that there is little difference for general nonconvex problem 
this paper introduces a novel statistical mixture model for probabilistic grouping of distributional histogram data adopting the bayesian framework we propose to perform annealed maximum a posteriori estimation to compute optimal clustering solution in order to accelerate the optimization process an efficient multiscale formulation is developed we present a prototypical application of this method for the unsupervised segmentation of textured image based on local distribution of gabor 
we propose a method of approximate dynamic programming for markov decision process mdps using algebraic decision diagram add we produce near optimal value function and policy with much lower time and space requirement than exact dynamic programming our method reduces the size of the intermediate value function generated during value iteration by replacing the value at the terminal of the add with range of value our method is demonstrated on a class of large mdps with up to billion state and we compare the result with the optimal value function 
algorithm for the segmentation of an audio video source into topically cohesive segment based on automatic speech recognition asr transcription is presented a novel two pas algorithm is described that combine a boundary based method with a content based method in the first pas the temporal proximity and the rate of arrival of ngram feature is analyzed in order to compute an initial segmentation in the contentbased second pas change in content bearing word are detected by using the ngram feature a query in an information retrieval system the second pas validates the initial segment and merges them a needed feasibility of the segmentation task can vary enormously depending on the structure of the audio content and the accuracy of asr for real world corporate training data our method identifies at worst a single salient segment of the audio and at best a high level table of content we illustrate the algorithm in detail with some example and validate the result with segmentation boundary generated manually 
the support vector machine svm is known for it good performance in binary classification but it extension to multi class classification is still an on going research issue in this paper we propose a new approach for classification called the import vector machine ivm which is built on kernel logistic regression klr we show that the ivm not only performs a well a the svm in binary classification but also can naturally be generalized to the multi class case furthermore the ivm provides an estimate of the underlying probability similar to the support point of the svm the ivm model us only a fraction of the training data to index kernel basis function typically a much smaller fraction than the svm this give the ivm a computational advantage over the svm especially when the size of the training data set is large is the conditional probability of a point being in class given in this paper we propose a new approach called the import vector machine ivm to address the classification problem we show that the ivm not only performs a well a the svm in binary classification but also can naturally be generalized to the multi class case furthermore the ivm provides an estimate of the probability similar to the support point of the svm the ivm model us only a fraction of the training data to index the kernel basis function we call these training data import point the computational cost of the svm is while the computational cost of the ivm is 
in this paper we discus our approach toward establishing a model of the acquisition of english grammatical structure by user of our english language tutoring system which ha been designed for deaf user of american sign language we explore the correlation between a corpus of error tagged text and their holistic proficiency score assigned by expert in order to draw initial conclusion about what language error typically occur at different level of proficiency in this population since error made at lower level and not at higher level presumably represent construction acquired before those on which error are found only at higher level this should provide insight into the order of acquisition of english grammatical form 
we consider the problem of how an agent creates a discrete spatial representation from it continuous interaction with the environment such representation will be the minimal one that explains the experience of the agent in the environment in this 
reasoning about action ha been a focus of interest in ai from the beginning and continues to receive attention rut the range of situation considered ha been rather narrow and fall well short of what is needed for understanding natural language language understanding requires sophisticated reasoning about action and event and the world s language employ a variety of grammatical and lexical device to construe direct attention and focus on and control inference about action and event we implemented a neurally inspired computational model that is able to reason about linguistic action and event description such a those found in news story the system us an active event representation that also seems to provide natural and cognitiveiy motivated solution to classical problem in logical theory of reasoning about action for logical approach to reasoning about action we suggest that looking at story understanding set up fairly strong desideratum both in term of the fine grained event and action distinction and the kind of real time inference required 
before applying learning algorithm to datasets practitioner often globally discretize any numeric attribute if the algorithm cannot handle numeric attribute directly prior discretization is essential even if it can prior discretization often accelerates induction and may produce simpler and more accurate classifier a it is generally done global discretization denies the learning algorithm any chance of taking advantage of the ordering information implicit in numeric attribute however a simple transformation of discretized data preserve this information in a form that learner can use we show that compared to using the discretized data directly this transformation significantly increase the accuracy of decision tree built by c decision list built by part and decision table built using the wrapper method on several benchmark datasets moreover it can significantly reduce the size of the resulting classifier this simple technique make global discretization an even more useful tool for data preprocessing 
many motion detection and tracking algorithm rely on the process of background subtraction a technique which detects change from a model of the background scene we present a new algorithm for the purpose of background model initialization the algorithm take a input a video sequence in which moving object are present and output a statistical background model describing the static part of the scene multiple hypothesis of the background value at each pixel are generated by locating period of stable intensity in the sequence the likelihood of each hypothesis is then evaluated using opticalow information from the neighborhood around the pixel and the most likely hypothesis is chosen to represent the background our result are compared with those of several standard background modeling technique using surveillance video of human in indoor environment 
we present a computational model of the neural mechanism in the parietal and temporal lobe that support spatial navigation recall of scene and imagery of the product of recall long term representation are stored in the hippocampus and are associated with local spatial and object related feature in the parahippocampal region viewer centered representation are dynamically generated from long term memory in the parietal part of the model the model thereby simulates recall and imagery of location and object in complex environment after parietal damage the model exhibit hemispatial neglect in mental imagery that rotates with the imagined perspective of the observer a in the famous milan square experiment our model make novel prediction for the neural representation in the parahippocampal and parietal region and for behavior in healthy volunteer and neuropsychological patient we perform spatial computation everday task such a reaching and navigating around visible obstacle are predominantly sensory driven rather than memory based and presumably rely upon egocentric or viewer centered representation of space these representation and the ability to translate between them have been accounted for in several computational model of the parietal cortex e g in other situation such a route planning recall and imagery for scene or event one must also reply upon representation of spatial layout from long term memory neuropsychological and neuroimaging study implicate both the parietal and hippocampal region in such task with the long term memory component associated with the hippocampus the discovery of place cell in the hippocampus provides evidence that hippocampal representation are allocentric in that absolute location in open space are encoded irrespective of viewing direction this paper address the nature and source of the spatial representation in the hippocampal and parietal region and how they interact during recall and navigation we assume that in the hippocampus proper long term spatial memory are stored allocentrically whereas in the parietal cortex view based image are created on the fly during perception or recall intuitively it make sense to use an allocentric representation for long term storage a the 
one of the major application of data mining is in helping company determine which potential customer to market to if the expected profit from a customer is greater than the cost of marketing to her the marketing action for that customer is executed so far work in this area ha considered only the intrinsic value of the customer i e the expected profit from sale to her we propose to model also the customer s network value the expected profit from sale to other customer she may influence to buy the customer those may influence and so on recursively instead of viewing a market a a set of independent entity we view it a a social network and model it a a markov random field we show the advantage of this approach using a social network mined from a collaborative filtering database marketing that exploit the network value of customer also known a viral marketing can be extremely effective but is still a black art our work can be viewed a a step towards providing a more solid foundation for it taking advantage of the availability of large relevant database 
we will demonstrate our cvpr paper measurement of color invariant for the case of image retrieval based on query by example and for color image segmentation both are of importance in content based access of image and video data we demonstrate the usefulness of the proposed color invariant in image retrieval by example system we show that an image retrieval query should include the type of invariance expected in the result we demonstrate such query by using the imagesurf retrieval system segmentation of image based on the proposed color invariant is demonstrated by the pictovision system the system provides image processing functionality through the world wide web and is publicly accessible at www science uva nl research isi pictovision html 
this research seek to quantify the impact of thechoice of reward function on behavioral diversity inlearning robot team the methodology developedfor this work ha been applied to multirobot foraging soccer and cooperative movement this paperfocuses specifically on result in multirobot foraging in these experiment three type of reward areused with q learning to train a multirobot team toforage a local performance based reward a globalperformance based reward and a 
we present new algorithm for the k mean clustering problem they use the kd tree data structure to reduce the large number ofnearest neighbor query issued by the traditional algorithm sufficientstatistics are stored in the node of the kd tree then an analysis ofthe geometry of the current cluster center result in great reductionof the work needed to update the center our algorithm behaveexactly a the traditional k mean algorithm proof of correctnessare included the 
recently more and more researcher have been supporting the view that learning is a goaldriven process one of the key property of a goal driven learner is introspectiveness the ability to notice the gap in it knowledge and to reason about the information required to fill in those gap in this paper we introduce a quantitative introspective learning paradigm into case based reasoning cbr the result is an integrated problem solving model which will learn introspectively feature weight in a case base in order to be responsive dynamically to it user in contrast to the existing qualitative method for introspective learning our model ha the advantage of being able to capture accurate learning information in the interaction with it user a cbr system equipped with quantitative introspective learning ability can allow the feature weight to be captured automatically and to track it user changing preference continuously in such a system while the reasoning part is still case based the learning part is shouldered by a quantitative introspective learning model weight learning and evolution are accomplished in the background the effectiveness of this integration will be demonstrated through a series of empirical experiment 
domain specific search engine are becoming increasingly popular because they offer increased accuracy and extra feature not possible with general web wide search engine unfortunately they are also difficult and time consuming to maintain this 
neural network ensemble is a learning paradigm where several neural network are jointly used to solve a problem in this paper the relationship between the generalization ability of the neural network ensemble and the correlation of the individual neural network is analyzed which reveals that ensembling a selective subset of individual network is superior to ensembling all the individual network in some case therefore an approach named gasen is proposed which train several individual neural network and then employ genetic algorithm to select an optimum subset of individual network to constitute an ensemble experimental result show that comparing with a popular ensemble approach i e averaging all and a theoretically optimum selective ensemble approach i e enumerating gasen ha preferable performance in generating ensemble with strong generalization ability in relatively small computational cost 
human evaluation of machine translation are extensive but expensive human evaluation can take month to finish and involve human labor that can not be reused we propose a method of automatic machine translation evaluation that is quick inexpensive and language independent that correlate highly with human evaluation and that ha little marginal cost per run we present this method a an automated understudy to skilled human judge which substitute for them when there is need for quick or frequent evaluation 
robot are gradually entering into diverse application domain such a home office and playing field this article present advanced research activity related to these domain first is robocup which is an attempt to promote ai and robotics research by providing a common task for evaluation of various performance theory algorithm and robot architecture in order for robot both physical robot and soft agent to play a soccer game reasonably well a wide range of technology need to be integrated and a number of technical breakthrough must be accomplished the recent result from the last two robocups are reviewed and future league are introduced second the richer domain of service robotics ha also received significant interest recently the task here is to serve a a human assistant in an office or domestic environment for task like cleaning and delivery the human robot interaction is a key issue to success which pose new challenge in term of integration of spoken dialogue gesture body language etc in addition mobile manipulation and safe navigation around human is essential to success these two area integrates many different discipline including control perception natural language processing hybrid system and handling of uncertainty and applied to tour guiding mail delivery domestic service and rescue activity 
this paper present a layered component model to support web based interactive and collaborative application it is intended to let programmer focus on the particular logic of their application avoiding most of the issue related to collaboration access control and networking management a web based educational application ha been developed over this framework this tele education system which follows the recommendation by the main institution involved in the learning technology 
a single signal processing algorithm can be represented by many mathematically equivalent formula however when these formula are implemented in code and run on real machine they have very different running time unfortunately it is extremely difficult to model this broad performance range further the space of formula for real signal transforms is so large that it is impossible to search it exhaustively for fast implementation we approach this search question a a control learning problem we present a new method for learning to generate fast formula allowing u to intelligently search through only the most promising formula our approach incorporates signal processing knowledge hardware feature and formula performance data to learn to construct fast formula our method learns from performance data for a few formula of one size and then can construct formula that will have the fastest run time possible across many size 
we introduce a new probabilistic model for combining the output of an arbitrary number of query retrieval system by gathering simple statistic on the average performance of a given set of query retrieval system we construct a bayes optimal mechanism for combining the output of these system our construction yield a metasearch strategy whose empirical performance nearly always exceeds the performance of any of the constituent system our construction is also robust in the sense that if good and bad system are combined the performance of the composite is still on par with or exceeds that of the best constituent system finally our model and theory provide theoretical and empirical avenue for the improvement of this metasearch strategy 
a novel trainable snake model called eigensnake is presented in the bayesian framework in the eigensnake prior knowledge of a specific object shape such a that of face outline and facial feature is derived from a training set of the shape and incorporated into a bayesian snake model in the form of the prior distribution further a shape space which is constructed on the basis of a set of eigenvectors obtained from principle component analysis is used to restrict and stabilize the search for the optimal solution the effectiveness is demonstrated by experiment which show that the eigensnake produce more reliable and accurate result than existing model 
multi processor system are becoming more commonplace and affordable based on analysis of actual parsings we argue that to exploit the capability of such machine unification based grammar parser should distribute work at the level of individual unification operation we present a generic approach to parallel chart parsing that meet this requirement and show that an implementation of this technique for lingo achieves considerable speedup 
low dimensional representation are key to solving problem in highlevel vision such a face compression and recognition factorial coding strategy for reducing the redundancy present in natural i mages on the basis of their second order statistic have been successfu l in accounting for both psychophysical and neurophysiological property of early vision class specific representation are presumably for med later at the higher level stage of cortical processing here we sho w that when retinotopic factorial code are derived for ensemble of na tural object such a human face not only redundancy but also dimensionality is reduced we also show that object are built from part in a non gaussian fashion which allows these local feature code to have dimensionality that are substantially lower than the respective nyquist sa mpling rate 
we develop a framework for d shape and motion recovery of articulated deformable object we propose a formalism that incorporates the use of implicit surface into earlier robotics approac he that were designed to handle articulated structure w e demonstrate it effectiveness for human body modeling fr om video sequence our method is both robust and generic it could easily be applied to other shape and motion recovery problem 
we analyze the condition under which synaptic learning rule based on action potential timing can be approximated by learning rule based on ring rate in particular we consider a form of plasticity in which synapsis depress when a presynaptic spike is followed by a postsynaptic spike and potentiate with the opposite temporal ordering such differential anti hebbian plasticitycan be approximated under certain condition by a learning rule that depends on the time derivative of the postsynaptic ring rate such a learning rule act to stabilize persistent neural activity pattern in recurrent neural network 
if the promise of computational modeling is to be fully realized in higherlevel cognitive domain such a language processing principled method must be developed to construct the semantic representation used in such model in this paper we propose the use of an established formalism from mathematical psychology additive clustering a a mean of automatically constructing binary representation for object using only pairwise similarity data however existing method for the unsupervised learning of additive clustering model do not scale well to large problem we present a new algorithm for additive clustering based on a novel heuristic technique for combinatorial optimization the algorithm is simpler than previous formulation and make fewer independence assumption extensive empirical test on both human and synthetic data suggest that it is more effective than previous method and that it also scale better to larger problem by making additive clustering practical we take a significant step toward scaling connectionist model beyond hand coded example 
for many problem the correct behavior of a model depends not only on it input output mapping but also on property of it jacobian matrix the matrix of partial derivative of the model s output with respect to it input we introduce the j prop algorithm an efcient general method for computing the exact partial derivative of a variety of simple function of the jacobian of a model with respect to it free parameter the algorithm applies to any parametrized feedforward model including nonlinear regression multilayer perceptrons and radial basis function network 
many natural image contain reflection and transparency i e they contain mixture of reflected and transmitted light when viewed from a moving camera these appear a the superposition of component layer image moving relative to each other the problem of multiple motion recovery ha been previously studied by a number of researcher however no one ha yet demonstrated how to accurately recover the component image themselves in this paper we develop an optimal approach to recovering layer image and their associated motion from an arbitrary number of composite image we develop two different technique for estimating the component layer image given known motion estimate the first approach us constrained least square to recover the layer image the second approach iteratively refines lower and upper bound on the layer image using two novel compositing operation namely minimumand maximum composite of aligned image we combine these layer extraction technique with a dominant motion estimator and a subsequent motion refinement stage this result in a completely automated system that recovers transparent image and motion from a collection of input image 
for many application it is important to accuratelydistinguish false negative result fromfalse positive this is particularly importantfor medical diagnosis where the correct balancebetween sensitivity and specificity play an importantrole in evaluating the performance of aclassifier in this paper we discus two schemesfor adjusting the sensitivity and specificity ofsupport vector machine and the descriptionof their performance using receiver operatingcharacteristic roc curve 
real time monitoring call for decision making capability in reaction to observed event associative model provide efficiency by matching the observed situation to a recorded pattern equipped with an accurate decision we rely on a decision tree accounting for the context and temporal chronicle expressing dynamic pattern in highly reactive domain i e when action get a frequent a observation the decision must anticipate the complete recognition of a pattern comparing possible evolution this paper focus on the on line decision process a game against nature in the general case a timed game automaton gather the possible next step with associated goodness value and us an opportunistic algorithm to compute a temporally expressive decision maximizing it utility i e the chance of winning 
we consider the problem of reconstructing the location of a moving d point seen from a monocular moving camera i e to reconstruct moving object from line of sight measurement only since the point is moving while the camera is moving then even if the camera motion is known it is impossible to reconstruct the d location of the point under general circumstance however we show that if the point is moving along a straight line then the parameter of the line and hence the d position of the point at each time instance can be uniquely recovered and by linear method from at least view consequently we propose a new approach for dealing with dynamic scene rich with moving object in which once the camera motion is recovered the d trajectory straight line of the moving target can be recovered even when the moving target consists of a single point 
automated tracking of event from chronologically ordered document stream is a new challenge for statistical text classification existing learning technique must be adapted or improved in order to effectively handle difficult situation where the number of positive training instance per event is extremely small the majority of training document are unlabelled and most of the event have a short duration in time we adapted several supervised text categorization method specifically several new variant of the k nearest neighbor knn algorithm and a rocchio approach to track event all of these method showed significant improvement up to reduction in weighted error rate over the performance of the original knn algorithm on tdt benchmark collection making knn among the top performing system in the recent tdt official evaluation furthermore by combining these method we significantly reduced the variance in performance of our event tracking system over different data collection suggesting a robust solution for parameter optimization 
simple temporal network have proved useful in application that involve metric time however many application involve event whose timing is not controlled by the execution agent a number of property relating to overall controllability in such case have been introduced in vidal and ghallab and vidal and fargier including weak and strong controllability we derive some new result concerning these property in particular we prove the negation of weak controllability is np hard confirming a conjecture in vidal and fargier we also introduce a more general controllability property of which weak and strong controllability are special case a propagation algorithm is provided for determining whether the property hold and we identify tractable case where the algorithm run in polynomial time 
strand resnik is a language independent system for automatic discovery of text in parallel translation on the world wide web this paper extends the preliminary strand result by adding automatic language identification scaling up by order of magnitude and formally evaluating performance the most recent end product is an automatically acquired parallel corpus comprising english french document pair approximately million word per language 
simple recurrent network srns have been widely used in natural language task sardsrn extends the srn by explicitly representing the input sequence in a sardnet self organizing map the distributed srn component lead to good generalization and robust cognitive property whereas the sardnet map provides exact representation of the sentence constituent this combination allows sardsrn to learn to parse sentence with more complicated structure than can the srn alone and suggests that the approach could scale up to realistic natural language 
corner model in the literature have lagged behind edge model with respect to color and shading we use both a region model based on distribution of pixel color and an edge model which remove false positive to perform corner detection on color image whose region contain texture we show result on a variety of natural image at different scale that highlight the problem that occur when boundary between region have curvature 
this article present a support vector machine svm like learning system to handle multi label problem such problem are usually decomposed into many two class problem but the expressive power of such a system can be weak we explore a new direct approach it is based on a large margin ranking system that share a lot of common property with svms we tested it on a yeast gene functional classification problem with positive result 
in this paper we propose two generic text summarization method that create text summary by ranking and extracting sentence from the original document the first method us standard ir method to rank sentence relevance while the second method us the latent semantic analysis technique to identify semantically important sentence for summary creation both method strive to select sentence that are highly ranked and different from each other this is an attempt to create a summary with a wider coverage of the document s main content and le redundancy performance evaluation on the two summarization method are conducted by comparing their summarization output with the manual summary generated by three independent human evaluator the evaluation also study the influence of different vsm weighting scheme on the text summarization performance finally the cause of the large disparity in the evaluator manual summarization result are investigated and discussion on human text summarization pattern are presented 
this paper present a variational method for supervised texture segmentation which is based on idea coming from the curve propagation theory we assume that a preferable texture pattern is known e g the pattern that we want to distinguish from the rest of the image the textured feature space is generated by filtering the input and the preferable pattern image using gabor filter and analyzing their response a multi component conditional probability density function the texture segmentation is obtained by minimizing a geodesic active contour model objective function where the boundary based information is expressed via discontinuity on the statistical space associated with the multi modal textured feature space this function is minimized using a gradient descent method where the obtained pde is implemented using a level set approach that handle naturally the topological change finally a fast method is used for the level set implementation the performance of our method is demonstrated on a variety of synthetic and real textured image 
in a headed tree each terminal word can be uniquely labeled with a governing word and grammatical relation this labeling is a summary of a syntactic analysis which eliminates detail reflects aspect of semantics and for some grammatical relation such a subject of finite verb is nearly uncontroversial we define a notion of expected governor markup which sum vector indexed by governor and scaled by probabilistic tree weight the quantity is computed in a parse forest representation of the set of tree analysis for a given sentence using vector sum and scaling by inside probability and flow 
we introduce a new regularization criterionthat exploit unlabeled data to adaptivelycontrol hypothesis complexity in generalsupervised learning task the technique isbased on an abstract metric space view ofsupervised learning that ha been successfullyapplied to model selection in previous research the new regularization criterion weintroduce involves no free parameter and yetperforms well on a variety of regression andconditional density estimation task the only 
we propose to incorporate a priori geometric constraint in a d stereo reconstruction scheme to cope with the many case where image information alone is not sufficient to accurately recover d shape our approach is based on the iterative deformation of a d surface mesh to minimize an objective function we show that combining anisotropic meshing with a nonquadratic approach to regularization enables u to obtain satisfactory reconstruction result using triangulation with few vertex structural or numerical constraint can then be added locally to the reconstruction process through a constrained optimization scheme they improve the reconstruction result and enforce their consistency with a priori knowledge about object shape the strong description and modeling property of differential feature make them useful tool that can be efficiently used a constraint for d reconstruction nalizes large variation of the function to recover and tends to isotropically smooth the solution no matter what the true variation of the function ought to be this problem ha been especially addressed in the case of image restoration stereo reconstruction optical flow computation and motion estimation these approach however are image based whereas given the task of reconstructing a surface from multiple image whose vantage point may be very different we need a surface representation that can be used to generate image of the surface from arbitrary viewpoint taking into account selfocclusion self shadowing and other viewpoint dependent effect clearly a single image centered representation is inadequate for this purpose instead an object centered surface representation is required furthermore geometric constraint are typically easier to express in the d world which make the use of an object centered representation even more desirable 
the principle of maximizing mutual information is applied to learning overcomplete and recurrent representation the underlyi ng model consists of a network of input unit driving a larger number of ou tput unit with recurrent interaction in the limit of zero noise the network is deterministic and the mutual information can be related to the entropy of the output unit maximizing this entropy with respect to both the feedforward connection a well a the recurrent interaction r esults in simple learning rule for both set of parameter the conventiona l independent component ica learning algorithm can be recovered a a special case where there is an equal number of output unit and no recurrent connection the application of these new learning rule is ill ustrated on a simple two dimensional input example 
a new tracker is presented two set are identified one which contains all possible curve a found in the im age and a second which contains all curve which characterize the object of interest the former is constructed out of edge point in the image while the latter is learned prior to running the tracked curve is taken to be the element of the first set which is nearest the second set the formalism for the learned set of curve allows for mathematically well understood group of transformation e g affine projective to be treated on the same footing a le well understood deformation which may be learned from training curve an algorithm is proposed to solve the tracking problem and it property are theoretically demonstrated it solves the global optimization problem and doe so with certain complexity bound experimental result applying the proposed algorithm to the tracking of a moving finger are presented and compared with the result of a condensation approach 
estimating insurance premia from data is a difficult regression problem for several reason the large number of variable many of which are discrete and the very peculiar shape of the noise distribution asymmetric with fat tail with a large majority zero and a few unreliable and very large value we introduce a methodology for estimating insurance premia that ha been applied in the car insurance industry it is based on mixture of specialized neural network in order to reduce the effect of outlier on the estimation statistical comparison with several different alternative including decision tree and generalized linear model show that the proposed method is significantly more precise allowing to identify the least and most risky contract and reducing the median premium by charging more to the most risky customer 
topic detection and tracking tdt is a variant of classicationin which the set of class grows over time this paper describesa new approach to tdt based on probabilistic generative model strong statistical technique address the many challenge hierarchicalshrinkage for sparse data statistical garbage collection quot fornew event detection clustering in time to separate the dierentevents of a common topic and deterministic annealing to createthe hierarchy preliminary 
a central problem in part of speech tagging especially for new language for which limited annotated resource are available is estimating the distribution of lexical probability for unknown word this paper introduces a new paradigmatic similarity measure and present a minimally supervised learning approach combining effective selection and weighting method based on paradigmatic and contextual similarity measure populated from large quantity of inexpensive raw text data this approach is highly language independent and requires no modification to the algorithm or implementation to shift between language such a french and english 
we develop a clas so f dif ferential motion tracker that automatically stabilize when in finite domain most differential tracker compute motion only relative to one previous frame accumulating error indefinitely we estimate pose change between a set of past frame and develop a probabilistic framework for integrating those estimate we use an approximation to the posterior distribution of pose change a an uncertainty model for parametric motion in order to help arbitrate the use of multiple base frame we demonstrate this framework on a simple d translational tracker and a d degree of freedom tracker 
wepresenttwo solution for the scale selection problemin computer vision the first one is completely nonparametricand is based on the the adaptive estimationof the normalized density gradient employing the samplepoint estimator we define the variable bandwidthmean shift prove it convergence and show it superiorityoverthe fixed bandwidth procedure the secondtechnique ha a semiparametric nature and imposes alocal structure on the data to extract reliable scale information the 
using a saliency measure based on the global property of contour closure we have developed a method that reliably segment out salient contour bounding unknown object from real edge image the measure also incorporates the gestalt principle of proximity and smooth continuity that previous method have exploited unlike previous measure we incorporate contour closure by finding the eigen solution associated with a stochastic process that model the distribution of contour passing through edge in the scene the segmentation algorithm utilizes the saliency measure to identify multiple closed contour by finding stronglyconnected component on an induced graph the determination of strongly connected component is a direct consequence of the property of closure we report for the first time result on large real image for which segmentation take an average of about sec per object on a general purpose workstation the segmentation is made efficient for such large image by exploiting the inherent symmetry in the task 
abstract we discus the problem of ranking instance where an instance is associated with an integer from to in other word the specialization of the general multi class learning problem when there exists an ordering among the instance a problem known a ordinal regression or ranking learning this problem arises in various setti ng both in visual recognition and other information retrieval task in the context of applying a large margin principle to this learning proble m we introduce two main approach for implementing the large margin optimization criterion for margin the first is the fixed margin policy in which the margin of the closest neighboring class is being maximized which turn out to be a direct generalization of svm to ranking learning the second approach allows for different margin where the sum of margin is maximized thus effectively having the solution biased towards the pair of neighboring class which are the farthest apart from each other this approach is shown to reduce to when the number of class both approach are optimal in size of the dual functional of where is the total number of training example experiment performed on visual classification and collab orative filtering show that both approach outperform existing ordinal regression algorithm applied for ranking and multi class svm applied to general multi class classification 
this paper reveals a previously ignored connection between two important field regularization and independent component analysis ica we show that at least one representative of a broad class of algorithm regularizers that reduce network complexity extract independent feature a a by product this algorithm is flat minimum search fm a recent general method for finding low complexity network with high generalization capability fm work by minimizing both training error and 
the compass operator detects step edge without assuming that the region on either side have constant color using distribution of pixel color rather than the mean the operator find the orientation of a diameter that maximizes the difference between two half of a circular window junction can also be detected by exploiting their lack of bilateral symmetry this approach is superior to a multidimensional gradient method in situation that often result in false negative and it localizes edge better a scale increase 
in his paper we introduce two improvement to the three dimensional gamut mapping approach to computational colour constancy this approach consist of two separate part first the possible solution are constrained this part is dependent on the diagonal model of illumination change which in turn is a function of the camera sensor in this work we propose a robust method for relaxing this reliance on the diagonal model the second part of the gamut mapping paradigm is to choose a solution from the feasible set currently there are two general approach for doing so we propose a hybrid method which embodies the benefit of both and generally performs better than either we provide result using both generated data and a carefully calibrated set of image in the case of the modification for diagonal model failure we provide synthetic result using two camera with a distinctly different degree of support for the diagonal model here we verify that the new method doe indeed reduce error due to the diagonal model we also verify that the new method for choosing the solution offer significant improvement both in the case of synthetic data and with real image 
we demonstrate d mode a software system that build d model of object and scene by taking a few minimally photograph using a digital camera at possibly largely separated position it ha recently been commercialized by d medium co ltd d mode ha the following step taking photo manually obtaining and matching a few feature point on the object automatically computing d structure and motion automatic delaunay triangulation and manual modification automatic acquisition of texture for each triangular patch if necessary assign a new coordinate system and scale of space the system doe not need input of camera parameter but requires manual input of feature point in a special manner such that matching information is input simultaneously once epipolar line are available they are used to help locate corresponding point in other image 
a non parametric method for texture synthesis is proposed the texture synthesis process grows a new image outward from an initial seed one pixel at a time a markov random field model is assumed and the conditional distribution of a pixel given all it neighbor synthesized so far is estimated by querying the sample image and finding all similar neighborhood the degree of randomness is controlled by a single perceptually intuitive parameter the method aim at preserving a much local structure a possible and produce good result for a wide variety of synthetic and real world texture 
this paper describes a structure from motion and recognition paradigm for generating d model from d set of image in particular we consider the domain of architectural photograph a model based approach is adopted with the architectural model built from a lego kit of parameterised part the approach taken is different from traditional stereo or shape from x approach in that identification of the parameterised component such a window door buttress etc from one image is combined with parallax information in order to generate the d model this model based approach ha two main benefit first it allows the inference of shape and texture where the evidence from the image is weak and second it recovers not only geometry and texture but also an interpretation of the model which can be used for automatic enhancement technique such a the application of reflective texture to window mrf prior usually fail to adequately account for occlusion or enforce to constraint between adjacent epipolar line whereas a model based approach doe both in this paper we propose a model based approach to structure from motion recovery in which prior on shape and texture are explicitly stated and used to overcome image ambiguity in previous work only the reprojection of the model into each image wa used for it verification whereas in this paper we propose that learnt statistic of the appearance of each model also be used to help determine the most appropriate model and hence the shape of the scene the combined use of correspondence and appearance data help to more accurately identify which model is most appropriate 
in this paper we introduce a multi agent deontic update semantics that build on a logic of prescriptive obligation norm and a logic of descriptive obligation normative proposition in this preference based logic we formalize right a a new type of strong prescriptive permission and duty and commitment a prescriptive obligation between agent 
research on texture ha been pursued along two different line the first line of research pioneered by julesz seek the essential ingredient in term of feature and statistic in human texture perception this lead u to a mathematical definition of texture a a julesz ensemble a julesz ensemble is the maximum set of image that share the same value of some basic feature statistic a the image math or equivalently it is a uniform distribution on this set the second line of research study statistical model in particular markov random field mrf and frame model zhu wu and mumford to characterize texture pattern locally in this article we bridge the two line by the fundamental principle of equivalence of ensemble in statistical mechanic gibbs we prove that the conditional probability of an arbitrary image patch given it environment under the julesz ensemble or the uniform model is inevitably a frame mrf model and the limit of the frame mrf model which we called the gibbs ensemble is equivalent to a julesz ensemble a math thus the advantage of the two methodology can be fully utilized 
a foundational approach to modelling belief contraction and revision is presented based on a notion of similarity between belief set in contracting from a belief set the result is the belief set s most similar to the original in which is not believed similar consideration apply to belief revision the modelling of belief change generalises the grove modelling based on a system of sphere where instead of having a total order on set of possible world we have a total order on set of belief set given this modelling set of postulate are determined for contraction and revision the resulting postulate set subsume those in the agm approach the approach shed light on the foundation of belief revision in that first it provides a more general framework than the agm approach second it illustrates assumption under lying the agm approach and third it allows a fine grained investigation of proposed principle underlying belief change lastly it demonstrates that at their most basic revision and contraction of belief are not interdefinable notion but rather distinct concept 
this paper present a new approach to track object in motion when observed by a fixed camera with severe occlusion merging splitting object and defect in the detection we first detect region corresponding to moving object in each frame then try to establish their trajectory we propose to implement the temporal continuity constraint efficiently and apply it to tracking problem in realistic scenario the method is based on a spatiotemporal d t representation of the moving region and us the tensor voting methodology to enforce smoothness in space and time of the tracked object although other characteristic may be considered only the connected component of the moving region are used without further assumption about the object being tracked we demonstrate the performance of the system on several real sequence 
a system for recovering d hand pose from monocular color sequence is proposed the system employ a non linear supervised learning framework the specialized mapping architecture sma to map image feature to likely d hand pose the sma s fundamental component are a set of specialized forward mapping function and a single feedback matching function the forward function are estimated directly from training data which in our case are example of hand joint configuration and their corresponding visual feature the joint angle data in the training set is obtained via a cyberglove a glove with sensor that monitor the angular motion of the palm and finger in training the visual feature are generated using a computer graphic module that render the hand from arbitrary viewpoint given the joint angle the viewpoint is encoded by two real value therefore real value represent a hand pose we test our system both on synthetic sequence and on sequence taken with a color camera the system automatically detects and track both band of the user calculates the appropriate feature and estimate the d hand joint angle and viewpoint from those feature result are encouraging given the complexity of the task 
a great deal of work ha been done demonstrating the ability of machine learning algorithm to automatically extract linguistic knowledge from annotated corpus very little work ha gone into quantifying the difference in ability at this task between a person and a machine this paper is a first step in that direction 
in this paper we present a novel algorithm for adaptivefuzzy segmentation of mri data and estimation ofintensity inhomogeneity using fuzzy logic mri intensityinhomogeneities can be attributed to imperfectionsin the rf coil or some problem associated with theacquisition sequence the result is a slowly varyingshading artifact over the image that can produce errorswith conventional intensity based classification ouralgorithm is formulated by modifying the objectivefunction of the 
we present a statistical approach to adapting the sample set size of particle filter on thefly the key idea of the kld sampling method is to bound the error introduced by the samplebased representation of the particle filter thereby our approach chooses a small number of sample if the density is focused on a small subspace of the state space and it chooses a large number of sample if the state uncertainty is high both the implementation and computation overhead of this approach are 
extracting rule from rbfs is not a trivial task because of nonlinear function or high input dimensionality in such case some of the hidden unit of the rbf network have a tendency to be shared across several output class or even may not contribute 
clustering is a widely used technique in data mining application to discover pattern in the underlying data most traditional clustering algorithm are limited to handling datasets that contain either continuous or categorical attribute however datasets with mixed type of attribute are common in real life data mining problem in this paper we propose a distance measure that enables clustering data with both continuous and categorical attribute this distance measure is derived from a probabilistic model that the distance between two cluster is equivalent to the decrease in log likelihood function a a result of merging calculation of this measure is memory efficient a it depends only on the merging cluster pair and not on all the other cluster zhang et al proposed a clustering method named birch that is especially suitable for very large datasets we develop a clustering algorithm using our distance measure based on the framework of birch similar to birch our algorithm first performs a pre clustering step by scanning the entire dataset and storing the dense region of data record in term of summary statistic a hierarchical clustering algorithm is then applied to cluster the dense region apart from the ability of handling mixed type of attribute our algorithm differs from birch in that we add a procedure that enables the algorithm to automatically determine the appropriate number of cluster and a new strategy of assigning cluster membership to noisy data for data with mixed type of attribute our experimental result confirm that the algorithm not only generates better quality cluster than the traditional k mean algorithm but also exhibit good scalability property and is able to identify the underlying number of cluster in the data correctly the algorithm is implemented in the commercial data mining tool clementine which support the pmml standard of data mining model deployment 
most machine learning algorithm are lazy they extract from the training set the minimum information needed to predict it label unfortunately this often lead to model that are not robust when feature are removed or obscured in future test data for example a backprop net trained to steer a car typically learns to recognize the edge of the road but doe not learn to recognize other feature such a the stripe painted on the road which could be useful when road edge disappear in tunnel or are obscured by passing truck the net learns the minimum necessary to steer on the training set in contrast human driving is remarkably robust a feature become obscured motivated by this we propose a framework for robust learning that bias induction to learn many different model from the same input we present a meta algorithm for robust learning called featureboost and demonstrate it on several problem using backprop net k nearest neighbor and decision tree 
we present a novel probabilistic method for topic segmentation on unstructured text one previous approach to this problem utilizes the hidden markov model hmm method for probabilistically modeling sequence data the hmm treat a document a mutually independent set of word generated by a latent topic variable in a time series we extend this idea by embedding hofmann s aspect model for text into the segmenting hmm to form an aspect hmm ahmm in doing so we provide an intuitive topical dependency between word and a cohesive segmentation model we apply this method to segment unbroken stream of new york time article a well a noisy transcript of radio program on speechbot an online audio archive indexed by an automatic speech recognition engine we provide experimental comparison which show that the ahmm outperforms the hmm for this task 
the order of prenominal adjectival modifier in english is governed by complex and difficult to describe constraint which straddle the boundary between competence and performance this paper describes and compare a number of statistical and machine learning technique for ordering sequence of adjective in the context of a natural language generation system 
a framework for photo realistic view dependent image synthesis of a shiny object from a sparse set of image and a geometric model is proposed each image is aligned with the d model and decomposed intotwoimages with regard to the reflectance component based on the intensity variation of object surface point the view independent surface reflection diffuse reflection is stored a one texture map the view dependent reflection specular reflection image are used to recover the initial approximation of the illumination distribution and then a two step numerical minimization algorithm utilizing a simplified torrance sparrow reflection model is used to estimate the reflectance parameter and refine the illumination distribution this provides a very compact representation of the datanecessary to render synthetic image from arbitrary viewpoint we have conducted experiment with real object to synthesize photorealistic view dependent image within the proposed framework 
educator are interested in essay evaluation system that include feedback about writing feature that can facilitate the essay revision process for instance if the thesis statement of a student s essay could be automatically identified the student could then use this information to reflect on the thesis statement with regard to it quality and it relationship to other discourse element in the essay using a relatively small corpus of manually annotated data we use bayesian classification to identify thesis statement this method yield result that are much closer to human performance than the result produced by two baseline system 
in human perception convex surface have a strong tendency to be perceived a the figure convexity ha a stronger influence on figural organization than other global shape property such a symmetry and yet there ha been very little work on convexity property in computer vision we present a model for figure ground segregation which exhibit a preference for convex region a the figure i e the foreground the model also show a preference for smaller region to be selected a figure which is also known to hold for human visual perception e g koffka the model is based on the machinery of markov random field random walk diffusion process so that the global shape property are obtained via local and stochastic computation experimental result demonstrate that our model performs well on ambiguous figure ground display which were not captured before in particular in ambiguous display where neither region is strictly convex the model show preference to the more convex region thus offering a continuous measure of convexity in agreement with human perception 
diabetic related eye disease are the most common cause of blindness in the world so far the most effective treatment for these eye disease is early detection through regular screening to lower the cost of such screening we employ state of the art image processing technique to automatically detect the presence of abnormality in the retinal image obtained during the screening the author focus on one of the abnormal sign the presence of exudate lesion in the retinal image we propose a novel approach that combine brightness adjustment procedure with statistical classification method and local window based verification strategy experimental result indicate that we are able to achieve accuracy in term of identifying all the retinal image with exudate while maintaining a accuracy in correctly classifying the truly normal retinal image a normal this translates to a huge amount of saving in term of the number of retinal image that need to be manually reviewed by the medical professional each year 
current system for object detection in video sequence rely on explicit dynamical model like kalman filter or hidden markov model there is significant overhead needed in the development of such system a well a the a priori assumption that the object dynamic can be described with such a dynamical model this paper describes a new pattern classification technique for object detection in video sequence that us a rich over complete dictionary of wavelet feature to describe an object class unlike previous work where a small subset of feature wa selected from the dictionary this system doe no feature selection and learns the model in the full dimensional feature space comparison using different sized set of several type of feature are given we extend this representation into the time domain without assuming any explicit model of dynamic this data driven approach produce a model of the physical structure and short time dynamical characteristic of people from a training set of example no assumption are made about the motion of people just that short sequence characterize their dynamic sufficiently for the purpose of detection one of the main benefit of this approach is that transient false positive are reduced this technique compare favorably with the static detection approach and could be applied to other object class we also present a real time version of one of our static people detection system 
a linear self calibration method is given for computing the calibration of a stationary but rotating camera the internal parameter of the camera are allowed to vary from image to image allowing for zooming change of focal length and possible variation of the principal point of the camera in order for calibration to be possible some constraint must be placed on the calibration of each image the method work under the minimal assumption of zeroskew rectangular pixel or the more restrictive but reasonable condition of square pixel known pixel aspect ratio and known principal point being linear the algorithm is extremely rapid and avoids the convergence problem characteristic of iterative algorithm 
the paper present a novel approach to unsupervised text summarization the novelty lie in exploiting the diversity of concept in text for summarization which ha not received much attention in the summarization literature a diversity based approach here is a principled generalization of maximal marginal relevance criterion by carbonell and goldstein cite carbonell goldstein we propose in addition aninformation centricapproach to evaluation where the quality of summary is judged not in term of how well they match human created summary but in term of how well they represent their source document in ir task such document retrieval and text categorization to find the effectiveness of our approach under the proposed evaluation scheme we set out to examine how a system with the diversity functionality performs against one without using the bmir j corpus a test data developed by a japanese research consortium the result demonstrate a clear superiority of a diversity based approach to a non diversity based approach 
diffusion process which are widely used in low level vision are presented a a result of an underlying stochastic process the short time non linear diffusion is interpreted a a fokker planck equation which governs the evolution in time of a probability distribution for a brownian motion on a riemannian surface the non linearity of the diffusion ha a direct relation to the geometry of the surface a short time kernel to the diffusion a well a generalization are found 
separation of connected component from a graph with disconnected graph component mostly use breadth first search bfs or depth first search dfs graph algorithm here we propose a new algebraic method to separate disconnected and nearly disconnected component this method is based on spectral graph partitioning following a key observation that disconnected component will show up after properly sorted a step function like curve in the lowest eigenvectors of the laplacian matrix of the graph following an perturbative analysis framework we systematically analyzed the graph structure first on the disconnected subgraph case and second on the effect of adding edge sparsely connecting different subgraphs a a perturbation several new result are derived providing insight to spectral method and related clustering objective function example are given illustrating the concept and result our method comparing to the standard graph algorithm this method ha the same o verbar e verbar verbar v verbar log verbar v verbar complexity but is easier to implement using readily available eigensolvers further more the method can easily identify articulation point and bridge on nearly disconnected graph segmentation of a real example of web graph for query amazon is given we found that each disconnected or nearly disconnected component form a cluster on a clear topic 
the fastest growing community of web user is that of mobile visitor who browse with wireless pda cell phone and pager unfortunately most web site today are optimized exclusively for desktop broadband client and deliver content poorly suited for mobile device device that can display only a few line of text using slow wireless network to best serve the need of this growing community we propose building web site personalizersthat observe the behavior of web visitor and automatically customize and adapt site for each individual mobile visitor in this paper we give an overview of our approach to web site personalization a utilitymaximizing search through the space of personalized web site following this framework we have implemented two personalizers proteus and minpath proteus allows change to site navigation adding or removing link a well a content manipulation rearranging or eliding content and evaluates the result with a learned model of the current visitor m inpath concentrate exclusively on adding shortcut link but us a model learned by clustering visitor based on their sequence of page request we introduce proteus and minpath and outline our current and future direction for these personalizers 
a sentence extract summary of a document is a subset of the document s sentence that contains the main idea in the document we present an approach to generating such summary a hidden markov model that judge the likelihood that each sentence should be contained in the summary we compare the result of this method with summary generated by human showing that we obtain significantly higher agreement than do earlier method 
we address two open theoretical question in policy gradient reinforcement learning the first concern the efficacy of using function approximation to represent the state action value function q theory is presented showing that linear function approximation representation of q can degrade the rate of convergence of performance gradient estimate by a factor of o ml relative to when no function approximation of q is used where m is the number of possible action and l is the number of basis function in the function approximation representation the second concern the use of a bias term in estimating the state action value function theory is presented showing that a non zero bias term can improve the rate of convergence of performance gradient estimate by o m where m is the number of possible action experimental evidence is presented showing that these theoretical result lead to significant improvement in the convergence property of policy gradient reinforcement learning algorithm 
extracting rule from rbfs is not a trivial task because of nonlinear function or high input dimensionality in such case some of the hidden unit of the rbf network have a tendency to be shared across several output class or even may not contribute 
how should we decide among competing explanation of a cognitive process given limited observation the problem of model selection is at the heart of progress in cognitive science in this paper minimum description length mdl is introduced a a method for selecting among computational model of cognition we also show that differential geometry provides an intuitive understanding of what drive model selection in mdl finally adequacy of mdl is demonstrated in two area of cognitive modeling 
in the last several year the computational complexity of classical planning and htn planning have been studied but in both case it is assumed that the planner ha complete knowledge about the initial state recently there ha been proposal to use sensing action to plan in presence of incompleteness in this paper we study the complexity of planning in such case in our study we use the action description language a proposed in by m gelfond and v lifschitz and it extension the language a allows planning in the situation with complete information it is known that if we consider only plan of feasible polynomial length the planning problem for such situation is np complete even checking whether a given objective is attainable from a given initial state is np complete in this paper we show that the planning problem in presence of incompleteness is indeed harder it belongs to the next level of complexity hierarchy in precise term it is p complete to overcome the complexity of this problem c baral and t son have proposed several approximation we show that under certain condition one of these approximation o approximation make the problem np complete thus indeed reducing it complexity 
a coordination mechanism for a system of sparsely communicating agent is described the mechanism is based on a stochastic version of cellular automaton a parameter similar to a temperature can be tuned to change the behaviour of the system it is found that the best coordination occurs near a phase transition between order and chaos coordination doe not rely on any particular structure of the connection between agent thus it may be applicable to a large array of sparsely communicating mobile robot 
abstract this paper describes an approach to characterize camera and object motion based on the analysis of spatio temporal image volume in the spatio temporal slice of image volume motion is depicted a oriented pattern we propose a tensor histogram computation algorithm to represent these oriented pattern the motion trajectory in a histogram are tracked to de scribe both the camera and object motion in addition we exploit the similarity of the temporal slice in a vol ume to reliably partition a volume into motion tractable unit 
the ability to generate effective evaluative argument is critical for system intended to advise and persuade their user we have developed a system that generates evaluative argument that are tailored to the user properly arranged and concise we have also devised an evaluation framework in which the effectiveness of evaluative argument can be measured with real user this paper present the result of a formal experiment we performed in our framework to verify the influence of user tailoring on argument effectiveness 
progressive processing is a model of computation that allows a system to tradeoff computational resource against the quality of result this paper generalizes the existing model to maice it suitable for dynamic composition of information retrieval technique the new framework address effectively the uncertainty associated with the duration and output quality of each component we show how to construct an optimal meta level controller for a single task based on solving a corresponding markov decision problem and how to extend the solution to the case of multiple and dynamic task using the notion of an opportunity cost 
the motion of a planar surface between two camera view induces a homography the homography depends on the camera intrinsic and extrinsic parameter a well a on the d plane parameter while camera parameter vary across different view the plane geometry remains the same based on this fact we derive linear subspace constraint on the relative motion of multiple math plane across multiple view the paper ha three main contribution i we show that the collection of all relative homographies of a pair of plane homology across multiple view span a dimensional linear subspace ii we show how this constraint can be extended to the case of multiple plane across multiple view iii we suggest two potential application area which can benefit from these constraint a the accuracy of homography estimation can be improved by enforcing the multi view subspace constraint b violation of these multi view constraint can be used a a cue for moving object detection all the result derived in this paper are true for uncalibrated camera 
a new algorithm for computing the maximum entropy ranking over model is presented the algorithm handle arbitrary set of propositional default with associated strength assignment and succeeds whenever the set satisfies a robustness condition failure of this condition implies the problem may not be sufficiently specified for a unique solution to exist this work extends the applicability of the maximum entropy approach detailed in goldszmidt et al and clarifies the assumption on which the method is based 
the rule based bootstrapping introduced by yarowsky and it cotraining variant by blum and mitchell have met with considerable empirical success earlier work on the theory of co training ha been only loosely related to empirically useful co training algorithm here we give a new pac style bound on generalization error which justifies both the use of confidence partial rule and partial labeling of the unlabeled data and the use of an agreement based objective function a suggested by collins and singer our bound apply to the multiclass case i e where instance are to be assigned one of label for one well known form of bootstrapping is the em algorithm dempster laird and rubin this algorithm iteratively update model parameter by using the current model to infer a probability distribution on label for the unlabeled data and then adjusting the model parameter to fit the distribution on filled in label when the model defines a joint probability distribution over observable data and unobservable label each iteration of the em algorithm can be shown to increase the probability of the observable data given the model parameter however em is often subject to local minimum situation in which the filled in data and the model parameter fit each other well but the model parameter are far from their maximum likelihood value furthermore even if em doe find the globally optimal maximum likelihood parameter a model with a large number of parameter will over fit the data no pac style guarantee ha yet been given for the generalization accuracy of the maximum likelihood model 
when trying to recover d structure from a set of image themost difficult problem is establishing the correspondence betweenthe measurement most existing approach assume that featurescan be tracked across frame whereas method that exploit rigidityconstraints to facilitate matching do so only under restricted cameramotion in this paper we propose a bayesian approach thatavoids the brittleness associated with singling out one quot best quot correspondence and instead consider the 
we report a result of perturbation analysis on decoding error of the belief propagation decoder for gallager code the analysis is based on information geometry and it show that the principal term of decoding error at equilibrium come from the m embedding curvature of the log linear submanifold spanned by the estimated pseudoposteriors one for the full marginal and k for partial posterior each of which take a single check into account where k is the number of check in the gallager code it is then shown that the principal error term vanishes when the parity check matrix of the code is so sparse that there are no two column with overlap greater than 
we have written hardware simulator in acl in order to unify highspeed simulator and formal analysis model the technique used for these simulator extend to other kind of software which we demonstrate in this paper by implementing a much faster version of an algorithm for graph path nding previously verified by j moore using acl this exercise also highlight a weakness in acl the occasional need to add computational complexity to function in order to admit them to the 
team partitioned opaque transition reinforcement learning tpot rl is a distributed reinforcement learning technique that allows a team of independent agent to learn a collaborative task tpot rl wa first successfully applied to simulated robotic soccer stone veloso this paper demonstrates that tpot rl is general enough to apply to a completely different domain namely network packet routing empirical result in an abstract network routing simulator indicate that agent situated at individual node can learn to efficiently route packet through a network that exhibit changing traffic pattern based on locally observable sensation 
we investigate the short term dynamic of the recurrent competition and neural activity in the primary visual cortex in term of info rmation processing and in the context of orientation selectivity we pr opose that after stimulus onset the strength of the recurrent excitatio n decrease due to fast synaptic depression a a consequence the network shift from an initially highly nonlinear to a more linear operating reg ime sharp orientation tuning is established in the first highly compet itive phase in the second and le competitive phase precise signaling of multiple orientation and long range modulation e g by intraand inter areal connection becomes possible surround effect thus the network first extract the salient feature from the stimulus and then st art to process the detail we show that this signal processing strategy is optimal if the neuron have limited bandwidth and their objective is to transmit the maximum amount of information in any time interval beginning with the stimulus onset 
in cluster based segmentation pixel are mapped into various feature space whereupon they are subjected to a grouping algorithm in this paper we develop a robust and versatile non parametric clustering algorithm that is able to handle the unbalanced and irregular cluster encountered in such segmentationapplications the strength of our approach lie in the definition and use of two cluster validity index that are independent of the cluster topology by combining them an excellent clustering can be identified and experiment confirm that the associated cluster do indeed correspond to perceptually salient image region 
in this paper we present a novel method for finding the optimal affine transformation for matching of image the method requires no feature point doe not rely on normalization of image and can be tuned to highlight interesting part in the image furthermore the method doe not need any derivative for obtaining the affine transformation and it ha a computational cost proportional to n logn for n n image the problem of finding the optimal affine transformation is solved by an iterative algorithm in each step a global optimization is performed by the use of fft this global characteristic help the algorithm from getting trapped in a local optimum novel theoretical result are presented that show under what restriction the algorithm can be expected to work properly it intended primary use is for reconstruction problem in computer vision these rely heavily on the establishment of point correspondence in the image since the method make no assumption on the image it can be used when feature point are difficult to detect experiment on real image are included and it is shown that the algorithm is robust and performs well even in difficult situation with occlusion 
abstract learning real time a lrta is a popular control method that interleaf planning and plan execution and ha been shown to solve search problem in known environment efficiently in this paper we apply lrta to the problem of getting to a given goal location in an initially unknown environment uninformed lrta with maximal lookahead always move on a shortest path to the closest unvisited state that is to the closest potential goal state this wa believ ed to be a good exploration heuristic but we show that it doe not minimize the worst case plan execution time compared to other uninformed exploration method this result is also of interest to reinforcement learning researcher since many reinforce ment learning method use asynchronous dynamic programming interleave planning and plan execution and exhibit optimism in the face of uncertainty just like lrta 
in this paper we discus some new research direction in automaticspeech recognition asr and which somewhat deviate from theusual approach more specically we will motivate and brieydescribe new approach based on multi stream and multi bandasr these approach extend the standard hidden markov model hmm based approach by assuming that the dierent frequency channel representing the speech signal are processed by dierent independent expert quot each expert focusing 
one advantage of case based reasoning cbr is the relative ease of constructing and maintaining cbr system especially a a number of commercial cbr tool are available however there are area of cbr that current tool have not yet addressed one of these is easing or automating the acquisition of adaptation knowledge since task like design or planning typically require a significant amount of adaptation cbr system for these task still do not fully benefit from cbr s promise of reducing the development effort to address this we have developed several knowledge light method for learning adaptation knowledge from the case in the case base these method perform substitutional adaptation for both nominal and numerical value and are suitable for decomposable design problem in particular formulation and configuration test performed on a tablet formulation domain show promising result the automatic adaptation method we present can easily be incorporated in general purpose cbr tool thus further contributing to reducing the cost of cbr system 
an approximate word matching algorithm for chinese is presented based on this algorithm an effective approach to chinese spelling error detection and correction is implemented with a word tri gram language model the optimal string is searched from all possible derivation of the input sentence using operation of character substitution insertion and deletion comparing the original sentence with the optimal string spelling error detection and correction is realized simultaneously 
experimental data ha shown that synaptic strength modificationin some type of biological neuron depends upon precise spike timingdifferences between presynaptic and postsynaptic spike severaltemporally asymmetric hebbian learning rule motivated bythis data have been proposed we argue that such learning rulesare suitable to analog vlsi implementation we describe an easilytunable circuit to modify the weight of a silicon spiking neuronaccording to those learning rule test 
extracting rule from rbfs is not a trivial task because of nonlinear function or high input dimensionality in such case some of the hidden unit of the rbf network have a tendency to be shared across several output class or even may not contribute 
rambow wier and vijay shanker rambow et al point out the difference between tag derivation structure and semantic or predicate argument dependency and joshi and vijay shanker joshi and vijay shanker describe a monotonic compositional semantics based on attachment order that represents the desired dependency of a derivation without underspecifying predicate argument relationship at any stage in this paper we apply the joshi and vijay shanker conception of compositional semantics to the problem of preserving semantic dependency in synchronous tag translation shieber and schabes abeill et al in particular we describe an algorithm to obtain the semantic dependency on a tag parse forest and construct a target derivation forest with isomorphic or locally non isomorphic dependency in o n time 
hypernym link acquired through an information extraction procedure are projected on multi word term through the recognition of semantic variation the quality of the projected link resulting from corpus based acquisition is compared with projected link extracted from a technical thesaurus 
in this paper we describe a new region based approach to active contour for segmenting image composed of two or three type of region characterizable by a given statistic the essential idea is to derive curve evolution which separate two or more value of a predetermined set of statistic computed over geometrically determined subset of the image both global and local image information is used to evolve the active contour image derivative however are avoided thereby giving rise to a further degree of noise robustness compared to most edge based snake algorithm 
we present a new knowledge representation and reasoning framework for modeling nonlinear dynamical system the goal of this framework are to smoothly incorporate varying level of domain knowledge and to tailor the reasoning method and hence the search space accordingly our solution exploit generalized physical network gpn a rneta level representation of idealized two terminal element together with a hierarchy of qualitative and quantitative analysis tool to produce a dynamic modeling domain whose complexity naturally adapts to the amount of available information about the target system 
the paper develops a constraint based theory of prosodic phrasing and prominence based on an hpsg framework with an implementation in ale prominence and juncture are represented by n ary branching metrical tree the general aim is to define prosodic structure recursively in parallel with the definition of syntactic structure we address a number of prima facie problem arising from the discrepancy between syntactic and prosodic structure 
this paper make two contribution the first contribution is an approach called the customized query approach cqa to content based image retrieval the second is an algorithm called fssem that performs feature selection and clustering simultaneously the customizedqueries approach first classifies a query using the feature that best differentiate the major class and then customizes the query to that class by using the feature that best distinguish the image within the chosen major class this approach is motivated by the observation that the feature that are most effective in discriminating among image from different class may not be the most effective for retrieval of visually similar image within a class this occurs for domain in which not all pair of image within one class have equivalent visual similarity i e subclass exists because we are not given subclass label we must simultaneously find the feature that best discriminate the subclass and at the same time find these subclass we use fssem to find these feature we apply this approach to content based retrieval of high resolution tomographic image of patient with lung disease and show that this approach radically improves the retrieval precision over the traditional approach that performs retrieval using a single feature vector 
the explosion of content in distributed information retrieval ir system requires new mechanism to attain timely and accurate retrieval of unstructured text in this paper we compare two mechanism to improve ir system performance partial collection replication and caching when query have locality both mechanism return result more quickly than sending query to the original collection s cache return result when query exactly match a previous one partial replica are a form of caching that return result when the ir technology determines the query is a good match cache are simpler and faster but replica can increase locality by detecting similarity between query that are not exactly the same we use real trace from thomas and excite to measure query locality and similarity with a very restrictive definition of query similarity similarity improves query locality up to over exact match we use a validated simulator to compare their performance and find that even if the partial replica hit rate increase only to it will outperform simple caching under a variety of configuration a combined approach will probably yield the best performance 
this paper describes a system for unsupervised learning of morphological affix from text or word list the system is composed of a generative probability model and a search algorithm experiment on the wall street journal and the hansard corpus french and english demonstrate the effectiveness of this approach the result suggest that more integrated system for learning both affix and morphographemic adjustment rule may be feasible in addition several definition and a theorem are developed so that our search algorithm can be formalized in term of the lattice formed by subset of suffix under inclusion this formalism is expected to be useful for investigating alternative search strategy over the same morphological hypothesis space 
this paper address the problem of constructing the scale space aspect graph of a solid of revolution whose surface is the zero set of a polynomial volumetric density undergoing a gaussian diffusion process equation for the associated visual event surface are derived and polynomial curve tracing technique are used to delineate these surface an implementation and example are presented and limitation a well a extension of the proposed approach are discussed 
in recent year statistical approach on atr automatic term recognition have achieved good result however there are scope to improve the performance in extracting term still further for example domain dictionary can improve the performance in atr this paper focus on a method for extracting term using a dictionary hierarchy our method produce relatively good result for this task 
we address the problem of detection and tracking of moving object in a video stream obtained from a moving airborne platform the proposed method relies on a graph representation of moving object which allows to derive and maintain a dynamic template of each moving object by enforcing their temporal coherence this inferred template along with the graph representation used in our approach allows u to characterize object trajectory a an optimal path in a graph the proposed tracker allows to deal with partial occlusion stop and go motion in very challenging situation we demonstrate result on a number of different real sequence we then define an evaluation methodology to quantify our result and show how tracking overcome detection error 
this paper develops a theoretical learning model of text classification for support vector machine svms it connects the statistical property of text classification task with the generalization performance of a svm in a quantitative way unlike conventional approach to learning text classifier which rely primarily on empirical evidence this model explains why and when svms perform well for text classification in particular it address the following question why can support 
we present a method to learn object class model forthe purpose of object recognition we focus on a particulartype of model where object are represented asconstellations of rigid feature part the variabilitywithin a class is represented by a joint probabilitydensity function pdf on the shape of the constellationand the output of feature detector the pdf maybe estimated from training data once a model structure type and number of feature ha been specied themethod 
we present efficient technique for computing near optimal strategy for a class of stochastic commodity trading problem modeled a markov decision process mdps the process ha a continuous state space and a large action space and cannot be solved efficiently by standard dynamic programming method we exploit structural property of the process and combine it with monte carlo estimation technique to obtain novel and efficient algorithm that closely approximate the optimal strategy 
this paper focus on the detection of object with lambertian surface under both varying dlumination and pose we offer to apply a novel detection method that proceeds by modeling the d erent illumination from a small number of image in the training set this automatically void the illumination effect allowing fast dlumination invariant detection without having to create a large training set it is demonstrated that the method t in nicely with previous work about the modeling oj the set of object appearance under varying illumination in the experiment an object wa correctly detected under image plane rotation in a degree range and a wide variety of dl erent illumination 
algorithm for feature selection fall into twobroad category wrapper that use thelearning algorithm itself to evaluate the usefulnessof feature and lters that evaluatefeatures according to heuristic based on generalcharacteristics of the data for applicationto large database lters have provento be more practical than wrapper becausethey are much faster however most existinglter algorithm only work with discreteclassication problem this paper describesa 
in a known environment object may be tracked in multiple view using a set of background model stereobased model can be illumination invariant but often have undefined value which inevitably lead to foreground classification error we derive dense stereo model for object tracking using long term extended dynamic range imagery and by detecting and interpolating uniform but unoccluded planar region foreground point are detected quickly in new image using pruned disparity search we adopt a late segmentation strategy using an integrate dp lan viewdensity representation foreground point are segmented into object region only when a trajectory is finally estimated using a dynamic programming based method object entry and exit are optimally determined and are not restricted to special spatial zone 
the aim of this study is a systematic evaluationand comparison of four state of the art datadrivenlearning algorithm applied to part ofspeech tagging of swedish the algorithm includedin this study are hidden markov model maximum entropy memory based learning and transformation based learning the systemsare evaluated from several aspect boththe eects of tag set and the eects of the sizeof training data are examined the accuracy iscalculated a well a the error rate for 
extractive summarization technique cannot generate document summary shorter than a single sentence something that is often required an ideal summarization system would understand each document and generate an appropriate summary directly from the result of that understanding a more practical approach to this problem result in the use of an approximation viewing summarization a a problem analogous to statistical machine translation the issue then becomes one of generating a target document in a more concise language from a source document in a more verbose language this paper present result on experiment using this approach in which statistical model of the term selection and term ordering are jointly applied to produce summary in a style learned from a training corpus 
one general strategy for approximatelysolving large markov decision process is quot divide and conquer quot the original problemis decomposed into sub problem which interactwith each other but yet can besolved independently by taking into accountthe nature of the interaction in this paperwe focus on a class of quot policy coupled quot semi markov decision process smdps which arise in many nonstationary real worldmulti agent task such a manufacturing androbotics the nature of the 
the technology of data warehousing data mining hypertext analysis information visualization and web information resource are rapidly converging the challenge is to architect these technology into a system for systematic business intelligence for a corporation we need to move from an information refining process that is often haphazard and narrow to one that is reliable and continuous web farming is a new area that suggests a methodology and architecture for accomplishing this 
we present a noun phrase coreference system that extends the work of soon et al and to our knowledge produce the best result to date on the muc and muc coreference resolution data set f measure of and respectively improvement arise from two source extra linguistic change to the learning framework and a large scale expansion of the feature set to include more sophisticated linguistic knowledge 
nowadays firm formerly considering the human operator a the main error source in process control bend their effort towards anthropocentric approach to re integrate the human factor especially the knowledge he she ha been developping a the essential resource for a high quality decision process a the expert operator remains a rare resource and in order to capitalize his her knowledge and know how the development of tool integrating this new dimension ha become an important challenge this paper deal with a tool for knowledge acquisition under cognitive constraint assuming that cognitive principle could be sometimes useful to improve machine learning tool result additionally we have to cope with the difficulty linked to the fact that the acquired strategy have to be adapted on line after describing the underlying cognitive principle we will introduce the decision representation space and it related notation we will then show the difficulty linked to the search of an optimal representation of the expert strategy set and how the heuristic used by the algorithm studied avoid these np complete problem finally the current result and our work perspective are stated 
algorithm for the alignment of word in translated text are well established however only recently new approach have been proposed to identify word translation from non parallel or even unrelated text this task is more difficult because most statistical clue useful in the processing of parallel text cannot be applied to non parallel text whereas for parallel text in some study up to of the word alignment have been shown to be correct the accuracy for non parallel text ha been around up to now the current study which is based on the assumption that there is a correlation between the pattern of word co occurrence in corpus of different language make a significant improvement to about of word translation identified correctly 
the problem of machine translation can be viewed a consisting of two subproblems a lexical selection and b lexical reordering we propose stochastic finite state model for these two subproblems in this paper stochastic finite state model are efficiently learnable from data effective for decoding and are associated with a calculus for composing model which allows for tight integration of constraint from various level of language processing we present a method for learning stochastic finite state model for lexical choice and lexical reordering that are trained automatically from pair of source and target utterance we use this method to develop model for english japanese translation and present the performance of these model for translation on speech and text we also evaluate the efficacy of such a translation model in the context of a call routing task of unconstrained speech utterance 
this paper present a novel statistical latent class model for text mining and interactive information access the described learning architecture called cluster abstraction model cam is purely data driven and utilizes contact specific word occurrence statistic in an intertwined fashion the cam extract hierarchical relation between group of document a well a an abstractive organization of keywords an annealed version of the expectation maximization em algorithm for maximum likelihood estimation of the model parameter is derived the benefit of the cam for interactive retrieval and automated cluster summarization are investigated experimentally 
we show that support vector machine svm s are much better than conventional algorithm in a relevancy feedback rf environment in information retrieval ir of text document we track performance a a function of feedback iteration and show that while the conventional algorithm do very well in the initial feedback iteration if the topic searched for ha high visibility in the data base they do very poorly if the relevant document are a small percentage of the data base svm s however do very well when the number of document returned in the preliminary search is low and the number of relevant document is small the competitive algorithm examined are rocchio ide regular and ide dec hi 
in this paper we introduce a statistic snake that learns and track image feature by mean of statistic learning technique using probabilistic principal component analysis a feature description is obtained from a training set of object profile in our approach a sound statistical model is introduced to define a likelihood estimate of the grey level local image profile together with their local orientation this likelihood estimate allows to define a probabilistic potential field of the snake where the elastic curve deforms to maximise the overall probability of detecting learned image feature to improve the convergence of snake deformation we enhance the likelihood map by a physic based model simulating a dipole dipole interaction a new extended local coherent interaction is introduced defined in term of extended structure tensor of the image to give priority to parallel coherence vector structure to cope with the appearance variance the search space is reduced by adding knowledge from the application domain obtaining better response from feature detector the global snake performance is increased in principal component analysis pca is used to model face shape and grey level image a limiting disadvantage of pca is the absence of a probability density model and an associated likelihood measure the need of a probability density framework is clearly present in problem where saliency is formulated in term of visual similarity the derivation of pca from a perspective of density estimation offer the advantage that the probability density function give a measure of the novelty of a new data point given the advantage of probabilistic pca ppca a a straightforward technique to construct statistic image feature description and snake a a global segmentation and tracking technique in this paper we propose a combination of these technique to track non rigid elongated object object profile are learned from a training set and a statistic classifier is constructed a a potential field of the snake the map is built in two step first a structure tensor is applied to assign a coherence direction to each pixel of the target image second image profile perpendicular to the coherence orientation are weighed by ppca defining a likelihood map therefore each point ha assigned a probability measure to belong to the learned feature category additionally we refine the likelihood map applying to the coherence direction field an extended local coherent detection between neighbour of the likelihood map a a function of derivative up to the second order we show that this detection prioritises region of pixel with parallel coherent direction improving the localisation of elongated structure a a result the refined likelihood map ha good response around the object while small amount of false response are observed to avoid false stationary state of the snake we introduce a hybrid potential map a a combination of refined likelihood map and distance map that assures slow movement far from the object and fast convergence when approaching the object of interest 
in this paper we discus the semiparametric statistical model for blind deconvolution first we introduce a lie group to the manifold of noncausal fir filter then blind deconvolution problem is formulated in the framework of a semiparametric model and a family of estimating function is derived for blind deconvolution a natural gradient learning algorithm is developed for training noncausal filter stability of the natural gradient algorithm is also analyzed in this framework 
the article focus on distributed reinforcementlearning in cooperative multiagent decision process where an ensembleof simultaneously and independently actingagents try to maximize a discounted sumof reward we assume that each agenthas no information about it teammate behaviour thus in contrast to single agentreinforcement learning each agent ha to considerits teammate behaviour and to nd acooperative policy we propose a model freedistributed q learning 
abstract a new algorithm for support vector regression is described for a priori chosen it automatically adjusts a flexible tube of minimal radius to the data such that at most a fraction of the data point lie outside moreover it is shown how to use parametric tube shape with non constant radius the algorithm is analysed theoretically and experimentally 
this paper describes a novel gabor feature class er gfc method for face recognition the gfc method employ an enhanced fisher discrimination model on an augmented gabor feature vector which is derived from the gabor wavelet transformation office image the gabor wavelet whose kernel are similar to the receptive field profile of the nianinialian cortical simple cell exhibit desirable characteristic of spatial locality and orientation selectivity a a result the gabor transformed face image produce salient local and discriminating feature that are suitable forface recognition the feasibility of the new gfc method ha been successfully tested on face recognition using feret frontal face image which involve different illumination and varied facial expression of subject the effectiveness of the novel gfc method is shown in ternis of both absolute performance index and comparative performance against some popular face recognition scheme such a the eigenfaces method and some other gabor wavelet based class cation method in particular the novel gfc method achieves recognition accuracy using only feature 
abstract locally weighted projection regression is a new algorithm that achieves nonlinear function approximation in high dimensional space with redundant and irrelevant input dimension at it core it us locally linear model spanned by a small number of univariate regression in selected direction in input space this paper evaluates different method of projection regression and derives a nonlinear function approximator based on them this nonparametric local learning system i learns rapidly with second order learning method based on incremental training ii us statistically sound stochastic cross validation to learn iii adjusts it weighting kernel based on local information only iv ha a computational complexity that is linear in the number of input and v can deal with a large number of possibly redundant input a shown in evaluation with up to dimensional data set to our knowledge this is the first truly incremental spatially localized learning method to combine all these property 
we propose a design for displaying and manipulating html form on small pda screen the form input widget are not shown until the user is ready to fill them in at that point only one widget is shown at a time the form is summarized on the screen by displaying just the text label that prompt the user for each widget s information the challenge of this design is to automatically find the match between each text label in a form and the input widget for which it is the prompt we developed eight algorithm for performing such label widget match some of the algorithm are based on n gram comparison while others are based on common form layout convention we applied a combination of these algorithm to simple html form with an average of four input field per form these experiment achieved a matching accuracy we developed a scheme that combine all algorithm into a matching system this system did well even on complex form achieving accuracy in our experiment involving input field spread over complex form 
mesoscopical mathematical description of dynamic of populationsof spiking neuron are getting increasingly important for theunderstanding of large scale process in the brain using simulation 
we introduce the first algorithm for off policy temporal difference learning that is stable with linear function approximation off policy learning is of interest because it form the basis for popular reinforcement learning method such a q learning which ha been known to diverge with linear function approximation and because it is critical to the practical utility of multi scale multi goal learning framework such a option ham and maxq our new algorithm combine td over state action pair with importance sampling idea from our previous work we prove that given training under any soft policy the algorithm converges w p to a close approximation a in tsitsiklis and van roy tadic to the action value function for an arbitrary target policy variation of the algorithm designed to reduce variance introduce additional bias but are also guaranteed convergent we also illustrate our method empirically on a small policy evaluation problem our current result are limited to episodic task with episode of bounded length 
the inductive learning is effective for a variety of natural language processing problem however it need expensive training data the quality of learned rule often depends on the quality of training data used in this paper we propose a method to detect error in training data automatically to improve the quality of the training data we consider a japanese word segmentation problem in which training data mean the corpus including word boundary sign we use two clue to detect error one is a property of a decision list and the other is the nature of adaboost in experiment we provided doubtful instance from the kyoto univeristy corpus which is the most trustworthy japanse tagged corpus most of them had tag which we could not easily determine to be correct or incorrect moreover instance were actually error 
query clustering is crucial for automatically discovering frequently asked query faq or most popular topic on a question answering search engine due to the short length of query the traditional approach based on keywords are not suitable for query clustering this paper describes our attempt to cluster similar query according to their content a well a the document click information in the user log 
this paper considers the framework of the so called market basket problem in which a database of transaction is mined for the occurrence of unusually frequent item set in our case unusually frequent involves estimate of the frequency of each item set divided by a baseline frequency computed a if item occurred independently the focus is on obtaining reliable estimate of this measure of interestingness for all item set even item set with relatively low frequency for example in a medical database of patient history unusual item set including the item patient death or other serious adverse event might hopefully be flagged with a few a or occurrence of the item set it being unacceptable to require that item set occur in a many a of million of patient report before the data mining algorithm detects a signal similar consideration apply in fraud detection application thus we abandon the requirement that interesting item set must contain a relatively large fixed minimal support and adopt a criterion based on the result of fitting an empirical bayes model to the item set count the model allows u to define a bayesian lower confidence limit for the interestingness measure of every item set whereupon the item set can be ranked according to their empirical bayes confidence limit for item set of size j we also distinguish between multi item association that can be explained by the observed j j pairwise association and item set that are significantly more frequent than their pairwise association would suggest such item set can uncover complex or synergistic mechanism generating multi item association this methodology ha been applied within the u s food and drug administration fda to database of adverse drug reaction report and within at t to customer international calling history we also present graphical technique for exploring and understanding the modeling result 
in a directed query distributed search engine wa integrated into a new department of energy virtual library of energy science and technology million of page of government information across multiple agency were made immediately searchable via one query setting the stage for the development of a variety of interagency initiative and application 
this paper introduces a new free form surface representation scheme for the purpose of fast and accurate registration and matching accurate registration of surface is a common task in computer vision the proposed representation scheme capture the surface curvature information seen from certain point and produce image called surface signature at these point matching signature of different surface enables the recovery of the transformation parameter between these surface we propose to use template matching to compare the signature image to enable partial matching another criterion the overlap ratio is used this representation scheme can be used a a global representation of the surface a well a a local one and performs near real time registration we show that the signature representation can be used to match object in d scene in the presence of clutter and occlusion application presented include free form object matching multimodal medical volume registration and dental teeth reconstruction from intra oral image 
in the present study we introduce a simple iterative procedure that allows to correct the output of a classifler with respect to the new a priori probability of a new data set to be scored even when these new a priori probability are unknown in advance we also show that a signiflcant increase in classiflcation accuracy can be observed when using this procedure properly more speciflcally by applying the correcting procedure to the output of a simple logistic regression model we observe an increase of of classiflcation rate on a di cult real world multi class problem the automatic labeling of geographical map based on remote sensing information moreover the resulting classifler the logistic regression model whose output have been adjusted according to our procedure outperformed by more that all of our previous model in term of classiflcation accuracy including bagfs a multiple classifler system based on c decision tree the best obtained model up to now 
there are many challenging problem forvietnamese language processing it will be along time before these challenge are met evensome apparently simple problem such asspelling correction are quite difficult and havenot been approached systematically yet in thispaper we will discus one aspect of this type ofwork designing the so called vietools to detectand correct spelling of vietnamese text byusing a spelling database based on telexcode vietools is also extended to serve 
this report present a svm like learning system to handle multi label problem such problem arise naturally in bio informatics consider for instance the mips yeast genome database in it is formed by around gene associated to their functional class one gene can have many class and different gene do not belong to the same number of functional category such a problem can not be solved directly with classical approach and it is ge nerally decomposed into many two class problem the binary decomposition ha been done partially by different researcher on the yeast dataset but it doe not provide a satisfactory answer we exp lore in this report a new direct approach it is based on a large margin ranking system that share a lot of c ommon property with support vector machine we tested it on a toy problem and on real datasets with positive result we also present a new method to do feature selection with multi labelled datasets our method is based on the multiplicative update rule defined in 
we propose a new and efficient technique for incorporating co ntextual information into object classification most of the current techniq ues face the problem of exponential computation cost in this paper we propose a new general framework that incorporates partial context at a linear cost this technique is applied to microscopic urinalysis image recognition resulting in a significant improvement of recognition rate over the context free approach thi s gain would have been impossible using conventional context incorporation technique 
this paper present a novel approach for model based realtime tracking of highly articulated structure such a human this approach is based on an algorithm which efciently propagates statistic of probability distribution through a kinematic chain to obtain maximum a posteriori estimate of the motion of the entire structure this algorithm yield the least square solution in linear time in the number of component of the model and can also be applied to non gaussian statistic using a simple but powerful trick the resulting implementation run in real time on standard hardware without any pre processing of the video data and can thus operate on live video result from experiment performed using this system are presented and discussed 
abstract to classify a large number of unlabeled example we combine a limited number of labeled example with a markov random walk representation over the unlabeled example the random walk representation exploit any low dimensional structure in the data in a robust probabilistic manner we develop and compare several estimation criterion algorithm suited to this representation this includes in particular multi way classification with an average margin criterion which permit a closed form solution the time scale of the random walk regularizes the representation and can be set through a margin based criterion favoring unambiguous classification we also extend this basic regularization by adapting time scale for individual example we demonstrate the approach on synthetic example and on text classification problem 
we propose a locally adaptive technique toaddress the problem of setting the bandwidthparameters optimally for kernel density estimation our technique is efficient and canbe performed in only two dataset pass wealso show how to apply our technique toefficiently solve range query approximation classification and clustering problem for verylarge datasets we validate the efficiency andaccuracy of our technique by presenting experimentalresults on a variety of both 
voxel occupancy is one approach for reconstructing the dimensional shape of an object from multiple view in voxel occupancy the task is to produce a binary labeling of a set of voxels that determines which voxels are filled and which are empty in this paper we give an energy minimization formulation of the voxel occupancy problem the global minimum of this energy can be rapidly computed with a single graph cut using a result due to greig porteous and seheult the energy function we minimize contains a data term and a smoothness term the data term is a sum over the individual voxels where the penalty for a voxel is based on the observed intensity of the pixel that intersect it the smoothness term is the number of empty voxels adjacent to filled one our formulation can be viewed a a generalization of silhouette intersection with two advantage we do not compute silhouette which are a major source of error and we can naturally incorporate spatial smoothness we give experimental result showing reconstruction from both real and synthetic imagery reconstruction using this smoothed energy function is not much more time consuming than simple silhouette intersection it take about second to reconstruct a one million voxel volume 
the speech waveform can be modelled a a piecewise stationary linear stochastic state space system and it parameter can be est imated using an expectation maximisation em algorithm one problem is the initialisation of the em algorithm standard initialisation s chemes can lead to poor formant trajectory but these trajectory howev er are important for vowel intelligibility the aim of this paper is to in vestigate the suitability of subspace identification method to initiali se em the paper compare the subspace state space system identific ation sid method with the em algorithm the sid and em method are similar in that they both estimate a state sequence but usin g kalman filter and kalman smoother respectively and then estimate parameter but using least square and maximum likelihood respectively the similarity of sid and em motivates the use of sid to initialise em also sid is non iterative and requires no initialisation wher ea em is iterative and requires initialisation however sid is sub opti mal compared to em in a probabilistic sense during experiment on real speech sid method compare favourably with conventional initialisation technique they produce smoother formant trajectory have greater f requency resolution and produce higher likelihood 
based on a set of design principle automated visual presentation system promise to simplify an application programmer s design task by automatically constructing appropriate visual explanation for different information however these automated presentation system must be equipped with a powerful inference approach to suit practical application here we present a planning based practical inference approach that can design a series of connected visual presentation in interactive environment our emphasis here is on a set of important visual planning feature and how they facilitate visual design this set of feature includes a knowledge rich representation of visual planning variable and constraint a novel object decomposition model that can be used with action decomposition to simplify the visual synthesis process and practical temporal and spatial reasoning capability to facilitate coherent visual design and presentation in addition we have implemented our visual planning approach in a visual planner called previse a part of our automated presentation testbed system a set of example is also given to illustrate the necessity and utility of our visual planning approach 
the stop system which generates personalised smoking cessation letter wa evaluated by a randomised controlled clinical trial we believe this is the largest and perhaps most rigorous task effectiveness evaluation ever performed on an nlg system the detailed result of the clinical trial have been presented elsewhere in the medical literature in this paper we discus the clinical trial itself it structure and cost what we did and did not learn from it especially considering that the trial showed that stop wa not effective and how it compare to other nlg evaluation technique 
using finite state automaton for the text analysis component in a text to speech system is problematic in several respect the rewrite rule from which the automaton are compiled are difficult to write and maintain and the resulting automaton can become very large and therefore inefficient converting the knowledge represented explicitly in rewrite rule into a more efficient format is difficult we take an indirect route learning an efficient decision tree representation from data and tapping information contained in existing rewrite rule which increase performance compared to learning exclusively from a pronunciation lexicon 
we show that discourse structure need not bear the full burden of conveying discourse relation by showing that many of them can be explained nonstructurally in term of the grounding of anaphoric presupposition van der sandt this simplifies discourse structure while still allowing the realisation of a full range of discourse relation this is achieved using the same semantic machinery used in deriving clause level semantics 
abstract a neural model is described which us oscillatory correlation to segregate speech from interfering sound source the core of the model is a two layer neural oscillator network a sound stream is represented by a synchronized population of oscillator and different stream are represented by desynchronized oscillator population the model ha been evaluated using a corpus of speech mixed with interfering sound and produce an improvement in signal to noise ratio for every mixture 
in this paper we present a novel approach for frontal face detection in gray scale image we represent both face and clutter by using two dimensional wavelet decomposition to characterize the statistical dependency between different level of wavelet we introduce a hidden markov model hmm in which a number of discrete state at each level capture the diversity of face a well a clutter our experiment indicate that the proposed algorithm outperforms conventional template based method such a matched filter and eigenface method 
we propose an algorithm eciently implementing td using the innite tree of haar basis function the algorithm canmaintain and update the information of theinnite tree of coecients in it nitely compressedform by taking advantage of the factthat the information obtained from nitetraining data is nite our algorithm computesthe whole updating at each time stepin time linear in the precision measured bythe number of bit of each observation thesystem of haar 
this paper present a revision learning method that achieves high performance with small computational cost by combining a model with high generalization capacity and a model with small computational cost this method us a high capacity model to revise the output of a small cost model we apply this method to english part of speech tagging and japanese morphological analysis and show that the method performs well 
the ability to efficiently and robustly recover accurate d terrain model from set of stereoscopic image is important to many civilian and military application our long term goal is to develop an automatic multi image d reconstruction algorithm that can be applied to these domain to develop an effective and practical terrain modeling system method must be found for detecting unreliable elevation in digital elevation map dems and for fusing several dems from multiple source into an accurate and reliable result this paper focus on two key factor for generating robust d terrain model the ability to detect unreliable elevation estimate and to fuse the reliable elevation into a single optimal terrain model the technique discussed in this paper are based on the concept of using self consistency to identify potentially unreliable point we apply the self consistency methodology to both the two image and multi image scenario we demonstrate that the recently developed concept of self consistency can be effectively employed to determine the reliability of value in a dem estimate with a reliability below an error threshold can be excluded from further processing we test the effectiveness of the methodology a well a the relationship between error rate and scene geometry by processing both real and photo realistic simulation 
most information retrieval system on the internet rely primarily on similarity ranking algorithm based solely on term frequency statistic information quality is usually ignored this lead to the problem that document are retrieved without regard to their quality we present an approach that combine similarity based similarity ranking with quality ranking in centralized and distributed search environment six quality metric including the currency availability information to noise ratio authority popularity and cohesiveness were investigated search effectiveness wa significantly improved when the currency availability information to noise ratio and page cohesiveness metric were incorporated in centralized search the improvement seen when the availability information tonoise ratio popularity and cohesiveness metric were incorporated in site selection wa also significant finally incorporating the popularity metric in information fusion resulted in a significant improvement in summary the result show that incorporating quality metric can generally improve search effectiveness in both centralized and distributed search environment 
this paper describes a method of dogleg trust region step or restrictedlevenberg marquardt step based on a projection processonto the krylov subspace for neural network nonlinear leastsquares problem in particular the linear conjugate gradient cg method work a the inner iterative algorithm for solving the linearizedgauss newton normal equation whereas the outer nonlinearalgorithm repeatedly take so called quot krylov dogleg quot step relyingonly on matrix vector 
the quest for a vision system capable of representing and recognizing arbitrary motion benefit from a low dimensional non specific representation of flow field to be used in high level classification task we present zernike polynomial a an ideal candidate for such a representation the basis of zernike polynomial is complete and orthogonal and can be used for describing many type of motion at many scale starting from image sequence locally smooth image velocity are derived using a robust estimation procedure from which are computed compact representation of the flow using the zernike basis continuous density hidden markov model are trained using the temporal sequence of vector thus obtained and are used for subsequent classification we present result of our method applied to image sequence of facial expression both with and without significant rigid head motion and to sequence of lip motion from a known database we demonstrate that the zernike representation yield result competitive with those obtained using principal component while not committing to specific type of motion it is therefore ideal a a fundamental building block for a vision system capable of classifying arbitrary motion type 
greedy approximation algorithm have been frequently used to obtain sparse solution to learning problem in this paper we present a general greedy algorithm for solving a class of convex optimization problem we derive a bound on the rate of approximation for this algorithm and show that our algorithm includes a number of earlier study a special case 
this paper present a d active contour model for boundary detection and tracking of non rigid object which applies stereo vision and motion analysis to the class of energy minimizing deformable contour model known a snake the proposed contour evolves in three dimensional space in reaction to a d potential function which is derived by projecting the contour onto the d stereo image the potential function is augmented by a kinetic term which is related to the velocity field along the contour this term is used to guide the inter image contour displacement the incorporation of inter frame velocity estimate in the tracking algorithm is especially important for contour which evolve in d space where the added freedom of motion can easily result in loss of tracking the proposed scheme incorporates local velocity information seamlessly in the snake model with little computational overhead and doe not require exogenous computation of the optical flow or related quantity in each image the resulting algorithm is shown to provide good tracking performance with only one iteration per frame which provides a considerable advantage for real time operation 
the noisy channel model ha been applied to a wide range of problem including spelling correction these model consist of two component a source model and a channel model very little research ha gone into improving the channel model for spelling correction this paper describes a new channel model for spelling correction based on generic string to string edits using this model give significant performance improvement compared to previously proposed model 
abstract this paper examines the application of reinforcement learning to a telecommunication networking problem the problem requires that revenue be maximized while simultaneously meeting a quality of service constraint that forbids entry into certain state we present a general solution to this multi criterion problem that is able to earn significan tly higher revenue than alternative 
we present the application of feature mining technique to the developmental therapeutic program s aid antiviral screen database the database consists of compound which were measured for their capability to protect human cell from hiv infection according to these measurement the compound were classified a either active moderately active or inactive the distribution of class is extremely skewed only of the molecule is known to be active and is known to be moderately active given this database we were interested in molecular substructure i e feature that are frequent in the active molecule and infrequent in the inactives in data mining term we focused on feature with a minimum support in active compound and a maximum support in inactive compound we analyzed the database using the levelwise version space algorithm that form the basis of the inductive query and database system molfea molecular feature miner within this framework it is possible to declaratively specify the feature of interest such a the frequency of feature on possibly different datasets a well a on the generality and syntax of them assuming that the detected substructure are causally related to biochemical mechanism it should be possible to facilitate the development of new pharmaceutical with improved activity 
we propose a new approach to reinforcement learning which combine least square function approximation with policy iteration our method is model free and completely off policy we are motivated by the least square temporal difference learning algorithm lstd which is known for it efficient use of sample experience compared to pure temporal difference algorithm lstd is ideal for prediction problem however it heretofore ha not had a straightforward application to control problem moreover approximation learned by lstd are strongly influenced by the visitation distribution over state our new algorithm least square policy iteration lspi address these issue the result is an off policy method which can use or reuse data collected from any source we have tested lspi on several problem including a bicycle simulator in which it learns to guide the bicycle to a goal efficiently by merely observing a relatively small number of completely random trial 
belief propagation bp wa only supposed to work for tree likenetworks but work surprisingly well in many application involvingnetworks with loop including turbo code however there hasbeen little understanding of the algorithm or the nature of thesolutions it nd for general graph we show that bp can only converge to a stationary point of anapproximate free energy known a the bethe free energy in statisticalphysics this result characterizes bp xed point and make 
background estimation and removal based on the joint use of range and color data produce superior result than can be achieved with either data source alone this is increasingly relevant a inexpensive real time passive range system become more accessible through novel hardware and increased cpu processing speed range is a powerful signal for segmentation which is largely independent of color and hence not effected by the classic color segmentation problem of shadow and object with color similar to the background however range alone is also not sufficient for the good segmentation depth measurement are rarely available at all pixel in the scene and foreground object may be indistinguishable in depth when they are close to the background color segmentation is complementary in these case surprisingly little work ha been done to date on joint range and color segmentation we describe and demonstrate a background estimation method based on a multidimensional range and color clustering at each image pixel segmentation of the foreground in a given frame is performed via comparison with background statistic in range and normalized color important implementation issue such a treatment of shadow and low confidence measurement are discussed in detail in this paper we present a passive method for background estimation and removal based on the joint use of range and color which produce superior result than can be achieved with either data source alone this approach is now practical for general application a inexpensive real time passive range data is becoming more accessible through novel hardware and increased cpu processing speed the joint use of color and range produce cleaner segmentation of the foreground scene in comparison to the commonly used color based background subtraction or rangebased segmentation 
word alignment problem between parallel corpus is based on the similar characteristic of two aligned word in two language we investigate linguistic knowledge based word similarity measure while other previous work heavily rely on statistical information and their limit will be discussed linguistic knowledge is acquired from linguistic comparison of all layer between two language for chinese and korean in this paper 
filled pause are characteristic of spontaneous speech and can present considerable problem for speech recognition by being often recognized a short word an um can be recognized a thumb or arm if the recognizer s language model doe not adequately represent fp s recognition of quasi spontaneous speech medical dictation is subject to this problem a well result from medical dictation by family practice physician show that using an fp model trained on the corpus populated with fp s produce overall better result than a model trained on a corpus that excluded fp s or a corpus that had random fp s 
motivated by our recent work on rooted tree matching in this paper we provide a solution to the problem of matching two free i e unrooted tree by constructing an association graph whose maximal clique are in one to one correspondence with maximal common subtrees we then solve the problem using simple replicator dynamic from evolutionary game theory experiment on hundred of uniformly random tree are presented the result are impressive despite the inherent inability of these simple dynamic to escape from local optimum they always returned a globally optimal solution 
this work proposes and evaluates a probabilistic cross lingual retrieval system the system us a generative model to estimate the probability that a document in one language is relevant given a query in another language an important component of the model is translation probability from term in document to term in a query our approach is evaluated when the only resource is a manually generated bilingual word list the only resource is a parallel corpus and both resource are combined in a mixture model the combined resource produce about of monolingual performance in retrieving chinese document for spanish the system achieves of monolingual performance using only a pseudo parallel spanish english corpus retrieval result are comparable with those of the structural query translation technique pirkola when bilingual lexicon are used for query translation when parallel text in addition to conventional lexicon are used it achieves better retrieval result but requires more computation than the structural query translation technique it also produce slightly better result than using a machine translation system for clir but the improvement over the mt system is not significant 
when designing computer vision system for the blind and visually impaired it is important to determine the orientation of the user relative to the scene we observe that most indoor and outdoor city scene are designed on a manhattan three dimensional grid this manhattan grid structure put strong constraint on the intensity gradient in the image we demonstrate an algorithm for detecting the orientation of the user in such scene based on bayesian inference using statistic which we have learnt in this domain our algorithm requires a single input image and doe not involve pre processing stage such a edge detection and hough grouping we demonstrate strong experimental result on a range of indoor and outdoor image we also show that estimating the grid structure make it significantly easier to detect target object which are not aligned with the grid 
we develop an efficient algorithm to track point feature supported by image patch undergoing affine deformation and change in illumination the algorithm is based on a combined model of geometry and photometry that is used to track feature a well a to detect outlier in a hypothesis testing framework the algorithm run in real time on a personal computer and is available to the public 
we formulate the problem of reconstructing the shape and radiance of a scene a the minimization of the information divergence between blurred image and propose an algorithm that is provably convergent and guarantee that the solution is admissible in the sense of corresponding to a positive radiance and imaging kernel the motivation for the use of information divergence come from the work of csisz r while the fundamental element of the proof of convergence come from work by snyder et al extended to handle unknown imaging kernel i e the shape of the scene 
q is a reinforcement learning algorithm that combine q learning and td online implementation of q that use eligibility trace have been shown to speed basic q learning in this paper we present an asymptotic analysis of watkins q with accumulative eligibility trace we first introduce an asymptotic approximation of q that appears to be a gain matrix variant of basic qlearning using the ode method we then determine an optimal gain matrix for q learning that maximizes it rate of convergence toward the optimal value function the similarity between this optimal gain and the asymptotic gain of q explains the relative efficiency of the latter for furthermore by minimizing the difference between these two gain optimal value for the parameter and the decreasing learning rate can be determined this optimal strongly depends on the exploration policy during learning a robust approximation of these learning parameter lead to the definition of a new efficient algorithm called aq learning average q learning that show a close resemblance to schwartz r learning our result have been demonstrated through numerical simulation 
image sequence capturing hurricane luis throughmeteorological satellite go and go areused to estimate hurricane top height structure andhurricane wind motion this problem is difficultnot only due to the absence of correspondence but alsodue to the lack of depth cue in the d hurricane image scaled orthographic projection in this paper wepresent a structure and motion analysis system calledsmas in this system the hurricane image are firstsegmented into small 
ever since the beginning of the web finding useful information from the web ha been an important problem existing approach include keyword based search wrapper based information extraction web query and user preference these approach essentially find information that match the user s explicit specification this paper argues that this is insufficient there is another type of information that is also of great interest i e unexpected information which is unanticipated by the user finding unexpected information is useful in many application for example it is useful for a company to find unexpected information bout it competitor e g unexpected service and product that it competitor offer with this information the company can learn from it competitor and or design counter measure to improve it competitiveness since the number of page of a typical commercial site is very large and there are also many relevant site competitor it is very difficult for a human user to view each page to discover the unexpected information automated assistance is needed in this paper we propose a number of method to help the user find various type of unexpected information from his her competitor web site experiment result show that these technique are very useful in practice and also efficient 
we present a new approach to partial parsingof natural language text that relies onmachine learning method the approachcombines corpus based grammar inductionwith a very simple pattern matching algorithmand an optional constituent verificationstep the grammar induction algorithm acquiresa set of rule for each level of linguisticanalysis using a new technique for errordrivenpruning of treebank grammar theconstituent verification step employ standardinductive 
image retrieval algorithm are generally based on the assumption that visually similar image are located close to each other in the feature space since the feature vector usually exist in a very high dimensional space a parametric characterization of their distribution is impossible so non parametric approach like the k nearest neighbor search are used for retrieval this paper introduces a graph theoretic approach for image retrieval by formulating the database search a a graph clustering problem by using a constraint that retrieved image should be consistent with each other close in the feature space a well a being individually similar close to the query image the experiment that compare retrieval precision with and without clustering showed an average precision of after clustering which is an improvement by over the average precision before clustering 
we tackle the problem of d surface reconstruction by a single static camera extracting the maximum amount of information from gray level change caused by object motion under illumination by a fixed set of light source we basically search for the depth at each point on the surface of the object while exploiting the recently proposed geotensity constraint that accurately governs the relationship between four or more image of a moving object in spite of the illumination variance due to object motion the thrust of this paper is then to extend the availability of the geotensity constraint to the case of multiple point light source instead of a single light source we first show that it is mathematically possible to identify multiple illumination subspace for an arbitrary unknown number of light source we then propose a new technique to effectively carry out the separation of the subspace by introducing the surface interaction matrix finally we construct a framework for surface recovery taking the multiple illumination subspace into account the theoretical proposition are investigated through experiment and shown to be practically useful 
for an effective teacher student interaction the teacher ha to maintain a constant understanding of what is going on in the student s mind when coming to physic the teacher s ability to propose and to relate explanation at different level of abstraction a a chain of causal interaction deep or a a set of observable phenomenon shallow may determine a successful and lasting learning in the student here we describe a knowledge representation to be used by the teacher to depict to herself the student s mental model and to tune her future lesson according to the current student comprehension supported by a cognitive theory of child physic learning we used the system why for modeling the evolution of a student s learning a it appeared at the teacher s eye two of why s feature turned out to be essential a to deal with explanation having different level of abstraction and b the possibility to continuously evaluate the coherence of the hypothesized learner s model with respect to her explanation in the long term the work s outcome might contribute to the development of teaching assistant system that support the teacher in identifying what ha to be explained next 
statistical learning and probabilistic inference technique are used to infer the hand position of a subject from multi electrode recording of neural activity in motor cortex first an array of electrode provides training data of neural firing conditioned on hand kinematics we learn a nonparametric representation of this firing activity using a bayesian model and rigorously compare it with previous model using cross validation second we infer a posterior probability distribution over hand motion conditioned on a sequence of neural test data using bayesian inference the learned firing model of multiple cell are used to define a nongaussian likelihood term which is combined with a prior probability for the kinematics a particle filtering method is used to represent update and propagate the posterior distribution over time the approach is compared with traditional linear filtering method the result suggest that it may be appropriate for neural prosthetic application 
the smart vision chip ha a large potential for application in general purpose high speed image processing system in order to fabricate smart vision chip including photo detector compactly we have proposed the application of three dimensional lsi technology for smart vision chip three dimensional technology ha great potential to realize new neuromorphic system inspired by not only the biological function but also the biological structure in this paper we describe our three dimensional lsi technology for neuromorphic circuit and the design of smart vision chip 
we seek a knowledge free method for inducing multiword unit from text corpus for use a machine readable dictionary headword we provide two major evaluation of nine existing collocation finder and illustrate the continuing need for improvement we use latent semantic analysis to make modest gain in performance but we show the significant challenge encountered in trying this approach 
deciding whether a propositional formula in conjunctive normal form is satisfiable sat is an npcomplete problem the problem becomes linear when the formula contains binary clause only interestingly the reduction to sat of a number of well known and important problem such a classical ai planning and automatic test pattern generation for circuit yield formula containing many binary clause in this paper we introduce and experiment with simplify a formula simplifier targeted at such problem simplify construct the implication graph corresponding to the binary clause in the formula and us this graph to deduce new unit literal the deduced literal are used to simplify the formula and update the graph and so on until stabilization finally we use the graph to construct an equivalent simpler set of binary clause experimental evaluation of this simplifier on a number of bench mark formula produced by encoding ai planning problem prove simplify to be fast and effective 
a central problem in case based reasoning cbr is how to store and retrieve case one approach to this problem is to use exemplar based model where only the prototypical case are stored however the development of an exemplar based model ebm requires the solution of several problem i how can a ebm be represented ii given a new case how can a suitable exemplar be retrieved iii what make a good exemplar iv how can an ebm be learned incrementally this paper develops a new model called a probabilistic exemplar based model that address these question the model utilizes bayesian network to develop a suitable representation and us probabilistic propagation for assessing and retrieving exemplar when a new case is presented the model learns incrementally by revising the exemplar retained and by updating the conditional probability required by the bayesian network the paper also present the result of evaluating the model on three datasets 
charge coupled device ccd camera are widely used imaging sensor in computer vision system many photometric algorithm such a shape from shading color constancy and photometric stereo implicitly assume that the image intensity is proportional to scene radiance the actual image measurement deviate significantly from this assumption since the transformation from scene radiance to image intensity is non linear and is a function of various factor including noise source in the ccd sensor a well a various transformation occurring in the camera including white balancing gamma correction and automatic gain control this paper illustrates how careful modelling of the error source and the various processing step enable u to accurately estimate the response function the inverse mapping from image measurement to scene radiance for a given camera exposure setting it is shown that the estimation algorithm outperforms the calibration procedure known to u in term of reduced bias and variance further we demonstrate how the error modelling help u to obtain uncertainty estimate of the camera irradiance value the power of this uncertainty modeling is illustrated by a vision task involving high dynamic range image generation followed by change detection change can be detected reliably even in situation where the two image the reference scene image and the current image are taken several hour apart 
anchoring is the process of creating and maintaining the correspondence between symbol and percept that refer to the same physical object although this process must necessarily be present in any symbolic reasoning system embedded in a physical environment e g an autonomous robot the systematic study of anchoring a a clearly separated problem is just in it initial phase in this paper we focus on the use of symbol in action and plan and the consequence this ha for anchoring in particular we introduce action property and partial matching of object description we also consider the use of indefinite reference in the context of action the use of our formalism is exemplified in a mobile robotic domain 
this paper present an electronic system that extract the periodicity of a sound it us three analogue vlsi building block a silicon cochlea two inner hair cell circuit and two spiking neuron chip the silicon cochlea consists of a cascade of filter because of the delay between two output from the silicon cochlea spike train created at these output are synchronous only for a narrow range of periodicity in contrast to traditional band pas filter where an increase in selectivity ha to be traded off against decrease in response time the proposed system responds quickly independent of selectivity 
we present an algorithm for extracting and classifying two dimensional motion in an image sequence based on motion trajectory first a multiscale segmentation is performed to generate homogeneous region in each frame region between consecutive frame are then matched to obtain view correspondence affine transformation are computed from each pair of corresponding region to define pixel match pixel match over consecutive image pair are concatenated to obtain pixel level motion trajectory across the image sequence motion pattern are learned from the extracted trajectory using a time delay neural network we apply the proposed method to recognize hand gesture of american sign language experimental result show that motion pattern in hand gesture can be extracted and recognized with high recognition rate using motion trajectory 
we address the problem of non convergence of online reinforcement learning algorithm e g q learning and sarsa by adopting an incremental batch approach that separate the exploration process from the function fitting process our bfbp batch fit to best path algorithm alternate between an exploration phase during which trajectory are generated to try to find fragment of the optimal policy and a function fitting phase during which a function approximator is fit to the best known 
we develop a framework for formalizing semantic construction within grammar expressed in typed feature structure logic including hpsg the approach provides an alternative to the lambda calculus it maintains much of the desirable flexibility of unification based approach to composition while constraining the allowable operation in order to capture basic generalization and improve maintainability 
our tdt tracking system wa inuenced by prior work on detection we found that the dual thresholded clustering paradigm of detection system worked well for topic tracking after minimal modification furthermore we found that many of the feature that were beneficial for topic detection continued to be helpful for tracking our basic document document scoring formula wa a symmetrized version of the okapi formula document cluster were represented by the centroid of the cluster and 
a simplified color image formation model is used to construct an algorithm for image reconstruction from ccd sensor sample the proposed method involves two successive step the first is motivated by cok s template matching technique while the second step us steerable inverse diffusion in color classical linear signal processing technique tend to oversmooth the image and result in noticeable color artifact along edge and sharp feature the question is how should the different color channel support each other to form the best possible reconstruction our answer is to let the edge support the color information and the color channel support the edge and thereby achieve better perceptual result than those that are bounded by the sampling theoretical limit 
to find out how the representation of structured visual object depend on the co occurrence statistic of their part we exposed subject to a set of composite image with controlled conditional probability of the constituent fragment we then compared the part verification response time for various probe target combination before and after the exposure with composite probe the drop in verification rt following exposure wa much larger for target that contained pair of fragment perfectly predictive of each other compared to those that did not for lonefragment probe this difference wa reversed this pattern of result is consistent with the principle according to which object should be treated a whole unless their part are observed sufficiently frequently in more than one configuration 
this paper introduces a new approach to morpho syntactic analysis through humor high speed unification morphology a reversible and unification based morphological analyzer which ha already been integrated with a variety of industrial application humor successfully cope with problem of agglutinative e g hungarian turkish estonian and other highly inflectional language e g polish czech german very effectively the author conclude the paper by arguing that the approach used in humor is general enough to be well suitable for a wide range of language and can serve a basis for higher level linguistic operation such a shallow parsing 
variational approximation are becoming a widespread tool forbayesian learning of graphical model we provide some theoreticalresults for the variational update in a very general family ofconjugate exponential graphical model we show how the beliefpropagation and the junction tree algorithm can be used in theinference step of variational bayesian learning applying these resultsto the bayesian analysis of linear gaussian state space modelswe obtain a learning procedure that 
there are fundamental limitation on using mental attitude to formalise the semantics of an agent communication language acl instead we define a general semantic framework for an acl in term of protocol we then argue that the proper role of mental attitude is to link what an agent think about the content of a message to what it doe in response to receiving that message we formalise this connection through normative and informative specification and demonstrate it use in communication between two bdi style agent 
the main challenge in articulated body motion tracking is the large number of degree of freedom around to be recovered search algorithm either deterministic or stochastic that search such a space without constraint fall foul of exponential computational complexity one approach is to introduce constraint either labelling using marker or colour coding prior assumption about motion trajectory or view restriction another is to relax constraint arising from articulation and track limb a if their motion were independent in contrast here we aim for general tracking without special preparation of subject or restrictive assumption the principal contribution of this paper is the development of a modified particle filter for search in high dimensional configuration space it us a continuation principle based on annealing to introduce the influence of narrow peak in the fitness function gradually the new algorithm termed annealed particle filtering is shown to be capable of recovering full articulated body motion efficiently 
accurate well calibrated estimate of class membership probability are needed in many supervised learning application in particular when a cost sensitive decision must be made about example with example dependent cost this paper present simple but successful method for obtaining calibrated probability estimate from decision tree and naive bayesian classifier using the large and challenging kdd contest dataset a a testbed we report the result of a detailed experimental comparison of ten method according to four evaluation measure we conclude that binning succeeds in significantly improving naive bayesian probability estimate while for improving decision tree probability estimate we recommend smoothing by estimation and a new variant of pruning that we call curtailment 
a simple and effective object recognition scheme is torepresent and match image on the basis of color histogram to obtain robustness against varying imaging circumstance e g a change in illumination object pose andviewpoint color histogram are constructed from color invariant however in general color invariant are negativelyaffected by sensor noise due to the instability of thesecolor invariant transforms at many rgb value to suppressthe effect of noise blow up for 
this paper describes and evaluates the confidence based dual reinforcement q routing algorithm cdrq routing for adaptive packet routing in communication network cdrq routing is based on the q learning framework of q routing the main contribution of this work is the increased quantity and improved quality of exploration in cdrq routing which lead to faster adaptation and better routing policy learned a compared to q routing the state of the art adaptive bellman ford routing and the non adaptive shortest path routing experiment over several network topology have shown that at different load cdrq routing learns superior policy significantly faster than q routing moreover cdrq routing learns policy that sustain higher load level than q routing analysis show that overhead due to exploration is insignificant a eqmpared to the improvement in cdrq routing 
this paper present a novel algorithm for fast nearest neighbor search at the preprocessing stage the proposed algorithm construct a lower bound tree by agglomeratively clustering the sample point in the database calculation of the distance between the query and the sample point can be avoided if the lower bound of the distance is already larger than the minimum distance the search process can thus be accelerated because the computational cost of the lower bound which can be calculated by using the internal node of the lower bound tree is le than that of the distance to reduce the number of the lower bound actually calculated the winner update search strategy is used for traversing the tree moreover the query and the sample point can be transformed for further efficiency improvement our experiment show that the proposed algorithm can greatly speed up the nearest neighbor search process when applying to the real database used in nayar s object recognition system the proposed algorithm is about one thousand time faster than the exhaustive search 
omnidirectional video camera are becoming increasingly popular in computer vision one family of these camera us a catadioptric system with a paraboloidal mirror and an orthographic lens to produce an omnidirectional image with a single center ofprojection in this paper we develop a novel calibration model that we combine with a beacon based pose estimation algorithm our approach relaxes the assumption of an ideal paraboloidal catadioptric system and achieves an order of magnitude improvement in pose estimation accuracy compared to calibration with an ideal camera model our complete standalone system placed on a radio controlled motorized cart move in a room size environment capturing high resolution frame to disk and recovering camera pose with an average error of in a region foot in diameter 
stochastic unification based grammar subgs define exponential distribution over the par generated by a unification based grammar ubg existing algorithm for parsing and estimation require the enumeration of all of the par of a string in order to determine the most likely one or in order to calculate the statistic needed to estimate a grammar from a training corpus this paper describes a graph based dynamic programming algorithm for calculating these statistic from the packed ubg parse representation of maxwell and kaplan which doe not require enumerating all par like many graphical algorithm the dynamic programming algorithm s complexity is worst case exponential but is often polynomial the key observation is that by using maxwell and kaplan packed representation the required statistic can be rewritten a either the max or the sum of a product of function this is exactly the kind of problem which can be solved by dynamic programming over graphical model 
this paper present an unsupervised method for choosing the correct translation of a word in context it learns disambiguation information from nonparallel bilingum corpus preferably in the same domain free from tagging our method combine two existing unsupervised disambiguation algorithm a word sense disambiguation algorithm based on distributional clustering and a translation disambiguation algorithm using target language corpus for the given word in context the former algorithm identifies it meaning a one of a number of predefined usage class derived by clustering a large amount of usage in the source language corpus the latter algorithm is responsible for associating each usage class i e cluster with a target word that is most relevant to the usage this paper also show preliminary result of translation experiment 
this paper address the issue of multipleinstanceinduction of rule in the presence ofnoise it first proposes a multiple instanceextensions of rule based learning algorithm then it show what kind of noise can appear inmultiple instance data and how to handle ittheoretically finally it describes theimplementation of such a noise tolerant multipleinstance learner and show it performance onseveral problem including the well knownmutagenesis prediction task 
we introduce a method for document classification based on using the chi square test to identify characteristic vocabulary of document class 
in this paper we use various method for multiple neural network combination in task of prepositional phrase attachment experiment with aggregation function such a unweighted and weighted average owa operator choquet integral and stacked generalization demonstrate that combining multiple network improve the estimation of each individual neural network using the ratnaparkhi data set the complete training set and the complete test set we obtained an accuracy score of in spite of the high cost in computational time of neural net training the response time in test mode is faster than others method 
this paper present a new technique for the perception and recognition of activity using statistical description of their spatio temporal property a set of motion energy receptive field is designed in order to sample the power spectrum of a moving texture their structure relates to the spatio temporal energy model of adelson and bergen where measure of local visual motion information are extracted by comparing the output of a triad of gabor energy filter then the probability density function required for bayes rule is estimated for each class of activity by computing multi dimensional histogram from the output from the set of receptive field the perception of activity is achieved according to bayes rule the result at each instant of time is the map of the conditional probability that each pixel belongs to each one of the activity of the training set since activity are perceived over a short integration time a temporal analysis of output is done using hidden markov model the approach is validated with experiment in the perception and recognition of activity of people walking in visual surveillance scenari the presented work is in progress and preliminary result are encouraging since recognition is robust to variation in illumination condition to partial occlusion and to change in texture it is shown that it constitute a powerful early vision tool for human behavior analysis for smart environnements 
the problem of action selection by autonomous agent becomes increasingly difficult when acting in continuous non deterministic and dynamic environment pursuing multiple and possibly conflicting goal we propose a method that exploit additional information gained from continuous state is able to deal with unexpected situation and take multiple and conflicting goal into account including additional motivational aspect such a dynamic goal which allow for situation dependent motivational influence on the agent further we show some domain independent property of this algorithm along with empirical result gained using the robocup simulated soccer environment 
eligibility trace have been shown to speed reinforcement learning to make it more robust to hidden state and to provide a link between monte carlo and temporal difference method here we generalize eligibility trace to off policy learning in which one learns about a policy different from the policy that generates the data off policy method can greatly multiply learning a many policy can be learned about from the same data stream and have been identified a particularly useful for learning about subgoals and temporally extended macro action in this paper we consider the off policy version of the policy evaluation problem for which only one eligibility trace algorithm is known a monte carlo method we analyze and compare this and four new eligibility trace algorithm emphasizing their relationship to the classical statistical technique known a importance sampling o ur main result are to establish the consistency and bias property of the new method and to empirically rank the new method showing improvement over one step and monte carlo method our result are restricted to model free table lookup method and to offline updating at the end of each episode although several of the algorithm could be applied more generally 
this paper ha no novel learning or statistic it is concerned with making a wide class of preexisting statistic and learning algorithm computationally tractable when faced with data set with massive number of record or attribute it briefly review the static ad tree structure of moore and lee and offer a new structure with more attractive property the new structure scale better with the number of attribute in the data set it ha zero initial build time it adaptively cache only statistic relevant to the current task and it can be used incrementally in case where new data is frequently being appended to the data set we provide a careful explanation of the data structure and then empirically evaluate the performance under varying access pattern induced by different learning algorithm such a association rule decision tree and bayes net structure we conclude by discussing the longer term benefit of the new structure the eventual ability to apply ad tree to data set with real valued attribute 
keeping track of multiple object over time is a problem that arises in many real world domain the problem is often complicated by noisy sensor and unpredictable dynamic previous work by huang and russell drawing on the data association literature provided a probabilistic analysis and a threshold based approximation algorithm for the case of multiple object detected by two spatially separated sensor this paper analysis the case in which large number of sensor are involved we show that the approach taken by huang and russell who used pairwise sensor based appearance probability a the elementary probabilistic model doe not scale when more than two observation are made the object intrinsic property must be estimated these provide the necessary conditional independency to allow a spatial decomposition of the global probability model we also replace huang and russell s threshold algorithm for object identification with a polynomial time approximation scheme based on markov chain monte carlo simulation using sensor data from a freeway traffic simulation we show that this allows accurate estimation of long range origin destination information even when the individual link in the sensor chain are highly unreliable 
this paper describes afastandrobust approach torecovering structure and motion from videoframes itfirst describes a robust recursive factorization methodfor affine projection using the least median ofsquares lmeds criterion the method estimate thedominant d affine motion and discard featurepointsregarded a outlier the computational cost of theoverall procedureisreducedbycombining this robuststatistics based method with a recursive factorizationmethod that can at each frame 
stochastic game are a generalization of mdps to multiple agent and can be used a a framework for investigating multiagent learning hu and wellman recently proposed a multiagent q learning method for general sum stochastic game in addition to describing the algorithm they provide a proof that the method will converge to a nash equilibrium for the game under specified condition the convergence depends on a lemma stating that the iteration used by this method is a contraction mapping unfortunately the proof is incomplete in this paper we present a counterexample and flaw to the lemma s proof we also introduce strengthened assumption under which the lemma hold and examine how this affect the class of game to which the theoretical result can be applied 
in this paper we address the problem of curve and surface reconstruction from set of point we introduce regular interpolants which are polygonal approximation of planar curve and surface verifying a local sampling criterion property of regular interpolants lead to new polygonal reconstruction method from set of organized and unorganized point these method do not need any parameter or additional information apart from the original point and allow unorganized set of point to be easily handled 
dlr is an expressive description logic dl with n ary relation particularly suited for modeling database schema although dlr ha constituted one of the crucial step for applying dl technology to data management there is one important aspect of database schema that dl including dlr do not capture yet namely the notion of identification constraint and functional dependency in this paper we introduce a dl which extends dlr and fully capture the semantics of such constraint and we address the problem of reasoning in such a logic we show that verifying knowledge base satisfiability and logical implication in the presence of identification constraint and nonunary functional dependency can be done in exptime thus with the same worst case computational complexity a for plain dlr we also show that adding just unary functional dependency to dlr lead to undecidability 
we present feature transformation useful for exploratory data analysis or for pattern recognition transformation are learned from example data set by maximizing the mutual information between transformed data and their class label we make use of renyi s quadratic entropy and we extend the work of principe et al to mutual information between continuous multidimensional variable and discrete valued class label 
this paper describes a decoding algorithm for a syntax based translation model yamada and knight the model ha been extended to incorporate phrasal translation a presented here in contrast to a conventional word to word statistical model a decoder for the syntax based model build up an english parse tree given a sentence in a foreign language a the model size becomes huge in a practical setting and the decoder considers multiple syntactic structure for each word alignment several pruning technique are necessary we tested our decoder in a chinese to english translation system and obtained better result than ibm model we also discus issue concerning the relation between this decoder and a language model 
inferior temporal cortex it neuron have large receptive field when a single effective object stimulus is shown against a blank background but have much smaller receptive field when the object is placed in a natural scene thus translation invariant object recognition is reduced in natural scene and this may help object selection we describe a model which account for this by competition within an attractor in which the neuron are tuned to different object in the scene and the fovea ha a higher cortical magnification factor than the peripheral visual field furthermore we show that top down object bias can increase the receptive field size facilitating object search in complex visual scene and providing a model of object based attention the model lead to the prediction that introduction of a second object into a scene with blank background will reduce the receptive field size to value that depend on the closeness of the second object to the target stimulus we suggest that mechanism of this type enable the output of it to be primarily about one object so that the area that receive from it can select the object a a potential target for action 
we present a framework for learning feature for visual discrimination the learning system is exposed to a sequence of training image whenever it fails to recognize a visual context adequately new feature are sought that discriminate further between the true and false class feature consist of hierarchical combination of primitive feature local edge and texture characteristic that are sampled from example image the system continues to learn better feature even after all recognition error have been eliminated similarly to mechanism underlying human visual expertise whenever the probabilistic recognition algorithm return any posterior class probability greater than zero and le than one the system attempt to find new feature that improve discrimination between the class in question our experiment indicate that this procedure tends to improve classification accuracy on independent test image while reducing the number of feature used for recognition 
rendering photorealistic virtual object from their real image is one of the main research issue in mixed reality system we previously proposed the eigen texture method a new rendering method for generating virtual image of object from thier real image to deal with the problem posed by past work in image based method and modelbased method eigen texture method sample apperances of a real object under various illumination and viewing condition and compress them in the d coordinate system defined on the d model surface however we had a serious limitation in our system due to the alignment problem of the d model and color image in this paper we deal with this limitation by solving the alignment problem we do this by using the method orginally designed by viola this paper describes the method and report on how we implement it 
in presentation recording special effort is usually put into the automation of the production process that is in automatically creating high quality data file without much or any need for manual recording and post editing with the advent of such system and their usage in classroom teaching at conference etc there is an increasing need for technique and ability which enable user to search in those document and to localize some specific information in this paper we describe how we integrated information retrieval technique into the authoring on the fly aof system an approach for automatic presentation recording we have chosen the aof system for two reason on the one hand it is a well established way for presentation recording used by various university and institution on the other hand it is general enough to illustrate typical problem and challenge a developer is facing when designing a system for information retrieval from multimedia data stream which occur in the presentation recording scenario with the authoring on the fly aof approach multimedia presentation are given with an electronic whiteboard program which is used to present material such a slide and annotation a well a external application e g computer animation or mpeg movie during a presentation all data stream that is the audio the slide and whiteboard annotation the video of the lecturer a well a the command controlling the external application are automatically recorded hence the produced file is a multimedia file which consists of several data stream of various medium type aof offer a flexible and convenient way for the synchronized replay of these data stream see for a detailed description of the synchronization model used in the aof system the main feature separating aof from other approach for presentation recording are that the produced multimedia file offer high quality with example of recorded presentation can be found at http www viror de and at http ad informatik uni freiburg de mmgroup aof docdownload the aof software is freely available at http ad informatik uni freiburg de mmgroup tool a reasonable amount of data and that navigation in and access to the recorded data is supported in a convenient way the latter feature is a result of the used synchronization model which realizes random access into the recorded file during replay i e any data stream independent of it medium type can be accessed at any position in real time two other feature which can be realized because of the random accessibility are random visible scrolling and unrestricted cross referencing both are important for information retrieval and will be introduced in section for a detailed description of aof and a comparison with other approach for presentation recording we refer to 
typically the lexicon model used in statistical machine translation system do not include any kind of linguistic or contextual information which often lead to problem in performing a correct word sense disambiguation one way to deal with this problem within the statistical framework is to use maximum entropy method in this paper we present how to use this type of information within a statistical machine translation system we show that it is possible to significantly decrease training and test corpus perplexity of the translation model in addition we perform a rescoring of n best list using our maximum entropy model and thereby yield an improvement in translation quality experimental result are presented on the so called verbmobil task 
we propose a flexible new technique to easily calibrate a camera it only requires the camera to observe a planar pattern shown at a few at least two different orientation either the camera or the planar pattern can be freely moved the motion need not be known radial lens distortion is modeled the proposed procedure consists of a closed form solution followed by a nonlinear refinement based on the maximum likelihood criterion both computer simulation and real data have been used to test the proposed technique and very good result have been obtained compared with classical technique which use expensive equipment such a two or three orthogonal plane the proposed technique is easy to use and flexible it advance d computer vision one step from laboratory environment to real world use the corresponding software is available from the author s web page 
the error in variable eiv model from statistic is often employedin computervision thoughonly rarely under this name in an eiv model all the measurement are corrupted by noise while the a priori information is captured with a nonlinear constraint among the true unknown value of these measurement to estimate the model parameter and the uncorrupted data the constraint can be linearized i e embedded in a higher dimensional space we show that linearization introducesdata dependent heteroscedastic noise and propose an iterative procedure the heteroscedastic eiv heiv estimator to obtain consistent estimate in the most general multivariate case analytical expression for the covariance of the parameter estimate and corrected data point a generic method for the enforcement of ancillary constraint arising from the underlyinggeometryare also given the heivestimatorminimizes the firstorder approximation of the geometric distance between the measurement and the true data point and thus can be a substitute for the widely used levenbergmarquardt based direct solution of the original nonlinear problem the heiv estimator ha however the advantage of a weaker dependenceon the initial solution and a faster convergence in comparison to kanatani s renormalization paradigm an earlier solution of the same problem the heiv estimator ha more solid theoretical foundation which translate into better numerical behavior we show that the heiv estimator can provide an accurate solution to most d vision estimation task and illustrate it performancethrough two case study calibration and the estimation of the fundamentalmatrix 
the problem of similarity search query by content ha attracted much research interest it is a difficult problem because of the inherently high dimensionality of the data the most promising solution involve performing dimensionality reduction on the data then indexing the reduced data with a multidimensional index structure many dimensionality reduction technique have been proposed including singular value decomposition svd the discrete fourier transform dft the discrete wavelet transform dwt and piecewise polynomial approximation in this work we introduce a novel framework for using ensemble of two or more representation for more efficient indexing the basic idea is that instead of committing to a single representation for an entire dataset different representation are chosen for indexing different part of the database the representation are chosen based upon a local view of the database for example section of the data that can achieve a high fidelity representation with wavelet are indexed a wavelet but highly spectral section of the data are indexed using the fourier transform at query time it is necessary to search several small heterogeneous index rather than one large homogeneous index a we will theoretically and empirically demonstrate this result in much faster query response time 
we present a new algorithm for polynomial time learning of near optimal behavior in stochastic game this algorithm incorporates and integrates important recent result of kearns and singh in reinforcement learning and of monderer and tennenholtz in repeated game in stochas tic game we face an exploration v exploitation dilemma more complex than in markov decision process namely given information about partic ular part of a game matrix how much effort should the agent invest in learning it unknown part we explain and address these issue within the class of single controller stochastic game this solution can be extended to stochastic game in general 
we show that the recently proposed variant of the support vectormachine svm algorithm known a svm can be interpretedas a maximal separation between subset of the convex hull of thedata which we call soft convex hull the soft convex hull arecontrolled by choice of the parameter if the intersection of theconvex hull is empty the hyperplane is positioned halfway betweenthem such that the distance between convex hull measured alongthe normal is maximized and if it is not 
projection matrix from projective space to have long been used in multiple view geometry to model the perspective projection created by the pin hole camera in this work we introduce higher dimensional mapping for the representation of various application in which the world we view is no longer rigid we also describe the multi view constraint from these new projection matrix and method for extracting the nonrigid structure and motion for each application 
this paper present a methodology for assessing the degree of diagnosability of a system i e given a set of sensor which fault can be discriminated and characterising and determining the minimal additional sensor which guarantee a specified degree of diagnosability this method ha been applied to several subsystem of a ge neral electric frame gas turbine owned by a major uk utility 
we present a method for learning a set of visual landmark which are useful for pose estimation the landmark learning mechanism is designed to be applicable to a wide range of environment and generalized for different approach to computing a pose estimate initially each landmark is detected a a local extreme of a measure of distinctiveness and represented by a principal component encoding which is exploited for matching attribute of the observed landmark can be parameterized using a generic parameterization method and then evaluated in term of their utility for pose estimation we present experimental evidence that demonstrates the utility of the method 
a major problem for kernel based predictor such a support vector machine and gaussian process is that the amount of computation required to find the solution scale a o n where n is the number of training example we show that an approximation to the eigendecomposition of the gram matrix can be computed by the nystr m method which is used for the numerical solution of eigenproblems this is achieved by carrying out an eigendecomposition on a smaller system of size m lt n and then 
we describe a new framework for dependency grammar with a modular decomposition of immediate dependency and linear precedence our approach distinguishes two orthogonal yet mutually constraining structure a syntactic dependency tree and a topological dependency tree the syntax tree is nonprojective and even non ordered while the topological tree is projective and partially ordered 
i highlight some inefficiency of graphplan s backward search algorithm and describe how these can be eliminated by adding explanation based learning and dependency directed backtracking capability to graphplan i will then demonstrate the effectiveness of these augmentation by describing result of empirical study that show dramatic improvement in run time w speedup a well a solvability horizon on benchmark problem across seven different domain 
neural activity appears to be a crucial component for shaping the receptive field of cortical simple cell into adjacent oriented subregions alternately receiving onand off center excitatory geniculate input it is known that the orientation selective response of v neuron are refined by visual experience after eye opening the spatiotemporal structure of neural activity in the early stage of the visual pathway depends both on the visual environment and on how the environment is scanned we have used computational modeling to investigate how eye movement might affect the refinement of the orientation tuning of simple cell in the presence of a hebbian scheme of synaptic plasticity level of correlation between the activity of simulated cell were examined while natural scene were scanned so a to model sequence of saccade and fixational eye movement such a microsaccades tremor and ocular drift the specific pattern of activity required for a quantitatively accurate development of simple cell receptive field with segregated on and off subregions were observed during fixational eye movement but not in the presence of saccade or with static presentation of natural visual input these result suggest an important role for the eye movement occurring during visual fixation in the refinement of orientation selectivity 
in this paper we present an exercise in compiler source level verification actually the source and target language and the compiler have already been used in our article for the acl case study book goe a where we prove that source level correctness is not at all sufficient to prove compiler executables correct however the proof is interesting for itself and the fact that the compiler used in goe a is indeed proved correct is essential for the message of that article so we want 
we propose the following general method for scaling learning algorithm to arbitrarily large data set consider the model m n learned by the algorithm using n i example in step i n n nm and the model m that would be learned using infinite example upper bound the loss l m n m between them a a function of n and then minimize the algorithm s time complexity f n subject to the constraint that l m m n be at most with probability at most we apply this 
this paper describes a correlation based iterative multi resolution algorithm which estimate both scene structure and the motion of the camera rig through an environment from the stream s of incoming image both single camera rig and multiple camera rig can be accommodated the use of multiple synchronized camera result in more rapid convergence of the iterative approach the algorithm us a global ego motion constraint to refine estimate of inter frame camera rotation and translation it us local window based correlation to refine the current estimate of scene structure all analysis is performed at multiple resolution in order to combine in a straightforward way the correlation surface from multiple viewpoint and from multiple pixel in a support region each pixel s correlation surface is modeled a a quadratic this parameterization allows direct explicit computation of incremental refinement for ego motion and structure using linear algebra batch can be of arbitrary size allowing a trade off between accuracy and latency batch can also be daisychained for extended sequence result of the algorithm are shown on synthetic and real outdoor image sequence 
a two tier model for the description of morphological syntactic and semantic variation of multi word term is presented it is applied to term normalization of french and english corpus in the medical and agricultural domain five differenct source of morphological and semantic knowledge are exploited multext celex agrovoc wordnet and microsoft word thesaurus 
this paper address recent progress in speaker independent large vocabulary continuous speech recognition which ha opened up a wide range of near and mid term application one rapidly expanding application area is the processing of broadcast audio for information access at limsi broadcast news transcription system have been developed for english french german mandarin and portuguese and system for other language are under development audio indexation must take into account the specificity of audio data such a needing to deal with the continuous data stream and an imperfect word transcription some near term application area are audio data mining selective dissemination of information and medium monitoring 
abstract cached statistic are a mean of extendingthe reach of traditional statistical machinelearning algorithm into application areaswhere computational complexity is a limitingfactor recent work ha shown that cachedstatistics greatly reduce the computationalrequirements of building a mixture model of adistribution using expectation maximizationfor a small trade off in model error this paperdescribes a method whereby a mixturemodel built using cached statistic is usedas a 
the way group of auditory neuron interact to code acoustic information is investigated using an information theoretic approach identifying the case of stimulus conditioned independent neuron we develop redundancy measure that allow enhanced information estimation for group of neuron these measure are then applied to study the collaborative coding eciency in two processing station in the auditory pathway the inferior colliculus ic and the primary auditory cortex a under two dierent coding paradigm we show dierences in both information content and group redundancy between ic and cortical auditory neuron these result provide for the rst time a direct evidence for redundancy reduction along the ascending auditory pathway a ha been hypothesized by barlow the redundancy eects under the single spike coding paradigm are signicant only for group larger than ten cell and cannot be revealed with the standard redundancy measure that use only pair of cell our result suggest that redundancy reduction transformation are not limited to low level sensory processing aimed to reduce redundancy in input statistic but are applied even at cortical sensory station 
in this paper we present a new method of estimating the novelty of rule discovered by data mining method using wordnet a lexical knowledge base of english word we ass the novelty of a rule by the average semantic distance in a knowledge hierarchy between the word in the antecedent and the consequent of the rule the more the average distance more is the novelty of the rule the novelty of rule extracted by the discotex text mining system on amazon com book description were evaluated by both human subject and by our algorithm by computing correlation coefficient between pair of human rating and between human and automatic rating we found that the automatic scoring of rule based on our novelty measure correlate with human judgment about a well a human judgment correlate with one another text mining 
we model hippocampal place cell and head direction cell by combining allothetic visual and idiothetic proprioceptive stimulus visual input provided by a video camera on a miniature robot is preprocessed by a set of gabor filter on node of a log polar retinotopic graph unsupervised hebbian learning is employed to incrementally build a population of localized overlapping place field place cell serve a basis function for reinforcement learning experimental result for goal oriented navigation of a mobile robot are presented 
the definition of the basic concept rule and constraint of centering theory involve underspecified notion such a previous utterance realization and ranking we attempted to find the best way of defining each such notion among those that can be annotated reliably and using a corpus of text in two domain of practical interest our main result is that trying to reduce the number of utterance without a backward looking center cb result in an increased number of case in which some discourse entity but not the cb get pronominalized and viceversa 
a www grows at an increasing speed a classifier targeted at hypertext ha become in high demand while document categorization is quite a mature the issue of utilizing hypertext structure and hyperlink ha been relatively unexplored in this paper we propose a practical method for enhancing both the speed and the quality of hypertext categorization using hyperlink in comparison against a recently proposed technique that appears to be the only one of the kind we obtained up to of improvement in effectiveness while reducing the processing time dramatically we attempt to explain through experiment what factor contribute to the improvement 
given a set of m observation on n variable an o mn algorithm is proposed to find a basis of all affine relation between these variable satisfied by the observation on a variable example this new algorithm is time faster than the all subset option for linear regression of the sa package which is a non polynomial alternative extension to the case where square ratio product of pair of variable or logarithm of such term appear in the relation is straightforward and remains polynomial the method is first tested with data for several classical discovery studied previously by the bacon program then it is added to the autographix system for computer aided graph theory thus making it entirely automated to demonstrate the power of the resulting system five novel relation or conjecture in graph theory are found two of which pertain to mathematical chemistry three conjecture involve five invariant which is more than in most proposition of graph theory proof of two conjecture are also given 
suppose you are given some dataset drawn from an underlying probability distribution and you want to estimate a subset of input space such that the probability that a test point drawn from lie outside of is bounded by some a priori specified we propose an algorithm which approach this problem by trying to estimate a function which is positive on and negative on the complement the functional form of is given by a kernel expansion in term of a potentially small subset of the training data it is regu larized by controlling the length of the weight vector in an associated fea ture space the algorithm is a natural extension of the support vector al gorithm to the case of unlabelled data 
principal component analysis and fisher linear discriminant method have demonstrated their success in face detection recognition and tracking the representation in these subspace method is based on second order statistic of the image set and doe not address higher order statistical dependency such a the relationship among three or more pixel recently higher order statistic and independent component analysis ica have been used a informative low dimensional representation for 
native and non native use of language differs depending on the proficiency of the speaker in clear and quantifiable way it ha been shown that customizing the acoustic and language model of a natural language understanding system can significantly improve handling of non native input in order to make such a switch however the nativeness status of the user must be known in this paper we show that naive bayes classification can be used to identify non native utterance of english the advantage of our method is that it relies on text not on acoustic feature and can be used when the acoustic source is not available we demonstrate that both read and spontaneous utterance can be classified with high accuracy and that classification of errorful speech recognizer hypothesis is accurate than classification of perfect transcription we also characterize part of speech sequence that play a role in detecting non native speech 
this paper deal with first the uniqueness of the selfcalibrationof a rotating and zooming camera theoretically we assume that the principal point and the aspect ratio arefixed but the focal length change a the camera move inthis case at least one inter image homography is requiredin order to compute the internal calibration parameter aswell a the rotation secondly we analyze the effect of thedeviation of the principal point on the estimation of the focallength and the 
a new form of covariance modelling for gaussian mixture model and hidden markov model is presented this is an extension to an efficient form of covariance modelling used in speech recognition semi tied covariance matrix in the standard form of semi tied covari ance matrix the covariance matrix is decomposed into a highly shared decorrelating transform and a component specific diagonal covariance mat rix the use of a factored decorrelating transform is presented in this p aper this factoring effectively increase the number of possible transf orms without increasing the number of free parameter maximum likelihood estimation scheme for all the model parameter are presented includin g the component transform assignment transform and component parameter this new model form is evaluated on a large vocabulary speech recognition task it is shown that using this factored form of covariance modelling reduces the word error rate 
latent semantic analysis lsa is a statistical corpus based text comparison mechanism that wa originally developed for the task of information retrieval but in recent year ha produced remarkably human like ability in a variety of language task lsa ha taken the test of english a a foreign language and performed a well a non native english speaker who were successful college applicant it ha shown an ability to learn word at a rate similar to human it ha even graded paper a reliably a human grader we have used lsa a a mechanism for evaluating the quality of student response in an intelligent tutoring system and it performance equal that of human raters with intermediate domain knowledge it ha been claimed that lsa s text comparison ability stem primarily from it use of a statistical technique called singular value decomposition svd which compress a large amount of term and document co occurrence information into a smaller space this compression is said to capture the semantic information that is latent in the corpus itself we test this claim by comparing lsa to a version of lsa without svd a well a a simple keyword matching model 
a number of important scientific and engineering application such a fluid dynamic simulation and aircraft design require analysis of spatially distributed data from expensive experiment and complex simulation in such data scarce application it is advantageous to use model of given sparse data to identify promising region for additional data collection this paper present a principled mechanism for applying domain specific knowledge to design focused sampling strategy in particular our approach us ambiguity identified in a multi level qualitative analysis of sparse data to guide iterative data collection two case study demonstrate that this approach lead to highly effective sampling decision that are also explainable in term of problem structure and domain knowledge 
transaction data can arrive at a ferocious rate in the order that transaction are completed the data contain an enormous amount of information about customer not just transaction but extracting up to date customer information from an ever changing stream of data and mining it in real time is a challenge this paper describes a statistically principled approach to designing short accurate summary or signature of high dimensional customer behavior that can be kept current with a stream of transaction a signature database can then be used for data mining and to provide approximate answer to many kind of query about current customer quickly and accurately a an empirical study of the calling pattern of wireless customer who made about million wireless call over a three month period show 
we introduce a new non parametric and principled distance basedclustering method this method combine a pairwise based approachwith a vector quantization method which provide a meaningfulinterpretation to the resulting cluster the idea is basedon turning the distance matrix into a markov process and thenexamine the decay of mutual information during the relaxation ofthis process the cluster emerge a quasi stable structure duringthis relaxation and then are extracted 
in a question answering system the question canbe classied based on their degree of diculty asthe level of diculty increase question answeringsystems need to rely on richer semantic ontologiesand larger knowledge base this paper is concernedwith question whose answer are spread across severaldocuments and thus require answer fusion tond such answer the system need to develop domainspecic ontology a method is presented foron line acquisition of ontological 
we focus on the problem of finding pattern across two large multidimensional datasets for example given feature vector of healthy and of non healthy patient we want to answer the following question are the two cloud of point separable what is the smallest largest pair wise distance across the two datasets which of the two cloud doe a new point feature vector come from we propose a new tool the tri plot and it generalization the pq plot which help u answer the above question we provide a set of rule on how to interpret a tri plot and we apply these rule on synthetic and real datasets we also show how to use our tool for classification when traditional method nearest neighbor classification tree may fail 
this paper focus on the problem of calibration from a single view and a map of a scene this situation arises quite often when modelling urban scene e g for augmented reality purpose we show how some scene constraint can be used to achieve a calibration like procedure an example excerpted from a sequence of picture for which selfcalibration like technique consistently fail illustrat e some of the benefit of the approach 
abstract in this paper we show how a simple feedforward neural network can be trained to filter document when only positive information is available and that this method seems to be superior to more standard method such a tf idf retrieval based on an average vector a novel experimental finding that retrieval is enhanced substantially in this context by carrying out a certain kind of uniform transformation hadamard of the information prior to the training of the network 
lp is an algorithm for adaptive information extraction from web related text that induces symbolic rule by learning from a corpus tagged with sgml tag induction is performed by bottom up generalisation of example in a training corpus training is performed in two step initially a set of tagging rule is learned then additional rule are induced to correct mistake and imprecision in tagging shallow nlp is used to generalise rule beyond the flat word structure generalization allows a better coverage on unseen text a it limit data sparseness and overfitting in the training phase in experiment on publicly available corpus the algorithm outperforms any other algorithm presented in literature and tested on the same corpus experiment also show a significant gain in using nlp in term of effectiveness reduction of training time and training corpus size in this paper we present the machine learning algorithm for rule induction in particular we focus on the nlp based generalisation and the strategy for pruning both the search space and the final rule set 
accurate d surface model of dense urban areasare essential for a variety of application such a cartography urban planning and monitoring mobile communication etc since manual surface reconstructionis very costly and time consuming the development ofautomated algorithm is of great importance whilemost of existing algorithm focus on surface reconstructioneither in rural or sub urban area we presentan approach dealing with dense urban scene the approachutilizes different 
in this paper it is shown how to extract a hypothesis with small risk from the ensemble of hypothesis generated by an arbitrary on line learning algorithm run on an independent and identically distributed i i d sample of data using a simple large deviation argument we prove tight data dependent bound for the risk of this hypothesis in term of an easily computable statistic mn associated with the on line performance of the ensemble via sharp pointwise bound on mn we then obtain risk tail bound for kernel perceptron algorithm in term of the spectrum of the empirical kernel matrix these bound reveal that the linear hypothesis found via our approach achieve optimal tradeoff between hinge loss and margin size over the class of all linear function an issue that wa left open by previous result a distinctive feature of our approach is that the key tool for our analysis come from the model of prediction of individual sequence i e a model making no probabilistic assumption on the source generating the data in fact these tool turn out to be so powerful that we only need very elementary statistical fact to obtain our final risk bound 
we generalize the classical algorithm of valiant and haussler for learning conjunction and disjunction of boolean attribute to the problem of learning these function over arbitrary set of feature including feature that are constructed from the data the result is a general purpose learning machine suitable for practical learning task that we call the set covering machine we present a version of the set covering machine that us generalized ball for it set of data dependent feature and compare it performance to the famous support vector machine by extending a technique pioneered by littlestone and warmuth we bound it generalization error a function of the amount of data compression it achieves during training 
towards the goal of realizing a generic automatic human activity recognition system a new formalism is proposed activity are described by a chained hierarchical representation using three type of entity image feature mobile object property and scenario taking image feature of tracked moving region from an image sequence a i nput mobile object property are first computed by specific method while noise is suppressed by statistical method scenario are recognized from mobile object property based on bayesian analysis a sequential occurrence s everal scenario are recognized by an algorithm using a probabilistic finite state automaton a variant of structured hmm the demonstration of the optimality of these recognition method is discussed finally the validity and the effectiveness of our approach is demonstrated on both real world and perturbed data 
in order to build a statistical model of appearance we require a set of image each with a consistent set of landmark we address the problem of automatically placing a set of landmark to define the correspondence across an image set we can estimate correspondence between any pair of image by locating salient point on one and finding their corresponding position in the second however we wish to determine a globally consistent set of correspondence across all the image we present an iterative scheme in which these pair wise correspondence are used to determine a global correspondence across the entire set we show result on several training set and demonstrate that an appearance model trained on the correspondence can be of higher quality than one built from hand marked image 
we consider the problem of reliably choosing a near best strategy from a restricted class of strategy in a partially observable markov decision process pomdp we assume we are given the ability to simulate the pomdp and study what might be called the sample complexity that is the amount of data one must generate in the pomdp in order to choose a good strategy we prove upper bound on the sample complexity showing that even for infinitely large and arbitrarily complex pomdps the amount of data needed can be finite and depends only linearly on the complexity of the restricted strategy class and exponentially on the horizon time this latter dependence can be eased in a variety of way including the application of gradient and local search algorithm our measure of complexity generalizes the classical supervised learning notion of vc dimension to the setting of reinforcement learning and planning 
survey are an important part of marketing and customer relationship management and open answer i e answer to open question in particular may contain valuable information and provide an important basis for making business decision we have developed a text mining system that provides a new way for analyzing open answer in questionnaire data the product is able to perform the following two function a accurate extraction of characteristic for individual analysis target b accurate extraction of the relationship among characteristic of analysis target in this paper we describe the working of our text mining system it employ two statistical learning technique rule analysis and correspondence analysis for performing the two function our text mining system ha already been put into use by a number of large corporation in japan in the performance of text mining on various type of survey data including open answer about brand image open answer about company image complaint about product comment written on home page business report and help desk record in this it ha been found to be useful in forming a basis for effective business decision 
the prisoner s dilemma is a useful model for studying the balance between self interest and group interest in multi agent system although many stratergies have been developed that perform well most of these stratergies make strong assumption about the information available to the agent it is in this context that we describe a satisficing learning stratergy for the prisoner s dilemma and present evidance that stable outcome other than the nash equilibrium are possible in addition we offer emperical evidence that under typical circumstance mutual cooperation is the most likely outcome and identify condition under which two satisficing agent will learn to cooperate 
many mobile robot task can be most efficiently solved when a group of robot is utilized the type of organization and the level of coordination and communication within a team of robot affect the type of task that can be solved this paper examines the tradeoff of homogeneity versus heterogeneity in the control system by allowing a team of robot to coevolve their high level controller given different level of difficulty of the task our hypothesis is that simply increasing the difficulty of a task is not enough to induce a team of robot to create specialist the key factor is not difficulty per se but the number of skill set necessary to successfully solve the task a the number of skill needed increase the more beneficial and necessary heterogeneity becomes we demonstrate this in the task domain of herding where one or more robot must herd another robot into a confined space 
in this paper we propose some new idea for tracking multiple moving object by the propagation of curve we assume a static observer a well a the existence of a background reference frame the tracking is performed using an improved geodesic active contour model that incorporates boundary based and region based motion information this model is called a geodesic active region model initially a statistical analysis is performed which provides a measurement that distinguishes between the 
we present a new framework for reasoning about point interval and duration point interval duration network pidn the pidn adequately handle both qualitative and quantitaive temporal information we show that interval algebra point algebra tcsp pdn and apdn become special case of pidn the underlying algebraic structure of pidn is closed under composition and intersection determinig consistency of p i dn is np ilard however we identify some tractable subclass of pidn we show that path consistency is not sufficient to ensure global consistency of the tractable subclass of pidn we identify a subclass for which enforcing consistency suffices to ensure the global consistency and prove that this subclass is maximal for qualitative constraint our approach is based on the geometric interpretation of the domain of temporal object interestingly the classical helly s theorem of is used to prove the complexity for the tractable subclass 
although behavior knowledge space bk method doe not need any assumption in combining multiple expert it should build theoretically exponential storage space for storing and managing jointly observed k decision from k expert that is combining k expert need a k st order probability distribution however it is well known that the distribution becomes unmanageable in storing and estimating even for a small k in order to overcome such weakness it would be attractive to decompose the distribution into a number of component distribution and to approximate the distribution with a product of the component distribution one of such previous work is to apply a conditional independence assumption to the distribution another work is to approximate the distribution with a product of only first order tree dependency or second order distribution in this paper a dependency based framework is proposed to optimality approximate a probability distribution with a product set of dth order dependency where d k and to combine multiple expert based on the product set using the bayesian formalism this framework wa experimented and evaluated with a standardized cen pariml data base 
there ha been considerable interest recently in various approach to scaling up machine learning system to large and distributed data set we have been studying approach based upon the parallel application of multiple learning program at distributed site followed by a meta learning stage to combine the multiple model in a principled fashion in this paper we empirically determine the best data partitioning scheme for a selected data set to compose appropriatelysized subset and we evaluate and compare three different strategy voting stacking and stacking with correspondence analysis scann for combining classification model trained over these subset we seek to find way to efficiently scale up to large data set while maintaining or improving predictive performance measured by the error rate a cost model and the tp fp spread 
experiment are presented which measure the perplexity reduction derived from incorporating into the predictive model utilised in a standard tag n gram part of speech tagger contextual information from previous sentence of a document the tagset employed is the roughly tag atr general english tagset whose tag are both syntactic and semantic in nature the kind of extrasentential information provided to the tagger is semantic and consists in the occurrence or non occurrence within the past sentence of the document being tagged of word tagged with particular tag from the tagset and of boolean combination of such condition in some case these condition are combined with the requirement that the word being tagged belong to a particular set of word thought most likely to benefit from the extrasentential information they are being conjoined with the baseline model utilized is a maximum entropy based tag ngram tagging model embodying a standard tag n gram approach to tagging i e constraint for tag trigram bigram and and the word tag occurrence frequency of the specific word being tagged form the basis of prediction added into to this baseline tagging model is the extrasentential semantic information just indicated the performance of the tagging model with and without the added contextual knowledge is contrasted training from the word atr general english treebank and testing on the accompanying word test treebank result are that a significant reduction in testset perplexity is achieved via the added semantic extrasentential information of the richer model the model with both long range tag trigger and more complex linguistic constraint achieved a perplexity reduction of 
due to limited bandwidth storage and computational resource and to the dynamic nature of the web search engine cannot index every web page and even the covered portion of the web cannot be monitored continuously for change therefore it is essential to develop effective crawling strategy to prioritize the page to be indexed the issue is even more important for topic specific search engine where crawler must make additional decision based on the relevance of visited page however it is difficult to evaluate alternative crawling strategy because relevant set are unknown and the search space is changing we propose three different method to evaluate crawling strategy we apply the proposed metric to compare three topic driven crawling algorithm based on similarity ranking link analysis and adaptive agent 
algebraic geometry is essential to learning theory in hierarchical learning machine such a layered neural network and gaussian mixture the asymptotic normality doe not hold since fisher information matrix are singular in this paper the rigorous asymptotic form of the stochastic complexity is clarified based on resolution of singularity and two different problem are studied if the prior is positive then the stochastic complexity is far smaller than bic resulting in the smaller generalization error than regular statistical model even when the true distribution is not contained in the parametric model if jeffreys prior which is coordinate free and equal to zero at singularity is employed then the stochastic complexity ha the same form a bic it is useful for model selection but not for generalization 
hypertext pose new text classification research challenge a hyperlink content of linked document and meta data about related web site all provide richer source of information for hypertext classification that are not available in traditional text classification we investigate the use of such information for representing web site and the effectiveness of different classifier naive bayes nearest neighbor and sc foil in exploiting those representation we find that using word in web page alone often yield suboptimal performance of classifier compared to exploiting additional source of information beyond document content on the other hand we also observe that linked page can be more harmful than helpful when the linked neighborhood are highly noisy and that link have to be used in a careful manner more importantly our investigation suggests that meta data which is often available or can be acquired using information extraction technique can be extremely useful for improving classification accuracy finally the relative performance of the different classifier being tested give u insight into the strength and limitation of our algorithm for hypertext classification 
most machine learning algorithm share thefollowing drawback they only output barepredictions but not the confidence in thosepredictions in the s algorithmic informationtheory supplied universal measuresof confidence but these are unfortunately non computable in this paper we combinethe idea of algorithmic information theorywith the theory of support vector machinesto obtain practicable approximation to universalmeasures of confidence we show thatin some 
linear perspective projection ha served a the dominant imaging model in computer vision recent development in image sensing make the perspective model highly restrictive this paper present a general imaging model that can be used to represent an arbitrary imaging system it is observed that all imaging system perform a mapping from incoming scene ray to photo sensitive element on the image detector this mapping can be conveniently described using a set of virtual sensing element called raxels raxels include geometric radiometric and optical property we present a novel calibration method that us structured light pattern to extract the raxel parameter of an arbitrary imaging system experimental result for perspective a well a non perspective imaging system are included 
abstract we present a constancy rate principle governing language generation we show that this principle implies that local measure of entropy ignoring context should increase with the sentence number we demonstrate that this is indeed the case by measuring entropy in three dierent way we also show that this eect ha both lexical which word are used and non lexical how the word are used cause 
our goal is to automatically answer brief factual question of the form when wa the battle of hastings or who wrote the wind in the willow since the answer to nearly any such question can now be found somewhere on the web the problem reduces to finding potential answer in large volume of data and validating their accuracy we apply a method for arbitrary passage retrieval to the first half of the problem and demonstrate that answer redundancy can be used to address the second half the success of our approach depends on the idea that the volume of available web data is large enough to supply the answer to most factual question multiple time and in multiple context a query is generated from a question and this query is used to select short passage that may contain the answer from a large collection of web data these passage are analyzed to identify candidate answer the frequency of these candidate within the passage is used to vote for the most likely answer the approach is experimentally tested on question taken from the trec question answering test collection a an additional demonstration the approach is extended to answer multiple choice trivia question of the form typically asked in trivia quiz and television game show 
the emergence of xml a a standard interchange format for structured document data ha given rise to many xml query language proposal however some of these language do not support information retrieval style ranked query based on textual similarity there have been several extension to these query language to support keyword search but the resulting query language cannot express query such a find book and cd with similar title either these extension use keywords a mere boolean filter or similarity can be calculated only between data value and constant rather than two data value we propose elixir an textbf underline e xpressive and textbf underline e fficient textbf underline l anguage for textbf underline x ml textbf underline i nformation textbf underline r etrieval that extends the query language xml ql cite deutsch www deutsch deb with a textual similarity operator elixir is a general purpose xml information retrieval language sufficiently expressive to handle the above query our algorithm for answering elixir query rewrite the original elixir query into a series of xml ql query that generate intermediate relational data and us relational database technique to efficiently evaluate the similarity operator on this intermediate data yielding an xml document with node ranked by similarity our experiment demonstrate that our prototype scale well with the size of the xml data and complexity of the query 
in this paper we describe a new strategy for combining orientation adaptive filtering and edge preserving filtering the filter adapts to the local orientation and avoids filtering across border the local orientation for steering the filter will be estimated in a fixed sized window which never contains two orientation field this can be achieved using generalized kuwahara filtering this filter selects from a set of fixed sized window that contain the current pixel the orientation of the window with the highest anisotropy we compare our filter strategy with a multi scale approach we found that our filter strategy ha a lower complexity and yield a constant improvement of the snr 
lyapunov analysis is a standard approach to studying the stability of dynamical system and to designing controller we propose to design the action of a reinforcement learning rl agent to be descending on a lyapunov function for minimum cost to target problem this ha the theoretical benefit of guaranteeing that the agent will reach a goal state on every trial regardless of the rl algorithm it us in practice lyapunov descent constraint can significantly shorten learning trial improve initial and worst case performance and accelerate learning although this method of constraining action may limit the extent to which an rl agent can minimize cost it allows one to construct robust rl system for problem in which lyapunov domain knowledge is available this includes many important individual problem a well a general class of problem such a the control of feedback linearizable system e g industrial robot and continuous state path planning problem we demonstrate the general approach on two simulated control problem pendulum swing up and robot arm control 
this paper present a method of evaluating unsupervised texture segmentation algorithm the control scheme of texture segmentation ha been conceptualized a two modular process l feature computation and segmentation of homogeneous region based on the feature value three feature extraction method are considered gray level co occurrence matrax law texture energy and gabor multi channel filtering three segmentation algorithm are considered fuzzy c mean clustering square error clustering and split and merge a set of real scene image with manually specified ground truth wa compiled performance is measured against ground truth on real image using region based and pixel based performance metric 
stereo correspondence is a central issue in computer vision the traditional approach involves extracting image feature establishing correspondence based on photometric and geometric criterion and finally determine a dense disparity field by interpolation in this context occlusion are considered a undesirable artifact and often ignored the challenging problem addressed in this paper are a finding an image representation that facilitates or even trivializes the matching procedure and b detecting and including occlusion point in such representation we propose a new image representation called intrinsic image that can be used to solve correspondence problem within a natural and intuitive framework intrinsic image combine photometric and geometric descriptor of a stereo image pair we extend this framework to deal with occlusion and brightness change between two view we show that this new representation greatly simplifies the computation of dense disparity map and the synthesis of novel view of a given scene obtained directly from this image representation result are shown to illustrate the performance of the proposed methodology under perspective effect and in the presence of occlusion 
child learn to ride a bicycle by using training wheel they are actually trying to learn one task riding without training wheel by training another one in general solving a difficult problem can be facilitated by training other problem this is the basic idea of shaping it is essential to ensure that spending time on the modified task will help solving the original one in this paper we prove that given a finite mdp with a limited reward signal and we are guaranteed that if a series of task converge to the original one then the optimal value function converges to the original one a well 
a pictorial structure is a collection of part arranged in a deformable configuration each part is represented using a simple appearance model and the deformable configuration is represented by spring like connection between pair of part while pictorial structure were introduced a number of year ago they have not been broadly applied to matching and recognition problem this ha been due in part to the computational difficulty of matching pictorial structure to image in this paper 
the multi stream dependency detection algorithm find rule that capture statistical dependency between pattern in multivariate time series of categorical data oates and cohen c rule strength is measured by the g statistic wickens and an upper bound on the value of g for the descendant of a node allows msdd s search space to be pruned however in the worst case the algorithm will explore exponentially many rule this paper present and empirically evaluates two way of addressing this problem the first is a set of three method for reducing the size of msdd s search space based on information collected during the search process second we discus an implementation of msdd that distributes it computation over multiple machine on a network 
we consider the existence of efficient algorithm for learning theclass of half space in nin the agnostic learning model i e makingno prior assumption on the example generating distribution the resulting combinatorial problem finding the best agreementhalf space over an input sample is np hard to approximate towithin some constant factor we suggest a way to circumvent thistheoretical bound by introducing a new measure of success for suchalgorithms an algorithm is 
an algorithm is described for modelling and recognising temporal structure of visual activity the method is based on learning prior probabilistic knowledge using hidden markov model automatic temporal clustering of hidden markov state based on expectation maximisation and using observation augmented conditional density distribution to reduce the number of sample required for propagation and therefore improve recognition speed and robustness 
robust real time tracking of object from visual data requires probabilistic fusion of multiple visual cue previous approach have either been ad hoc or relied on a bayesian network with discrete spatial variable which suffers from discretisation and computational complexity problem we present a new bayesian modalityfusion network that us continuous domain variable the network architecture distinguishes between cue that are necessary or unnecessary for the object s presence computationally expensive and inexpensive modality are also handled differently to minimise cost the method provides a formal tractable and robust probabilistic method for simultaneously tracking multiple object while instantaneous inference is exact approximation is required for propagation over time 
previous work ha shown that automatic method can be used in building semantic lexicon this work go a step further by automatically creating not just cluster of related word but a hierarchy of noun and their hypernym akin to the hand built hierarchy in wordnet 
the blackbox planning system unifies the planning a satisfiability framework kautz and selman with the plan graph approach to strip planning blum and furst we show that strip problem can be directly translated into sat and efficiently solved using new randomized systematic solver for certain computationally challenging benchmark problem this unified approach outperforms both satplan and graphplan alone we also demonstrate that polynomialtime sat simplification algorithm applied to the encoded problem instance are a powerful complement to the mutex propagation algorithm that work directly on the plan graph 
in this paper i discus issue pertinent to the design of a task based evaluation methodology for a spoken machine translation mt system processing human to human communication rather than human to machine communication i claim that system mediated human to human communication requires new evaluation criterion and metric based on goal complexity and the speaker s prioritization of goal 
developing dialogue system is a complex process in particular designing efficient dialogue management strategy is often difficult a there are no precise guideline to develop them and no sure test to validate them several suggestion have been made recently to use reinforcement learning to search for the optimal management strategy for specific dialogue situation these approach have produced interesting result including application involving real world dialogue system however reinforcement learning suffers from the fact that it is state based in other word the optimal strategy is expressed a a decision table specifying which action to take in each specific state it is therefore difficult to see whether there is any generality across state this limit the analysis of the optimal strategy and it potential for re use in other dialogue situation in this paper we tackle this problem by learning rule that generalize the state based strategy these rule are more readable than the underlying strategy and therefore easier to explain and re use we also investigate the capability of these rule in directing the search for the optimal strategy by looking for generalization whilst the search proceeds 
most research in text classification to date ha used a bag of word representation in which each feature corresponds to a single word the paper examines some alternative way to represent text based on syntactic and semantic relationship between word phrase synonym and hypernym we describe the new representation and try to justify our hypothesis that they could improve the performance of a rule based learner the representation are evaluated using the ripper learning algorithm on the reuters and digitrad test corpus on their own the new representation are not found to produce significant performance improvement we also try combining classifier based on different representation using a majority voting technique and this improves performance on both test collection in our opinion more sophisticated natural language processing technique need to be developed before better text representation can be produced for classification 
while the generative view of language processing build bigger unit out of smaller one by mean of rewriting step the axiomatic view eliminates invalid linguistic structure out of a set of possible structure by mean of well formedness principle we present a generator based on the axiomatic view and argue that when combined with a tag like grammar and a flat semantics this axiomatic view permit avoiding drawback known to hold either of top down or of bottom up generator 
a system ha been developed to extract diagnostic information from jet engine carcass vibration data support vector machine applied to novelty detection provide a measure of how unusual the shape of a vibration signature is by learning a representation of normality we descri be a novel method for support vector machine of including information from a second class for novelty detection and give result from the application to jet engine vibration analysis 
this study investigates a population decoding paradigm in which the estimation of stimulus in the previous step is used a prior knowledge for consecutive decoding we analyze the decoding accuracy of such a bayesian decoder maximum a posteriori estimate and show that it can be implemented by a biologically plausible recurrent network where the prior knowledge of stimulus is conveyed by the change in recurrent interaction a a result of hebbian learning 
we have developed a system that generates evaluative argument that are tailored to the user properly arranged and concise we have also developed an evaluation framework in which the effectiveness of evaluative argument can be measured with real user this paper present the result of a formal experiment we have performed in our framework to verify the influence of argument conciseness on argument effectiveness 
this paper describes new technique for self calibration and for recovering the motion from a projective reconstruction when the calibration is known we show that our approach deal with the ambiguity in self calibration produced by special motion we extend our technique to deal with varying calibration parameter in passing we prove convergence for the iterative projective reconstruction algorithm of sturm triggs and berthilsson heyden sparr 
this paper examines the use of an unsupervised statistical model for determining the attachment of ambiguous coordinate phrase cp of the form n p n cc n the model presented here is based on ar an unsupervised model for determining prepositional phrase attachment after training on unannotated wall street journal text the model performs at accuracy on a development set from section through of the wsj treebank msm 
revi miner is a kdd environment which support the detection and analysis of deviation in warranty and goodwill cost statement the system wa developed within the framework of a cooperation between daimlerchrysler research technology and global service and part gsp and is based upon the crisp dm methodology a a widely accepted process model for the solution of data mining problem also we have implemented different approach based on machine learning and statistic which can be utilized for data cleaning in the preprocessing phase the data mining model applied have been developed by using a statistical deviation detection approach the tool support controller in their task of auditing the authorized repair shop in this paper we describe the development phase which have led to revi miner 
in this paper we investigate determine and classify the critical configuration for solving structure and motion problem for d retina vision we give a complete categorization of all ambiguous configuration for a d perspective camera irrespective of the number of point and view both calibrated and uncalibrated camera are considered several example and illustration are provided to explain the result and to provide geometrical insight 
this paper provides a characterization of biasfor evaluation metric in classification e g information gain gini etc our characterizationprovides a uniform representationfor all traditional evaluation metric such representation lead naturally to a measurefor the distance between the bias oftwo evaluation metric we give a practicalvalue to our measure by observing ifthe distance between the bias of two evaluationmetrics correlate with difference in 
neuron in area v have relatively large receptive field rf so multiple visual feature are simultaneously seen by these cell recording from single v neuron suggest that simultaneously presented stimulus compete to set the output firing rate and that attention act to isolate individual feature by biasing the competition in favor of the attended object we propose that both stimulus competition and attentional biasing arise from the spatial segregation of afferent synapsis onto different region of the excitable dendritic tree of v neuron the pattern of feedforward stimulus driven input follows from a hebbian rule excitatory afferent with similar rf tend to group together on the dendritic tree avoiding randomly located inhibitory input with similar rf the same principle guide the formation of input that mediate attentional modulation using both biophysically detailed compartmental model and simplified model of computation in single neuron we demonstrate that such an architecture could account for the response property and attentional modulation of v neuron our result suggest an important role for nonlinear dendritic conductance in extrastriate cortical processing 
perceptual experiment indicate that corner and curvature are very important feature in the process of recognition this paper present a new method to efficiently detect rotational symmetry which describe complex curvature such a corner circle starand spiral pattern the method is designed to give selective and sparse response it work in three step first extract local orientation from a gray scale or color image second correlate the orientation image with rotational symmetry filter and third let the filter response inhibit each other in order to get more selective response the correlation can be made efficient by separating the d filter into a small number of d filter these symmetry can serve a feature point at a high abstraction level for use in hierarchical matching structure for d estimation object recognition etc 
this paper present a bottom up generator that make use of information retrieval technique to rank potential generation candidate by comparing them to a data base of stored instance we introduce two general technique to address the search problem expectation driven search and dynamic grammar rule selection and present the architecture of an implemented generation system called igen our approach us a domain specific generation grammar that is automatically derived from a semantically tagged treebank we then evaluate the efficiency of our system 
a common method for texture representation is to use the marginal probability density over the output of a set of multi orientation multi scale filter a a description of the texture we propose a technique based on independent component analysis for choosing the set of filter that yield the most informative marginals meaning that the product over the marginals most closely approximates the joint probability density function of the filter output the algorithm is implemented using a steerable filter space experiment involving both texture classification and synthesis show that compared to principal component analysis ica provides superior performance for modeling of natural and synthetic texture 
in order to generate high quality explanation in technical or mathematical domain the presentation must be adapted to the knowledge of the intended audience current proof presentation system only communicate proof on a fixed degree of abstraction independently of the addressee s knowledge in this paper we propose an architecture for an interactive proof explanation system called prex based on the theory of human cognition act r it dialog planner exploit a cognitive model in which both the user s knowledge and his cognitive process are modeled by this mean his cognitive state are traced during the explanation the explicit representation of the user s cognitive state in act r allows the dialog planner to choose a degree of abstraction tailored to the user for each proof step to be explained moreover the system can revise it assumption about the user s knowledge and react to his interaction 
this paper explores a framework for recognition of image sequence using partially observable stochastic differential equation sde model monte carlo importance sampling technique are used for efficient estimation of sequence likelihood and sequence likelihood gradient once the network dynamic are learned we apply the sde model to sequence recognition task in a manner similar to the way hidden markov model hmms are commonly applied 
the task of information filtering is to classifydocuments from a stream into either relevantor irrelevant according to a particular user interestwith the objective to reduce informationload when using an information filter in anenvironment that is changing a time proceeds method for adapting the filter should be consideredin order to retain the desired accuracyin classification we favor a methodology thatattempts to detect change and adapts the informationfilter only if 
the fundamental matrix defines a nonlinear d variety in the joint image space of multiple projective or uncalibrated perspective image we show that in the case of two image this variety is a d cone whose vertex is the joint epipole namely the d point obtained by stacking the two epipoles in the two image affine or para perspective projection approximates this nonlinear variety with a linear subspace both in two view and in multiple view we also show that the tangent to the projective joint image at any point on that image is obtained by using local affine projection approximation around the corresponding d point we use these observation to develop a new approach for recovering multiview geometry by integrating multiple local affine joint image into the global projective joint image given multiple projective image the tangent to the projective joint image are computed using local affine approximation for multiple image patch the affine parameter from different patch are combined to obtain the epipolar geometry of pair of projective image we describe two algorithm for this purpose including one that directly recovers the image epipoles without recovering the fundamental matrix a an intermediate step 
information extraction and text classication are usually seen a complementary form of shallow text processing in that they are aimed at very dierent task in this paper we describe two simple but real world domain in which text classication technique can be used directly for information extraction specically we describe system for extracting information from business card and for automatically processing change of address email message that are based primarily on text classication technique our main technical contribution is a novel integration of hidden markov model and text classiers 
human computer intelligent interaction hcii is an emerging field of science aimed at providing natural way for human to use computer a aid it is argued that for the computer to be able to interact with human it need to have the communication skill of human one of these skill is the ability to understand the emotional state of the person the most expressive way human display emotion is through facial expression this work focus on automatic facial expression recognition from live video input using temporal cue method for using temporal information have been extensively explored for speech recognition application among these method are template matching using dynamic programming method and hidden markov model hmm this work exploit existing method and proposes a new architecture of hmms for automatically segmenting and recognizing human facial expression from video sequence the novelty of this architecture is that both segmentation and recognition of the facial expression are done automatically using a multilevel hmm architecture while increasing the discrimination power between the different class in the work we explore person dependent and person independentrecognition of expression 
we give a graph theoretical characterization of answer set of normal logic program we show that there is a one to one correspondence between answer set and a special non standard graph coloring of so called block graph of logic program this lead u to an alternative implementation paradigm to compute answer set by computing non standard graph coloring our approach is rule based and not atom based like most of the currently known method we present an implementation for computing answer set which work on polynomial space 
function approximation is essential to reinforcement learning butthe standard approach of approximating a value function and determininga policy from it ha so far proven theoretically intractable in this paper we explore an alternative approach in which the policyis explicitly represented by it own function approximator independentof the value function and is updated according to the gradientof expected reward with respect to the policy parameter williams sreinforce method and 
we present linear relational embedding lre a new method of learning a distributed representation of concept from data consisting of instance of relation between given concept it final goal is to be able to generalize i e infer new instance of these relation among the concept on a task involving family relationship we show that lre can generalize better than any previously published method we then show how lre can be used effectively to find compact distributed representation for variable sized recursive data structure such a tree and list 
this paper is concerned with the problem of detecting outlier from unlabeled data in prior work we have developed smartsifter which is an on line outlier detection algorithm based on unsupervised learning from data on the basis of smartsifter this paper yield a new framework for outlier filtering using both supervised and unsupervised learning technique iteratively in order to make the detection process more effective and more understandable the outline of the framework is a follows in the first round for an initial dataset we run smartsifter to give each data a score with a high score indicating a high possibility of being an outlier next giving positive label to a number of higher scored data and negative label to a number of lower scored data we create labeled example then we construct an outlier filtering rule by supervised learning from them here the rule is generated based on the principle of minimizing extended stochastic complexity in the second round for a new dataset we filter the data using the constructed rule then among the filtered data we run smartsifter again to evaluate the data in order to update the filtering rule applying of our framework to the network intrusion detection we demonstrate that it can significantly improve the accuracy of smartsifter and outlier filtering rule can help the user to discover a general pattern of an outlier group 
this paper present an active learning method that directly optimizes expected future error this is in contrast to many other popular technique that instead aim to reduce version space size these method are popular because for many learning model closed form calculation of the expected future error is intractable our approach is made feasible by taking a monte carlo approach to estimating the expected reduction in error due to the labeling of a query in experimental result on three real world data set we reach high accuracy with four time fewer labelled example than competing method 
the problem of developing good policy for partially observable markov decision problem pomdps remains one of the most challenging area of research in stochastic planning one line of research in this area involves the use of reinforcement learning with belief state probability distribution over the underlying model state this is a promising method for small problem but it application is limited by the intractability of computing or representing a full belief state for large problem recent work show that in many setting we can maintain an approximate belief state which is fairly close to the true belief state in particular great success ha been shown with approximate belief state that marginalize out correlation between state variable in this paper we investigate two method of full belief state reinforcement learning and one novel method for reinforcement learning using factored approximate belief state we compare the performance of these algorithm on several well known problem from the literature our result demonstrate the importance of approximate belief state representation for large problem 
we describe a set of image measurement which areinvariant to the camera internals but are location variant we show that using these measurement it is possibleto calculate the self localization of a robot usingknown landmark and uncalibrated camera we alsoshow that it is possible to compute using uncalibratedcameras the euclidean structure of d world pointsusing multiple view from known position we are freeto alter the internal parameter of the camera duringthese 
we consider the problem of how an agent creates a discrete spatial representation from it continuous interaction with the environment such representation will be the minimal one that explains the experience of the agent in the environment in this 
while several image based rendering technique have been proposed to successfully render scene object from a large collection e g thousand of image without ex plicitly recovering d structure the minimum number of image needed to achieve a satisfactory rendering result re main an open problem this paper is the first attempt to in vestigate the lower bound for the number of sample needed in the lumigraph light field rendering to simplify the analysis we consider an ideal scene with only a point that is between a minimum and a maximum range furthermore constant depth assumption and bilin ear interpolation are used for rendering the constant depth assumption serf to choose nearby ray for interpola tion our criterion to determine the lower bound is to avoid horizontal and vertical double image which are caused by interpolation using multiple nearby ray this criterion is based on the causality requirement in scale space theory i e no spurious detail should be generated while smooth ing using this criterion closed form solution of lower bound are obtained for both d plenoptic function concen tric mosaic and d plenoptic function light field the bound are derived completely from the aspect of geometry and are closely related to the resolution of the camera and the depth range of the scene these lower bound are further verified by our experimental result 
we propose a new probabilistic approach to information retrieval based upon the idea and method of statistical machine translation the central ingredient in this approach is a statistical model of how a user might distill or quot translate quot a given document into a query to ass the relevance of a document to a user s query we estimate the probability that the query would have been generated a a translation of the document and factor in the user s general preference in the form of a prior 
this paper describes a program which revise a draft text by aggregating together description of discourse entity in addition to deleting extraneous information in contrast to knowledge rich sentence aggregation approach explored in the past this approach exploit statistical parsing and robust coreference detection in an evaluation involving revision of topic related summary using informativeness measure from the tipster summac evaluation the result show gain in informativeness without compromising readability 
we present a new image segmentation algorithm based on graph cut our main tool is separation of each pixel from a special point outside the image by a cut of a minimum cost such a cut creates a group of pixel around each pixel we show that these group are either disjoint or nested in each other and so they give a natural segmentation of the image in addition this property allows an efficient implementation of the algorithm because for most pixel the computation of is not performed on the whole graph we inspect all s and discard those which are not interesting for example if they are too small this procedure automatically group small component together or merges them into nearby large cluster effectively our segmentation is performed by extracting significant non intersecting closed contour we present interes ting segmentation result on real and artificial image 
an approach that allows a user to assist an automatic system in modeling building is described the approach is designed to be efficient in user time and effort while preserving the quality of the model created currently our system is able to handle the rectangular building with flat roof or symmetric gabled roof model can be created by only one or two click in many case efficient editing of automatically derived model is also possible introduction and overview generating d model from a set of image is a common task for computer vision in spite of substantial research in this area the performance of machine algorithm remains significantly below that of human in this paper we explore an approach to bridge this gap by allowing a human in the loop but by requiring only simple interaction from the user to generate accurate model efficiently we illustrate this approach for the task of building modeling from aerial image which is a difficult and important task significant progress ha been made in recent year in the goal of extracting model of building from aerial image by completely automatic system but the result are not completely accurate completely manual system require an unacceptable amount of effort from a human modeler both in term of time and cost we describe an approach that attempt to provide user assist to an automatic system in a way that the user effort is diminished significantly while the quality of the result is still preserved several approach to user assisted modeling are possible the conventional approach is to provide a set of generic model which are then fit to the image data by changing model and viewing parameter in this approach the system provides geometric computation but substantial time and effort are required from the user newer approach have attempted to combine user input with varying amount of automatic processing in the author suggest providing just an approximate building location to extract a building in other interactive tool are described including method for replicating to model building that are identical or very similar to others in an automatic system construct topological relation among d roof point collected by a user for each roof this system can work with several type of complex roof in our approach basic modeling task are still performed by an automatic system but this system receives simple but critical assist from a user the assisted system s capability are limited by those of the underlying system in our case the shape of the building are restricted to be rectilinear the roof may be either flat or symmetric gable the underlying automatic system is the multi view 
most ie system process text in sequential step or phase ranging from lexical and morpho logical processing recognition and typing of proper name parsing of larger syntactic constituent resolution of anaphora and coreference and the ultimate extraction of domain relevent event and relationship from the text we discus each of these system component and various approach to their design 
we develop a linear model of commonly observed joint color change in image due to variation in lighting and certain non geometric camera parameter this is done by observing how all of the color are mapped between two image of the same scene under various real world lighting change we represent each instance of such a joint color mapping a a d vector field in rgb color space we show that the variance in these map is well represented by a lowdimensional linear subspace of these vector field we dub the principal component of this space the color eigenflows when applied to a new image the map define an image subspace different for each new image of plausible variation of the image a seen under a wide variety of naturally observed lighting condition we examine the ability of the eigenflows and a base image to reconstruct a second image taken under different lighting condition showing our technique to be superior to other method setting a threshold on this reconstruction error give a simple system for scene recognition 
we show how a wavelet basis may be adapted to best representnatural image in term of sparse coefficient the wavelet basis which may be either complete or overcomplete is specified by asmall number of spatial function which are repeated across spaceand combined in a recursive fashion so a to be self similar acrossscale these function are adapted to minimize the estimated codelength under a model that assumes image are composed of a linearsuperposition of sparse 
for many learning task where data is collected over an extended period of time it underlying distribution is likely to change a typical example is information filtering i e the adaptive classification of document with respect to a particular user interest both the interest of the user and the document content change over time a filtering system should be able to adapt to such concept change this paper proposes a new method to recognize and handle concept change with support vector machine the method maintains a window on the training data the key idea is to automatically adjust the window size so that the estimated generalization error is minimized the new approach is both theoretically well founded a well a effective and efficient in practice since it doe not require complicated parameterization it is simpler to use and more robust than comparable heuristic experiment with simulated concept drift scenario based on real world text data compare the new method with other window management approach we show that it can effectively select an appropriate window size in a robust way 
application of dynamic programming to the deformablecontours ha many advantage such a guaranteed optimalityand numerical stability however long executiontimes of these method almost always force researcher touse dynamic programming in combination with multiresolutionmethods multiresolution method shorten the executiontime by subsampling the original image after anapplication of a smoothing filter however this speedupcomes at the expense of contour optimality due to the 
decision tree have been successfully used for the task of classification however state of the art algorithm do not incorporate the user in the tree construction process this paper present a new user centered approach to this process where the user and the computer can both contribute their strength the user provides domain knowledge and evaluates intermediate result of the algorithm the computer automatically creates pattern satisfying user constraint and generates appropriate visualization of these pattern in this cooperative approach domain knowledge of the user can direct the search of the algorithm additionally by providing adequate data and knowledge visualization the pattern recognition capability of the human can be used to increase the effectivity of decision tree construction furthermore the user get a deeper understanding of the decision tree than just obtaining it a a result of an algorithm to achieve the intended level of cooperation we introduce a new visualization of data with categorical and numerical attribute a novel technique for visualizing decision tree is presented which provides deep insight into the process of decision tree construction a a key contribution we integrate a state of the art algorithm for decision tree construction such that many different style of cooperation ranging from completely manual over combined to completely automatic classification are supported an experimental performance evaluation demonstrates that our cooperative approach yield an efficient construction of decision tree that have a small size but a high accuracy 
we consider a scene containing many object moving with constant velocity along straight line path seen from three reference viewpoint at three different time the scene mayeven consist only of moving object with no static feature we wish to create a new image sequence showing the scene from arbitrary viewing position and arbitrary time we make use of a newly discovered tool the dual htensor that connects together three view of a coplanar configuration of unlabeled static and moving point the newly synthesized image use constant velocity in the world to achive realistic and physically correct image 
in recent year increasing effort ha gone into evaluating computer vision algorithm in general and edge detection algorithm in particular most of the evaluation technique use only a few test image leaving open the question of how broadly their result can be interpreted our research test the consistency of the receiver operating characteristic roc curve and demonstrates why consistent edge detector evaluation is difficult to a chieve we show how easily the framework can be 
we present technique for rendering and animation of realistic scene by analyzing and training on short video sequence this work extends the new paradigm for computer animation video texture which us recorded video to generate novel animation by replaying the video sample in a new order here we concentrate on video sprite which are a special type of video texture in video sprite instead of storing whole image the object of interest is separated from the background and the video sample are stored a a sequence of alpha matted sprite with associated velocity information they can be rendered anywhere on the screen to create a novel animation of the object we present method to create such animation by finding a sequence of sprite sample that is both visually smooth and follows a desired path to estimate visual smoothness we train a linear classifier to estimate visual similarity between video sample if the motion path is known in advance we use beam search to find a good sample sequence we can specify the motion interactively by precomputing the sequence cost function using q learning 
for many problem which would be natural for reinforcement learning the reward signal is not a single scalar value but ha multiple scalar component example of such problem include agent with multiple goal and agent with multiple user creating a single reward value by combining the multiple component can throw away vital information and can lead to incorrect solution we describe the multiple reward source problem and discus the problem with applying traditional reinforcement learning we then present an new algorithm for finding a solution and result on simulated environment 
when image captured by a tilted camera are mosaiced into a panorama the resulting mosaic is curled this happens for example with a panning camera that is not perfectly horizontal and with a translating camera facing a tilted planar surface the tilt of the camera cause difference in image velocity between the top and bottom part of the image causing the curled mosaic in rectified mosaicing these distortion are overcome by warping the strip into rectangle while keeping some image feature invariant this warping equalizes the image motion at the different image part and the resulting mosaic is straight mosaicing is done without camera calibration or knowledge of the scene and the process adapts automatically to smooth change in the scene and the imaging condition figure a curled mosaic constructed by manifold mosaicing the panning camera wa slightly tilted upwards 
previous comparison of document and query translation suffered difficulty due to differing quality of machine translation in these two opposite direction we avoid this difficulty by training identical statistical translation model for both translation direction using the same training data we investigate information retrieval between english and french incorporating both translation direction into both document translation and query translation based information retrieval a well a into hybrid system we find that hybrid of document and query translation based system out perform query translation system even human quality query translation system 
wideband source recorded using closely spaced receiver can feasibly be separated based only on second order statistic when using a physical model of the mixing process in this case we show that the parameter estimation problem can be essentially reduced to considering direction of arrival and attenuation of each signal the paper present two demixing method operating in the time and frequency domain and experimentally show that it is always possible to demix signal arriving at different angle moreover one can use spatial cue to solve the channel selection problem and a post processing wiener filter to ameliorate the artifact caused by demixing 
virtual reality vr provides immersive and controllable experimentalenvironments it expands the bound of possible evoked potential ep experiment by providing complex dynamic environment in orderto study cognition without sacrificing environmental control vralso serf a a safe dynamic testbed for brain computer interface bci research however there ha been some concern about detecting ep signalsin a complex vr environment this paper show that eps exist atred green 
in this paper we focus on mining surprising periodic pattern in a sequence of event in many application e g computational biology an infrequent pattern is still considered very significant if it actual occurrence frequency exceeds the prior expectation by a large margin the traditional metric such a support is not necessarily the ideal model to measure this kind of surprising pattern because it treat all pattern equally in the sense that every occurrence carry the same weight towards the assessment of the significance of a pattern regardless of the probability of occurrence a more suitable measurement information is introduced to naturally value the degree of surprise of each occurrence of a pattern a a continuous and monotonically decreasing function of it probability of occurrence this would allow pattern with vastly different occurrence probability to be handled seamlessly a the accumulated degree of surprise of all repetition of a pattern the concept of information gain is proposed to measure the overall degree of surprise of the pattern within a data sequence the bounded information gain property is identified to tackle the predicament caused by the violation of the downward closure property by the information gain measure and in turn provides an efficient solution to this problem empirical test demonstrate the efficiency and the usefulness of the proposed model 
building predictive model and finding useful rule are two important task of data mining while building predictive model ha been well studied finding useful rule for action still present a major problem a main obstacle is that many data mining algorithm often produce too many rule existing research ha shown that most of the discovered rule are actually redundant or insignificant pruning technique have been developed to remove those spurious and or insignificant rule in this paper we argue that being a significant rule or a non redundant rule however doe not mean that it is a potentially useful rule for action many significant rule unpruned rule are in fact not actionable this paper study this issue and present an efficient algorithm to identify these non actionable rule experiment result on many real life datasets show that the number of non actionable rule is typically quite large the proposed technique thus enables the user to focus on fewer rule and to be assured that the remaining rule are non redundant and potentially useful for action 
information filtering system based on statistical retrieval model usually compute a numeric score indicating how well each document match each profile document with score above profile specificdissemination thresholdsare delivered an optimal dissemination threshold is one that maximizes a given utility function based on the distribution of the score of relevant and non relevant document the parameter of the distribution can be estimated using relevance information but relevance information obtained while filtering isbiased this paper present a new method of adjusting dissemination threshold that explicitly model and compensates for this bias the new algorithm which is based on the maximum likelihood principle jointly estimate the parameter of the density distribution for relevant and non relevant document and the ratio of the relevant document in the corpus experiment with trec and trec filtering track data demonstrate the effectiveness of the algorithm 
generating expression which communicate information already known to the hearer building enthymematic argument and characterising refutation all pose significant problem to traditional natural language generation technique after exploring these problem an approach is proposed which through it employment of a notion of saliency handle them cleanly and offer support for further feature including clue word generation it is argued that propositional salience and it interaction with intentional attentional epistemic and structural component of a text generation system have a key role to play in the design and realisation of persuasive text 
a novel approach to colour based object recognition and image retrieval the multimodal neighbourhood signatureis proposed object appearance is represented by colour based feature computed from image neighbourhood with multi modal colour density function stable invariant are derived from mode of the density function that are robustly located by the mean shift algorithm the problem of extracting local invariant colour feature is addressed directly without a need for prior segmentation or edge detection the signature is concise an image is typically represented by a few hundred byte a few thousand for very complex scene the algorithm s performance is first tested on a region based image retrieval task achieving a good hit rate at a speed of image comparison per second the method is shown to operate successfully under changing illumination viewpoint and object pose a well a non rigid object deformation partial occlusion and the presence of background clutter dominating the scene the performance of the multimodal neighbourhood signature method is also evaluated on a standard colour object recognition task using a publicly available dataset very good recognition performance average match percentile wa achieved in real time average second for recognising a single image which compare favourably with result reported in the literature 
this paper proposes and analyzes an efficient and effective approach for estimating the generalization performance of a support vector machine svm for text classification without any computation intensive resampling the new estimator are computationally much more efficient than cross validation or bootstrapping they can be computed at essentially no extra cost immediately after training a single svm moreover the estimator developed here address the special performance measure needed for evaluating text classifier they can be used not only to estimate the error rate but also to estimate recall precision and f a theoretical analysis and experiment show that the new method can effectively estimate the performance of svm text classifier in an efficient way 
a new algorithm for approximating intensity image with adaptive triangular mesh keeping image discontinuity and avoiding optimization is presented the algorithm consists of two main stage in the first stage the original image is adaptively sampled at a set of point taking into account both image discontinuity and curvature in the second stage the sampled point are triangulated by applying a constrained d delaunay algorithm the obtained triangular mesh are compact representation that model the region and discontinuity present in the original image with many fewer point thus image processing operation applied upon those mesh can perform faster than upon the original image a an example four simple operation translation rotation scaling and deformation have been implemented in the d geometric domain and compared to their image domain counterpart 
this paper present the dynamic scene analysis partof an original unified and efficient framework to videopartitioning camera motion estimation and multiplemotion analysis in the context of content based videoindexing all the information required to achieve thesegoals result from handling the apparent motion withinconsecutive image pair through robust estimation of d parametric motion model using the estimatedglobal dominant motion and it spatial support thevideo is 
pinhole camera model is a simplified subset of geometric optic in special case like the image formation of the cone a degenerate conic section mirror in an omnidirectional view catadioptric system there are more complex optical phenomenon involved that the simple pinhole model can not explain we show that using the full geometric optic model a true single viewpoint cone mirror omni directional system can be built we show how such system is built first and then show in detail how each optical phenomenon work together to make the system true single viewpoint the new system requires only simple off the shelf component and still outperforms other single viewpoint omni system for many application 
topic distillation is the analysis of hyperlink graph structure to identify mutually reinforcing authority popular page and hub comprehensive list of link to authority topic distillation is becoming common in web search engine but the best known algorithm model the web graph at a coarse grain with whole page a single node such model may lose vital detail in the markup tag structure of the page and thus lead to a tightly linked irrelevant subgraph winning over a relatively sparse relevant subgraph a phenomenon called topic drift or contamination the problem get especially severe in the face of increasingly complex page with navigation panel and advertisement link we present an enhanced topic distillation algorithm which analyzes text the markup tag tree that constitute html page and hyperlink between page it thereby identifies subtrees which have high textand hyperlink based coherence w r t the query these subtrees get preferential treatment in the mutual reinforcement process using over query from earlier topic distillation work we analyzed over page and obtained quantitative and anecdotal evidence that the new algorithm reduces topic drift 
this paper introduces the open loop intermittent feedback optimal predictive olifo controller a an alternative to human movement control modelsbased on system inverse control olifo ha the advantage of being applicableto any system not requiring a desired system trajectory and handling naturallysystems with time delay and constraint moreover it share important functionalcharacteristics with the human movement control system it behaviour isillustrated through the control of a 
the human figure exhibit complex and rich dynamic behavior that is both nonlinear and time varying however most work on tracking and analysis of figure motion ha employed either generic or highly specific hand tailored dynamic model superficially coupled with hidden markov model hmms of motion regime recently an alternative class of learned dynamic model known a switching linear dynamic system sldss ha been cast in the framework of dynamic bayesian network dbns and applied to analysis and tracking of the human figure in this paper we further study the impact of learned slds model on analysis and tracking of human motion and contrast them to the more common hmm model we develop a novel approximate structured variational inference algorithm for slds a globally convergent dbn inference scheme and compare it with standard slds inference technique experimental result on learning and analysis of figure dynamic from video data indicate the significant potential of the slds approach 
the paper present a series of noise detectionexperiments in a medical problem of coronaryartery disease diagnosis the following algorithmsfor noise detection and elimination aretested a saturation filter a classification filter a combined classification saturation filter and a consensus saturation filter thedistinguishing feature of the novel consensussaturation filter is it high reliability which isdue to the multiple detection of potentiallynoisy example reliable 
we analyze the problem of detecting a road target in background clutter and investigate the amount of prior i e target specific knowledge needed to perform this search task the problem is formulated in term of bayesian inference and we define a bayesian ensemble of problem instance this formulation implies that the performance measure of different model depend on order parameter which characterize the problem this demonstrates that if there is little clutter then only weak knowledge about the target is required in order to detect the target however at a critical value of the order parameter there is a phase transition and it becomes effectively impossible to detect the target unless high level target specific knowledge is used these phase transition determine different regime within which different search strategy will be effective these result have implication for bottom up and top down theory of vision 
ai research ha often been driven by popular vision hal asimov s robot star trek and by critical application area medical expert system spoken dialogue system etc these vision and application serve to inspire and guide researcher posing challenge illustrating technical weakness and generally channeling creative energy without doubt the widely held vision of the autonomous robot ha exerted a substantial integrative force such that numerous discipline ranging from mechanical engineering to cognitive science can see how their intellectual endeavor can contribute to the overall endeavor in this brief position paper and in the accompanying talk i would like to propose that the next generation of intelligent multimodal user interface can offer a similar intellectual focus for ai researcher after providing a brief overview of our work in this area and two example i would like to suggest the potential impact that such interface could have in the relatively near term 
this paper describes a machine learning approach for visualobject detection which is capable of processing image extremely rapidly and achieving high detection rate this work is distinguished by three key contribution the first is the introduction of a new image representation called the integral image which allows the feature used by our detector to be computed very quickly the second is a learning algorithm based on adaboost which selects a small number of critical visual feature from a larger set and yield extremely efficient classifier the third contribution is a method for combining increasingly more complex classifier in a cascade which allows background region of the image to be quickly discarded while spending more computation on promising object like region the cascade can be viewed a an object specific focus of attention mechanism which unlike previous approach provides statistical guarantee that discarded region are unlikely to contain the object of interest in the domain of face detection the system yield detection rate comparable to the best previous system used in real time application the detector run at frame per second without resorting to image differencing or skin color detection 
we present a shift reduce rhetorical parsing algorithm that learns to construct rhetorical structure of text from a corpus of discourse parse action sequence the algorithm exploit robust lexical syntactic and semantic knowledge source 
this paper describes improvement to the temporal difference td learning method the standard form of the td method ha the problem that two control parameter learning rate and temporal discount need to be chosen appropriately these parameter can have a major effect on performance particularly the learning rate parameter which affect the stability of the process a well a the number of observation required our extension to the td algorithm automatically set and subsequently adjusts these parameter the learning rate adjustment is based on a new concept we call temporal coherence tc the experiment reported here compare the extended td algorithm performance with human chosen parameter and with an earlier method for learning rate adjustment in a complex game domain the learning task wa that of learning the relative value of piece without any initial domain specific knowledge and from self play only the result show that the improved method lead to better learning i e faster and le subject to the effect of noise than the selection of human chosen value for the control parameter and a comparison method 
direct policy search is a practical way to solve reinforcement learning problem involving continuous state and action space the goal becomes finding policy parameter that maximize a noisy objective function the pegasus method convert this stochastic optimization problem into a deterministic one by using fixed start state and fixed random number sequence for comparing policy ng jordan we evaluate pegasus and other paired comparison method using the mountain car problem and a difficult pursuer evader problem we conclude that i paired test can improve performance of deterministic and stochastic optimization procedure ii our proposed alternative to pegasus can generalize better by using a different test statistic or changing the scenario during learning iii adapting the number of trial used for each policy comparison yield fast and robust learning 
a key challenge for reinforcement learning is scaling up to largepartially observable domain in this paper we show how a hierarchyof behavior can be used to create and select among variablelength short term memory appropriate for a task at higher levelsin the hierarchy the agent abstract over lower level detailsand look back over a variable number of high level decision intime we formalize this idea in a framework called hierarchicalsux memory hsm hsm us a 
we show how event extraction can be used for handling delayed response task with arbitrary delay period between the stimulus and the cue for response our approach is based on a number of information processing level where the lowest level work on raw time stepped based sensory data this data is classified using an unsupervised clustering mechanism the second level work on this classified data but still on the individual time step basis an event extraction mechanism detects and signal transition between class this form the basis for the third level a this level only is updated when event occur it is independent of the time scale of the lower level interaction we also sketch how an event filtering mechanism could be constructed which discard irrelevant data from the event stream such a mechanism would output a fourth level representation which could be used for delayed response task where irrelevant or distracting event could occur during the delay 
we present a novel information theoretic algorithm for unsupervised segmentation of sequence into alternating variable memory markov source the algorithm is based on competitive learning between markov model when implemented a prediction suffix tree ron et al using the mdl principle by applying a model clustering procedure based on rate distortion theory combined with deterministic annealing we obtain a hierarchical segmentation of sequence between alternating markov 
multiple camera are needed to cover large environment for monitoring activity to track people successfully in multiple perspective imagery one need to establish correspondence between object captured in multiple camera we present a system for tracking people in multiple uncalibrated camera the system is able to discover spatial relationship between the camera field of view and use this information to correspond between different perspective view of the same person we employ the novel approach of finding the limit of field of view fov of a camera a visible in the other camera using this information when a person is seen in one camera we are able to predict all the other camera in which this person will be visible moreover we apply the fov constraint to disambiguate between possible candidate of correspondence we present result on sequence of up to three camera with multiple people the proposed approach is very fast compared to camera calibration based approach 
many problem in vision can be formulated a bayesian inference it is important to determine the accuracy of these inference and how they depend on the problem domain in recent work coughlan and yuille showed that for a restricted class of problem the performance of bayesian inference could be summarized by an order parameter k which depends on the probability distribution which characterize the problem domain in this paper we generalize the theory of order parameter so that it applies to domain for which the probability model can be obtained by minimax entropy learning theory by analyzing order parameter it is possible to determine whether a target can be detected using a general purpose generic model or whether a more specific high level model is needed at critical value of the order parameter the problem becomes unsolvable without the addition of extra prior knowledge 
intelligent scissors is an interactive image segmentation tool that allows a user to select piece wise globally optimal contour segment that correspond to a desired object boundary we present a new and faster method of computing the optimal path by over segmenting the image using tobogganing and then imposing a weighted planar graph on top of the resulting region boundary the resulting region based graph is many time smaller than the pixel based graph used previously thus providing faster graph search and immediate user interaction further the region based graph provides an efficient framework to compute a parameter edge model allowing subpixel localization a well a a measure of edge blur 
abstract a specialized formulation of azarbayejani and pentland s framework for recursive recovery of motion structure and focal length from feature correspondence tracked through an image sequence is presented the specialized formulation address the case where all tracked point lie on a plane this planarity constraint reduces the dimension of the original state vector and consequently the number of feature point needed to estimate the state experiment with synthetic data and real imagery illustrate the system performance the experiment confirm that the specialized formulation provides improved accuracy stability to observation noise and rate of convergence in estimation for the case where the tracked point lie on a plane 
we introduce a d tracing method based on differential geometry in gaussian blurred image the line point detection part of the tracing method start with calculation of the line direction from the eigenvectors of the hessian matrix the sub voxel center line position is estimated from a second order taylor approximation of the d intensity profile perpendicular to the line in curved line structure the method turn out to be biased we model the bias in center line position using the first order taylor expansion of the gradient in scale and position based on this model we found that the bias in a torus with a generalized line profile wa proportional to this result wa applied in a procedure to remove the bias and to measure the radius of curvature in a curved line structure the line diameter is obtained using the theoretical scale dependency of the th and nd order gaussian derivative at the line center experiment on synthetic image reveal that the localization of the centerline is mainly affected by line curvature and is well predicted by our theoretical analysis the diameter measurement is accurate for diameter a low a voxels result in image from a confocal microscope show that the tracing method is able to trace in image highly corrupted with noise and clutter the diameter measurement procedure turn out to be accurate and largely independent of the scale of observation 
in this contribution we focus on the calibration of very long image sequence from a hand held camera that sample the viewing sphere of a scene view sphere sampling is important for plenoptic image based modeling that capture the appearance of a scene by storing image from all possible direction the plenoptic approach is appealing since it allows in principle fast scene rendering of scene with complex geometry and surface reflection without the need for an explicit geometrical scene model however the acquired image have to be calibrated and current approach mostly use pre calibrated acquisition system this limit the generality of the approach we propose a way out by using an uncalibrated handheld camera only the image sequence is acquired by simply waving the camera around the scene object creating a zigzag scan path over the viewing sphere we extend the sequential camera tracking of an existing structure frommotion approach to the calibration of a mesh of viewpoint novel view are generated by piecewise mapping and interpolating the new image from the nearest viewpoint according to the viewpoint mesh local depth map estimate enhance the rendering process extensive experiment with ground truth data and hand heldsequencesconfirm the performance of our approach 
recent research ha shown the promise of using propositional reasoning and search to solve ai planning problem in this paper we further explore this area by applying integer programming to solve ai planning problem the application of integer programming to ai planning ha a potentially significant advantage a it allows quite naturally for the incorporation of numerical constraint and objective into the planning domain moreover the application of integer programming to ai planning address one of the challenge in propositional reasoning posed by kautz and selman who conjectured that the principal technique used to solve integer program the linear programming lp relaxation is not useful when applied to propositional search we discus various ip formulation for the class of planning problem based on strip style planning operator our main objective is to show that a carefully chosen ip formulation significantly improves the strength of the lp relaxation and that the resultant lp are useful in solving the ip and the associated planning problem our result clearly show the importance of choosing the right representation and more generally the promise of using integer programming technique in the ai planning domain 
new face recognition approach are needed becausealthough much progress ha been recently achieved inthe field e g within the eigenspace domain still manyproblems are to be robustly solved two of these problemsare occlusion and the imprecise localization offaces which ultimately imply a failure in identification while little ha been done to account for thefirst problem almost nothing ha been proposed to accountfor the second this paper present a probabilisticapproach 
this paper describes the application of the paradise evaluation framework to the corpus of human computer dialogue collected in the june darpa communicator data collection we describe result based on the standard logfile metric a well a result based on additional qualitative metric derived using the date dialogue act tagging scheme we show that performance model derived via using the standard metric can account for of the variance in user satisfaction and that the addition of date metric improved the model by an absolute 
this paper explores the use of hierarchical structure for classifying a large heterogeneous collection of web content the hierarchical structure is initially used to train different second level classifier in the hierarchical case a model is learned to distinguish a second level category from other category within the same top level in the flat non hierarchical case a model distinguishes a second level category from all other second level category scoring rule can further take advantage of the hierarchy by considering only second level category that exceed a threshold at the top level we use support vector machine svm classifier which have been shown to be efficient and effective for classification but not previously explored in the context of hierarchical classification we found small advantage in accuracy for hierarchical model over flat model for the hierarchical approach we found the same accuracy using a sequential boolean decision rule and a multiplicative decision rule since the sequential approach is much more efficient requiring only of the comparison used in the other approach we find it to be a good choice for classifying text into large hierarchical structure 
we examine the statistic of natural monochromatic image decomposed using a multi scale wavelet basis although the coefficient of this rep resentation are nearly decorrelated they exhibit important higher order statistical dependency that cannot be eliminated with purely linear processing in particular rectified coefficient corresponding to basis function at neighboring spatial position orientation and scale are hig hly correlated a method of removing these dependency is to divide each coefficient by a weighted combination of it rectified neighbor several successful model of the steady state behavior of neuron in primary visual cortex are based on such divisive normalization computation and thus our analysis provides a theoretical justification for these model s perhaps more importantly the statistical measurement explicitly specif y the weight that should be used in computing the normalization signal we demonstrate that this weighting is qualitatively consistent with r ecent physiological experiment that characterize the suppressive effect of stimulus presented outside of the classical receptive field our observation thus provide evidence for the hypothesis that early visual neu ral processing is well matched to these statistical property of image an appealing hypothesis for neural processing state that sensory system develop in response to the statistical property of the signal to which they are e xposed e g this ha led many researcher to look for a mean of deriving a model of cortical processing purely from a statistical characterization of sensory signal in parti cular many such attempt are based on the notion that neural response should be statisti cally independent the pixel of digitized natural image are highly redundant but one can always find a linear decomposition i e principal component analysis that eliminates second order corresearch supported by an alfred p sloan fellowship to eps and by the sloan center for theoretical neurobiology at nyu 
we propose a technique to improve the performance of hierarchical model based diagnosis based on structural abstraction given a hierarchical representation and the set of currently available observation the technique is able to dynamically derive a tailored hierarchical representation to diagnose the current situation we implement our strategy a an extension to the well known mozetic s approach mozetic and illustrate the obtained performance improvement our approach is more efficient than mozetic s one when due to abstraction fewer observation are available at the coarsest hierarchical level 
in this paper we address the problem of global registration between multiple dimensional point pattern with a given correspondence the actual overlapping is not necessarily between pair instead it can be between any number of pattern it is assumed that each pattern is a portion of an image of an unobserved object under a distinct rigid transformation we derive an iterative solution for the problem of global registration of the pattern in order to reconstruct the original object our solution is based on the em algorithm and it generalizes the well known solution for the two pattern case we also suggest a very efficient method to implement the proposed algorithm experimental result demonstrate the improved performance of the proposed method 
abstract most imaging sensor have a limited dynamic range and hence can satisfactorily respond to only a part of il lumination level present in a scene this is particularly disadvantageous for omnidirectional and panoramic camera since larger eld of view have larger bright ness range we propose a simple modi cation to exist ing high resolution omnidirectional panoramic camera in which the process of increasing the dynamic range is coupled with the process of increasing the eld of view this is achieved by placing a graded transparency mask in front of the sensor which allows every scene point to be imaged under multiple exposure setting a the camera pan a process anyway required to capture large eld of view at high resolution the sequence of image are then mosaiced to construct a high resolution high dynamic range panoramic omnidirectional image our method is robust to alignment error between the mask and the sensor grid and doe not require the mask to be placed on the sensing surface we have designed a panoramic camera with the proposed modi cation and have discussed various theoretical and practical issue encountered in obtaining a robust design we show with an example of high resolution high dynamic range panoramic image obtained from the camera we designed 
recently a number of author have proposed treating dialogue system a markov decision process mdps however the practical application of mdp algorithm to dialogue system face a number of severe technical challenge we have built a general software tool rlds for reinforcement learning for dialogue system based on the mdp framework and have applied it to dialogue corpus gathered from two dialogue system built at at t lab our experiment demonstrate that rlds hold promise a a tool for browsing and understanding correlation in complex temporally dependent dialogue corpus 
it ha been known that people after being exposed to sentencesgenerated by an articial grammar acquire implicit grammaticalknowledge and are able to transfer the knowledge to input that aregenerated by a modied grammar we show that a second orderrecurrent neural network is able to transfer grammatical knowledgefrom one language generated by a finite state machine to anotherlanguage which dier both in vocabulary and syntax representationof the grammatical knowledge in 
much recent attention both experimental and theoretical ha been focussed on classificationalgorithms which produce voted combination of classifier recent theoreticalwork ha shown that the impressive generalization performance of algorithm like adaboostcan be attributed to the classifier having large margin on the training data we present an abstract algorithm for finding linear combination of function that minimizearbitrary cost functionals i e functionals that do not 
state abstraction is of central importance in remforcement learning and markov decision process this paper study the case of variable resolution state abstraction for continuous state deterministic dynamic control problem in which near optimal policy are required we describe variable resolution policy and value function representation based on kuhn triangulation embedded in a kd tree we then consider top down approach to choosing which cell to split in order to generate improved policy we begin with local approach based on value function property and policy property that use only feature of individual cell in making splitting choice later by introducing two new non local measure influence and variance we derive a splitting criterion that allows one cell to efficiently take into account it impact on other cell when deciding whether to split we evaluate the performance of a variety of splitting criterion on many benchmark problem published on the web paying careful attention to their number of cell versus closeness to optimality tradeoff curve 
bayesian prediction are stochastic just like prediction of any otherinference scheme that generalize from a finite sample while a simplevariational argument show that bayes averaging is generalizationoptimal given that the prior match the teacher parameterdistribution the situation is le clear if the teacher distribution isunknown i define a class of averaging procedure the temperatedlikelihoods including both bayes averaging with a uniform priorand maximum likelihood 
to navigate reliably in indoor environment a mobile robot must know where it is this includes both the ability of globally localizing the robot from scratch a well a tracking the robot s position once it location is known vision ha long been advertised a providing a solution to these problem but we still lack efficient solution in unmodified environment many existing approach require modification of the environment to function properly and those that work within unmodified environment seldomly address the problem of global localization in this paper we present a novel vision based localization method based on the condensation algorithm a bayesian filtering method that us a samplingbased density representation we show how thecondensation algorithm can be used in a novel way to track the position of the camera platform rather than tracking an object in the scene in addition it can also be used to globally localize the camera platform given a visual map of the environment based on these two observation we present a visionbased robot localization method that provides a solution to a difficult and open problem in the mobile robotics community a evidence for the viability of our approach we show both global localization and tracking result in the contex t of a state of the art robotics application 
in this work we explore homeostasis in a silicon integrate and fire neuron the neuron adapts it firing rate over long time period on the order of second or minute so that it return to it spontaneous firing rate after a lasting perturbation homeostasis is implemented via two scheme one scheme look at the presynaptic activity and adapts the synaptic weight depending on the presynaptic spiking rate the second scheme adapts the synaptic threshold depending on the neuron s activity the threshold is lowered if the neuron s activity decrease over a long time and is increased for prolonged increase in postsynaptic activity both these mechanism for adaptation use floating gate technology the result shown here are measured from a chip fabricated in a m cmos process 
a new method to pre segment image by mean of a hierarchical description is proposed this description is obtained from an investigation of the deep structure of a scale space image the input image and the gaussian filtered one simultaneously we concentrate on scale space critical point point with vanishing gradient with respect to both spatial and scale direction we show that these point are always saddle point they turn out to be extremely useful since the iso intensity manifold through these point provide a scale space hierarchy tree and induce a segmentation without a priori knowledge moreover together with the so called catastrophe point these scale space saddle form the critical point of the parameterised critical curve the curve along which the spatial saddle point move in scale space experimental result with respect to the hierarchy and segmentation are given based on an artificial image and a simulated mri 
the emergence of internet a a global information repository where information of all kind is stored requires intelligent information processing tool i e computer application to help the information seeker to retrieve the stored information to build these intelligent information processing tool we need to build computer application that understand human language since most of those information is represented in human language this is where computational linguistics becomes important especially for country like indonesia that host more than million people we need to develop a systematic understanding of the bahasa indonesia the indonesian national language to enable u develop the needed computer application that will help u manage information intelligently 
this paper address the problem of tracking several non rigid object over a sequence of frame acquired from a static observer using boundary and region based information under a coupled geodesic active contour framework given the current frame a statistical analysis is performe d on the observed difference frame which provides a measurement that distinguishes between the static and mobile region in term of conditional probability an objective function is defined that integrates boundary based and region based module by seeking curve that attract the object boundary and maximize the a posteriori segmentation probability on the interior curve region with respect to in tensity and motion property this function is minimized using a gradient descent method the associated eulerlagrange pde is implemented using a level set approach where a very fast front propagation algorithm evolves the initial curve towards the final tracking result very promis ing experimental result are provided using real video sequence 
a mixed signal paradigm is presented for high resolution parallel innerproduct computation in very high dimension suitable for efficient implementation of kernel in image processing at the core of the externally digital architecture is a high density low power analog array performing binary binary partial matrix vector multiplication full digital resolution is maintained even with low resolution analog to digital conversion owing to random statistic in the analog summation of binary product a random modulation scheme produce near bernoulli statistic even for highly correlated input the approach is validated with real image data and with experimental result from a cid dram analog array prototype in m cmos 
web caching and prefetching are well known strategy for improving the performance of internet system when combined with web log mining these strategy can decide to cache and prefetch web document with higher accuracy in this paper we present an application of web log mining to obtain web document access pattern and use these pattern to extend the well known gdsf caching policy and prefetching policy using real web log we show that this application of data mining can achieve dramatic improvement to web access performance 
the necessary and sufficient condition for being able to estimate scene structure motion and camera calibration from a sequence of image are very rarely satisfied in practice what exactly can be estimated in sequence of practical importance when such condition are not satisfied in this paper we give a complete answer to this question for every camera motion that fails to meet the condition we give explicit formula for the ambiguity in the reconstructed scene motion and calibration such a characterization is crucial both for designing robust estimation algorithm that do not try to recover parameter that cannot be recovered and for generating novel view of the scene by controlling the vantage point to this end we characterize explicitly all the vantage point that give rise to a valid euclidean reprojection regardless of the ambiguity in the reconstruction we also characterize vantage point that generate view that are altogether invariant to the ambiguity all the result are presented using simple notation that involves no tensor nor complex projective geometry and should be accessible with basic background in linear algebra 
learning in many visual perceptual task ha been shown to be specific to practiced stimulus while new stimulus have to be learned from scratch here we demonstrate generalization using a novel paradigm in motion discrimination where learning ha been previously shown to be specific we trained subject to discriminate direction of moving dot and verified the previous result that learning doe not transfer from a trained direction to a new one however by tracking the subject performance across time in the new direction we found that their speed of learning doubled therefore we found generalization in a task previously considered too difficult to generalize we also replicated in a second experiment transfer following training with easy stimulus when the difference between motion direction is enlarged in a third experiment we found a new mode of generalization after mastering the task with an easy stimulus subject who have practiced briefly to discriminate the easy stimulus in a new direction generalize to a difficult stimulus in that direction this generalization depends on both the mastering and the brief practice the specificity of perceptual learning and the dichotomy between learning of easy versus difficult task have been assumed to involve different learning process at different cortical area here we show how to interpret these result in term of signal detection theory with the assumption of limited computational capacity we obtain the observed phenomenon direct transfer and acceleration of learning for increasing level of task difficulty human perceptual learning and generalization therefore concur with a generic discrimination system 
we study the impact of backbone in optimization and approximation problem we show that some optimization problem like graph coloring resemble decision problem with problem hardness positively correlated with backbone size for other optimization problem like block world planning and traveling salesperson problem problem hardness is weakly and negatively correlated with backbone size while the cost of finding optimal and approximate solution is positively correlated with backbone size a third class of optimization problem like number partitioning have region of both type of behavior we find that to observe the impact of backbone size on problem hardness it is necessary to eliminate some symmetry perform trivial reduction and factor out the effective problem size 
this paper present an approach to automatically extracting the bilingual translation of many web query term through mining the web anchor text some preliminary experiment are conducted on using web page containing both chinese and english anchor text in their in link to extract chinese translation of english query selected from popular query term in taiwan it is found that the effective translation of of the popular query term can be extracted in which cannot be obtained in common translation dictionary 
this paper proposes a new method for effecting feature correspondence between image the method operates from coarse to fine and is superior to previous method in that it can solve the wide baseline stereo problem even when the image ha been deformed or rotated at the coarsest level a ransac style estimator is used to estimate the two view image constraint r which is then used to guide matching the two view relation is an augmented fundamental matrix being a fundamental matrix plus a homography consistent with that fundamental matrix this is akin to the plane plus parallax representation with the homography being used to help guide matching and to mitigate the effect of image deformation in order to propagate the information from coarse to fine image the distribution of the parameter of r is encoded using a set of particle and an importance sampling function it is not known in general how to choose the importance sampling function but a new method impsac is presented that automatically generates such a function it is shown that the method is superior to previous single resolution ransac style feature matcher 
hebbian and competitive hebbian algorithm are almost ubiquitous in modeling pattern formation in cortical development we analyse in theoretical detail a particular model adapted from piepenbro ck obermayer for the development of d stripe like pattern which place competitive and interactive cortical influence and free a nd restricted initial arborisation onto a common footing 
extracting rule from rbfs is not a trivial task because of nonlinear function or high input dimensionality in such case some of the hidden unit of the rbf network have a tendency to be shared across several output class or even may not contribute 
we describe a biographical multi document summarizer that summarizes information about people described in the news the summarizer us corpus statistic along with linguistic knowledge to select and merge description of people from a document collection removing redundant description the summarization component have been extensively evaluated for coherence accuracy and non redundancy of the description produced 
we consider the problem of how an agent creates a discrete spatial representation from it continuous interaction with the environment such representation will be the minimal one that explains the experience of the agent in the environment in this 
tomasi and kanade introduced the factorization method for recovering d structure from d video in their formulation the d shape and d motion are computed by using an svd to approximate a matrix that is rank in a noiseless situation in this paper we reformulate the problem using the fact that the x and y coordinate of each feature are known from their projection onto the image plane in frame w e show how to compute the d shape i e the relative depth z and the d motion by a simple factorization of a matrix that is rank in a noiseless situation this allows the use of very fast algorithm even when using a large number of feature and large number of frame we also show how to accommodate confidence weight for the feature trajectory this is done without additional computational cost by rewriting the problem a the factorization of a modified matrix 
in this paper an efficient global algorithm for vectorizing line drawing is presented it first extract a seed segment of a graphic entity from a raster image to obtain it direction and width then track the pixel under the guidance of the direction so that the tracking can track through function and is not affected by noise and degradation of image quality thus an entity will be vectorized in one step without postprocessing the relation among line are also used to realize the continuous vectorization of a line net the speed and quality of vectorization are greatly improved with this algorithm the performance evaluation is carried out both by theoretical analysis and by experiment comparison with other vectorization algorithm are also made 
we present a learning algorithm for non parametric hidden markov model with continuous state and observation space all necessary probability density are approximated using sample along with density tree generated from such sample a monte carlo version of baum welch em is employed to learn model from data regularization during learning is achieved using an exponential shrinking technique the shrinkage factor which determines the effective capacity of the learning algorithm is annealed down over multiple iteration of baum welch and early stopping is applied to select the right model once trained monte carlo hmms can be run in an any time fashion we prove that under mild assumption monte carlo hidden markov model converge to a local maximum in likelihood space just like conventional hmms in addition we provide empirical result obtained in a gesture recognition domain 
we exploit and extend the generative lexicon theory to develop a formal description of adnominal constituent in a lexicon which can deal with linguistic phenomenon found in japanese adnominal constituent we classify the problematic behavior into static disambiguation and dynamic disambiguation task static disambiguation can be done using lexical information in a dictionary whereas dynamic disambiguation requires inference at the knowledge representation level 
many organization employ lesson learned ll process to collect analyze store and distribute validated experiential knowledge lesson of their member that when reused can substantially improve organizational decision process unfortunately deployed ll system do not facilitate lesson reuse and fail to bring lesson to the attention of the user when and where they are needed and applicable i e they fail to bridge the lesson distribution gap our approach for solving this problem named monitored distribution tightly integrates lesson distribution with these decision process we describe a case based implementation of monitored distribution alds in a plan authoring tool suite hicap we evaluate it utility in a simulated military planning domain our result show that monitored distribution can significantly improve plan evaluation measure for this domain 
we compare the performance of two database selection algorithm reported in the literature their performance is compared using a common testbed designed specifically for database selection technique the testbed is a decomposition of the trec tipster data into subcollections we present result of a recent investigation of the performance of the cori algorithm and compare the performance with earlier work that examined the performance of ggloss the database from our testbed were ranked using both the ggloss and cori technique and compared to the rbr baseline a baseline derived from trec relevance judgement we examined the degree to which cori and ggloss approximate this baseline our result confirm our earlier observation that the ggloss ideal l rank do not estimate relevance based rank well we also find that cori is a uniformly better estimator of relevance based rank than ggloss for the test environment used in this study part of the advantage of the cori algorithm can be explained by a strong correlation between ggloss and a size based baseline sbr we also find that cori produce consistently accurate ranking on testbeds ranging from site however for a given level of recall search effort appears to scale linearly with the number of database 
high dimensional data ha always been a challenge for clustering algorithm because of the inherent sparsity of the point therefore technique have recently been proposed to find cluster in hidden subspace of the data however since the behavior of the data may vary considerably in different subspace it is often difficult to define the notion of a cluster with the use of simple mathematical formalization in fact the meaningfulness and definition of a cluster is best characterized with the use of human intuition in this paper we propose a system which performs high dimensional clustering by effective cooperation between the human and the computer the complex task of cluster creation is accomplished by a combination of human intuition and the computational support provided by the computer the result is a system which leverage the best ability of both the human and the computer in order to create very meaningful set of cluster in high dimensionality 
this paper considers the task of predicting the future decision of an agent a based on his past decision we assume that a is rational he us the principle of maximum expected utility we also assume that the probability distribution p he assigns to random event is known so that we need only infer his utility function u to model his decision process we consider the task of using a s previous decision to learn about u in particular a s past decision can be viewed a constraint on u 
in combinatorial auction multiple good are sold simultaneously and bidder may bid for arbitrary combination of good determining the outcome of such an auction is an optimization problem that is np complete in the general case we propose two method of overcoming this apparent intractability the first method which is guaranteed to be optimal reduces running time by structuring the search space so that a modified depth first search usually avoids even considering allocation that contain conflicting bid caching and pruning are also used to speed searching our second method is a heuristic market based approach it set up a virtual multi round auction in which a virtual agent represents each original bid bundle and place bid according to a fixed strategy for each good in that bundle we show through experiment on synthetic data that a our first method find optimal allocation quickly and offer good anytime performance and b in many case our second method despite lacking guarantee regarding optimality or running time quickly reach solution that are nearly optimal 
markov decision process mdps provide a coherent mathematical framework for planning under uncertainty however exact mdp solution algorithm require the manipulation of a value function which specifies a value for each state in the system most real world mdps are too large for such a representation to be feasible preventing the use of exact mdp algorithm various approximate solution algorithm have been proposed many of which use a linear combination of basis function a a compact approximation to the value function almost all of these algorithm use an approximation based on the weighted l norm euclidean distance this approach prevents the application of standard convergence result for mdp algorithm all of which are based on max norm this paper make two contribution first it present the first approximate mdp solution algorithm both value and policy iteration that use max norm projection thereby directly optimizing the quantity required to obtain the best error bound second it show how these algorithm can be applied efficiently in the context of factored mdps where the transition model is specified using a dynamic bayesian network 
there are obvious reason for trying to automate the production of multilingual document among them are the rapidly growing need for such document the high cost and low availability of good translator and the fact that translator often need more time than is available to produce good multilingual version these problem are compounded when equivalent version of a document are needed in not just two or three but many language a is often the case in europe where there are now eleven official language in the european community this talk present some recent development in multilingual natural language generation mnlg these allow the automatic production of high quality multilingual document while avoiding many of the well known pitfall of the more familiar alternative of machine translation mt for example the difficulty of information extraction from a source document and the danger of source language bias 
large collection of image can be indexed by projection on a few eigenfeatures the dominant eigenvectors of the image covariance matrix a preliminary step of registering the image is common practice a quantitative analysis of what is being gained by registration wa not performed in previous work and heuristic were used to determine on what to register the image we show that the registration improves the accuracy of indexing and optimal improvement is obtained when the image are registered on their eigenfeatures subspace similarly if multiple image are to be registered on a subspace the optimal subspace is the one spanned by the dominant eigenfeatures of the registered image an algorithm that simultaneously register the image and computes their eigenfeatures is proposed the key idea is to iterate the following two step eigenfeatures are computed from the image new image are computed by registering the image on the subspace of these eigenfeatures in the next iteration step is applied to the set of image that were most recently computed in step it is demonstrated that the algorithm produce improved eigenfeatures and register multiple image 
supervised classification is one of the most active area of machine learning research most work ha focused on classification in static domain where an instantaneous snapshot of attribute is meaningful in many domain attribute are not static in fact it is the way they vary temporally that can make classification possible example of such domain include speech recognition gesture recognition and electrocardiograph classification while it is possible to use ad hoc domain specific 
we describe the use of xml tokenisation tagging and mark up tool to prepare a corpus for parsing our technique are generally applicable but here we focus on parsing medline abstract with the anlt wide coverage grammar hand crafted grammar inevitably lack coverage but many coverage failure are due to inadequacy of their lexicon we describe a method of gaining a degree of robustness by interfacing po tag information with the existing lexicon we also show that xml tool provide a sophisticated approach to pre processing helping to ameliorate the messiness in real language data and improve parse performance 
we present a document compression system that us a hierarchical noisy channel model of text production our compression system first automatically derives the syntactic structure of each sentence and the overall discourse structure of the text given a input the system then us a statistical hierarchical model of text production in order to drop non important syntactic and discourse constituent so a to generate coherent grammatical document compression of arbitrary length the system outperforms both a baseline and a sentence based compression system that operates by simplifying sequentially all sentence in a text our result support the claim that discourse knowledge play an important role in document summarization 
we present two family of reflective surface that are capable of providing a wide field of view and yet still approximate a perspective projection to a high degree these surface are derived by considering a plane perpendicular to the axis of a surface of revolution and finding the equation governing the distortion of the image of the plane in this surface we then view this relation a a differential equation and prescribe the distortion term to be linear by choosing appropriate initial condition for the differential equation and solving it numerically we derive the surface shape and obtain a precise estimate a to what degree the resulting sensor can approximate a perspective projection thus these surface act a computational sensor allowing for a wide angle perspective view of a scene without processing the image in software the application of such a sensor should be numerous including surveillance robotics and traditional photography recently many researcher in the robotics and vision community have begun to consider visual sensor that are able to obtain wide field of view such device are the natural solution to various difficulty encountered with conventional imaging system the two most common mean of obtaining wide field of view are fish eye lens and reflective surface also known a catoptrics when catoptrics are combined with conventional lens system known a dioptrics the resulting sensor are known a catadioptrics the possible us of these system include application such a robot control and surveillance in this paper we will consider only catadioptri based sensor often such system consist of a camera pointing at a convex mirror how to interpret and make use of the visual information obtained by such system e g how they should be used to control robot is not at all obvious there are infinitely many different shape that a mirror can have and at least two different camera model perspective and orthographic projection with which to combine each mirror the property of the resulting sensor are very sensitive to these choice the classic need for wide angle lens have of course been in photography in particular underwater and architectural photography are two example in which having a wide angle lens is often crucial the commercially available lens with the widest field of view without radial distortion that the author are aware of is the nikon mm f nikkor ai which provides a field of view of degree at a cost of u note that our prototype orthographic sensor provides a field of view of degree 
the general problem of surface matching is taken up in this study the process described in this work hinge on a geodesic distance equation for a family of surface embedded in the graph of a cost function the cost function represents the geometrical matching criterion between the two d surface this graph is a hypersurface in dimensional space and the theory presented herein is a generalization of the geodesic curve evolution method introduced by r kimmel et al it also generalizes a d matching process developed in an eulerian level set formulation of the geodesic surface evolution is also used leading to a numerical scheme for solving partial differential equation originating from hyperbolic conservation law which ha proven to be very robust and stable the method is applied on example showing both small and large deformation and arbitrary topological change 
it is desirable that a complex decision making problem in an uncertain world be adequately modeled by a markov decision process mdp whose structural representation is adaptively designed by a parsimonious resource allocation process resource include time and cost of exploration amount of memory and computational time allowed for the policy or value function representation concerned about making the best use of the available resource we address the problem of e ciently estimating where 
we report on the use of reinforcement learning with cobot a software agent residing in the wellknown online community lambdamoo our initial work on cobot isbell et al provided him with the ability to collect social statistic and report them to user here we describe our application of rl to allow cobot to proactively take action in this complex social environment and adapt his behavior from multiple source of human reward after month of training and reward and punishment event from different lambdamoo user cobot learned nontrivial preference for a number of user modifing his behavior based on his current state here we describe lambdamoo and the state and action space of cobot and report the statistical result of the learning experiment 
we consider the problem of learning to attain multiple goal in a dynamic environment which is initially unknown in addition the environment may contain arbitrarily varying element related to action of other agent or to non stationary movesofnature thisproblemismodelledasastochastic markov gamebetween the learning agent and an arbitrary player with a vector valued reward function the objective of the learning agent is to have it long term average reward vector belongtoagiventargetset wedeviseanalgorithmforachievingthistask which isbasedonthetheoryofapproachabilityforstochasticgames thisalgorithmcombines in an appropriate way a flnite set of standard scalar reward learning algorithm su cientconditionsaregivenfortheconvergenceofthelearningalgorithm to a general target set the specialization of these result to the single controller markov decision problem are discussedas well 
several algorithm have already been implemented which combine association rule with first order logic formula although this resulted in several usable algorithm little attention wa payed until recently to the efficiency of these algorithm in this paper we present some new idea to turn one important intermediate step in the process of discovering such rule i e the discovery of frequent item set more efficient using an implementation that we coined farmer we show that indeed a speed up is obtained and that using these idea the performance is much more comparable to original association rule algorithm 
we propose a class of benchmark for edge detector evaluation that require no ground truth each benchmark consists of a large number of image of a carefully designed scene for which we enforce a constraint on the edge for example that they are co linear we sample the space of edge appearance a densely a possible by capturing the image under widely varying imaging condition not only do we change the viewing geometry and the illumination direction but we also vary the camera parameter and the physical property of the object in the scene we show that the degree to which the constraint hold in the output edge map can be used a highly discriminating measure of edge detector performance the code image and result which form our benchmark are all available from the website http www c columbia edu cave the code and image enable a user to compare any new detector against several previous one with minimal effort 
existing conflict detection method for csp s such a de kleer ginsberg cannot make use of powerful propagation which make them unusable for complex real world problem on the other hand powerful constraint propagation method lack the ability to extract dependency or conflict which make them unusable for many advanced ai reasoning method that require conflict a well a for interactive application that require explanation in this paper we present a non intrusive conflict detection algorithm called quickxplain that tackle those problem it can be applied to any propagation or inference algorithm a powerful a it may be our algorithm improves the efficiency of direct non intrusive conflict detector by recursively partitioning the problem into subproblems of half the size and by immediately skipping those subproblems that do not contain an element of the conflict q uickxplain is used a explanation component of an advanced industrial constraint based configuration tool 
we describe two direct quasilinear method for camera pose absolute orientation and calibration from a single image of or known d point they generalize the point direct linear transform method by incorporating partial prior camera knowledge while still allowing some unknown calibration parameter to be recovered only linear algebra is required the solution is unique in non degenerate case and additional point can be included for improved stability both method fail for coplanar point but we give an experimental eigendecomposition based one that handle both planar and nonplanar case our method use recent polynomial solving technology and we give a brief summary of this one of our aim wa to try to understand the numerical behaviour of modern polynomial solver on some relatively simple test case with a view to other vision application 
this paper present a corpus based algorithm capable of inducing inflectional morphological analysis of both regular and highly irregular form such a brought bring from distributional pattern in large monolingual text with no direct supervision the algorithm combine four original alignment model based on relative corpus frequency contextual similarity weighted string similarity and incrementally retrained inflectional transduction probability starting with no paired example for training and no prior seeding of legal morphological transformation accuracy of the induced analysis of past tense test case in english exceeds for the set with currently over accuracy on the most highly irregular form and accuracy on form exhibiting non concatenative suffixation 
in this contribution we introduce a new model free method for object tracking the tracking is posed a a segmentation problem which we solve using the watershed algorithm a framework is defined to compute the required topographic surface from distance to the predicted contour intensity edge and motion edge this multifeature tracking approach yield accurate result in the presence of object corner image clutter and camera motion result on real sequence confirm the stability and robustness of the method object are tracked over long sequence and in the presence of fast object motion 
a general formulation for geodesic distance propagation of surface is presented starting from a surface lying on a manifold in ir we set up a partial differential equation governing the propagation of surface at equal geodesic distance on the manifold from the given original surface this propagation scheme generalizes a result of kimmel et al and provides a way to compute distance map on manifold moreover the propagation equation is generalized to any number of dimension using an eulerian formulation with level set it give stable numerical algorithm for computing distance map this theory is used to present a new method for surface matching which generalizes a curve matching method matching path are obtained a the orbit of the vector field defined a the sum of two distance map gradient value this surface matching technique applies to the case of large deformation and topological change 
introduction we describe the gpt system and it utilization over a number of example gpt general planning tool is an integrated software tool for modeling analyzing and solving a wide range of planning problem dealing with uncertainty and partial information that ha been used for u and others for research and teaching our approach is based on different state model that can handle various type of action dynamic deterministic and probabilistic and sensor feedback null partial 
a common method for real time segmentation of moving region in image sequence involves background subtraction or thresholding the error between an estimate of the image without moving object and the current image the numerous approach to this problem differ in the type of background model used and the procedure used to update the model this paper discus modeling each pixel a a mixture of gaussians and using an on line approximation to update the model the gaussian distribution of the adaptive mixture model are then evaluated to determine which are most likelyto result from a background process each pixel is classified based on whether the gaussian distribution which represents it most effectivelyis considered part of the background model this result in a stable real time outdoor tracker which reliablydeals with lighting change repetitive motion from clutter and long term scene change this system ha been run almost continuously for month hour a day through rain and snow 
combinatorial auction ca where bidder can bid on bundle of item can be very desirable market mechanism when the item sold exhibit complementarity and or substitutability so the bidder s valuation for bundle are not additive however in a basic ca the bidder may need to bid on exponentially many bundle leading to difficulty in determining those valuation undesirable information revelation and unnecessary communication in this paper we present a design of an auctioneer agent that us topological structure inherent in the problem to reduce the amount of information that it need from the bidder an analysis tool is presented a well a data structure for storing and optimally assimilating the information received from the bidder using this information the agent then narrow down the set of desirable welfare maximizing or pareto efficient allocation and decides which question to ask next several algorithm are presented that ask the bidder for value order and rank information 
although a partially observable markov decision process pomdp provides an appealing model for problem of planning under uncertainty exact algorithm for pomdps are intractable this motivates work on approximation algorithm and grid based approximation is a widely used approach we describe a novel approach to grid based approximation that us a variable resolution regular grid and show that it outperforms previous grid based approach to approximation 
we present a linear approach to the d reconstruction problem from occluding contour using algebraic surface the problem of noise and missing data in the occluding contour extracted from the image lead u to this approach our approach is based first on the intensive use of the duality property between d point and tangent plane and second on the algebraic representation of d surface by implicit polynomial of degree and higher 
this paper describes an extension to the hidden markov model for part of speech tagging using second order approximation for both contextual and lexical probability this model increase the accuracy of the tagger to state of the art level these approximation make use of more contextual information than standard statistical system new method of smoothing the estimated probability are also introduced to address the sparse data problem 
we present a statistical model of feature occurrence over time and develop test based on classical hypothesis testing for significance of term appearance on a given date using additional classical hypothesis testing we are able to combine these term to generate topic a defined by the topic detection and tracking study the grouping of term obtained can be used to automatically generate an interactive timeline displaying the major event and topic covered by the corpus to test the validity of our technique we extracted a large number of these topic from a test corpus and had human evaluator judge how well the selected feature captured the gist of the topic and how they overlapped with a set of known topic from the corpus the resulting topic were highly rated by evaluator who compared them to known topic 
this paper proposes evaluation method based on the use of non dichotomous relevance judgement in ir experiment it is argued that evaluation method should credit ir method for their ability to retrieve highly relevant document this is desirable from the user point of view in modern large ir environment the proposed method are a novel application of p r curve and average precision computation based on separate recall base for document of different degree of relevance and two novel measure computing the cumulative gain the user obtains by examining the retrieval result up to a given ranked position we then demonstrate the use of these evaluation method in a case study on the effectiveness of query type based on combination of query structure and expansion in retrieving document of various degree of relevance the test wa run with a best match retrieval system in query in a text database consisting of newspaper article the result indicate that the tested strong query structure are most effective in retrieving highly relevant document the difference between the query type are practically essential and statistically significant more generally the novel evaluation method and the case demonstrate that non dichotomous relevance assessment are applicable in ir experiment may reveal interesting phenomenon and allow harder testing of ir method 
in this paper we specialize the projective unifocal bifocal and trifocal tensor to the affine case and show how the tensor obtained relate to the registered tensor encountered in previous work this enables u to obtain an affine specialization of known projective relation connecting point and line across two or three view in the simpler case of affine camera we give neccessary and sufficient constraint on the component of the trifocal tensor together with a simple geometric interpretation finally we show how the estimation of the tensor from point correspondence is achieved through factorization and discus the estimation from line correspondence 
many boundary between object in the world project onto curve in an image however boundary involving natural object e g tree hair water smoke are often unworkable under this model because many pixel r eceive light from more than one object we propose a technique for estimating alpha the proportion in which two color mix to produce a color at the boundary the technique extends blue screen matting to background that have almost arbitrary color distribution though coarse knowledge of the boundary s location is required result show a number of different object moved from one image to another while maintaining naturalism 
singapore ha one of the busiest port in the world ship berthing is one of the problem faced by the planner at the port in this paper we study the ship berthing problem we first provide the problem formulation and study the complexity of the problem with different restriction in general the ship berthing problem is np complete although some of it variant may be solved quickly while a geometrical model is intuitive the model cannot be easily extended to handle clearance constraint and berth restriction rather than solving the problem geometrically we transform the problem into the problem of fixing direction of edge in graph to form directed acyclic graph with minimal lonqest path since the problem is np complete solving the problem exactly in polynomial time is highly unlikely a a result we devise a fast and effective greedy algorithm to can generate good solution the greedy method together with a tabu search like post optimization algorithm is able to return optimal or near optimal solution 
in this paper we propose a new method of text categorization based on feature space restructuring for svms in our method independent component of document vector are extracted using ica and concatenated with the original vector this restructuring make it possible for svms to focus on the latent semantic space without losing information given by the original feature space using this method we achieved high performance in text categorization both with small number and large number of labeled data 
this paper address the problem of inversereinforcement learning irl in markov decisionprocesses that is the problem of extractinga reward function given observed optimal behavior irl may be useful forapprenticeship learning to acquire skilled behavior and for ascertaining the reward functionbeing optimized by a natural system werst characterize the set of all reward functionsfor which a given policy is optimal wethen derive three algorithm for irl therst two 
this paper introduces a novel linear algorithm fordetermining the affine calibration between two cameraviews of a dynamic scene the affine calibration iscomputed directly from the fundamental matrix associatedwith various moving object in the scene aswell a from the fundamental matrix for the static backgroundif the camera are at different location a minimumof two fundamental matrix are required butany number of additional fundamental matrix canbe incorporated into the 
we address the problem of rejecting false match of point between two perspective view even the best algorithm for image matching make some mistake and output some false match we present an algorithm for identification of the false match between the view the algorithm exploit the possibility of rotating one of the image to achieve some common behaviour of the correct match those match that deviate from this common behaviour turn out to be false match the statistical tool we use is the mean shift mode estimator our algorithm doe not use in any way the image characteristic of the matched feature in particular it avoids problem that cause the false match in the first place the algorithm may be run a a post processing step on output from any point matching algorithm use of the algorithm may significantly improve the ratio of correct match to incorrect match on real image our algorithm ha improved the percentage of correct match from an initial to a final for robust estimation algorithm which are later employed this is a very desirable quality since it reduces significantly their computational cost we present the algorithm identify the condition under which it work and present result of testing it on both synthetic and real image 
an algebraic curve is defined a the zero set of a polynomial in two variable algebraic curve are practical for modeling shape much more complicated than conic or superquadrics the main drawback in representing shape by algebraic curve ha been the lack of repeatability in fitting algebraic curve to data a regularized fast linear fitting method based on ridge regression and restricting the representation to well behaved subset of polynomial is proposed and it property are investigated the fitting algorithm is of sufficient stability for very fast position invariant shape recognition position estimation and shape tracking based on new invariant and representation and is appropriate to open a well a closed curve of unorganized data among appropriate application are shape based indexing into image database 
a technique is proposed for computing the weakest sufficient wsc and strongest necessary snc condition for formula in an expressive fragment of first order logic using quantifier elimination technique the efficacy of the approach is demonstrated by using the technique to compute snc s and wsc s for use in agent communication application theory approximation and generation of abductive hypothesis additionally we generalize recent result involving the generation of successor state axiom in the propositional situation calculus via snc s to the first order case subsumption result for existing approach to this problem and a re interpretation of the concept of forgetting a a process of quantifier elimination are also provided 
this paper present reinforcement learning with a long shorttermmemory recurrent neural network rl lstm model freerl lstm using advantage learning and directed explorationcan solve non markovian task with long term dependency betweenrelevantevents this is demonstrated in a t maze task aswell a in a di cult variation of the pole balancing task 
in this paper we present a method to removecommercials from talk and game show video and tosegment these video into host and guest shot in ourapproach we mainly rely on information contained inshot transition rather than analyzing the scene contentof individual frame we utilize the inherent difference inscene structure of commercial and talk show todifferentiate between them similarly we make use of thewell defined structure of talk show which can beexploited to 
in this paper we report on a series of experiment designed to investigate query modification technique motivated by the area of abductive reasoning in particular we use the notion of abductive explanation explanation being a description of data that highlight important feature of the data we describe several method of creating abductive explanation exploring term reweighting and query reformulation technique and demonstrate their suitability for relevance feedback 
ongoing work towards appearance based d hand pose estimation from a single image is presented using a d hand model and computer graphic a large database of synthetic view is generated the view display different hand shape a seen from arbitrary viewpoint each syntheticview is automaticallylabeledwith parametersdescribing it hand shape and viewing parameter given an input image the system retrieves the most similar database view and us the shape and viewing parameter of those view a candidate estimate for the parameter of the input image preliminary result are presented in which appearance based similarity is defined in term of the chamfer distance between edge image 
we provide a perspective on knowledge compilation which call for analyzing different compilation approach according to two key dimension the succinctness of the target compilation language and the class of query and transformation that the language support in polytime we argue that such analysis is necessary for placing new compilation approach within the context of existing one we also go beyond classical flat target compilation language based on cnf and dnf and consider a richer nested class based on directed acyclic graph which we show to include a relatively large number of target compilation language 
the recent introduction of the relevance vector machine ha eectivelydemonstrated how sparsity may be obtained in generalisedlinear model within a bayesian framework using a particularform of gaussian parameter prior learning is the maximisation with respect to hyperparameters of the marginal likelihood of thedata this paper study the property of that objective function and demonstrates that conditioned on an individual hyperparameter the marginal likelihood ha a 
this paper show how two image sequence that have no spatial overlap between their field of view can be aligned both in time and in space such alignment is possible when the two camera are attached closely together and are moved jointly in space the common motion induces similar change over time within the two sequence this correlated temporal behavior is used to recover the spatial and temporal transformation between the two sequence the requirement of coherent appearance in standard image alignment technique is therefore replaced by coherent temporal behavior which is often easier to satisfy this approach to alignment can be used not only for aligning non overlapping sequence but also for handling other case that are inherently difficult for standard image alignment technique we demonstrate application of this approach to three real world problem i alignment of non overlapping sequence for generating wide screen movie ii alignment of image sequence obtained at significantly different zoom for surveillance application and iii multi sensor image alignment for multi sensor fusion 
nearest neighbor classification assumes locally constant class conditional probability this assumption becomes invalid in high dimension with finite sample due to the curse of dimensionality severe bias can be introduced under these condition when using the nearest neighbor rule we propose a locally adaptive nearest neighbor classification method to try to minimize bias we use achisquared distance analysis to compute a flexible metric for producing neighborhood that are highly adaptive to query location neighborhood are elongated along le relevant feature dimension and constricted along most influential one a a result the class conditional probability tend to be smoother in the modified neighborhood whereby better classification performance can be achieved the efficacy of our method is validated and compared against other technique using a variety of simulated and real world data 
adacost a variant of adaboost is a misclassication cost sensitive boosting method it us the cost of misclassications to update the training distribution on successive boosting round the purpose is to reduce the cumulative misclassication cost more than adaboost we formally show that adacost reduces the upper bound of cumulative misclassication cost of the training set empirical evaluation have shown signican t reduction in the cumulative misclassication cost over adaboost without consuming additional computing power 
despite the fact that mental arithmetic is based on only a few hundredbasic fact and some simple algorithm human have a difficulttime mastering the subject and even experienced individualsmake mistake associative multiplication the process of doingmultiplication by memory without the use of rule or algorithm is especially problematic human exhibit certain characteristicphenomena in performing associative multiplication both in thetype of error and in the error frequency we 
automatically labeling document cluster with word which indicate their topic is difficult to do well the most commonly used method labeling with the most frequent word in the cluster end up using many word that are virtually void of descriptive power even after traditional stop word are removed another method labeling with the most predictive word often includes rather obscure word we present two method of labeling document cluster motivated by the model that word are generated by a hierarchy of mixture component of varying generality the first method assumes existence of a document hierarchy manually constructed or resulting from a hierarchical clustering algorithm and us a test of significance to detect different word usage across category in the hierarchy the second method selects word which both occur frequently in a cluster and effectively discriminate the given cluster from the other cluster we compare these method on abstract of document selected from a subset of the hierarchy of the cora search engine for computer science research paper label produced by our method showed superior result to the commonly employed method 
we present a system for identifying the semantic relationship or filled by constituent of a sentence within a semantic frame various lexical and syntactic feature are derived from parse tree and used to derive statistical classifier from hand annotated training data 
a spatio temporal representation for complex optical flow event is developed that generalizes traditional parameterized motion model e g affine these generative spatio temporal model may be non linear or stochastic and are event specific in that they characterize a particular type of object motion e g sitting or walking within a bayesian framework we seek the appropriate model phase rate spatial position and scale to account for the image variation the posterior distribution over this parameter space conditioned on image measurement is typically nongaussian the distribution is represented using factored sampling and is predicted and updated over time using the condensation algorithm the resulting framework automatically detects localizes and recognizes motion event 
many planning problem exhibit a high degree of symmetry that cannot yet be exploited successfully by modern planning technology for example problem in the gripper domain in which a robot with two grippers must transfer ball from one room to another are trivial to the human problem solver because the high degree of symmetry in the domain mean that the order in which pair of ball are transported is irrelevant to the length of the shortest transportation plan however planner typically search all possible ordering giving rise to an exponential explosion of the search space this paper describes a way of detecting and exploiting symmetry in the solution of problem that demonstrate these characteristic we have implemented our technique in stan a graphplan based planner that us state analysis technique in a number of way to exploit the underlying structure of domain we have achieved a dramatic improvement in performance in solving problem exhibiting symmetry we present a range of result and indicate the further development we are now pursuing 
we study the dynamic of a hebbian ica algorithm extracting a single non gaussian component from a high dimensional gaussian background for both on line and batch learning we find that a surprisingly large number of example are required to avoid trapping in a sub optimal state close to the initial condition to extract a skewed signal at least example are required for dimensional data and example are required to extract a symmetrical signal with non zero kurtosis 
the hr program by colton et al performs theory formation in mathematics byexploring a space of mathematical concept by enabling hr to determine when it hasfound a particular concept and by adding aforward looking mechanism we have appliedhr to the problem of identifying mathematicalconcepts we illustrate this by using hrto identify and extrapolate integer sequencesand by performing a qualitative comparisonwith the machine learning program progol introduction 
text mining concern the discovery of knowledge from unstructured textual data one important task is the discovery of rule that relate specific word and phrase although existing method for this task learn traditional logical rule soft matching method that utilize word frequency information generally work better for textual data this paper present a rule induction system textrise that allows for partial matching of text valued feature by combining rule based and instance based learning we present initial experiment applying textrise to corpus of book description and patent document retrieved from the web and compare it result to those of traditional rule and instance based method 
people are active experimenter constantly seeking new information relevant to their goal a reasonable approach to active information gathering is to ask question and conduct experiment that minimize the expected state of uncertainty or maximize the expected information gain given current belief fedorov mackay oaksford chater in this paper we present result on an exploratory experiment designed to study people s active information gathering behavior on a concept learning task the result of the experiment suggest subject behavior may be explained well from the point of view of bayesian information maximization introduction 
a fast and general method to extract anomaly in an arbitrary iniage is proposed the basic idea is to compute a probability density for sub region in an image conditioned upon the area surrounding the sub region linear estimation and independent component analysis ica are combined to obtain the probability estimate pseudo non parametric correlation is used to group set of similar surrounding pattern from which a probability for the occurrence of a given sub region is derived a carefully designed multi dimensional histogram based on compressed vector representation enables eficient and high resolution extraction of anonlalies from the image our current unoptimized implementation performs anomaly extraction in about second for a x image using a mhz pc experimental result are included that demonstrate the perforniance of the proposed method 
we address the problem of moving object segmentationusing active contour a far a segmentation of movingobjects is concerned region based term must be incorporatedin the evolution equation of the active contour in additionto classical boundary based term in this paper wepropose a general framework for region based active contour novel aspect of the segmentation method includea new eulerian proof to compute the evolution equation ofthe active contour from the minimization of a 
we address the problem of moving object segmentation using active contour a far a segmentation of moving object is concerned region based term must be incorporated in the evolution equation of the active contour in addition to classical boundary basedterms in this paper we propose a general framework for region based active contour novel aspect of the segmentation method include a new eulerian proof to compute the evolution equation of the active contour from the minimization of a criterion and the introduction of function named descriptor of the region in this proof the dynamical scheme is directly introduced in the criterion before differentiation with such a method the case of descriptor depending on the evolution of the curve i e depending upon feature globally attached to the region can readily be taken into account the variation of these descriptor upon the evolution of the curve induces additional term in the evolution equation of the active contour the proof ensures the fastest decrease of the active contour towards a minimum of the criterion inside this theoretical framework a set of descriptor is evaluated on real sequence for the detection of moving object 
we investigate a novel approach to solve the problem of sparse data through dimension reduction linear algebraic technique called lsa svd is used to find co relationship of sparse word three variant estimation method are suggested and they are evaluated for estimating unseen noun verb co occurrence probability the model show possibility to be alternative probability smoothing method 
we propose a formal framework for modelling case based inference cbi which is a crucial part of the case based reasoning cbr methodology a a representation of the similarity structure of a system the concept of a similarity profile is introduced this concept make it possible to formalize the cbr hypothesis that similar problem have similar solution and to realize cbi in the form of constraint based inference in order to exploit the similarity structure more efficiently a probabilistic generalization of the constraintbased view is developed this formalization allows for realizing cbi in the context of probabilistic reasoning and statistical inference and hence make a powerful methodological framework accessible to cbr within the generalized setting a formalized cbr hypothesis corresponds to the assumption of a certain stochastic model and a memory of case can be seen a statistical data underlying the inference process a a particular result we establish an approximate probabilistic reasoning scheme which generalizes the constraint based approach 
acetylcholine ach ha been implicated in a wide variety of task involving attentional process and plasticity following extensive animal study it ha previously been suggested that ach report on uncertainty and control hippocampal cortical and cortico amygdalar plasticity we extend this view and consider it effect on cortical representational inference arguing that ach control the balance between bottom up inference inuenced by input stimulus and top down inference inuenced by contextual information we illustrate our proposal using a hierarchical hidden markov model 
whereas matching in description logic is now relatively well investigated there are only very few formal result on matching under additional side condition though these side condition were already present in the original paper by borgida and mcguinness introducing matching in dl the present paper close this gap for the dl and it sublanguages 
a novel learning approach for human face detection using a network of linear unit is presented the snow learning architecture is a sparse network of linear function over a pre defined or incrementally learned feature space and is specifically tailored for learning in the presence of a very large number of feature a wide range of face image in different pose with different expression and under different lighting condition are used a a training set to capture the variation of human 
we present a novel way of obtaining pac style bound on the generalizationerror of learning algorithm explicitly using their stabilityproperties a stable learner being one for which the learnedsolution doe not change much for small change in the trainingset the bound we obtain do not depend on any measure of thecomplexity of the hypothesis space e g vc dimension but ratherdepend on how the learning algorithm search this space andcan thus be applied even when the vc 
this paper considers statistical parsing of czech which differs radically from english in at least two respect it is a highly inflected language and it ha relatively free word order these difference are likely to pose new problem for technique that have been developed on english we describe our experience in building on the parsing model of collins our final result dependency accuracy represent good progress towards the accuracy of the parser on english wall street journal text 
in this paper we introduce a new sparseness inducing prior which doe not involve any hyper parameter that need to be adjusted or estimated although other application are possible we focus here on supervised learning problem regression and classification experiment with several publicly available benchmark data set show that the proposed approach yield state of the art performance in particular our method outperforms support vector machine and performs competitively with the best alternative technique both in term of error rate and sparseness although it involves no tuning or adjusting of sparsenesscontrolling hyper parameter 
many computational problem can be solved by multiple algorithm with different algorithm fastest for different problem size input distribution and hardware characteristic we consider the problem of algorithm selection dynamically choose an algorithm to attack an instance of a problem with the goal of minimizing the overall execution time we formulate the problem a a kind of markov decision process mdp and use idea from reinforcement learning to solve it this paper introduces a kind of mdp that model the algorithm selection problem by allowing multiple state transition the well known q learning algorithm is adapted for this case in a way that combine both monte carlo and temporal difference method also this work us and extends in a way to control problem the least square temporal difference algorithm lstd of boyan the experimental study focus on the classic problem of order statistic selection and sorting the encouraging result reveal the potential of applying learning method to traditional computational problem 
an active set strategy is applied to the dual of a simple reformulation of the standard quadratic program of a linear support vector machine this application generates a fast new dual algorithm that consists of solving a finite number of linear equation with a typically large dimensionality equal to the number of point to be classified however by making novel use of the sherman morrisonwoodbury formula a much smaller matrix of the order of the original input space is inverted at each step thus a problem with a dimensional input space and million point required inverting positive definite symmetric matrix of size with a total running time of minute on a mhz pentium ii the algorithm requires no specialized quadratic or linear programming code but merely a linear equation solver which is publicly available 
the paper show that movement or equivalent computational structure changing operation of any kind at the level of logical form can be dispensed with entirely in capturing quantifier scope ambiguity it offer a new semantics whereby the effect of quantifier scope alternation can be obtained by an entirely monotonic derivation without type changing rule the paper follows fodor fodor and sag and park in viewing many apparent scope ambiguity a arising from referential category rather than true generalized quantifier 
the appearance of an object can vary considerably with change in illumination condition method have been developed to describe these difference for diffuse reflection using the lambertian model but little work ha been done in characterizing specular appearance towards a more comprehensive global reflectance descriptor this paper focus on a representation of specular appearance based on an approximate specular reflection model derived from torrance sparrow we propose that under certain illumination and surface condition local specular structure can be expressed by the logarithm of three intensity normalized photometric image the total number of photometric image needed for representing global specular appearance depends on the object surface roughness and we suggest an illumination planning method for determining the number of image experimental result demonstrate the effectiveness of this logarithmic model a a specular descriptor 
in many scientific and engineering application detecting and understanding difference between two group of example can be reduced to a classical problem of training a classifier for labeling new example while making a few mistake a possible in the traditional classification setting the resulting classifier is rarely analyzed in term of the property of the input data captured by the discriminative model however such analysis is crucial if we want to understand and visualize the detected difference we propose an approach to interpretation of the statistical model in the original feature space that allows u to argue about the model in term of the relevant change to the input vector for each point in the input space we define a discriminative direction to be the direction that move the point towards the other class while introducing a little irrelevant change a possible with respect to the classifier function we derive the discriminative direction for kernel based classifier demonstrate the technique on several example and briefly discus it use in the statistical shape analysis an application that originally motivated this work 
this paper present a method of matching ambiguous feature set extracted from image the method is based on wilson and hancock s bayesian matching framework which is extended to handle the case where the feature measurement are ambiguous a multimodal evolutionary optimisation framework is proposed which is capable of simultaneously producing several good alternative solution unlike other multimodal genetic algorithm the one reported here requires no extra parameter solution yield are maximised by removing bias in the selection step while optimisation performance is maintained by a local search step an experimental study demonstrates the effectiveness of the new approach on synthetic and real data the framework is in principle applicable to any multimodal optimisation problem where local search performs well 
abstract the contrast response function crf of many neuron in the primary vi sual cortex saturates and shift towards higher contrast value following prolonged presentation of high contrast visual stimulus using a recurrent neural network of excitatory spiking neuron with adapting synapsis we show that both effect could be explained by a fast and a slow compo nent in the synaptic adaptation i fast synaptic depression lead to sat uration of the crf and phase advance in the cortical response to high contrast stimulus ii slow adaptation of the synaptic transmitter release probability is derived such that the mutual information between the input and the output of a cortical neuron is maximal this component given by infomax learning rule explains contrast adaptation of the averaged membrane potential dc component a well a the surprising experi mental result that the stimulus modulated component f component of a cortical cell s membrane potential adapts only weakly based on our result we propose a new experiment to estimate the strength of the ef fective excitatory feedback to a cortical neuron and we also suggest a relatively simple experimental test to justify our hypothesized synaptic mechanism for contrast adaptation 
vapnik s result that the expectation of the generalisation error of the o ptimal hyperplane is bounded by the expectation of the ratio of the number of support vector to the number of training example is extended to a broad class of kernel machine the class includes support vector machine for soft margin classification and regression and regularization network with a variety of kernel and cost function we show that key inequality in vapnik s result become equality once the classification error is replaced by the margin error with the latter defined a an instance with positive cost in particular we show that expectation of th e true margin error and the empirical margin error are equal and that the sparse solution for kernel machine are possible only if the cost fun ction is partially insensitive 
the world around u change constantly knowing what ha changed is an important part of our life for business recognizing change is also crucial it allows business to adapt themselves to the changing market need in this paper we study change of association rule from one time period to another one approach is to compare the support and or confidence of each rule in the two time period and report the difference this technique however is too simplistic a it tends to report a huge number of rule change and many of them are in fact simply the snowball effect of a small subset of fundamental change here we present a technique to highlight the small subset of fundamental change a change is fundamental if it cannot be explained by some other change the proposed technique ha been applied to a number of real life datasets experiment result show that the number of rule whose change are unexplainable is quite small about of the total number of change discovered and many of these unexplainable change reflect some fundamental shift in the application domain 
in this work we use a large text corpus toorder noun by their level of specificity thissemantic information can for most noun bedetermined with over accuracy usingsimple statistic from a text corpus withoutusing any additional source of semanticknowledge this kind of semantic informationcan be used to help in automaticallyconstructing or augmenting a lexicaldatabase such a wordnet introductionlarge lexical database such a wordnet see fellbaum are in common 
this paper present a method that assist in maintaining a rule based named entity recognition and classification system the underlying idea is to use a separate system constructed with the use of machine learning to monitor the performance of the rule based system the training data for the second system is generated with the use of the rule based system thus avoiding the need for manual tagging the disagreement of the two system act a a signal for updating the rule based system the generality of the approach is illustrated by applying it to large corpus in two different language greek and french the result are very encouraging showing that this alternative use of machine learning can assist significantly in the maintenance of rule based system 
partial information can trigger a complete memory at the same time human memory is not perfect a cue can contain enough information to specify an item in memory but fail to trigger that item in the context of word memory we present experiment that demonstrate some basic pattern in human memory error we use cue that consist of word fragment we show that short and long cue are completed more accurately than medium length one and study some of the factor that lead to this behavior we then present a novel computational model that show some of the flexibility and pattern of error that occur in human memory this model iterates between bottom up and top down computation these are tied together using a markov model of word that allows memory to be accessed with a simple feature set and enables a bottom up process to compute a probability distribution of possible completion of word fragment in a manner similar to model of visual perceptual completion 
finding model of a predicate logic formula is a well known hard problem whose complexity is exponential in the number of variable however even though this number is kept constant substantial difference in complexity arise when searching for solution in different problem instance such a behavior appears to be quite general according to recent result reported in the literature in fact several class of hard problem exhibit a narrow phase transition with respect to some order parameter in correspondence of which the complexity dramatically rise up still remaining tractable elsewhere in this paper we provide an extensive experimental study on the emergence of a phase transition in the problem of matching a horn clause to a universe searching for a model of the clause or for a proof that no such model exists a it turn out phase transition in the matching problem depends in an essential way on two order parameter one capturing syntactic aspect of the clause structure intensional aspect while the other related to the structure of the universe extensional aspect 
machine learning typically involves discovering regularity in a training set then applying these learned regularity to classify object in a test set in this paper we present an approach to discovering additional regularity in the test set and show that in relational domain such test set regularity can be used to improve classification accuracy beyond that achieved using the training set alone for example we have previously shown how foil a relational learner can learn to classify web page by discovering training set regularity in the word occurring on target page and on other page related by hyperlink here we show how the classification accuracy of foil on this task can be improved by discovering additional regularity on the test set page that must be classified our approach can be seen a an extension to kleinberg s hub and authority algorithm that analyzes hyperlink relation among web page we present evidence that this new algorithm lead to better test set precision and recall on three binary web classification task where the test set web page are taken from different web site than the training set 
we describe a new technique to detect and analyze periodic motion a seen from both a static and moving camera by tracking object of interest we compute an object s self similarity a it evolves in time for periodic motion the self similarity measure is also periodic and we apply time frequency analysis to detect and characterize the periodic motion a real time system ha been implemented to track and classify object using periodicity example of object classification person counting and non stationar y periodicity are provided 
based on a statistical mechanic approach we develop a methodfor approximately computing average case learning curve for gaussianprocess regression model the approximation work well inthe large sample size limit and for arbitrary dimensionality of theinput space we explain how the approximation can be systematicallyimproved and argue that similar technique can be applied togeneral likelihood model introductiongaussian process gp model have gained considerable interest in 
over the year many proposal have been made to incorporate assorted type of feature in language model however discrepancy between training set evaluation criterion algorithm and hardware environment make it difficult to compare the model objectively in this paper we take an information theoretic approach to select feature type in a systematic manner we describe a quantitative analysis of the information gain and the information redundancy for various combination of feature type inspired by both dependency structure and bigram structure using a chinese treebank and taking word prediction a the object the experiment yield several conclusion on the predictive value of several feature type and feature type combination for word prediction which are expected to provide guideline for feature type selection in language modeling 
the minimal data necessary for projective reconstruction from point correspondence is well known when the point are visible in all image in this paper we formulate and propose solution to a new family of reconstruction problem from multiple image with minimal data where there are missing point in some of the image the ability to handle the minimal case with missing data is of great theoretical and practical importance it is unavoidable to use them to bootstrap robust estimation such a ransac and lm algorithm and optimal estimation such a bundle adjustment first we develop a framework to parametrize the multiple view geometry needed to handle the missing data case then we present a solution to the minimal case of point in image where one of the point is missing in one of the three image we prove that there are in general a many a solution for this minimal case furthermore all minimal case with missing data for and in image are catalogued finally we demonstrate the method on both simulated and real image and show that the algorithm presented in this paper can be used for practical problem 
this paper address the issue of motion estimation on image sequence the standard motion equation used to compute the apparent motion of image irradiance pattern is an invariance brightness based hypothesis called the optical flow constraint other equation can be used in particular the extended optical flow constraint which is a variant of the optical flow constraint inspired by the fluid mechanic mass conservation principle in this paper we propose a physical interpretation of this extended optical flow equation and a new model unifying the optical flow and the extended optical flow constraint we present result obtained for synthetic and meteorological image 
the statistic of photographic image when represented usingmultiscale wavelet base exhibit two striking type of nongaussianbehavior first the marginal density of the coefficientshave extended heavy tail second the joint density exhibit variancedependencies not captured by second order model we examineproperties of the class of gaussian scale mixture and showthat these density can accurately characterize both the marginaland joint distribution of natural 
ensemble learning algorithm combine the result of several classifier to yield an aggregate classification we present a normative evaluation of combination method applying and extending existing axiomatizations from social choice theory and statistic for the case of multiple class we show that several seemingly innocuous and desirable property are mutually satisfied only by a dictatorship a weaker set of property admit only the weighted average combination rule for the case of binary classification we give axiomatic justification for majority vote and for weighted majority we also show that even when all component algorithm report that an attribute is probabilistically independent of the classification common ensemble algorithm often destroy this independence information we exemplify these theoretical result with experiment on stock market data demonstrating how ensemble of classifier can exhibit canonical voting paradox 
in the analysis of data recorded by optical imaging from intr insic signal measurement of change of light reflectance from cortical t issue the removal of noise and artifact such a blood vessel pattern is a serious problem often bandpass filtering is used but the underlyin g assumption that a spatial frequency exists which separate the mappin g component from other component especially the global signal is qu estionable here we propose alternative way of processing optical imaging data using blind source separation technique based on the spatial decorrelation of the data we first perform benchmark on artificial data in o rder to select the way of processing which is most robust with respect to sensor noise we then apply it to recording of optical imaging experiment from macaque primary visual cortex we show that our bs technique is able to extract ocular dominance and orientation preferenc e map from single condition stack for data where standard post pro cessing procedure fail artifact especially blood vessel pattern can often be completely removed from the map in summary our method for blind source separation using extended spatial decorrelation is a superior technique for the analysis of optical recording data 
in this paper we present a novel approach to multichannel blindseparation generalized deconvolution assuming that both mixingand demixing model are described by stable linear state space system we decompose the blind separation problem into two process separation and state estimation based on the minimizationof kullback leibler divergence we develop a novel learning algorithmto train the matrix in the output equation to estimate thestate of the demixing model we introduce a new 
in this article we show how a bilingual texttranslation alignment method can be adapted to deal with more than two version of a text experiment on a trilingual corpus demonstrate that this method yield better bilingual alignment than can be obtained with bilingual textalignment method moreover for a given number of text the computational complexity of the multilingual method is the same a for bilingual alignment 
explaining away ha mostly been considered in term of inference of state in belief network we show how it can also arise in a bayesian context in inference about the weight governing relationship such a those between stimulus and reinforcer in conditioning experiment such a backward blocking we show how explaining away in weight space can be accounted for using an extension of a kalman filter model provide a new approximate way of looking at the kalman gain matrix a a whitener for the correlation matrix of the observation process suggest a network implementation of this whitener using an architecture due to goodall and show that the resulting model exhibit backward blocking 
the ac algorithm is a basic and widely used arc consistency enforcing algorithm in constraint satisfaction problem csp it strength lie in that it is simple empirically efficient and extensible however it worst case time complexity wa not considered optimal since the first complexity result for ac mackworth and freuder with the bound o ed where e is the number of constraint and d the size of the largest domain in this paper we show suprisingly that ac achieves the optimal worst case time complexity with o ed the result is applied to obtain a path consistency algorithm which ha the same time and space complexity a the best known theoretical result our experimental result show that the new approach to ac is comparable to the traditional ac implementation for simpler problem where ac is more efficient than other algorithm and significantly faster on hard instance 
given the size of the web the search engine industry ha argued that engine should be evaluated by their ability to retrieve highly relevant page rather than all possible relevant page to explore the role highly relevant document play in retrieval system evaluation assessor for the mbox trec web track used a three point relevance scale and also selected best page for each topic the relative effectiveness of run evaluated by different relevant document set differed confirming the hypothesis that different retrieval technique work better for retrieving highly relevant document yet evaluating by highly relevant document can be unstable since there are relatively few highly relevant document trec assessor frequently disagreed in their selection of the best page and subsequent evaluation by best page across different assessor varied widely the discounted cumulative gain measure introduced by j a rvelin and kek a l a inen increase evaluation stability by incorporating all relevance judgment while still giving precedence to highly relevant document 
background modeling is a common component in video surveillance system and is used to quickly identify region of interest to increase the robustness of background subtraction technique researcher have developed technique to update the background model and also developed probabilistic statistical approach for thresholding the difference this paper present an error analysis of this type of background modeling and pixel labeling providing both theoretical analysis and experimental validation evaluation is centered around the tradeoff of probability of false alarm and probability of miss detection and this paper show how to efficiently compute these probability from simpler value that are more easily measured it includes an analysis for both static and dynamic background modeling the paper also examines the assumption of gaussian and mixture of gaussian model for a pixel 
this paper describes automatic technique for mapping entry in a database of english verb to wordnet sens the verb were initially grouped into class based on syntactic feature mapping these verb into wordnet sens provides a resource that support disambiguation in multilingual application such a machine translation and cross language information retrieval our technique make use of a training set of disambiguated entry representing verb entry from class word sense probability from frequency count in a tagged corpus semantic similarity of wordnet sens for verb within the same class probabilistic correlation between wordnet data and attribute of the verb class the best result achieved precision and recall versus a lower bound of precision and recall for assigning the most frequently occurring wordnet sense and an upper bound of precision and recall for human judgment 
in this paper we present approach for detectingcamera cut wipe and dissolve based on the analysisof spatio temporal slice obtained from video theseslices are composed of spatially and temporally coherentregions which can be perceived a shot in the proposedmethods camera break are located by performingcolor texture segmentation and statistical analysison these video slice in addition to detecting camerabreaks our method can classify the detected break ascamera cut 
in stereoscopic image the behavior of a curve in space is related to the appearance of the curve in the left and right image plane formally this relationship is governed by the projective geometry induced by the stereo camera configuration and by the differential structure of the curve in the scene we propose that the correspondence problem matching corresponding point in the image plane can be solved by relating the differential structure in the left and right image plane to the geometry of curve in space specifically the compatibility between two pair of corresponding point and tangent at those point is related to the local approximation of a space curve using an osculating helix to guarantee robustness against small change in the camera parameter we select a specific osculating helix a relaxation labeling network demonstrates that the compatibility can be used to infer the appropriate correspondence in a scene example on which standard approach fail are demonstrated 
in proc of ieee int l conf on computer vision vancouver canada it is often tedious and expensive to label largetraining data set for learning based object recognitionsystems this problem could be alleviated by selfsupervisedlearning technique which take a hybrid oflabeled and unlabeled training data to learn classifier discriminant em d em proposed a framework forsuch task and current d em algorithm employed lineardiscriminant analysis however the algorithm is 
this paper describes how we annotated and analysed the np modifier in a corpus of museum description to discover rule for the selection and realisation of such modifier in particular non referring one we implemented the regularity into an extension of the ilex system to generate complex np capable of serving multiple communicative goal 
population code often rely on the tuning of the mean response to the stimulus parameter however this information can be greatly suppressed by long range correlation here we study the efficiency of coding information in the second order statistic of the population response we show that the fisher information of this system grows linearly with the size of the system we propose a bilinear readout model for extracting information from correlation code and evaluate it performance in discrimination and estimation task it is shown that the main source of information in this system is the stimulus dependence of the variance of the single neuron response neuronal population response can represent information in the higher order statistic of the response not only in their mean in this work we study the accuracy of coding information in the second order statistic we call such scheme correlation code specifically we assume that the neuronal response obey multivariate gaussian statistic governed by a stimulus dependent correlation matrix we ask whether the fisher information of such a system is extensive even in the presence of strong correlation in the neuronal 
color is a useful feature for machine vision task however it effectiveness is often limited by the fact that the measured pixel value in a scene are influenced by both object surface reflectance property and incident illumination color constancy algorithm attempt to compute color feature which are invariant of the incident illumination by estimating the parameter of the global scene illumination and factoring out it effect a number of recently developed algorithm utilize statistical method to estimate the maximum likelihood value of the illumination parameter this paper detail the use of kl divergence a a mean of selecting estimated illumination parameter value we provide experimental result demonstrating the usefulness of the kl divergence technique for accurately estimating the global illumination parameter of real world image 
a theory of categorization is presented in which knowledge ofcausal relationship between category feature is represented a abayesian network referred to a causal model theory this theorypredicts that object are classified a category member to theextent they are likely to have been produced by a category s causalmodel on this view people have model of the world that leadthem to expect a certain distribution of feature in categorymembers e g correlation between 
a new general approach is described for approximate inference in first order probabilistic language using markov chain monte carlo mcmc technique in the space of concrete possible world underlying any given knowledge base the simplicity of the approach and it lazy construction of possible world make it possible to consider quite expressive language in particular we consider two extension to the basic relational probability model rpm defined by koller and pfeffer both of which have caused difficulty for exact algorithm the first extension deal with uncertainty about relation among object where mcmc sample over relational structure the second extension deal with uncertainty about the identity of individual where mcmc sample over set of equivalence class of object in both case we identify type of probability distribution that allow local decomposition of inference while encoding possible domain in a plausible way we apply our algorithm to simple example and show that the mcmc approach scale well 
in this paper we describe a systematic approach for creating a dialog management system based on a construct algebra a collection of relation and operation on a task representation these relation and operation are analytical component for building higher level abstraction called dialog motivator the dialog manager consisting of a collection of dialog motivator is entirely built using the construct algebra 
the principle of minimal change is prevalent in various guise throughout the development of area such a reasoning about action belief change and nonmonotonic reasoning recent literature ha witnessed the proposal of several theory of action that adopt an explicit representation of causality it is claimed that an explicit notion of causality is able to deal with the frame problem in a manner not possible with traditional approach based on minimal change however such claim remain untested by all but representative example it is our purpose here to objectively test these claim in an abstract sense to determine whether an explicit representation of causality is capable of providing something that the principle of minimal change is unable to capture working towards this end we provide a precise characterisation of the limit of applicability of minimal change 
a panorama for visual stereo consists of a pair of panoramic image where one panorama is for the left eye and another panorama is for the right eye a panoramic stereo pair provides a stereo sensation up to a full degree a stereo panorama cannot be photographed by two omnidirectional camera from two viewpoint it is normally constructed by mosaicing together image from a rotating stereo pair or from a single moving camera capturing stereo panoramic image by a rotating camera make it impossible to capture dynamic scene at video rate and limit stereo panoramic imaging to stationary scene this paper present two possibility for capturing stereo panoramic image using optic without any moving part a special mirror is introduced such that viewing the scene through this mirror creates the same ray a those used with the rotating camera such a mirror enables the capture of stereo panoramic movie with a regular video camera a lens for stereo panorama is also introduced the design of the mirror and of the lens are based on curve whose caustic is a circle 
humanoid robot are high dimensional movement system for which analytical system identification and control method are insufficient due to unknown nonlinearities in the system structure a a way out supervised learning method can be employed to create model based nonlinear controller which use function in the control loop that are estimated by learning algorithm however internal model for humanoid system are rather high dimensional such that conventional learning algorithm would suffer from slow learning speed catastrophic interference and the curse of dimensionality in this paper we explore a new statistical learning algorithm locally weighted projection regression lwpr for learning internal model in real time lwpr is a nonparametric spatially localized learning system that employ the le familiar technique of partial least square regression to represent functional relationship in a piecewise linear fashion the algorithm can work successfully in very high dimensional space and detect irrelevant and redundant input while only requiring a computational complexity that is linear in the number of input dimension we demonstrate the application of the algorithm in learning two classical internal model of robot control the inverse kinematics and the inverse dynamic of an actual seven degree of freedom anthropomorphic robot arm for both example lwpr can achieve excellent real time learning result from le than one hour of actual training data 
in this paper we discus the use of cascaded finite state transducer for machine translation a number of small dedicated transducer is applied to convert sentence pair from a bilingual corpus into generalized translation pattern these pattern together with the transducer are then used a a hierarchical translation memory for fully automatic translation result on the german english verbmobil corpus are given 
traditionally computer application to game domain have taken a brute force approach relying on sheer computational power to overcome the complexity of the domain although many of these program have been quite successful it is interesting to note that human can still perform extremely well against them thus we are compelled to ask if no human could match the computational power of most of these program are there method for learning and performance in game domain that more closely reflect human cognition in response to this question this paper attempt to model how human learn and play game by developing a backgammon playing algorithm based on cognition analysis of this algorithm show that it is efficient and commensurate with human ability suggesting that it provides a cognitively plausible theory of learning in backgammon 
we adopt stochastic game a a general framework for dynamicnoncooperative system this framework provides a way of describingthe dynamic interaction of agent in term of individual markovdecision process by studying this framework we go beyond thecommon practice in the study of learning in game which primarilyfocus on repeated game or extensive form game for stochasticgames with incomplete information we design a multiagent reinforcementlearning method which allows agent 
kernel are problem specific function that act a an interface between the learning system and the data while it is well known when the combination of two kernel is again a valid kernel it is an open question if the resulting kernel will perform well in particular in which situation can a combination of kernel be expected to perform better than it component considered separately intuitively one would like each of the two kernel to contribute information that is not available to the other this characterization hence must consider the data at hand both the kernel and also the task that is the information given by the label we investigate this problem by looking at the task of designing kernel for hypertext classification where both word and link information can be exploited firstly we introduce a novel kernel whose gram matrix is the well known co citation matrix from bibliometrics and demonstrate on real data that it ha a good performance then we study the problem of combining it with a standard bag of word kernel we provide sufficient condition that indicate when an improvement can be expected highlighting and formalising the notion of independent kernel experimental result confirm the prediction of the theory in the hypertext domain 
the problem of extracting continuous structure from noisy or cluttered image is a difficult one successful extraction depends critically on the ability to balance prior constraint on continuity and smoothness against evidence garnered from image analysis exact deterministic optimisation algorithm based on discretized functionals suffer from severe limitation on the form of prior constraint that can be imposed tractably this paper proposes a sequential monte carlo technique termed jetstream that enables constraint on curvature corner and contour parallelism to be mobilized all of which are infeasible under exact optimization the power of jetstream is demonstrated in two context interactive cut out in photo editing application and the recovery of road in aerial photograph 
recommender system use rating from user on item such a movie and music for the purpose of predicting the user preference on item that have not been rated prediction are normally done by using the rating of other user of the system by learning the user preference a a function of the feature of the item or by a combination of both these method in this paper we pose the problem a one of collaboratively learning of preference function by multiple user of the recommender system we study several mixture model for this task we show via theoretical analysis and experiment on a movie rating database how the model can be designed to overcome common problem in recommender system including the new user problem the recurring startup problem the sparse rating problem and the scaling problem 
this paper discus visual method that can be used to understand and interpret the result of classification using support vector machine svm on data with continuous real valued variable svm induction algorithm build pattern classifier by identifying a maximal margin separating hyperplane from training example in high dimensional pattern space or space induced by suitable nonlinear kernel transformation over pattern space svm have been demonstrated to be quite effective in a number of practical pattern classification task since the separating hyperplane is defined in term of more than two variable it is necessary to use visual technique that can navigate the viewer through high dimensional space we demonstrate the use of projection based tour method to gain useful insight into svm classifier with linear kernel on dimensional data 
in this paper we show how the use of hard constraint in solving estimation problem by allowing multiple source of information to be taken into account during optimization increase robustness and improves efficiency over alternative method such a the statistical combination of separate optimization result our argument is based on an empirical evaluation of the technique which us a model based optical flow constraint in a deformable model framework for tracking a face the flow constraint make the model toedge alignment optimization problem easier by projecting away the portion of the search space that optical flow make unlikely while a kalman filter is used to reconcile hard constraint with the uncertainty in the optical flow data using these hard constraint the system converges more quickly at each iteration and avoids local minimum in solution that cause other method to lose track we conjecture that this use of constraint will be effective in any integration application where there are disparity in the difficulty of computational problem associated with the use of different information source 
this work present an information retrieval model developed to deal with hyperlinked environment the model is based on belief network and provides a framework for combining information extracted from the content of the document with information derived from cross reference among the document the information extracted from the content of the document is based on statistic regarding the keywords in the collection and is one of the basis for traditional information retrieval ir ranking algorithm the information derived from cross reference among the document is based on link reference in a hyperlinked environment and ha received increased attention lately due to the success of the web we discus a set of strategy for combining these two type of source of evidential information and experiment with them using a reference collection extracted from the web the result show that this type of combination can improve the retrieval performance without requiring any extra information from the user at query time in our experiment the improvement reach up to in term of average precision figure 
an eigenvalue method is developed for analyzing periodic structure in speech signal are analyzed by a matrix diagonalization reminiscent of method for principal component analysis pca and independent component analysis ica our method called periodic component analysis ca us constructive interference to enhance periodic component of the frequency spectrum and destructive interference to cancel noise the front end emulates important aspect of auditory processing such a cochlear filtering nonlinear compression and insensitivity to phase with the aim of approaching the robustness of human listener the method avoids the inefficiency of autocorrelation at the pitch period it doe not require long delay line and it correlate signal at a clock rate on the order of the actual pitch a opposed to the original sampling rate we derive it cost function and present some experimental result 
we present a novel mixed state dynamic bayesian network dbn framework for modeling and classifying timeseries data such a object trajectory a hidden markov model hmm of discrete action is coupled with a linear dynamical system lds model of continuous trajectory motion this combination allows u to model both the discrete and continuous cause of trajectory such a human gesture the model is derived using a rich theoretical corpus from the bayesian network literature this allows u to use an approximate structured variational inference technique to solve the otherwise intractable inference of action and system state using the same dbn framework we show how to learn the mixed state model parameter from data experiment show that with high statistical confidence the mixed state dbns perform favorably when compared to decoupled hmm lds model on the task of recognizing human gesture made with a computer mouse 
we consider the question how much strong generative power can be squeezed out of a formal system without increasing it weak generative power and propose some theoretical and practical constraint on this problem we then introduce a formalism which under these constraint maximally squeeze strong generative power out of context free grammar finally we generalize this result to formalism beyond cfg 
we investigate the problem of learning a classification task on data represented in term of their pairwise proximity this representation doe not refer to an expli cit feature representation of the data item and is thus more general than the standard approach of using euclidean feature vector from which pairwise proximity can always be calculated our first approach is based on a combined linea r embedding and classification procedure resulting in an extension of the optimal hyperplane algorithm to pseudo euclidean data a an alternative we present another approach based on a linear threshold model in the proximity value themselves which is optimized using structural risk minimization we show that prior knowledge about the problem can be incorporated by the choice of distance measure and examine different metric w r t their genus lization finally the algorithm are successfully applie d to protein structure data and to data from the cat s cerebral cortex they show better performance than k nearestneighbor classification 
driven by the progress in the field of single trial analysis of eeg there is a growing interest in brain computer interface bcis i e system that enable human subject to control a computer only by mean of their brain signal in a pseudo online simulation our bci detects upcoming finger movement in a natural keyboard typing condition and predicts their laterality this can be done on average m before the respective key is actually pressed i e long before the onset of emg our approach is appealing for it short response time and high classification accuracy in a binary decision where no human training is involved we compare discriminative classifier like support vector machine svms and different variant of fisher discriminant that posse favorable regularization property for dealing with high noise case inter trial variablity 
in the present paper we introduce the notion of variable assignment problem vap a an abstract framework for characterizing diagnosis component of the system to be diagnosed are put in correspondence with variable behavioral mode of the component are the value of the variable and a diagnosis is a variable assignment which explains the observation of the diagnostic problem by considering the constraint put by the domain theory in order to have a concise representation of diagnosis and to reduce the search space we introduce the notion of scenario for representing a set of diagnosis the paper discus the definition of preference criterion for ranking solution and their use for guiding the heuristic search for diagnosis experimental data are reported for the evaluation of such a heuristic search on a real world diagnostic problem concerning the identification of fault in a space robot arm in this domain where a high number of diagnosis may be possible our approach allows one to get a concise representation of the large number of solution and to define effective diagnostic strategy able to provide relevant information about fault localization and identification 
the classification of human body motion is a difficult problem in particular the automatic segmentation of sequence containing more than one class of motion is challenging an effective approach is to use mixed discrete continuous state to couple perception with classification a spline contour is used to track the outline of the person we show that for a quasi periodic human body motion an autoregressive process is a suitable model for the contour dynamic this can then be used a a dynamical model for mixed statecondensation filtering switching automatically between different motion class we have developed partial importance sampling to enhance the efficiency of the mixed state condensation filter it is also shown here that the importance sampling can be done in linear time in place of the previous quadratic algorithm tying of discrete state is used to obtain further efficiency improvement automatic segmentation is demonstrated on video sequence of aerobic exercise performance is promising but there remains a residual misclassification rate and possible explanation for this are discussed 
this essay give advice to author of paperson machine learning although much of it carriesover to other computational discipline the issue covered include the material thatshould appear in a well balanced paper factorsthat arise in different approach to evaluation and way to improve a submission sability to communicate idea to it reader introductionalthough machine learning ha become a scientific discipline the effective communication of it idea remainsan 
we propose a new framework for calibrating parameter of energy functionals a used in image analysis the method learns parameter from a family of correct example and given a probabilistic construct for generating wrong example from correct one we introduce a measure of frustration to penalize case in which wrong response are preferred to correct one and we design a stochastic gradient algorithm which converges to parameter which minimize this measure of frustration we also present a rst set of experiment in this context and introduce extension to deal with data dependent energy 
we introduce and evaluate treedt a novel gene mapping method which is based on discovering and assessing tree like pattern in genetic marker data gene mapping aim at discovering a statistical connection from a particular disease or trait to a narrow region in the genome in a typical case control setting data consists of genetic marker typed for a set of disease associated chromosome and a set of control chromosome a computer scientist would view this data a a set of string treedt extract essentially in the form of substring and prefix tree information about the historical recombination in the population this information is used to locate fragment potentially inherited from a common diseased founder and to map the disease gene into the most likely such fragment the method measure for each chromosomal location the disequilibrium of the prefix tree of marker string starting from the location to ass the distribution of disease associated chromosome we evaluate experimentally the performance of treedt on realistic simulated data set and comparison to state of the art method tdt hpm show that treedt is very competitive 
in this article we describe a possibilistic probabilistic conditional planner called ptlplan being inspired by bacchus and kabanza s tlplan ptlplan is a progressive planner that us strategic knowledge encoded in a temporal logic to reduce it search space action effect and sensing can be context dependent and uncertain and the information the planning agent ha at each point in time is represented a a set of situation with associated possibility or probability besides presenting the planner itself it representation of action and plan and it algorithm we also provide some promising data from performance test 
we present an algorithm that extract curve from a set of edgels within a specific class in a decreasing order of their length the algorithm inherits the perceptual grouping approach but instead of using only local cue a global constraint is imposed to each extracted subset of edgels that the underlying curve belongs to a specific class in order to reduce the complexity of the solution we work with a linearly parameterized class of curve function of one image coordinate this allows first to use a recursive kalman based fitting and second to cast the problem a an optimal path search in an directed graph experiment on finding lane marking on road demonstrate that real time processing is achievable 
the most popular algorithm for object detection require the use of exhaustive spatial and scale search procedure in such approach an object is defined by mean of local feature in this paper we show that including contextual information in object detection procedure provides an efficient way of cutting down the need for exhaustive search we present result with real image showing that the proposed scheme is able to accurately predict likely object class location and size 
while an exact definition of texture is somewhat elusive texture can be qualitatively described a a distribution of color albedo or local normal on a surface in the literature the word texture is often used to describe a color or albedo variation on a smooth surface we refer to such texture a d texture in real world scene texture is often due to surface height variation and can be termed d texture because of local foreshortening and masking oblique view of d texture are not simple transformation of the frontal view consequently texture representation such a the correlation function or power spectrum are also affected by local foreshortening and masking this work present a correlation model for a particular class of d texture the model characterizes the spatial relationship among neighboring pixel in an image of d texture and the change of this spatial relationship with viewing direction 
abstract a critical component of applying machine learning algorithm is evaluating the per formance of the model induced and using the evaluation to guide further development traditionally the most common evaluation metric is error or loss however this provides very little information for the designer to use when constructing a system we argue that an evaluation method should provide detailed feedback on the performance of an algorithm and that this feedback should be in the lan guage of the problem our goal is to char acterize model error or the di erences be tween model in the feature space we pro vide a framework for this that allows di er ent algorithm to be used a the discovery engine and we consider two approach a classi cation strategy where we use a stan dard rule learner such a c a descriptive paradigm where we use a new discovery algo rithm a contrast set miner we show that c su er from several problem that make it unsuitable for this task 
this paper describes a system that us a camera and a point light source to track a user s hand in three dimension using depth cue obtained from projection of the hand and it shadow the system computes the d position and orientation of two finger thumb and pointing finger the system recognizes one dynamic and two static gesture recognition and pose estimation are user independent and robust the system operates at the rate of hz and can be used a an intuitive input interface 
the problem that we address in this paper is how a mobile robot can plan in order to arrive at it goal with minimum uncertainty traditional motion planning algorithm often assume that a mobile robot can track it position reliably however in real world situation reliable localization may not always be feasible partially observable markov decision process pomdps provide one way to maximize the certainty of reaching the goal state but at the cost of computational intractability for large state space the method we propose explicitly model the uncertainty of the robot s position a a state variable and generates trajectory through the augmented pose uncertainty space by minimizing the positional uncertainty at the goal the robot reduces the likelihood it becomes lost we demonstrate experimentally that coastal navigation reduces the uncertainty at the goal especially with degraded localization 
we propose a general framework for object tracking in video image it consists in low order parametric model for the image motion of a target region these model are used to predict the movement and to track the target the difference of intensity between the pixel belonging to the current region and the pixel of the selected target learnt during an off line stage allows a straightforward prediction of the region position in the current image the proposed algorithm allows to track in real time le than m any planar textured target under homographic motion this algorithm is very simple a few line of code and very efficient le than m on a mhz hardware 
one of the difficulty of color tracking is that color change in different lighting condition and static color model would be inadequate to capture the nonstationary color distribution over time although some work ha been done on adaptive color model this problem still need further investigation different from many other approach we formulate the nonstationary color tracking problem a a transductive learning problem in which the generalization of a trained color classifier is only defined on the pixel in a specific image rather than the whole color space this formulation offer a way to design and transduce color classifier through non stationary color distribution instead of assuming a color transition model we assume that some unlabeled pixel in a new image frame can be confidently labeled by a weak classifier according to a preset confidence level the proposed discriminant em d em algorithm offer an effective way to transduce color classifier a well a automatically select a good color space experiment show that d em successfully handle some problem in color tracking a a component our natural gesture interface this algorithm give tight bounding box of the hand or face region in video sequence 
hierarchical reinforcement learning rl is a general framework which study how to exploit the structure of action and task to accelerate policy learning in large domain prior work in hierarchical rl such a the maxq method ha been limited to the discrete time discounted reward semimarkov decision process smdp model this paper generalizes the maxq method to continuous time discounted and average reward smdp model we describe two hierarchical reinforcement learning algorithm continuous time discounted reward maxq and continuous time average reward maxq we apply these algorithm to a complex multiagent agv scheduling problem and compare their performance and speed with each other a well a several well known agv scheduling heuristic 
submitted to the ieee conference on computer vision and pattern recognition in this paper we describe a novel generative modelfor video analysis called the transformed hidden markovmodel thmm the video sequence is modeled a a set offrames generated by transforming a small number of classimages that summarize the sequence for each frame thetransformation and the class are discrete latent variablesthat depend on the previous class and transformation inthe sequence the set of 
within the framework of constraint satisfaction problem we propose a new scheme of cooperative parallel search the cooperation is realized by exchanging nogoods instantiation which can t be extended to a solution we associate a process with each solver and we introduce a manager of no good in order to regulate exchange of nogoods each solver run the algorithm forward checking with nogood recording we add to algorithm a phase of interpretation which limit the size of the search tree according to the received nogoods solver differ from each other in ordering variable and or value by using different heuristic the interest of our approach is shown experimentally in particular we obtain linear or superlinear speed up for consistent problem like for inconsistent one up to about ten solver 
in what sense is a grammar the union of it rule this paper adapts the notion of composition well developed in the context of programming language to the domain of linguistic formalism we study alternative definition for the semantics of such formalism suggesting a denotational semantics that we show to be compositional and fully abstract this facilitates a clear mathematically sound way for defining grammar modularity 
olshausen field demonstrated that a learning algorithm that attempt to generate a sparse code for natural scene develops a complete family of localised oriented bandpass receptive field similar to those of simple cell in v this paper describes an algorithm which find a sparse code for sequence of image that preserve information about the input this algorithm when trained on natural video sequence develops base representing the movement in particular direction with particular speed similar to the receptive field of the movement sensitive cell observed in cortical visual area furthermore in contrast to previous approach to learning direction selectivity the timing of neuronal activity encodes the phase of the movement so the precise timing of spike is crucially important to the information encoding 
we consider prediction model evaluation in the context of marketing campaign planning in order to evaluate and compare model with specific campaign objective in mind we need to concentrate our attention on the appropriate evaluation criterion these should portray the model s ability to score accurately and to identify the relevant target population in this paper we discus some applicable model evaluation and selection criterion their relevance for campaign planning their robustness under changing population distribution and their employment when constructing confidence interval we illustrate our result with a case study based on our experience from several project 
we apply support vector machine svms to identify english base phrase chunk svms are known to achieve high generalization performance even with input data of high dimensional feature space furthermore by the kernel principle svms can carry out training with smaller computational overhead independent of their dimensionality we apply weighted voting of svms based system trained with distinct chunk representation experimental result show that our approach achieves higher accuracy than previous approach 
existing algorithm for pronoun resolution typically cast the problem into a coreference task which mean they simply identify an antecedent noun phrase for each pronoun selection of the antecedent is usually based on a calculation of salience or focus this simplified approach is unable to account for pronoun without noun phrase antecedent example are abstract referent such a event proposition and speech act that might appear in the linguistic surface form a sentential complement verbal construction or entire sentence a well a consequence or outcome that don t appear in the surface form at all this paper contains a survey of current method of pronoun resolution for natural language understanding it then proposes a strategy for resolving pronominal reference to abstract entity that incorporates semantic information in addition to salience calculation preliminary experiment are described that show the strategy to perform well above baseline on a collection of spoken task oriented dialog 
clustering algorithm conduct a search through the space of possible organization of a data set in this paper we propose two type of instance level clustering constraint must link and cannot link constraint and show how they can be incorporated into a clustering algorithm to aid that search for three of the four data set tested our result indicate that the incorporation of surprisingly few such constraint can increase clustering accuracy while decreasing runtime we also investigate the relative eects of each type of constraint and nd that the type that contributes most to accuracy improvement depends on the behavior of the clustering algorithm without constraint 
this paper considers the problem of self calibration of a camera from an image sequence in the case where the camera s internal parameter most notably focal length may change the problem of camera self calibration from a sequence of image ha proven to be a difficult one in practice due to the need ultimately to resort to non linear method which have often proven to be unreliable in a stratified approach to self calibration a projective reconstruction is obtained first and this is successively refined first to an affine and then to a euclidean or metric reconstruction it ha been observed that the difficult step is to obtain the affine reconstruction or equivalently to locate the plane at infinity in the projective coordinate frame the problem is inherently non linear and requires iterative method that risk not finding the optimal solution the present paper overcomes this difficulty by imposing cheirality constraint to limit the search for the plane at infinity to a dimensional cubic region of parameter space it is then possible to carry out a dense search over this cube in reasonable time for each hypothesised placement of the plane at infinity the calibration problem is reduced to one of calibration of a nontranslating camera for which fast non iterative algorithm exist a cost function based on the result of the trial calibration is used to determine the best placement of the plane at infinity because of the simplicity of each trial speed of over trial per second are achieved on a mhz processor it is shown that this dense search allows one to avoid area of local minimum effectively and find global minimum of the cost function 
most work in statistical parsing ha focused on a single corpus the wall street journal portion of the penn treebank while this ha allowed for quantitative comparison of parsing technique it ha left open the question of how other type of text might aect parser performance and how portable parsing model are across corpus we examine these question by comparing result for the brown and wsj corpus and also consider which part of the parser s probability model are particularly tuned to the corpus on which it wa trained this lead u to a technique for pruning parameter to reduce the size of the parsing model 
categorial grammar ha traditionally used the calculus to represent meaning we present an alternative dependency based perspective on linguistic meaning and situate it in the computational setting this perspective is formalized in term of hybrid logic and ha a rich yet perspicuous propositional ontology that enables a wide variety of semantic phenomenon to be represented in a single meaning formalism finally we show how we can couple this formalization to combinatory categorial grammar to produce interpretation compositionally 
deductive mode estimation ha become an essential component of robotic space system like nasa s deep space probe future robot will serve a component of large robotic network monitoring these network will require modeling language and estimator that handle the sophisticated behavior of robotic component this paper introduces rmpl a rich modeling language that combine reactive programming construct with probabilistic constraint based modeling and that offer a simple semantics in term of hidden markov model hmms to support efficient realtime deduction we translate rmpl model into a compact encoding of hmms called probabilistic hierarchical constraint automaton phca finally we use these model to track a system s most likely state by extending traditional hmm belief update 
the choice of an svm kernel corresponds to the choice of a representation of the data in a feature space and to improve performance it should therefore incorporate prior knowledge such a known transformation invariance we propose a technique which extends earlier work and aim at incorporating invariance in nonlinear kernel we show on a digit recognition task that the proposed approach is superior to the virtual support vector method which previously had been the method of choice 
given explosive data trac in the world wide web www it is crucial to achieve the scalable performance of web server the overall performance and resource utilization can be improvedby spreading document request among a group ofweb server this lead to the design and implementationof distributed cooperative apache webserver in this paper we describe the unique feature ofthe system to migrate and replicate documentsamong cooperating server using 
the visual hull is a geometric tool which relates the d shape of a concave object to it silhouette or shadow this paper develops the theory of the visual hull of object bounded by smooth curved surface from the basic definition of visual hull we determine the surface which bound the visual hull of such object we show that these surface are patch of some surface which partition the viewpoint space of the aspect graph of the object the surface concerned are those generated by the visual event tangent crossing and triple point these ruled surface are analyzed for finding their active part i e the part which could actually bound the visual hull this analysis is based on the shape of the surface of the object at the tangency point of the ruled surface an algorithm for computing the visual hull of a smooth curved object is outlined which exploit the algorithm for computing it aspect graph 
this paper further develops aumann and lindell s proposal for a variant of association rule for which the consequent is a numeric variable it is argued that these rule can discover useful interaction with numeric data that cannot be discovered directly using traditional association rule with discretization alternative measure for identifying interesting rule are proposed efficient algorithm are presented that enable these rule to be discovered for dense data set for which application of auman and lindell s algorithm is infeasible 
study of image motion typically address motion category on a case by case basis example include a moving point a moving contour or a d optical flow field the typical assumption made in these study is that there is a unique velocity at each moving point in the image in this paper we relax this assumption we introduce a broader set of motion category in which the set of motion at a moving point can be d d or d we consider one new motion category in detail which we call optical snow this motion category occurs for example when an observer translates relative to a massively cluttered scene example include the motion seen by an observer moving through bush or falling snow seen by a stationary observer optical snow is characterized by a d set of velocity at each moving point and a such it cannot be analyzed using a classical computational method such a optical flow we introduce a technique for analyzing optical snow which is based on a bow tie signature of the motion in the frequency domain we demonstrate the effectiveness of the technique using both synthetic and real image sequence 
random projection have recently emerged a a powerful method for dimensionality reduction theoretical result indicate that the method preserve distance quite nicely however empirical result are sparse we present experimental result on using random projection a a dimensionality reduction tool in a number of case where the high dimensionality of the data would otherwise lead to burden some computation our application area are the processing of both noisy and noiseless image and information retrieval in text document we show that projecting the data onto a random lower dimensional subspace yield result comparable to conventional dimensionality reduction method such a principal component analysis the similarity of data vector is preserved well under random projection however using random projection is computationally significantly le expensive than using e g principal component analysis we also show experimentally that using a sparse random matrix give additional computational saving in random projection 
the paper describes a scheme for detecting vehicle in image the proposed method approximately model the unknown distribution of the image of vehicle by learning higher order statistic ho information of the vehicle class from sample image given a test image statistical information about the background is learnt on the y an ho based decision measure then classifies test pattern a vehicle or otherwise when tested on real image of aerial view of vehicular activity the method give good result even on complicated scene it doe not require any a priori information about the site however it is amenable to augmentation with contextual information the method can serve a an important step towards building an automated roadway monitoring system 
classifying an unknown input is a fundamental problem in pattern recognition one standard method is finding it nearest neighbor in a reference set it would be very time consuming if computed feature by feature for all template in the reference set this naive method is o nd where n is the number of template in the reference set and d is the number of feature or dimension for this reason we present a technique for quickly eliminating most template from consideration a possible neighbor the remaining candidate template are then evaluated feature by feature against the query vector we utilize frequency of feature a a pre processing to reduce query processing time burden the most notable advantage of the new method over other existing technique occurs where the number of feature is large and the type of each feature is binary although it work for other type feature we improved our ocr system by at least a factor of without a threshold or faster with higher threshold value 
in this paper a simple classification modelbased on a linguistic processing that producessyntactic information i e grammatical categoriesof word in document is described moreover an experimental set up able to dynamicallysupport several test of different approacheshas been realized in order to get largescale empirical evidence the evidence forthis are discussed with comparative evaluationagainst other alternative more complex model definition of the problem 
we introduce five method for summarizing part of web page on handheld device such a personal digital assistant pda or cellular phone each web page is broken into text unit that can each be hidden partially displayed made fully visible or summarized the method accomplish summarization by different mean one method extract significant keywords from the text unit another attempt to find each text unit s most significant sentence to act a a summary for the unit we use information retrieval technique which we adapt to the world wide web context we tested the relative performance of our five method by asking human subject to accomplish single page information search task using each method we found that the combination of keywords and single sentence summary provides significant improvement in access time and number of pen action a compared to other scheme 
despite it popularity for general clustering k mean suffers three major shortcoming it scale poorly computationally the numberof cluster k ha to be supplied by theuser and the search is prone to local minimum we propose solution for the first twoproblems and a partial remedy for the third building on prior work for algorithmic accelerationthat is not based on approximation we introduce a new algorithm that efficiently search the space of cluster location and 
we present new simulation result in which a computational model of interacting visual neuron simultaneously predicts the modulation of spatial vision threshold by focal visual attention for five dual task human psychophysics experiment this new study complement our previous finding that attention activates a winner take all competition among early visual neuron within one cortical hypercolumn this quot intensified competition quot hypothesis assumed that attention equally affect all 
this paper present an approach to automatically build a semantic perceptron net spn for topic spotting it us context at the lower layer to select the exact meaning of key word and employ a combination of context co occurrence statistic and thesaurus to group the distributed but semantically related word within a topic to form basic semantic node the semantic node are then used to infer the topic within an input document experiment on reuters data set demonstrate that spn is able to capture the semantics of topic and it performs well on topic spotting task 
output coding is a general method for solving multiclass problem by reducing them to multiple binary classification problem previous research on output coding ha employed almost solely predefined discrete code we describe an algorithm that improves the performance of output code by relaxing them to continuous code the relaxation procedure is cast a an optimization problem and is reminiscent of the quadratic program for support vector machine we describe experiment with the proposed algorithm comparing it to standard discrete output code the experimental result indicate that continuous relaxation of output code often improve the generalization performance especially for short code 
we present a new formulation of sequential least square applied to scene and motion reconstruction from image feature we argue that recursive technique will become more important both for real time control application and also interactive vision application the aim is to approximate a well a possible the result of the batch bundle adjustment method in previously published work we described an algorithm which work well if the same feature are visible throughout the sequence here we show how to deal with new feature in a way that avoids deterioration of the result the main theoretical advance here is showing how to adjust the system information matrix when scene camera parameter are removed from the reconstruction we show how this procedure affect the sparseness of the information matrix and thus how to achieve an efficient recursive solution to the reconstruction problem 
based on the document centric view of xml we present the query language xirql current proposal for xml query language lack most ir related feature which are weighting and ranking relevance oriented search datatypes with vague predicate and semantic relativism xirql integrates these feature by using idea from logic based probabilistic ir model in combination with concept from the database area for processing xirql query a path algebra is presented that also serf a a starting point for query optimization 
in this paper we compare the relative effect of segment order segmentation and segment contiguity on the retrieval performance of a translation memory system we take a selection of both bag of word and segment order sensitive string comparison method and run each over both character and word segmented data in combination with a range of local segment contiguity model in the form of n gram over two distinct datasets we find that indexing according to simple character bigram produce a retrieval accuracy superior to any of the tested word n gram model further in their optimum configuration bag of word method are shown to be equivalent to segment order sensitive method in term of retrieval accuracy but much faster we also provide evidence that our finding are scalable 
two basic problem in image interpretation are a determining which interpretation are the most plausible amongst many possibility and b controlling the search for plausible interpretation we address these issue using a bayesian approach with the plausibility ordering and search pruning based on the posterior probability of interpretation however due to the need for detailed quantitative prior probability and the need to evaluate complex integral over various conditional distribution a full bayesian approach is currently impractical except in tightly constrained domain to circumvent these difficulty we introduce the notion of qualitative probabilistic analysis in particular given spatial and contrast resolution parameter we consider only the asymptotic order of the posterior probability for any interpretation a these resolution are made finer we introduce this approach for a simple card world domain and present computational result for block world image 
document filtering is a task to retrieve document relevant to a user s profile from a flow of document generally filtering system calculate the similarity between the profile and each incoming document and retrieve document with similarity higher than a threshold however many system set a relatively high threshold to reduce retrieval of non relevant document which result in the ignorance of many relevant document in this paper we propose the use of a non relevant information profile to reduce the mistaken retrieval of non relevant document result from experiment show that this filter ha successfully rejected a sufficient number of non relevant document resulting in an improvement of filtering performance 
this paper introduces a statistical model for query relevant summarization succinctly characterizing the relevance of a document to a query learning parameter value for the proposed model requires a large collection of summarized document which we do not have but a a proxy we use a collection of faq frequently asked question document taking a learning approach enables a principled quantitative evaluation of the proposed system and the result of some initial experiment on a collection of usenet faq and on a faq like set of customer submitted question to several large retail company suggest the plausibility of learning for summarization 
we propose in this paper a new d fully parallelthinning algorithm that we believe to be the most concisedue to it simple characterization the algorithmis indeed completely dened by a set of ve pattern three removing condition and two non removing condition these pattern are designed from the two fundamentaland compatible constraint usually expectedin skeleta topology preservation and medialsurface from these two constraint the removing pattern and 
a few research g roups are now proposing a series of step and methodology for developing ontology however mainly due to the fact t hat ontological engineering is still a relatively immature discipline each work group employ it own methodology our goal is to present the most representative methodology used in ontology development and to perform an analysis of such methodology against the same framework of reference so the goal of this paper is not to provide new insight about methodology but to put it all i n one place and help people to select which methodology to use 
existing software system for automated essay scoring can provide nlp researcher with opportunity to test certain theoretical hypothesis including some derived from centering theory in this study we employ ets s e rater essay scoring system to examine whether local discourse coherence a defined by a measure of rough shift transition might be a significant contributor to the evaluation of essay our positive result indicate that rough shift do indeed capture a source of incoherence one that ha not been closely examined in the centering literature these result not only justify rough shift a a valid transition type but they also support the original formulation of centering a a measure of discourse continuity even in pronominal free text 
automatic albuming the automatic organization of photograph either a an end in itself or for use in other application is an application that promise to be of great assistance to photographer relatively sophisticated image content analysis technique have been used for image indexing organization and retrieval in this paper we describe a method of organizing photograph into event using spoken photograph caption the result of this process can be used to improve image indexing and retrieval 
this paper examines the extent to which verb diathesis alternation are empirically attested in corpus data we automatically acquire alternating verb from large balanced corpus by using partialparsing method and taxonomic information and discus how corpus data can be used to quantify linguistic generalization we estimate the productivity of an alternation and the typicality of it member using type and token frequency 
in the present work we examine the causal theory of action put forward by mccain and turner mccain and turner for determining ramification our principal aim is to provide a characterisation of this causal theory of action in term of a shoham like preferential semantics shoham this would have a twofold advantage it would place mccain and turner s theory in perspective allowing a comparison with other logic of action and it would allow u to glean further insight into the nature of causality underlying their work we begin by showing that our aim is not attainable by a preferential mechanism alone at this point we do not abandon preferential semantics altogether but augment it in order to arrive at the desired result we draw the following moral which is at the heart of our paper two component minimal change under a preferential structure and causality are required to provide a concise solution to the frame and ramification problem 
instead of a standard support vector machine svm that classifies point by assigning them to one of two disjoint half space point are classified by assigning them to the closest of two parallel plane in input or feature space that are pushed apart a far a possible this formulation which can also be interpreted a regularized least square and considered in the much more general context of regularized network lead to an extremely fast and simple algorithm for generating a linear or nonlinear classifier that merely requires the solution of a single system of linear equation in contrast standard svms solve a quadratic or a linear program that require considerably longer computational time computational result on publicly available datasets indicate that the proposed proximal svm classifier ha comparable test set correctness to that of standard svm classifier but with considerably faster computational time that can be an order of magnitude faster the linear proximal svm can easily handle large datasets a indicated by the classification of a million point attribute set in second all computational result are based on line of matlab code 
we propose a new approach to the problem of searching a space of stochastic controller for a markov decision process mdp or a partially observable markov decision process pomdp following several other author our approach is based on searching in parameterized family of policy for example via gradient descent to optimize solution quality however rather than trying to estimate the value and derivative of a policy directly we do so indirectly using estimate for the probability density that the policy induces on state at the different point in time this enables our algorithm to exploit the many technique for efficient and robust approximate density propagation in stochastic system we show how our technique can be applied both to deterministic propagation scheme where the mdp s dynamic are given explicitly in compact form and to stochastic propagation scheme where we have access only to a generative model or simulator of the mdp we present empirical result for both of these variant on complex problem 
named entity ne recognition is a task in which proper noun and numerical information in a document are detected and classified into category such a person organization location and date ne recognition play an essential role in information extraction system and question answering system it is well known that hand crafted system with a large set of heuristic rule are difficult to maintain and corpus based statistical approach are expected to be more robust and require le human intervention several statistical approach have been reported in the literature in a recent japanese ne workshop a maximum entropy me system outperformed decision tree system and most hand crafted system here we propose an alternative method based on a simple rule generator and decision tree learning our experiment show that it performance is comparable to the me approach we also found that it can be trained more efficiently with a large set of training data and that it improves readability 
we present a new method to shape based segmentation of deformable anatomical structure in medical image and validate this approach by detecting and tracking the endocardial border in an echographic image sequence to this end a global prior knowledge of the endocardial contour is captured by a prototype template with a set of admissible deformation to take into account it inherent natural variability over time in this approach the data likelihood model rely on an accurate statistical modeling of the grey level distribution of each class present in the image the parameter of this distribution mixture are given by a preliminary estimation step which take into account the distribution shape of each class then the tracking problem is stated in a bayesian framework where it end up a an optimization problem this one is then efficiently solved by a genetic algorithm combined with a steepest ascent procedure this technique ha been successfully applied on synthetic image and on a real echocardiographic image sequence 
in this paper we consider the projection task determining what doe or doe not hold after performing a sequence of action in a general setting where a solution to the frame problem may or may not be available and where online information from sensor may or may not be applicable we formally characterize the projection task for action theory of this sort and show how a generalized form of regression produce correct answer whenever it can be used we characterize condition on action theory sequence of action and sensing information that are sufficient to guarantee that regression can be used and present a provably correct regressionbased procedure in prolog for performing the task under these condition 
consider the situation of a monocular image sequence with known ego motion observing a d point moving simultaneously but along a path of up to second order i e it can trace a line in d or a conic shaped path we wish to reconstruct the d path from the projection of the tangent to the path at each time instance this problem is analogue to the trajectory triangulation of line and conic section recently introduced in but instead of observing a point projection we observe a tangent projection and thus obtain a far simpler solution to the problem we show that the d path can be solved in a natural manner and linearly using degenerate quadric envelope specifically the disk quadric our approach work seamlessly with both linear and second order path thus there is no need to know in advance the shape of the path a with the previous approach for which line and conic were treated a distinct our approach is linear in both straight line and conic path unlike the non linear solution associated with point trajectory we provide experiment that show that our method behaves extremely well on a wide variety of scenario including those with multiple moving object along line and conic shaped path 
news report are an important source of information about society their analysis allows to understand it current interest and to measure the social importance and influence of different event in this paper we focus on the study of a very common phenomenon of news the influence of the peak news topic on other current news topic we propose a text mining method to analyze such influence we differentiate between the observable association those discovered from the newspaper and the real world association and propose a technique in which the real one can be i nferred from the observable one we argue that the discovery of the ephemeral association can be translated into knowledge about interest of society and social behavior 
identifying and classifying personal geographic institutional or other name in a text is an important task for numerous application this paper describes and evaluates a language independent bootstrapping algorithm based on iterative learning and re estimation of contextual and morphological pattern captured in hierarchically smoothed trie model the algorithm learns from unannotated text and achieves competitive performance when trained on a very short labelled name list with no other required language specific information tokenizers or tool 
this paper describes a probabilistic multiple hypothesis framework for tracking highly articulated object in this framework the probability density of the tracker state is represented a a set of mode with piecewise gaussians characterizing the neighborhood around these mode the temporal evolution of the probability density is achieved through sampling from the prior distribution followed by local optimization of the sample position to obtain updated mode this method of generating hypothesis from state space search doe not require the use of discrete feature unlike classical multiple hypothesis tracking the parametric form of the model is suited for high dimensional state space which cannot be efficiently modeled using non parametric approach result are shown for tracking fred astaire in a movie dance sequence 
a very promising approach for integrating top down and bottom up proof search is the use of bottom up generated lemma in top down provers when generating lemma however the currently used lemma generation procedure suffer from the well known problem of forward reasoning method e g the proof goal is ignored in order to overcome these problem we propose two relevancy based lemma generation method for top down provers the first approach employ a bottom up level saturation procedure controlled by top down generated pattern which represent promising subgoals the second approach us evolutionary search and provides a self adaptive control of lemma generation and goal decomposition 
dual estimation refers to the problem of simultaneously estimating thestate of a dynamic system and the model which give rise to the dynamic algorithm include expectation maximization em dual kalmanfiltering and joint kalman method these method have recently beenexplored in the context of nonlinear modeling where a neural networkis used a the functional form of the unknown model typically an extendedkalman filter ekf or smoother is used for the part of the algorithm 
dual estimation refers to the problem of simultaneously estimating the state of a dynamic system and the model which give rise to the dynamic algorithm include expectation maximization em dual kalman filtering and joint kalman method these method have recently been explored in the context of nonlinear modeling where a neural network is used a the functional form of the unknown model typically an extended kalman filter ekf or smoother is used for the part of the algorithm that estimate the clean state given the current estimated model an ekf may also be used to estimate the weight of the network this paper point out the flaw in using the ekf and proposes an improvement based on a new approach called the unscented transformation ut a substantial performance gain is achieved with the same order of computational complexity a that of the standard ekf the approach is illustrated on several dual estimation method 
the human figure exhibit complex and rich dynamic behavior that is both nonlinear and time varying effective model of human dynamic can be learned from motion capture data using switching linear dynamic system slds model we present result for human motion synthesis classification and visual tracking using learned slds model since exact inference in slds is intractable we present three approximate inference algorithm and compare their performance in particular a new variational inference algorithm is obtained by casting the slds model a a dynamic bayesian network classification experiment show the superiority of slds over conventional hmm s for our problem domain in this paper we present a framework for slds learning and apply it to figure motion modeling we derive three different approximate inference scheme viterbi variational and gpb we apply learned motion model to three task classification motion synthesis and visual tracking our result include an empirical comparison between slds and hmm model on classification and one step ahead prediction task the slds model class consistently outperforms standard hmms even on fairly simple motion sequence our result suggest that slds model are a promising tool for figure motion analysis and could play a key role in application such a gesture recognition visual surveillance and computer animation in addition this paper provides a summary of approximate inference technique which is lacking in the previous literature on slds furthermore our variational inference algorithm is novel and it provides another example of the benefit of interpreting classical statistical model a mixed state graphical model 
allen s well known interval algebra ha been developed for temporal representation and reasoning but there are also interesting spatial application where interval can be used a prototypical example are traffic scenario where car and their region of influence can be represented a interval on a road a the underlying line there are several difference of temporal and spatial interval which have to be considered when developing a spatial interval algebra in this paper we analyze the first important difference a opposed to temporal interval spatial interval can have an intrinsic direction with respect to the underlying line we develop an algebra for qualitative spatial representation and reasoning about directed interval identify tractable subset and show that path consistency is sufficient for deciding consistency for a particular subset which contains all base relation 
in this paper we present a new method for vision based reactive robot navigation that enables a robot to move in the middle of the free space by exploiting both central and peripheral vision the robot employ a forward looking camera for central vision and two side looking camera for sensing the periphery of it visual field the developed method combine the information acquired by this trinocular vision system and produce low level motor command that keep the robot in the middle of the free space the approach follows the purposive vision paradigm in the sense that vision is not studied in isolation but in the context of the behavior that the system is engaged a well a the environment and the robot s motor capability it is demonstrated that by taking into account these issue vision processing can be drastically simplified still giving rise to quite complex behavior the proposed method doe not make strict assumption about the environment requires very low level information to be extracted from the image produce a robust robot behavior and is computationally efficient result obtained by bath simulation and from a prototype on line implementation demonstrate the effectiveness of the method 
in this work we introduce an interactive part ip model a an alternative to hidden markov model hmms we tested both model on a database of on line cursive script we show that implementation of hmms and the ip model in which all letter are assumed to have the same average width give comparable result however in contrast to hmms the ip model can handle duration modeling without an increase in computational complexity 
we present a new technique for question answering called predictive annotation predictive annotation identifies potential answer to question in text annotates them accordingly and index them this technique along with a complementary analysis of question passage level ranking and answer selection produce a system effective at answering natural language fact seeking question posed against large document collection experimental result show the effect of different parameter setting and lead to a number of general observation about the question answering problem 
we use cluster analysis a a unifying principle for problem from low middle and high level vision the clustering problem is viewed a graph partitioning where node represent data element and the weight of the edge represent pairwise similarity our algorithm generates sample of cut in this graph by using david karger s contraction algorithm and computes an ave rage cut which provides the basis for our solution to the clustering problem the stochastic nature of our method make it robust against noise including accidental edge and small spurious cluster the complexity of our algorithm is very low for object and a fixed accuracy level without additional computational cost our algorithm provides a hierarchy of nested partition we demonstrate the superiority of our method for image segmentation on a few real color image our second application includes the concatenation of edge in a cluttered scene perceptual grouping where we show that the same clustering algorithm achieves a good a grouping if not better a more specialized method tween the visual entity the affinity is a function of the relevant attribute low level attribute may be the spatial location intensity level color composition or filter response of a pixel in the image mid level attribute in the case of edge element may be spatial location orientation or curvature and the affinity associated with them may reflect property such a proximity symmetry co circuitry and good continuity high level attribute may be a complex a the entire shape of an object in the scene or the color distribution of all the pixel in an image the second stage in this approach follows the unifying principle and applies cluster analysis to the organizion of the visual object pixel edgels image into coherent group these group reflect internal structure among the entity where roughly speaking the affinity within group is larger than the affinity between group therefore a cluster of pixel in the image sharing similar location and color is expected to account for an object or a part of an object in the scene a cluster of edge element is expected to exhibit a meaningful aggregation into a complete edge and a cluster of image in a database is expected to be related with a common topic we present in section our stochastic pairwise clustering algorithm it is an efficient robust and model free pairwise hierarchical algorithm see also the robust 
we formulate the problem of retrieving image from visual database a a problem of bayesian inference this lead to natural and effective solution for two of the most challenging issue in the design of a retrieval system providing support for region based query without requiring prior image segmentation and accounting for user feedback during a retrieval session we present a new learning algorithm that relies on belief propagation to account for both positive and negative example of the user s interest 
we develop an intuitive geometric framework for support vector regression svr by examining when tube exist we show that svr can be regarded a a classication problem in the dual space hard and soft tube are constructed by separating the convex or reduced convex hull respectively of the training data with the response variable shifted up and down by a novel svr model is proposed based on choosing the max margin plane between the two shifted datasets maximizing the margin corresponds to shrinking the eectiv e tube in the proposed approach the eects of the choice of all parameter become clear geometrically 
both probabilistic context free grammar pcfgs and shift reduce probabilistic pushdown automaton ppdas have been used for language modeling and maximum likelihood parsing we investigate the precise relationship between these two formalism showing that while they define the same class of probabilistic language they appear to impose different inductive bias 
we replace the commonly used gaussian noise model in nonlinear regression by a moreexible noise model based on the student tdistribution the degree of freedom of the t distribution can be chosen such that a special case either the gaussian distribution or the cauchy distribution are realized the latter is commonly used in robust regression since the t distribution can be interpreted a being an in nite mixture of gaussians parameter and hyperparameters such a the degree of freedom of the t distribution can be learned from the data based on an em learning algorithm we show that modeling using the t distribution lead to improved predictor on real world data set in particular if outlier are present the t distribution is superior to the gaussian noise model in effect by adapting the degree of freedom the system can learn to distinguish between outlier and non outlier especially for online learning task one is interested in avoiding inappropriate weight change due to measurement outlier to maintain stable online learning capability we show experimentally that using the t distribution a a noise model lead to stable online learning algorithm and outperforms state of the art online learning method like the extended kalman lter algorithm 
a fundamental problem with the modeling of chaotic time series data is that minimizing short term prediction error doe not guarantee a match between the reconstructed attractor of model and experiment we introduce a modeling paradigm that simultaneously learns to short term predict and to locate the outline of the attractor by a new way of nonlinear principal component analysis closed loop prediction are constrained to stay within these outline to prevent divergence from the attractor learning is exceptionally fast parameter estimation for the sample laser data from the santa fe time series competition took le than a minute on a mhz pentium pc 
reinforcement learning is often done using parameterized function approximators to storevalue function algorithm are typically developed for lookup table and then appliedto function approximators by using backpropagation this can lead to algorithmsdiverging on very small simple mdps and markov chain even with linear functionapproximators and epoch wise training these algorithm are also very difficult toanalyze and difficult to combine with other algorithm a series of new 
in this paper we consider nite mdps withfatal state we dene the risk under a policyas the probability of entering a fatal state which is dierent to the notion of risk normallyused in dp and rl most often regardingthe variance of the return we considerthe problem of nding optimal policieswith bounded risk i e where the risk issmaller than some user specied threshold and formalize it a a constrained mdp withtwo innite horizon criterion a discountedone for 
we present a new on line scheme for the recognition and pose estimation of a large isolated d object which may not entirely fit in a camera s field of view we do not assume any knowledge of the internal parameter of the camera or their constancy we use a probabilistic reasoning framework for recognition and next view planning we show result of successful recognition and pose estimation even in case of a high degree of interpretation ambiguity associated with the initial view 
we present a live web defect detection and classification system that run on a laptop pc with an attached camera feature invariant with respect to rotation and translation based on local integration are extracted from the grabbed image they are then presented to a neural network for classification 
abstract this paper describes a new methodfortracking of a human body in d motion by using constraint imposed on the body from the scene an image basedapproach for tracking exclusively us a geometrical model of the human body since the model usually ha a large number of degree of freedom dof a chancetobe corrupted by noise increase during the tracking process and the tracking may fall in an ill posedproblem tocope with this problem we pay our attention to that a human body can not move 
we investigate the generalization performance of some learning problem in hilbert functional space we introduce a notion of convergence of the estimated functional predictor to the best underlying predictor and obtain an estimate on the rate of the convergence this estimate allows u to derive generalization bound on some learning formulation 
the end user of medical digital library need quick access to information that is specic to the patient under their care we present a summarization system that nd and extract result from multiple medical journal article returned by a search lters result that match the patient and merges and order the remaining fact for the summary our approach feature an integration of text categorization information extraction information fusion and text reformulation for the summarization task 
in this paper we give necessary and sucient condition under which kernel of dot product type k x y k x y satisfy mercer s condition and thus may be used in support vector machine svm regularization network rn or gaussian process gp in particular we show that if the kernel is analytic i e can be expanded in a taylor series all expansion coecients have to be nonnegative we give an explicit functional form for the feature map by calculating it eigenfunctions and eigenvalue 
this article is an edited transcript of a lecture given at ijcai stockholm sweden on august the article summarizes concept principle and tool that were found useful in application involving causal modeling the principle are based on structural model semantics in which functional or counterfactua relationship representing autonomous physical process are the fundamental building block the article present the conceptual basis of this semantics illustrates it application in simple problem and discus it ramification to computational and cognitive problem concerning causation 
in previous work on transformed mixture of gaussians and transformed hidden markov model we showed how the em algorithm in a discrete latent variable model can be used to jointly normalize data e g center image pitch normalize spectrogram and learn a mixture model of the normalized data the only input to the algorithm is the data a list of possible transformation and the number of cluster to nd the main criticism of this work wa that the exhaustive computation of the posterior probability over transformation would make scaling up to large feature vector and large set of transformation intractable here we describe how a tremendous speed up is acheived through the use of a variational technique for decoupling transformation and a fast fourier transform method for computing posterior probability for n n image learning c cluster under n rotation n scale n x translation and n y translation take only c logn n scalar operation per iteration in contrast the original algorithm take cn operation to account for these transformation we give result on learning a component mixture model from a video sequence with frame of size the model account for rotation and translation each iteration of em take only second per frame in matlab which is over million time faster than the original algorithm 
this paper describes a face detection framework that is capable of processing image extremely rapidly while achieving high detection rate there are three key contribution the first is the introduction of a new image representation called the integral image which allows the feature used by our detector to be computed very quickly the second is a simple and efficient classifier which is built using the adaboost learning algorithm freund and schapire to select a small number of critical visual feature from a very large set of potential feature the third contribution is a method for combining classifier in a cascade which allows background region of the image to be quickly discarded while spending more computation on promising face like region a set of experiment in the domain of face detection is presented the system yield face detection performance comparable to the best previous system sung and poggio rowley et al schneiderman and kanade roth et al implemented on a conventional desktop face detection proceeds at frame per second 
ing the sequence causally the pose ofthe virtual object at time t is decided based on observationsof the sequence up to time t this enables a whole newthis research is supported in part by intel grant figure once the camera motion and the scene structure areestimated we insert a quot virtual quot vase into the scene a it can be observed it relative position is fixed within the scene four renderedviews corresponding to the snapshot in figure are shown range of 
we introduce an annotation scheme for temporal expression and describe a method for resolving temporal expression in print and broadcast news the system which is based on both hand crafted and machine learnt rule achieves an accuracy f measure against hand annotated data some initial step towards tagging event chronology are also described 
this paper investigates several refinement of a generic tabular parser for tree adjoining grammar the resulting parser is simpler and more efficient in practice even though the worst case complexity is not optimal 
for grey value image it is well accepted that the neighborhood rather than the pixel carry the geometrical interpretation interestingly the spatial configuration of the neighborhood is the basis for the perception of human common practise in color image processing is to use the color information without considering the spatial structure we aim at a physical basis for the local interpretation of color image we propose a framework for spatial color measurement based on the gaussian scale space theory we consider a gaussian color model which inherently us the spatial and color information in an integrated model the framework is well founded in physic a well a in measurement science the framework delivers sound and robust spatial color invariant feature the usefulness of the proposed measurement framework is illustrated by edge detection where edge are discriminated a shadow highlight or object boundary other application of the framework include color invariant image retrieval and color constant edge detection 
a multiagent environment become more prevalent we need to understand how this change the agent based paradigm one aspect that is heavily affected by the presence of multiple agent is learning traditional learning algorithm have core assumption such a markovian transition which are violated in these environment yet understanding the behavior of learning algorithm in these domain is critical singh kearns and mansour examine gradient ascent learning specifically within a restricted class of repeated matrix game they prove that when using this technique the average of expected payoff over time converges on the other hand they also show that neither the player strategy nor their expected payoff themselves are guaranteed to converge in this paper we introduce a variable learning rate for gradient ascent along with the wolf win or learn fast principle for regulating the learning rate we then prove that this modification to gradient ascent ha the stronger notion of convergence that is strategy and payoff converge to a nash equilibrium 
this paper discus some trend in model based diagnosis we consider some recent application and discus why they were possible the lesson we learned from them the new impulse that they gave to research in the field and the new challenge that emerged from them 
existing sequential feature based registration algorithm involving search typically either select feature randomly e g the ransac approach m fischler and r bolles or assume a predefined intuitive ordering for the feature e g based on size or resolution the paper present a formal framework for computing an ordering for feature which maximizes search efficiency feature are ranked according to matching ambiguity measure and an algorithm is proposed which couple the feature selection with the parameter estimation resulting in a dynamic feature ordering the analysis is extended to template feature where the matching is non discrete and a sample refinement process is proposed the framework is demonstrated effectively on the localization of a person in an image using a kinematic model with template feature different prior are used on the model parameter and the result demonstrate nontrivial variation in the optimal feature hierarchy 
we developed and then evaluated a music information retrieval mir system based upon the interval found within the melody of a collection of folksong the song were converted to an interval only representation of monophonic melody and then fragmented t into length n subsection called n gram the length of these n gram and the degree to which we precisely represent the interval are variable analyzed in this paper we constructed a collection of musical word database using the text based smart information retrieval system a group of simulated query some of which contained simulated error wa run against these database the result were evaluated using the normalized precision and normalized recall measure our concept of musical word show great merit thus implying that useful mir system can be constructed simply and efficiently using pre existing text based information retrieval software second this study is a formal and comprehensive evaluation of a mir system using rigorous statistical analysis to determine retrieval effectiveness 
many algorithm for approximate reinforcement learning are notknown to converge in fact there are counterexample showingthat the adjustable weight in some algorithm may oscillate withina region rather than converging to a point this paper show that for two popular algorithm such oscillation is the worst that canhappen the weight cannot diverge but instead must convergeto a bounded region the algorithm are sarsa and v thelatter algorithm wa used in the 
end user base the relevance judgement of the searched document on the expected contribution to their task of the information contained in the document there is a shortage of study analyzing the relationship between the experienced contribution relevance assessment and type of information initially sought this study categorizes the type of information in document being used in writing a research proposal for a master s thesis by eleven student throughout the various stage of the proposal writing process the role of the specificity of the searched information in influencing it contribution is analyzed the result demonstrate that different type of information are sought at different stage of the writing process and thus the contribution of the information also differs at the different stage the category of the contributing information can be understood of topicality 
if all feature causing heterogeneity were observed a mixture of expert approach jacob et al is likely to be superior to using a single model when unobserved or very noisy spatial feature are the cause for the heterogeneity the observed feature space of homogeneous subset can highly overlap leading to a biased global model or biased mixture of expert our goal is to allow more accurate prediction in such situation here a supervised machine learning algorithm for the analysis of heterogeneous spatial data is proposed it is based on partitioning the data set into more homogeneous region by competition of regression model linear or nonlinear the algorithm start from learning a global model and add new model into the competition until each model becomes specialized for one of the region the competition convergence is proven theoretically also the influence of filtering the competing model residual for improving convergence speed and accuracy is discussed a number of experiment on artificial and real life spatial data are performed to validate some aspect of the algorithm and to illustrate it potential application the obtained result provide strong evidence that homogeneous region can be identified with high accuracy by using the proposed approach even when their observed feature space highly overlap an assumption of data independence valid for most of standard machine learning data set is often unrealistic for spatial variable whose dependence is strongly tied to a location where observation spatially close to each other are more likely to be similar than observation widely separated in space a a consequence error of spatial prediction model are also spatially correlated cressie the method proposed here incorporates knowledge of spatial correlation for more accurate partitioning of heterogeneous spatial data set the new partitioning algorithm is based on three important mechanism a competition among learning model for spatial data point b averaging error of each competing model over neighboring data point and c an incremental introduction of additional model into the competition when needed 
several recent stochastic parser use bilexical grammar where each word type idiosyncratically prefers particular complement with particular head word we present o n parsing algorithm for two bilexical formalism improving the prior upper bound of o n for a common special case that wa known to allow o n parsing eisner we present an o n algorithm with an improved grammar constant 
neural logic network or neulonet is a hybrid of neural network expert system it strength lie in it ability to learn and to represent human logic in decision making using component net rule the technique originally employed in neulonet learning is backpropagation however the resulting weight adjustment will lead to a loss in the logic of the net rule a new technique is now developed that allows the neulonet to learn by composing net rule using genetic programming this paper present experimental result to demonstrate this new and exciting capability in capturing human decision logic from example comparison will also be made between the use of net rule and the use of standard boolean logic of negation disjunction and conjunction in evolutionary computation 
we present a framework for tracking rigid object based on an adaptive bayesian recognition technique that incorporates dependency between object feature at each frame we find a maximum a posteriori map estimate of the object parameter that include positioning and configuration of non occluded feature this estimate may be rejected based on it quality our careful selection of data point in each frame allows temporal fusion via kalman filtering despite unimodality of our tracking scheme we demonstrate fairly robust result in highly cluttered aerial scene our technique form a natural feedback loop between the recognition method and the filter that help to explain such robustness we study this loop and derive a number of interesting property first the effective threshold for recognition in each frame is adaptive it depends on the current level of noise in the system this allows the system to identify partially occluded or distorted object a long a the predicted location are accurate but requires a very good match if there is uncertainty a to the object location second the search area for the recognition method is automatically pruned based on the current system uncertainty yielding an efficient overall method 
the paper describes a case study in combiningdifferent method for acquiring medicalknowledge given a huge amount of noisy high dimensional numerical time series datadescribing patient in intensive care the supportvector machine is used to learn whenand how to change the dose of which drug given medical knowledge about and expertisein clinical decision making a first orderlogic knowledge base about effect of therapeuticalinterventions ha been built a apreprocessing 
the aim of our research is to build a reflexive agent that is able to either manifest an emotion it is feeling or to hide it if the agent decides to manifest it emotion it can establish what verbal or nonverbal signal to employ in it communication and how to combine and synchronize them in the decision of whether to express an emotion in a given context a number of factor are considered such a the agent s own personality and goal the interlocutor s characteristic and the context in planning how to communicate an emotion various factor are considered a well the available modality face gaze voice etc the cognitive ease in producing and processing the various signal the expressiveness of every signal in communicating specific meaning and finally the appropriateness of signal to social situation 
an algorithm is given for computing projective structure from a set of six point seen in a sequence of many image the method is based on the notion of duality between camera and point first pointed out by carlsson and weinshall the current implementation avoids the weakness inherent in previous implementation of this method in which numerical accuracy is compromised by the distortion of image point error distribution under projective transformation it is shown in this paper that one may compute the dual fundamental matrix by minimizing a cost function giving a first order approximation to geometric distance error in the original untransformed image measurement this is done by a modification of a standard near optimal method for computing the fundamental matrix subsequently the error measurement are adjusted optimally to conform with exact imaging geometry by application of the triangulation method of hartley sturm 
in this paper we present a theory for obtaining density that are important for computer vision a a result of the theory we compute the exact and novel density of the slope of a line fitted to image point this density make it possible to obtain confidence interval for the slope or to make hypothesis testing about if two intersecting line form a corner or not the theory also let u derive a novel technique for maximum likelihood estimation that can be used for computing the fundamental matrix conic or any other constraint that can be expressed by polynomial of degree we present exact and novel density for the fundamental matrix and conic constraint that are needed for the estimation experiment show how the result can be used in practice to compute maximum likelihood estimate of the fundamental matrix 
we present an aspect graph approach to d object recognition where the definition of an aspect is motivated by it role in the subsequent recognition step specifically we measure the similarity between two view by a d shape metric of similarity measuring the distance between the pro jected segmented shape of the d object this endows the viewing sphere with a metric which is used to group similar view into aspect and to represent each aspect by a prototype the same shape similarity metric is then used to rate the similarity between unknown view of unknown object and stored prototype to identify the object and it pose the performance of this approach on a database of object each viewed in five degree increment along the ground viewing plane is demonstrated 
we present a method for region identification in multiple image a set of region in different image and the correspondence on their boundary can be thought of a a boundary in the multi dimensional space formed by the product of the individual image domain we minimize an energy functional on the space of such boundary thereby identifying simultaneously both the optimal region in each image and the optimal correspondence on their boundary we use a ratio form for the energy functional thus enabling the global minimization of the energy functional using a polynomial time graph algorithm among other desirable property we choose a simple form for this energy that favour boundary that lie on high intensity gradient in each image while encouraging correspondence between boundary in different image that match intensity value the latter tendency is weighted by a novel heuristic energy that encourages the boundary to lie on disparity or optical flow discontinuity although no dense optical flow or disparity map is computed 
this paper describes a statistical methodology ibr automatically retrieving collocation from po tagged korean text using interrupted bigram the free order of korean make it hard to identify collocation we devised four statistic frequency randomness condensation and correlation to account for the more flexible word order property of korean collocation we extracted meaningful bigram using an evaluation ihnction and extended the bigram to n gram collocation by generating equivalence set a cover we view a modeling problem for n gram collocation a that for clustering of cohesive word 
many researcher have shown that server driven consistencyprotocols can potentially reduce read latency server drivenconsistency protocol are particularly attractive for largescaledynamic web workload because dynamically generateddata can change rapidly and unpredictably however there have been no report on engineering server driven consistencyfor such a workload this paper report our experiencein engineering server driven consistency for a sportingand event web site hosted by 
there ha recently been significant interest in using representation based on abstraction of blum s skeleton into a graph for qualitative shape matching the application of these technique to large database of shape hinge on the availability of numerical algorithm for computing the medial axis unfortunately this computation can be extremely subtle approach based on voronoi technique preserve topology but heuristic pruning measure are introduced to remove unwanted edge method based on euclidean distance function can localize skeletal point accurately but often at the cost of altering the object s topology in this paper we introduce a new algorithm for computing subpixel skeleton which is robust and accurate ha low computational complexity and preserve topology the key idea is to measure the net outward flux of a vector field per unit area and to detect location where a conservation of energy principle is violated this is done in conjunction with a thinning process applied in a rectangular lattice we illustrate the approach with several example of skeletal graph for biological and man made silhouette 
in term of both speed and memory consumption graph unification remains the most expensive component of unification based grammar parsing we present a technique to reduce the memory usage of unification algorithm considerably without increasing execution time also the proposed algorithm is thread safe providing an efficient algorithm for parallel processing a well 
we introduce a language independent strategy for inducing part of speech tag from corpus unlike ot her technique that use language specific lexicon ru lesets and so forth to tag our algorithm bootstrap on ly from cluster property and language universal we describe the theory and illustrate an introductory exp eriment which verifies the feasibility of this nearknowledge free tag induction strategy we induce ta g for perfect syntactic c luster generated from the brown corpus getting of ou r hypothesis correct 
organization conducting electronic commerce e commerce can greatly benefit from the insight that data mining of transactional and clickstream data provides such insight help not only to improve the electronic channel e g a web site but it is also a learning vehicle for the bigger organization conducting business at brick and mortar store the e commerce site serf a an early alert system for emerging pattern and a laboratory for experimentation for successful data mining several ingredient are needed and e commerce provides all the right one the good web server log which are commonly used a the source of data for mining e commerce data were designed to debug web server and the data they provide is insufficient requiring the use of heuristic to reconstruct event moreover many event are never logged in web server log limiting the source of data for mining the bad many of the problem of dealing with web server log data can be resolved by properly architecting the e commerce site to generate data needed for mining even with a good architecture however there are challenging problem that remain hard to solve the ugly lesson and metric based on mining real e commerce data are presented 
the light reflected from a surface depends on the scene geometry the incident illumination and the surface material one of the property of the material is it albedo r l and it variation with respect to wavelength the albedo of a surface is purely a physical property our perception of albedo is commonly referred to a colour this paper present a novel methodology for extracting the albedo of the various material in the scene independent of incident light and scene geometry a scene is captured under different narrow band colour filter and the spectral derivative of the scene are computed the resulting spectral derivative form a spectral gradient at each pixel this spectral gradient is a normalized albedo descriptor which is invariant to scene geometry and incident illumination for diffuse surface 
this paper deal with translation ambiguity and target polysemy problem together two monolingual balanced corpus are employed to learn word co occurrence for translation ambiguity resolution and augmented translation restriction for target polysemy resolution experiment show that the model achieves of monolingual information retrieval and is addition to the select all model combining the target polysemy resolution the retrieval performance is about increase to the model resolving translation ambiguity only 
this paper present disciple coa the most recent learning agent shell developed in the disciple framework that aim at changing the way an intelligent agent is built from being programmed by a knowledge engineer to being taught by a domain expert disciple coa can collaborate with the expert to develop it knowledge base consisting of a frame based ontology that defines the term from the application domain and a set of plausible version space rule expressed with these term it central component is a plausible reasoner that can distinguish between four type of problem solving situation routine innovative inventive and creative this ability guide the interaction with the expert during which the agent learns general rule from specific example by integrating a wide range of knowledge acquisition and machine learning strategy including apprenticeship learning empirical inductive learning from example and explanation and analogical learning disciplecoa wa developed in the darpa s high performance knowledge base program to solve the challenge problem of critiquing military course of action that were developed a hasty candidate plan for ground combat operation we present the course of action challenge problem the process of teaching disciple coa to solve it and the result of darpa s evaluation in which disciple coa demonstrated the best knowledge acquisition rate and problem solving performance we also present a separate knowledge acquisition experiment conducted at the battle command battle lab where expert with no prior knowledge engineering experience succeeded to rapidly teach disciple coa to correctly critique course of action 
we propose to extend the temporal causal graph formalism used in model based diagnosis in order to deal with non trivial interaction like partial cancellation of fault effect a high level causal language is defined in which property such a the persistence of effect and the triggering or sustaining property of cause can be expressed various interaction phenomenon are associated with these feature instead of proposing an ad hoc reasoning mechanism to process this extended formalism the specification in this language are automatically translated into an event calculus based language having well established semantics our approach improves the way fault interaction and intermittent fault are coped with in temporal abductive diagnosis 
this paper present a database containing ground truth segmentation produced by human for image of a wide variety of natural scene we define an error measure which quantifies the consistency between segmentation of differing granularity and find that different human segmentation of the same image are highly consistent use of this dataset is demonstrated in two application evaluating the performance of segmentation algorithm and measuring probability distribution associated with gestalt grouping factor a well a statistic of image region property 
we address the conflict between identification and control or alternatively the conflict between exploration and exploitation within the framework of reinforcement learning q learning ha recently become a popular off policy reinforcement learning method the conflict between exploration and exploitation slows down q learning algorithm their performance doe not scale up and degrades rapidly a the number of state and action increase one reason for this slowness is that exploration lack the ability to extrapolate and interpolate from learning and to a large extent ha to reinvent the wheel moreover not all reinforcement problem one encounter are finite state and action system our approach to solving continuous state and action problem is to approximate the continuous state and action space with finite set of state and action and then to apply a finite state and action learning method this approach provides the mean for solving continuous state and action problem but doe not yet address the performance problem associated with scaling up state and action we address the scaling problem using functional approximation method towards that end this paper introduces two new reinforcement algorithm qlvq and quad q learning respectively and show their successful application for cart centering and fractal compression 
inferring both d structure and motion of nonrigidobjects from monocular image is an important problemin computational vision the challenge stem notonly from the absence of point correspondence but alsofrom the structure ambiguity in this paper a hierarchicalmethod which integrates both local patch analysisand global shape description is devised to solvethe dual problem of structure and nonrigid motion recoveryby using an elastic geometric model extendedsuperquadrics the 
maximization of cross correlation is a commonly used principle for intensity based object localization that give a single estimate of location however to facilitate sequential inference eg over time or scale and to allow the representation of ambiguity it is desirable to represent an entire probability distribution for object location although the cross correlation itself or some function of it ha sometimes been treated a a probability distribution this is not generally justifiable bayesian correlation achieves a consistent probabilistic treatment by combining several development the first is the interpretation of correlation matching function in probabilistic term a observation likelihood second probability distribution of filter bank response are learned from training example inescapably response learning also demand statistical modeling of background intensity and there are link here with image coding and independent component analysis lastly multi scale processing is achieved in a bayesian context by mean of a new algorithm layered sampling for which asymptotic property are derived 
combining learning with vision technique in interactive image retrieval ha been an active research topic during the past few year however existing learning technique eith er are based on heuristic or fail to analyze the working condition furthermore there is almost no in depth study on how to effectively learn from the user when there are multiple visual feature in the retrieval system to address thes e limitation in this paper we present a vigorous optimization formulation of the learning process and solve the problem in a principled way by using lagrange multiplier we have derived explicit solution which are both optimal and fast to compute extensive comparison against state ofthe art technique have been performed experiment were carried out on a large size heterogeneous image collection consisting of image retrieval performance wa tested under a wide range of condition various evaluation criterion including precision recall curve and rank measu re have demonstrated the effectiveness and robustness of the proposed technique 
given video footage of a person s face we present new technique to automatically recover the face position and the facial expression from each frame in the video sequence a d face model is fitted to each frame using a continuous optimization technique our model is based on a set of d face model that are linearly combined using d morphing our method ha the advantage over previous technique of fitting directly a realistic dimensional face model and of recovering parameter that can be used directly in an animation system we also explore many application including performance driven animation applying the recovered position and expression of the face to a synthetic character to produce an animation that mimic the input video relighting the face varying the camera position and adding facial ornament such a tattoo and scar 
abstract an improved method for deformable shape based image segmentation is described image region are merged together and or split apart based on their agreement with an a priori distribution on the global deformation parameter for a shape template the quality of a candidate region merging is evaluated by a cost measure that includes homogeneity of image property within the combined region degree of overlap with a deformed shape model and a deformation likelihood term perceptually motivated criterion are used to determine where how to split region based on the local shape property of the region group s bounding contour a globally consistent interpretation is determined in part by the minimum description length principle experiment show that the model based splitting strategy yield a significant improvement in segmention over a method that us merging alone 
we compare discriminative and generative learning a typified by logistic regression and naive bayes we show contrary to a widely held belief that discriminative classifier are almost always to be preferred that there can often be two distinct regime of performance a the training set size is increased one in which each algorithm doe better this stem from the observation which is borne out in repeated experiment that while discriminative learning ha lower asymptotic error a 
in recent year several technique have been proposed for modelling the low dimensional manifold or subspace of natural image example include principal component analysis a used for instance in eigen face independent component analysis and auto encoder neural network such method suffer from a number of restriction such a the limitation to linear manifold or the absence of a probablistic representation in this paper we exploit recent development in the field of variational inference and latent variable model to develop a novel and tractable probabilistic approach to modelling manifold which can handle complex non linearity our framework comprises a mixture of sub space component in which both the number of component and the effective dimensionality of the subspace are determined automatically a part of the bayesian inference procedure we illustrate our approach using two classical problem modelling the manifold of face image and modelling the manifold of hand written digit 
bayesian belief propagation in graphical model ha been recently shown to have very close tie to inference method based in statistical physic after yedidia et al demonstrated that belief propagation xed point correspond to extremum of the so called bethe free energy yuille derived a double loop algorithm that is guaranteed to converge to a local minimum of the bethe free energy yuille s algorithm is based on a certain decomposition of the bethe free energy and he mention that other decomposition are possible and may even be fruitful in the present work we begin with the bethe free energy and show that it ha a principled interpretation a pairwise mutual information minimization and marginal entropy maximization mime next we construct a family of free energy function from a spectrum of decomposition of the original bethe free energy for each free energy in this family we develop a new algorithm that is guaranteed to converge to a local minimum preliminary computer simulation are in agreement with this theoretical development 
an algorithm for semantic interpretation is explained the algorithm is based on predicate defined for wordnet verb class the algorithm is driven by the definition of these predicate whose thematic role are linked to the wordnet ontology for noun and to the syntactic relation that realize them the algorithm ha been tested in the identification of the meaning of the verb thematic role and temporal and spatial adjunct 
modeling an experimental system often result in a number of alternative model that are justified equally well by the experimental data in order to discriminate between these model additional experiment are needed we present a method for the discrimination of model in the form of semiquantitative differential equation the method is a generalization of previous work in model discrimination it is based on an entropy criterion for the selection of the most informative experiment which can handle case where the model predict multiple qualitative behavior the applicability of the method is demonstrated on a real life example the discrimination of a set of competing model of the growth of phytoplankton in a bioreactor 
this paper introduces a new unsupervised algorithm for noun phrase coreference resolution it differs from existing method in that it view corererence resolution a a clustering task in an evaluation on the muc coreference resolution corpus the algorithm achieves an f measure of placing it firmly between the worst and best system in the muc evaluation more importantly the clustering approach outperforms the only muc system to treat coreference resolution a a learning problem the clustering algorithm appears to provide a flexible mechanism for coordinating the application of context independent and context dependent constraint and preference for accurate partitioning of noun phrase into coreference equivalence class 
we present three new visualization front end that aid navigation through the set of document returned by a search engine hit document we cluster the hit document to visually group these document and label the group with related word the dierent front end cater for dierent user need but all can browse cluster information a well a drilling up or down in one or more cluster and refining the search using one or more of the suggested related keywords 
in this paper we propose a potts spin mean field annealed network to address the open independent and incompatibility class of causal reasoning also said abduction abductive diagnosis the strong feature of the current work is it characterization of the reasoning task in these class by an energy target function computation of a scenario also said explanation is done by mean of mean field equation the application of the model to small and large scale causal problem reveals it efficacy and robustness in handling varied and multiple causal interaction 
in this paper we present an approach to tackle three important problem of text normalization sentence boundary disambiguation disambiguation of capitalized word when they are used in position where capitalization is expected and identification of abbreviation the main feature of our approach is that it us a minimum of pre built resource instead dynamically inferring disambiguation clue from the entire document itself this make it domain independent closely targeted to each individual document and portable to other language we thoroughly evaluated this approach on several corpus and it showed high accuracy 
weighted finite state transducer suffer from the lack of a training algorithm training is even harder for transducer that have been assembled via finite state operation such a composition minimization union concatenation and closure a this yield tricky parameter tying we formulate a parameterized fst paradigm and give training algorithm for it including a general bookkeeping trick expectation semirings that cleanly and efficiently computes expectation and gradient 
we present a modeland exemplar based technique for head pose tracking because of the dynamic nature it is not possible to represent face appearance by a single texture image instead we sample the complex face appearance space by a few reference image exemplar by taking advantage of the rich geometric information of a d face model and the flexible representation provided by exemplar our system is able to track head pose robustly under occlusion and or varying facial expression the system start with a simple learning stage the user move his her head with a neutral expression in front of the camera within the working space our system automatically build a personalized d face model by fitting a generic mesh model to a near frontal facial image and acquires a few reference image at distinct pose to sparsely sample the facial appearance space when tracking the head under occlusion and varying expression we match the current view against the most appropriate reference image according to the predicted pose which is much easier and more robust than if only a single texture image is used a robust motion segmentation algorithm is used to separate point match corresponding to rigid head motion from those corresponding to facial deformation the head pose can then be reliably estimated from the rigid motion point with the help of the d face mesh model even when the number of point is small since we use reference image during tracking the accumulative error inherent in frame by frame tracking is avoided and more accurate pose estimation is achieved we demonstrate the validity of our approach with several video sequence acquired in a casual environment 
the purpose of this paper is to present an on going research that is intended to construct a live thesaurus directly from search term log of real world search engine such a thesaurus designed can contain representative search term their frequency in use the corresponding subject category the associated and relevant term and the hot visiting web site page the search term may reach 
this paper address the problem of predicting fundamentalperformance of vote based object recognitionusing d point feature it present a method for predictinga tight lower bound on performance unlikeprevious approach the proposed method considersdata distortion factor namely uncertainty occlusion and clutter in addition to model similarity simultaneously the similarity between every pair of modelobjects is captured by comparing their structure a afunction of the 
computing least common subsumers lcs is an inference task that can be used to support the bottom up construction of knowledge base for kr system based on description logic previous work on how to compute the lcs ha concentrated on description logic that allow for universal value restriction but not for existential restriction the main new contribution of this paper is the treatment of description logic with existential restriction more precisely we show that for the description logic ale which allows for conjunction universal value restriction existential restriction negation of atomic concept a well a the top and the bottom concept the lcs always exists and can effectively be computed our approach for computing the lcs is based on an appropriate representation of concept description by certain tree and a characterization of subsumption by homomorphism between these tree the lcs operation then corresponds to the product operation on tree 
in this paper a new language model the multi class composite n gram is proposed to avoid a data sparseness problem for spoken language in that it is difficult to collect training data the multi class composite n gram maintains an accurate word prediction capability and reliability for sparse data with a compact model size based on multiple word cluster called multi class in the multi class the statistical connectivity at each position of the n gram is regarded a word attribute and one word cluster each is created to represent the positional attribute furthermore by introducing higher order word n gram through the grouping of frequent word succession multi class n gram are extended to multi class composite n gram in experiment the multi class composite n gram result in lower perplexity and a lower word error rate in speech recognition with a smaller parameter size than conventional word gram 
this paper report the first part of a project that aim to develop a knowledge extraction and knowledge discovery system that extract causal knowledge from textual database in this initial study we develop a method to identify and extract cause effect information that is explicitly expressed in medical abstract in the medline database a set of graphical pattern were constructed that indicate the presence of a causal relation in sentence and which part of the sentence represents the cause and which part represents the effect the pattern are matched with the syntactic parse tree of sentence and the part of the parse tree that match with the slot in the pattern are extracted a the cause or the effect 
we present a theorem prover for quantified boolean formula and evaluate it on random quantified formula and formula that represent problem from automated planning even though the notion of quantified boolean formula is theoretically important automated reasoning with qbf ha not been thoroughly investigated universal quantifier are needed in representing many computational problem that cannot be easily translated to the propositional logic and solved by satisfiability algorithm therefore efficient reasoning with qbf is important the davis putnam procedure can be extended to evaluate quantified boolean formula a straightforward algorithm of this kind is not very efficient we identify universal quantifier a the main area where improvement to the basic algorithm can be made we present a number of technique for reducing the amount of search that is needed and evaluate their effectiveness by running the algorithm on a collection of formula obtained from planning and generated randomly for the structured problem we consider the technique lead to a dramatic speed up 
real scale semantic web application such a knowledge portal and e marketplace require the management of large volume of metadata i e information describing the available web content and service better knowledge about their meaning usage accessibility or quality will considerably facilitate an automated processing of web resource the resource description framework rdf enables the creation and exchange of metadata a normal web data although voluminous rdf description are already appearing sufficiently expressive declarative language for querying both rdf description and schema are still missing in this paper we propose a new rdf query language called rql it is a typed functional language a la oql and relies on a formal model for directed labeled graph permitting the interpretation of superimposed resource description by mean of one or more rdf schema rql adapts the functionality of semistructured xml query language to the peculiarity of rdf but foremost it enables to uniformly query both resource description and schema we illustrate the rql syntax semantics and typing system by mean of a set of example query and report on the performance of our persistent rdf store employed by the rql interpreter 
line scratch are common degradation in motion picture film this paper present an efficient method for line scratch detection strengthened by a kalman filter a new interpolation technique dealing with both low and high frequency i e film grain around the line artifact is investigated to achieve a nearby invisible reconstruction of damaged area our line scratch detection and removal technique have been validated on several film sequence 
this paper introduces an algorithm that combine na ve bayes classification with feature weighting most of the related approach to feature transformation for na ve bayes suggest various heuristic and non exhaustive search strategy for selecting a subset of feature with which na ve bayes performs better than with the complete set of feature in contrast the algorithm introduced in this paper employ feature weighting performed by a support vector machine the weight are optimised such that the danger of overfitting is reduced to the best of our knowledge this is the first time that na ve bayes classification ha been combined with feature weighting experimental result on uci domain demonstrate that wbcsvm compare favourably to state of theart machine learning approach 
the quadrifocal tensor which connects image measurement along view is not yet well understood a it counterpart the fundamental matrix and the trifocal tensor this paper establishes the structure of the tensor a an epipole homography pairing qijkl v j hikl v k hijl v l hijk where v v v are the epipoles in view h is the homography tensor the view analogue of the homography matrix and the index i j k l are attached to view respectively i e hikl is the homography tensor of view in the course of deriving the structure qijkl we show that linear line complex llc mapping are the basic building block in the process we also introduce a complete break down of the tensor slice slice are homography tensor and slice are llc mapping furthermore we present a closed form formula of the quadrifocal tensor described by the trifocal tensor and fundamental matrix and also show how to recover projection matrix from the quadrifocal tensor we also describe the form of the non linear constraint a quadrifocal tensor must adhere to 
we are interested in the mechanism by which individual monitor and adjust their performance of simple cognitive task we model a speeded discrimination task in which individual are asked to classify a sequence of stimulus jones braver response conflict arises when one stimulus class is infrequent relative to another resulting in more error and slower reaction time for the infrequent class how do control process modulate behavior based on the relative class frequency we explain performance from a rational perspective that cast the goal of individual a minimizing a cost that depends both on error rate and reaction time with two additional assumption of rationality that class prior probability are accurately estimated and that inference is optimal subject to limitation on rate of information transmission we obtain a good fit to overall rt and error data a well a trial by trial variation in performance consider the following scenario while driving you approach an intersection at which the traffic light ha already turned yellow signaling that it is about to turn red you also notice that a car is approaching you rapidly from behind with no indication of slowing should you stop or speed through the intersection the decision is difficult due to the presence of two conflicting signal such response conflict can be produced in a psychological laboratory a well for example stroop asked individual to name the color of ink on which a word is printed when the word are color name incongruous with the ink color e g blue printed in red reaction time are slower and error rate are higher we are interested in the control mechanism underlying performance of high conflict task conflict requires individual to monitor and adjust their behavior possibly responding more slowly if error are too frequent in this paper we model a speeded discrimination paradigm in which individual are asked to classify a sequence of stimulus jones braver the stimulus are letter of the alphabet a z presented in rapid succession in a choice task individual are asked to press one response key if the letter is an x or another response key for any letter other than x a a shorthand we will refer to non x stimulus a y in a go no go task individual 
we investigate four hierarchical clustering method single link complete link groupwise average and single pas and two linguistically motivated text feature noun phrase head and proper name in the context of document clustering a statistical model for combining similarity information from multiple source is described and applied to darpa s topic detection and tracking phase tdt data this model based on log linear regression alleviates the need for extensive search in order to determine optimal weight for combining input feature through an extensive series of experiment with more than document from multiple news source and modality we establish that both the choice of clustering algorithm and the introduction of the additional feature have an impact on clustering performance we apply our optimal combination of feature to the tdt test data obtaining partition of the document that compare favorably with the result obtained by participant in the official tdt competition 
this paper present a novel variational method for supervised texture segmentation the textured feature space is generated by filtering the given textured image using isotropic and anisotropic filter and analyzing their response a multi component conditional probability density function the texture segmentation is obtained by unifying region and boundary based information a an improved geodesic active contour model the defined objective function is minimized using a gradient descent method where a level set approach is used to implement the obtained pde according to this pde the curve propagation towards the final solution is guided by boundary and region based segmentation force and is constrained by a regularity force the level set implementation is performed using a fast front propagation algorithm where topological change are naturally handled the performance of our method is demonstrated on a variety of synthetic and real textured frame 
integration in the head direction system is a computation by which horizontal angular head velocity signal from the vestibular nucleus are integrated to yield a neural representation of head direction in the thalamus the postsubiculum and the mammillary nucleus the head direction representation ha the form of a place code neuron have a preferred head direction in which their firing is maximal blair and sharp blair et al integration is a difficult computation given that head velocity can vary over a large range previous model of the head direction system relied on the assumption that the integration is achieved in a firing rate based attractor network with a ring structure in order to correctly integrate head velocity signal during high speed head rotation very fast synaptic dynamic had to be assumed here we address the question whether integration in the head direction system is possible with slow synapsis for example excitatory nmda and inhibitory gaba b type synapsis for neural network with such slow synapsis rate based dynamic are a good approximation of spiking neuron ermentrout we find that correct integration during high speed head rotation imposes strong constraint on possible network architecture 
in this paper we consider the projection task determining what doe or doe not hold after performing a sequence of action in a general setting where a solution to the frame problem may or may not be available and where online information from sensor may or may not be applicable we formally characterize the projection task for action theory of this sort and show how a generalized form of regression produce correct answer whenever it can be used we characterize condition on action theory sequence of action and sensing information that are sufficient to guarantee that regression can be used and present a provably correct regressionbased procedure in prolog for performing the task under these condition 
this paper describes method for tracking using point and line feature in affine view to provide fundamental invariance to change of focal length it first demonstrates how an earlier method of transfer based tracking using spatio temporal matching of point feature in a stereo active head is indeed zoom invariant in order to also make use of line the paper then illustrates how the affine triand quadrifocal tensor may be applied to tracking with zoom in monocular and stereo system respectively the usefulness of the tensor is evident from their ability to transfer a fixation point in two uncalibrated image into novel view whereas the trifocal tensor is already familiar and in common use for matching and reconstruction we believe this to be the first practical application of a quadrifocal tensor we develop expression for affine triand quadrifocal tensor and using novel affine specialization of existing projective algorithm we show how computation of the tensor is faster simpler and more stable experiment on real image are presented 
in this paper we present a scheme of postulate for revising epistemic state by conditional belief these postulate are supported mainly by following the specific non classical nature of conditionals and the aim of preserving conditional belief is achieved by studying specific interaction between conditionals represented properly by two relation because one of the postulate claim propositional belief revision to be a special case of conditional belief revision our framework also cover the work of darwiche and pearl darwiche and pearl and we show that all postulate presented there may be derived from our postulate we state representation theorem for the principal postulate and finally we present a conditional belief operator obeying all of the postulate by using ordinal conditional function a representation of epistemic state 
abstract this paper show that linguistic techniquesalong with machine learningcan extract high quality noun phrasesfor the purpose of providing the gistor summary of email message wedescribe a set of comparative experimentsusing several machine learningalgorithms for the task of salient nounphrase extraction three main conclusionscan be drawn from this study i the modifier of a noun phrase can besemantically a important a the head for the task of gisting ii linguistic 
theory of cue combination suggest the possibility of constructing visual stimulus that evoke different pattern of neural activity in sensory area of the brain but that cannot be distinguished by any behavioral measure of perception such stimulus if they exist would be interesting for two reason first one could know that none of the difference between the stimulus survive past the computation used to build the percept second it can be difficult to distinguish stimulus driven component of measured neural activity from top down component such a those due to the interestingness of the stimulus changing the stimulus without changing the percept could be exploited to measure the stimulusdriven activity here we describe stimulus in which vertical and horizontal disparity trade during the construction of percept of slanted surface yielding stimulus equivalence class equivalence class membership changed after a change of vergence eye posture alone without change to the retinal image a formal correspondence can be drawn between these perceptual metamers and more familiar sensory metamers such a color metamers 
we introduce radial basis function with compact support for elastic registration of medical image with these basis function the influence of a landmark on the registration result is limited to a circle in d and respectively to a sphere in d therefore the registration can be locally constrained which especially allows to deal with rather local change in medical image due to e g tumor resection an important property of the used rbfs is that they are positive definite thus the solvability of the resulting system of equation is always guaranteed we demonstrate our approach for synthetic a well a for d and d tomographic image 
presented at the ieee conference on computer vision and pattern recognition ft collins co june mixture modeling and clustering algorithm are effective simple way to represent image using a set of data center however in situation where the image include background clutter and transformation such a translation rotation shearing and warping these method extract data center that include clutterand represent different transformation of essentiallythe same data taking 
ai research ha developed an extensive collection of method to solve state space problem using the challenging domain of sokoban this paper study the effect of search enhancement on program performance we show that the current state of the art in at generally requires a large programming and research effort into domain dependent method to solve even moderately complex problem in such difficult domain the application of domain specific knowledge to exploit property of the search space can result in large reduction in the size of the search tree often several order of magnitude per search enhancement understanding the effect of these enhancement on the search lead to a new taxonomy of search enhancement and a new framework for developing single agent search application this is used to illustrate the large gap between what is portrayed in the literature versus what is needed in practice 
data intensive web site have created a new form of knowledge base a richly structured body of data several novel system for creating data intensive web site support declarative specification of a site s structure and content i e the page the data available in each page and the link between page declarative system provide a platform on which a technique can be developed that further simplify the task of constructing and maintaining web site this paper address the problem of specifying and verifying integrity constraint on a web site s structure we describe a language that can capture many practical constraint and an accompanying sound and complete verification algorithm the algorithm ha the important property that if the constraint are violated it proposes fix to either the constraint or to the site definition finally we establish tight bound on the complexity of the verification problem we consider 
we present a new composite similarity metric that combine information from multiple linguistic indicator to measure semantic distance between pair of small textual unit several potential feature are investigated and an optireal combination is selected via machine learning we discus a more restrictive definition of similarity than traditional document level and information retrieval oriented notion of similarity and motivate it by showing it relevance to the multi document text summarization problem result from our system are evaluated against standard information retrieval technique establishing that the new method is more effective in identifying closely related textual unit 
this paper describes using multi agent reinforcement learning rl algorithm for learning traffic light controller to minimize the overall waiting time of car in a city the rl system learn value function estimating expected waiting time for car given different setting of traffic light selected setting of traffic light result from combining the predicted waiting time of all car involved we investigate rl system using different kind of global communicated information between 
conventional vision system are designed to perform in clear weather however any outdoor vision system is incomplete without mechanism that guarantee satisfactory performance under poor weather condition it is known that the atmosphere can significantly alter light energy reaching an observer therefore atmospheric scattering model must be used to make vision system robust in bad weather in this paper we develop a geometric framework for analyzing the chromatic effect of atmospheric scattering first we study a simple color model for atmospheric scattering and verify it for fog and haze then based on the physic of scattering we derive several geometric constraint on scene color change caused by varying atmospheric condition finally using these constraint we develop algorithm for computing fog or haze color depth segmentation extracting three dimensional structure and recovering true scene color from two or more image taken under different but unknown weather condition spective of scene radiance they also proposed a dichromatic atmospheric scattering model that describes the dependence of atmospheric scattering on wavelength however the algorithm they developed to recover structure using this model requires a clear day image of the scene in this paper we develop a general chromatic framework for the analysis of image taken under poor weather condition the wide spectrum of atmospheric particle make a general study of vision in bad weather hard so we limit ourselves to weather condition that result from fog and haze we begin by describing the key mechanism of scattering next we analyze the dichromatic model proposed in nn a nd experimentally verify it for fog and haze then we derive several useful geometric constraint on scene color change due to different but unknown atmospheric condition finally we develop algorithm to compute fog or haze color to construct depth map of arbitrary scene and to recover scene color a they would appear on a clear day all of our method only require image of the scene taken under two or more poor weather condition and not a clear day image of the scene 
object shape and camera motion recovery from animage sequence is a crucial issue in computer visionand many method have been proposed by researcher theoretically these method are perfect but they aresensitive to noise so that in many practical situation satisfactory result cannot be obtained to solvethis problem we propose a shape and motion recoverymethod using a gyro sensor attached on a video camerafor compensating image we made an experimentalsystem with a ccd camera 
this paper is concerned with providing a common framework for both the logical specification and execution of agent while numerous high level agent theory have been proposed in order to model agent such a theory of intention these often have little formal connection to practical agentbased system on the other hand many of the agent based programming language used for implementing real agent lack firm logical semantics our approach is to define a logical framework in which agent can be specified and then show how such specification can be directly executed in order to implement the agent s behaviour we here extend this approach to capture an important aspect of practical agent namely their resource bounded nature we present a logic in which resource boundedness can be specified and then consider how specification within this logic can be directly executed the mechanism we use to capture finite resource is to replace the standard modal logic previously used to represent an agent s belief with a multi context representation of belief thus providing tight control over the agent s reasoning capability where necessary this logical framework provides the basis for the specification and execution of agent comprising dynamic temporal activity deliberation concerning goal and resource bounded reasoning 
we present a novel algorithm performing projective rectification which doe not require explicit computation of the epipolar geometry and specifically of the fundamental matrix instead of finding the epipoles and computing two homographies mapping the epipoles to infinity a done in recent work on projective rectification we exploit the fact that the fundamental matrix of a pair of rectified image ha a particular known form this allows u to set up a minimization that yield the rectifying homographies directly from image correspondence experimental result show that our method work quite robustly even in the presence of noise and with inaccurate point correspondence the code of our implementation will be made available at the author s web site 
this paper present a novel practical framework for bayesian modelaveraging and model selection in probabilistic graphical model our approach approximates full posterior distribution over modelparameters and structure a well a latent variable in an analyticalmanner these posterior fall out of a free form optimizationprocedure which naturally incorporates conjugate prior unlikein large sample approximation the posterior are generally nongaussianand no hessian need 
design and development of novel human computer interface pose a challenging problem action and intention of user have to be inferred from sequence of noisy and ambiguous multi sensory data such a video and sound temporal fusion of multiple sensor ha been efficiently formulated using dynamic bayesian network dbns which allow the power of statistical inference and learning to be combined with contextual knowledge of the problem unfortunately simple learning method can cause such appealing model to fail when the data exhibit complex behavior we formulate a learning framework for dbns based on error feedback and statistical boosting theory we apply this framework to the problem of audio visual speaker detection in an interactive kiosk environment using off theshelf visual and audio sensor face skin texture mouth motion and silence detector detection result obtained in this setup demonstrate superiority of our learning framework over that of the classical ml learning in dbns 
observer translation relative to the world creates image ow that expands from the observer s direction of translation heading from which the observer can recover heading direction yet the image flow is often more complex depending on rotation of the eye scene layout and translation velocity a number of model have been proposed on how the human visual system extract heading from flow in a neurophysiologically plausible way these model represent heading by a set of neuron that 
the appearance of object consists of region of localstructure a well a dependency between these region the local structure can be characterized by a vector oflocal feature measured by local operator such a gaussianderivatives or gabor filter this paper present atechnique in which the appearance of object is representedby the joint statistic of local neighborhood operator a probabilistic technique based on joint statisticsis developed for the identification of multiple 
this paper proposes a novel technique to computing geometric information from image captured under parallel projection parallel image are desirable for stereo reconstruction because parallel projection significantly reduces foreshortening a a result correlation based matching becomes more effective since parallel projection camera are not commonly available we construct parallel image by rebinning a large sequence of perspective image epipolar geometry depth recovery and projective invariant for both d and d parallel stereo are studied from the uncertainty analysis of depth reconstruction it is shown that parallel stereo is superior to both conventional perspective stereo and the recently developed multiperspective stereo for vision reconstruction in that uniform reconstruction error is obtained in parallel stereo traditional stereo reconstruction technique e g multi baseline stereo can still be applicable to parallel stereo without any modification because epipolar line in a parallel stereo are perfectly straight experimental result further confirm the performance of our approach 
we propose an efficient solution to the general m view projective reconstruction problem using matrix factorization and iterative least square the method can accept input with missing data meaning that not all point are necessarily visible in all view it run much faster than the often used non linear minimization method while preserving the accuracy of the latter the key idea is to convert the minimization problem into a series of weighted least square sub problem with drastically reduced matrix size additionally we show that good initial value can always be obtained experimental result on both synthetic and real data are presented potential application are also demonstrated 
we describe a programmable multi chip vlsi neuronal system that can be used for exploring spike based information processing model the system consists of a silicon retina a pic microcontroller and a transceiver chip whose integrate and fire neuron are connected in a soft winner take all architecture the circuit on this multi neuron chip approximates a cortical microcircuit the neuron can be configured for different computational property by the virtual connection of a selected set of pixel on the silicon retina the virtual wiring between the different chip is effected by an event driven communication protocol that us asynchronous digital pulse similar to spike in a neuronal system we used the multi chip spike based system to synthesize orientation tuned neuron using both a feedforward model and a feedback model the performance of our analog hardware spiking model matched the experimental observation and digital simulation of continuous valued neuron the multi chip vlsi system ha advantage over computer neuronal model in that it is real time and the computational time doe not scale with the size of the neuronal network 
jensen s inequality is a powerful mathematical tool and one of the workhorse in statistical learning it application therein include the em algorithm bayesian estimation and bayesian inference jensen computes simple lower bound on otherwise intractable quantity such a product of sum and latent log likelihood this simplification then permit operation like integration and maximization quite often i e in discriminative learning upper bound are needed a well we derive and prove an efficient analytic inequality that provides such variational upper bound this inequality hold for latent variable mixture of exponential family distribution and thus span a wide range of contemporary statistical model we also discus application of the upper bound including maximum conditional likelihood large margin discriminative model and conditional bayesian inference convergence efficiency and prediction result are shown 
planning under partial observability is one of the most significant and challenging planning problem it ha been shown to be hard both theoretically and experimentally in this paper we present a novel approach to the problem of planning under partial observability in non deterministic domain we propose an algorithm that search through a possibly cyclic and or graph induced by the domain the algorithm generates conditional plan that are guaranteed to achieve the goal despite of the uncertainty in the initial condition the uncertain effect of action and the partial observability of the domain we implement the algorithm by mean of bdd based symbolic model checking technique in order to tackle in practice the exponential blow up of the search space we show experimentally that our approach is practical by evaluating the planner with a set of problem taken from the literature and comparing it with other state of the art planner for partially observable domain 
we propose a new method for tracking rigid object in image sequence using template matching a kalman filter is used to make the template adapt to change in object orientation or illumination this approach is novel since the kalman filter ha been used in tracking mainly for smoothing the object trajectory the performance of the kalman filter is further improved by employing a robust and adaptive filtering algorithm special attention is paid to occlusion handling 
we investigate the motion that lead to ambiguous euclidean scene reconstruction under several common calibration constraint giving a complete description of such critical motion for i internally calibrated orthographic and perspective camera ii in two image for camera with unknown focal length either different or equal one aim of the work wa to evaluate the potential of modern algebraic geometry tool for rigorously proving property of vision algorithm so we use idealtheoretic calculation a well a classical algebra and geometry we also present numerical experiment showing the effect of near critical configuration for the varying and fixed focal length method 
in this paper we introduce a new tool called a pseudo distance map pdm for extracting skeleton from grayscale image without region segmentation or edge detection given an edge strength function esf of a gray scale image the pdm is computed from the esf using the partial differential equation we propose the pdm can be thought of a a relaxed version of a euclidean distance map therefore it ridge correspond to the skeleton of the original gray scale image and it provides information on the approximate width of skeletonized structure since the pdm is directly computed from the esf without thresholding it the skeletonization result is generally robust and le noisy we tested our method using a variety of synthetic and real image the experimental result show that our method work well on such image 
web usage mining is the application of data mining techniquesto large web data repository in order to extract usage pattern a with many data mining application domain the identification of patternsthat are considered interesting is a problem that must be solved inaddition to simply generating them a necessary step in identifying interestingresults is quantifying what is considered uninteresting in order toform a basis for comparison several research effort have relied on 
this paper explores the problem of featuresubset selection for unsupervised learningwithin the wrapper framework in particular we examine feature subset selection wrappedaround expectation maximization em clusteringwith order identification identifyingthe number of cluster in the data weinvestigate two different performance criteriafor evaluating candidate feature subset scatter separability and maximum likelihood when the quot true quot number of cluster k is unknown our 
a case based approach to adaptation for estimation task is presented in which there is no requirement for explicit adaptation knowledge instead a target case is estimated from the value of three existing case one retrieved for it similarity to the target case and the others to provide the knowledge required to adapt the similar case with recursive application of the adaptation process any problem space can be fully covered by fewer than nk selected case where n is the number of case attribute and k is the number of value of each attribute moreover a k k problem space is fully covered by any set of k known case provided there is no redundancy in the case library circumstance in which the approach is appropriate are identified by theoretical analysis and confirmed by experimental result 
a new definition of affine invariant skeleton for shape representation is introduced a point belongs to the affine skeleton if and only if it is equidistant from at least two point of the curve with the distance being a minimum and given by the area between the curve and it corresponding chord the skeleton is robust eliminating the need for curve denoising previous approach have used either the euclidean or affine distance thereby resulting in a much le robust computation we propose a simple method to compute the skeleton and give example with real image and show that the proposed definition work also for noisy data we also demonstrate how to use this method to detect affine skew symmetry 
the project pursued in this paper is to develop from rstinformation geometric principle a general method for learningthe similarity between text document each individual documentis modeled a a memoryless information source based ona latent class decomposition of the term document matrix a lowdimensional curved multinomial subfamily is learned from thismodel a canonical similarity function known a the fisher kernel is derived our approach can be applied for 
we incorporate prior knowledge to construct nonlinear algorithm for invariant feature extraction and discrimination employing a unified framework in term of a nonlinear variant of the rayleigh coefficient we propose non linear generalization of fisher s discriminant and oriented pca using support vector kernel function extensive simulation show the utility of our approach 
the paper proposes an information theory based method for feature type analysis in probabilistic evaluation modelling for statistical parsing the basic idea is that we use entropy and conditional entropy to measure whether a feature type grasp some of the information for syntactic structure prediction our experiment quantitatively analyzes several feature type power for syntactic structure prediction and draw a series of interesting conclusion 
the paper proposes an information theory based method for feature type analysis in probabilistic evaluation modelling for statistical parsing the basic idea is that we use entropy and conditional entropy to measure whether a feature type grasp some of the information for syntactic structure prediction our experiment quantitatively analyzes several feature type power for syntactic structure prediction and draw a series of interesting conclusion 
in kernel based method such a regularizationnetworks large datasets pose signi cant problem since the number of basis functionsrequired for an optimal solution equalsthe number of sample we present a sparsegreedy approximation technique to constructa compressed representation of the designmatrix experimental result are given andconnections to kernel pca sparse kernelfeature analysis and matching pursuit arepointed out introductionmany recent advance in 
extracting rule from rbfs is not a trivial task because of nonlinear function or high input dimensionality in such case some of the hidden unit of the rbf network have a tendency to be shared across several output class or even may not contribute 
this paper present a technique for blindly removing image non linearity in the absence of any calibration information or explicit knowledge of the imaging device the basic approach exploit the fact that a non linearity introduces specific higher order correlation in the frequency domain beyond second order these correlation can be detected using tool from polyspectral analysis the non linearity can then be estimated and removed by simply minimizing these correlation 
stochastic meta descent smd is a new technique for online adaptation of local learning rate in arbitrary twice differentiable system like matrix momentum it us full second order information while retaining o n computational complexity by exploiting the efficient computation of hessian vector product here we apply smd to independent component analysis and employ the resulting algorithm for the blind separation of time varying mixture by matching individual learning rate to the rate of change in each source signal s mixture coefficient our technique is capable of simultaneously tracking source that move at very different a priori unknown speed 
i present a method of identifying cognate in the vocabulary of related language i show that a measure of phonetic similarity based on multivalued feature performs better than orthographic measure such a the longest common subsequence ratio lcsr or dice s coefficient i introduce a procedure for estimating semantic similarity of gloss that employ keyword selection and wordnet test performed on vocabulary of four algonquian language indicate that the method is capable of discovering on average nearly percent of cognate at precision 
traditional method for the system oriented evaluation of boolean ir system suffer from validity and reliability problem laboratory based research neglect the searcher and study suboptimal query research on operational system fails to make a distinction between searcher performance and system performance this approach is neither capable of measuring performance at standard point of operation e g across r r a new laboratory based evaluation method for boolean ir system is proposed it is based on a controlled formulation of inclusive query plan on an automatic conversion of query plan into elementary query and on combining elementary query into optimal query at standard point of operation major result of a large case experiment are reported the validity reliability and efficiency of the method are considered in the light of empirical and analytical test data 
model of computer user that are learned on the basis of data can make use of two type of information data about user in general and data about the current individual user focusing on user model that take the form of bayesian network we compare four type of model that represent different way of combining these two type of data model of the four type are applied to the data of an experiment and they are evaluated according to theoretical empirical and practical criterion one of the model type is a new variant of the ahugin method for adapting the probability of a bayesian network while it is being used differential adaptation is a principled way of determining the speed with which each aspect of a network is adapted to an individual user 
since the human hand is highly articulated and deformable hand posture recognition is a challenging example in the research on view independent object recognition due to the difficulty of the model based approach the appearance based learning approach is promising to handle large variation in visual input however the generalization of many proposed supervised learning method to this problem often suffers from the insufficiency of labeled training data this paper describes an approach to alleviate this difficulty by adding a large unlabeled training set combining supervised and unsupervised learning paradigm a novel and powerful learning approach the discriminant em d em algorithm is proposed in this paper to handle the case of a small labeled training set experiment show that d em outperforms many other learning method based on this approach we implement a gesture interface to recognize a set of predefined gesture command and it is also extended to hand detection this algorithm can also apply to other object recognition task 
with the increasing number of user of mobile computing device e g personal digital assistant and the advent of third generation mobile phone wireless communication are becoming increasingly important many application rely on the device maintaining a replica of a data structure which is stored on a server for example news database calendar and e mail in this paper we explore the question of the optimal strategy for synchronising such replica we utilise probabilistic model to represent how the data structure evolve and to model user behaviour we then formulate objective function which can be minimised with respect to the synchronisation timing we demonstrate using two real world data set that a user can obtain more up to date information using our approach 
mass collaboration is a new p p style approach to large scale knowledge sharing with application in customer support focused community development and capturing knowledge distributed within large organization effectively supporting this paradigm raise many technical challenge and offer intriguing opportunity for mining massive amount of data captured continually from user interaction data mining offer the promise of increased business intelligence and also improved user experience leading to increased participation and greater quality in the knowledge that is captured both of which are central objective in mass collaboration in this talk i will introduce mass collaboration and discus some important data mining related issue 
the initialisation of segmentation method aiming at the localisation of biological structure in medical imagery is frequently regarded a a given precondition in practice however initialisation is usually performed manually or by some heuristic preprocessing step moreover the same framework is often employed to recover from imperfect result of the subsequent segmentation therefore it is of crucial importance for everyday application to have a simple and effective initialisation method at one s disposal this paper proposes a new model based framework to synthesise sound initialisation by calculating the most probable shape given a minimal set of statistical landmark and the applied shape model shape information coded by particular point is first iteratively removed from a statistical shape description that is based on the principal component analysis of a collection of shape instance by using the inverse of the resulting operation it is subsequently possible to construct initial outline with minimal effort the whole framework is demonstrated by mean of a shape database consisting of a set of corpus callosum instance furthermore both manual and fully automatic initialisation with the proposed approach is evaluated the obtained result validate it suitability a a preprocessing step for semi automatic a well a fully automatic segmentation and last but not least the iterative construction of increasingly point invariant shape statistic provides a deeper insight into the nature of the shape under investigation 
abstract traditional plane alignment technique are typically performed between pair of frame in this paper we present a method for extending existing two frame planar motion estimation technique into a simultane ous multi frame estimation by exploiting multi frame geometric constraint of planar surface the paper ha three main contribution i we show that when the camera calibration doe not change the collection of all parametric image motion of a planar surface in the scene across multiple frame is embedded in a low dimensional linear subspace ii we show that the rel ative image motion of multiple planar surface across multiple frame is embedded in a yet lower dimensional linear subspace even with varying camera calibration and iii we show how these multi frame constraint can be incorporated into simultaneous multi frame e timation of planar motion without explicitly recovering any d information or camera calibration the result ing multi frame estimation process is more constrained than the individual two frame estimation leading to more accurate alignment even when applied to small image region 
a new method for d rigid motion estimation from stereo is proposed in this paper the appealing feature of this method is that it directly us the disparity image obtained from stereo matching we assume that the stereo rig ha parallel camera and show in that case the geometric and topological property of the disparity image then we introduce a rigid transformation called d motion that map two disparity image of a rigidly moving object we show how it is related to the euclidean rigid motion and a motion estimation algorithm is derived we show with experiment that our approach is simple and more accurate than standard approach 
this paper present a system to support writinga survey of a specific domain the systemutilizes reference information that consistsof reference relationship between paper andthe information derived from the descriptionaround citation we think the following areinevitable for writing a survey collecting papersof the specific domain and understandingtheir essence and difference among them therefore we firstly extract fragment of paperswhere the author describes the 
from a early a month of age human child distinguish between motion pattern generated by animate object from pattern generated by moving inanimate object even when the only stimulus that the child observes is a single point of light moving against a blank background the mechanism by which the animate inanimate distinction are made are unknown but have been shown to rely only upon the spatial and temporal property of the movement in this paper i present both a multiagent architecture that performs this classification a well a detailed comparison of the individual agent contribution against human baseline 
a mechanism is proposed that integrates low level image processing mid level recursive d trajectory estimation and high level action recognition process it is assumed that the system observes multiple moving object via a single uncalibrated video camera a novel extended kalman filter formulation is used in estimating the relative d motion trajectory up to a scale factor the recursive estimation process provides a prediction and error measure that is exploited in higher level stage of action recognition conversely higher level mechanism provide feedback that allows the system to reliable segment and maintain the tracking of moving object before during and after occlusion the d trajectory occlusion and segmentation information are utilized in extracting stabilized view of the moving object trajectory guided recognition tgr is proposed a a new and efficient method for adaptive classification of action the tgr approach is demonstrated using motion history image that are then recognized via a mixture of gaussian classifier the system wa tested in recognizing various dynamic human outdoor activity e g running walking roller blading and cycling experiment with synthetic data set are used to evaluate stability of the trajectory estimator with respect to noise 
we develop an intuitive geometric interpretation of the standard support vector machine svm for classification of both linearly separable and inseparable data and provide a rigorous derivation of the concept behind the geometry for the separable case finding the maximum margin between the two set is equivalent to finding the closest point in the smallest convex set that contain each class the convex hull we now extend this argument to the inseparable case by using a reduced convex hull reduced away from outlier we prove that solving the reduced convex hull formulation is exactly equivalent to solving the standard inseparable svm for appropriate choice of parameter some additional advantage of the new formulation are that the effect of the choice of parameter becomes geometrically clear and that the formulation may be solved by fast nearest point algorithm by changing norm these argument hold for both the standard norm and norm svm a successful application there are three key idea needed to understand svm maximizing margin the dual formulation and kernel most people intuitively grasp the idea that maximizing margin should help improve generalization but changing from the primal to dual formulation is typically black magic for those uninitiated in duality theory duality is really the key concept frequently missing in the understanding of svm in this paper we provide an intuitive geometric explanation of svm for classification from the dual perspective along with a mathematically rigorous derivation of the idea behind the geometry we begin with an explanation of the geometry of svm based on the idea of convex hull for the separable case this geometric explanation ha existed in various form vapnik mangasarian keerthi et al bennett bredensteiner in press the new contribution is the adaptation of the convex hull argument for the inseparable case to the most commmonly used norm and norm soft margin svm the primal form resulting from this argument can be regarded a an especially elegant minor variant of the svm formulation scholkopf et al or a soft margin form of the msm method mangasarian related geometric idea for the svm formulation were developed independently by crisp and burges the primary contribution of this paper are 
this paper investigates condition underwhich modification to the reward functionof a markov decision process preserve the optimalpolicy it is shown that besides thepositive linear transformation familiar fromutility theory one can add a reward for transitionsbetween state that is expressible asthe difference in value of an arbitrary potentialfunction applied to those state furthermore this is shown to be a necessary conditionfor invariance in the sense that anyother 
we propose a new definition of abduction in logic programming and contrast it with that of kakas and mancarella s we then introduce a rewriting system for answering query and generating explanation and show that it is both sound and complete under the partial stable model semantics and sound and complete under the answer set semantics when the underlying program is so called odd loop free we discus an application of the work to a problem in reasoning about action and provide some experimental result 
in this paper parallelepiped and their use in camera calibration and d reconstruction process are studied parallelepiped naturally characterize rigidity constra ints present in a scene such a parallelism and orthogonality a subclass of parallelepiped the cuboid ha been frequently used over the past to partially calibrate camera however the full potential of parallelepiped in camera calibration a well a in scene reconstruction ha never been clearly established we propose a new framework for the use of parallelepiped which is based on an extensive study of this potential in particular we exhibit the complete duality that exists between the intrinsic metric char acteristics of a parallelepiped and the intrinsic paramete r of a camera our framework allows to fully exploit parallelepiped and thus overcomes several limitation of calibration approach based on cuboid to illustrate this framework we present an original and very efficient interactive method for d reconstruction from single image this method allows to quickly build a scene model from a single uncalibrated image 
the emergence of networked context aware mobile computing appliance potentially offer opportunity for remote access to huge online information resource information access in context aware information appliance can utilize existing technique developed for effective information retrieval and information filtering however practical physical and operational feature of these device and the availability of context information itself suggest that the document selection process should make use of this contextual data 
several new algorithm for visual correspondence based on graph cut have recently been developed while these method give very strong result in practice they do not handle occlusion properly specifically they treat the two input image asymmetrically and they do not ensure that a pixel corresponds to at most one pixel in the other image in this paper we present two new method which properly address occlusion while preserving the advantage of graph cut algorithm we give experimental result for stereo a well a motion which demonstrate that our method perform well both at detecting occlusion and computing disparity 
we describe our research in applying data mining technique to construct intrusion detection model the key idea are to mine system audit data for consistent and useful pattern of program and user behavior and use the set of relevant system feature presented in the pattern to compute inductively learned classifier that can recognize anomaly and known intrusion our past experiment showed that classification rule can be used to detect intrusion provided that sufficient audit data is available for training and the right set of system feature are selected we use the association rule and frequent episode computed from audit data a the basis for guiding the audit data gathering and feature selection process in order to compute only the relevant useful pattern we consider the order of importance and reference relation among the attribute of data and modify these two basic algorithm accordingly to use axis attribute s and reference attribute s a form of item constraint in the data mining process we also use an iterative level wise approximate mining procedure for uncovering the low frequency but important pattern we report our experiment in using these algorithm on real world audit data 
the project pursued in this paper is to develop from rstinformation geometric principle a general method for learningthe similarity between text document each individual documentis modeled a a memoryless information source based ona latent class decomposition of the term document matrix a lowdimensional curved multinomial subfamily is learned from thismodel a canonical similarity function known a the fisher kernel is derived our approach can be applied for 
poker is an interesting test bed for artificialintelligence research it is a game of imperfectknowledge where multiple competing agentsmust deal with risk management opponentmodeling unreliable information and deception much like decision making application in thereal world opponent modeling is one of themost difficult problem in decision makingapplications and in poker it is essential toachieving high performance this paperdescribes and evaluates the implicit and 
r max is a very simple model based reinforcement learning algorithm which can attain near optimal average reward in polynomial time in r max the agent always maintains a complete but possibly inaccurate model of it environment and act based on the optimal policy derived from this model the model is initialized in an optimistic fashion all action in all state return the maximal possible reward hence the name during execution it is updated based on the agent s observation r max improves upon several previous algorithm it is simpler and more general than kearns and singh s e algorithm covering zero sum stochastic game it ha a built in mechanism for resolving the exploration v exploitation dilemma it formally justifies the optimism under uncertainty bias used in many rl algorithm it is simpler more general and more efficient than brafman and tennenholtz s lsg algorithm for learning in single controller stochastic game it generalizes the algorithm by monderer and tennenholtz for learning in repeated game it is the only algorithm for learning in repeated game to date which is provably efficient considerably improving and simplifying previous algorithm by banos and by megiddo 
we consider the problem of how an agent creates a discrete spatial representation from it continuous interaction with the environment such representation will be the minimal one that explains the experience of the agent in the environment in this 
in this paper we describe a statistical method for d object detection we represent the statistic of both object appearance and non object appearance using a product of histogram each histogram represents the joint statistic of a subset of wavelet coefficient and their position on the object our approach is to use many such histogram representing a wide variety of visual attribute using this method we have developed the first algorithm that can reliably detect human face with out of plane rotation and the first algorithm that can reliably detect passenger car over a wide range of viewpoint 
a simple effective way to model image is to represent each input pattern by a linear combination of component vector where the amplitude of the vector are modulated to match the input this approach includes principal component analysis independent component analysis and factor analysis in practice image are subjected to randomly selected transformation of a known nature such a translation and rotation direct use of the above method will lead to severely blurred component that tend to ignore the more interesting and useful structure in previous work we introduced a clustering algorithm that is invariant to transformation in this paper we propose a method called transformed component analysis which incorporates a discrete hidden variable that account for transformation and us the expectation maximization algorithm to jointly extract component and normalize for transformation we illustrate the algorithm using a shading problem facial expression modeling and written digit recognition 
it is commonly held that skeleton are sensitive to noise it is also believed that smoothing typically invoked to combat noise obeys the causality principle that no new structure are created via smoothing we demonstrate that both view are incorrect we characterize how smooth point of the skeleton evolve under a general boundary evolution with the corollary that when the boundary is smoothed by a geometric heat equation the skeleton evolves according to a related geometric heat equation the surprise is that while certain aspect of the skeleton simplify a one would expect others can behave wildly including the creation of new skeleton branch fortunately such section can be flagged a ligature or those portion of the skeleton related to shape concavity our analysis also includes junction and an explicit model for boundary noise provided a smoothness condition is met the skeleton can often reduce noise however when the smoothness condition is violated the skeleton can change violently which we speculate corresponds to situation in which part are created e g when the handle appears on a rotating cup 
a challenge of future knowledge management and decision support system is to combine the storage and effective reuse of data systematically captured a process or system information with user experience in dealing with problem and non trivial situation in cbr situation specific user experience are typically captured in case in our approach case are linked within a semantic network of more general domain knowledge in this paper we present a way to automate the construction and dynamical refinement of such a model of case specific and general knowledge on the basis of external process data continuously being generated a data mining method based on a bayesian network approach is used we are also looking into how the notion of causality being a central issue in both bns and model based ai can be compared and better understood by relating it to such a combined model 
the question of how software agent can learn strategic behavior in complex continually changing multi agent environment is not only a challenging forefront of theoretical research but potentially of immense practical importance a well in such system it would be difficult at best to hand code fixed strategy that would always perform well with high confidence especially if the other agent in the environment change their behavior over time using adaptive learning algorithm hence the need for learning a a component of overall agent programming methodology is readily apparent we expect this to be particularly true in the domain of agent economy in which large population of agent engage in various form of economic activity with each other and possibly with human a well 
td is a popular family of algorithm for approximate policy evaluation in large mdps td work by incrementally updating the value function after each observed transition it ha two major drawback it make inefficient use of data and it requires the user to manually tune a stepsize schedule for good performance for the case of linear value function approximation and the least square td lstd algorithm of bradtke and barto eliminates all stepsize parameter and improves data efficiency this paper extends bradtke and barto s work in three significant way first it present a simpler derivation of the lstd algorithm second it generalizes from to arbitrary value of at the extreme of the resulting algorithm is shown to be a practical formulation of supervised linear regression third it present a novel intuitive interpretation of lstd a a model based reinforcement learning technique 
we consider the problem of how an agent creates a discrete spatial representation from it continuous interaction with the environment such representation will be the minimal one that explains the experience of the agent in the environment in this 
this paper present a pronoun resolution algorithm that adheres to the constraint and rule of centering theory grosz et al and is an alternative to brennan et al s algorithm the advantage of this new model the left right centering algorithm lrc lie in it incremental processing of utterance and in it low computational overhead the algorithm is compared with three other pronoun resolution method hobbs syntax based algorithm strube s s list approach and the bfp centering algorithm all four method were implemented in a system and tested on an annotated subset of the treebank corpus consisting of pronoun the noteworthy result were that hobbs and lrc performed the best 
this paper is concerned with the simulation of the partial differential equation pde driven evolution of a closed surface by mean of an implicit representation in most application the natural choice for the implicit representation is the signed distance function to the closed surface osher and sethian propose to evolve the distance function with a hamilton jacobi equation unfortunately the solution to this equation is not a distance function a a consequence the practical application of the level set method is plagued with such question a when do we have to reinitialize the distance function how do we reinitialize the distance function etc which reveal a disagreement between the theory and it implementation this paper proposes an alternative to the use of hamilton jacobi equation which eliminates this contradiction in our method the implicit representation always remains a distance function by construction and the implementation doe not differ from the theory anymore this is achieved through the introduction of a new equation besides it theoretical advantage the proposed method also ha several practical advantage which we demonstrate in three application i the segmentation of the human cortex surface from mri image using two coupled surface ii the construction of a hierarchy of euclidean skeleton of a d surface iii the reconstruction of the surface of d object through stereo 
we present a new view of image segmentation by pairwise similarity we interpret the similarity a edge ows in a markovrandom walk and study the eigenvalue and eigenvectors of thewalk s transition matrix this interpretation show that spectralmethods for clustering and segmentation have a probabilistic foundation in particular we prove that the normalized cut methodarises naturally from our framework finally the framework providesa principled method for learning the 
the concept of averaging over classiers is fundamental to thebayesian analysis of learning based on this viewpoint it ha recentlybeen demonstrated for linear classiers that the centre ofmass of version space the set of all classiers consistent with thetraining set also known a the bayes point exhibit excellentgeneralisation ability however the billiard algorithm a presentedin is restricted to small sample size because it requireso m of memory and o n 
kernel based learning algorithm allow the mapping of data set into an infinite dimensional feature space in which a classification may be perform ed a such kernel method represent a powerful approach to the solution of many non linear problem however kernel method do suffer from one unfortunate drawback the gram matrix contains row and column where is the number of data point many operation are precluded e g matrix inverse when data set containing more than about point are encountered one approach to resolving these issue is to look for s parse representation of the data set a sparse representation contains a reduc ed number of example loosely speaking we are interested in extracting the maximum amount of information from the minimum number of data point to achieve this in a principled manner we are interested in estimating the amount of information each data point contains in the framework presented here we make use of the bayesian methodology to determine how much information is gained from each data point 
the paper present a structured modeling language sml and a relational database framework for specification and automated generation of causal model the framework describes a relational database scheme for encoding a library of causal network template modeling the basic component in a modeling domain sml provides a formal language for specifying model a structured component that can be composed from the basic component the language enables specification of model a parameterized relational query that can be instantiated for specific model instance the paper describes an algorithm that given a library and a specification computes a causal model in time and space linear in the number of basic component the algorithm enables model reuse by combining model fragment from the template library to compose new model the present automated modeling approach ha been implemented using the structured query language sql and a relational database environment the approach ha been successfully used for modeling an automated work cell in a real life digital manufacturing application 
considerable research effort ha been invested in improving the effectiveness of information retrieval system technique such a relevance feedback thesaural expansion and pivoting all provide better quality response to query when tested in standard evaluation framework but such enhancement can add to the cost of evaluating query in this paper we consider the pragmatic issue of how to improve the cost effectiveness of searching we describe a new inverted file structure using quantized weight that provides superior retrieval effectiveness compared to conventional inverted file structure when early termination heuristic are employed that is we are able to reach similar effectiveness level with le computational cost and so provide a better cost performance compromise than previous inverted file organisation 
we investigate a new kernel based classifier the kernel fisher discriminant kfd a mathematical programming formulation based on the observation that kfd maximizes the average marginpermits an interesting modification of the original kfd algorithm yielding the sparse kfd we find that both kfd and the proposed sparse kfd can be understood in an unifying probabilistic context furthermore we show connection to support vector machine and relevance vector machine from this understanding we are able to outline an interesting kernel regression technique based upon the kfd algorithm simulation support the usefulness of our approach 
this paper present a geometric based approach for multiple mobile robot motion coordination all the robot path being computed independently we address the problem of coordinating the motion of the robot along their own path in such a way they do not collide each other the proposed algorithm is based on a bounding box representation of the obstacle in the so called coordination diagram the algorithm is resolution complete it efficiency is illustrated by example involving more than robot 
to perform rational decision making autonomous agent need considerable computational resource in multi agent setting when other agent are present in the environment these demand are even more severe we investigate way in which the agent s knowledge and the result of deliberative decision making can be compiled to reduce the complexity of decision making procedure and to save time in urgent situation we use machine learning algorithm to compile decision theoretic deliberation into condition action rule on how to coordinate in a multi agent environment using different learning algorithm we endow a resource bounded agent with a tapestry of decision making tool ranging from purely reactive to fully deliberative one the agent can then select a method depending on the time constraint of the particular situation we also propose combining the decision making tool so that for example more reactive method serve a a pre processing stage to the more accurate but slower deliberative decision making one we validate our framework with experimental result in simulated coordinated defense the experiment show that compiling the result of decision making save deliberation time while offering good performance in our multi agent domain 
we present a method for extracting part of object from whole e g speedometer from car given a very large corpus our method find part word with ranked by the system the part list could be scanned by an end user and added to an existing ontology such a wordnet or used a a part of a rough semantic lexicon 
we consider the problem of measuring the eigenvalue of a randomlydrawn sample of point we show that these value can bereliably estimated a can the sum of the tail of eigenvalue furthermore the residual when data is projected into a subspace isshown to be reliably estimated on a random sample experimentsare presented that conrm the theoretical result 
the blind source separation problem is concerned with extractionof the underlying source signal from a set of theirlinear mixture where the mixing matrix is unknown it wasdiscovered recently that exploiting the sparsity of sourcesin an appropriate representation according to some signaldictionary dramatically improves the quality of separation in this work we use the property of multiscale transforms such a wavelet or wavelet packet to decompose signalsinto set of local 
the paper proposes and empirically motivates an integration of supervised learning with unsupervised learning to deal with human bias in summarization in particular we explore the use of probabilistic decision tree within the clustering framework to account for the variation a well a regularity in human created summary the corpus of human created extract is created from a newspaper corpus and used a a test set we build probabilistic decision tree of different flavor and integrate each of them with the clustering framework experiment with the corpus demonstrate that the mixture of the two paradigm generally give a significant boost in performance compared to case where either of the two is considered alone 
reinforcement learning rl problem withhidden state present significant obstaclesto prevailing rl method in this paper we present experiment conducted usinga straightforward approach to solving suchproblems that train artificial neural networkswith recurrent connection to representaction policy using evolutionary search 
we introduce a problem class which we term activity monitoring such problem involve monitoring the behavior of alarge population of entity for interesting event requiringaction we present a framework within which each of theindividual problem ha a natural expression a well a amethodology for evaluating performance of activity monitoringtechniques we show that two superficially differenttasks news story monitoring and intrusion detection canbe expressed naturally within the 
abstract we address the problem of integrating multi frame stereo and shading cue within the framework of optimization in the inflnite dimensional space of piecewise smooth surface cue integration then reduces to the determination of region where prior assumption on the re ectance of the surface can be enforced by combining cue our formulation allows deflning a well posed problem even when reconstruction from stereo or shading in isolation would be ill posed for a simplifled model we prove the necessary condition for optimality and propose an iterative optimization algorithm which we implement using ultranarrowband level set method 
this paper present a novel approach for generating and analyzing epipolar plane image epi from video s equences taken from a moving platform subject to vibration s o that the d model of an arbitrary scene can be constructed two problem are solved in our approach how to gen erate epi from video under a more general motion than a pure translation how to analyze the huge amount of data in the epi robustly and efficiently for the first proble m a d image stabilization method is proposed which decoup le the vibration from the vehicle s motion so that good ep is and panoramic view image pvis can be generated for the second problem we propose an efficient panoramic e pi analysis pepia method in which only one scanline of each epi is processed the pepia combine advantage of pvis and epi and consists of three important step loc u orientation detection motion boundary localization and occlusion resolution recovery the output of the pe pia a layered d panorama is very useful in visual navig ation and virtual reality modeling since camera calibration image segmentation feature extraction and matching are a voided all the proposed algorithm are fully automatic and rather general result on real image sequence are given 
we analyze the bit error probability of multiuser demodulator for directsequence binary phase shift keying d bpsk cdma channel with additive gaussian noise the problem of multiuser demodulation is cast into the finite temperature decoding problem and replica analysis is applied to evaluate the performance of the resulting mpm marginal posterior mode demodulator which include the optimal demodulator and the map demodulator a special case an approximate implementation of demodulator is proposed using analog valued hopfield model a a naive mean field approximation to the mpm demodulator and it performance is also evaluated by the replica analysis result of the performance evaluation show effectiveness of the optimal demodulator and the mean field demodulator compared with the conventional one especially in the case of small information bit rate and low noise level 
this paper describes an extension of a technique for the recognition and tracking of every day object in cluttered scene the goal is to build a system in which ordinary desktop object serve a physical icon in a vision based system for man machine interaction in such a system the manipulation of object replaces user command a view variant recognition technique developed by the second author ha been adapted by the first author for a problem of recognising and tracking object on a cluttered background in the presence of occlusion this method is based on sampling a local appearance function at discrete viewpoint by projecting it onto a vector of receptive field which have been normalised to local scale and orientation this paper report on the experimental validation of the approach and of it extension to the use of receptive field based on colour the experimental result indicate that the second author s technique doe indeed provide a method for building a fast and robust recognition technique furthermore the extension to coloured receptive field provides a greater degree of local discrimination and an enhanced robustness to variable background condition the approach is suitable for the recognition of general object a physical icon in an augmented reality 
we use the color cooccurrence histogram ch for recognizing object in image the color ch keep track of the number of pair of certain colored pixel that occur at certain separation distance in image space the color ch add geometric information to the normal color histogram which abstract away all geometry we compute model chs based on image of known object taken from different point of view these model chs are then matched to subregions in test image to find the object by adjusting the number of color and the number of distance used in the ch we can adjust the tolerance of the algorithm to change in lighting viewpoint and the flexibility of the object we develop a mathematical model of the algorithm s false alarm probability and use this a a principled way of picking most of the algorithm s adjustable parameter we demonstrate our algorithm on different object showing that it recognizes object in spite of confusing background clutter partial occlusion and flexing of the object 
we present a system that consists of one camera connected to a personal computer that can a select and track a number of high contrast point feature on a sequence of image b estimate their three dimensional motion and position relative to an inertial reference frame assuming rigidity c handle occlusion that cause point feature to disappear a well a new feature to appear the system can also d perform partial self calibration and e check for consistency of the rigidity 
the ability to learn from user interaction is an important asset for content based image retrieval cbir system over short time scale it enables the integration of information from successive query assuring faster convergence to the desired target image over long time scale retrieval session it allows the retrieval system to tailor itself to the preference of particular user we address the issue of learning by formulating retrieval a a problem of bayesian inference the new formulation is shown to have various advantage over previous approach it lead to the minimization of the probability of retrieval error enables region based query without prior image segmentation and suggests elegant procedure for combining multiple user specification a a consequence of all this it enables the design of short and long term learning mechanism that are simple intuitive and extremely efficient in term of computational and storage requirement we introduce two such algorithm and present experimental evidence illustrating the clear advantage of learning for cbir 
we present a new approach to bounding the true error rate of a continuous valued classifier based upon pac bayes bound the method first construct a distribution over classifier by determining how sensitive each parameter in the model is to noise the true error rate of the stochastic classifier found with the sensitivity analysis can then be tightly bounded using a pac bayes bound in this paper we demonstrate the method on artificial neural network with result of a order of magnitude improvement v the best deterministic neural net bound 
intelligent room equipped with video camera can exhibit compelling behavior many of which depend on object recognition unfortunately object recognition algorithm are rarely written with a normal consumer in mind leading to program that would be impractical to use for a typical person these impracticality include speed of execution elaborate training ritual and setting adjustable parameter we present an algorithm that can be trained with only a few image of the object that requires only two parameter to be set and that run at hz on a normal pc with a normal color camera the algorithm represents an object s feature a small quantized edge template and it represents the object s geometry with hough kernel the hough kernel implement a variant of the generalized hough transform using simple d image correlation the algorithm also us color information to eliminate part of the image from consideration we give our result in term of roc curve for recognizing a computer keyboard with partial occlusion and background clutter even with two hand occluding the keyboard the detection rate is with a false alarm rate of 
we propose a method to learn heterogeneous model of object class for visual recognition the training image contain a preponderance of clutter and learning is unsupervised our model represent object a probabilistic constellation of rigid part feature the variability within a class is represented by a joint probability density function on the shape of the constellation and the appearance of the part our method automatically identifies distinctive feature in the training set the set of model parameter is then learned using expectation maximization see the companion paper for detail when trained on different unlabeled and unsegmented view of a class of object each component of the mixture model can adapt to represent a subset of the view similarly different component model can also specialize on sub class of an object class experiment on image of human head leaf from different specie of tree and motor car demonstrate that the method work well over a wide variety of object 
we introduce an expandable bayesian network ebn to handle the combination of diverse multiple homogeneous evidence set an ebn is an augmented bayesian network which instantiates it structure at runtime according to the structure of input we show an application of an ebn for a multi view d object description problem in computer vision the experiment show that the proposed method give reasonable performance even for an unlearned structure of input data 
the application of boosting procedure to decisiontree algorithm ha been shown to producevery accurate classifier these classifiersare in the form of a majority vote overa number of decision tree unfortunately these classifier are often large complex anddifficult to interpret this paper describes anew type of classification rule the alternatingdecision tree which is a generalization ofdecision tree voted decision tree and voteddecision stump at the same time 
an algorithm is presented for video georegistration with a particular concern for aerial video i e video captured from an airborne platform the algorithm s input is a video stream with telemetry camera model specifcation suficient to define an initial estimate of the view and geodetically calibrated reference imagery coaligned digital orthoimage and elevation map the output is a spatial registration of the video to the reference so that it inherits the available geodetic coordinate the video is processed in a continuous fashion to yield a corresponding stream of georegistered result quantitative result of evaluating the developed approach with real world aerial video also are presented the result suggest that the developed approach may provide valuable input to the analysis and interpretation of aerial video 
we perform a linguistic analysis of document during indexing for information retrieval by eliminating index term that occur only in subordinate clause index size is reduced by approximately without adversely affecting precision or recall these result hold for two corpus a sample of the world wide web and an electronic encyclopedia 
in this paper a computational approach for resolving zero pronoun in spanish text is proposed our approach ha been evaluated with partial parsing of the text and the result obtained show that these pronoun can be resolved using similar technique that those used for pronominal anaphora compared to other well known baseline on pronominal anaphora resolution the result obtained with our approach have been consistently better than the rest 
this paper deal with the problem of incorporating natural regularity condition on the motion in an map estimator for structure and motion recovery from uncalibrated image sequence the purpose of incorporating these constraint is to increase performance and robustness autocalibration and structure and motion algorithm are known to have problem with i the frequently occurring critical camera motion ii local minimum in the non linear optimization and iii the high correlation between different intrinsic and extrinsic parameter of the camera e g the coupling between focal length and camera position the camera motion both intrinsic and extrinsic parameter is modelled a a random walk process where the inter frame motion are assumed to be independentlynormally distributed the proposed scheme is demonstrated on both simulated and real data showing the increased performance 
context specific independence csi refers to conditional independency that are true only in specific context it ha been found useful in various inference algorithm for bayesian network this paper study the role of csi in general we provide a characterization of the computational leverage offered by csi without referring to particular inference algorithm we identify the issue that need to be addressed in order to exploit the leverage and show how those issue can be addressed we also provide empirical evidence that demonstrates the usefulness of csi 
an energy model based approach for estimating object boundary is presented we study a particular energy which minimizer can be determined the method estimate the unknown number of object and draw object boundary by selecting the best level line computed from level set of the original image unlike previous standard method the proposed method doe not require iteration for minimizing the energy in addition our segmentation algorithm combine anisotropic diffusion based regularization with level line selection to extract smooth object boundary experimental result on d biomedical and meteorological image are reported 
the golomb ruler problem ha been proposed a a challengingconstraint satisfaction problem we consider a large number ofdifferent model of this problem both binary and non binary the problemcan be modelled using quaternary constraint but in practice usinga set of auxiliary variable and ternary constraint give better result a binary encoding of the problem give a smaller search tree but isimpractical because it take far longer to run we compare variable orderingheuristics 
most statistical and machine learning algorithm assume that the data is a random sample drawn from a stationary distribution unfortunately most of the large database available for mining today violate this assumption they were gathered over month or year and the underlying process generating them changed during this time sometimes radically although a number of algorithm have been proposed for learning time changing concept they generally do not scale well to very large database in this paper we propose an efficient algorithm for mining decision tree from continuously changing data stream based on the ultra fast vfdt decision tree learner this algorithm called cvfdt stay current while making the most of old data by growing an alternative subtree whenever an old one becomes questionable and replacing the old with the new when the new becomes more accurate cvfdt learns a model which is similar in accuracy to the one that would be learned by reapplying vfdt to a moving window of example every time a new example arrives but with o complexity per example a opposed to o w where w is the size of the window experiment on a set of large time changing data stream demonstrate the utility of this approach 
we develop a hierarchical generative model to study cue combination the model map a global shape parameter to local cuespecific parameter which in turn generate an intensity image inferring shape from image is achieved by inverting this model inference produce a probability distribution at each level using distribution rather than a single value of underlying variable at each stage preserve information about the validity of each local cue for the given image this allows the model unlike standard combination model to adaptively weight each cue based on general cue reliability and specific image context we describe the result of a cue combination psychophysics experiment we conducted that allows a direct comparison with the model the model provides a good fit to our data and a natural account for some interesting aspect of cue combination understanding cue combination is a fundamental step in developing computational model of visual perception because many aspect of perception naturally involve multiple cue such a binocular stereo motion texture and shading it is often formulated a a problem of inferring or estimating some relevant parameter e g depth shape position by combining estimate from individual cue an important finding of psychophysical study of cue combination is that cue vary in the degree to which they are used in different visual environment weight assigned to estimate derived from a particular cue seem to reflect it estimated reliability in the current scene and viewing condition for example motion and stereo are weighted approximately equally at near distance but motion is weighted more at far distance presumably due to distance limit on binocular disparity experiment have also found these weighting sensitive to image manipulation if a cue is weakened such a by adding noise then the uncontaminated cue is utilized more in making depth judgment a recent study ha shown that observer can adjust the weighting they assign to a cue based on it relative utility for a particular task from these and other experiment we can identify two type of information that determine relative cue weighting cue reliability it relative utility in the context of the task and general viewing condition and region informativeness cue information available locally in a given image 
extracting rule from rbfs is not a trivial task because of nonlinear function or high input dimensionality in such case some of the hidden unit of the rbf network have a tendency to be shared across several output class or even may not contribute 
a non parametric estimator of density gradient the mean shift is employed in the joint spatial range value domain of gray level and color image for discontinuity preserving filtering and image segmentation property of the mean shift are reviewed and it convergence on lattice is proven the proposed filtering method associate with each pixel in the image the closest local mode in the density distribution of the joint domain segmentation into a piecewise constant structure requires only one more step fusion of the region associated with nearby mode the proposed technique ha two parameter controlling the resolution in the spatial and range domain since convergence is guaranteed the technique doe not require the intervention of the user to stop the filtering at the desired image quality several example for gray and color image show the versatility of the method and compare favorably with result described in the literature for the same image 
we investigate the ability of two central encoding method to propagate reachability and relevance information using resolution step more specifically we compare the ability of unit propagation and higher order resolution step to propagate reachability and relevance information in the context of the linear and graphplan encoding scheme to the ability of a natural class of reachability and relevance algorithm that operate at the plan level a a result of our observation and additional consideration we experiment with a preprocessing step based on limited binary resolution that show nice result 
we analyze gallager code by employing a simple mean field ap proximation that distorts the model geometry and preserve important interaction between site the method naturally recovers the prob ability propagation decoding algorithm a an extremization of a proper free energy we find a thermodynamic phase transition that coincides with information theoretical upper bound and explain the practical co de performance in term of the free energy landscape 
we propose a distribution based pruning of n gram backoff language model instead of the conventional approach of pruning n gram that are infrequent in training data we prune n gram that are likely to be infrequent in a new document our method is based on the n gram distribution i e the probability that an n gram occurs in a new document experimental result show that our method performed word perplexity reduction better than conventional cutoff method 
in this paper we propose a novel inhomogeneous gibbs model by the minimax entropy principle and apply it to face modeling the maximum entropy principle generalizes the statistical property of the observed sample and result in the gibbs distribution while the minimum entropy principle make the learnt distribution close to the observed one to capture the fine detail of a face an inhomogeneous gibbs model is derived to learn the local statistic of facial feature paint to alleviate the high dimensionality problem of face model we propose to learn the distribution in a subspace reduced by principal component analysis or pca we demonstrate that our model effectively capture important and subtle non gaussian face pattern and efficiently generates good face model 
accurate tracking can facilitate the automatic extraction of metric information from video analysis many tracking system rely on a sufficiently accurate dynamic model these dynamic model must be either known a priori or learnt this paper address the problem of determining dynamical system model from observed visual motion where it is assumed that the motion cannot be modeled by a single dynamical system the change in motion from one system to another need to be detected previous work ha dealt with maintaining multiple hypothesis for repetitive motion rather than maintaining multiple hypothesis one can learn the dynamic model that apply and identify the change between the model specifically a method for high dimensional motion segmentation is presented by using a two step recursive least square algorithm break point of system dynamic at which a model switching must be performed are predicted after segmentation system identification technique can be used to fit dynamic model 
in this paper we present an approach to the disambiguation of capitalized word when they are used in the position where capitalization is expected such a the first word in a sentence or after a period quote etc such word can act a proper name or can be just capitalized variant of common word the main feature of our approach is that it us a minimum of prebuilt resource and tire to dynamically infer the disambiguation clue from the entire document the approach wa thoroughly tested and achieved about accuracy on unseen text from the new york time corpus 
subproblem generation solution and recombination is a standard approach to combinatorial optimization problem in many setting identifying suitable subproblems is itself a significant component of the technique such subproblems are often identified using a heuristic rule here we show how to use machine learning to make this identification in particular we use a learned objective function to direct search in an appropriate space of subproblem decomposition we demonstrate the efficacy of our technique for problem decomposition on a particular wellknown combinatorial optimization problem graph coloring for geometric graph 
we describe a framework that help student learn from example by generating example problem solution whose level of detail is tailored to the student domain knowledge the framework us natural language generation technique and a probabilistic student model to selectively introduce gap in the example solution so that the student can practice applying rule learned from previous example in problem solving episode of difficulty adequate to her knowledge filling in solution gap is part of the meta cognitive skill known a self explanation generate explanation to oneself to clarify an example solution which is crucial to effectively learn from example in this paper we describe how example with tailored solution gap are generated and how they are used to support student in learning through gap filling self explanation 
extracting rule from rbfs is not a trivial task because of nonlinear function or high input dimensionality in such case some of the hidden unit of the rbf network have a tendency to be shared across several output class or even may not contribute 
machine perception can benefit from the use of feature extracted from data provided by a variety of sensor modality recent advance in sensor design make it possible to incorporate multiple sensor into vision system for increased capability two important issue must be considered for the integration task the sensor must be spatially coregistered and the phenomenology must be compatible in this paper we address these issue a they apply to the problem of automatic modeling of building structure from aerial view we present a methodology to incorporate cue extracted from ifsar interferometric synthetic aperture radar to significantly improve the performance and the quality of the result of an existing system that relies on electro optical panchromatic image while reducing processing time quantitative evaluation are given 
in this paper we describe a new technique for general purpose interactive segmentation of n dimensional image the user mark certain pixel a object or background to provide hard constraint for segmentation additional soft constraint incorporate both boundary and region information graph cut are used to find the globally optimal segmentation of the n dimensional image the obtained solution give the best balance of boundary and region property among all segmentation satisfying the constraint the topology of our segmentation is unrestricted and both object and background segment may consist of several isolated part some experimental result are present ed in the context of photo video editing and medical image segmentation we also demonstrate an interesting gestalt example a fast implementation of our segmentation method is possible via a new max flow algorithm in 
in this paper we address the problem of matching two image with two different resolution a high resolution image and a low resolution one on the premise that change in resolution act a a smoothing equivalent to change in scale a scale space representation of the high resolution image is produced hence the one to one classical image matching paradigm becomes one to many because the lowresolution image is compared with all the scale space representation of the high resolution one key to the success of such a process is the proper representation of the feature to be matched in scale space we show how to extract interest point at variable scale and we devise a method allowing the comparison of two image at two different resolution the method comprises the use of photometricand rotationinvariant descriptor a geometric model mapping the highresolution image onto a low resolution image region and an image matching strategy based on the robust estimation of this geometric model extensive experiment show that our matching method can be used for scale change up to a factor 
we are interested in the problem of determining a course of action to achieve a desired objective in a non deterministic environment markov decision process mdps provide a framework for representing this action selection problem and there are a number of algorithm that learn optimal policy within this formulation this framework ha also been used to study state space abstraction problem decomposition and policy reuse these technique sacrifice optimality of their solution for improved learning speed in this paper we examine the suboptimality of reusing policy that are solution to subproblems this is done within a restricted class of mdps namely those where non zero reward is received only upon reaching a goal state we introduce the definition of a subproblem within this class and provide motivation for how reuse of subproblem solution can speed up learning the contribution of this paper is the derivation of a tight bound on the loss in optimality from this reuse we examine a bound that is based on bellman error which applies to all mdps but is not tight enough to be useful we contribute our own theoretical result that give an empirically tight bound on this suboptimality 
psychophysical and physiological evidence show that sound localizationof acoustic signal is strongly inuenced by their synchronywith visual signal this eect known a ventriloquism is at workwhen sound coming from the side of a tv set feel a if it werecoming from the mouth of the actor the ventriloquism eectsuggests that there is important information about sound locationencoded in the synchrony between the audio and video signal inspite of this evidence audiovisual 
the paper present a new approach to identifying discourse relation which make use of a particular sampling method called committeebased sampling cbs in the committee based sampling multiple learning model are generated to measure the utility of an input example in classification if it is judged a not useful then the example will be ignored the method ha the effect of reducing the amount of data required for training in the paper we extend cbs for decision tree classifier with an additional extension called error feedback it is found that the method achieves an increased accuracy a well a a substantial reduction in the amount of data for training classifier 
in this paper a new rectification methodis proposed the method is both simple and efficient and can deal with all possible camera motion a minimal image size without any pixel loss is guaranteed the only required information is the oriented fundamental matrix the whole rectification process is carried out directly in the image the idea consists of using a polar parametrization of the image around the epipole the transfer between the image is obtained through the fundamental matrix the proposed method ha important advantage compared to the traditional rectification scheme in some case these approach yield very large image or can not rectify at all even the recently proposed cylindrical rectification method can encounter problem in some case these problem are mainly due to the fact that the matching ambiguity is not reduced to half epipolar line although this last method is more complex than the one proposed in this paper the resulting image are in general larger the performance of the new approach is illustrated with some result on real image pair 
mobile interface need to allow the user and system to adapt their choice of communication mode according to user preference the task at hand and the physical and social environment we describe a multimodal application architecture which combine finite state multimodal language processing a speech act based multimodal dialogue manager dynamic multimodal output generation and user tailored text planning to enable rapid prototyping of multimodal interface with flexible input and adaptive output our testbed application match multimodal access to city help provides a mobile multimodal speech pen interface to restaurant and sub way information for new york city 
this paper present a method by which a reinforcement learning agent can automatically discover certain type of subgoals online by creating useful new subgoals while learning the agent is able to accelerate learning on the current task and to transfer it expertise to other related task through the reuse of it ability to attain subgoals the agent discovers subgoals based on commonality across multiple path to a solution we cast the task of finding these commonality a a multiple instance learning problem and use the concept of diverse density to find solution we illustrate this approach using several gridworld task 
bayesian inference ha been used successfully formany problem where the aim is to infer the parametersof a model of interest in this paper we formulatethe three dimensional reconstruction problemas the problem of inferring the parameter of a surfacemodel from image data and show how bayesianmethods can be used to estimate the parameter of thismodel given the image data thus we recover the threedimensional description of the scene this approachalso give great flexibility we can 
previous work in mixture model clusteringhas focused primarily on the issue of modelselection model scoring function includingpenalized likelihood and bayesian approximation can guide a search of the model parameterand structure space relatively littleresearch ha addressed the issue of howto move through this space local optimizationtechniques such a expectation maximization solve only part of the problem westill need to move between different local optimum the 
we present in this paper a formalization of multiset relation in acl and we show how multisets can be used to prove non trivial termination property in acl intuitively multisets are set that admit multiple occurrence of element 
following a dependency based framework that admits no intermediate phrasal node and allows no crossing of syntactic dependency link we discus how chinese sentence are analysed and annotated using an sgml based scheme issue related to tolerance of error at various level of analysis and compatibility with other syntactic framework are addressed 
clustering algorithm have become increasingly important in handling and analyzing data considerable work ha been done in devising effective but increasingly specific clustering algorithm in contrast we have developed a generalized framework that accommodates diverse clustering algorithm in a systematic way this framework view clustering a a general process of iterative optimization that includes module for supervised learning and instance assignment the framework ha also suggested several novel clustering method in this paper we investigate experimentally the efficacy of these algorithm and test some hypothesis about the relation between such unsupervised technique and the supervised method embedded in them 
decision tree grafting add node to an existing decision tree with the objective of reducing prediction error a new grafting algorithm is presented that considers one set of training data only for each leaf of the initial decision tree the set of case that fail at most one test on the path to the leaf this new technique is demonstrated to retain the error reduction power of the original grafting algorithm while dramatically reducing compute time and the complexity of the inferred tree bias variance analysis reveal that the original grafting technique operated primarily by variance reduction while the new technique reduces both bias and variance 
recent contribution to statistical language modeling for speech recognition have shown that probabilistically parsing a partial word sequence aid the prediction of the next word leading to structured language model that have the potential to outperform n gram existing approach to structured language modeling construct node in the partial parse tree after all of the underlying word have been predicted this paper present a different approach based on probabilistic left corner grammar plcg parsing that extends a partial parse both from the bottom up and from the top down leading to a more focused and more accurate though somewhat le robust search of the parse space at the core of our new structured language model is a fast context sensitive and lexicalized plcg parsing algorithm that us dynamic programming preliminary perplexity and word accuracy result appear to be competitive with previous one while speed is increased 
selecting the most appropriate sense for an ambiguous word in a sentence is a central problem in natural language processing in this paper we present a method that attempt to disambiguate all the noun verb adverb and adjective in a text using the sens provided in wordnet the sens are ranked using two source of information the internet for gathering statistic for word word cooccurrences and wordnet for measuring the semantic density for a pair of word we report an average accuracy of for the first ranked sense and for the first two ranked sens extension of this method for larger window of more than two word are considered 
abstract a new method for d rigid motion estimation is derived under the most general assumption that the measurement are corrupted by inhomogeneous and anisotropic i e heteroscedastic noise this is the case for example when the motion of a calibrated stereo head is to be determined from image pair linearization in the quaternion space transforms the problem into a multivariate heteroscedastic error in variable heiv regression from which the rota tion and translation estimate are obtained simultane ously the signi cant performance improvement is il lustrated for real data by comparison with the result of quaternion subspace and renormalization based ap proaches described in the literature extensive use is made of bootstrap an advanced numerical tool from statistic both to estimate the covariance of the d data point and to obtain con dence region for the rotation and translation estimate bootstrap enables an accurate recovery of these information using only the two image pair serving a input 
requirement engineering is a complex task which benefit from computer support despite the progress made in automatic reasoning on requirement the tool supporting requirement elicitation remain difficult to use in this paper we propose a novel approach where a tool s reasoning is intimately linked to the dialogue it ha with it user because the dialogue is guided by rule ensuring coherence the interaction with the tool is more natural we discus in detail the rule we use to organise the dialogue and how we apply them to the requirement elicitation tool we present an evaluation of this approach demonstrating improvement in usability during the elicitation process 
this paper we propose a set of techniquesto largely automate the process of ka by usingtechnologies based on information extraction ie information retrieval and natural language processing weaim to reduce all the impeding factor mention above andthereby contribute to the wider utility of the knowledgemanagement tool in particular we intend to reduce theintrospection of knowledge engineer or the extendedelicitations of knowledge from expert by extensive textualanalysis using 
in this paper a hybrid disambiguation method for the prepositional phrase pp attachment and interpretation problem is presented the data needed semantic pp interpretation rule and an annotated corpus is described first then the three major step of the disambiguation method are explained cross validated evaluation result for german correct for binary attachment ambiguity correct for interpretation ambiguity show that disambiguation method combining interpretation rule and statistical method might yield significantly better result than nonhybrid disambiguation method 
the use of linear data fusion is a fast developing area in the field of military information and combat system however the use of data fusion in conventional application area is not a wide spread to date linear data fusion ha been used only in application in which substantial knowledge of both the problem domain and the sensor device in use are available however in application such a condition monitoring the problem domain can be very complex with little or no knowledge about the interaction between measured parameter this paper describes the use of non linear self learning or self organising system a a tool for data fusion since these system can learn complex interrelationship between a number of parameter and use this information a a tool for improved classification 
probabilistic graphical model in particular bayesian ne tworks are useful model for representing statistical pattern in propositional domain recent work develops effective technique for learning these model directly from data however these technique apply only to attribute value i e flat representation of the data probabilistic relational model prms allow u to represent much richer dependency structure involving multiple entity and the rela tions between them they allow the property of an entity to depend probabilistically on property of related entity prms represent a generic dependence which is then instantiated for specific circumstance i e for a particular se t of entity and relation between them friedman et al showed how to learn prms from relational data and presented technique for learning both parameter and probabilistic depe ndency structure for the attribute in a relational model he re we examine the benefit that class hierarchy can provide prms we show how the introduction of subclass allows u to use inheritance and specialization to refine our model we show how to learn prms with class hierarchy prmch in two setting in the first the class hierarchy is provided a part of the input in the relational schema for the domain in the second setting in addition to learning the prm we must learn the class hierarchy finally we discus how prm chs allow u to build model that can represent model for both particular instance in our domain and class of object in our domain bridging the gap between a class based model and an attribute value based model 
vector based information retrieval method such a the vector space model vsm latent semantic indexing lsi and the generalized vector space model gvsm represent both query and document by high dimensional vector learned from analyzing a training corpus of text vsm scale well to large collection but cannot represent term term correlation which prevents it from being used in translingual retrieval gvsm and lsi can represent term term correlation but do not scale well to very large retrieval collection we present a novel method we call approximate dimension equalization ade that combine idea from vsm lsi and gvsm to produce a method that performs well on large collection scale well computationally and can represent term term correlation we compare the performance of ade to the other method on both large and small collection of both monolingual and bilingual text ade outperforms all other method on large bilingual collection and performs close to the best in all other case 
the paper address the problem of class based recognition and image synthesis with varying illumination the classbased synthesis and recognition task are defined a follows given a single input image of an object and a sample of image with varying illumination condition of other object of the same general class capture the equivalence relationship by generation of new image or by invariant among all image of the object corresponding to new illumination condition the key result in our approach is based on a definition of an illumination invariant signature image we call the quotient image which enables an analytic generation of the image space with varying illumination from a single input image and a very small sample of other object of the class in our experiment a few a two object in many case the recognition result outperform by far conventional method and the image synthesis is of remarkable quality considering the size of the database of example image and the mild pre process required for making the algorithm work 
previously the plan language cc golog wa introduced for the purpose of specifying event driven behavior typically found in robot controller so far however cc golog is usable only for projecting the outcome of a plan and it is unclear how to actually execute plan on line on a robot in this paper we provide such an execution model for cc golog and in addition show how to interleave execution with a new kind of time bounded projection along the way we also demonstrate how a typical robot control architecture where a high level controller communicates with low level process via message can be directly modelled in cc golog 
an important problem in clustering is how to decide what is the best set of cluster for a given data set in term of both the number of cluster and the membership of those cluster in this paper we develop four criterion for measuring the quality of different set of cluster these criterion are designed so that different criterion prefer cluster set that generalise at different level of granularity we evaluate the suitability of these criterion for non hierarchical clustering of the result returned by a search engine we also compare the number of cluster chosen by these criterion with the number of cluster chosen by a group of human subject our result demonstrate that our criterion match the variability exhibited by human subject indicating there is no single perfect criterion instead it is necessary to select the correct criterion to match a human subject s generalisation need 
financial prediction is so far the most important application in contemporary scientific study in this paper we present a fully integrated stock prediction system norn finance forecaster a neural oscillatory based recurrent network for finance prediction system to provide both a long term trend prediction and b short term stock price prediction one of the major characteristic of the proposed system is the automation of the conventional financial technical analysis technique such a market pattern analysis via noegm neural oscillatory based elastic graph matching model and it integration with the time difference recurrent neural network model this will provide a fully integrated and automated tool for analytic and investigation of stock investment from the implementation point of view the stock pricing information of major hong kong stock in the period of to are being adopted for system training and evaluation a compared with contemporary neural prediction model the proposed system ha achieved challenging result in term of efficiency and accuracy 
the analysis of human action captured in video sequence ha been a topic of considerable interest in computer vision much of the previous work ha focused on the problem of action or activity recognition but ignored the problem of detecting action boundary in a video sequence containing unfamiliar and arbitrary visual action this paper present an approach to this problem based on detecting temporal discontinuity of the spatial pattern of image motion that capture the action we represent frame to frame optical flow in term of the coefficient of the most significant principal component computed from all the flow field within a given video sequence we then detect the discontinuity in the temporal trajectory of these coefficient based on three different measure we compare our segment boundary against those detected by human observer on the same sequence in a recent independent psychological study of human perception of visual event we show experimental result on the two sequence that were used in this study our experimental result are promising both from visual evaluation and when compared against the result of the psychological study 
although bayesian model averaging is theoretically the optimal method for combining learned model it ha seen very little use in machine learning in this paper we study it application to combining rule set and compare it with bagging and partitioning two popular but more ad hoc alternative our experiment show that surprisingly bayesian model averaging s error rate are consistently higher than the other method further investigation show this to be due to a marked tendency to overt on the part of bayesian model averaging contradicting previous belief that it solves or avoids the overtting problem 
an approach for estimating composite independentobject and camera image motion is proposed the approach employ spatio temporal flow modelslearned through observing typical movement ofthe object to decompose image motion into independentobject and camera motion the spatiotemporalflow model of the object motion are representedas a set of orthogonal flow base that arelearned using principal component analysis of instantaneousflow measurement from a stationarycamera these model 
levin s taxonomy of verb and their class is a widely used resource for lexical semantics in her framework some verb such a give exhibit no class ambiguity but other verb such a write can inhabit more than one class in some of these ambiguous case the appropriate class for a particular token of a verb is immediately obvious from inspection of the surrounding context in others it is not and an application which want to recover this information will be forced to rely on some more or le elaborate process of inference we present a simple statistical model of verb class ambiguity and show how it can be used to carry out such inference 
we describe a reconstruction method of multiple motion scene which are the scene containing multiple moving object from uncalibrated view assuming that the object are moving with constant velocity the method recovers the scene structure the trajectory of the moving object the camera motion and the camera intrinsic parameter except skews simultaneously the number of the moving object is automatically detected without prior motion segmentation the method is based on a uni ed geometrical representation of the static scene and the moving object it rst performs a projective reconstruction using a bilinear factorization algorithm and then convert the projective solution to a euclidean one by enforcing metric constraint experimental result on synthetic and real image are presented 
this paper is to definetransformations on the symmetry map and illustrateresults for them specifically we illustrate how spuriouselements can be removed gap completed andparts computed despite significant noise 
we describe a joint probabilistic model for modeling the content and inter connectivity of document collection such a set of web page or research paper archive the model is based on a probabilistic factor decomposition and allows identifying principal topic of the collection a well a authoritative document within those topic furthermore the relationship between topic is mapped out in order to build a predictive model of link content among the many application of this approach are information retrieval and search topic identification query disambiguation focused web crawling web authoring and bibliometric analysis 
the hit and the pagerank algorithm are eigenvector method for identifying authoritative or influential article given hyperlink or citation information that such algorithm should give consistent answer is surely a desideratum and in this paper we address the question of when they can be expected to give stable ranking under small perturbation to the hyperlink pattern using tool from matrix perturbation theory and markov chain theory we provide condition under which these method are stable and give specific example of instability when these condition are violated we also briefly describe a modification to hit that improves it stability 
in this paper we consider the problem of estimating the fundamental matrix from point correspondence it is well known that the most accurate estimate of this matrix are obtained by criterion minimizing geometric error when the data are affected by noise it is also well known that these criterion amount to solving non convex optimization problem and hence their solution is affected by the optimization starting point generally the starting point is chosen a the fundamental matrix estimated by a linear criterion but this estimate can be very inaccurate and therefore inadequate to initialize method with other error criterion here we present a method for obtaining a more accurate estimate of the fundamental matrix with respect to the linear criterion it consists of the minimization of the algebraic error taking into account the rank constraint of the matrix our aim is twofold first we show how this nonconvex optimization problem can be solved avoiding local minimum using recently developed convexification technique second we show that the estimate of the fundamental matrix obtained using our method is more accurate than the one obtained from the linear criterion where the rank constraint of the matrix is imposed after it computation by setting the smallest singular value to zero this suggests that our estimate can be used to initialize non linear criterion such a the distance to epipolar line and the gradient criterion in order to obtain a more accurate estimate of the fundamental matrix a a measure of the accuracy the obtained estimate of the epipolar geometry are compared in experiment with synthetic and real data 
we investigate a meta model approach called meta learning using document feature characteristic mudof for the task of automatic textual document categorization it employ a meta learning phase using document feature characteristic document feature characteristic derived from the training document set capture some inherent category specific property of a particular category different from existing categorization method mudof can automatically recommend a suitable algorithm for each category based on the category specific statistical characteristic hence different algorithm may be employed for different category experiment have been conducted on a real world document collection demonstrating the effectiveness of our approach the result confirm that our meta model approach can exploit the advantage of it component algorithm and demonstrate a better performance than existing algorithm 
skewed distribution appear very often in practice unfortunately the traditional zipf distribution often fails to model them well in this paper we propose a new probability distribution the discrete gaussian exponential dgx to achieve excellent fit in a wide variety of setting our new distribution includes the zipf distribution a a special case we present a statistically sound method for estimating the dgx parameter based on maximum likelihood estimation mle we applied dgx to a wide variety of real world data set such a sale data from a large retailer chain u age data from at t and internet clickstream data in all case dgx fit these distribution very well with almost a correlation coefficient in quantile quantile plot our algorithm also scale very well because it requires only a single pas over the data finally we illustrate the power of dgx a a new tool for data mining task such a outlier detection 
the three dimensional motion of human is underdetermined when the observation is limited to a single camera due to the inherent d ambiguity of d video we present a system that reconstructs the d motion of human subject from single camera video relying on prior knowledge about human motion learned from training data to resolve those ambiguity after initialization in d the tracking and d reconstruction is automatic we show result for several video sequence the result show the power of treating d body tracking a an inference problem 
over the last year particle filter have been applied with great success to a variety of state estimation problem we present a statistical approach to increasing the efficiency of particle filter by adapting the size of sample set on the fly the key idea of the kld sampling method is to bound the approximation error introduced by the sample based representation of the particle filter the name kld sampling is due to the fact that we measure the approximation error by the kullback leibler 
this paper discus the supervised learning of morphology using stochastic transducer trained using the expectation maximization em algorithm two approach are presented first using the transducer directly to model the process and secondly using them to define a similarity measure related to the fisher kernel method jaakkola and haussler and then using a memory based learning mbl technique these are evaluated and compared on data set from english german slovene and arabic 
xspirit is a professional expert system shell for knowledge acquisition inference and response using conditional logic and probability composed conditionals on propositional variable with finite domain are the communication tool between the user and the knowledge base making the process of acquisition inference and query comfortable and intelligible xspirit allows partial rather than complete information about the knowledge domain and supplement missing part by the principle of information fidelity by virtue of evident temporary information knowledge undergoes a well defined adaptation process respecting this principle again the construction and transformation of probability distribution a developed here allow acquired knowledge remaining uncertainty and strength of inference to be measured in the information unit bit xspirit allows large scale application with hundred of composed conditionals and umpteen variable 
cognitive modeling with neural network unrealistically ignores the role of knowledge in learning by starting from random weight it is likely that effective use of knowledge by neural network could significantly speed learning a new algorithm knowledge based cascadecorrelation kbcc find and adapts it relevant knowledge in new learning comparison to multi task learning mtl reveals that kbcc us it knowledge more effectively to learn faster 
we propose a statistical dialogue analysis model to determine discourse structure a well a speech act using maximum entropy model the model can automatically acquire probabilistic discourse knowledge from a discourse tagged corpus to resolve ambiguity we propose the idea of tagging discourse segment boundary to represent the structural information of discourse using this representation we can effectively combine speech act analysis and discourse structure analysis in one framework 
a major challenge in producing large scalesimulations of the type used in ecosystemmodeling is the problem of model calibration this paper present a method for solvinga particularly dicult model calibrationtask that arose a part of a global climatechange research project an obvious approachto solving calibration problem is toformulate them a global optimization problemsin which the goal is to nd value forthe free parameter that minimize the errorof the model on 
we propose a novel method for continuous d depth recovery and tracking using calibrated stereo the method integrates stereo correspondence surface reconstruction and tracking by using a new single deformable dual mesh optimization resulting in simplicity robustness and efficiency in order to combine stereo correspondence and structure recovery the method introduces an external energy function defined for a d volume based on cross correlation between the stereo pair the internal energy functional of the deformable dual mesh imposes smoothness on the surface and it serf a a communication tool between the two mesh under the force produced by the energy term the dual mesh deforms to recover and track the d surface the newly introduced dual mesh model which is one of the main contribution of this paper make the system robust against local minimum and yet it is efficient a coarse to fine minimization approach make the system even more efficient tracking is achieved by using the recovered surface a an initial position for the next time frame although the system can effectively utilize initial surface position and disparity data they are not needed for a successful operation which make this system applicable to a wide range of area we present the result of a number of experiment on stereo human face and cloud image which prof that our new method is very effective 
this paper present exclaim a hybrid language for knowledge representation and reasoning originally developed a an operationalization language for the kads knowledge based system kb development methodology exclaim ha a meta level architecture it structure the knowledge on three level namely the domain inference and task level an extension of a description logic is used for implementing the domain level the inference and task level are general logic program integrated with the domain level by mean of upward and downward reflection rule which describe the automatic domain operation performed whenever argument of inference or task are accessed inference and task support non deterministic reasoning which in turn requires a non monotonic domain level description logic offer a set of inference service some not available in other knowledge representation language which are extremely useful in knowledge modeling such inference service include domain level deduction semantic consistency verification and automatic classification of concept we argue that such validation and verification facility are important in assisting a knowledge engineer in developing model these model are reusable due to the layered architecture a well a to the possibility of writing generic inference using a reified membership relation 
we give result about the learnability and required complexity of logical formula to solve classification problem these result are obtained by linking propositional logic with kernel machine in particular we show that decision tree and disjunctive normal form dnf can be represented by the help of a special kernel linking regularized risk to separation margin subsequently we derive a number of lower bound on the required complexity of logic formula using property of algorithm for generation of linear estimator such a perceptron and maximal perceptron learning 
we address the issue of on line detection of communication problem in spoken dialogue system the usefulness is investigated of the sequence of system question type and the word graph corresponding to the respective user utterance by applying both rule induction and memory based learning technique to data obtained with a dutch train time table information system the current paper demonstrates that the aforementioned feature indeed lead to a method for problem detection that performs significantly above baseline the result are interesting from a dialogue perspective since they employ feature that are present in the majority of spoken dialogue system and can be obtained with little or no computational overhead the result are interesting from a machine learning perspective since they show that the rule based method performs significantly better than the memory based method because the former is better capable of representing interaction between feature 
prepositional phrase attachment is a common source of ambiguity in natural language processing we present an unsupervised corpus based approach to prepositional phrase attachment that achieves similar performance to supervised method unlike previous unsupervised approach in which training data is obtained by heuristic extraction of unambiguous example from a corpus we use an iterative process to extract training data from an automatically parsed corpus attachment decision are made using a linear combination of feature and low frequency event are approximated using contextually similar word 
helping end user build and check process model is a challenge for many science and engineering field many ai researcher have investigated useful way of verifying and validating knowledge base for ontology and rule but it is not easy to directly apply them to checking process model other technique developed for checking and refining planning knowledge tend to focus on automated plan generation rather than helping user author process information in this paper we propose a complementary approach which help user author and check process model our system called kanal relates piece of information in process model among themselves and to the existing kb analyzing how different piece of input are put together to achieve some effect it build interdependency model from this analysis and us them to find error and propose fix our initial evaluation show that kanal wa able to find most of the error in the process model and suggest useful fix including the fix that directly point to the source of the error 
it is known that decision tree learning can be viewed a a formof boosting however existing boosting theorem for decision treelearning allow only binary branching tree and the generalization tomulti branching tree is not immediate practical decision tree algorithm such a cart and c implement a trade off betweenthe number of branch and the improvement in tree quality asmeasured by an index function here we give a boosting justificationfor a particular quantitative 
it is known that decision tree learning can be viewed a a formof boosting however existing boosting theorem for decision treelearning allow only binary branching tree and the generalization tomulti branching tree is not immediate practical decision tree algorithm such a cart and c implement a trade off betweenthe number of branch and the improvement in tree quality asmeasured by an index function here we give a boosting justificationfor a particular quantitative 
extracting rule from rbfs is not a trivial task because of nonlinear function or high input dimensionality in such case some of the hidden unit of the rbf network have a tendency to be shared across several output class or even may not contribute 
this paper discus a decision tree approach to the problem of assigning probability to word following a given text in contrast with previous decision tree language model attempt an algorithm for selecting nearly optimal question is considered the model is to be tested on a standard task the wall street journal allowing a fair comparison with the well known trigram model 
in this paper we investigate polysemous adjective whose meaning varies depending on the noun they modify e g fast we acquire the meaning of these adjective from a large corpus and propose a probabilistic model which provides a ranking on the set of possible interpretation we identify lexical semantic information automatically by exploiting the consistent correspondence between surface syntactic cue and lexical meaning we evaluate our result against paraphrase judgment elicited experimentally from human and show that the model s ranking of meaning correlate reliably with human intuition meaning that are found highly probable by the model are also rated a plausible by the subject 
in application such a translation and paraphrase operation are carried out on grammar at the meta level this paper show how a meta grammar defining structure at the meta level is useful in the case of such operation in particular how it solves problem in the current definition of synchronous tag shieber caused by ignoring such structure in mapping between grammar for application such a translation moreover essential property of the formalism remain unchanged 
invariance to topographic transformation such a translation andshearing in an image ha been successfully incorporated into feedforwardmechanisms e g convolutional neural network quot tangentpropagation quot we describe a way to add transformation invarianceto a generative density model by approximating the nonlineartransformation manifold by a discrete set of transformation anem algorithm for the original model can be extended to the newmodel by computing expectation over 
modern classification application necessitate supplementing the fewavailable labeled example with unlabeled example to improve classificationperformance we present a new tractable algorithm for exploitingunlabeled example in discriminative classification this is achievedessentially by expanding the input vector into longer feature vector viaboth labeled and unlabeled example the resulting classification methodcan be interpreted a a discriminative kernel density estimate 
this paper describes algorithm which rerank the top n hypothesis from a maximum entropy tagger the application being the recovery of named entity boundary in a corpus of web data the first approach us a boosting algorithm for ranking problem the second approach us the voted perceptron algorithm both algorithm give comparable significant improvement over the maximum entropy baseline the voted perceptron algorithm can be considerably more efficient to train at some cost in computation on test example 
in this paper we study the structure from motion problem a a constrained nonlinear least square problem which minimizes the so called reprojection error subject to all co nstraints among multiple image by converting this constrained optimization problem to an unconstrained one we obtain a multiview version of the normalized epipolar constraint of two view such a multiview normalized epipolar constraint serf a a statistically optimal objective fun ction for motion and structure estimation since such a function is defined naturally on a product of stiefel manifold we show how to use geometric optimization technique to minimize it we present experimental result on real image to evaluate the proposed algorithm 
we introduce a number of new result in the context of multiview geometry from general algebraic curve we start with the derivation of the extended kruppa s equation which are responsible for describing the epipolar constraint of two projection of a general non planar algebraic curve a part of the derivation of those constraint we address the issue of dimension analysis and a a result establish the minimal number of algebraic curve required for a solution of the epipolar geometry a a function of their degree and genus we then establish new result on the reconstruction of general algebraic curve from multiple view we address three different representation of curve i the regular point representation for which we show that the reconstruction from two view of a curve of degree admits two solution one of degree and the other of degree ii the dual space representation tangent for which we derive a lower bound for the number of view necessary for reconstruction a a function of the curve degree and genus and iii a new representation to computer vision based on the set of line meeting the curve which doe not require any curve fitting in image space for which we also derive lower bound for the number of view necessary for reconstruction a a function of the curve degree alone 
hierarchy have long been used for organization summarization and access to information in this paper we define summarization in term of a probabilistic language model and use the definition to explore a new technique for automatically generating topic hierarchy by applying a graph theoretic algorithm which is an approximation of the dominating set problem the algorithm efficiently chooses term according to a language model we compare the new technique to previous method proposed for constructing topic hierarchy including subsumption and lexical hierarchy a well a the top tf idf term our result show that the new technique consistently performs a well a or better than these other technique they also show the usefulness of hierarchy compared with a list of term 
we present a sequential monte carlo method applied to additive noise compensation for robust speech recognition in time varying noise the method generates a set of sample according to the prior distribution given by clean speech model and noise prior evolved from previous estimation an explicit model representing noise effect on speech feature is used so that an extended kalman fllter is constructed for each sample generating the updated continuous state estimate a the estimation of the noise parameter and prediction likelihood for weighting each sample minimum mean square error mmse inference of the time varying noise parameter is carried out over these sample by fusion the estimation of sample according to their weight a residual resampling selection step and a metropolis hastings smoothing step are used to improve calculation e ciency experiment were conducted on speech recognition in simulated non stationary noise where noise power changed artiflcially and highly non stationary machinegun noise in all the experiment carried out we observed that the method can have signiflcant recognition performance improvement over that achieved by noise compensation with stationary noise assumption 
in this paper a novel framework for the recovery of d surface of face from single image is developed the underlying principle is shape from recognition i e the idea that pre recognizing face part can constrain the space of possible solution to the image irradiance equation thus allowing robust recovery of the d structure of a specific part shape recovery of the recognized part is based on specialized backpropagation based neural network each of which is employed in the recovery of a particular face part representation using principal component allows to efficiently encode class of object such a nose lip etc the specialized network are designed and trained to map the principal component coefficient of the shading image to another set of principal component coefficient that represent the corresponding d surface shape a method for integrating recovered d surface region by minimizing the sum squared error in overlapping area is also derived quantitative analysis of the reconstruction of the surface part show relatively small error indicating that the method is robust and accurate the recovery of a complete face is performed by minimal squared error merging of face part 
this paper address the problem of probabilistic recognition of activity from local spatio temporal appearance joint statistic of space time filter are employed to define histogram which characterize the activity to be recognized these histogram provide the joint probability density function required for recognition using bayes rule the result is a technique for recognition of activity which is robust to partial occlusion a well a change in illumination in this paper the framework and background for this approach is first described then the family of spatio temporal receptive field used for characterizing activity is presented this is followed by a review of probabilistic recognition of pattern from joint statistic of receptive field response the approach is validated with the result of experiment in the discrimination of person walking in different direction and the recognition of a simple set of hand gesture in an augmented reality scenario 
the paradigm of hebbian learning ha recently received a novel interpretation with the discovery of synaptic plasticity that depends on the relative timing of pre and post synaptic spike this paper derives a temporally dependent learning rule from the basic principle of mutual information maximization and study it relation to the experimentally observed plasticity we nd that a supervised spike dependent learning rule sharing similar structure with the experimentally observed plasticity increase mutual information to a stable near optimal level moreover the analysis reveals how the temporal structure of time dependent learning rule is determined by the temporal lter applied by neuron over their input these result suggest experimental prediction a to the dependency of the learning rule on neuronal biophysical parameter 
until now interval based temporal description logic dl did if at all only admit tboxes of a very restricted form namely acyclic macro definition in this paper we present a temporal dl that overcomes this deficieny and combine interval based temporal reasoning with general tboxes we argue that this combination is very interesting for many application domain an automaton based decision procedure is devised and a tight exptime complexity bound is obtained since the presented logic can be viewed a being equipped with a concrete domain our result can be seen from a different perspective we show that there exist interesting concrete domain for which reasoning with general tboxes in decidable 
time series prediction by artificial neural network anns are traditionally formulated a unconstrained optimization problem a an unconstrained formulation provides little guidance on search direction when a search get stuck in a poor local minimum we have proposed recently to use a constrained formulation in order to use constraint violation to provide additional guidance in this paper we formulate ann learning with cross validation for time series prediction a a non differentiable nonlinear constrained optimization problem based on our theory of lagrange multiplier for discrete constrained optimization we propose an efficient learning algorithm called violation guided back propagation vgbp that computes an approximate gradient using back propagation bp that introduces annealing to avoid blind acceptance of trial point and that applies a relax and tighten r t strategy to achieve faster convergence extensive experimental result on well known benchmark when compared to previous work show one to two order of magnitude improvement in prediction quality while using le weight 
clustering is a widely used knowledge discovery technique it help uncovering structure in data that were not previouslyknown the clustering of large data set ha receiveda lot of attention in recent year however clustering is astill a challenging task since many published algorithm failto do well in scaling with the size of the data set and thenumber of dimension that describe the point or in ndingarbitrary shape of cluster or dealing e ectively with thepresence of noise 
an important issue in visualizing categorical data is how to order categorical value the focus of this paper is on constructing such ordering efficiently without compromising their visual quality 
the problem constructing weak classifier for boosting algorithm is studied we presentan algorithm that produce a linear classifier that is guaranteed to achieve an error betterthan random guessing for any distribution on the data while this weak learneris not useful for learning in general we show that under reasonable condition on thedistribution it yield an effective weak learner for one dimensional problem preliminarysimulations suggest that the same behavior can be 
in this paper we propose a unified non quadratic loss function for regression known a soft insensitive loss function silf silf is a flexible model and posse most of the desirable characteristic of popular non quadratic loss function such a laplacian huber s and vapnik s insensitive loss function we describe the property of silf and illustrate our assumption on the underlying noise model in detail moreover the introduction of silf in regression make it possible to apply bayesian technique on support vector method experimental result on simulated and real world datasets indicate the feasibility of the approach 
we propose a solution to the generic bilinear calibration estimation problem when using a quadratic cost function and restricting to locally translation invariant imaging model we apply the solution to the problem of reconstructing the three dimensional shape and radiance of a scene from a number of defocused image since the imaging process map the continuum of three dimensional space onto the discrete pixel grid rather than discretizing the continuum we exploit the structure of map between finite and infinite dimensional hilbert space and arrive at a principled algorithm that doe not involve any choice of basis or discretization rather these are uniquely determined by the data and exploited in a functional singular value decomposition in order to obtain a regularized solution 
the effect of out of vocabulary oov item in spoken document retrieval sdi are investigated several set of transcription were created for the trec sdr task using a speech recognition system varying the vocabulary size and oov rate and the relative retrieval performance measured the effect of oov term on a simple baseline ir system and on more sophisticated retrieval system are described the use of a parallel corpus for query and document expansion is found to be especially beneficial and with this data set good retrieval performance can be achieved even for fairly high oov rate 
this paper present a formal analysis for a large class of word called alternative marker which includes other than such a and besides these word appear frequently enough in dialog to warrant serious attention yet present natural language search engine perform poorly on query containing them i show that the performance of a search engine can be improved dramatically by incorporating an approximation of the formal analysis that is compatible with the search engine s operational semantics the value of this approach is that a the operational semantics of natural language application improve even larger improvement are possible 
in this paper we investigate the geometry and algebra of multiple projection of line with affine camera previously the case of seven line in three image ha been studied it wa thought that this wa the minimal data necessary for recovering affine structure and motion and that there are in general two solution it wa also thought that these two solution persist with more than seven line in this paper it is shown that the minimal case are six line in three image and five line in four image these case are solved and it is shown that there are in general four solution in both problem two almost minimal case seven line in three image and six line in four image are solved using linear method furthermore it is shown that the solution is in general unique in these almost minimal case finally experiment are conducted on both simulated and real data in order to show the applicability of the theory 
the author propose a novel technique for video summarization based on singular value decomposition svd for the input video sequence we create a feature frame matrix a and perform the svd on it from this svd we are able to not only derive the refined feature space to better cluster visually similar frame but also define a metric to measure the amount of visual content contained in each frame cluster using it degree of visual change then in the refined feature space we find the most static frame cluster define it a the content unit and use the context value computed from it a the threshold to cluster the rest of the frame based on this clustering result either the optimal set of keyframes or a summarized motion video with the user specified time length can be generated to support different user requirement for video browsing and content overview our approach ensures that the summarized video representation contains little redundancy and give equal attention to the same amount of content 
the cluster variation method is a class of approximation method containing the bethe and kikuchi approximation a special case we derive two novel iteration scheme for the cluster variation method one is a fixed point iteration scheme which give a significant improvement over loopy bp mean field and tap method on directed graphical model the other is a gradient based method that is guaranteed to converge and is shown to give useful result on random graph with mild frustration we 
a bayes network based classifier for distinguishing terrestrial rock from meteorite is implemented onboard the nomad robot equipped with a camera spectrometer and eddy current sensor this robot searched the ice sheet of antarctica and autonomously made the first robotic identification of a meteorite in january at the elephant moraine this paper discus rock classification from a robotic platform and describes the system onboard nomad 
experimental data have shown that synapsis are heterogeneous different synapsis respond with different sequence of amplitude of postsynaptic response to the same spike train neither the role of synapt ic dynamic itself nor the role of the heterogeneity of synaptic dynamic s for computation in neural circuit is well understood we present in this article method that make it feasible to compute for a given synapse with known synaptic parameter the spike train that is optimally fitted to the synapse for example in the sense that it produce the largest sum of postsynaptic response to our surprise we find that most of these optim ally fitted spike train match common firing pattern of specific type of neuron that are discussed in the literature 
the web is a valuable source of language specific resource but collecting organizing and utilizing this information is difficult we describe corpusbuilder an approach for automatically generating web search query to collect document in a minority language it differs from pseudo relevance feedback in that retrieved document are labeled by an automatic language classifier a relevant or irrelevant and a subset of document is used to generate new query we experiment with various query generation method and query length to find inclusion exclusion term that are helpful for finding document in the target language and find that using odds ratio score calculated over the document acquired so far wa one of the most consistently accurate query generation method we also describe experiment using a handful of word elicited from a user instead of initial document and show that the method perform similarly applying the same approach to multiple language show that our system generalizes to a variety of language 
market based mechanism such a auction are being studied a an appropriate mean for resource allocation in distributed and inultiagenl decision problem when agent value resource in combination rather than in isolation one generally relies on combinatorial auction where agent bid tor resource bundle or simultaneous auction for all resource we develop a different model where agent bid for required reources sequentially this model ha the advantage that it can be applied in setting where combinatorial and simultaneous model are infeasible e g when resource are made available at different point in time by different party a well a certain benefit in setting where combinatorial model are applicable we develop a dynamic programming model tor agent to compute bidding policy based on estimated distribution over price we also describe how these distribution are updated to provide a learning model for bidding behavior 
we examine the benefit of using multiple agent to produce explanation in particular we identify the ability to construct prior plan a a key issue constraining the effectiveness of a single agent approach we describe an implemented system that us multiple agent to tackle a problem for which prior planning is particularly impractical real time soccer commentary our commentary system demonstrates a number of the advantage of decomposing an explanation task among several agent most notably it show how individual agent can benefit from following different discourse strategy further it illustrates that discourse issue such a controlling interruption abbreviation and maintaining consistency can also be decomposed rather than considering them at the single level of one linear explanation they can also be tackled separately within each individual agent we evaluate our system s output and show that it closely compare to the speaking pattern of a human commentary team 
a tight integration of mitchell s version space algorithm with agrawal et al s apriori algorithm is presented the algorithm can be used to generate pattern that satisfy a variety of constraint on data constraint that can be imposed on pattern include the generality relation among pattern and imposing a minimum or a maximum frequency on data set of interest the theoretical framework is applied to an important application in chemo informatics i e that of finding fragment of interest within a given set of compound fragment are linearly connected substructure of compound an implementation a well a preliminary experiment within the application are presented 
it is known that recovering projection matrix from planar configuration is ambiguous thus posing the problem of model selection is the scene planar d or non planar d for a d scene one would recover a homography matrix whereas for a d scene one would recover the fundamental matrix or trifocal tensor the task of model selection is especially problematic when the scene is neither d nor d for example a thin volume in space in this paper we show that for certain task such a reprojection there is no need to select a model the ambiguity that arises from a d scene is orthogonal to the reprojection process thus if one desire to use multilinear matching constraint for transferring point along a sequence of view it is possible to do so under any situation of d d or thin volume 
the adaptive tap gibbs free energy for a general densely connected probabilistic model with quadratic interaction and arbritary single site constraint is derived we show how a specific sequential minimization of the free energy lead to a generalization of minka s expectation propagation lastly we derive a sparse representation version of the sequential algorithm the usefulness of the approach is demonstrated on classification and density estimation with gaussian process and on an independent component analysis problem 
we present a novel implementation of the recently introduced information bottleneck method for unsupervised document clustering given a joint empirical distribution of word and document p x y we first cluster the word y so that the obtained word cluster ytilde maximally preserve the information on the document the resulting joint distribution p x ytilde contains most of the original information about the document i x ytilde ap i x y but it is much le sparse and noisy using the same procedure we then cluster the document x so that the information about the word cluster is preserved thus we first find word cluster that capture most of the mutual information about to set of document and then find document cluster that preserve the information about the word cluster we tested this procedure over several document collection based on subset taken from the standard newsgroups corpus the result were assessed by calculating the correlation between the document cluster and the correct label for these document finding from our experiment show that this double clustering procedure which us the information bottleneck method yield significantly superior performance compared to other common document distributional clustering algorithm moreover the double clustering procedure improves all the distributional clustering method examined here 
the gibbs classifier is a simple approximationto the bayesian optimal classifier inwhich one sample from the posterior for theparameter and then classifies using thesingle classifier indexed by that parametervector in this paper we study the votinggibbs classifier which is the extension of thisscheme to the full monte carlo setting inwhich n sample are drawn from the posteriorand new input are classified by votingthe n resulting classifier we show that theerror 
for building implementable and industryvaluable classification solution machine learning method must focus not only on accuracy but also o n computational and space complexity we discus a multistage method namely cascading where there is a sequence of classifier ordered in term of increasing complexity and specificity such that early classifier are simple and g eneral whereas later one are more c omplex and specific being localized on pattern rejected by the previous classifier we present t he technique and it rationale and validate it use by comparing it with the individual classifier a well a the widely accepted ensemble method bagging and adaboost on eight data set from t he uci repository we do see that cascading increase accuracy without t he c oncomitant i ncrease in complexity and cost 
when a transparent surface is present between an observer and an object an image reflected by the surface may be superimposed on the image of the observed object we present a new approach to recover the scene layer and to classify which is the reflected transmitted one based on imaging through a polarizing filter at two orientation estimate of the separate layer are obtained by weighted pixel wise difference of these image inverting the image formation process however the weight depend on the angle of incidence hence on the inclination of the transparent invisible surface this angle is estimated by seeking the angle value which through the weight lead to decorrelation of the estimated layer experimental result obtained using real photo of actual object demonstrate the success of angle estimation and consequent layer separation and labeling the method is shown to be superior to earlier method where only raw optical data wa used 
we study resource limited online learning motivated by the problem of conditional branch outcome prediction in computer architecture in particular we consider parallel time and space efficient ensemble learner for online setting empirically demonstrating benefit similar to those shown previously for offline ensemble our learning algorithm are inspired by the previously published boosting by filtering framework a well a the offline arc x boosting style algorithm we train ensemble of online decision tree using a novel variant of the id online decision tree algorithm a the base learner and show empirical result for both boosting and bagging style online ensemble method our result evaluate these method on both our branch prediction domain and online variant of three familiar machine learning benchmark our data justifies three key claim first we show empirically that our extension to id significantly improve performance for single tree and additionally are critical to achieving performance gain in tree ensemble second our result indicate significant improvement in predictive accuracy with ensemble size for the boosting style algorithm the bagging algorithm we tried showed poor performance relative to the boosting style algorithm but still improve upon individual base learner third we show that ensemble of small tree are often able to outperform large single tree with the same number of node and similarly outperform smaller ensemble of larger tree that use the same total number of node this make online boosting particularly useful in domain such a branch prediction with tight space restriction i e the available realestate on a microprocessor chip 
estimating the parameter of sparse multinomial distribution isan important component of many statistical learning task recentapproaches have used uncertainty over the vocabulary of symbolsin a multinomial distribution a a mean of accounting for sparsity 
in this paper we present our recent work on the development of a scalable personalized web based multi document summarization and recommendation system webinessence webinessence is designed to help end user effectively search for useful information and automatically summarize selected document based on the user personal profile we address some of the design issue to improve the scalability and readability of our multi document summarizer included in webinessence some evaluation result with different configuration are also presented 
locally linear embedding lle is an elegant nonlineardimensionality reduction technique recently introduced by roweisand saul it fails when the data is divided into separate group 
the representational issue of preference in the framework of a possibilistic qualitative ordinal decision model under uncertainty were originally introduced few year ago by dubois and prade and more recently linked to case based decision problem by dubois et al in this approach the uncertainty is assumed to be of possibilistic nature uncertainty or similarity and preference on consequence are both measured on commensurate ordinal scale however in case based decision problem similarity or preference on consequence may sometimes take value that are incomparable in order to cope with some of these situation we propose an extension of the model where both preference and uncertainty arc graded on distributive lattice providing axiomatic setting for characterising a pessimistic and an optimistic qualitative utility finally we extend our proposal to also include belief state that may be partially inconsistent supplying element for a qualitative case based decision methodology 
this paper introduces grounded model and compare them to axiomatic model of mathematics grounded model differ from axiomatic theory in establishing explicit connection between language and reality that are learnt through language game they are constructed and updated by autonomous agent connected to their environment through sensor and actuator using some conceptualization mechanism and language game described in steel they are based on conceptualization and support a form of intuitive reasoning which can be done sometimes by constraint satisfaction and it is argued to be the basis of some axiomatizations this is illustrated with a simple example of spatial reasoning 
interactive visualization are effective tool in mining scientific engineering and business data to support decision making activity star coordinate is proposed a a new multi dimensional visualization technique which support various interaction to stimulate visual thinking in early stage of knowledge discovery process in star coordinate coordinate ax are arranged on a two dimensional surface where each axis share the same origin point each multi dimensional data element is represented by a point where each attribute of the data contributes to it location through uniform encoding interaction feature of star coordinate provide user the ability to apply various transformation dynamically integrate and separate dimension analyze correlation of multiple dimension view cluster trend and outlier in the distribution of data and query point based on data range our experience with star coordinate show that it is particularly useful for the discovery of hierarchical cluster and analysis of multiple factor providing insight in various real datasets including telecommunication churn 
a new algorithm for solving the three dimensional container packing problem is proposed in this paper this new algorithm deviate from the traditional approach of wall building and layering it us the concept of building growing from multiple side of the container we tested our method using all test case from the or library experimental result indicate that the new algorithm is able to achieve an average packing utilization of more than this is better than the result reported in the literature 
our english chinese cross language ir system is trained from parallel corpus we investigate it performance a a function of training corpus size for three different training corpus we find that the performance of the system a trained on the three parallel corpus can be related by a simple measure namely the out of vocabulary rate of query word 
the strong correlation between the frequency of word and their naming latency ha been well documented however a early a the age of acquisition aoa of a word wa alleged to be the actual variable of interest but these study seem to have been ignored in most of the literature recently there ha been a resurgence of interest in aoa while some study have shown that frequency ha no effect when aoa is controlled for more recent study have found independent contribution of frequency and aoa connectionist model have repeatedly shown strong effect of frequency but little attention ha been paid to whether they can also show aoa effect indeed several researcher have explicitly claimed that they cannot show aoa effect in this work we explore these claim using a simple feed forward neural network we find a significant contribution of aoa to naming latency a well a condition under which frequency provides an independent contribution 
we describe and demonstrate a texture region descriptor which is invariant to affine geometric and photometric transformation and insensitive to the shape of the textur e region it is applicable to texture patch which are locall y planar and have stationary statistic the novelty of the de scriptor is that it is based on statistic aggregated over th e region resulting in richer and more stable descriptor tha n those computed at a point two texture matching application of this descriptor are demonstrated it is used to automatically identify region of the same type of texture but with varying surface pose within a single image it is used to support wide baseline stereo i e to enable the automatic computation o f the epipolar geometry between two image acquired from quite separated viewpoint result are presented on several set of real image 
this paper describes a system for segmenting chinese text into word using the mbdp algorithm mbdp is a knowledge free segmentation algorithm that bootstrap it own lexicon which start out empty experiment on chinese and english corpus show that mbdp reliably outperforms the best previous algorithm when the available hand segmented training corpus is small a the size of the hand segmented training corpus grows the performance of mbdp converges toward that of the best previous algorithm the fact that mbdp can be used with a small corpus is expected to be useful not only for the rare event of adapting to a new language but also for the common event of adapting to a new genre within the same language 
we describe a video rate surveillance algorithm for determining whether people are carrying object or moving unencumbered from a stationary camera the contribution of the paper is the shape analysis algorithm that both determines whether a person is carrying an object and segment the object from the person so that it can be tracked e g during an exchange of object between two people a the object is segmented an appearance model of the object is constructed the method combine periodic motion estimation with static symmetry analysis of the silhouette of a person in each frame of the sequence experimental result demonstrate robustness and real time performance of the proposed algorithm 
in robot navigation one of the important and fundamental issue is to reconstruct position of landmark or vision sensor locating around the robot this paper proposes a method for reconstructing qualitative position of multiple vision sensor from qualitative information observed by the vision sensor i e motion direction of moving object the process iterates the following step observing motion direction of moving object from the vision sensor classifying the vision sensor into spatially classified pair acquiring three point constraint and propagating the constraint the method have been evaluated with simulation 
the rag proposal for generic specification of nlg system includes a detailed account of data representation but only an outline view of processing aspect in this paper we introduce a modular processing architecture with a concrete implementation which aim to meet the rag goal of transparency and reusability we illustrate the model with the rich system a generation system built from simple linguistically motivated module 
we describe in this paper closed form solution to the following problem in multi view geometry of n th order curve i recovery of the fundamental matrix from or more conic match in two view ii recovery of the homography matrix from a single n th order n matching curve and in turn recovery of the fundamental matrix from two matching n th order planar curve and iii d reconstruction of a planar algebraic curve from two view although some of these problem notably i and iii were introduced in the past our derivation are analytic with resulting closed form solution we have also conducted synthetic experiment on i and real image experiment on ii and iii with subpixel performance level thus demonstrating the practical use of our result 
the voting technique which combine the prediction of several classifier can improve the generalization performance significantly by increasing the fraction of training example with large margin romma the relaxed online maximum margin algorithm is a perceptron like online learning algorithm to approximate the optimal margin classifier and aggressive romma update it prediction vector whenever the output produced by the current prediction vector doe not exceed the wanted threshold 
in this paper we present an average case analysisof the naive bayesian classifier a simpleinduction algorithm that performs wellin many domain our analysis assumes amonotone m of n target concept and trainingdata that consists of independent booleanattributes the analysis supposes a knowntarget concept and distribution of instance but includes parameter for the number oftraining case the number of irrelevant relevant and necessary attribute the probabilityof 
visual search is the task of finding a target in an image against abackground of distractors unique feature of target enable themto pop out against the background while target defined by lack offeatures or conjunction of feature are more difficult to spot it isknown that the ease of target detection can change when the rolesof figure and ground are switched the mechanism underlyingthe ease of pop out and asymmetry in visual search have beenelusive this paper show that a 
stochastic local search sl technique are very effective in solving hard propositional satisfiability problem this ha lead to the popularity of the encode solve paradigm in which different problem are encoded a propositional satisfiability problem to which sl technique are applied in ai planning is the main area in which this methodology is used yet it seems plausible that sl method should perform better when applied to the original problem space whose structure they can exploit a part of our attempt to validate this thesis we experimented with lpsp a planner that applies sl technique to the space of linear plan lpsp outperforms sl applied to encoded planning problem that enforce a similar linearity assumption because of it ability to exploit the special structure of planning problem additional experiment reported in a longer version of this paper conducted on the hamiltonian circuit problem lend farther support to our thesis 
we introduce in this paper a new face coding and recognition method which employ the enhanced fld fisher linear discrimimant model efm on integrated shape vector and texture shape free image information shape encodes the feature geometry of a face while texture provides a normalized shape free image by warping the original face image to the mean shape i e the average of aligned shape the dimensionality of the shape and the texture space are first reduced using principal component analysis pca the corresponding but reduced shape find texture feature are then integrated through a normalization procedure to form augmented feature the dimensionality reduction procedure constrained by efm for enhanced generalization maintains a proper balance between the spectral energy need of pca for adequate representation and the fld discrimination requirement that the eigenvalue of the within class scatter matrix should not include small trailing value after the dimensionality reduction procedure a they appear in the denominator 
corner and curve are important image feature in many vision based application corner are usually more stable and easier to match than curve while curve contain richer information of scene structure in previously work corner are often used to recover the epipolar geometry between two view which is then used in curve matching to reduce the search space however information of the scene structure contained in this set of matched corner is ignored in this paper we present a curve matching algorithm that is guided by a set of matched corner within a probabilistic framework the role of the corner guidance is explicitly defined by a set of similarity invariant unary measurement and by a similarity function the similarity function provides stronger capability of resolving matching ambiguity than the epipolar constraint and is integrated into a relaxation scheme to reduce computational complexity and improve accuracy of curve matching experimental result clearly demonstrate the benefit of integrating corner match into the curve matching procedure 
we present a clustering algorithm for arabic word sharing the same root root based cluster can substitute dictionary in indexing for ir modifying adamson and boreham our two stage algorithm applies light stemming before calculating word pair similarity coefficient using technique sensitive to arabic morphology test show a successful treatment of infix and accurate clustering to up to for unedited arabic text sample without the use of dictionary 
this paper describes an approach to feature subset selection that take into account problem specific and learning algorithm characteristic it is developed for the naive bayesian classifier applied on text data since it combine well with the addressed learning problem we focus on domain with many feature that also have a highly unbalanced class distribution and asymmetric misclassification cost given only implicitly in the problem by asymmetric misclassification cost we mean that one of the class value is the target class value for which we want to get prediction and we prefer false positive over false negative our example problem is automatic document categorization using machine learning where we want to identify document relevant for the selected category usually only about of example belong to the selected category our experimental comparison of eleven feature scoring measure show that considering domain and algorithm characteristic significantly improves the result of classification 
chunk parsing ha focused on the recognition of partial constituent structure at the level of individual chunk little attention ha been paid to the question of how such partial analysis can be combined into larger structure for complete utterance such larger structure are not only desirable for a deeper syntactic analysis they also constitute a necessary prerequisite for assigning function argument structure the present paper offer a similarity based algorithm for assigning functional label such a subject object head complement etc to complete syntactic structure on the basis of prechunked input the evaluation of the algorithm ha concentrated on measuring the quality of functional label it wa performed on a german and an english treebank using two different annotation scheme at the level of function argument structure the result of correct functional label for german and for english validate the general approach 
deciding whether a propositional formula in conjunctive normal form is satisfiable sat is an np complete problem the problem becomes linear when the formula contains binary clause only interestingly the reduction to sat of a number of well known and important problem such a classical ai planning and automatic test pattern generation for circuit yield formula containing many binary clause in this paper we introduce and experiment with simplify a formula simplifier targeted at such problem simplify construct the implication graph corresponding to the binary clause in the formula and us this graph to deduce new unit literal the deduced literal are used to simplify the formula and update the graph and so on until stabilization finally we use the graph to construct an equivalent simpler set of binary clause experimental evaluation of this simplifier on a number of bench mark formula produced by encoding ai planning problem prove simplify to be fast and effective 
a recursive method is presented for recovering d object shape and camera motion under orthography from an extended sequence of video image this may be viewed a a natural extension of both the original and the sequential factorization method a critical aspect of these factorization approach is the estimation of the so called shape space and they may in part be characterized by the manner in which this subspace is computed if point are tracked through frame the recursive leastsquares method proposed in this paper update the shape space with complexity per frame in contrast the sequential factorization method update the shape space with complexity per frame the original factorization method is intended to be used in batch mode using point tracked across all available frame it effectively computes the shape space with complexity after frame unlike other method the recursive approach doe not require the estimation or updating of a large measurement or covariance matrix experiment with real and synthetic image sequence confirm the recursive method s low computational complexity and good performance and indicate that it is well suited to real time application 
in this paper we present hidden markov model for korean part of speech tagging which consider korean characteristic such a high agglutinativity word spacing and high lexical correlativity in order ot consider rich information in context the model adopt a le strict markov assumption in the model sparse data problem is very serious and their parameter tend to be estimated unreliably because they have a large number of parameter to overcome sparse data problem our model us a simplified version of the well known back off smoothing method to mitigate unreliable estimation problem our model assume joint independence instead of conditional independence because joint probability have the same degree of estimation reliability experimental result show that model with rich context perform even better than standard hmms and that joint independent assumption is effective in some model 
this paper investigates whether region of uniform surface topography can be extracted from intensity image using shape from shading and subsequently used for the purpose of d object recognition we draw on the constant shape index maximal patch representation of dorai and jain we commence by showing that the resulting shape index region are stable under different viewing angle based on this observation we investigate the effectiveness of various structural representation and region attribute for d object recognition we show that region curvedness and a string ordering of the region according to size provides recognition accuracy of about by polling various recognition scheme including a graph matching method we show that a recognition rate of is achievable 
abstract we present a class of approximate inference algorithm for graphical model of the qmr dt type we give convergence rate for these algorithm and for the jaakkola and jordan algorithm and verify these theoretical prediction empirically we also presen t empirical result on the difficult qmr dt network problem obtaining per formance of the new algorithm roughly comparable to the jaakkola and jordan algorithm 
incomplete data set have become almost ubiquitous in a wide variety of application domain common example can be found in climate and image data set sensor data set and medical data set the incompleteness in these data set may arise from a number of factor in some case it may simply be a reflection of certain measurement not being available at the time in others the information may be lost due to partial system failure or it may simply be a result of user being unwilling to specify attribute due to privacy concern when a significant fraction of the entry are missing in all of the attribute it becomes very difficult to perform any kind of reasonable extrapolation on the original data for such case we introduce the novel idea of conceptual reconstruction in which we create effective conceptual representation on which the data mining algorithm can be directly applied the attraction behind the idea of conceptual reconstruction is to use the correlation structure of the data in order to express it in term of concept rather the original dimension a a result the reconstruction procedure estimate only those conceptual aspect of the data which can be mined from the incomplete data set rather than force error created by extrapolation we demonstrate the effectiveness of the approach on a variety of real data set 
this paper considers projective reconstruction with a hierarchical computational structure of trifocal tensor that integrates feature tracking and geometrical validation of the feature track the algorithm wa embedded into a system aimed at completely automatic euclidean reconstruction from uncalibrated handheld amateur video sequence the algorithm wa tested a part of this system on a number of sequence grabbed directly from a low end video camera without editing the proposed approach can be considered a generalisation of a scheme of fitzgibbon and zisserman eccv the proposed scheme try to adapt itself to the motion and frame rate in the sequence by finding good triplet of view from which accurate and unique trifocal tensor can be calculated this is in contrast to the assumption that three consecutive view in the video sequence are a good choice using trifocal tensor with a wider span suppresses error accumulation and make the scheme le reliant on bundle adjustment the proposed computational structure may also be used with fundamental matrix a the basic building block 
consider the task of exploring the web in order to find page of a particular kind or on a particular topic this task arises in the construction of search engine and web knowledge base the paper argues that the creation of efficient web spider is best framed and solved by reinforcement learning a branch of machine learning that concern itself with optimal sequential decision making one strength of reinforcement learning is that it provides a formalism for measuring the utility of action that give benefit only in the future we present an algorithm for learning a value function that map hyperlink to future discounted reward using a naive bayes text classifier experiment on two real world spidering task show a three fold improvement in spidering efficiency over traditional breadth first search and up to a two fold improvement over reinforcement learning with immediate reward only 
this paper deal with the automated creation of geometricand photometric correct d model of theworld those model can be used for virtual reality tele presence digital cinematography and urban planning application the combination of range dense depth estimate and image sensing color information providesdata set which allow u to create geometrically correct photorealistic model of high quality the d model are first built from range data using avolumetric set 
we present efficient algorithm for all point pair problem or nbody like problem which are ubiquitous in statistical learning wefocus on six example including nearest neighbor classification kerneldensity estimation outlier detection and the two point correlation 
in the last year the investigation on description logic dl ha been driven by the goal of applying them in several area such a software engineering information system database information integration and intelligent access to the web the modeling requirement arising in the above area have stimulated the need for very rich language including fixpoint construct to represent recursive structure we study a dl comprising the most general form of fixpoint construct on concept all classical concept forming construct plus inverse role n ary relation qualified number restriction and inclusion assertion we establish the exptime decidability of such logic by presenting a decision procedure based on a reduction to nonemptiness of alternating automaton on infinite tree we observe that this is the first decidability result for a logic combining inverse role number restriction and general fixpoints 
background maintenance is a frequent element of video surveillance system we develop wallflower a threecomponent system for background maintenance the pixellevel component performs wiener filtering to make probabilistic prediction of the expected background the region level component fill in homogeneous region of foreground object and the frame level component detects sudden global change in the image and swap in better approximation of the background we compare our system with other background subtraction algorithm wallflower is shown to outperform previous algorithm by handling a greater set of the difficult situation that can occur finally we analyze the experimental result and propose normative principle for background maintenance 
this paper address the problem of self calibration from one unknown motion of an uncalibrated stereo rig unlike the existing method for stereo rig self calibration which have been focused on applying the autocalibration paradigm using both motion and stereo correspondence our method doe not require the recovery of stereo correspondence our method combine purely algebraic constraint with implicit geometric constraint assuming that the rotational part of the stereo geometry ha two unknown degree of freedom i e the third dof is roughly known and that the principle point of each camera is known we first show that the computation of the intrinsic and extrinsic parameter of the stereo rig can be recovered from the motion correspondence only i e the monocular fundamental matrix we then provide an initialization procedure for the proposed non linear method we provide an extensive performance study for the method in the presence of image noise in addition we study some of the aspect related to the d motion that govern the accuracy of the proposed self calibration method experiment conducted on synthetic and real data image demonstrate the effectiveness and efficiency of the proposed method 
we provide a logical definition of minimalist grammar that are stabler s formalization of chomsky s minimalist program our logical definition lead to a neat relation to categorial grammar yielding a treatment of montague semantics a parsing a deduction in a resource sensitive logic and a learning algorithm from structured data based on a typing algorithm and type unification here we emphasize the connection to montague semantics which can be viewed a a formal computation of the logical form 
efficient learning of dfa is a challengingresearch problem in grammatical inference both exact and approximate in the pacsense identifiability of dfa from examplesis known to be hard pitt in his seminalpaper posed the following open researchproblem quot are dfa pac identifiable if examplesare drawn from the uniform distribution or some other known simple distribution quot pitt we demonstrate that theclass of simple dfa i e dfa whose canonicalrepresentations have 
the process through which reader evoke mental representation of phonological form from print constitute a hotly debated and controversial issue in current psycholinguistics in this paper we present a computational analysis of the grapho phonological system of written french and an empirical validation of some of the obtained descriptive statistic the result provide direct evidence demonstrating that both grapheme frequency and grapheme entropy influence performance on pseudoword naming we discus the implication of those finding for current model of phonological coding in visual word recognition 
image segmentation is not only hard and unnecessary for texture based image retrieval but can even be harmful image of either individual or multiple texture are best described by distribution of spatial frequency descriptor rather than single descriptor vector over pre segmented region a retrieval method based on the earth mover distance with an appropriate ground distance is shown to handle both complete and partial multi textured query a an illustration different image of the same type of animal are easily retrieved together at the same time animal with subtly different coat like cheetah and leopard are properly distinguished 
transformation based learning ha been successfully employed to solve many natural language processing problem it achieves state of the art performance on many natural language processing task and doe not overtrain easily however it doe have a serious drawback the training time is often intorelably long especially on the large corpus which are often used in nlp in this paper we present a novel and realistic method for speeding up the training time of a transformation based learner without sacrificing performance the paper compare and contrast the training time needed and performance achieved by our modified learner with two other system a standard transformation based learner and the ica system hepple the result of these experiment show that our system is able to achieve a significant improvement in training time while still achieving the same performance a a standard transformation based learner this is a valuable contribution to system and algorithm which utilize transformation based learning at any part of the execution 
one of the more controversial recent planning algorithm is the shop algorithm an htn planning algorithm that plan for task in the same order that they are to be executed shop can use domaindependent knowledge to generate plan very quickly but it can be difficult to write good knowledge base for shop our hypothesis is that this difficulty is because shop s total ordering requirement for the subtasks of it method is more restrictive than it need to be to examine this hypothesis we have developed a new htn planning algorithm called shop like shop shop is sound and complete and it construct plan in the same order that they will later be executed but unlike shop shop allows the subtasks of each method to be partially ordered our experimental result suggest that in some problem domain the difficulty of writing shop knowledge base derives from shop s total ordering requirement and that in such case shop can plan a efficiently a shop using knowledge base simpler than those needed by shop 
extracting rule from rbfs is not a trivial task because of nonlinear function or high input dimensionality in such case some of the hidden unit of the rbf network have a tendency to be shared across several output class or even may not contribute 
a an application of nlp to computer assisted language learning call we propose a diagnostic processing of japanese being able to detect error and inappropriateness of sentence composed by the student in the given situation and the context of the exercise text using ltag lexicalized tree adjoining grammar formalism we have implemented a prototype of such a diagnostic parser a a component of a call system being developed 
we introduce an information system for organization and retrieval of news article from web publication incorporating a classification framework based on support vector machine we present the data model for storage and management of news data and the system architecture for news retrieval classification and generation of topical collection we also discus the classification result obtained with a collection of news article gathered from a set of online newspaper 
this paper describes a generalized formulation of optical flow estimation based on modelsof brightness variation that are caused by time dependent physical process these include changing surface orientation withrespect to a directional illuminant motion of the illuminant and physical model of heat transport in infrared image with thesemodels we simultaneously estimate the d image motion and the relevant physical parameter of the brightness change model theestimation problem 
this study compare five well known association rule algorithm using three real world datasets and an artificial dataset the experimental result confirm the performance improvement previously claimed by the author on the artificial data but some of these gain do not carry over to the real datasets indicating overfitting of the algorithm to the ibm artificial dataset more importantly we found that the choice of algorithm only matter at support level that generate more rule than would be useful in practice for support level that generate le than rule which is much more than human can handle and is sufficient for prediction purpose where data is loaded into ram apriori finish processing in le than minute on our datasets we observed super exponential growth in the number of rule on one of our datasets a change in the support increased the number of rule from le than a million to over a billion implying that outside a very narrow range of support value the choice of algorithm is irrelevant 
method for the analysis of image of the same scene taken under three different lighting condition are illustrated a technique that separate the effect of geometry and surface coloration texture in this tri luminal environment is developed and experimental result are shown exploiting this technique to isolate geometric information two method which extract differential geometric property of surface the sign of gaussian curvature and it magnitude to within a multiplicative factor directly from tri luminal photometric data are derived and demonstrated 
clustering is traditionally viewed a an unsupervisedmethod for data analysis however in some case information about theproblem domain is available in addition tothe data instance themselves in this paper we demonstrate how the popular k meansclustering algorithm can be protably modi ed to make use of this information in experimentswith articial constraint on sixdata set we observe improvement in clusteringaccuracy we also apply this methodto the real world 
method are presented for increasing the coverage and accuracy of image mosaic constructed from multiple uncalibrated weak perspective view of the human retina ex tending our previous algorithm for registering pair of image using a non invertible parameter quadratic image transformation model and a hierarchical robust estimatio n technique two important innovation are presented th e first is a linear non iterative method for jointly estimating the transformation of all image onto the mosaic this employ constraint derived from pairwise matching between the non mosaic image frame it allows the transformation to be estimated for image that do not overlap the mosaic anchor frame and result in mutually consistent transformation for all image this mean the mosaic can cover a much broader area of the retinal surface even though the transformation model is not closed under composition this capability is particularly valuable for mosaicing the reti nal periphery in the context of disease such a aid cmv the second innovation is a method to improve the accuracy of the pairwise match a well a the joint estimation by refining the feature location and by adding new feature based on the transformation estimate themselves fo r matching image frame of size this cut the registration error from the range of to pixel to about pixel the overall transformation error in final mosaic construction is pixel based on experiment over a large set of eye 
abstract bagging form a committee of classifier by bootstrap aggregationof training set from a pool of training data asimple alternative to bagging is to partition the data intodisjoint subset experiment on various datasets show that given the same size partition and bag the use of disjointpartitions result in better performance than the useof bag many application e g protein structure prediction involve the use of datasets that are too large to handlein the memory of the 
we present a set of algorithm that enable u to translate natural language sentence by exploiting both a translation memory and a statistical based translation model our result show that an automatically derived translation memory can be used within a statistical framework to often find translation of higher probability than those found using solely a statistical model the translation produced using both the translation memory and the statistical model are significantly better than translation produced by two commercial system our hybrid system translated perfectly of the sentence in a test collection while the commercial system translated perfectly only of them 
wordnet is a rich source of world knowledge from which formal axiom can be derived in this paper we present a method for transforming the wordnet gloss into logic form and further into axiom the transformation of wordnet gloss into logic form is useful for theorem proving and other application the paper demonstrates the utility of the wordnet axiom in a question answering system to rank and extract answer 
we introduce a framework for recovering the d shape and motion of unknown arbitrarily moving curve from two or more image sequence acquired simultaneously from distinct point in space we use this framework to identify ambiguity in the multi view recovery of rigid or nonrigid d motion for arbitrary curve and identify a novel spatio temporal constraint that couple the problem of d shape and d motion recovery in the multi view case we show that this constraint lead to a simple hypothesizeand test algorithm for estimating d curve shape and motion simultaneously experiment performed with synthetic data suggest that in addition to recovering d curve motion our approach yield shape estimate of higher accuracy than those obtained when stereo analysis alone is applied to a multi view sequence 
camera calibration is a primary crucial step inmany computer vision task in this paper we presenta new neural approach for cameracalibration unlikesome existing neural approach our calibrating networkcan tell the perspective projection transformationmatrix between the world d point and the corresponding d image pixel starting from random initialweights the net can specify the cameramodel parameterssatisfying the orthogonality constraint onthe rotational transformation the 
value function approach for markov decision process have been used successfully to find optimal policy for a large number of problem recent findi ng have demonstrated that policy search can be used effectively in reinforcement learning wh en standard value function technique become overwhelmed by the size and dimensionality of the state space we demonstrate that substantial benefit can be achieved by combining the tw o approach we use an approximate value function solution in a low dimensional space to seed policy search in a continuous high dimensional space we demonstrate our approach a a motion planner on a mobile robot we show that this combination can in practice find good policy more efficiently t han policy search alone and is capable of solving more complex problem than value function alone 
we demonstrate the benefit of a multilingual approach to automatic lexical semantic verb classification based on statistical analysis of corpus in multiple language our research incorporates two interrelated thread in one we exploit the across language in the syntactic expression of semantic property to show that complementary information about english verb can be extracted from their translation in a second language chinese the use of multilingual feature improves classification performance of the english verb achieving an accuracy of baseline 
data mining started it move out of the statistic and machine learning ghetto and into the mainstream almost year ago with great fanfare and a large influx of venture capital data mining wa going to change the very nature of business yet data mining product have had relatively modest success in the marketplace the reason include limitation and misplaced emphasis in the product feature and function unrealistic expectation set by message from the data mining community and a lack of readiness by many prospective user this session will look at where vendor have succeeded and failed with their product what expectation user should have and suggestion for achieving the potential of this exciting and valuable technology 
a challenging unsolved problem in the speech recognition community is recognizing speech signal that are corrupted by loud highly nonstationary noise one approach to noisy speech recognition is to automatically remove the noise from the cepstrum sequence before feeding it in to a clean speech recognizer in previous work published in eurospeech we showed how a probability model trained on clean speech and a separate probability model trained on noise could be combined for the purpose of 
abstract language modeling approach to information retrieval are attractive and promising because they connect the problem of retrieval with that of language model estimation which ha been studied extensively in other application area such a speech recognition the basic idea of these approach is to estimate a language model for each document and then rank document by the likelihood of the query according to the estimated language model a core problem in language model estimation is smoothing which adjusts the maximum likelihood estimator so a to correct the inaccuracy due to data sparseness in this paper we study the problem of language model smoothing and it influence on retrieval performance we examine the sensitivity of retrieval performance to the smoothing parameter and compare several popular smoothing method on different test collection 
the light reflected from a surface depends on the scene geometry the incident illumination and the surface material a novel methodology is presented which extract reflectivity information of the various material in the scene independent of incident light and scene geometry a scene is captured under different narrow band color filter and the spectral derivative of the scene are computed the resulting spectral derivative form a spectral gradient at each pixel this spectral gradient is a material descriptor which is invariant to scene geometry and incident illumination for smooth diffuse surface spectral gradient can discriminate among smooth dielectric with different reflectance property independent of viewing condition 
this paper address the problem of the local scale parameter selection for recognition technique based on gaussian derivative pattern are described in a feature space of which each dimension is a scale and orientation normalized receptive field a unit composed of normalized gaussian based filter scale invariance is obtained by automatic selection of an appropriate local scale lin b and followed by normalisation of the receptive field to the appropriate scale orientation invariance is obtained by the determination of the dominant local orientation and by steering the receptive field to this orientation data is represented structurally in a feature space that is designed for the recognition of static object configuration in this space an image is modeled by the vectorial representation of the receptive field response at each pixel forming a surface in the feature space recognition is achieved by measuring the distance between the vector of normalized receptive field response of an observed neighborhood and the surface point of the image model the power of a scale equivariant feature space is validated by experimental result for point correspondence in image of different scale and the recognition of object under different view point 
the number of feature that can be computed over an image is for practical purpose limitless unfortunately the number of feature that can be computed and exploited by most computer vision system is considerably le a a result it is important to develop technique for selecting feature from very large data set that include many irrelevant or redundant feature this work address the feature selection problem by proposing a three step algorithm the first step us a variation of the well known relief algorithm to remove irrelevance the second step cluster feature using k mean to remove redundancy and the third step is a standard combinatorial feature selection algorithm this three step combination is shown to be more effective than standard feature selection algorithm for large data set with lot of irrelevant and redundant feature it is also shown to be no worse than standard technique for data set that do not have these property finally we show a third experiment in which a data set with feature is reduced to of it original size with very little information loss 
the temporal difference td learning algorithm offer the hope that the arduous task of manually tuning the evaluation function weight of game playing program can be automated with one exception td gammon td learning ha not been demonstrated to be effictive in a high performance world class game palying program further there ha been doubt expressed by game program developer that learned weight could compete with the best hand tuned weight chinook is the world man machine tuned over year this paper show that td learinng is capable of competing with the best human effort 
we study the complexity of model checking in proposit ional nonmonotonic logic specifically we first define the problem of model checking in such formalism based on the fact that several nonmonotonic logic make use of interpretation structure i e default extension stable expansion universal kripke model which are more complex than standard interpretation of propositional logic then we analyze the complexity of checking whether a given interpretation structure satisfies a nonmonotonic theory in particular we characterize the complexity of model checking for reiter s default logie and it restriction moore s autoepistemie logic and several nonmonotonic modal logic the result obtained show that in all such formalism model checking is computationally easier than logical inference 
the problem of selecting a camera model is addressed here using the geometric aic akaike information criterion proposed by kanatani which considers both the residual of the data fitting to the model a well a the complexity of the model camera model describe the geometrical relation between the d location of object point and the image location of their projection the most commonly used camera model are the projective perspective camera model and the affine camera model intuitively the projective camera model which is nonlinear and is characterized by more parameter model the imaging geometry better but also is believed to lead to numerically le stable solution the affine camera model which is an approximation to the projective camera model with le parameter is recommended to be used when the object depth is much smaller than the object distance however there is no quantitative criterion for the decision which camera model should be used projective or affine in this paper the geometric aic criterion is used for deciding between the two camera model is the context of two task estimating the projection matrix from d and corresponding d data and estimating the fundamental matrix from two set of d data it is found that in most case it is the projective camera model which is more appropriate still in the case where the affine camera model is traditionally used the measure of appropriateness of the two model are roughly the same with a small advantage to the affine camera model 
decision making is particularly important for emergency manager a they often need to make quick and high quality decision under stress based on scratch and inadequate information and to follow expert knowledge or past experience the potential release of radioactive material from the guangdong nuclear power station gnp at daya bay though is highly unlikely could perhaps be the most dreaded disaster which would cause drastic damage to life and property the government of the hong kong special administrative region government hksar ha therefore in completed the daya bay contingency plan dbcp to prepare for such disaster to supplement the expert in assisting disaster manager with a useful tool to make better quality decision based on well structured accurate sufficient expert knowledge a prototype expert system ha been developed to cover two major area of the plan namely a determination of activation level of the dbcp and provision of an action checklist and b recommendation on counter measure 
the success of evolutionary method on standard control learning task ha created a need for new benchmark the classic pole balancing problem is no longer difficult enough to serve a a viable yard stick for measuring the learning efficiency of these system the double pole case where two pole connected to the cart must be balanced simultaneously is much more difficult especially when ve locity information is not available in this article we demonstrate a neuroevolution system enforced sub population esp that is used to evolve a con troller for the standard double pole task and a much harder non markovian version in both case our result show that esp is faster than other neuroevolution method in addition we introduce an in cremental method that evolves on a sequence of task and utilizes a local search technique deltacoding to sustain diversity this method enables the system to solve even more difficult version of the task where direct evolution cannot 
this paper present a new method for detecting scale invariant interest point the method is based on two recent result on scale space interest point can be adapted to scale and give repeatable result geometrically stable local extremum over scale of normalized derivative indicate the presence of characteristic local structure ou r method first computes a multi scale representation for the harris interest point detector we then select point at whi ch a local measure the laplacian is maximal over scale this allows a selection of distinctive point for which the characteristic scale is known these point are invariant t o scale rotation and translation a well a robust to illumin ation change and limited change of viewpoint for indexing the image is characterized by a set of scale invariant point the scale associated with each point allows the computation of a scale invariant descriptor our descriptor are in addition invariant to image rotation to affine illumination change and robust to small perspective deformation experimental result for indexing show an excellent performance up to a scale factor of for a database with more than image 
abstract global localization is the problem of determining the position of a robot under global uncertainty this problem can be divided in two phase from the sensor data or sensor view determine the set of location where the robot can be and devise a strategy by which the robot can correctly eliminate all but the right location the approach proposed in this paper is based on markov localization it applies the principal component method to get rotation invariant feature for each location of the map a bayesian classification system to cluster the feature and polar correlation between the sensor view and the local map view to determine the location where the robot can be in order to solve efficiently the localization problem a well a to consider the perceptual limitation of the sensor the possible location of the robot are restricted to be in a roadmap that keep the robot close to obstacle and correlation between the possible local map view are pre computed the hypothesis are clustered and a greedy search determines the robot movement to reduce the number of cluster of hypothesis this approach is tested using a simulated and a real mobile robot with promising result 
we propose a data mining model that capture the user navigation behaviour pattern the user navigation session are modelled a a hypertext probabilistic grammar whose higher probability string correspond to the user s preferred trail an algorithm to efficiently mine such trail is given we make use of the n gram model which assumes that the last n page browsed affect the probability of the next page to be visited the model is based on the theory of probabilistic grammar providing it with a sound theoretical foundation for future enhancement moreover we propose the use of entropy a an estimator of the grammar s statistical property extensive experiment were conducted and the result show that the algorithm run in linear time the grammar s entropy is a good estimator of the number of mined trail and the real data rule confirm the effectiveness of the model 
we show that a simple memory based technique for view based face recognition motivated by the real world task of visitor identification can outperform more sophisticated algorithm that use principal component analysis pca and neural network this technique is closely related to correlation template however we show that the use of novel similarity measure greatly improves performance we also show that augmenting the memory base with additional synthetic face image result in further improvement in performance result of extensive empirical testing on two standard face recognition datasets are presented and direct comparison with published work show that our algorithm achieves comparable or superior result this paper further demonstrates that our algorithm ha desirable asymptotic computational and storage behavior and is ideal for incremental training our system is incorporated into an automated visitor identification system that ha been operating successfully in an outdoor environment for several month 
abstract representation of negotiation in electronic market and their support are important issue in today s e commerce research whereas most activity are focused on automation aspect only few effort address the design of electronic negotiation e g the sequence of action or obligation and responsibility of the negotiating party however an explicit negotiation design can also address what is commonly referred to a the ontology problem of electronic negotiation how can one ensure that the negotiating party have the same understanding regarding the issue that are subject to the negotiation the solution this paper proposes is to perform a communication design for electronic negotiation that explicitly specifies the common syntax and semantics of the negotiating party the logical space of the electronic negotiation furthermore xml schema is suggested a the mechanism for the runtime representation of the logical space and the validation of actual negotiation from a syntactical and semantical perspective on the basis of this approach organisation creating an electronic market or seller who intend to offer their buyer the ability to bargain can design and generate support mechanism for electronic negotiation in a flexible and efficient way the communication design actionand 
this paper present a new approach for achieving distortion invariant recognition and classification a test example to be classified is viewed a a query intended to find similar example in the training set or class model derived from the training set the key idea is that instead of querying with a single pattern we construct a more robust query based on the family of pattern formed by distorting the test example although query execution is slower than if the invariance were successfully pre compiled during training there are significant advantage in several context i providing invariance in memory based learning ii in model selection where reducing training time at the expense of test time is a desirable trade off and iii in enabling robust ad hoc search based on a single example preliminary test for memory based learning on the nist handwritten digit database with a limited set of shearing and translation distortion produced an error rate of 
abstract 
the problem of reinforcement learning in large factored markov decision process is explored the q value of a state action pair is approximated by the free energy of a product of expert network network parameter are learned on line using a modified sarsa algorithm which minimizes the inconsistency of the q value of consecutive state action pair action are chosen based on the current value estimate by fixing the current state and sampling action from the network using gibbs sampling the algorithm is tested on a co operative multi agent task the product of expert model is found to perform comparably to table based q learning for small instance of the task and continues to perform well when the problem becomes too large for a table based representation 
the design of an effective architecture for content based retrieval from visual library requires careful consideration of the interplay between feature selection feature representation and similarity metric we present a solution where all the module strive to optimize the same performance criterion the probability of retrieval error this solution consists of a bayesian retrieval criterion shown to generalize the most prevalent similarity metric in current use and an embedded mixture representation over a multiresolution feature space shown to provide a good trade off between retrieval accuracy invariance perceptual relevance of similarity judgment and complexity the new representation extends standard model histogram and gaussian by providing simultaneous support for high dimensional feature and multi modal density and performs well on color texture and generic image database 
we present a probabilistic latent variable framework for data visualisation a key feature of which is it applicability to binary andcategorical data type for which few established method exist avariational approximation to the likelihood is exploited to derive afast algorithm for determining the model parameter illustrationsof application to real and synthetic binary data set are given introductionvisualisation is a powerful tool in the exploratory analysis of multivariate 
we give necessary and sufficient condition for uniqueness of thesupport vector solution for the problem of pattern recognition andregression estimation for a general class of cost function we showthat if the solution is not unique all support vector are necessarilyat bound and we give some simple example of non unique solution we note that uniqueness of the primal dual solution doesnot necessarily imply uniqueness of the dual primal solution weshow how to compute 
in many vision application the practice of supervised learning face several difficulty one of which is that insufficient labeled training data result in poor generalization in image retrieval we have very few labeled image from query and relevance feedback so that it is hard to automatically weight image feature and select similarity metric for image classification this paper investigates the possibility of including an unlabeled data set to make up the insufficiency of labeled data different from most current research in image retrieval the proposed approach try to cast image retrieval a a transductive learning problem in which the generalization of an image classifier is only defined on a set of image such a the given image database formulating this transductive problem in a probabilistic framework the proposed algorithm discriminantem d em not only estimate the parameter of a generative model but also find a linear transformation to relax the assumption of probabilistic structure of data distribution a well a select good feature automatically our experiment show that d em ha a satisfactory performance in image retrieval application d em algorithm ha the potential to many other application 
several method for computing observer motion from monocular and stereo image sequence have been proposed however accurate positioning over long distance requires a higher level of robustness than previously achieved this paper describes several mechanism for improving robustness in the context of a maximum likelihood stereo egomotion method we demonstrate that even a robust system will accumulate super linear error in the distance traveled due to increasing orientation error however when an absolute orientation sensor is incorporated the error growth is reduced to linear in the distance traveled and grows much more slowly in practice our experiment including a trial with stereo pair indicate that these technique can achieve error below of the distance traveled this method ha been implemented to run on board a prototype mar rover 
in this paper we propose that information maximization can providea unified framework for understanding saccadic eyemovements in this framework the mutual information among the corticalrepresentations of the retinal image the prior constructedfrom our long term visual experience and a dynamic short terminternal representation constructed from recent saccade providesa map for guiding eyenavigation by directing the eye to locationsof maximum complexity in neuronal ensemble 
stochastic local search sl algorithm for prepositional satisfiability testing sat have become popular and powerful tool for solving suitably encoded hard combinatorial from different domain like e g planning consequently there is a considerable interest in finding sat encoding which facilitate the efficient application of sl algorithm in this work we study how two encoding scheme for combinatorial problem like the well known constraint satisfaction or hamilton circuit problem affect sl performance on the sat encoded instance to explain the observed performance difference we identify feature of the induces search space which affect sl performance we furthermore present initial result of a comparitive analysis of the performance of the sat encoding and solving approach versus that of native sl algorithm directly applied to the unencoded problem instance 
a model of auditory grouping is described in which auditory attention play a key role the model is based upon an oscillatory correlation framework in which neural oscillator representing a single perceptual stream are synchronised and are desynchronised from oscillator representing other stream the model suggests a mechanism by which attention can be directed to the high or low tone in a repeating sequence of tone with alternating frequency in addition it simulates the perceptual segregation of a mistuned harmonic from a complex tone 
we have developed an easy and cost effective system that construct textured d animated face model from video with minimal user interaction our system first take with an ordinary video camera image of a face of a person sitting in front of the camera turning the head from one side to the other after five manual click on two image to tell the system where the eye corner nose top and mouth corner are the system automatically generates a realistic looking d human head model and the constructed model can be animated immediately different pose facial expression and talking a user with a pc and a video camera can use our system to generate his her face model in a few minute the face model can then be imported in his her favorite game and the user see themselves and their friend take part in the game they are playing we will demonstrate the system on a laptop computer live at the conference and participant can try it to model their own face 
in nature animal encounter high dimensional sensory stimulus that have complex statistical and dynamical structure attempt to study the neural coding of these natural signal face challenge both in the selection of the signal ensemble and in the analysis of the resulting neural response for zebra finch naturalistic stimulus can be defined a sound that they encounter in a colony of conspecific bird we assembled an ensemble of these sound by recording group of zebra finch and then analyzed the response of single neuron in the songbird central auditory area field l to continuous playback of long segment from this ensemble following method developed in the fly visual system we measured the information that spike train provide about the acoustic stimulus without any assumption about which feature of the stimulus are relevant preliminary result indicate that large amount of information are carried by spike timing with roughly half of the information accessible only at time resolution better than m additional information is still being revealed a time resolution is improved to m information can be decomposed into that carried by the locking of individual spike to the stimulus or modulation of spike rate v that carried by timing in spike pattern initial result show that in field l temporal pattern give at least extra information thus single central auditory neuron can provide an informative representation of naturalistic sound in which spike timing may play a significant role 
abstract 
computational model of analogical problem solving have traditionally described source and target domain in term of their causal structure but psychological research show that visual reasoning play a part for many kind of analogy this paper describes a model that transfer a solution from a source analog to a new target problem using only visual knowledge represented symbolically the knowledge representation is based on a language of primitive visual element and transformation we found that visual knowledge is sufficient for transfer but that causal knowledge is needed to determine if the transferred solution is appropriate 
abstract a new approach to inference in belief network ha been recently proposed which is based on an algebraic representation of belief network using multi linear function according to this approach the key computational question is that of representing multi linear function compactly since inference reduces to a simple process of evaluating and dierentiating such function we show here that mainstream inference algorithm based on jointrees are a special case of this approach in a very precise sense we use this result to prove new property of jointree algorithm and then discus some of it practical and theoretical implication 
this paper develops a new approach for extremely fast detection in domain where the distribution of positive and negative example is highly skewed e g face detection or database retrieval in such domain a cascade of simple classifier each trained to achieve high detection rate and modest false positive rate can yield a final detector with many desirable feature including high detection rate very low false positive rate and fast performance achieving extremely high detection rate rather than low error is not a task typically addressed by machine learning algorithm we propose a new variant of adaboost a a mechanism for training the simple classifier used in the cascade experimental result in the domain of face detection show the training algorithm yield significant improvement in performance over conventional adaboost the final face detection system can process frame per second achieves over detection and a false positive rate of in a 
abstract a novel approach for estimating articulated body posture and motion from monocular video sequence is proposed human pose is defined a the instantaneous two dimensional configuration i e the projection onto the image plane of a single articulated body in term of the position of a predetermined set of joint first statistical segmentation of the human body from the background is performed and low level visual feature are found given the segmented body shape the goal is to be able to map these generally low level visual feature to body configuration the system estimate different mapping each one with a specific cluster in the visual feature space given a set of body motion sequence for training unsupervised clustering is obtained via the expectation maximation algorithm then for each of the cluster a function is estimated to build the mapping between low level feature to d pose currently this mapping is modeled by a neural network given new visual feature a mapping from each cluster is performed to yield a set of possible pose from this set the system selects the most likely pose given the learned probability distribution and the visual feature similarity between hypothesis and input performance of the proposed approach is characterized using a new set of known body posture showing promising result 
this paper describes a simple pattern matching algorithm for recovering empty node and identifying their co indexed antecedent in phrase structure tree that do not contain this information the pattern are minimal connected tree fragment containing an empty node and all other node co indexed with it this paper also proposes an evaluation procedure for empty node recovery procedure which is independent of most of the detail of phrase structure which make it possible to compare the performance of empty node recovery on parser output with the empty node annotation in a gold standard corpus evaluating the algorithm on the output of charniak s parser charniak and the penn treebank marcus et al show that the pattern matching algorithm doe surprisingly well on the most frequently occuring type of empty node given it simplicity 
the paper introduces a query translation model that reflects the structure of the cross language information retrieval task the model is based on a structured bilingual dictionary in which the translation of each term are clustered into group with distinct meaning query translation is modeled a a two stage process with the system first determining the intended meaning of a query term and then selecting translation appropriate to that meaning that might appear in the document collection an implementation of structured translation based on automatic dictionary clustering is described and evaluated by using chinese query to retrieve english document structured translation achieved an average precision that wa statistically indistinguishable from pirkola s technique for very short query but pirkola s technique outperformed structured translation on long query the paper concludes with some observation on future work to improve retrieval effectiveness and on other potential us of structured translation in interactive cross language retrieval application 
we explore combining reinforcement learning with a hand crafted local controller in a manner suggested by the chaotic control algorithm of vincent schmitt and vincent a closedloop controller is designed using conventional mean that creates a domain of attraction about a target state chaotic behavior is used or induced to bring the system into this region at which time the local controller is turned on to bring the system to the target state and stabilize it there we describe experiment in which we use reinforcement learning instead of and in addition to chaotic behavior to learn an efficient policy for driving the system into the local controller s domain of attraction using a simulated double pendulum we illustrate how this method allows reinforcement learning to be effective in a problem that cannot be easily solved by reinforcement learning alone and we show how reinforcement learning can improve upon the chaotic control algorithm when the domain of attraction can only be approximately determined similar result are shown using the h non map this is a simple and effective way of extending reinforcement learning to more difficult problem 
the amount of readily available on line text ha reached hundred of billion of word and continues to grow yet for most core natural language task algorithm continue to be optimized tested and compared after training on corpus consisting of only one million word or le in this paper we evaluate the performance of different learning method on a prototypical natural language disambiguation task confusion set disambiguation when trained on order of magnitude more labeled data than ha previously been used we are fortunate that for this particular application correctly labeled training data is free since this will often not be the case we examine method for effectively exploiting very large corpus when labeled data come at a cost 
we propose an algorithm to automatically induce the morphology of inflectional language using only text corpus and no human input our algorithm combine cue from orthography semantics and syntactic distribution to induce morphological relationship in german dutch and english using celex a a gold standard for evaluation we show our algorithm to be an improvement over any knowledge free algorithm yet proposed 
imagine a professional web site designer who constantly ha to come up with innovative look for the client s homepage how doe he find the new content the major search engine such a hotbot http www hotbot com allow u to find text on the web but typically have few or no capability for finding visual medium in this article we discus method for finding visual medium on the www the emphasis is on iconic query which are essentially drag and drop visual concept or simple semantics we describe our method for finding static feature set and then describe a novel method called active feature set which chooses a feature set based on the context in the image 
the reliability and accuracy of spike train have been shown to depend on the nature of the stimulus that the neuron encodes adding ion channel stochasticity to neuronal model result with a macroscopic behavior that replicates the input dependent reliability and precision of real neuron we calculate the amount of information that an ion channel based stochastic hodgkin huxley hh neuron model can encode about a wide set of stimulus we show that both the information rate and the information per spike of the stochastic model is similar to the value reported experimentally moreover the amount of information that the neuron encodes is correlated with the amplitude of fluctuation in the input and le so with the average firing r ate of the neuron we also show that for the hh ion channel density the information capacity is robust to change in the density of ion channel in the membrane whereas changing the ratio between the and ion channel ha a considerable effect on the information that the neuron can encode this suggests that neuron may maximize their information capacity by appropriately balancing the density of the different ion channel that underlies neuronal excitability 
we propose a general bayesian framework for performing independent component analysis ica which relies on ensemble learning and linear response theory known from statistical physic we apply it to both discrete and continuous source for the continuous source the underdetermined overcomplete case is studied the naive mean fie ld approach fails in this case whereas linear response theory which giv e an improved estimate of covariance is very efficient the example giv en are for source without temporal correlation however this deri vation can easily be extended to treat temporal correlation finally th e framework offer a simple way of generating new ica algorithm without needing to define the prior distribution of the source explicitly 
principal component analysis pca ha been widely used for the representation of shape appearance and motion one drawback of typical pca method is that they are least square estimation technique and hence fail to account for outlier which are common in realistic training set in computer vision application outlier typically occur within a sample image due to pixel that are corrupted by noise alignment error or occlusion we review previous approach for making pca robust to outlier and present a new method that us anintra sample outlier process to account for pixel outlier we develop the theory of robust principal component analysis rpca and describe a robust m estimation algorithm for learning linear multivariate representation of high dimensional data such a image quantitative comparison with traditional pca and previous robust algorithm illustrate the benefit of rpca when outlier are present detail of the algorithm are described and a software implementation is being made publically available 
this paper present a novel information retrieval system that includes the addition of concept to facilitate the identification of the correct word sense a natural language query interface the inclusion of weight and penalty for proper noun that build upon the okapi weighting scheme and a term clustering technique that exploit the spatial proximity of search term in a document to further improve the performance the effectiveness of the system is validated by experimental result 
in this paper we introduce virtual snake for generatingocclusion hypothesis initially snake are clusteredbased on their motion to form object hypothesesa type of motion segmentation when two snakesintersect four virtual snake are generated a backgroundand a foreground snake for each of the originaltwo the two foreground virtual snake are allowed torelax while the two background virtual snake move inaccordance with their previous motion the combinedenergies of the snake 
this paper present a novel landmark based shape deformation method this method effectively solves two problem inherent in landmark based shape deformation a identification of landmark point from a given input image and b regularized deformation of the shape of an object defined in a template the second problem is solved using a new constrained support vector machine svm regression technique in which a thin plate kernel is utilized to provide non rigid shape deformation this method offer several advantage over existing landmark based method first it ha a unique capability to detect and use multiple candidate landmark point in an input image to improve landmark detection second it can handle the case of missing landmark which often arises in dealing with occluded image we have applied the proposed method to extract the scalp contour from brain cryosection image with very encouraging result 
human understanding of spoken language appears to integrate the use of contextual expectation with acoustic level perception in a tightly coupled sequential fashion yet computer speech understanding system typically pas the transcript produced by a speech recognizer into a natural language parser with no integration of acoustic and grammatical constraint one reason for this is the complexity of implementing that integration to address this issue we have created a robust semantic parser a a single finite state machine fsm a such it run time action is le complex than other robust parser that are based on either chart or generalized left right glr architecture therefore we believe it is ultimately more amenable to direct integration with a speech decoder 
now that the complete genome of numerousorganisms have been determined a key problemin computational molecular biology is uncoveringthe relationship that exist amongthe gene in each organism and the regulatorymechanisms that control their operation 
we describe the g factor which relates probability distribution on image feature to distribution on the image themselves the g factor depends only on our choice of feature and lattice quantization and is independent of the training image data we illustrate the importance of the g factor by analyzing minimax entropy learning mel which learns image distribution in term of clique potential corresponding to feature statistic we rst use our analysis of the g factor to determine when the mel clique potential decouple for dieren t feature secondly we show that mel clique potential can be computed analytically by approximating the g factor we support our analysis by computer simulation 
chinese input is one of the key challenge for chinese pc user this paper proposes a statistical approach to pinyin based chinese input this approach us a trigram based language model and a statistically based segmentation also to deal with real input it also includes a typing model which enables spelling correction in sentence based pinyin input and a spelling model for english which enables modeless pinyin input 
genetic programming gp can learn complexconcepts by searching for the target conceptthrough evolution of population of candidatehypothesis program however unlikesome learning technique such a artificialneural network anns gp doe not havea principled procedure for changing part ofa learned structure based on that structure sperformance on the training data gp ismissing a clear locally optimal update procedure an equivalent of gradient descent backpropagation 
an object recognition system ha been developed that us a new class of local image feature the feature are invariant to image scaling translation and rotation and partially invariant to illumination change and affine or d projection these feature share similar property with neuron in inferior temporal cortex that are used for object recognition in primate vision feature are efficiently detected through a staged filtering approach that identifies stable point in scale space image key are created that allow for local geometric deformation by representing blurred image gradient in multiple orientation plane and at multiple scale the key are used a input to a nearest neighbor indexing method that identifies candidate object match final verification of each match is achieved by finding a low residual least square solution for the unknown model parameter experimental result show that robust object recognition can be achieved in cluttered partially occluded image with a computation time of under second 
statistical approach to text mining can be enhanced and improved through the qualitative representation of free text ideally a representation which accommodates ambiguity and imprecision we introduce a specialized lexicon that assigns semantic category to word together with numeric value for centrality and intensity within each category from this lexicon we automatically generate an additional set of resource to implement some of the common operation of text mining profiling querying and query profile expansion and compression in qualitative domain we exploit the hierarchical structure of free text i e sentence paragraph document and develop a set of operator whose argument are fuzzy representation profile of text at any hierarchical level various operator compute the centrality and intensity of category within a profile a profile s overall intensity and the cardinality and fuzziness of a profile others are used in profile merging profile expansion or compression and discovery of related category from a profile we address the meaning and mode of deployment of these operator using practical example finally we discus the utility of fuzzy typing for various task such a qualitative browsing and similarity estimate we discus how the existing approach can be enhanced using automatic lexicon expansion and information extraction technique we offer a practical software demonstration with several visualization example illustrating the power of the proposed operator in affect analysis of news report and movie review 
we present monte carlo generalized em equation for learning in non linear state space model the dif culties lie in the monte carlo e step which consists of sampling from the posterior distribution of the hidden variable given the observation the new idea presented in this paper is to generate sample from a gaussian approximation to the true posterior from which it is easy to obtain independent sample the parameter of the gaussian approximation are either derived from the extended kalman lter or the fisher scoring algorithm in case the posterior density is mul timodal we propose to approximate the posterior by a sum of gaussians mixture of mode approach we show that sampling from the approxi mate posterior density obtained by the above algorithm lead to better model than using point estimate for the hidden state in our exper iment the fisher scoring algorithm obtained a better approximation of the posterior mode than the ekf for a multimodal distribution the mix ture of mode approach gave superior result 
domain specific search engine are becoming increasingly popular because they offer increased accuracy and extra feature not possible with general web wide search engine unfortunately they are also difficult and time consuming to maintain this 
in this paper we describe our research in computer aided image analysis we have incorporated machine learning methodology with traditional image processing to perform unsupervised image segmentation first we apply image processing technique to extract from an image a set of training case which are histogram peak described by their intensity range and spatial and textural attribute second we use learning by discovery methodology to cluster these case the first methodology we use is based on cobweb a conceptual clustering approach whose objective is to cluster the case incrementally a the concept hierarchy is refined the second methodology is based on an aggregated population equalization ape strategy this approach attempt to maintain similar strength for all population in it environment the clustering result of either approach tell u the number of visually significant class in the image and what these class are and thus enables u to perform unsupervised segmentation i e the labeling of all image pixel based on the result of the visual evaluation of the segmented image we have built an unsupervised segmentation software tool called asis and have applied it to a range of remotely sensed image such a sea ice and vegetation index in this paper we present our machine learning approach to unsupervised image segmentation and discus our experiment and their result 
the pagerank algorithm used in the google search engine greatly improves the result of web search by taking into account the link structure of the web pagerank assigns to a page a score propor tional to the number of time a random surfer would visit that page if it surfed indefinitely from page to page following all outlinks from a page with equal probability we propose to improve page rank by using a more intelligent surfer one that is guided by a probabilistic model of the relevance of a page to a query efficient execution of our algorithm at query time is made possible by pre computing at crawl time and thus once for all query the neces sary term experiment on two large subset of the web indicate that our algorithm significantly outperforms pagerank in the hu man rated quality of the page returned while remaining efficient enough to be used in today s large search engine 
we demonstrate a method for evaluating edge detector performance based on receiver operating characteristic roc curve edge detector output is matched against ground truth to count true positive and false positive edge pixel a detector s parameter setting are trained to give a best roc curve on one image and then tested on separate image we compute aggregate roc curve based on set of object image and another set of aerial image we analyze the performance of different edge detector reported in the literature 
we are developing corpus based technique for identifying semantic relation at an intermediate level of description more specific than those used in case frame but more general than those used in traditional knowledge representation system in this paper we describe a classification algorithm for identifying relationship between two word noun compound we find that a very simple approach using a machine learning algorithm and a domain specific lexical hierarchy successfully generalizes from training instance performing better on previously unseen word than a baseline consisting of training on the word themselves 
we describe the task of user oriented anomaly detection for computer security in this domain the goal is to develop a model of a computer user s normal behavioral pattern and to detect anomalous condition a deviation from expected behavior we present an instance based learning ibl system for profiling user and examine some domain constraint with respect to our approach in particular we explore the data reduction problem this domain is subject to unbounded data and concept drift but is constrained by limited resource so we must limit the size of the learned model we empirically examine the data reduction performance of two clustering method an em procedure k center and a greedy clustering method developed to address domain characteristic we evaluate the relative strength of the two method along three performance ax accuracy mean time to generation of an alarm tta and data compression 
weakening implication by assuming the object identity bias allows for both a model theoretical and a proof theoretical definition of a novel and more manageable ordering relationship over clausal space in this paper we give two important result namely the soundness and the refutation completeness through a subsumption theorem of the underlying derivation procedure that make this relationship particularly appealing for inducing a generalization model for clausal search space 
this paper investigates the problem of policy learning in multiagentenvironments using the stochastic game framework whichwe briefly overview we introduce two property a desirable forum learning agent when in the presence of other learning agent namely rationality and convergence we examine existing reinforcementlearning algorithm according to these two propertiesand notice that they fail to meet both criterion we then contributea new learning algorithm adjusted policy 
in many application one would like to use information from both color and texture feature in order to segment an image we propose a novel technique to combine soft segmentation computed for two or more feature independently our algorithm merges model according to a maximum descriptiveness criterion and allows to choose any number of class for the final grouping this technique also allows to improve the quality of supervised classification based on one feature e g color by merging information from unsupervised segmentation based on another feature e g texture 
new application in field such a augmented or virtualized reality have created a demand for dense accurate real time stereo reconstruction our goal is to reconstruct a user and her office environment for networked tele immersion which requires accurate depth value in a relatively large workspace in order to cope with the combinatorics of stereo correspondence we can exploit the temporal coherence of image sequence by using coarse optical flow estimate to bound disparity search range at the next iteration we use a simple flood fill segmentation method to cluster similar disparity value into overlapping window and predict their motion over time using a single optical flow calculation per window we assume that a contiguous region of disparity represents a single smooth surface which allows u to restrict our search to a narrow disparity range the value in the range may vary over time a object move nearer or farther away in z but we can limit the number of disparity to a feasible search size per window further the disparity search and optical flow calculation are independent for each window and allow natural distribution over a multi processor architecture we have examined the relative complexity of stereo correspondence on full image versus our proposed window system and found that depending on the number of frame in time used to estimate optical flow the window based system requires about half the time of standard correlation stereo experimental comparison to full image correspondence search show our window based reconstruction compare favourably to those generated by the full algorithm even after several frame of propagation via estimated optical flow the result is a system twice a fast a conventional dense correspondence without significant degradation of extracted depth value 
plan library are the most important knowledge source of many plan recognition system the plan decomposition they contain provide information about how a plan ha to be executed to actually achieve it associated goal and be recognized by the system this paper present an approach to the automatic acquisition of plan decomposition from sample action sequence in particular a clustering algorithm is introduced that allows group of similar sequence to be discovered and used for the generation of plan library empirical test indicate that these library can indeed be successfully used for plan recognition purpose 
reliable estimation of the trifocal tensor is crucial for d reconstruction from uncalibrated camera the estimation process is based on minimizing the geometric distance between the measurement and the corrected data point the underlyingnonlinearoptimizationproblembeing most often solved with the levenberg marquardt lm algorithm we employ for this task the heteroscedastic error in variable heiv estimator and take into account both the singularity of the multivariate tensor constraint and the bifurcation which can appear for noisy data in comparisonto the gold standard method the new approach is significantlyfaster while having the same performance and it is le sensitive to initialization when the data is close to degenerate analytical expression for the covariancesof the parameterand corrected image point estimate are available for the heiv estimator and thus the confidenceregions of the corrected measurement can be delineated in the image trifocal tensor the trifocal tensor describes the intrinsic projective property of a group of three image taken with uncalibrated camera the role of trifocal tensor in the projective reconstruction of d structure is extensively discussed in the literature and we refer to the recent book pp for an excellent treatment of all the relevant topic and to for an comprehensive discussion of the involved optimization method in this paper will focus on the problem of estimating the trifocal tensor from view point correspondence i e from the matched image of d point we will assume that all the correspondence are correct while the estimation method presented here can be easily robustifiedsimilar to we concentrate on issue related to the behavior of the estimation process i e on numerical robustness will start by reviewing the geometric relation needed in the sequel given three camera characterized by unknown projective matrix the image of a d point in each view will be denoted in homogeneous coordinate and similarly for the point in the other two image the projective ambiguity allows to express the camera matrix a 
we present a stochastic parsing system consisting of a lexical functional grammar lfg a constraint based parser and a stochastic disambiguation model we report on the result of applying this system to parsing the upenn wall street journal wsj treebank the model combine full and partial parsing technique to reach full grammar coverage on unseen data the treebank annotation are used to provide partially labeled data for discriminative statistical estimation using exponential model disambiguation performance is evaluated by measuring match of predicate argument relation on two distinct test set on a gold standard of manually annotated f structure for a subset of the wsj treebank this evaluation reach f score an evaluation on a gold standard of dependency relation for brown corpus data achieves f score 
we explore the use of a connectionistlearningsystem designed to allow the applicationof reinforcement learning to robot control in particular we compare direct and indexedpartitioning method and find indexedpartitioning ha advantage in time and spacecomplexity learning speed measured in trial and success rate we make these comparisonsbased on extensive simulation andruns on a real robot learning on line 
an approach to automatic detection of syllable boundary is presented we demonstrate the use of several manually constructed grammar trained with a novel algorithm combining the advantage of treebank and bracketed corpus training we investigate the effect of the training corpus size on the performance of our system the evaluation show that a hand written grammar performs better on finding syllable boundary than doe a treebank grammar 
in we presented a new velocity estimation algorithm using orientation tensor and parametric motion model to provide both fast and accurate result one of the tradeoff between accuracy and speed wa that no attempt were made to obtain region of coherent motion when estimating the parametric model in this paper we show how this can be improved by doing a simultaneous segmentation of the motion field the resulting algorithm is slower than the previous one but more accurate this is shown by evaluation on the well known yosemite sequence where already the previous algorithm showed an accuracy which wa substantially better than for earlier published method this result ha now been improved further 
most conventional law equation discovery system such a bacon require experimental environment to acquire their necessary data the mathematical technique such a linear system identification and neural network fitting presume the class of equation to model given observed data set the study reported in this paper proposes a novel method to discover an admissible model equation from a given set of observed data while the equation is ensured to reflect first principle governing the objective system the power of the proposed method come from the use of the scale type of the observed quantity a mathematical property of identity and quasi bi variate fitting to the given data set it principle and algorithm are described with moderately complex example and it practicality is demonstrated through a real application to psychological and sociological law equation discovery 
we describe a shape from texture method that construct a maximum a posteriori estimate of surface coefficient using both the deformation of individual texture element a in local method and the overall distribution of element a in global method the method described applies to a much larger family of texture than any previous method local or global we demonstrate an analogy with shape from shading and use this to produce a numerical method example of reconstruction for synthetic image of surface are provided and compared with ground truth the method is defined for orthographic view but can be generalised to perspective view simply keywords shape from texture texture computer vision surface fit 
this paper is a contribution to the recovery of shape from texture under perspective projection we regard shape from texture a a statistical estimation problem the texture be ing the realization of a stochastic process there are two minimal condition in order for the problem to be solvable the first is a stationarity condition on the texture with respect to the surface the second is the regularity of the surface information about the surface is obtained by estimating a deformation map we prove that at a fine scale the wavelet decomposition of the image obeys a transport pde the coefficient of which can be estimated and related to the deformation map we show how the global surface shape can then be integrated 
a method is presented to recover d scene structure and camera motion from multiple image without the need for correspondence information the problem is framed a finding the maximum likelihood structure and motion given only the d measurement integrating over all possible assignment of d feature to d measurement this goal is achieved by mean of an algorithm which iteratively refines a probability distribution over the set of all correspondence assignment at each iteration a new structure from motion problem is solved using a input a set of virtual measurement derived from this probability distribution the distribution needed can be efficiently obtained by markov chain monte carlo sampling the approach is cast within the framework of expectation maximization which guarantee convergence to a local maximizer of the likelihood the algorithm work well in practice a will be demonstrated using result on several real image sequence 
in this paper we explore the power of surface text pattern for open domain question answering system in order to obtain an optimal set of pattern we have developed a method for learning such pattern automatically a tagged corpus is built from the internet in a bootstrapping process by providing a few hand crafted example of each question type to altavista pattern are then automatically extracted from the returned document and standardized we calculate the precision of each pattern and the average precision for each question type these pattern are then applied to find answer to new question using the trec question set we report result for two case answer determined from the trec corpus and from the web 
we present a simple sparse greedy technique to approximate the maximum a posteriori estimate of gaussian process with much improved scaling behaviour in the sample size m in particular computational requirement are o n m storage is o nm the cost for prediction is o n and the cost to compute con dence bound is o nm where n m we show how to compute a stopping criterion give bound on the approximation error and show application to large scale problem introduction gaussian 
we have been developing a theory for the generic representation of d shape where structural description are derived from theshocks singularity of a curve evolution process acting on boundingcontours we now apply the theory to the problem of shape matching the shocksare organized into a directed acyclic shock graph and complexity ismanaged by attending to the most significant central shape componentsfirst the space of all such graph is highly structured and can becharacterized by the rule of a shock graph grammar the grammarpermits a reduction of a shock graph to a unique rooted shock tree weintroduce a novel tree matching algorithm which find the best set ofcorresponding node between two shock tree in polynomial time using adiverse database of shape we demonstrate our system s performance underarticulation occlusion and moderate change in viewpoint 
disjunctive logic programming dlp with sta ble model semantics is a powerful nonmono tonic formalism for knowledge representation and reasoning reasoning with dlp is harder than with normal v free logic program liecause stable model checking deciding whether a given model is a stable model of a propositional dlp program is co np complet e while it is polynomial for normal logic program this paper proposes a new transformation 
most document are about more than one subject but many nlp and ir technique implicitly assume document have just one topic we describe new clue that mark shift to new topic novel algorithm for identifying topic boundary and the us of such boundary once identified we report topic segmentation performance on several corpus a well a improvement on an ir task that benefit from good segmentation 
we present the multilingual summarization functionality for verb mobil a speech translation system we reuse resource of the system to create a summary after content extraction we interpret the result in the dialog context a summary generator provides the input to generation a first evaluation indicates the feasibility of the approach 
network routing is a distributed decision problem which naturally admits numerical performance measure such a the average time for a packet to travel from source to destination olpomdp a policy gradient reinforcement learning algorithm wa successfully applied to simulated network routing under a number of network model multiple distributed agent router learned cooperative behavior without explicit interagent communication and they avoided behavior which wa individually desirable 
extracting rule from rbfs is not a trivial task because of nonlinear function or high input dimensionality in such case some of the hidden unit of the rbf network have a tendency to be shared across several output class or even may not contribute to any output class to address this we have developed an algorithm called lrex for local rule extraction which tackle these issue by extracting rule at two level hrex extract rule by examining the hidden unit to class assignment while mrex extract rule based on the input space to output space mapping the rule extracted by our algorithm are compared and contrasted against a competing local rule extraction system the central claim of this paper is that local function network such a radial basis function rbf network have a suitable architecture based on gaussian function that is amenable to rule extraction 
we explore the statistical property of natural sound stimulus preprocessed with a bank of linear filter the response of such filter exhibit a striking form of statistical dependency in which the response variance of each filter grows with the response amplitude of filter tuned for nearby frequency these dependency may be substantially reduced using an operation known a divisive normalization in which the response of each filter is divided by a weighted sum of the rectified response of other filter the weight may be chosen to maximize the independence of the normalized response for an ensemble of natural sound we demonstrate that the resulting model account for non linearity in the response characteristic of the auditory nerve by comparing model simulation to electrophysiological recording in previous work nip we demonstrated that an analogous model derived from the statistic of natural image account for non linear property of neuron in primary visual cortex thus divisive normalization appears to be a generic mechanism for eliminating a type of statistical dependency that is prevalent in natural signal of different modality 
we present a simple architecture for parsing transcribed speech in which an edited word detector first remove such word from the sentence string and then a standard statistical parser trained on transcribed speech par the remaining word the edit detector achieves a misclassification rate on edited word of the null model which mark everything a not edited ha an error rate of to evaluate our parsing result we introduce a new evaluation metric the purpose of which is to make evaluation of a parse tree relatively indifferent to the exact tree position of edited node by this metric the parser achieves precision and recall 
kernel principal component analysis pca is an elegant nonlineargeneralisation of the popular linear data analysis method where a kernel function implicitly denes a nonlinear transformationinto a feature space wherein standard pca is performed unfortunately the technique is not sparse since the componentsthus obtained are expressed in term of kernel associated with everytraining vector this paper show that by approximating thecovariance matrix in feature space by a reduced 
a key question in neuroscience is how to encode sensory stimulus such a image and sound motivated by study of response property of neuron in the early cortical area we propose an encoding scheme that dispenses with absolute measure of signal intensity or contrast and us instead only local ordinal measure in this scheme the structure of a signal is represented by a set of equality and inequality across adjacent region in this paper we focus on characterizing the fidelity of this representation strategy we develop a regularization approach for image reconstruction from ordinal measure and thereby demonstrate that the ordinal representation scheme can faithfully encode signal structure we also present a neurally plausible implementation of this computation that us only local update rule the result highlight the robustness and generalization ability of local ordinal encoding for the task of pattern classification in this paper we introduce and characterize a biologically plausible representation scheme for encoding signal structure the scheme employ a simple vocabulary of local ordinal relation of the kind that early sensory neuron are capable of extracting our result so far suggest that this scheme posse several desirable characteristic including tolerance to object appearance variation computational simplicity and low memory requirement we develop and demonstrate our idea in the visual domain but they are intended to be applicable to other sensory modality a well the starting point for our proposal lie in study of the response property of neuron in the early sensory cortical area these response property constrain 
this paper describes pairwise bisection a nonparametric approach to optimizing anoisy function with few function evaluation the algorithm us nonparametric reasoningabout simple geometric relationship to findminima efficiently two factor often frustrateoptimization noise and cost outputcan contain significant quantity of noise orerror while time or money allows for onlya handful of experiment pairwise bisectionis used here to attempt to automate the processof 
this paper present a new formalization of a unificationor join preserving encoding of partially ordered set that more essentially capture what it mean for an encoding to preserve join generalizing the standard definition in ai research it then show that every statically typable ontology in the logic of typed feature structure can be encoded in a data structure of fixed size without the need for resizing or additional union find operation this is important for any grammar implementation or development system based on typed feature structure a it significantly reduces the overhead of memory management and reference pointer chasing during unification 
extracting rule from rbfs is not a trivial task because of nonlinear function or high input dimensionality in such case some of the hidden unit of the rbf network have a tendency to be shared across several output class or even may not contribute 
the major challenge that face american sign language asl recognition now is to develop method that will scale well with increasing vocabulary size unlike in spoken language phoneme can occur simultaneously in asl the number of possible combination of phonemesafter enforcing linguistic constraint is approximately gesture recognition which is le constrained than asl recognition suffers from the same problem thus it is not feasible to train conventional hidden markov model hmms for large scale asl application factorial hmms and coupled hmms are two extension to hmms that explicitly attempt to model several process occuring in parallel unfortunately they still require consideration of the combination at training time in this paper we present a novel approach to asl recognition that aspires to being a solution to the scalability problem it is based on parallel hmms pahmms which model the parallel process independently thus they can also be trained independently and do not require consideration of the different combination at training time we develop the recognition algorithm for pahmms and show that it run in time polynomial in the number of state and in time linear in the number of parallel process we run several experiment with a sign vocabulary and demonstrate that pahmms can improve the robustness of hmm based recognition even on a small scale thus pahmms are a very promising general recognition scheme with application in both gesture and asl recognition 
we describe a speedup for training conditional maximum entropy model the algorithm is a simple variation on generalized iterative scaling but converges roughly an order of magnitude faster depending on the number of constraint and the way speed is measured rather than attempting to train all model parameter simultaneously the algorithm train them sequentially the algorithm is easy to implement typically us only slightly more memory and will lead to improvement for most maximum entropy problem 
we define a new image feature called the color correlogramand use it for image indexing and comparison this feature distills the spatial correlation of color and when computed efficiently turn out to be both effective and inexpensive for content based image retrieval the correlogram is robust in tolerating large change in appearance and shape caused by change in viewing position camera zoom etc experimental evidence show that this new feature outperforms not only the traditional color histogram method but also the recently proposed histogram refinement method for image indexing retrieval we also provide a technique to cut down the storage requirement of the correlogram so that it is the same a that of histogram with only negligible performance penalty compared to the original correlogram we also suggest the use of color correlogram a a generic indexing tool to tackle various problem arising from image retrieval and video browsing we adapt the correlogram to handle the problem of image subregion querying object localization object tracking and cut detection experimental result again suggest that the color correlogram is more effective than the histogram for these application with insignificant additionalstorage or processing cost 
this paper investigates how the splitting criterion and pruning method of decision tree learning algorithm are influenced by misclassification cost or change to the class distribution splitting criterion that are relatively insensitive to cost class distribution are found to perform a well a or better than in term of expected misclassification cost splitting criterion that are cost sensitive consequently there are two opposite way of dealing with imbalance one is to combine a costinsensitive splitting criterion with a cost insensitive pruning method to produce a decision tree algorithm little affected by cost or prior class distribution the other is to grow a cost independent tree which is then pruned in a cost sensitive manner 
an important issue in neural computing concern the description oflearning dynamic with macroscopic dynamical variable recentprogress on on line learning only address the often unrealisticcase of an innite training set we introduce a new framework tomodel batch learning of restricted set of example widely applicableto any learning cost function and fully taking into account thetemporal correlation introduced by the recycling of the example for illustration we analyze 
this paper focus on matching d structure by variational method we provide rigorous rule for the construction of the cost function on the basis of an analysis of property which should be satisfied by the optimal matching a new exact dynamic programming algorithm is then designed for the minimization we conclude with experimental result on shape comparison 
spoken dialog manager have benefited from stochastic planner such a mdps however so far mdps do not handle well noisy and ambiguous utterance from the user we address this problem by inverting the notion of dialog state the state represents the user s intention rather than the system state this approach allows for simple and intuitive dialog description at the sacrifice of state observability we use a pomdp style approach to generate dialog policy however the intractability of pomdp solution requires an approximate solution we instead augment the state representation of the mdp by providing the system with the maximum likelihood state and a compressed representation of the belief state in this way the system can approximate the optimal pomdp solution but at mdp like speed 
this paper address the problem of recovering structure and motion from silhouette silhouette are projection of contour generator which are viewpoint dependent and hence do not readily provide point correspondence for exploitation in motion estimation previous work have exploited correspondence induced by epipolar tangency and a successful solution ha been developed in the special case of circular motion turntable sequence however the main drawback are new view cannot be added easily at a later time and part of the structure will always remain invisible under circular motion in this paper we overcome the above problem by incorporating arbitrary general view and estimating the camera pose using silhouette alone we present a complete and practical system which produce high quality d model from d uncalibrated silhouette the d model thus obtained can be refined incrementally by adding new arbitrary view and estimating their pose experimental result on various object are presented demonstrating the quality of the reconstruction 
the bayesian framework is ideally suited for induction problem the probability of observing x t at time t given past observation x x t can be computed with bayes rule if the true distribution mu of the sequence x x x is known the problem however is that in many case one doe not even have a reasonable estimate of the true distribution in order to overcome this problem a universal distribution xi is defined a a weighted sum of distribution mu i in m where m is any countable set of distribution including mu this is a generalization of solomonoff induction in which m is the set of all enumerable semi measure system which predict y t given x x t and which receive loss l x t y t if x t is the true next symbol of the sequence are considered it is proven that using the universal xi a a prior is nearly a good a using the unknown true distribution mu furthermore game of chance defined a a sequence of bet observation and reward are studied the time needed to reach the winning zone is bounded in term of the relative entropy of mu and xi extension to arbitrary alphabet partial and delayed prediction and more active system are discussed 
sensitivity analysis of neural network is useful for network design pich used a stochastic model to describe the multilayer perceptron mlp but it doesn t match the true mlp closely and too severe limitation are imposed on both input and weight perturbation this paper attempt to generalize pich s stochastic model of mlp and derive an universal expression of mlp s sensitivity for all sigmoidal activation function without any restriction on input and output perturbation the effect of network design parameter such a the number of layer the number of neuron per layer and the chosen activation function are analyzed and they provide useful information for network design decision making furthermore we use our sensitivity expression to design mlp for a given application it can help to design the network structure a well a the training of mlp 
sentence planning is a set of inter related but distinct task one of which is sentence scoping i e the choice of syntactic structure for elementary speech act and the decision of how to combine them into one or more sentence in this paper we present spot a sentence planner and a new methodology for automatically training spot on the basis of feedback provided by human judge we reconceptualize the task into two distinct phase first a very simple randomized sentence plan generator spg generates a potentially large list of possible sentence plan for a given text plan input second the sentence plan ranker spr rank the list of output sentence plan and then selects the top ranked plan the spr us ranking rule automatically learned from training data we show that the trained spr learns to select a sentence plan whose rating on average is only worse than the top human ranked sentence plan 
the computation of optical flow from image derivativesis biased in region of non uniform gradient distribution a least square or total least square approachto computing optic flow from image derivativeseven in region of consistent flow can lead to a systematicbias dependent upon the direction of the opticflow the distribution of the gradient direction andthe distribution of the image noise the bias a consistentunderestimation of length and a directional error similar result 
we analyse the complexity of standard and weak model checking for propositional default logic in particular we solve the open problem of complexity in case of normal default theory and introduce a new ample class of default theory with a tractable model checking problem 
tree structured probabilistic model admit simple fast inference however they are not well suited to phenomenon such a occlusion where multiple component of an object may disappear simultaneously mixture of tree appear to address this problem at the cost of representing a large mixture we demonstrate an efficient and compact representation of this mixture which admits simple learning and inference algorithm we use this method to build an automated tracker for muybridge sequence of a variety of human activity tracking is difficult because the temporal dependency rule out simple inference method we show how to use our model for efficient inference using a method that employ alternate spatial and temporal inference the result is a tracker that a us a very loose motion model and so can track many different activity at a variable frame rate and b is entirely automatic 
i consider the problem of learning concept from small number of po itive example a feat which human perform routinely but which com puters are rarely capable of bridging machine learning and cognitive science perspective i present both theoretical analysis and an empirical study with human subject for the simple task of learning concept corre sponding to axis aligned rectangle in a multidimensional feature space existing learning model when applied to this task cannot explain how subject generalize from only a few example of the concept i propose a principled bayesian model based on the assumption that the example are a random sample from the concept to be learned the model give precise fit to human behavior on this simple task and provides qualitative insight into more complex realistic case of concept learning 
we present a hidden markov model hmm for inferring the hiddenpsychological state or neural activity during single trail fmri activationexperiments with blocked task paradigm inference is based onbayesian methodology using a combination of analytical and a varietyof markov chain monte carlo mcmc sampling technique the advantageof this method is that detection of short time learning effect betweenrepeated trail is possible since inference is based only on singletrail 
we present a hidden markov model hmm for inferring the hidden psychological state or neural activity during single trial fmri activation experiment with blocked task paradigm inference is based on bayesian methodology using a combination of analytical and a variety of markov chain monte carlo mcmc sampling technique the advantage of this method is that detection of short time learning effect between repeated trial is possible since inference is based only on single trial experiment 
we present a novel method for clustering using the support vector machine approach data point are mapped to a high dimensional feature space where support vector are used to define a sphere enclosing them the boundary of the sphere form in data space a set of closed contour containing the data a the kernel parameter is varied these contour fit the data more tightly and splitting of contour occurs the contour are interpretedas cluster boundariesand the point within each disconnected contour are defined a a cluster cluster boundary can take on arbitrary geometricalshapesandclustersareseparatedbyvalleysintheunderlying probability distribution a in other sv algorithm outlier can be dealt with by introducing a soft margin constant leading to smoother cluster boundary the hierarchical structure of the data is explored by varying the two parameter we investigate the dependence of our method on these parameter and apply it to several data set 
the proliferation of online information resource increase the importance of effective and efficient distributed searching distributed searching is cast in three part database selection query processing and result merging in this paper we examine the effect of database selection on retrieval performance we look at retrieval performance in three different distributed retrieval testbeds and distill some general result first we find that good database selection can result in better retrieval effectiveness than can be achieved in a centralized database second we find that good performance can be achieved when only a few site are selected and that the performance generally increase a more site are selected finally we find that when database selection is employed it is not necessary to maintain collection wide information cwi e g global idf local information can be used to achieve superior performance this mean that distributed system can be engineered with more autonomy and le cooperation this work suggests that improvement in database selection can lead to broader improvement in retrieval performance even in centralized i e single database system given a centralized database and a good selection mechanism retrieval performance can be improved by decomposing that database conceptually and employing a selection step 
using method of statistical physic we investigate the r amp ocirc le of model complexity in learning with support vector machine svms we show the advantage of using svms with kernel of infinite complexity on noisy target rule which in contrast to common theoretical belief are found to achieve optimal generalization error although the training error doe not converge to the generalization error moreover we find a universal asymptotics of the learning curve which only depend on the 
croma keying is the process of segmenting object from image and video using color cue a blue or green screen placed behind an object during recording is used in special effect and in virtual studio the blue color is later replaced by a different background blue screen is an example ofchroma keying where the keying signal is chroma difference a new method for automatic keying using invisible signal is presented the advantage of the new approach over conventional chroma keying include i unlimited color range for foreground object ii no foreground contamination by background color iii better performance in non uniform illumination iv feature for generating refraction and reflection of dynamic object the method can be used in real time and no user assistance is required new design of catadioptric camera and a single chip sensor for keying is also presented 
nonlinear support vector machine svms are investigated forvisual sex classification with low resolution quot thumbnail quot face by pixel processed from image from the feret facedatabase the performance of svms is shown to be superior totraditional pattern classifier linear quadratic fisher linear discriminant nearest neighbor a well a more modern techniquessuch a radial basis function rbf classifier and large ensemblerbfnetworks furthermore the svm 
abstract 
guided by an initial idea of building a complex non linear decision surface with maximal local margin in input space we give a possible geometrical intuition a to why k nearest neighbor knn algorithm often perform more poorly than svms on classification task we then propose modified k nearest neighbor algorithm to overcome the perceived problem the approach is similar in spirit to tangent distance but with invariance inferred from the local neighborhood rath er than prior knowledge experimental result on real world classificati on task suggest that the modified knn algorithm often give a dramatic im provement over standard knn and perform a well or better than svms 
the singular value decomposition svd of a matrix is a linear algebra tool that ha been successfully applied to a wide variety of domain the present paper is concerned with the problem of estimating the jacobian of the svd component of a matrix with respect to the matrix itself an exact analytic technique is developed that facilitates the estimation of the jacobian using calculation based on simple linear algebra knowledge of the jacobian of the svd is very useful in certain application involving multivariate regression or the computation of the uncertainty related to estimate obtained through the svd the usefulness and generality of the proposed technique is demonstrated by applying it to the estimation of the uncertainty for three different vision problem namely self calibration epipole computation and rigid motion estimation 
in this paper we present and compare automatically generated title for machine translated document using several different statistic based method a na ve bayesian a k nearest neighbour a tf idf and an iterative expectation maximization method for title generation were applied to original english news document and again to the same document translated from english into portuguese french or german and back to english using systran the autosummarization function of microsoft word wa used a a base line result on several metric show that the statistic based method of title generation for machine translated document are fairly language independent and title generation is possible at a level approaching the accuracy of title generated for the original english document 
multi resolution technique have been used in a wide range of vision application unfortunately the costly operation of building a proper pyramid strongly reduces it value a a tool for reducing computational cost a new approach physical panoramic pyramid is introduced in this paper physical panoramic pyramid measure multiple resolution simultaneouslyresulting in multi resolution panoramic image no computationis needed to construct these image pyramid we also analyze general noise sensitivity in image pyramid including the interaction of the loss of resolution random background noise and aliasing noise the paper also discus the issue of indexing between the neighboring layer the viewpoint variation and the application of the physical panoramic pyramid 
image based interpolation creates smooth and photorealistic view between two view point the concept of joint view triangulation jvt ha been proven to be an efficient multi view representation to handle visibility issue however the existing jvt built only on a regular sampling grid often produce undesirable artifact for artificial object to tackle these problem a new edge constrained joint view triangulation is developed in this paper to integrate contour point and artificial rectilinear object a triangulation constraint also a super sampling technique is introduced to refine visible boundary the new algorithm is successfully demonstrated on many real image pair 
given a sequence of pair of image gathered with an uncalibrated stereo camera pair and given a set of point to point correspondence between these image pair we describe a method that segment the observed scene into static and moving object while it reject badly matched point unlike many approach which were suggested in the past the method allows for both motion of the camera pair egomotion and non rigid scene scene composed of static object a well a object undergoing various motion first we establish the projective framework enabling u to characterize rigid motion in projective space second we use this characterization in conjunction with a robust estimation technique to determine egomotion third we describe a method based on data classification which further considers the non static scene point anal group them into several moving object finally we show some preliminary experiment involving a moving stereo head observing both static and moving object 
this paper describes the region occlusion calculus roc that can be used to model spatial occlusion and the effect of motion parallax of arbitrary shaped object roc assumes the region based ontology of rcc and extends galton s line of sight calculus by allowing concave shaped object into the modelled domain this extension is used to describe the effect of mutually occluding body the inclusion of van benthem s axiomatisation of comparative nearness facilitates reasoning about relative distance between occluding body further an envisionment table is developed to model sequence of occlusion event enabling reasoning about object and their image formed in a changing visual field 
abstract we show how map learning can be formulated a inference in a graphical model which allows u to handle changing environment in a natural manner we describe several different approximation scheme for the problem and illustrate some result on a simulated grid wo rld with door that can open and close we close by briefly discussing how to l earn more general model of partially observed environment which can contain a variable number of object with changing internal state 
the paper present a new approach to computing depth map from a large collection of image where the camera motion ha been constrained to planar concentric circle we resample the resulting collection of regular perspective image into a set of multiperspective panorama and then compute depth map directly from these resampled image only a small number of multiperspective panorama is needed to obtain a dense and accurate d reconstruction since our panorama sample uniformly in three dimension rotation angle inverse radial distance and vertical elevation using multiperspective panorama avoids the limited overlap between the original input image that cause problem in conventional multi baseline stereo our approach differs from stereo matching of panoramic image taken from different location where the epipolar constraint are sine curve for our multiperspective panorama the epipolar geometry to first order consists of horizontal line therefore any traditional stereo algorithm can be applied to multiperspective panorama without modification experimental result show that our approach generates good depth map that can be used for image based rendering task such a view interpolation and extrapolation 
we formulate colour constancy a a problem ofbayesian inference where one is trying to representthe posterior on possible interpretation given imagedata we represent the posterior a a set of sample drawn from that distribution using a markov chainmonte carlo method we show how to build an efficientsampler this approach ha the advantage that it unifies theconstraints on the problem and represents possibleambiguities in turn a good description of possibleambiguities mean that 
we present performance measurement result for a parallel sql based information retrieval system implemented on a pc cluster system we used the web trec dataset under a left deep query execution plan we achieved satisfactory speed up 
an approach to extract watershed and watercourse a well a their corresponding valley and hill from image with subpixel precision is proposed the critical point of the terrain are essential a the starting point for the construction of these separatrix they are extracted efficiently with subpixel precision using an approach based on derivative of gaussian filter the separatrix are extracted by integrating their defining differential equation finally the hill and valley are constructed by an efficient graph search algorithm example show the quality of the result that can be achieved with the proposed approach 
there are many task that require information finding some can be largely automated and others greatly benefit from successful interaction between system and searcher we are interested in the task of answering question where some synthesis of information is required the answer would not generally be given from a single passage of a single document we investigate whether variation in the way a list of document is delivered affected searcher performance in the question answering task we will show that there is a significant difference in performance using a list customized to the task type compared with a standard web engine list this indicates that paying attention to the task and the searcher interaction may provide substantial improvement in task performance 
we present in this paper the design of an interactive tool for selecting object using simple freehand sketch the objective is to extract object boundary precisely while requiring little skill and time from the user the tool proposed achieves this objective by integrating user input and image computation in a two phase algorithm in the first phase the input sketch is used along with a coarse global segmentation of the image to derive an initial selection and a triangulation of the region around the boundary the triangle are used to formulate subproblems of local finer grained segmentation and selection each of the subproblems is processed independently in the second phase where a linear approximation of the local boundary a well a a local finer grained segmentation are computed the approximate boundary is then used with the local segmentation to compute a final selection represented with an alpha channel to fully capture diffused object boundary experimental result show that the tool allows very simple sketch to be used to select object with complex boundary therefore the tool ha immediate application in graphic system for image editing manipulation synthesis retrieval and processing 
we propose a method to estimate the motion of a person filmed by two or more fixed camera the novelty of our technique is it ability to cope with fast movement self occlusion and noisy image our algorithm are based on the latest work on calibration and image segmentation developed in our lab we compare the projection of a d model of a person on the image to the detected silhouette of the person and create force that will move the d model towards the final estimation of the real pose we developed a fast algorithm that computes the motion of the articulated d model we show that our result are good even if the camera are not synchronized 
we show that node of high degree tend to occur infrequently in random graph but frequently in a wide variety of graph associated with real world search problem we then study some alternative model for randomly generating graph which have been proposed to give more realistic topology for example we show that watt and strogatz s small world model ha a narrow distribution of node degree on the other hand barab si and albert s power law model give graph with both node of high degree and a small world topology these graph may therefore be useful for benchmarking we then measure the impact of node of high degree and a small world topology on the cost of coloring graph the long tail in search cost observed with small world graph disappears when these graph are also constructed to contain node of high degree we conjecture that this is a result of the small size of their backbone pair of edge that are frozen to be the same color 
data noise is present in many machine learning problem domain some of these are well studied but others have received le attention in this paper we propose an algorithm for constructing a kernel fisher discriminant kfd from training example with noisy label the approach allows to associate with each example a probability of the label being flipped we utilise an expectation maximization em algorithm for updating the probability the e step us class conditional probability estimated a a by product of the kfd algorithm the m step update the flip probability and determines the parameter of the discriminant we demonstrate the feasibility of the approach on two real world data set 
abstract we investigate a learning algorithm for the classification of nonnegative data by mixture model multiplicative update rule are derived that directly optimize the performance of these model a classifier the update rule have a simple closed form and an intuitive appeal our algorithm retains the main virtue of the expectation maximization em algorithm it guarantee of monotonic improvement and it absence of tuning parameter with the added advantage of optimizing a discriminative objective function the algorithm reduces a a special case to the method of generalized iterative scaling for log linear model the learning rate of the algorithm is controlled by the sparseness of the training data we use the method of nonnegative matrix factorization nmf to discover sparse distributed representation of the data this form of feature selection greatly accelerates learning and make the algorithm practical on large problem experiment show that discriminatively trained mixture model lead to much better classification than comparably sized model trained by em 
abstract this paper describes an approach to reinforcementlearning in multiagent general sumgames in which a learner is told to treat eachother agent a either a friend quot or foe quot thisq learning style algorithm provides strongconvergence guarantee compared to an existingnash equilibrium based learning rule 
in recent year auction have grown in interest within the ai community a innovative mechanism for resource allocation the primary contribution of this paper is to identify a family of hybrid auction called survival auction which combine the benefit of both sealed bid auction namely quick and predictable termination time and ascending bid auction namely more information revelation often leading among other thing to better allocation and greater expected revenue survival auction are multi round sealed bid auction with an information revelation component in which some bidder are eliminated from the auction from one round to the next these auction are intuitive easy to implement and most importantly provably optimal more precisely we show that a the survival auction in which all but the lowest bidder make it into the next round the auction last for n round when there are n bidder is strategically equivalent to the japanese ascending bid auction which itself ha been proven to be optimal in many setting and that b under certain symmetry condition even a survival auction in which only the two highest bidder make it into the next round the auction last only two round is nash outcome equivalent to the japanese auction 
object oriented representation of image sequence require s accurate motion segmentation and depth ordering technique unfortunately the lack of precise motion estimate s at the object boundary make these two task very difficult in this paper we present a detailed analysis of the behaviour of dense motion estimation technique at object boundary which reveals the systematic nature of the motion estimatio n error the motion of the occluding surface is observed in a small neighbourhood on the occluded side we then show how the joint use of still image segmentation and robust regression can eliminate this error furthermore we present a novel technique which us the position of the error a a depth cue the validity of this technique which requires only sub pixel motion and which is capable of distinguishing between different type of intensity discontinuity such a object boundary surface mark and illumination discontinuity is then demonstrated on several synthetic an d real image sequence 
we propose randomized technique for speeding up kernel principal component analysis on three level sampling and quantization of the gram matrix in training randomized rounding in evaluating the kernel expansion and random projection in evaluating the kernel itself in all three case we give sharp bound on the accuracy of the obtained approximation rather intriguingly all three technique can be viewed a instantiation of the following idea replace the kernel function by a randomized kernel which behaves like in expectation 
do improvement in system performance demonstrated by batch evaluation confer the same benefit for real user we carried out experiment designed to investigate this question after identifying a weighting scheme that gave maximum improvement over the baseline in a non interactive evaluation we used it with real user searching on an instance recall task our result showed the weighting scheme giving beneficial result in batch study did not do so with real user further analysis did identify other factor predictive of instance recall including number of document saved by the user document recall and number of document seen by the user 
most study concerning constraint satisfaction problem csps involve variable that take value from small domain this paper deal with an alternative form of temporal csps the number of variable is relatively small and the domain are large collection of interval such situation may arise in temporal database where several type of query can be modeled and processed a csps for these problem systematic csp algorithm can take advantage of temporal indexing to accelerate search directed search version of chronological backtracking and forward checking are presented and tested our result show that indexing can drastically improve search performance 
abstract this paper present a novel and fast k nn classifier that is based on a binary cmm correlation matrix memory neural network a robust encoding method is developed to meet cmm input requirement a hardware implementation of the cmm is described which give over time the speed of a current mid range workstation and is scaleable to very large problem when tested on several benchma rks and compared with a simple k nn method the cmm classifier gave le than lower accuracy and over and time speed up in softwa re and hardware respectively 
we propose a method for compiling propositional theory into a new tractable form that we refer to a decomposable negation normal form dnnf we show a number of result about our compilation approach first we show that every propositional theory can be compiled into dnnf and present an algorithm to this effect second we show that if a clausal form ha a bounded treewidth then it dnnf compilation ha a linear size and can be computed in linear time treewidth is a graphtheoretic parameter which measure the connectivity of the clausal form third we show that once a propositional theory is compiled into dnnf a number of reasoning task such a satisfiability and forgetting can be performed in linear time finally we propose two technique for approximating the dnnf compilation of a theory when the size of such compilation is too large to be practical one of the technique generates a sound but incomplete compilation while the other generates a complete but unsound compilation together these approximation bound the exact compilation from below and above in term for their ability to answer query 
we propose a formal system for representing the available reading of sentence displaying quantifier scope ambiguity in which partial scope may be expressed we show that using a theory of scope availability based upon the function argument structure of a sentence allows a deterministic polynomial time test for the availability of a reading while solving the same problem within theory based on the well formedness of sentence in the meaning language ha been shown to be np hard 
we introduce a novel algorithm termed ppa performance prediction algorithm that quantitatively measure the contributio n of element of a neural system to the task it performs the algorithm ide ntifies the neuron or area which participate in a cognitive or behavio ral task given data about performance decrease in a small set of lesion it also allows the accurate prediction of performance due to multi element lesion the effectiveness of the new algorithm is demonstrated in two model of recurrent neural network with complex interactionsamong the element the algorithm is scalable and applicable to the analysis of large neural network given the recent advance in reversible inactivation technique it ha the potential to significantly contribute to the unde rstanding of the organization of biological nervous system and to shed light on the long lasting debate about local versus distrib uted computation in the brain 
shop simple hierarchical ordered planner is a domain independent htn planning system with the following characteristic shop plan for task in the same order that they will later be executed this avoids some goal interaction issue that arise in other htn planner so that the planning algorithm is relatively simple since shop know the complete world state at each step of the planning process it can use highly expressive domain representation for example it can do planning problem that require complex numeric computation in our test shop wa several order of magnitude faster man blackbox and several time faster than tlpian even though shop is coded in lisp and the other planner are coded in c 
we describe a theoretically optimal algorithm for computing the homography between two image in relation to image mosaicing application first we derive a theoretical accuracy bound based on a mathematical model of image noise and do simulation to confirm that our renormalization technique effectively attains that bound our algorithm is optimal in that sense then we apply our technique to mosaicing of image with small overlap by using real image we show how our algorithm reduces the instability of the image mapping 
ontology are used in agent oriented software development information system and expert system in order to support interoperability declarativity and intelligent service in the frodo project we design a scalable agent based middleware for distributed organizational memory om in this paper we investigate which ontology related service should be provided a middleware component to this end we discus three basic dimension to characterize stored information that determine the concrete specification of ontology based system formality stability and sharing scope a short discussion of technique which are suited to find a balance on these dimension lead to a characterization of role of ontology related actor in the om scenario which are described with respect to their goal knowledge competency right and obligation these actor class and the related competency are candidate to define agent type speech act and standard service in the envisioned om middleware 
while real scene produce a wide range of brightness variation vision system use low dynamic range image detector that typically provide bit of brightness data at each pixel the resulting low quality image greatly limit what vision can accomplish today this paper proposes a very simple method for significantly enhancing the dynamic range of virtually any imaging system the basic principle is to simultaneously sample the spatial and exposure dimension of image irradiance one of several way to achieve this is by placing an optical mask adjacent to a conventional image detector array the mask ha a pattern with spatially varying transmittance thereby giving adjacent pixel on the detector different exposure to the scene the captured image is mapped to a high dynamic range image using an efficient image reconstruction algorithm the end result is an imaging system that can measure a very wide range of scene radiance and produce a substantially larger number of brightness level with a slight reduction in spatial resolution we conclude with several example of high dynamic range image computed using spatially varying pixel exposure 
the paper ha two main contribution the first is a set of method for computing structure and motion for m view of point it is shown that a geometric image error can be minimized over all view by a simple three parameter numerical optimization then that an algebraic image error can be minimized over all view by computing the solution to a cubic in one variable finally a minor point is that this quasi linear linear solution enables a more concise algorithm than any given previously for the reconstruction of point in view the second contribution is an m view n point robust reconstruction algorithm which us the point method a a search engine this extends the successful ransac based algorithm for view and view to m view the algorithm can cope with missing data and mismatched data and may be used a an efficient initializer for bundle adjustment the new algorithm are evaluated on synthetic and real image sequence and compared to optimal estimation result bundle adjustment 
this paper address the question whatis the outcome of multi agent learningvia no regret algorithm in repeatedgames specically can the outcomeof no regret learning be characterizedby traditional game theoreticsolution concept such a nash equilibrium the conclusion of this studyis that no regret learning is reminiscentof ctitious play play convergesto nash equilibrium in dominancesolvable constant sum and generalsum game but cycle exponentiallyin the 
a new method for tracking contour of moving object in clutter is presented for a given object a model of it contour is learned from training data in the form of a subset of contour space greater complexity is added to the contour model by analyzing rigid and non rigid transformation of contour separately in the course of tracking multiple contour may be observed due to the presence of extraneous edge in the form of clutter the learned model guide the algorithm in picking out the correct one the algorithm which is posed a a solution to a minimization problem is made efficient by the use of several iterative scheme result applying the proposed algorithm to the tracking of a fle xing fing er and to a conversing individual s lip are presented describes the evolution of the contour to be tracked assuming that the observation of the contour ha been corrupted by gaussian noise the conditional density of the contour given all past observation may be found and then used to estimate the contour position the condensation tracker also assumes a dynamical model describing contour motion is known and that imprecise observation are made however both the dynamical system and the observation process may be completely general and the conditional density may be propagated forward in time using the numerical technique known a the condensation method this density may then be used for estimating the current contour 
while previous work suggests that multiple goal can be addressed by a nominal expression there is no systematic work describing what goal in addition to identification might be relevant and how speaker can use nominal expression to achieve them in this paper we first hypothesize a number of communicative goal that could be addressed by nominal expression in task oriented dialogue we then describe the intentional influence model for nominal expression generation that attempt to simultaneously address the identification goal and these additional goal with a single nominal expression our evaluation result show that the intentional influence model fit the nominal expression in the coconut corpus a well a previous account that focus solely on the identification goal 
most web page are linked to others with related content this idea combined with another that say that text in and possibly around html anchor describe the page to which they point is the foundation for a usable world wide web in this paper we examine to what extent these idea hold by empirically testing whether topical locality mirror spatial locality of page on the web in particular we find that the likelihood of linked page having similar textual content to be high the similarity of sibling page increase when the link from the parent are close together title description and anchor text represent at least part of the target page and that anchor text may be a useful discriminator among unseen child page these result show the foundation necessary for the success of many web system including search engine focused crawler linkage analyzer and intelligent web agent 
human learn strategy for visual discrimination through interaction with their environment discrimination skill s are refined a demanded by the task at hand and are not a priori determined by any particular feature set task are typically incompletely specified and evolve continually this work present a general framework for learning visual discrimination that address some of these characteristic it is based on an infinite combinatorial feature space consisting of primitive feature such a oriented edgels and textur e signature and composition thereof feature are progre ssively sampled from this space in a simple to complex manner a simple recognition procedure query learned featur e one by one and rule out candidate object class that do not sufficiently exhibit the queried feature training image a re presented sequentially to the learning system which incre mentally discovers feature for recognition experimenta l result on two database of geometric object illustrate th e applicability of the framework 
we propose a novel clustering method that is an extension of idea inherent to scale space clustering and support vector clusteri ng like the latter it associate every data point with a vector in hilbert s pace and like the former it put emphasis on their total sum that is equal t o the scalespace probability function the novelty of our approach is t he study of an operator in hilbert space represented by the schroding er equation of which the probability function is a solution this schrodi nger equation contains a potential function that can be derived analytica lly from the probability function we associate minimum of the potential with cluster center the method ha one variable parameter the scale of it gaussian kernel we demonstrate it applicability on known data set by limiting the evaluation of the schrodinger potential to the locatio n of data point we can apply this method to problem in high dimension 
motivated by the problem of query answering over multiple structured commonsense theory we exploit graph based technique to improve the efficiency of theorem proving for structured theory theory are organized into subtheories that are minimally connected by the literal they share we presentmessage passing algorithm that reason over these theory using consequence finding specializing our algorithm for the case of first order resolution and for batch and concurrent theorem proving we provide an algorithm that restricts the interaction between subtheories by exploiting the polarity of literal we attempt to minimize the reasoning within each individual partition by exploiting existing algorithm for focused incremental and general consequence finding finally we propose an algorithm that compiles each subtheory into one in a reduced sublanguage we have proven the soundness and completeness of all of these algorithm 
we present a neural network model that show how the prefrontalcortex interacting with the basal ganglion can maintain a sequenceof phonological information in activation based working memory i e the phonological loop the primary function of this phonologicalloop may be to transiently encode arbitrary binding ofinformation necessary for task the combinatorial expressivepower of language enables very exible binding of essentially arbitrarypieces of information our model 
many machine learning method have recently been applied to natural language processing task among them the winnow algorithm ha been argued to be particularly suitable for nlp problem due to it robustness to irrelevant feature however in theory winnow may not converge for non separable data to remedy this problem a modification called regularized winnow ha been proposed in this paper we apply this new method to text chunking we show that this method achieves state of the art performance with significantly le computation than previous approach 
this paper describes extensionsto commandtalk to support spoken dialogue while we make no theoretical claim about thenature and structure of dialogue we are influencedby the theoretical work of grosz andsidner and will use terminology fromthat tradition when appropriate we also follow chu carroll and brown in distinguishing task initiative and dialogue initiative 
we consider the problem of how an agent creates a discrete spatial representation from it continuous interaction with the environment such representation will be the minimal one that explains the experience of the agent in the environment in this 
the concave convex procedure cccp is a way to construct discrete time iterative dynamical system which are guaranteed to monotonically decrease global optimization energy function this procedure can be applied to almost any optimization problem and many existing algorithm can be interpreted in term of 
detecting human in image is a useful application of computer vision loose and textured clothing occlusion and scene clutter make it a difficult problem because bottom up segmentation and grouping do not always work we address the problem of detecting human from their motion pattern in monocular image sequence extraneous motion and occlusion may be present we assume that we may not rely on segmentation or grouping and that the vision front end is limited to observing the motion of key point and textured patch in between pair of frame we do not assume that we are able to track feature for more than two frame our method is based on learning an approximate probabilistic model of the joint position and velocity of different body feature detection is performed by hypothesis testing on the maximum a posteriori estimate of the pose and motion of the body our experiment on a dozen of walking sequence indicate that our algorithm is accurate and efficient 
we perform a comprehensive theoretical and experimental analysis of the use of all different constraint we prove that generalized arc consistency on such constraint lie between neighborhood inverse consistency and under a simple restriction path inverse consistency on the binary representation of the problem by generalizing the argument of kondrak and van beek we prove that a search algorithm that maintains generalized arc consistency on all different constraint dominates a search algorithm that maintains arc consistency on the binary representation our experiment show the practical value of achieving these high level of consistency for example we can solve almost all benchmark quasigroup completion problem up to order with just a few branch of search these result demonstrate the benefit of using non binary constraint like all different to identify structure in problem 
effective method of capacity control via uniform convergence bound for function expansion have been largely limited to support vector machine where good bound are obtainable by the entropy number approach we extend these method to system with expansion in term of arbitrary parametrized basis function and a wide range of regularization method covering the whole range of general linear additive model this is achieved by a data dependent analysis of the eigenvalue of the corresponding design matrix 
abstract we show that it is possible to extend hidden markov model to havea countably infinite number of hidden state by using the theory ofdirichlet process we can implicitly integrate out the infinitely manytransition parameter leaving only three hyperparameters which can belearned from data these three hyperparameters define a hierarchicaldirichlet process capable of capturing a rich set of transition dynamic 
in a bayesian mixture model it is not necessary a priori to limit the number of component to be finite in this paper an infinite gaussian mixture model is presented which neatly sidestep the difficult problem of finding the right number of mixture component inference in the model is done using an efficient parameter free markov chain that relies entirely on gibbs sampling 
the performance of regular and irregular gallager type errorcorrectingcode is investigated via method of statistical physic the transmitted codeword comprises product of the original messagebits selected by two randomly constructed sparse matrix the number of non zero row column element in these matricesconstitutes a family of code we show that shannon s channelcapacity may be saturated in equilibrium for many of the regularcodes while slightly lower performance is 
an important class of problem can be cast a inference in noisyorbayesian network where the binary state of each variable isa logical or of noisy version of the state of the variable s parent for example in medical diagnosis the presence of a symptomcan be expressed a a noisy or of the disease that may cause thesymptom on some occasion a disease may fail to activate thesymptom inference in richly connected noisy or network is intractable but approximate method 
recent research ha addressed the problem of planning in non deterministic domain classical planning ha also been extended to the case of goal that can express temporal property however the combination of these two aspect is not trivial in non deterministic domain goal should take into account the fact that a plan may result in many possible different execution and that some requirement can be enforced on all the possible execution while others may be enforced only on some execution in this paper we address this problem we define a planning algorithm that generates automatically plan for extended goal in nondeterministic domain we also provide preliminary experimental result based on an implementation of the planning algorithm that us symbolic model checking technique 
we review our work on how to teach deformable model to maximize image segmentation correctness based on user specified criterion we then present new variant and application of learned snake modeled by four different probability density function pdfs at three scale and in the two very different medical domain of abdominalct slice and echocardiogram we review and extend our method for evaluating which criterion work best success depends on the relation of objective function the pdf output to shape correctness this relationship for all the above learned snake variant and domain is evaluated on perturbed ground truth shape in three way by the incidence of false positive scoring better than ground truth of randomized shape by the monotonicity of the objective function versus shape closeness to ground truth a given by a correlation coefficient and by the distance of this relationship to the nearest monotonicallyincreasing function a new performance measure which we introduce here we exhaustively demonstrate such evaluation on traditional snake and on snake for which image intensity and perpendicular gradient are learned separately and with their covariance and with separate learning over equallength sector optimal blur appears to depend on domain both sectoring and the use of covariance markedly improve result in abdominalct image where nearby image landmark i e organ stabilize learning result on echocardiogram however are le striking although the use of covariance doe show improvement on investigation this appears due to the non gaussian distribution of image feature in this domain 
perceptual consistency is important in many computer vision application unfortunately except for color computational feature and similarity measurement for other visual feature are not necessarily consistent with human s perception this paper address three critical issue regarding perceptually consistent texture analysis development of perceptual texture space assessment of how consistent computational feature are to human perception and mapping computational feature to perceptual space it demonstrates the construction of a reliable perceptual texture space which can be used a a yardstick for assessing the perceptual consistency of computational feature and similarity measurement moreover it is found that commonly used computational texture feature are not very consistent with human perception and mapping them to the perceptual space improves their perceptual consistency 
much of the recent research in object recognition hasadopted an appearance based scheme wherein objectsto be recognized are represented a a collection of prototypesin a multidimensional space spanned by a numberof characteristic vector eigen image obtainedfrom training view in this paper we extend theappearance based recognition scheme to handle range shape data the result of training is a set of eigensurfaces that capture the gross shape of the object these technique 
we discus technology to help a person monitor change in news coverage over time we define temporal summary of news story a extracting a single sentence from each event within a news topic where the story are presented one at a time and sentence from a story must be ranked before the next story can be considered we explain a method for evaluation and describe an evaluation corpus that we have built we also propose several method for constructing temporal summary and evaluate their effectiveness in comparison to degenerate case we show that simple approach are effective but that the problem is far from solved 
herein we present a variational model devoted to image classification coupled with an edge preserving regularization process in the last decade the variational approach ha proven it efficiency in the field of edge preserving restoration in this paper we add a classification capability which contributes to provide image compound of homogeneous region with regularized boundary the soundness of this model is based on the work developed on the phase transition theory in mechanic the 
accurate nominal compound analysis is crucial for in application of natural language processing such a information retrieval and extraction a well a nominal compound interpretation i n the nominal compound analysis area some corpus based approach have reported successful result by using statistal cooccurrences of noun but a nominal compound often ha the similar structure to a simple sentence e g the complement predicate structure a well a representing compound meaning with several noun combined due to the grammarical characteristic of nominal compound the fi amework based only on statistcal association between noun often fails to analyze their structure accurately especially in korean this pcper present a new model for korean nominal compound analysis on the basis of linguistic and statistical knowledge the syntactic relation often have an effect on determining the structure of nominal compound and we analyzed million word corpus in order to acquire syntactic and s tatistical knowledge the structure of a nominal compound is analyzed based on the linguistic lexical information extracted by experiment it is shown that our method is effective for accurate analysis of korean nominal compound 
the ability to identify the mineral composition of rock and soil is an important tool for the exploration of geological site for instance nasa intends to design robot that are sufficiently autonomous to perform this task on planetary mission spectrometer reading provide one important source of data for identifying site with mineral of interest reflectance spectrometer measure intensity of light reflected from surface over a range of wavelength spectral intensity pattern may in some case be sufficiently distinctive for proper identification of mineral or class of mineral for some mineral class carbonate for example specific short spectral interval are known to carry a distinctive signature finding similar distinctive spectral range for other mineral class is not an easy problem we propose and evaluate data driven technique that automatically search for spectral range optimized for specific mineral in one set of study we partition the whole interval of wavelength available in our data into sub interval or bin and use a genetic algorithm to evaluate a candidate selection of subintervals a alternative to this computationally expensive search technique we present an entropy based heuristic that give higher score for wavelength more likely to distinguish between class a well a other greedy search procedure result are presented for four different class showing reasonable improvement in identifying some but not all of the mineral class tested 
in recent year bayesian network have become highly successful tool for diagnosis analysis and decision making in real world domain we present an efficient algorithm for learning bayes network from data our approach construct bayesian network by first identifying each node s markov blanket then connecting node in a maximally consistent way in contrast to the majority of work which typically us hill climbing approach that may produce dense and causally incorrect net our approach yield much more compact causal network by heeding independency in the data compact causal network facilitate fast inference and are also easier to understand we prove that under mild assumption our approach requires time polynomial in the size of the data and the number of node a randomized variant also presented here yield comparable result at much higher speed 
we consider the problem of reconstructing the d coordinate of a moving point seen from a monocular moving camera i e to reconstruct moving object from line of sight measurement only the task is feasible only when some constraint are placed on the shape of the trajectory of the moving point we coin the family of such task a quot trajectory triangulation quot in this paper we focus on trajectory whose shape is a conic section and show that generally view are sufficient for a unique 
consider two view of a multi body scene consisting of planar body moving in pure translation one relative to the other we show that the fundamental matrix one per body live in a dimensional subspace which when represented a a step extensor is the common transversal on the collection of extensor defined by the homography matrix of the moving plane we show that a much a five body are necessary for recovering the common transversal from the homography matrix from which we show how to recover the fundamental matrix and the affine calibration between the two camera 
this paper empirically compare nine image dissimilarity measure that are based on distribution of color and texture feature summarizing over cpu hour of computational experiment ground truth is collected via a novel random sampling scheme for color and via an image partitioning method for texture quantitative performance evaluation are given for classification image retrieval and segmentation task and for a wide variety of dissimilarity measure it is demonstrated how the selection of a measure based on large scale evaluation substantially improves the quality of classification retrieval and unsupervised segmentation of color and texture image 
in this paper we follow the same general ideology a in gammerman et al and describe a new transductive learning algorithm using support vector machine the algorithm presented provides confidence value for it predicted classification of new example we also obtain a measure of credibility which serf a an indicator of the reliability of the data upon which we make our prediction experiment compare the new algorithm to a standard support vector machine and other transductive method which use support vector machine such a vapnik s margin transduction empirical result show that the new algorithm not only produce confidence and credibility measure but is comparable to and sometimes exceeds the performance of the other algorithm 
intrinsic image are a useful midlevel description of scene proposed by barrow and tenenbaum an image is decomposed into two image a reflectance image and an illumination image finding such a decomposition remains a difficult problem in computer vision here we focus on a slightly easier problem given a sequence of image where the reflectance is constant and the illumination change can we recover illumination image and a single reflectance image we show that this problem is still illposed and suggest approaching it a a maximum likelihood estimation problem following recent work on the statistic of natural image we use a prior that assumes that illumination image will give rise to sparse filter output we show that this lead to a simple novel algorithm for recovering reflectance image we illustrate the algorithm s performanceon real and synthetic image sequence in proc iccv 
we present a unified computational framework which properly implement the smoothness constraint to generate description in term of surface region curve and labelled junction from sparse noisy binary data in d or d each input site can be a point a point with an associated tangent direction a point with an associated normal direction or any combination of the above the methodology is grounded on two element tensor calculus for representation and linear voting for communication each input site communicates it information a tensor to it neighborhood through a predefined tensor field and therefore cast a tensor vote each site collect all the vote cast at it location and encodes them into a new tensor a local parallel marching process then simultaneously detects feature the proposed approach is very different from traditional variational approach a it is non iterative furthermore the only free parameter is the size of the neighborhood related to the scale we have developed several algorithm based on the proposed methodology to address a number of early vision problem including perceptual grouping in d and d shape from stereo and motion grouping and segmentation and the result are very encouraging 
principal component analysis pca approach to face recognition are data dependent and computationally expensive to classify unknown face they need to match the nearest neighbour in the stored database of extracted face feature in this paper discrete cosine transforms dcts are used to reduce the dimensionality of face space by truncating high frequency dct component the remaining coefficient are fed into a neural network for classification because only a small number of low 
this paper present a novel approach for reconstructing free form texture mapped d scene model from a single painting or photograph given a sparse set of user specified constraint on the local shape of the scene a smooth d surface that satisfies the constraint is generated this problem is formulated a a constrained variational optimization problem in contrast to previous work in single view reconstruction our technique enables high quality reconstruction of free form curved surface with arbitrary reflectance property a key feature of the approach is a novel hierarchical transformation technique for accelerating convergence on a non uniform piecewise continuous grid the technique is interactive and update the model in real time a constraint are added allowing fast reconstruction of photorealistic scene model the approach is shown to yield high quality result on a large variety of image 
an automated method for left ventricle detection in mr cardiac image is presented ventricle detection is the first step in a fully automated segmentation system used to compute volumetric information about the heart our method is based on learning the gray level appearance of the ventricle by maximizing the discrimination between positive and negative example in a training set the main difference from previously reported method are feature definition and solution to the optimization problem involved in the learning process our method wa trained on a set of mr cardiac image from which positive example and negative example were generated the detection result on a test set of different image demonstrate an excellent performance detection rate a false alarm rate of of the number of window analyzed false alarm per image and a detection time of second per image on a sun ultra for an scale search the false alarm are eventually eliminated by a position scale consistency check along all the image that represent the same anatomical slice 
reliable detection and tracking of eye is an important requirement for attentive user interface in this paper we present a methodology for detecting eye robustly in indoor environment in real time we exploit the physiological property and appearance of eye a well a head eye motion dynamic structured infrared lighting is used to capture the physiological property of eye kalman tracker are used to model eye head dynamic and a probabilistic based appearance model is used to represent eye appearance by combining three separate modality with specific enhancement within each modality our approach allows eye to be treated a robust feature that can be used for other higher level processing 
we present a stochastic clustering algorithm which us pairwise similarity of element based on a new graph theoretical algorithm for the sampling of cut in graph the stochastic nature of our method make it robust against noise including accidental edge and small spurious cluster we demonstrate the robustness and superiority of our method for image segmentation on a few synthetic example where other recently proposed method such a normalized cut fail in addition the complexity of our method is lower we describe experiment with real image showing good segmentation result 
a novel approach for comparing sequence of observation using an explicit expansion kernel is demonstrated the kernel is derived using the assumption of the independence of the sequence of observation and a mean squared error training criterion the use of an explicit expansion kernel reduces classifier model size and computation dramatically resulting in model size and computation one hundred time smaller in our application the explicit expansion also preserve the computational advantage of an earlier architecture based on mean squared error training training using standard support vector machine methodology give accuracy that significantly exceeds the performance of state of the art mean squared error training for a speaker recognition task 
optical flow estimation in noisy image sequence requires a special denoising strategy towards this end we introduce a new tensor driven anisotropic diffusion scheme which is designed to enhance optical flow like spatiotemporal structure this is achieved by selecting diffusivities in a special manner depending on the eigenvalue of the well known structure tensor we illustrate how the proposed choice differs from edgeand coherence enhancing anisotropic diffusion furthermore we extend a recently discovered discretization scheme for anisotropic diffusion to d data an automatic stop criterion to terminate the diffusion after a suitable time is given the performance of the introduced method is examined quantitatively using image sequence with a substantial amount of noise added 
bayesian network are graphical representation of probability distribution in virtually all of the work on learning these network the assumption is that we are presented with a data set consisting of randomly generated instance from the underlying distribution in many situation however we also have the option of active learning where we have the possibility of guiding the sampling process by querying for certain type of sample this paper address the problem of estimating the parameter of bayesian network in an active learning setting we provide a theoretical framework for this problem and an algorithm that chooses which active learning query to generate based on the model learned so far we present experimental result showing that our active learning algorithm can significantly reduce the need for training data in many situation 
parameter tuning through cross validation becomes very difficult when the validation set contains no or only a few example of the class in the evaluation set we address this open challenge by using a combination of classifier with different performance characteristic to effectively reduce the performance variance on average of the overall system across all class including those not seen before this approach allows u to tune the combination system on available but le representative validation data and obtain smaller performance degradation of this system on the evaluation data than using a single method classifier alone we tested this approach by applying k nearest neighbor rocchio and language modeling classifier and their combination to the event tracking problem in the topic detection and tracking tdt domain where new class event are created constantly over time and representative validation set for new class are often difficult to obtain on time when parameter tuned on an early benchmark tdt corpus were evaluated on a later tdt benchmark corpus with no overlapping event we observed a reduction in tracking cost a weighted combination of error by the combined system over the individual method evaluated under the same condition strongly suggesting the robustness of this approach a a solution for improving cross class performance consistency of statistical classifier when standard cross validation fails due to the lack of representative validation set 
the surface matching problem is investigated in this paper using a mathematical tool called harmonic map the theory of harmonic map study the mapping between different metric manifold from the energyminimization point of view with the application of harmonic map a surface representation called harmonic shape image is generated to represent and match d freeform surface the basic idea of harmonic shape image is to map a d surface patch with disc topology to a d domain and encode the shape information of the surface patch into the d image this simplifies the surface matching problem to a d image matching problem due to the application of harmonic map in generating harmonic shape image harmonic shape image have the following advantage they have sound mathematical background they preserve both the shape and continuity of the underlying surface and they are robust to occlusion and independent of any specific surface sampling scheme the performance of surface matching using harmonic map is evaluated using real data preliminary result are presented in the paper 
the thresholding of document score ha proved critical for the effectiveness of classification task we review the most important approach to thresholding and introduce thescore distributional s d threshold optimizationmethod the method is based on score distribution and is capable of optimizing any effectiveness measure defined in term of the traditional contingency table a a byproduct we provide a model forscore distribution and demonstrate it high accuracy in describing empirical data the estimation method can be performed incrementally a highly desirable feature for adaptive environment our work in modeling score distribution is useful beyond threshold optimization problem it directly applies to other retrieval environment that make use of score distribution e g distributed retrieval or topic detection and tracking the most accurate version of s d thresholding although incremental can be computationally heavy therefore we also investigate more practical solution we suggest practical approximation and discus adaptivity threshold initialization and incrementality issue the practical version of s d thresholding ha been tested in the context of the trec filtering track and found to be very effective 
the critical configuration for projective reconstruction from three view are discussed a set of camera and point is said to be critical if the projected image point are insufficient to determine the placement of the point and camera uniquely up to projective transformation for two view the classification of critical configuration is well known the configuration is critical if and only if the point and camera centre all lie on a ruled quadric for three view the critical configuration have not been identified previously in this paper it is shown that for any placement of three given camera there always exists a critical set consisting of a fourth degree curve any number of point on the curve form a critical set for the three camera dual to this result for a set of seven point there exists a fourth degree curve such that a configuration of any number of camera placed on this curve is critical for the set of point other critical configuration exist in case where the point all lie in a plane or one of the camera lie on a twisted cubic 
we present a general algorithm for plane based calibration that can deal with arbitrary number of view and calibration plane the algorithm can simultaneously calibrate different view from a camera with variable intrinsic parameter and it is easy to incorporate known value of intrinsic parameter for some minimal case we describe all singularity naming the parameter that can not be estimated experimental result of our method are shown that exhibit the singularity while revealing good performance in non singular condition several application of plane based d geometry inference are discussed a well in this paper we propose a general algorithm for calibrating a camera with possibly variable intrinsic parameter and position that cope well with an arbitrary number of calibration plane and camera view calibration is essentially done in two step first the d to d projection of planar calibration object onto the image plane s are computed each of these projection contributes to a system of homogeneous linear equation in the intrinsic parameter which are hence easily determined calibration can thus be achieved by solving linear equation but can of course be enhanced by subsequent non linear optimization in we describe our camera model and projection of planar object in we introduce the principle of plane based calibration a general algorithm is proposed in singularity are revealed in experimental result are presented in and some application described in 
we present a sample re weighting scheme inspired by recent result in margin theory the basic idea is to add to the training set replica of sample which are not classified with a sufficient margin we prove the convergence of the input distribution obtained in this way a study case we consider an instance of the scheme involving a nn classifier implementing a vector quantization algorithm that accommodates tangent distance model the tangent distance model created in this way have shown a significant improvement in generalization power with respect to the standard tangent model more over the obtained model were able to outperform state of the art algorithm such a svm 
a system emulating the functionality of a moving eye hence the name oculo motor system ha been built and successfully tested it is made of an optical device for shifting the field of view of an image sensor by up to in any direction four neuromorphic analog vlsi circuit implementing an oculo motor control loop and some off the shelf electronics the custom integrated circuit communicate with each other primarily by non arbitrated address event bus the system implement the behavior of saliency based saccadic exploration a ndsmooth pursuit of light spot the duration of saccade range from m to m which is comparable to human eye performance smooth pursuit operates on light source moving at up to s in the visual field 
facesync is an optimal linear algorithm that find the degree of synchronization between the audio and image recording of a human speaker using canonical correlation it find the best direction to combine all the audio and image data projecting them onto a single axis facesync us pearson s correlation to measure the degree of synchronization between the audio and image data we derive the optimal linear transform to combine the audio and visual information and describe an implementation that avoids the numerical problem caused by computing the correlation matrix motivation in many application we want to know about the synchronization between an audio signal and the corresponding image data in a teleconferencing system we might want to know which of the several people imaged by a camera is heard by the microphone then we can direct the camera to the speaker in post production for a film clean audio dialog is often dubbed over the video we want to adjust the audio signal so that the lip sync is perfect when analyzing a film we want to know when the person talking is in the shot instead of off camera when evaluating the quality of dubbed film we can measure of how well the translated word and audio fit the actor s face this paper describes an algorithm facesync that measure the degree of synchronization between the video image of a face and the associated audio signal we can do this task by synthesizing the talking face using technique such a video rewrite and then comparing the synthesized video with the test video that process however is expensive our solution find a linear operator that when applied to the audio and video signal generates an audio video synchronization error signal the linear operator gather information from throughout the image and thus allows u to do the computation inexpensively hershey and movellan describe an approach based on measuring the mutual information between the audio signal and individual pixel in the video the correlation between the audio signal x and one pixel in the image y is given by pearson s correlation r the mutual information between these two variable is given by i x y log r they create movie that show the region of the video that have high correlation with the audio 
generalized vickrey mechanism have received wide attention in the literature because they are efficient and strategy proof i e truthful bidding is optimal whatever the bid of other agent however it is well known that it is impossible for an exchange with multiple buyer and seller to be efficient and budget balanced even putting strategy proofness to one side a market maker in an efficient exchange must make more payment than it collect we enforce budget balance a a hard constraint and explore payment rule to distribute surplus after an exchange clear to minimize distance to vickrey payment different rule lead to different level of truth revelation and efficiency experimental and theoretical analysis suggest a simple threshold scheme which give surplus to agent with payment further than a certain threshold value from their vickrey payment the scheme appears able to exploit agent uncertainty about bid from other agent to reduce manipulation and boost allocative efficiency in comparison with other simple rule 
recent work ha shown impressive transform invariant modeling and clustering for set of image of object with similar appearance we seek to expand these capability to set of image of an object class that show considerable variation across individual instance e g pedestrian image using a representation based on pixel wise similarity similarity template because of it invariance to the color of particular component of an object this representation enables detection of instance of an object class and enables alignment of those instance further this model implicitly represents the region of color regularity in the class specic image set enabling a decomposition of that object class into component region 
this paper present a case study of analyzing and improving intercoder reliability in discourse tagging using statistical technique bias corrected tag are formulated and successfully used to guide a revision of the coding manual and develop an automatic classifier 
we present a statistical model of japanese unknown word consisting of a set of length and spelling model classified by the character type that constitute a word the point is quite simple different character set should be treated differently and the change between character type are very important because japanese script ha both ideogram like chinese kanji and phonogram like english katakana both word segmentation accuracy and part of speech tagging accuracy are improved by the proposed model the model can achieve tagging accuracy if unknown word are correctly segmented 
a problem of using mixture of gaussian model for unsupervised texture segmentation is that multimodal texture such a can often be encountered in natural image cannot be well represented by a single gaussian cluster we propose a divide andconquer method that group together gaussian cluster estimated via expectation maximization into homogeneous texture class this method allows to succesfully segment even rather complex texture a demonstrated by experimental test on natural image 
this paper describes a new method for determiningcorrespondence between point on pair of surfacesbased on shape using a combination of geodesic distanceand surface curvature an initial sparse setof corresponding point are generated using a shapebasedmatching procedure geodesic interpolation isemployed in order to capture the complex surface inaddition surface correspondence and triangulation arecomputed simultaneously in a hierarchical way resultsapplied to human cerebral 
this paper describes an algorithm for generating compact d model of indoor environment with mobile robot our algorithm employ the expectation maximization algorithm to fit a lowcomplexity planar model to d data collected by range finder and a panoramic camera the complexity of the model is determined during model fitting by incrementally adding and removing surface in a final post processing step measurement are converted into polygon and projected onto the surface model where possible empirical result obtained with a mobile robot illustrate that high resolution model can be acquired in reasonable time 
a fundamental function of any task oriented dialogue system is the ability to generate nominal expression that describe object in the task domain in this paper we report result from using machine learning to train and test a nominal expression generator on a set of nominal description from the coconut corpus of task oriented design dialogue result show that we can achieve a match to human performance a opposed to a baseline for just guessing the most frequent type of nominal expression in the coconut corpus to our surprise our result indicate that many of the central feature of previously proposed selection model did not improve the performance of the learned nominal expression generator 
let u consider the following problem given a probably huge set of set s and a query set q is there some set s in s such that s subseteq q this problem occurs in at least three application area the matching of a large number usually several s of production rule the processing of query in data base supporting set valued attribute and the identification of inconsistent subgoals during artificial intelligence planning in this paper we introduce a data structure and algorithm that allow a compact representation of such a huge set of set and an efficient answering of em subset and em superset query 
summarization research is notorious for it lack of adequat e corpus today there exist only a few small collection of text whose unit have been manually annotated for textual importance given the cost and tediousness of the annotation process it is very unlikely that we will ever manually annotate for textual importance sufficiently large corpus of text to circumvent this problem we have developed an algorithm that construct such corpus automatically our algorithm take a input an abstract text tuple and generates the corresponding extract i e the set of clause sentence in the text that were used to write the abstract the performance of the algorithm is shown to be close to that of human by mean of an empirical experiment the experiment also suggests extraction strategy that could impro ve the performance of automatic summarization system 
in order to generate high quality explanation in mathematical domain the presentation must be adapted to the knowledge of the intended audience most proof presentation system only communicate proof on a fixed degree of abstraction independently of the addressee s knowledge in this paper we shall present the proof explanation system p rex based on assumption about the addressee s knowledge it dialog planner chooses a degree of abstraction for each proof step to be explained in reaction to the user s interaction which are allowed at any time it enters clarification dialog to revise it user model and to adapt the explanation 
in this paper we propose an invariant signature representation for appearance of d object under varying view and illumination and a method for learning the signature from multi view appearance example the signature a nonlinear feature provides a good basis for d object detection and pose estimation due to it following property it location in the signature feature space is a simple function of the view and is insensitive or invariant to illumination it change continuously a the view change so that the object appearance at all possible view should constitute a known simple curve segment manifold in the feature space the coordinate of the object appearance in the feature space are correlated in a known way according to a predefined function of the view the first two property provide a basis for object detection and the third for view pose estimation to compute the signature representation from input we present a nonlinear regression method for learning a nonlinear mapping from the input e g image space to the feature space the idea of the signature representation and the learning method are illustrated with experimental result for the object of human face it is shown that the face object can be effectively modeled compactly in a d nonlinear feature space the d signature present excellent insensitivity to change in illumination for any view the correlation of the signature coordinate is well determined by the predefined parametric function application of the proposed method in face detection and pose estimation are demonstrated 
we present a bayesian recognition framework in which a model of the whole face is enhanced by model of facial feature position and appearance face recognition and facial expression recognition are carried out using maximum likelihood decision the algorithm find the model and facial expression that maximizes the likelihood of a test image in this framework facial appearance matching is improved by facial expression matching also change in facial feature due to expression are used 
much system oriented evaluation of information retrieval system ha used the cranfield approach based upon query run against test collection in a batch mode some researcher have questioned whether this approach can be applied to the real world but little data exists for or against that assertion we have studied this question in the context of the trec interactive track previous result demonstrated that improved performance a measured by relevance based metric in batch study did not correspond with the result of outcome based on real user searching task the experiment in this paper analyzed those result to determine why this occurred our assessment showed that while the query entered by real user into system yielding better result in batch study gave comparable gain in ranking of relevant document for those user they did not translate into better performance on specific task this wa most likely due to user being able to adequately find and utilize relevant document ranked further down the output list 
a new reinforcement learning rl methodology is proposed to design multi agent system in the realistic setting of situated agent with local perception the task of automatically building a coordinated system is of crucial importance we use simple reactive agent which learn their own behavior in a decentralized way to cope with the difficulty inherent to rl used in that framework we have developed an incremental learning algorithm where agent face more and more complex task we illustrate this general framework on a computer experiment where agent have to coordinate to reach a global goal 
the majority of naturally sounding musical performance ha musical expression fluctuation in tempo volume etc musical expression is affected by various factor such a the performer performative style mood and so forth however in past research on the computerized generation of musical expression these factor are treated a being le significant or almost ignored hence the majority of past approach find it relatively hard to generate multiple performance for a given piece of music with varying musical expression in this paper we propose a case based approach to the generation of expressively modulated performance this method enables the generation of varying musical expression for a single piece of music we have implemented the proposed case based method in a musical performance system and we also describe the system architecture and experiment performed on the system 
the product of expert learning procedure can discover a set of stochastic binary feature that constitute a non linear generative model of handwritten image of digit the quality of generative model learned in this way can be assessed by learning a separate model for each class of digit and then comparing the unnormalized probability of test image under the different class specific model to improve discriminative performance each of the digit model can be given more layer of feature detector the layer are trained sequentially and each layer learns a generative model of the pattern of feature activity in the preceding layer after training each layer of feature dectectors produce a separate unnormalized log probabilty score with three layer of feature detector in each of the digit model a test image produce score which can be used a input to a supervised logistic classification network that is trained on separate data on the mnist database our system is comparable with current state of the art discriminative method demonstrating that the product of expert learning procedure can produce effective generative model of high dimensional data 
we present a method for modeling a scene that is observed by a moving camera where only a portion of the scene is visible at any time this method us mixture model to represent pixel in a panoramic view and to construct a background image that contains only static non moving part of the scene the method can be used to reliably detect moving object in a video sequence detect pattern of activity over a wide eld of view and remove moving object from a video or panoramic mosaic the method also yield improved result in detecting moving object and in constructing mosaic in the presence of moving object when compared with technique that are not based on scene modeling we present example illustrating the result 
we report on the successful application of feature selection method to a classification problem in molecular biology involving only data point in a dimensional space our approach is a hybrid of filter and wrapper approach to feature selection we make use of a sequence of simple filter culminating in koller and sahami s markov blanket filter to decide on particular feature subset for each subset cardinality we compare between the resulting subset cardinality using cross validation the paper also investigates regularization method a an alternative to feature selection showing that feature selection method are preferable in this problem 
we present a system that offer a new way of assessing web document relevance and new approach to the web based evaluation of such a system provisionally named webdocsum the system is a query biased web page summariser that aim to provide an alternative to the short irrelevant abstract typical of many web search result list based on an initial evaluation the system appears to be more useful in helping user gauge document relevance than the traditional ranked title abstract approach 
domain knowledge ha been shown to be an important component of machine learning however the cost of obtaining domain knowledge to improve classifier generation can exceed the cost of manually creating classifier an alternative approach is to use existing knowledge source to collect relevant domain knowledge and improve machine learning we investigated the use of two existing knowledge source a natural language processor and controlled vocabulary metathesaurus to improve machine learning algorithm performance in building classifier for medical text report both knowledge source were found to significantly improve classifier performance this demonstrates that existing knowledge source can easily be used to improve machine learning performance 
this paper describes recent progress and the author s perspective of speech recognition technology application of speech recognition technology can be classified into two main area dictation and human computer dialogue system in the dictation domain the automatic broadcast news transcription is now actively investigated especially under the darpa project the broadcast news dictation technology ha recently been integrated with information extraction and retrieval technology and many application system such a automatic voice document indexing and retrieval system are under development in the human computer interaction domain a variety of experimental system for information retrieval through spoken dialogue are being investigated in spite of the remarkable recent progress we are still behind our ultimate goal of understanding free conversational speech uttered by any speaker under any environment this paper also describes the most important research issue that we should attack in order to advance to our ultimate goal of fluent speech recognition 
we describe a robust technique for detecting nonstationaryperiodic motion from a moving and static camera we also describe a robust technique for discriminatingmotion symmetry periodic motion classification whichwe apply to classifying running human biped and canine quadruped the system ha been implemented torun in real time hz on standard pc workstation introductionperiodic and cyclic motion are commonly found in theworld perhaps the most prevalent periodic 
the active selection of instance can significantlyimprove the generalisation performanceof a learning machine large marginclassifiers such a support vector machinesclassify data using the most informative instance the support vector this makesthem natural candidate for instance selectionstrategies in this paper we propose analgorithm for the training of support vectormachines using instance selection wegive a theoretical justification for the strategyand 
we consider the problem of how an agent creates a discrete spatial representation from it continuous interaction with the environment such representation will be the minimal one that explains the experience of the agent in the environment in this 
a key open problem in reinforcement learning is to assure convergence when using a compact hypothesis class to approximate the value function although the standard temporal difference learning algorithm ha been shown to converge when the hypothesis class is a linear combination of fixed basis function it may diverge with a general non linear hypothesis class this paper describes the bridge algorithm a new method for reinforcement learning and show that it converges to an approximate global optimum for any agnostically learnable hypothesis class convergence is demonstrated on a simple example for which temporal difference learning fails weak condition are identified under which the bridge algorithm converges for any hypothesis class finally connection are made between the complexity of reinforcement learning and the pac learnability of the hypothesis class 
we consider the problem of how an agent creates a discrete spatial representation from it continuous interaction with the environment such representation will be the minimal one that explains the experience of the agent in the environment in this 
we present a contrastive analysis of the syntactic realization of possessive and partitive in hebrew and english and conclude by presenting an input specification for complex np which is slightly more abstract than the one used in surge we define two main feature possessor and ref set and discus how the grammar handle complex syntactic co occurrence phenomenon based on this input we conclude by evaluating how the resulting input specification language is appropriate for both language syntactic realization grammar have traditionally attempted to accept input with the highest possible level of abstraction in order to facilitate the work of the component sentence planner preparing the input recently the search for higher abstraction ha been however challenged elhadad and robin lavoie and rambow busemann and horacek in this paper we contribute to the issue of selecting the ideal abstraction level in the input to syntactic realization grammar by considering the case of partitive and possessive in a bilingual hebrew english generation grammar in the case of bilingual generation the ultimate goal is to provide a single input structure where only the openclass lexical entry are specific to the language in that case the minimal abstraction required must cover the different syntactic constraint of the two language 
scalable vector graphic svg is a language that describes two dimensional vector graphic for storage and distribution on the web unlike raster image format svg based image scale nicely to arbitrary resolution and size however the current svg standard provides little fle xibility for taking into account varying viewing condition such a different screen format and there is little support for interactive exploration of a diagram we introduce an extension to svg called constraint scalable vector graphic csvg that permit a more fle xible description of figure with csvg an image can contain object whose position and other property are specified in relation to other object using constraint rather than being specified in absolute term for example a box can be specified to remain inside another box without being given an absolute position the precise layout can then be left to the browser which can adapt it dynamically to changing viewing condition on the client side further extension add support for alternate layout interaction and declarative animation leveraging well established method for linear constraint solving we implemented a prototype viewer for csvg by embedding our cassowary constraint solver into an existing svg renderer 
we present a new algorithm to reduce the space complexity of heuristic search it is most effec tive for problem space that grow polynomially with problem size but contain large number of short cycle for example the problem of finding a lowest cost corner to corner path in a d dimensional grid ha application to gene sequence alignment in computational biology the main idea is to perform a bidirectional search but saving only the open list and not the closed list once the search completes we have one node on an optimal path but don t have the solution path itself the path is then reconstructed by recursively applying the same algorithm between the initial node and the in termediate node and also between the inter mediate node and the goal node if n is the length of the grid in each dimension and d is the number of dimension this algorithm re duce the memory requirement from to the time complexity only increase by a constant factor of in two dimension and in three dimension 
a novel approach to visual servoing is presented which take advantage of the structure of the lie algebraof affine transformation the aim of this projectis to use feedback from a visual sensor to guide arobot arm to a target position the sensor is placedin the end effector of the robot the camera in hand approach and thus provides direct feedback of the robotmotion relative to the target scene via observed transformationsof the scene these scene transformationsare obtained by 
ontology and problem solving method are promising candidate for reuse in knowledge engineering ontology define domain knowledge at a generic level while problem solving method specify generic reasoning knowledge both type of component can be viewed a complementary entity that can be used to configure new knowledge system from existing reusable component in this paper we give an overview of approach for ontology and problem solving method 
we consider the problem of finding neighboring class set object of each instance of a neighboring class set are grouped using their euclidean distance from each other recently location based service are growing along with mobile computing infrastructure such a cellular phone and pda therefore we expect to see the development of spatial database that contains very large number of access record including location information the most typical type would be a database of point object record of the object may consist of requested service name number of packet transmitted in addition to x and y coordinate value indicating where the request came from the algorithm presented here efficiently find set of service name that were frequently close to each other in the spatial database for example it may find a frequent neighboring class set where ticket and timetable are frequently requested close to each other by recognizing this location based service provider can promote a ticket service for customer who access the timetable 
we present a system timemines that automatically generatestimelines from date tagged free text corpus timeminesdetects rank and group semantic feature based ontheir statistical property we use these feature to discoversets of related story that deal with a single topic 
adaboost and other ensemble method have successfully been appliedto a number of classification task seemingly defying problemsof overfitting adaboost performs gradient descent in an errorfunction with respect to the margin asymptotically concentratingon the pattern which are hardest to learn for very noisy problem however this can be disadvantageous indeed theoreticalanalysis ha shown that the margin distribution a opposed to justthe minimal margin play a crucial 
in this paper a memory based parsing method is extended for handling compositional structure the method is oriented for learning to parse any selected subset of target syntactic structure it is local yet can handle also compositional structure part of speech a well a embedded instance are being used simultaneously the output is a partial parse in which instance of the target structure are marked 
the qualification problem refers to the difficulty that arises in formalizing action because it is difficult or impossible to specify in advance all the precondition that should hold before an action can be executed we study the qualification problem in the setting of the situation calculus and give a simple formalization using nested abnormality theory a formalism based on circumscription the formalization that we present allows u to combine a solution to the frame problem with a solution to the qualification problem 
previous algorithm for the generation of referring expression have been developed specifically for this purpose here we introduce an alternative approach based on a fully generic aggregation method also motivated for other generation task we argue that the alternative contributes to a more integrated and uniform approach to content determination in the context of complete noun phrase generation 
extracting rule from rbfs is not a trivial task because of nonlinear function or high input dimensionality in such case some of the hidden unit of the rbf network have a tendency to be shared across several output class or even may not contribute 
the concept of jumping emerging pattern jeps ha been proposed to describe those discriminating feature which only occur in the positive training instance but do not occur in the negative class at all jeps have been used to construct classifier which generally provide better accuracy than the state of the art classifier such a c the algorithm for maintaining the space of jumping emerging pattern jep space are presented in this paper we prove that jep space satisfy the property of convexity therefore jep space can be concisely represented by two bound consisting respectively of the most general element and the most specific element in response to insertion of new training instance a jep space is modified by operating on it boundary element and the boundary element of the jep space associated with the new instance this strategy completely avoids the need to go back to the most initial step to build the new jep space in addition our maintenance algorithm can well handle such other case a deletion of instance insertion of new attribute and deletion of attribute 
this paper present a multi domain information extraction system in order to decrease the time spent on the elaboration of resource for the ie system and guide the end user in a new domain we suggest to use a machine learning system that help defining new template and associated resource this knowledge is automatically derived from the text collection in interaction with the end user to rapidly develop a local ontology giving an accurate image of the content of the text the system is finally evaluated using classical indicator 
recognition using only visual evidence cannot always be successful due to limitation of information and resource available during training considering relation among lexicon entry is sometimes useful for decision making in this paper we present a method to capture lexical similarity of a lexicon and reliability of a character recognizer which serve to capture the dynamism of the environment a parameter lexical similarity is defined by measuring these two factor a edit distance between lexicon entry and separability of each character s recognition result our experiment show that a utility function considering lexical similarity in a decision stage can enhance the performance of a conventional word recognizer 
recent work ha exploited boundedness of data in the unsupervised learning of new type of generative model for nonnegative data it wa recently shown that the maximum entropy generative model is a nonnegative boltzmann distributionnot a gaussian distribution when the model is constrained to match the first and second order stati stics of the data learning for practical sized problem is made difficul t by the need to compute expectation under the model distribution the computational cost of markov chain monte carlo method and low fideli ty of naive mean field technique ha led to increasing interest in advanced mean field theory and variational method here i present a secondorder mean field approximation for the nonnegative boltzma nn machine model obtained using a high temperature expansion the theory is tested on learning a bimodal dimensional model a high dimensional translationally invariant distribution and a generative model for handwritten digit 
a technique is presented for representing linear feature a probability density function in two or three dimension three chief advantage of this approach are a unified representation and algebra for manipulating point line and plane seamless incorporation of uncertainty information and a very simple recursive solution for maximum likelihood shape estimation application to uncalibrated affine scene reconstruction are presented with result on image of an outdoor environment 
the earth mover s distance emd is a distance measure between distribution with application in image retrieval and matching we consider the problem of computing a transformation of one distribution which minimizes it emd to another the application discussed here include estimation of the size at which a color pattern occurs in an image lighting invariant object recognition and point feature matching in stereo image pair we present a monotonically convergent iteration which can be applied to a large class of emd under transformation problem although the iteration may converge to only a locally optimal transformation we also provide algorithm that are guaranteed to compute a globally optimal transformation for a few specific problem including some emd under translation problem 
when not enough time is available to fully explore a search tree different algorithm will visit different leaf depth first search and depth bounded discrepancy search for example make opposite assumption about the distribution of good leaf unfortunately it is rarely clear a priori which algorithm will be most appropriate for a particular problem rather than fixing strong assumption in advance we propose an approach in which an algorithm attempt to adjust to the distribution of leaf cost in the tree while exploring it by sacrificing completeness such flexible algorithm can exploit information gathered during the search using only weak assumption a an example we show how a simple depth based additive cost model of the tree can be learned on line empirical analysis using a generic tree search problem show that adaptive probing is competitive with systematic algorithm on a variety of hard tree and outperforms them when the node ordering heuristic make many mistake result on boolean satisfiability and two different representation of number partitioning confirm these observation adaptive probing combine the flexibility and robustness of local search with the ability to take advantage of constructive heuristic 
this paper look at the use of a self organizing map som to link of record of crime of serious sexual attack once linked a profile can be derived of the offender s responsible the data wa drawn from the major crime database at the national crime faculty of the national police staff college bramshill uk the data wa encoded from text by a small team of specialist working to a well defined protocol the encoded data wa analyzed using som two exercise were conducted these resulted in the linking of several offence in to cluster each of which were sufficiently similar to have possibly been committed by the same offender s a number of cluster were used to form profile of offender some of these profile were confirmed by independent analyst a either belonging to known offender or appeared sufficiently interesting to warrant further investigation the prototype wa developed over week this contrast with an in house study using a conventional approach which took year to reach similar result a a consequence of this study the ncf intends to pursue an in depth follow up study 
training a support vector machine svm requires the solution of a very large quadratic programming qp problem this paper proposes an al gorithm for training svms sequential minimal optimization or smo smo break the large qp problem into a series of smallest possible qp problem which are analytically solvable thus smo doe not require a numerical qp library smo s computation time is dominated by eval uation of the kernel hence kernel optimization substantially quicken smo for the mnist database smo is time a fast a pcg chunk ing while for the uci adult database and linear svms smo can be time faster than the pcg chunking algorithm 
in optimality theoretic syntax optimization with unrestricted expressive power on the side of the ot constraint is undecidable this paper provides a proof for the decidability of optimization based on constraint expressed with reference to local subtrees which is in the spirit of ot theory the proof build on kaplan and wedekind s construction showing that lfg generation produce context free language 
text normalization is an important aspect of successful information retrieval from medical document such a clinical note radiology report and discharge summary in the medical domain a significant part of the general problem of text normalization is abbreviation and acronym disambiguation numerous abbreviation are used routinely throughout such text and knowing their meaning is critical to data retrieval from the document in this paper i will demonstrate a method of automatically generating training data for maximum entropy me modeling of abbreviation and acronym and will show that using me modeling is a promising technique for abbreviation and acronym normalization i report on the result of an experiment involving training a number of me model used to normalize abbreviation and acronym on a sample of rheumatology note with accuracy 
this paper present a grammatical and processing framework for handling the repair hesitation and other interruption in natural human dialog the proposed framework ha proved adequate for a collection of human human task oriented dialog both in a full manual examination of the corpus and in test with a parser capable of parsing some of that corpus this parser can also correct a pre parser speech repair identifier resulting in a increase in recall 
persistent connection address ineciencies associated withmultiple concurrent connection they can improve responsetime when successfully used with pipelining to retrieve a setof object from a web server in practice however there isinconsistent support for persistent connection particularlywith pipelining from web server user agent and intermediary web browser continue to open multiple concurrenttcp connection to the same server this paper proposes a new idea of 
the billion base pair sequence of the human genome is now available and attention is focusing on annotating it to extract biological meaning i will discus what we have obtained and the method that are being used to analyse biological sequence in particular i will discus approach using stochastic grammar analogous to those used in computational linguistics both for gene finding and protein family classification 
the algorithm presented here bcc is an enhancement of the well known backtrack used to solve constraint satisfaction problem though most backtrack improvement rely on propagation of local information bcc us global knowledge of the constraint graph structure and in particular it biconnected component to reduce search space permanently removing value and compiling partial solution during exploration this algorithm performs well by itself without any filtering when the biconnected component are small achieving optimal time complexity in case of a tree otherwise it remains compatible with most existing technique adding only a negligible overhead cost 
a view of plan recognition shaped by bothoperational and computational requirement ispresented operational requirement governingthe level of fidelity and nature of the reasoningprocess combine with computational requirementsincluding performance speed and softwareengineering effort to constrain the typesof solution available to the software developer by adopting machine learning to providespatio temporal recognition of environmentalevents and relationship an agent can 
fraud cause substantial loss to telecommunication carrier detec tion system which automatically detect illegal use of the network can be used to alleviate the problem previous approach worked on feature derived from the call pattern of individual user in this paper we present a call based detection system based on a hierarchical regime switching model the detection problem is formulated a an inference problem on the regime probability inference is implemented by applying the junc tion tree algorithm to the underlying graphical model the dynamic are learned from data using the em algorithm and subsequent discriminative training the method are assessed using fraud data from a real mobile communication network 
although feature selection is a central problemin inductive learning a suggested by thegrowing amount of research in this area mostof the work ha been carried out under the supervisedlearning paradigm paying little attentionto unsupervised learning task and particularly clustering task in this paper we analyze the particular benefit thatfeature selection may provide in hierarchicalclustering task and explore the power of featureselection method applied a a preprocessing 
relevance feedback is an appreciated process to produce increasingly better retrieval usually positive feedback play a fundamental role in the feedback process whereas the role of negative feedback is limited we think that negative feedback is a promising precision oriented mechanism and we propose a logical framework in which positive and negative feedback are homogeneously modeled evaluation result against small test collection are provided 
we present an algorithm that infers the model structure of a mixture of factor analyser using an efficient and deterministic variational approximation to full bayesian integration over model parameter this procedure can automatically determine the optimal number of component and the local dimensionality of each component i e the number of factor in each factor analyser alternatively it can be used to infer posterior distribution over number of component and dimensionality since 
case base maintenance is gaining increasing recognition in research and the practical application of case based reasoning cbr this intense interest is highlighted by smyth and keane s research on case deletion policy in their work smyth and keane advocated a case deletion policy whereby the case in a case base are classified and deleted based on their coverage potential and adaptation power the algorithm wa empirically shown to improve the competence of a cbr system and outperform a number of previous deletion based strategy in this paper we present a different case base maintenance policy that is based on case addition rather than deletion the advantage of our algorithm is that we can place a lower bound on the competence of the resulting case base we demonstrate that the coverage of the computed case base cannot be worse than the optimal case base in coverage by a fixed lower bound and the coverage is often much closer to optimum we also show that the smyth and keane s deletion based policy cannot guarantee any such lower bound our result highlight the importance of finding the right case base maintenance algorithm in order to guarantee the best case base coverage we demonstrate the effectiveness of our algorithm through an experiment in case based planning 
we conduct an average case analysis of the generalization error rate of holdout testing and n fold cross validation quot wrapper quot for model selection unlike previous approach we do not rely on worst case bound that hold for all possible learning problem instead we study the behavior of a learning algorithm with a cross validation wrapper for a given problem taking property of the problem that can be estimated using the sample into account we have to pay for this and the efficiency 
this paper present a corpus based approach to word sense disambiguation where a decision tree assigns a sense to an ambiguous word based on the bigram that occur nearby this approach is evaluated using the sense tagged corpus from the senseval word sense disambiguation exercise it is more accurate than the average result reported for of word and is more accurate than the best result for of word 
the robocup robot world cup soccer effort initiated to stimulate research in multi agent and robotics ha blossomed into a significant effort of international proportion robocup is simultaneously a fundamental research effort and a set of competition for testing research idea at ijcai a broad research challenge wa issued for the robocup synthetic agent covering area of multi agent learning teamwork and agent modeling this paper outline our attack on the entire breadth of the robocup research challenge on all of it category in the form of two fielded contrasting robocup team and two off line soccer analysis agent we compare the team and the agent to generalize the lesson learned in learning teamwork and agent modeling 
this paper is concerned with the construction of regression and classification tree that are more adapted to data mining application than conventional tree to this end we propose new splitting criterion for growing tree conventional splitting criterion attempt to perform well on both side of a split by attempting a compromise in the quality of fit between the left and the right side by contrast we adopt a data mining point of view by proposing criterion that search for interesting subset of the data a opposed to modeling all of the data equally well the new criterion do not split based on a compromise between the left and the right bucket they effectively pick the more interesting bucket and ignore the other a expected the result is often a simpler characterization of interesting subset of the data le expected is that the new criterion often yield whole tree that provide more interpretable data description surprisingly it is a flaw that work to their advantage the new criterion have an increased tendency to accept split near the boundary of the predictor range this so called end cut problem lead to the repeated peeling of small layer of data and result in very unbalanced but highly expressive and interpretable tree 
a general and expressive model of sequential decision making under uncertainty is provided by the markov decision process mdps framework complex application with very large state space are best modelled implicitly instead of explicitly by enumerating the state space for example a precondition effect operator the representation used in ai planning this kind of representation are very powerful and they make the construction of policy plan computationally very complex in many application average reward over unit time is the relevant rationality criterion a opposed to the more widely used discounted reward criterion and for providing a solid basis for the development of efficient planning algorithm the computational complexity of the decision problem related to average reward ha to be analyzed we investigate the complexity of the policy plan existence problem for mdps under the average reward criterion with mdps represented in term of conditional probabilistic precondition effect operator we consider policy with and without memory and with different degree of sensing observability the unrestricted policy existence problem for the partially observable case wa earlier known to be undecidable the result place the remaining computational problem to the complexity class exp and nexp deterministic and nondeterministic exponential time 
we present condition under which verb phrase are elided based on a corpus of positive and negative example factor that affect verb phrase ellipsis include the distance between antecedent and ellipsis site the syntactic relation between antecedent and ellipsis site and the presence or absence of adjunct building on these result we examine where in the generation architecture a trainable algorithm for vp ellipsis should be located we show that the best performance is achieved when the trainable module is located after the realizer and ha access to surface oriented feature error rate of 
recently we showed that for traditional bidirectional search with front to end evaluation it is not the meeting of search front but the cost of proving the optimality of a solution that is problematic using our improved understanding of the problem we developed a new approach to improving this kind of search switching to unidirectional search after the search frontier meet for the first time with the first solution found this new approach show improvement over previous bidirectional search approach and partly also over the corresponding unidirectional search approach in different domain together with a special purpose improvement for the tsp this approach showed better result than the standard search algorithm using the same knowledge 
knowledge representation is a key issue for any machine learning task there have already been many comparative study about knowledge representation with respect to machine learning in classification task however apart from some work done on reinforcement learning technique in relation to state representation very few study have concentrated on the effect of knowledge representation for machine learning applied to problem solving and more specifically to planning in this paper we 
this paper is concerned with the estimation of a classier s accuracy we present a number of novel bootstrap estimator based on kernel smoothing that consistently show superior performance on both synthetic and real data with respect to other established method we call the process of re sampling the data via kernel based smoothed bootstrap data cloning the new cloning method outperform cross validation and the bootstrap which according to efron and tibshirani is the estimator of choice finally we extend our estimator to complex real life data set in which a data point might include real bounded integer and nominal attribute thus allowing for better classier evaluation over limited real data repository such a the uci repository 
semantic interoperability is the faculty of interpretingknowledge imported from other language atthe semantic level i e to ascribe to each importedpiece of knowledge the correct interpretation or setof model it is a very important requirement fordelivering a worldwide semantic web this paperpresents preliminary investigation towards developinga unified view of the problem it proposesa definition of semantic interoperability based onmodel theory and show how it applies 
boosting is a method by which an ensembleof classiers can be assembled with generallyspeaking much improved result over individualclassiers and other ensemble method it work by re weighting the trainingset after each classier is induced so thatmisclassied training instance are given increasedweight in the subsequently inducedclassier the focus on misclassied instanceshas meant that boosting cannot continueif the learning system creates a classi er that t 
an increase in the off nadir viewing angle for an airborne visible near infrared through short wave infrared vnir swir imaging spectrometer lead to a decrease in upward atmospheric transmittance and an increase in line of sight scattered path radiance these effect combine to reduce the spectral contrast between different material in the sensed signal we analyze the impact of viewing angle on material discriminability for material over a wide range of condition material discriminability is quantified using a statistical algorithm that employ a subspace model to represent the set of spectrum for a material a condition vary we show that reliable material discrimination is possible over a range of condition even for large off nadir viewing angle we illustrate the performance of material identification over different viewing angle using simulated forest hyperspectral image 
we introduce an algorithm for estimating the value of a function at a set of test point x xm given a set of training point x y x y without estimating a an intermediate step the regression function we demonstrate that this direct transductive way for estimating value of the regression or classification in pattern recognition is more accurate than the traditional one based on two step first estimating the function and then calculating the value of this function at the point of interest 
writing english is a big barrier for most chinese user to build a computer aided system that help chinese user not only on spelling checking and grammar checking but also on writing in the way of native english is a challenging task although machine translation is widely used for this purpose how to find an efficient way in which human collaborates with computer remains an open issue in this paper based on the comprehensive study of chinese user requirement we propose an approach to machine aided english writing system which consists of two component a statistical approach to word spelling help and an information retrieval based approach to intelligent recommendation by providing suggestive example sentence both component work together in a unified way and highly improve the productivity of english writing we also developed a pilot system namely pen perfect english system preliminary experiment show very promising result 
computer security depends heavily on the strength of cryptographic algorithm thus cryptographic key search is often the search problem for many government and corporation in the recent year ai search technique have achieved notable success in solving real world problem following a recent result which showed that the property of the u s data encryption standard can be encoded in propositional logic this paper advocate the use of cryptographic key search a a benchmark for propositional reasoning and search benchmark based on the encoding of cryptographic algorithm optimally share the feature of real world and random problem in this paper two state of the art ai search algorithm walk sat by kautz selman and rel sat by bayardo schrag have been tested on the encoding of the data encryption standard to see whether they are up the task and we discus what lesson can be learned from the analysis on this benchmark to improve sat solver new challenge in this field conclude the paper 
automatic summarization of open domain spoken dialogue is a new research area this paper introduces the task the challenge involved and present an approach to obtain automatic extract summary for multi party dialogue of four different genre without any restriction on domain we address the following issue which are intrinsic to spoken dialogue summarization and typically can be ignored when summarizing written text such a newswire data i detection and removal of speech disfluency ii detection and insertion of sentence boundary iii detection and linking of cross speaker information unit question answer pair a global system evaluation using a corpus of relevance annotated dialogue containing topical segment show that for the two more informal genre our summarization system using dialogue specific component significantly outperforms a baseline using tfidf term weighting with maximum marginal relevance ranking mmr 
link based ranking method have been described in the literature and applied in commercial web search engine however according to recent trec experiment they are no better than traditional content based method we conduct a different type of experiment in which the task is to find the main entry point of a specific web site in our experiment ranking based on link anchor text is twice a effective a ranking based on document content even though both method used the same bm formula we obtained these result using two set of query on a million document set and another set of on a million document set this site finding effectiveness begin to explain why many search engine have adopted link method it also open a rich new area for effectiveness improvement where traditional method fail 
learning the dependency structure of a bayesian belief net involves a trade o betweensimplicity and goodness of t to thetraining data we describe the result ofan empirical comparison of three standardmodel selection criterion viz a minimumdescription length criterion mdl akaike s information criterion aic and across validation criterion xv applied tothis problem our result suggest that aicand xv are both good criterion for avoidingovertting but mdl doe 
lazy learning is a memory based technique that once a query is received extract a prediction interpolating locally the neighboring example of the query which are considered relevant according to a distance measure in this paper we propose a data driven method to select on a query by query basis the optimal number of neighbor to be considered for each prediction a an efficient way to identify and validate local model the recursive least square algorithm is introduced in the context of local approximation and lazy learning furthermore beside the winner take all strategy for model selection a local combination of the most promising model is explored the method proposed is tested on six different datasets and compared with a state of the art approach 
we present a new robust point matching algorithm rpm that can jointly estimate the correspondence and non rigid transformation between two point set that may be of different size the algorithm utilizes the softassign for the correspondence and the thinplate spline for the non rigid mapping embedded within a deterministic annealing framework the algorithm can automatically reject a raction of the point a outlier experiment on both d synthetic pointsets with varying degree of 
we address the problem of knowledge acquisition for alarm correlation in a complex dynamic system like a telecommunication network to reduce the amount of information coming from telecommunication equipment one need to preprocess the alarm stream and we propose here a way to acquire some knowledge to do that the key idea is that only the frequent alarm set are relevant for reducing the information stream we aggregate frequent relevant information and suppress frequent noisy information we propose algorithm for analysing alarm log first stage is to discover frequently occurring temporally constrained alarm set called chronicle and second stage is to filter them according to their interdependency level we also show experimental result with an actual telecommunication atm network 
this paper describes a new efficient speech act type tagging system this system cover the task of segmenting a turn into the optimal number of speech act unit sa unit and assigning a speech act type tag sa tag to each sa unit our method is based on a theoretically clear statistical model that integrates linguistic acoustic and situational information we report tagging experiment on japanese and english dialogue corpus manually labeled with sa tag we then discus the performance difference between the two language we also report on some translation experiment on positive response expression using sa tag 
we present a multiview method for the computation ofobject shape and reflectance characteristic based on the integrationof shape from shading sfs and stereo for nonconstantalbedo and non uniformly lambertian surface first we perform stereo fitting on the input stereo pair orimage sequence when the image are uncalibrated werecover the camera parameter using bundle adjustment based on the stereo result we can automatically segmentthe albedo map which is taken to be piece wise 
facial variation divide into a number of functional subspace an improved method of measuring these wa designed within the space defined by an appearance model initial estimate of the subspace lighting pose identi ty expression were obtained by principal component analysis on appropriate group of face an iterative algorith m wa applied to image coding to maximise the probability of coding across these non orthogonal subspace before obtaining the projection on each sub space and recalculating the space this procedure enhances identity recognition reduces overall sub space variance and produce principal component with greater span and le contamination 
transaction data is ubiquitous in data mining application example include market basket data in retail commerce telephone call record in telecommunication and web log of individual page request at web site profiling consists of using historical transaction data on individual to construct a model of each individual s behavior simple profiling technique such a histogram do not generalize well from sparse transaction data in this paper we investigate the application of probabilistic mixture model to automatically generate profile from large volume of transaction data in effect the mixture model represents each individual s behavior a a linear combination of basis transaction we evaluate several variation of the model on a large retail transaction data set and show that the proposed model provides improved predictive power over simpler histogram based technique a well a being relatively scalable interpretable and flexible in addition we point to application in outlier detection customer ranking interactive visualization and so forth the paper concludes by comparing and relating the proposed framework to other transaction data modeling technique such a association rule 
in this paper the problem of controlling the spatialposition and orientation of a robotic platform based onthe image data obtained from a video camera mountedon that platform is considered more specifically wepropose control law that generate translational andangular velocity that will cause the robot to achieveand maintain a fixed position and orientation with respectto a set of feature point in the scene the proposed control scheme make use of well establishedtechniques for 
a basic aspect of understanding the neural code of a neuron or aneural system is the ability to form a dictionary from the stimulipresented to the neuron or the system and the pattern of spikesthat the neuron responds with a neuron may respond unreliablyto their stimulus such a dictionary will be stochastic by nature ifthe neuron responds to many dierent stimulus in a similar way i e the number of stimulus feature that the neuron care about issmall then the 
the dialogue strategy used by a spoken dialogue system strongly influence performance and user satisfaction an ideal system would not use a single fixed strategy but would adapt to the circumstance at hand to do so a system must be able to identify dialogue property that suggest adaptation this paper focus on identifying situation where the speech recognizer is performing poorly we adopt a machine learning approach to learn rule from a dialogue corpus for identifying these situation our result show a significant improvement over the baseline and illustrate that both lower level acoustic feature and higher level dialogue feature can affect the performance of the learning algorithm 
we discus the advantage of lexicalized tree adjoining grammar a an alternative to lexicalized pcfg for statistical parsing describing the induction of a probabilistic ltag model from the penn treebank and evaluating it parsing performance we find that this induction method is an improvement over the em based method of hwa and that the induced model yield result comparable to lexicalized pcfg 
the development of user interface based on vision and speech requires the solution of a challenging statistical inference problem the intention and action of multiple individual must be inferred from noisy and ambiguousdata we argue that bayesian network model are an attractive statistical framework for cue fusion in these application bayes net combine a natural mechanism for expressing contextual information with efficient algorithm for learning and inference we illustrate these point through the development of a bayes net model for detecting when a user is speaking the model combine four simple vision sensor face detection skin color skin texture and mouth motion we present some promising experimental result 
fingerhut business intelligence bi ha a long and successful history of building statistical model to predict consumer behavior the model constructed are typically segmentation based model in which the target audience is split into subpopulation i e customer segment and individually tailored statistical model are then developed for each segment such model are commonly employed in the direct mail industry however segmentation is often performed on an ad hoc basis without directly considering how segmentation affect the accuracy of the resulting segment model fingerhut bi approached ibm research with the problem of how to build segmentation based model more effectively so a to maximize predictive accuracy the ibm advanced targeted marketing single eventstm ibm atm setm solution is the result of ibm research and fingerhut bi directing their effort jointly towards solving this problem this paper present an evaluation of atm se s modeling capability using data from fingerhut s catalog mailing 
coordination of agent activity is a key problem in multiagent system set in a larger decision theoretic context the existence of coordination problem lead to difficulty in evaluating the utility of a situation this in turn make defining optimal policy for sequential decision process problematic we propose a method for solving sequential multi agent decision problem by allowing agent to reason explicitly about specific coordination mechanism we define an extension of value iteration in which the system s state space is augmented with the state of the coordination mechanism adopted allowing agent to reason about the short and long term prospect for coordination the long term consequence of mi coordination and make decision to engage or avoid coordination problem based on expected value we also illustrate the benefit of mechanism generalization 
the recognition of proper noun pns is considered an important task in the area of information retrieval and extraction however the high performance of most existing pn classifier heavily depends upon the availability of large dictionary of domain specific proper noun and a certain amount of manual work for rule writing or manual tagging though it is not a heavy requirement to rely on some existing pn dictionary often these resource are available on the web it coverage of a domain corpus may be rather low in absence of manual updating in this paper we propose a technique for the automatic updating of an pn dictionary through the cooperation of an inductive and a probabilistic classifier in our experiment we show that whenever an existing pn dictionary allows the identification of of the proper noun within a corpus our technique allows without additional manual effort the successful recognition of about of the remaining 
we have developed a silicon neuron that is inspired by a mathematical model of the leech heartbeat hn interneuron the temporal and ionic current behavior of this silicon neuron are close to that of the living cell because of this similarity we were able to interface this silicon neuron to a living hn cell using a dynamic clamp technique we present data showing dynamic behavior of the hybrid half center oscillator 
this paper present a new approach to computing dense depth and motion estimate from multiple image rather than computing a single depth or motion map from such a collection we associate motion or depth estimate with each image in the collection or at least some subset of the image this ha the advantage that the depth or motion of region occluded in one image will still be represented in some other image thus task such a novel view interpolation or motion compensated prediction can be solved with greater fidelity furthermore the natural variation in appearance between different image can be captured to formulate motion and structure recovery we cast the problem a a global optimization over the unknown motion or depth map and use robust smoothness constraint to constrain the space of possible solution we develop and evaluate some motion and depth estimation algorithm based on this framework 
combined word based index and phonetic index have been used to improve the performance of spoken document retrieval system primarily by addressing the out of vocabulary retrieval problem however a known problem with phonetic recognition is it limited accuracy in comparison with word level recognition we propose a novel method for phonetic retrieval in the cuevideo system based on the probabilistic formulation of term weighting using phone confusion data in a bayesian framework we evaluate this method of spoken document retrieval against word based retrieval for the search level identified in a realistic video based distributed learning setting using our test data we achieved an average recall of with an average precision of for retrieval of out of vocabulary word on phonetic transcript with word error rate for in vocabulary word we achieved a improvement in recall over word based retrieval with a loss in precision for word error rite ranging from to 
wepresent a new method for information retrieval using hidden markov model hmms wedevelop a general framework for incorporating multiple word generation mechanism within the same model we then demonstrate that an extremely simple realization of this model substantially outperforms standard tf idf ranking on both the trec and trec ad hoc retrieval task we go on to present a novel method for performing blind feedback in the hmm framework a more complex hmm that model bigram 
there have been relatively little work to shed light on the effect of error in the intrinsic parameter on motion estimation and scene reconstruction given that the estimation of the extrinsic and intrinsic parameter from uncalibrated motion apts to be imprecise it is important to study the resulting distortion on the recovered structure by making use of the iso distortion framework we explicitly characterize the geometry of the distorted space recovered from d motion with freely varying focal length this characterization allows u to investigate the effectiveness of the visibility constraint in disambiguating uncalibrated motion by studying the negative distortion region and to make explicit those ambiguous error situation under which the visibility constraint is not effective an important finding is that under these ambiguous situation the direction of heading can nevertheless be accurately recovered and the structure recovered experienced a well behaved distortion the distortion is given by a relief transformation which preserve ordinal depth relation thus in the case where the only unknown intrinsic parameter is the focal length structure information in the form of depth relief can be obtained experiment were presented to support the use of the visibility constraint in obtaining such partial motion and structure solution 
by automatically reformulating the problem domain constructive induction ideally overcomes the defect of the initial description the reformulation presented here us the version space primitive d e f defined for any pair of example e and f a the set of hypothesis covering e and discriminating f from these primitive we derive a polynomial number of m of n concept experimentally many of these concept turn out to be significant and consistent a simple learning strategy thus consists of exhaustively exploring these concept and retaining those with sufficient quality tunable complexity is achieved in the monkei algorithm by considering a user supplied number of primitive d ei fi where ei and fi are stochastically sampled in the training set monkei demonstrates good performance on some benchmark problem and obtains outstanding result on the predictive toxicology evaluation challenge 
the facial action coding system facs is an objectivemethod for quantifying facial movement in term of componentactions this system is widely used in behavioral investigationsof emotion cognitive process and social interaction the codingis presently performed by highly trained human expert thispaper explores and compare technique for automatically recognizingfacial action in sequence of image these method includeunsupervised learning technique for nding 
the eigenfunction expansion of a kernel function k x y a used in support vector machine or gaussian process predictor is studied when the input data is drawn from a distribution p x in this case it is shown that the eigenfunctions f i g obey the equation k x y p x i x dx i i y this ha a number of consequence including i the eigenvalue vector of the n amp time n gram matrix k obtained by evaluating the kernel at all pair of training point k x i x j converge to the 
a opposed to traditional supervised learning multiple instance learning concern the problem of classifying a bag of instance given bag that are labeled by a teacher a being overall positive or negative current research mainly concentrate on adapting traditional concept learning to solve this problem in this paper we investigate the use of lazy learning and hausdorff distance to approach the multipleinstance problem we present two variant of the k nearest neighbor algorithm called bayesianknn and citation knn solving the multipleinstance problem experiment on the drug discovery benchmark data show that both algorithm are c ompetitive with the best one conceived in the c oncept l earning framework further work includes exploring of a combination of lazy and eager multiple instance problem classifier 
this paper present an incremental learning algorithm within the framework of a fuzzy intelligent system the incremental learning algorithm is based on priority value attached to fuzzy rule the priority value of a fuzzy rule is generated based on the fuzzy belief value of the fuzzy rule derived from the training data the fuzzy incremental algorithm ha three important property it can detect and recover from incorrect knowledge once new knowledge is available it will not lose the useful knowledge generated from the old data while it attempt to learn from new data and it provides a mechanism allowing to emphasize on knowledge learnt from the new data the incremental fuzzy learning algorithm ha been implemented in a fuzzy intelligent system for automotive engineering diagnosis it performance is presented in the paper 
gaussian process are powerful regression model specified by parameterized mean and covariance function standard approach to choose these parameter known by the name hyperparameters are maximum likelihood ml and maximum aposterior map approach in this paper we propose and investigate predictive approach based on geisser s predictive sample reuse psr methodology and the related stone s cross validation cv methodology more specifically we derive result for geisser s 
in this paper a compact representation d layered adaptive resolution and multi perspective panorama lamp is proposed for representing large scale and d scene with occlusion two kind of d lamp representation are constructed i e the relief like lamp and the image based lamp both of which concisely represent almost all the information from a long image sequence the relief like lamp is basically a single extended multi perspective panoramic view image with both texture and depth value but each pixel ha multiple value to represent result of occlusion recovery and resolution enhancement the image based lamp on the other hand consists of a set of multi perspective layer each of which ha both texture and depth map with adaptive time sampling scale depending on depth of scene point several example of d lamp construction for real image sequence are given the d lamp is a concise and powerful representation for image based rendering 
this paper present a novel approach to non linear black box system identification which combine qualitative reasoning qr method with fuzzy logic system such a method aim at building a good initialization of a fuzzy identifier so that it will converge to the input output relation which capture the nonlinear dynamic of the system fuzzy inference procedure should be initialized with a rule base predefined by the human expert when such a base is not available or poorly defined the inference procedure becomes extremely inefficient our method aim at solving the problem of the construction of a meaningful rule base fuzzy rule are automatically generated by encoding the knowledge of the system dynamic described by the outcome of it qualitative simulation both efficiency and robustness of the method are demonstrated by it application to the identification of the kinetics of thiamine vitamin b and it phosphoesters in the cell of the intestine tissue 
a new decomposition algorithm for training regression supportvector machine svm is presented the algorithm build onthe basic principle of decomposition proposed by osuna et al and address the issue of optimal working set selection the newcriteria for testing optimality of a working set are derived basedon these criterion the principle of quot maximal inconsistency quot is proposedto form approximately optimal working set experimentalresults show superior performance of the 
recently there ha been significant interest in supervised learningalgorithms that combine labeled and unlabeled data fortext learning task the co training setting blum amp mitchell applies to datasets that have a natural separation oftheir feature into two disjoint set we demonstrate thatwhen learning from labeled and unlabeled data algorithmsexplicitly leveraging a natural independent split of the featuresoutperform algorithm that do not when a naturalsplit doe not 
this is a paper that describes computational linguistic activity on philippine language the philippine is an archipelago with vast number of island and numerous language the task of understanding representing and implementing these language require enormous work an extensive amount of work ha been done on understanding at least some of the major philippine language but little ha been done on the computational aspect majority of the latter ha been on the purpose of machine translation 
in this paper we describe a multiagent system in which agent negotiate to allocate resource and satisfy constraint in a real time environment of multisensor target tracking the agent attempt to optimize the use of their own consumable resource while adhering to the global goal i e accurate and effective multisensor target tracking agent negotiate based on different strategy which are selected and instantiated using case based reasoning cbr agent are also fully reflective in that they are aware of all their resource including system level one such a cpu allocation and this allows them to achieve real time behavior we focus our discussion on multisensor target racking case based negotiation and real time behavior and present experimental result comparing our methodology to one using either no negotiation or using a static negotiation protocol 
we define a process called congealing in which element of a dataset image are brought into correspondence with each other jointly producing a data defined model it is based upon minimizing the summed component wise pixelwise entropy over a continuous set of transforms on the data one of the biproducts of this minimization is a set of transforms one associated with each original training sam ple we then demonstrate a procedure for effectively bringing test data into correspondence with the data defined model produced in the congealing process subsequently we develop a probability density over the set of transforms that arose from the congealing process we suggest that this density over transforms may be shared by many class and demonstrate how using this density a prior knowledge can be used to develop a classifier based on only a single training example for each class 
we present two method for learning the structure of personal name from unlabeled data the first simply us a few implicit constraint governing this structure to gain a toehold on the problem e g descriptor come before first name which come before middle name etc the second model also us possible coreference information we found that coreference constraint on name improve the performance of the model from to we are interested in this problem in it own right but also a a possible way to improve named entity recognition by recognizing the structure of different kind of name and a a way to improve noun phrase coreference determination 
since the discovery that the best error correcting decoding algorithm can be viewed a belief propagation in a cycle bound graph researcher have been trying to determine under what circumstance quot loopy belief propagation quot is effective for probabilistic inference 
factor analysis and principal component analysis can be used tomodel linear relationship between observed variable and linearlymap high dimensional data to a lower dimensional hidden space in factor analysis the observation are modeled a a linear combinationof normally distributed hidden variable we describe anonlinear generalization of factor analysis called product analysis quot that model the observed variable a a linear combinationof product of normally distributed hidden 
the attribute interdependency have strong effect on understandability of tree based model if strong dependency between the attribute are not recognized and these attribute are not used a split near the root of the tree this cause node replication in lower level of the tree blur the description of dependency and also might cause drop of accuracy if relief family of algorithm which is capable of estimating the attribute dependency is used for split selector we can partly overcome the problem however typically we still want to optimize accuracy of the tree and therefore use accuracy a the split selector measure near the fringe of the tree we present a technique which help u select a split criterion during tree growing based on some theoretical property of relief s estimate we support our claim with empirical result 
extracting rule from rbfs is not a trivial task because of nonlinear function or high input dimensionality in such case some of the hidden unit of the rbf network have a tendency to be shared across several output class or even may not contribute 
pitch accent placement is a major topic in intonational phonology research and it application to speech synthesis what factor influence whether or not a word is made intonationally prominent or not is an open question in this paper we investigate how one aspect of a word s local context it collocation with neighboring word influence whether it is accented or not result of experiment on two transcribed speech corpus in a medical domain show that such collocation information is a useful predictor of pitch accent placement 
the choice of an svm kernel corresponds to the choice of a representationof the data in a feature space and to improve performance it should therefore incorporate prior knowledge such a known transformationinvariances we propose a technique which extends earlierwork and aim at incorporating invariance in nonlinear kernel weshow on a digit recognition task that the proposed approach is superiorto the virtual support vector method which previously had been themethod of choice 
we demonstrate that statistical analysis of ill posed data set issubject to a bias which can be observed when projecting independenttest set example onto a basis dened by the training example 
we describe a coherent view of learning and reasoning with relational representation in the context of natural language processing in particular we discus the neuroidal architecture inductive logic programming and the snow system explaining the relationship among these and thereby offer an explanation of the theoretical basis for the snow system we suggest that extension of this system along the line suggested by the theory may provide new level of scalability and functionality 
this paper formulates and solves a new variant of the stereocorrespondence problem simultaneously recovering the disparity true color and opacity of visible surface element this problemarises in newer application of stereo reconstruction such a viewinterpolation and the layering of real imagery with synthetic graphicsfor special effect and virtual studio application while this problemis intrinsically more difficult than traditional stereo correspondence where only the disparity are being recovered it provides a principledway of dealing with commonly occurring problem such a occlusion andthe handling of mixed foreground background pixel near depthdiscontinuities it also provides a novel mean for separatingforeground and background object matting without the use of a specialblue screen we formulate the problem a the recovery of color andopacities in a generalized d x y d disparity space and solve theproblem using a combination of initial evidence aggregation followed byiterative energy minimization 
a nonlinear supervised learning model the specialized mappingsarchitecture sma is described and applied to the estimation ofhuman body pose from monocular image the sma consists ofseveral specialized forward mapping function and an inverse mappingfunction each specialized function map certain domainsof the input space image feature onto the output space bodypose parameter the key algorithmic problem faced are those oflearning the specialized domain and mapping 
cf loadingtexthtml cf contextpath cf ajaxscriptsrc cfide script ajax cf jsonprefix cf clientid bf ab d afd f a bbe using text classifier for numerical classification function settab var mytabs coldfusion layout gettablayout citationdetails mytabs on tabchange function tabpanel activetab document cookie picked activetab id function letemknow coldfusion window show letemknow function testthis alert test function loadalert alert i am in the load alert function loadalert alert i am in the load alert google load visualization package orgchart google setonloadcallback drawchart function drawchart var data new google visualization datatable data addcolumn string name data addcolumn string manager data addcolumn string tooltip data addrows v f cc for this article 
this paper describes computer vision algorithm to assist in retinal laser surgery which is widely used to treat leading blindness causing condition but only ha a success rate mostly due to a lack of spatial mapping and reckoning capability in current instrument the novel technique described here automatically construct a composite mosaic image of the retina from a sequence of incomplete view this mosaic will be useful to opthalmologists for both diagnosis and surgery the new technique go beyond published method in both the medical and computer vision literature because it is fully automated model the patient dependent curvature of the retina handle large interframe motion and doe not require calibra tion at the heart of the technique is a parameter image transformation model derived by modeling the retina a a quadratic surface and assuming a weak perspective camera and rigid motion estimating the parameter of this transformation model requires robustness to unmatchable image feature and mismatch between feature caused by large interframe motion the described estimation technique is a hierarchy of model and method the initial match set is pruned based on a th order transformation estimated using a similarity weighted histogram a st order affine transformation is estimated using the reduced match set and least median of square and the final nd order parameter transformation is estimated using an m estimator initialized from the st order result initia l experimental result show the method to be robust and accurate in accounting for the unknown retinal curvature in a fully automatic manner while preserving image detail 
the distinction between achievement and accomplishment is known to be an empirically important but subtle one it is argued here to depend on the atomicity rather than punctuality of event and to be strongly related to incrementality i e to event object mapping function a computational treatment of incrementality and atomicity is discussed in the paper and a number or related empirical problem considered notably lexical polysemy in verb argument relationship 
in this paper we investigate the structure and motion problem for calibrated one dimensional projection of a two dimensional environment in a previous paper the structure and motion problem for all case with non missing data wa classified and solved our aim is here to classify all structure and motion problem even those with missing data and to solve them although our focus here is on onedimensional retina the classification part work equally well for ordinary camera and we give some result for those a well 
this paper describes a new method for estimating the illumination distribution of a real scene from a radiance distribution inside shadow cast by an object in the scene first the illumination distribution of the scene is approximated by discrete sampling of an extended light source then the illumination distribution of the scene is estimated from a radiance distribution inside shadow cast by an object of known shape onto another object in the scene instead of assuming any particular reflectance property of the surface inside the shadow both the illumination distribution of the scene and the reflectance property of the surface are estimated simultaneously based on iterative optimization framework in addition this paper introduces an adaptive sampling of the illumination distribution of a scene rather than using a uniform discretization of the overall illumination distribution we adaptively increase sampling direction of the illumination distribution based on the estimation at the previous iteration using the adaptive sampling framework we are able to estimate overall illumination more efficiently by using fewer sampling direction the proposed method is effective for estimating an illumination distribution even under a complex illumination environment 
a three part study wa designed to document internet use in scholarly research using the annual sigir conference proceeding from through the result suggest an increasing trend toward electronic self publishing furthermore while electronic availability did not insure that one would be cited the most highly cited article were available on the free web the study also found that electronic availability ha not in most case decreased the length of time between publication and citation 
new invariant feature are presented that can be usedfor the recognition of planar color pattern such a label logo sign pictograms etc irrespective of theviewpoint or the illumination condition and withoutthe need for error prone contour extraction the newfeatures are based on moment of power of the intensitiesin the individual color band and combinationsthereof these moment implicitly characterizethe shape the intensity and the color distributionof the pattern in a 
ontology and information sharing have a major role to play in the development of knowledge based agent and the overcome of the knowledge acquisition bottleneck this paper support this claim by presenting an approach to ontology specification import and development that is part of disciple rkf disciple rkf is a theory methodology and learning agent shell for the rapid development of knowledge based agent by subject matter expert with limited assistance from knowledge engineer the disciple approach ha been subject of intensive evaluation a part of darpa s high performance knowledge base and rapid knowledge formation program demonstrating very good result 
despite many empirical success of spectral clustering method algorithm that cluster point using eigenvectors of matrix derivedfrom the distance between the point there are several unresolvedissues first there is a wide variety of algorithm thatuse the eigenvectors in slightly di erent way second many ofthese algorithm have no proof that they will actually compute areasonable clustering in this paper we present a simple spectralclustering algorithm that can be 
boosting is a machine learning algorithm that is not well known in computational linguistics we apply it to part of speech tagging and prepositional phrase attachment performance is very encouraging we also show how to improve data quality by using boosting to identify annotation error 
in this paper we show how machine learningtechniques for constructing and combining severalclassifiers can be applied to improve theaccuracy of an existing english po tagger m arquez and rodr iguez additionally the problem of data sparseness is also addressedby applying a technique of generating convexpseudo data breiman experimental resultsand a comparison to other state of the art tagger are reported keywords po tagging corpus based modeling decision 
in the past decade a large number of robotshave been built that explicitly implement biologicalnavigation behaviour we review thesebiomimetic approach using a framework thatallows for a common description of biologicaland technical navigation behaviour the reviewshows that biomimetic system make significantcontributions to two field of research first they provide a real world test of modelsof biological navigation behaviour second they make new navigation 
we describe a new practical domain independent task planner called shaper specially designed to deal efficiently with large problem shaper performs in two step in the first step executed off line for a given domain subclass shaper explores and build a compact representation of the state space called the shape graph the main contribution of shaper is it ability to resist to combinatorial explosion thanks to the manipulation of set of similar state description called shape the shape graph is then used by shaper to answer very efficiently to planning request a first version of the planner ha been implemented it ha been tested on several well known benchmark domain the result are very promising when compared with themost efficient planner from aips competition 
probabilistic mixture model are used for a broad range of data analysis task such a clustering classification predictive modeling etc due to their inherent probabilistic nature mixture model can easily be combined with other probabilistic or non probabilistic technique thus forming more complex data analysis system in the case of online data where there is a stream of data available model can be constantly updated to reflect the most current distribution of the incoming data however in many business application the model themselves represent a parsimonious summary of the data and therefore it is not desirable to change model frequently much le with every new data point in such a framework it becomes crucial to track the applicability of the mixture model and detect the point in time when the model fails to adequately represent the data in this paper we formulate the problem of change detection and propose a principled solution empirical result over both synthetic and real life data set are presented 
this paper describes a probabilistic syntactic approach to the detection and recognition of temporally extended activity and interaction between multiple agent a complete system consisting of an adaptive tracker an event generator and the parser performs segmentation and labeling of a surveillance video of a parking lot the system correctly identifies activity such a pick up and drop off which involve person vehicle interaction the main contribution of this paper are extending the parsing algorithm to handle multi agent interaction within a single parser providing a general mechanism for consistency based pruning and developing an efficient incremental parsing algorithm 
imagine that you wish to classify data consisting of ten of thousand of example residing in a twenty thousand dimensional space how can one apply standard machine learning algorithm we describe the parallel problem server ppserver and matlab p in tandem they allow user of networked computer to work transparently on large data set from within matlab this work is motivated by the desire to bring the many benefit of scientific computing algorithm and computational power to machine learning researcher we demonstrate the usefulness of the system on a number of task for example we perform independent component analysis on very large text corpus consisting of ten of thousand of document making minimal change to the original bell and sejnowski matlab source bell and sejnowski applying ml technique to data previously beyond their reach lead to interesting analysis of both data and algorithm this paper describes the parallel problem server ppserver and matlab p the ppserver is a linear algebra server that executes distributed memory algorithm on large data set together with matlab p user can manipulate large data set within matlab transparently this system brings the efficiency and power of highly optimized parallel computation to researcher using networked machine but maintain the many benefit of interactive environment we demonstrate the usefulness of the ppserver on a number of task for example we perform independent component analysis on very large text corpus consisting of ten of thousand of document with minimal change to the original bell and sejnowski matlab source bell and sejnowski applying ml technique to datasets previously beyond 
in most neural network model synapsis are treated a static weight that change only with the slow time scale of learning it is well known however that synapsis are highly dynamic and show use dependent plasticity over a wide range of time scale moreover synaptic transmission is an inherently stochastic process a spike arriving at a presynaptic terminal trigger the release of a vesicle of neurotransmitter from a release site with a probability that can be much le than one we consider a simple model for dynamic stochastic synapsis that can easily be integrated into common model for network of integrate and fire neuron spiking neuron the parameter of this model have direct interpretation in term of synaptic physiology we investigate the consequence of the model for computing with individual spike and demonstrate through rigorous theoretical result that the computational power of the network is increased through the use of dynamic synapsis 
we introduce new small motion multi frame equation applicable to the reconstruction ofdynamic scene in which point are allowed to move along straight line path with constant velocity the motion equation apply to both static and dynamic point thus prior segmentation is not necessary we present a reconstruction algorithm of camera motion scene structure and point trajectory embedded into a multi frame factorization principle which requires the minimum of image and point out of which at least are dynamic we show that the minimal number of view necessary for a factorization is and the minimal number of point required for a recovery of camera motion scene structure and point trajectory is we also derive the solution to reduced situation such a when the trajectory are embedded in a coplanar or collinear configuration and when the point position are coplanar 
function approximation fa representation of the state action value function q have been proposed in order to reduce variance in performance gradient estimate and thereby improve performance of policy gradient pg reinforcement learning in large continuous domain e g the pifa algorithm of sutton et al in press we show empirically that although pifa converges significantly faster than traditional pg algorithm such a reinforce which directly sample q without using fa fa representation of q are not necessary to reduce variance in performance gradient estimate and pg algorithm which use selective direct sample of q can converge order of magnitude faster than pifa we present a new pg algorithm called action transition policy gradient atpg which us direct sample of q and restricts estimate of the gradient to coincide with action transition thus obtaining relative value estimate of executing action without using fa representation of q we prove that atpg give an unbiased estimate of the performance gradient and converges to an optimal policy under piece wise continuity condition on the policy and the state action value function further in an experimental comparison with pifa and reinforce atpg always outperforms both algorithm taking order of magnitude fewer iteration to converge on all but very simple problem 
learning of a smooth but nonparametric probability density can be regularized using method of quantum field theory we implement a field theoretic prior numerically test it efficacy and show tha t the free parameter of the theory smoothness scale can be determine d self consistently by the data this form an infinite dimensional gen eralization of the mdl principle finally we study the implication of one s choice of the prior and the parameterization and conclude that the smoothness scale determination make density estimation very weakly sensitive to the choice of the prior and that even wrong choice can be advantageous for small data set one of the central problem in learning is to balance goodness of fit criterion against the complexity of model an important development in the bayesian approach wa thus the realization that there doe not need to be any extra penalty f or model complexity if we compute the total probability that data are generated by a model there is a factor from the volume in parameter space the occam factor that discriminates against model with more parameter this work remarkably well for system with a finite number of parameter and creates a complexity razor after occam s razor that is almost equivalent to the celebrated minimal description length mdl principle in addition if the a priori distribution involved are strictly gaussian the idea have also been proven to apply to some infinite dimensional nonparametric proble m it is not clear however what happens if we leave the finite dimensional setting to con sider nonparametric problem which are not gaussian such a the estimation of a smooth probability density a possible route to progress on the nonparametric problem wa opened by noticing that a bayesian prior for density estimation is equivalent to a qu antum field theory qft in particular there are field theoretic method for computing the infinite dimensional analog of the occam factor at least asymptotically for large number of example these observation have led to a number of paper exploring alternative formulation and their implication for the speed of learning here we return to the original formulation of ref and use numerical method to address some of the question left open by the analytic work what is the result of balancing the infini te dimensional occam factor against the goodness of fit is the qft inference optimal in u ing all of the information relevant for learning what happens if our learning problem is strongly atypical of the prior distribution following ref if i i d sample are observed then the probability 
this paper emphasizes the interest of xml meta language for corporate knowledge management taking into account the advantage of the world wide web and of ontology for knowledge management we present osirix a tool enabling enterprise ontologyguided search in xml document that may consitute a part of a corporate memory 
in information filtering if system user long term need are expressed a user profile the quality of a user profile ha a major unpact on the performance of if system the focus of the proposed research is on the study of user profile generation and update the paper introduces method for user profile generation and proposes a research agenda for their comparison and evaluation ke ywords information filtering user profile content based filtering rule based filtering 
we present an algorithm to solve the sensor planning problem for a trinocular active vision system this algorithm us an iterative optimization method to first solve for the translation between the three camera and then us this result to solve for parameter such a pan tilt angle of the camera and zoom setting 
we propose lcirc an extension of circumscription by allowing propositional combination and nesting of circumscriptive theory a shown lifschitzs nested abnormality theory nats introduced in aij are naturally embedded into this language we analyze the complexity of lcirc and nats and in particular the effect of nesting the latter is found a source of complexity a both formalism are proved to be pspace complete we identify case of lower complexity including a tractable case our result give insight into the cost of using lcirc resp nats a a host language for expressing other formalism such a narrative 
landmarking is a novel approach to describing task in meta learning previous approach to meta learning mostly considered only statistic inspired measure of the data a a source for the definition of meta attribute contrary to such approach landmarking try to determine the location of a specific learning problem in the space of all learning problem by directly measuring the performance of some simple and efficient learning algorithm themselves in the experiment reported we show how such a use of landmark value can help to distinguish between area of the learning space favouring different learner experiment both with artificial and real world database show that landmarking selects with moderate but reasonable level of success the best performing of a set of learning algorithm 
language model for speech recognition concentrate solely on recognizing the word that were spoken in this paper we advocate redefining the speech recognition problem so that it goal is to find both the best sequence of word and their po tag and thus incorporate po tagging to use po tag effectively we use clustering and decision tree algorithm which allow generalization between po tag and word to be effectively used in estimating the probability distribution we show that our po model give a reduction in word error rate and perplexity for the train corpus in comparison to word and class based approach by using the wall street journal corpus we show that this approach scale up when more training data is available 
in this paper we extend previous work on the boundary based approach to describing shape by deriving an unbounded hierarchy of atomic shape descriptor called token based on tangent bearing and it successive derivative and incorporating angle and cusp curve feature both open and closed curve have token string description at all level in the hierarchy we provide a pair of compatibility matrix for generating transition table for any level from which level specific token ordering graph that encode basic string syntax can be systematically constructed 
a challenging unsolved problem in the speech recognition communityis recognizing speech signal that are corrupted by loud highly nonstationary noise one approach to noisy speech recognitionis to automatically remove the noise from the cepstrum sequencebefore feeding it in to a clean speech recognizer in previouswork published in eurospeech we showed how a probability modeltrained on clean speech and a separate probability model trainedon noise could be combined for the purpose of 
this paper present the architecture operation and result obtained with the lasso question answering system developed in the natural language processing laboratory at smu to find answer the system relies on a combination of syntactic and semantic technique the search for the answer is based on a novel form of indexing called paragraph indexing a score of for short answer and for long answer wa achieved at the trec competition 
the trec question answering qa track wa the first large scale evaluation of domain independent question answering system in addition to fostering research on the qa task the track wa used to investigate whether the evaluation methodology used for document retrieval is appropriate for a different natural language processing task a with document relevance judging assessor had legitimate difference of opinion a to whether a response actually answer a question but comparative evaluation of qa system wa stable despite these difference creating a reusable qa test collection is fundamentally more difficult than creating a document retrieval test collection since the qa task ha no equivalent to document identifier 
measure of scatter are used in statistical pattern recognition to identify and select important feature computed a linear combination of the given feature example include principal component and linear discriminants the classic computational procedure require eigenvector decomposition of large matrix and in the case of image they are only practical for identifying a low dimensional feature subspace we investigate the case in which the selected feature are required to be a subset of the given feature it is shown that the same scatter measure used in the general case can also be used in this discrete selection case but the computational procedure no longer involves matrix eigenvector decomposition instead the selection of pixel that optimize scatter measure can be accomplished by a very simple and efficient discrete optimization technique that run in linear time regardless of the subspace size application to clustering and content based indexing are discussed 
a sequence of image taken along a camera trajectory capture a subset of scene appearance if visibility space is the space that encapsulates the appearance of the scene at every conceivable pose and viewing angle then the act of acquiring the image sequence constitutes carving a volume in visibility space we call such a volume a visual tunnel the analysis of the visual tunnel allows u to do the following predict the range of virtual camera pose in which the image can be reconstructed totally using the captured ray predict which part of the image can be generated for a given virtual camera pose and plan camera path for scene visualization at desired location we describe our visual tunnel concept and provide illustrative example in d and d 
this paper present a theoretically very simple yet efficient approach for gray scale and rotation invariant texture classification based on local binary pattern and nonparametric discrimination of sample and prototype distribution the proposed approach is very robust in term of gray scale variation since the operator are by definition invariant against any monotonic transformation of the gray scale another advantage is computational simplicity a the operator can be realized with a few operation in a small neighborhood and a lookup table excellent experimental result obtained in two true problem of rotation invariance where the classifier is trained at one particular rotation angle and tested with sample from other rotation angle demonstrate that good discrimination can be achieved with the statistic of simple rotation invariant local binary pattern these operator characterize the spatial configuration of local image texture and the performance can be further improved by combining them with rotation invariant variance measure that characterize the contrast of local image texture the joint distribution of these orthogonal measure are shown to be very powerful tool for rotation invariant texture analysis 
we present a model of the firing of place and head direction cell in rat hippocampus the model can predict the response of individual cell and population to parametric manipulation of both geometric e g o keefe burgess and orientational fenton et al a cue extending a previous geometric model hartley et al it provides a functional description of how these cell spatial response are derived from the rat s environment and make easily testable quantitative prediction consideration of the phenomenon of remapping muller kubie bostock et al indicates that the model may also be consistent with nonparametric change in firing and provides constraint for it future development 
we propose a three level video event detection algorithmand apply it to animal hunt detection in wildlife documentary the thetarst level extract texture color and motionfeatures and detects motion blob the mid level employsa neural network to verify whether the motion blob belongto object of interest this level also generates shotsummaries in term of intermediate level descriptor whichcombine low level feature from the thetarst level and containresults of mid level 
this paper develops a new paradigm for relational learning which allows for the representation and learning of relational information using propositional mean this paradigm suggests different tradeoff than those in the traditional approach to this problem the ilp approach and a a result it enjoys several significant advantage over it in particular the new paradigm is more flexible and allows the use of any propositional algorithm including probabilistic algorithm within it we evaluate the new approach on an important and relation intensive task information extraction and show that it outperforms existing method while being order of magnitude more efficient 
we present a model of binding of relationship information in a spatial domain e g square above triangle that us low order coarse coded conjunctive representation instead of more popular temporal synchrony mechanism supporter of temporal synchrony argue that conjunctive representation lack both efficiency i e combinatorial number of unit are required and systematicity i e the resulting representation are overly specific and thus do not support generalization to novel exemplar to counter these claim we show that our model a us far fewer hidden unit than the number of conjunction represented by using coarse coded distributed representation where each unit ha a broad tuning curve through high dimensional conjunction space and b is capable of considerable generalization to novel input 
we present a component based method and two global method for face recognition and evaluate them with respect to robustness against pose change in the component system we first locate facial component extract them and combine them into a single feature vector which is classified by a support vector machine svm the two global system recognize face by classifying a single feature vector consisting of the gray value of the whole face image in the first global system we trained a single svm classifier for each person in the database the second system consists of set of viewpoint specific svm classifier and involves clustering during training we performed extensive test on a database which included face rotated up to about in depth the component system clearly outperformed both global system on all test 
we introduce a new graph theoretic approach to image segmentation based on minimizing a novel class of mean cut cost function minimizing these cost function corresponds to finding a cut with minimum mean edge weight in a connected planar graph this approach ha several advantage over prior approach to image segmentation first it allows cut with both open and closed boundary second it guarantee that the partition are connected third the cost function doe not introduce an explicit bias such a a preference for large area foreground smooth or short boundary or similar weight partition this lack of bias allows it to produce segmentation that are better aligned with image edge even in the presence of long thin region finally the global minimum of this cost function is largely insensitive to the precise choice of edge weight function in particular we show that the global minimum is invariant under a linear transformation of the edge weight and thus insensitive to image contrast building on algorithm by ahuja magnanti and orlin we present a polynomial time algorithm for finding a global minimum of the mean cut cost function and illustrate the result of applying that algorithm to several synthetic and real image 
this paper present a system that simulates the emergence of realistic vowel system in a population of agent that try to imitate each other a well a possible the agent start with no knowledge of the sound system at all although none of the agent ha a global view of the language and none of the agent doe explicit optimization a coherent vowel system emerges that happens to be optimal for acoustic distinctiveness the result presented here fit in and confirm the theory of luc steel steel that view language a a complex dynamic system and the origin of language a the result of self organization and cultural evolution 
symmetrically connected recurrent network have recently been used a model of a host of neural computation however biological neural network have asymmetrical connection at the very least because of the separation between excitatory and inhibitory neuron in the brain we study characteristic difference between asymmetrical network and their symmetrical counterpart in case for which they act a selective amplifier for particular class of input pattern we show that the dramatically different dynamical behaviour to which they have access often make the asymmetrical network computationally superior we illustrate our result in network that selectively amplify oriented bar and smooth contour in visual input 
we introduce ocelot a prototype system for automatically generating the gist of a web page by summarizing it although most text summarization research to date ha focused on the task of news article web page are quite different in both structure and content instead of coherent text with a well defined discourse structure they are more often likely to be a chaotic jumble of phrase link graphic and formatting command such text provides little foothold for extractive summarization technique which attempt to generate a summary of a document by excerpting a contiguous coherent span of text from it this paper build upon recent work in non extractive summarization producing the gist of a web page by translating it into a more concise representation rather than attempting to extract a text span verbatim ocelot us probabilistic model to guide it in selecting and ordering word into a gist this paper describes a technique for learning these model automatically from a collection of human summarized web page 
abstract high dimensional data that lie on or near a low dimensional manifold can be described by a collection of local linear model such a description however doe not provide a global parameterization of the manifold arguably an important goal of unsupervised learning in this paper we show how to learn a collection of local linear model that solves this more difficult proble m our local linear model are represented by a mixture of factor analyzer and the global coordination of these model is achieved by adding a regularizing term to the standard maximum likelihood objective function the regularizer break a degeneracy in the mixture model s parameter space favoring model who se internal coordinate system are aligned in a consistent way a a result t he internal coordinate change smoothly and continuously a one traverse a connected path on the manifold even when the path cross the domain of many different local model the regularizer take the form of a kullback leibler divergence and illustrates an unexpected application of variational meth od not to perform approximate inference in intractable probabilistic model but to learn more useful internal representation in tractable one manifold learning 
this paper present a statistical approach to collaborative filtering and investigates the use of latent class model for predicting individual choice and preference based on observed preference behavior two model are discussed and compared the aspect model a probabilistic latent space model which model individual preference a a convex combination of preference factor and the two sided clustering model which simultaneously partition person and object into cluster we present em algorithm for different variant of the aspect model and derive an approximate em algorithm based on a variational principle for the two sided clustering model the benefit of the different model are experimentally investigated on a large movie data set 
we use graphical model to explore the question of how people learn simple causal relationship from data the two leading psychological theory can both be seen a estimating the parameter of a fixed graph we argue that a complete account of causal induction should also consider how people learn the underlying causal graph structure and we propose to model this inductive process a a bayesian inference our argument is supported through the discussion of three data set 
we present a practical vision based calibration system for large format multi projector display a spanning tree of homographies automatically constructed from several camera image accurately register arbitrarily mounted projector to a global reference frame experiment on the princeton display wall a projector array with resolution demonstrate that our algorithm achieves sub pixel accuracy even on large display surface a direct comparison with the previous best 
similarity between image in image retrieval is measured by computing distance between feature vector this paper present a probabilistic approach and describes two likelihood based similarity measure for image retrieval popular distance measure like the euclidean distance implicitly assign more weighting to feature with large range s than those with small range first we discus the effect of five feature normalization method on retrieval per formance then we show that the probabilistic method perform significantly better than geometric approach lik e the nearest neighbor rule with city block or euclidean distance they are also more robust to normalization effect and using better model for the feature improves the retrieval result compared to making only general assumption experiment on a database of approximately image show that studying the feature distribution are important and this information should be used in designing feature normalization method and similarity measure 
in this paper we present an example based facial sketch system our system automatically generates a sketch from an input image by learning from example sketch drawn with a particular style by an artist there are two key element in our system a non parametric sampling method and a flexible sketch model given an input image pixel and it neighborhood the conditional distribution of a sketch point is computed by querying the example and finding all similar neighborhood an expected sketch image is then drawn from the distribution to reflect the drawing style finally facial sketch are obtained by incorporating the sketch model experimental result demonstrate the effectiveness of our technique 
blob tracker have become increasingly powerful in recent year largely due to the adoption of statistical appearance model which allow effective background subtraction and robust tracking of deforming foreground object it ha been standard however to treat background and foreground modelling a separate process background subtraction is followed by blob detection and tracking which prevents a principled computation of image likelihood this paper present two theoretical advance which address this limitation and lead to a robust multiple person tracking system suitable for single camera real time surveillance application the first innovation is a multi blob likelihood function which assigns directly comparable likelihood to hypothesis containing different number of object this likelihood function ha a rigorous mathematical basis it is adapted from the theory of bayesian correlation but us the assumption of a static camera to create a more specific background model while retaining a unified approach to background and foreground modelling second we introduce a bayesian filter for tracking multiple object when the number of object present is unknown and varies over time we show how a particle filter can be used to perform joint inference on both the number of object present and their configuration finally we demonstrate that our system run comfortably in real time on a modest workstation when the number of blob in the scene is small 
ontology are set to play a key r le in the semantic web by providing a source of shared and precisely defined term that can be used in description of web resource reasoning over such description will be essential if web resource are to be more accessible to automated process shoq d is an expressive description logic equipped with named individual and concrete datatypes which ha almost exactly the same expressive power a the latest web ontology language e g oil and daml we present sound and complete reasoning service for this logic 
we present the embedded tree algorithm an iterative technique for estimation of gaussian process dened on arbitrary graph by exactly solving a series of modied problem on embedded spanning tree it computes the conditional mean with an eciency comparable to or better than other technique unlike other method the embedded tree algorithm also computes exact error covariance the error covariance computation is most ecien t for graph in which removing a small number of edge reveals an embedded tree in this context we demonstrate that sparse loopy graph can provide a signican t increase in modeling power relative to tree with only a minor increase in estimation complexity 
this paper describes initial work on deep read an automated reading comprehension system that accepts arbitrary text input a story and answer question about it we have acquired a corpus of development and test story of rd to th grade material each story is followed by short answer question an answer key wa also provided we used these to construct and evaluate a baseline system that us pattern matching bag of word technique augmented with additional automated linguistic processing stemming name identification semantic class identification and pronoun resolution this simple system retrieves the sentence containing the answer of the time 
reaching movement require the brain to generate motor command that rely on an internal model of the task s dynamic here we consider the error that subject make early in their reaching trajectory to various target a they learn an internal model using a framework from function approximation we argue that the sequence of error should reect the process of gradient descent if so then the sequence of error should obey hidden state transition of a simple dynamical system fitting the system to human data we nd a surprisingly good t accounting for of the variance this allows u to draw tentative conclusion about the basis element used by the brain in transforming sensory space to motor command to test the robustness of the result we estimate the shape of the basis element under two condition in a traditional learning paradigm with a consistent force eld and in a random sequence of force eld where learning is not possible remarkably we nd that the basis remains invariant 
in this paper we extend and integrate pre viously reported technique for resource con strained scheduling to develop a csp procedure for solving rcpsp max the resource con strained project scheduling problem with time window generalized precedence relation be tween start time of activity rcpsp max is a well studied problem within the opera tions research community and the presence of a large set of benchmark problem provides a good opportunity for comparative performance analysis our base csp scheduling model gen eralizes previous profile based approach to cumulative scheduling by focusing on global analysis of minimal conflicting set rather than pairwise conflict analysis this generalization increase the tendency for more effective conflict resolution since rcpsp max is an opti mization problem other idea from prior work are adapted to embed this base csp model within a multi pas iterative sampling procedure the overall procedure called i it erative sampling earliest solution is applied to the above mentioned set of benchmark problem i is shown to perform quite well in comparison to current state of the art procedure for rcpsp max particularly a search space size becomes limiting for systematic procedure 
this paper present a novel approach to the unsupervised learning of syntactic analysis of natural language text most previous work ha focused on maximizing likelihood according to generative pcfg model in contrast we employ a simpler probabilistic model over tree based directly on constituent identity and linear context and use an em like iterative procedure to induce structure this method produce much higher quality analysis giving the best published result on the atis dataset 
large margin linear classification method have been successfully applied to many application for a linearly separable problem it is known that under appropriate assumption the expected misclassification error of the computed optimal hyperplane approach zero at a rate proportional to the inverse training sample size this rate is usually characterized by the margin and the maximum norm of the input data in this paper we argue that another quantity namely the robustness of the input data distribution also play an important role in characterizing the convergence behavior of expected misclassification error based on this concept of robustness we show that for a large margin separable linear classification problem the expected misclassification error may converge exponentially in the number of training sample size 
this paper considers approach which rerank the output of an existing probabilistic parser the base parser produce a set of candidate par for each input sentence with associated probability that define an initial ranking of these par a second model then attempt to improve upon this initial ranking using additional feature of the tree a evidence we describe and compare two approach to the problem one based on markov random field the other based on boosting approach to reranking problem the method were applied to reranking output of the parser of collins on the wall street journal corpus with a relative decrease in error rate 
we describe an effective and novel approach to infer sign and direction of principal curvature at each input site from noisy d data unlike most previous approach no local surface fitting partial derivative computation of any kind nor oriented normal vector recovery is performed in our method these approach are noise sensitive since accurate local partial derivative information is often required which is usually unavailable from real data because of the unavoidable outlier noise inherent in many measurement phase also we can handle point with zero gaussian curvature uniformly i e without the need to localize and handle them first a a separate process our approach is based on tensor voting a unified salient structure inference process both the sign and the direction of principal curvature are inferred directly from the input each input is first transformed into a synthetic tensor a novel and robust approach based on tensor voting is proposed for curvature information estimation with faithfully inferred curvature information each input ellipsoid is aligned with curvature based dense tensor kernel to produce a dense tensor field surface and crease curve are extracted from this dense field by using an extremal feature extraction process the computation is non iterative doe not require initialization and robust to considerable amount of outlier noise a it effect is reduced by collecting a large number of tensor vote qualitative and quantitative result on synthetic a well a real and complex data are presented 
this paper investigates the use of evoluti onary algorithm for the search of hypothesis space in machine learning task a opposed to the common scalar evaluation function imposing a complete order onto the hypothesis space we propose genetic search incorporating pairwise comparison of hypothesis particularly we allow incomparability of hypothesis what implies a partial order in the hypothesis space we claim that such an extension protects the interesting hypothesis from being discarded in the search process and thus increase the diversity of the population allowing better exploration of the solution space a a result it is more probable to reach hypothesis with good predictive accuracy this supposition ha been positively verified in an extensive comparative experiment of evolutionary visual learning concerning the recognition of handwritten character 
a number of previous experiment on the role of lexical ambiguity in information retrieval are reproduced on the ir semcor test collection derived from semcor where both query and document are hand tagged with phrase part of speech and wordnet sens our result indicate that a word sense disambiguation can be more beneficial to information retrieval than the experiment of sanderson with artificially ambiguous pseudo word suggested b partof speech tagging doe not seem to help improving retrieval even if it is manually annotated c using phrase a indexing term is not a good strategy if no partial credit is given to the phrase component 
this paper describes a study of different adaptation of boosting algorithm for cost sensitive classification the purpose of the study is to improve our understanding of the behavior of various cost sensitive boosting algorithm and how variation in the boosting procedure affect misclassification cost and high cost error we find that boosting can be simplified for cost sensitive classification a new variant which excludes a factor used in ordinary boosting performs best at minimizing 
this paper describes multidimensional neural preference class and preference moore machine a a principle for integrating different neural and or symbolic knowledge source we relate neural preference to multidimensional fuzzy set representation furthermore we introduce neural preference moore machine and relate traditional symbolic transducer with simple recurrent network by using neural preference moore machine finally we demonstrate how the concept of preference class and preference moore machine can be used to integrate knowledge from different neural and or symbolic machine we argue that our new concept for preference moore machine contribute a new potential approach towards general principle of neural symbolic integration 
we describe the principle of building a moving vision platform a rig that once calibrated can thereon self adjust to change in it internal configuration and maintain an euclidean representation of the d world using only projective measurement we term this calibration paradigm omnirig we assume that after calibration the camera may change critical element of their configuration including internal parameter and center of projection theoretically we show that knowing only the rotation between a set of camera is sufficient for euclidean calibration even with varying internal parameter and unknown translation no other information of the world is required 
this paper address the problem of recovering d non rigid shape model from image sequence for example given a video recording of a talking person we would like to estimate a d model of the lip and the full head and it internal mode of variation many solution that recover d shape from d image sequence have been proposed these so called structure from motion technique usually assume that the d object is rigid for example tomasi and kanade s factorization technique is based on a rigid shape matrix which produce a tracking matrix of rank under orthographic projection we propose a novel technique based on a non rigid model where the d shape in each frame is a linear combination of a set of basis shape under this model the tracking matrix is of higher rank and can be factored in a three step process to yield to pose configuration and shape we demonstrate this simple but effective algorithm on video sequence of speaking people we were able to recover d non rigid facial model with high accuracy 
electromyographic signal may provide an important new class of user interface for consumer electronics in order to make such interface effective it will be crucial to map emg signal to user gesture in real time the mapping from signal to gesture will vary from user to user so it must be acquired adaptively in this paper we describe and compare three method for static classification of emg signal we then go on to explore method for adapting the classifier over time and for sequential analysis of the gesture stream by combining the static classification algorithm with a hidden markov model we conclude with an evaluation of the combined model on an unsegmented stream of gesture 
abstract we present a tree based reparameterization framework for the approximate estimation ofstochastic process on graph with cycle this framework provides a new conceptual view ofa large class of iterative algorithm for computing approximate marginals in graph with cycle 
in this work we use point line and the linear extremal contour of cylinder to estimate the position and orientation of the camera in the world coordinate system other line basedpose estimation method use the correspondence between d line in space and d image line although the model and it observation are nite line segment we present a noise model describing the probabilistic relationship between d line and cylinder and their noisy observation the noise model take the 
an approach to automatic detectionof syllable structure is presented wedemonstrate a novel application ofem based clustering to multivariatedata exemplied by the inductionof and dimensional probabilisticsyllable class the qualitativeevaluation show that the methodyields phonologically meaningful syllableclasses we then propose anovel approach to grapheme to phonemeconversion and show that syllablestructure represents valuableinformation for pronunciation 
in this paper we revisit pustejovsky s proposal to treat ontologically complex word meaning by so called dotted pair we use a higher order feature logic based on ohori s record calculus to model the semantics of word like book and library in particular their behavior in the context of quantification and cardinality statement 
we are interested in semantical underpinnings for existing approach to preference handling in extended logic programming within the framework of answer set programming a a starting point we explore three different approach that have been recently proposed in the literature because these approach use rather different formal mean we furnish a series of uniform characterization that allow u to gain insight into the relationship among these approach to be more precise we provide different characterization in term of i fixpoints ii order preservation and iii translation into standard logic program while the two former provide semantics for logic programming with preference information the latter furnishes implementation technique for these approach 
abstract this paper present a novel approach for detection and segmentation of generic shape in cluttered image the underlying assumption is that generic object that are man made frequently have surface which closely resemble standard model shape such a rectangle semi circle etc due to the perspective transformation of optical imaging system a model shape may appear differently in the image with various orientation and aspect ratio the set of possible appearance can be represented compactly by a few vectorial eigenbases that are derived from a small set of model shape which are affine transformed in a wide parameter range instead of regular boundary of standard model we apply a vectorial boundary which improves robustness to noise background clutter and partial occlusion the detection of generic shape is realized by detecting local peak of a similarity measure between the image edge map and an eigenspace combined set of the appearance at each local maximum a fast search approach based on a novel representation by an angle space is employed to determine the best matching between model and the underlying subimage we findthat angular representation in multidimensional search corresponds better to euclidean distance than conventional projection and yield improved classification of noisy shape experiment are performed in various interfering distortion and robust detection and segmentation are achieved 
this paper investigates how behavioralcloning can be used to decrease training timefor student learning to y on simulator the challenge presented to each studentmust be tailored to their unique learning experience this requires an intelligent trainingregime that exploit a model of each studentthat predicts where the student s performancewill be de cient here we show thatcloning the behavior of student pilot with amodular neural network result in the automatic 
reinforcement learning in nonstationary environment is generallyregarded a an important and yet difficult problem this paperpartially address the problem by formalizing a subclass of nonstationaryenvironments the environment model called hidden modemarkov decision process hm mdp assumes that environmentalchanges are always confined to a small number of hidden mode a mode basically index a markov decision process mdp andevolves with time according to a markov chain 
recently several author developed a new approach to bounding the generalization error of complex classifier of large or even infinite vc dimension obtained by combining simpler classifier the new bound are in term of the distribution of the margin of combined classifier and they provide some theoretical explanation of generalization performance of large neural network see bartlett ieee transaction on information theory a well a such technique of combining 
this paper describes a method to automate the diagnosis of student programming error in programming learning environment in order to recognize correct student program a well a to identify error in incorrect student program program are represented using an improved dependence graph representation the student program is compared with a specimen program also called a model program at the semantic level after both are standardized by program transformation the method is implemented using smalltalk in siples ii an automatic program diagnosis system for samlltalk programming learning environment the system ha been tested on approximately student program for various task experimental result show that using the method semantic error in a student program can be identified rigorously and safely semantics preserving variation in a student program can be eliminated or accommodated the test also show that the system can identify a wide range of error a well a produce indication of the correction needed this method is essential for the development of programming learning environment the technique of the improved program dependence graph representation program standardization by transformation and semantic level program comparison are also useful in other research field including program understanding and software maintenance 
we investigate a general characteristic of the trade o in learningproblems between goodness of t and model complexity speci cally we characterize a general class of learning problem where thegoodness of t function can be shown to be convex within rstorderas a function of model complexity this general propertyof diminishing return quot is illustrated on a number of real datasets and learning problem including nite mixture modeling andmultivariate linear regression 
this paper deal with a neural network architecture which establishes a portfolio management system similar to the black littermanapproach this allocation scheme distributes fund across various security or financial market while simultaneously complying with specific allocation constraint which meet the requirement of an investor the portfolio optimization algorithm is modeled by a feedforward neural network the underlying expected return forecast are based on error correction neural network ecnn which utilize the last model error a an auxiliary input to evaluate their own misspecification the portfolio optimization is implemented such that i the allocation comply with investor s constraint and that ii the risk of the portfolio can be controlled we demonstrate the profitability of our approach by constructing internationally diversified portfolio across different financial market of the g contries it turn out that our approach is superior to a preset benchmark portfolio 
m read is a prototype application implemented a an extension of the web browser that creates an evolving model of the user topic of interest it us that model to analyze document that are accessed while searching and browsing the web in the presented version of m read the model is used to highlight topic related terminology in the document m read model of the user need is created by applying natural language processing to search query captured within the browser and to topic description explicitly provided by the user while browsing and reading document it is semantically enhanced using linguistic and custom knowledge resource 
with the proliferation of the internet and the huge amount of data it transfer text summarization is becoming more important we present an approach to the design of an automatic text summarizer that generates a summary by extracting sentence segment first sentence are broken into segment by special cue marker each segment is represented by a set of predefined feature e g location of the segment average term frequency of the word occurring in the segment number of title word in the segment and the like then a supervised learning algorithm is used to train the summarizer to extract important sentence segment based on the feature vector result of experiment on u s patent indicate that the performance of the proposed approach compare very favorably with other approach including microsoft word summarizer in term of precision recall and classification accuracy 
a novel noise suppression scheme for speech signal is proposedwhich is based on a neurophysiologically motivated estimation ofthe local signal to noise ratio snr in dierent frequency channel 
this paper argues that the reuse of domain knowledge must be complemented by the reuse of problem solving method problem solving method psms provide a mean to structure search and can provide tractable solution to reasoning with a very large knowledge base we show that psms can be used in a way which complement large scale representation technique and optimisation such a those for taxonornie reasoning found in cyc our approach illustrates the advantage of task oriented knowledge modelling and we demonstrate that the resulting ontology have both task dependent and task independent element further we show how the task ontology can be organised into conceptual level to reflect knowledge typing principle 
we explore the use of genetic algorithm to directly evolve classification decision tree we argue on the suitability of such a concept learner due to it ability to efficiently search complex hypothesis space and discover conditionally dependent a well a irrelevant attribute the performance of the system is measured on a set of artificial and standard discretized concept learning problem and compared with the performance of two known algorithm c oner we demonstrate that the derived hypothesis of standard algorithm can substantially deviate from the optimum this deviation is partly because of their non universal procedural bias and it can be reduced using global metric of tree quality like the one proposed 
recently several approach for updating knowledge base represented a logic program have been proposed in this paper we present a generic framework for declarative specification of update policy which is built upon such approach it extends the lups language for update specification and incorporates the notion of event into the framework an update policy allows an agent to flexibly react upon new information arriving a an event and perform suitable change of it knowledge base the framework compiles update policy to logic program by mean of generic translation and can be instantiated in term of different concrete update approach it thus provides a flexible tool for designing adaptive reasoning agent 
resource co funded by several agent must be exploited in such a way that three kind of constraint are met physical problem hard constraint efficiency constraint aiming at maximizing the satisfaction of each agent a fairness constraint which is ideally satisfied when each agent receives an amount of the resource exactly proportional to it financial contribution this paper investigates a decision problem for which the common property resource is an earth observation satellite the problem is to decide on the daily selection of a subset of picture among a set of candidate picture which could be taken the next day considering the satellite trajectory this subset must satisfy the three kind of constraint stated above although fair division problem have received considerable attention for a long time especially from microeconomist this specific problem doe not fall entirely within a classical approach this is because the candidate picture may be incompatible and because a picture is only of value to the agent requesting it a in the general case efficiency and fairness constraint are antagonistic we propose three way for solving this share problem the first one give priority to fairness the second one to efficiency and the third one computes a set of compromise 
this paper considers a fundamental problem in visual motion perception namely the problem of egomotion estimation based on visual input many of the existing technique for solving this problem rely on restrictive assumption regarding the observer s motion or even the scene structure moreover they often resort to searching the high dimensional space of possible solution a strategy which might be inefficient in term of computational complexity and exhibit convergence problem if the search is initiated far away from the correct solution in this work a novel linear constraint that involves quantity that depend on the egomotion parameter is developed the constraint is defined in term of the optical flow vector pertaining to four collinear image point and is applicable regardless of the egomotion or the scene structure in addition it is exact in the sense that no approximation are made for deriving it combined with robust linear regression technique the constraint enables the recovery of the foe thereby decoupling the d motion parameter extensive simulation a well a experiment with real optical flow field provide evidence regarding the performance of the proposed method under varying noise level and camera motion 
abstract signal processing and pattern recognition algorithm make exten sive use of convolution in many case computational accuracy is not a important a computational speed in feature extraction for instance the feature of interest in a signal are usually quite distorted this form of noise justi e some level of quantization in order to achieve faster feature extraction our approach consists of approximating region of the signal with low degree polynomi al and then di erentiating the resulting signal in order to obtain impulse function or derivative of impulse function with this representation convolution becomes extremely simple and can be implemented quite e ectively the true convolution can be recov ered by integrating the result of the convolution this method yield substantial speed up in feature extraction and is applicable to convolutional neural network 
we show that it is possible to learn the context for linguistic operation which map a semantic representation to a surface syntactic tree in sentence realization with high accuracy we cast the problem of learning the context for the linguistic operation a classification task and apply straightforward machine learning technique such a decision tree learning the training data consist of linguistic feature extracted from syntactic and semantic representation produced by a linguistic analysis system the target feature are extracted from link to surface syntax tree our evidence consists of four example from the german sentence realization system code named case assignment assignment of verb position feature extraposition and syntactic aggregation 
we investigate the behavior of a hebbian cell assembly of spiking neuron formed via a temporal synaptic learning curve this learning function is based on recent experimental finding it includes potentiation for short time delay between preand post synaptic neuronal spiking and depression for spiking event occuring in the reverse order the coupling between the dynamic of the synaptic learning and of the neuronal activation lead to interesting result we find that the cell assembly 
in this work we approach the classic mumford shah problem from a curve evolution perspective in particular we let a given family of curve define the boundary between region in an image within which the data are modeled by piecewise smooth function plus noise a in the standard mumford shah functional the gradient descent equation of this functional is then used to evolve the curve each gradient descent step involves solving a corresponding optimal estimation problem which connects the mumford shah functional and our curve evolution implementation with the theory of boundary value stochastic process the resulting active contour model therefore inherits the attractive ability of the mumford shah technique to generate in a coupled manner both a smooth reconstruction of the image and a segmentation a well we demonstrate application of our method to problem in which data quality is spatially varying and to problem in which set of pixel measurement are missing finally we demonstrate a hierarchical implementation of our model which lead to a fast and efficient algorithm capable of dealing with important image feature such a triple point 
in this paper we present an algorithm for real time tracking of articulated structure in dense disparity map derived from stereo image sequence a statistical image formation model that account for occlusion play the central role in our tracking approach this graphical model a bayesian network assumes that the range image of each part of the structure is formed by drawing the depth candidate from a d gaussian distribution the advantage over the classical mixture of gaussians is that our model take into account occlusion by picking the minimum depth which could be regarded a a probabilistic version of z buffering the model also enforces articulation constraint among the part of the structure the tracking problem is formulated a an inference problem in the image formation model this model can be extended and used for other task in addition to the one described in the paper and can also be used for estimating probability distribution function instead of the ml estimate of the tracked parameter for the purpose of real time tracking we used certain approximation in the inference process which resulted in a real time two stage inference algorithm we were able to successfully track upper human body motion in real time and in the presence of self occlusion 
calibration is the degree to which an agent s probability estimate subjective probability correspond to actual frequency or the objective probability underlyingthem cognitive psychologist have studied human calibration a a part of theirprogram to investigate how human cognition deviate from the ideal in gambling one attempt to estimate the odds of an event so a to maximize return there aretwo major factor involved knowledge of the domain and the meta knowledge of the 
we present a framework for information retrieval that combine document model and query model using a probabilistic ranking function based on bayesian decision theory the framework suggests an operational retrieval model that extends recent development in the language modeling approach to information retrieval a language model for each document is estimated a well a a language model for each query and the retrieval problem is cast in term of risk minimization the query language model can be exploited to model user preference the context of a query synonomy and word sens while recent work ha incorporated word translation model for this purpose we introduce a new method using markov chain defined on a set of document to estimate the query model the markov chain method ha connection to algorithm from link analysis and social network the new approach is evaluated on trec collection and compared to the basic language modeling approach and vector space model together with query expansion using rocchio significant improvement are obtained over standard query expansion method for strong baseline tf idf system with the greatest improvement attained for short query on web data 
we examine mathematical model for semi supervised support vector machine s vm given a training set of labeled data and a working set of unlabeled data s vm construct a support vector machine using both the training and working set we use s vm to solve the transductive inference problem posed by vapnik in transduction the task is to estimate the value of a classificationfunction at the given point in the working set this contrast with inductive inference which estimate the classificationfunction at all possible value we propose a general s vm model that minimizes both the misclassification error and the function capacity based on all the available data depending on how poorly estimated unlabeled data are penalized different mathematical model result we examine several practical algorithm for solving these model the firstapproach utilizes the s vm model for norm linear support vector machine converted to a mixedinteger program mip a global solution of the mip is found using a commerical integer programming solver the second approach us a noncovex quadratic program variation of block coordinate descent algorithm are used to find local solution of this problem using this mip within a local learning algorithm produced the best result our experimental study on these statistical learning method indicates that incorporating working data can improve generalization 
we present a machine learning approach to evaluating the well formedness of output of a machine translation system using classifier that learn to distinguish human reference translation from machine translation this approach can be used to evaluate an mt system tracking improvement over time to aid in the kind of failure analysis that can help guide system development and to select among alternative output string the method presented is fully automated and independent of source language target language and domain 
in this paper we consider the problem of color constancy how given an image of a scene under an unknown illuminant can we recover an estimate of that light rather than recovering a single estimate of the illuminant a many previous author have done in the first instance we recover a measure of the likelihood that each possible illuminant wa the scene illuminant we do this by correlating image color with the color that can occur under each of a set of possible light we then recover an estimate of the scene illuminant based on these likelihood computation is expressed and performed in a generic correlation framework which we develop in this paper we develop a new probabilistic instantiation of this framework which delivers very good color constancy on synthetic and real image we show that the proposed framework is rich enough to allow many existing algorithm to be expressed within it e g the grey world and gamut mapping algorithm we explore too the relationship of these algorithm to other probabilistic and neural network approach 
this paper explores in detail the use of error correcting output coding ecoc for learning text classifier we show that the accuracy of a naive bayes classifier over text classification task can be significantly improved by taking advantage of the error correcting property of the code we also explore the use of different kind of code namely error correcting code random code and domain and data specific code and give experimental result for each of them the ecoc method scale well to large data set with a large number of class experiment on a real world data set show a reduction in classification error by up to over the traditional naive bayes classifier we also compare our empirical result to semi theoretical result and find that the two closely agree 
this paper present a new method for renderingviews especially those of large scale scene such asbroad city landscape the main contribution of ourmethod is that we are able to easily render any viewfrom an arbitrary point to an arbitrary direction on theground in a virtual environment our methodbelongsto the family of work that employ plenoptic function however unlike other work of this type this particularmethod allows u to render a novel view from almostany point on the 
we present method for learning and tracking human motion in video we estimate a statistical model of typical activity from a large set of d periodic human motion data by segmenting these data automatically into quot cycle quot then the mean and the principal component of the cycle are computed using a new algorithm that account for missing information and enforces smooth transition between cycle the learned temporal model provides a prior probability distribution over human motion that 
the deep layer of the superior colliculus dsc integrate multisensory input and initiate an orienting response toward the source of stimulation multisensory response enhancement mre is the augmentation of a neural response of a dsc neuron to sensory input of one modality by input of another modality the maximum likelihood model presented here extends the bayesian model for mre by anastasio et al by incorporating a decision strategy to maximize the number of correct decision it account for the inverse eectiveness observed in neurophysiological recording data and it predicts a functional relation between uniand bimodal level of discriminability that is testable both in neurophysiological and behavioral experiment 
in this paper we present an efficient hierarchical approach to structure from motion for long image sequence there are two key element to our approach accurate d reconstruction for each segment and efficient bundle adjustment for the whole sequence the image sequence is first divided into a number of segment so that feature point can be reliably tracked across each segment each segment ha a long baseline to ensure accurate d reconstruction to efficiently bundle adjust d structure from ail segment we reduce the number of frame in each segment by introducing virtual keyframes the virtual frame encode the d structure of each segment along with it uncertainty but they form a small subset of the original frame our method achieves significant speedup over conventional bundle adjustment method 
we consider the problem of maximizing thetotal number of success while learningabout a probability function determining thelikelihood of a success in particular weconsider the case in which the probabilityfunction is represented by a linear functionof the attribute vector associated with eachaction choice in the scenario we consider learning proceeds in trial and in each trial the algorithm is given a number of alternativesto choose from each having an attributevector 
motion is one of the important visual cue for scene analysis it is particularly useful when the scene is cluttered such a in typical home or office environment we present a motion segmentation algorithm that make use of temporal differencing to detect moving people in cluttered indoor scene the algorithm is devised based on a couple of perceptual organization principle to deal with missing data noise and outlier a robust segmentation and grouping technique called tensor voting is employed the resulting real time people detector can handle the presence of multiple person and varying body size and pose it requires no initialization us subjective threshold which defines the minimum saliency of significant motion and the only two parameter are the scale size of the local neighborhood for region and contour analysis 
we investigate applying theory renemen t to the task of extracting information from text in theory renemen t partial domain knowledge which may be incorrect is given to a supervised learner the provided knowledge guide the learner in it task but the learner can rene or even discard this knowledge during training our supervised learner is a knowledge based neural network that initially contains compiled prior knowledge about a particular information extraction ie task the prior knowledge need to specify the extraction slot for the specic ie task our approach us generate and test to address the ie task in the generation step we produce candidate extraction by intelligently searching the space of possible extraction in the test step we use the trained network to judge each candidate and output those that exceed a system selected threshold experiment on the cmu seminarannouncements and the yeast subcellularlocalization domain demonstrate our approach s value this paper we demonstrate how the theory renemen t approach e g towell shavlik can be used to build an ie system by using theory renemen t we are able to strike a balance between needing a large number of labeled example and having a complete and correct set of domain knowledge our system take advantage of the intuition that information retrieval ir and ie are nearly inverse problem of each other an ir system is given a set of keywords and is asked to rate the relevance of document an ie system is given a set of document and is asked to ll in the slot in a given template we explore how what is essentially an ir system can be used to address the ie task 
most lighting can be accurately modeled using a simplified planckian function if we form logarithm of color ratio of camera sensor value then in a lambertian plus specular two lobe model of reflection the temperaturedependent term is separate and is seen a a straight line i e changing lighting amount to changing each pixel value in a straight line for a given camera here we use a sensor camera in this case forming color ratio reduces the dimensionality to applying logarithm and projecting onto the plane in the d color space orthogonal to the lightchange direction result in an image representation that is invariant to illumination change for a given camera the position of the specular point in the d plane is always the same independent of the lighting thus a camera calibration produce illumination invariance at a single pixel in the plane matte surface reduce to point and specularities are almost straight line extending each pixel value back to the matte position postulated to be the maximum radius from the fixed specular point at any angle in the d plane remove specularity thus image are independent of shading by forming ratio independent of shadow by making them independent of illumination temperature and independent of specularities the method is examined by forming d image from hyperspectral image using real camera sensor with encouraging result 
this paper present method for a qualitative unbiased comparison of lexical association measure and the result we have obtained for adjective noun pair and preposition noun verb triple extracted from german corpus in our approach we compare the entire list of candidate sorted according to the particular measure to a reference set of manually identified true positive we also show how estimate for the very large number of hapaxlegomena and double occurrence can be inferred from random sample 
in this paper we present pva an adaptive personal view information agent system to track learn and manage user s interest in internet document when user s interest change pva in not only the content but also in the structure of user profile is modified to adapt to the change experimental result show that modulating the structure of user profile doe increase the accuracy of personalization system 
effort to improve web search facility call for improvedunderstanding of user characteristic we investigated the typesof knowledge that are relevant for web based informationseeking along with the knowledge structure and relatedstrategies in an exploratory field experiment establishedinternet expert were first interviewed about search strategiesand then performed a series of realistic search task on the a href http citeseer ist psu edu rd http aqsqqsqwww onmouseover self status http www return true onmouseout self status return true www a based on this preliminary study a model of informationsearching on 
narayanan and jurafsky proposed that human language comprehension can be modeled by treating human comprehenders a bayesian reasoner and modeling the comprehension process with bayesian decision tree in this paper we extend the narayanan and jurafsky model to make further prediction about reading time given the probability of difference par or interpretation and test the model against reading time data from a psycholinguistic experiment 
this paper deal with the concept of auto calibration i e method to calibrate a camera on line in particular w e deal with minimal condition on the intrinsic parameter needed to make a euclidean reconstruction called flexible calibration the main theoretical result are that it is onl y needed to know that one intrinsic parameter is constant the method is based on an initial projective reconstruction which is upgraded to a euclidean one the number of image needed increase with the complexity of the constraint but the number of point needed is only the number needed in order to obtain a projective reconstruction the theoretical result are exemplified in a number of experiment an algorithm based on bundle adjustment and a linear initialization method are presented and experiment s are performed on both synthetic and real data 
in this paper we investigate new approachesto dynamic programming based optimal controlof continuous time and space system weuse neural network to approximate the solutionto the hamilton jacobi bellman hjb equation which is in the deterministic casestudied here a first order non linear partialdifferential equation we derive the gradientdescent rule for integrating this equation insidethe domain given the condition on theboundary we apply this approach to the quot caron 
in this paper we argue that comparative evaluation in anaphora resolution ha to be performed using the same pre processing tool and on the same set of data the paper proposes an evaluation environment for comparing anaphora resolution algorithm which is illustrated by presenting the result of the comparative evaluation of three method on the basis of several evaluation measure 
linear relational embedding is a method of learning a distributed representation of concept from data consisting of binary relation between concept concept are represented a vector binary relation a matrix and the operation of applying a relation to a concept a a matrix vector multiplication that produce an approximation to the related concept a representation for concept and relation is learned by maximizing an appropriate discriminative goodness function using gradient 
we present a hybrid approach for the multidimensional knapsack problem the proposed approach combine linear programming and tabu search the resulting algorithm improves significantly on the best known result of a set of more than benchmark instance 
we present an approach to appearance based objectrecognition using single camera image our approachis based on using an attention mechanism to obtain visualfeatures that are generic robust and informative the feature themselves are recognized using principalcomponents in the frequency domain in this paper we show how the visual characteristicsof only a small number of such feature can be usedfor appearance based object recognition that is not confoundedby planar rotation or 
in this paper we describe improved alignmentmodels for statistical machine translation thestatistical translation approach us two typesof information a translation model and a lan guage model the language model used is abigram or general m gram model the translationmodel is decomposed into a lexical and analignment model we describe two different approachesfor statistical translation and presentexperimental result the first approach isbased on dependency between 
this paper present an active learning method that directly optimizes expected future error this is in contrast to many other popular technique that instead aim to reduce version space size these other method are popular because for many learning model closed form calculation of the expected future error is intractable our approach is made feasible by taking a sampling approach to estimating the expected reduction in error due to the labeling of a query in experimental result on two real world data set we reach high accuracy very quickly sometimes with four time fewer labeled example than competing method 
we consider the problem of how an agent creates a discrete spatial representation from it continuous interaction with the environment such representation will be the minimal one that explains the experience of the agent in the environment in this 
we develop an approach for a sparse representation for gaussian process gp model in order to overcome the limitation of gps caused by large data set the method is based on a combination of a bayesian online algorithm together with a sequential construction of a releva nt subsample of the data which fully specifies the prediction of the model experimental result on toy example and large real world dataset s indicate the efficiency of the approach 
the popular k mean clustering partition a data set by minimizing a sum of square cost function a coordinate descend method is then used to find local minimum in this paper we show that the minimization can be reformulated a a trace maximization problem associated with the gram matrix of the data vector furthermore we show that a relaxed version of the trace maximization problem posse global optimal solution which can be obtained by computing a partial eigendecomposition of the gram 
conceptual graph are very useful for representing structured knowledge however existing formulation of fuzzy conceptual graph are not suitable for matching image of natural scene this paper present a new variation of fuzzy conceptual graph that is more suited to image matching this variant differentiates between a model graph that describes a known scene and an image graph which describes an input image a new measurement is defined to measure how well a model graph match an image graph a fuzzy graph matching algorithm is developed based on error tolerant subgraph isomorphism test result show that the matching algorithm give very good result for matching image to predefined scene model 
an issue that is critical for the application of markov decision process mdps to realistic problem is how the complexity of planning scale with the size of the mdp in stochastic environment with very large or even infinite state space traditional planning and reinforcement learning algorithm are often inapplicable since their running time typically scale linearly with the state space size in this paper we present a new algorithm that given only a generative model simulator for an arbitrary mdp performs near optimal planning with a running time that ha no dependence on the number of state although the running time is exponential in the horizon time which depends only on the discount factor and the desired degree of approximation to the optimal policy our result establish for the first time that there are no theoretical barrier to computing near optimal policy in arbitrarily large unstructured mdps our algorithm is based on the idea of sparse sampling we prove that a randomly sampled look ahead tree that cover only a vanishing fraction of the full look ahead tree nevertheless suffices to compute near optimal action from any state of an mdp practical implementation of the algorithm are discussed and we draw tie to our related recent result on finding a near best strategy from a given class of strategy in very large partially observable mdps kmn 
this paper argues that developmental pattern in child language be taken seriously in computational model of language acquisition and proposes a formal theory that meet this criterion we first present developmental fact that are problematic for statistical learning approach which assume no prior knowledge of grammar and for traditional learnability model which assume the learner move from one ug defined grammar to another in contrast we view language acquisition a a population of grammar associated with weight that compete in a darwinian selectionist process selection is made possible by the variational property of individual grammar specifically their differential compatibility with the primary linguistic data in the environment in addition to a convergence proof we present empirical evidence in child language development that a learner is best modeled a multiple grammar in co existence and competition 
we present a novel method for clustering using the support vector machine approach data point are mapped to a high dimensional feature space where support vector are used to define a sphere enclosing them the boundary of the sphere form in data space a set of closed contour containing the data data point enclosed by each contour are defined a a cluster a the width parameter of the gaussian kernel is decreased these contour fit the data more tightly and splitting of contour occurs the algorithm work by separating cluster according to valley in the underlying probability distribution and thus cluster can take on arbitrary geometrical shape a in other sv algorithm outlier can be dealt with by introducing a soft margin constant leading to smoother cluster boundary the structure of the data is explored by varying the two parameter we investigate the dependence of our method on these parameter and apply it to several data set 
reformulating the costeira kanade algorithm a a pure mathematical theorem independent of the tomasi kanade factorization we present a robust segmentation algorithm by incorporating such technique a dimension correction model selection using the geometric aic and least median fitting doing numerical simulation we demonstrate that our algorithm dramatically outperforms existing method it doe not involve any parameter which need to be adjusted empirically 
technique for automatically training module of a natural language generator have recently been proposed but a fundamental concern is whether the quality of utterance produced with trainable component can compete with hand crafted template based or rule based approach in this paper we experimentally evaluate a trainable sentence planner for a spoken dialogue system by eliciting subjective human judgment in order to perform an exhaustive comparison we also evaluate a hand crafted template based generation component two rule based sentence planner and two baseline sentence planner we show that the trainable sentence planner performs better than the rule based system and the baseline and a well a the hand crafted system 
we investigate the formation of a hebbian cell assembly of spiking neuron usinga temporal synaptic learning curve that is based on recent experimental finding itincludes potentiation for short time delay between preand post synaptic neuronalspiking and depression for spiking event occurring in the reverse order the couplingbetween the dynamic of synaptic learning and that of neuronal activation lead tointeresting result one possible mode of activity is distributed synchrony 
general sfm method give poor result for image captured by constrained motion such a planar motion of concentric mosaic cm in this paper we propose new sfm algorithm for both image captured by cm and composite mosaic image from cm we first introduce d affine camera model for completing d camera model then we show that a d image captured by cm can be decoupled into two d image one d projective and one d affine a composite mosaic image can by rebinned into a calibrated d panorama projective camera finally we describe subspace reconstruction method and demonstrate both in theory and experiment the advantage of the decomposition method over the general sfm method by incorporating the constrained motion into the earliest stage of motion analysis 
we study online learning in boolean domain using kernel which capture feature expansion equivalent to using conjunction over basic feature we demonstrate a tradeoff between the computational efficiency with which these kernel can be computed and the generalization ability of the resulting classifier we first describe several kernel function which capture either limited form of conjunction or all conjunction we show that these kernel can be used to efficiently run the perceptron algorithm over an exponential number of conjunction however we also prove that using such kernel the perceptron algorithm can make an exponential number of mistake even when learning simple function we also consider an analogous use of kernel function to run the multiplicative update winnow algorithm over an expanded feature space of exponentially many conjunction while known upper bound imply that winnow can learn dnf formula with a polynomial mistake bound in this setting we prove that it is computationally hard to simulate winnow s behavior for learning dnf over such a feature set and thus that such kernel function for winnow are not efficiently computable 
this paper introduces an extension of hartley s selfcalibration technique based on property of the essential matrix allowing for the stable computation of varying focal length and principal point it is well known that the three singular value of an essential must satisfy two condition one of them must be zero and the other two must be identical an essential matrix is obtained from the fundamental matrix by a transformation involving the intrinsic parameter of the pair of camera associated with the two view thus constraint on the essential matrix can be translated into constraint on the intrinsic parameter of the pair of camera this allows for a search in the space of intrinsic parameter of the camera in order to minimize a cost function related to the constraint this approach is shown to be simpler than other method with comparable accuracy in the result another advantage of the technique is that it doe not require a input a consistent set of weakly calibrated camera matrix a defined by hartley for the whole image sequence i e a set of camera consistent with the correspondence and known up to a projective transformation 
the object of this paper is to find a quick and accuratemethod for computing the projection matricesof an image sequence so that the error is distributedevenly along the sequence it assumes that a set of correspondencesbetween point in the image is known and that these point represent rigid point in theworld this paper extends the algebraic minimisationapproach developed by hartley so that it can be usedfor long image sequence this is achieved by initiallycomputing a trifocal 
this paper describes a method of alignment between the noun part of wordnet and goi taikei s ontology the result of the automatic alignment show that matching word in japanese a well a in english is highly efficient in extracting correct match analysis of correct alignment by hand illustrate that in ontology alignment it is not always possible to preserve relative relation between concept such a hyponymy or antonymy 
robocup simulated soccer present many challenge to reinforcement learning method including a large state space hidden and uncertain state multiple agent and long and variable delay in the effect of action we describe our application of episodic smdp sarsa with linear tile coding function approximation and variable to learning higher level decision in a keepaway subtask of robocup soccer in keepaway one team the keeper try to keep control of the ball for a long a possible despite the effort of the taker the keeper learn individually when to hold the ball and when to pas to a teammate while the taker learn when to charge the ball holder and when to cover possible passing lane our agent learned policy that significantly out performed a range of benchmark policy we demonstrate the generality of our approach by applying it to a number of task variation including different field size and different number of player on each team 
this paper proposes a new model for gesture recognition the model called view and motion based aspect model vambam is an omnidirectional view based aspect model based on motion based segmentation this model realizes location free and rotation free gesture recognition with a distributed omnidirectional vision system dovs the distributed vision system consisting of multiple omnidirectional camera is a prototype of a perceptual information infrastructure for monitoring and recognizing the real world in addition to the concept of vabam this paper show how the model realizes robust and real time visual recognition of the dovs 
the encoding accuracy of a population of stochastically spiking neuron is studied for different distribution of their tuning widt h the situation of identical radially symmetric receptive field for all neu ron which is usually considered in the literature turn out to be disa dvantageous from an information theoretic point of view both a variabi lity of tuning width and a fragmentation of the neural population into specialized subpopulation improve the encoding accuracy 
given estimate of the motion field optic flow from an image sequence it is possible to recover translational direction t using a variety of technique one such technique known a subspace method generates constraint which are perpendicular to t so that two distinct constraint allow a solution for t in practice many constraint are used in a least square solution but it ha been observed that the recovered estimate for t are biased towards the optical axis while the cause of the bias is well known previous attempt to remove it have been awed this paper outline a new method which remove the bias the technique is simple to apply and computationally efficient 
recently lakemeyer and levesque proposed the logic aoc which amalgamates both the situation calculus and levesques logic of only knowing while very expressive the practical relevance of the formalism is unclear because it heavily relies on second order logic in this paper we demonstrate that the picture is not a bleak a it may seem in particular we show that for large class of aol knowledge base and query including epistemic one query evaluation requires first order reasoning only we also provide a simple semantic definition of progressing a knowledge base for a particular class of knowledge base adapted from earlier result by lin and reiter we show that progression is first order representable and easy to compute 
we describe a unified framework for the understanding of structure representation in primate vision a model derived from this framework is shown to be effectively systematic in that it ha the ability to interpret and associate together object that are related through a rearrangement of common middle scale part represented a image fragment the model address the same concern a previous work on compositional representation through the use of what where receptive field and attentional gain modulation it doe not require prior exposure to the individual part and avoids the need for abstract symbolic binding 
we present a statistical question answering system developed for trec in detail the system is an application of maximum entropy classification for question answer type prediction and named entity marking we describe our system for information retrieval which did document retrieval from a local encyclopedia and then expanded the query word and finally did passage retrieval from the trec collection we will also discus the answer selection algorithm which determines the best sentence given both the question and the occurrence of a phrase belonging to the answer class desired by the question a new method of analyzing system performance via a transition matrix is shown 
this paper considers three assumption conventionally made about signature in typed feature logic that are in potential disagreement with current practice among grammar developer and linguist working within feature based framework such a hpsg meet semi latticehood unique feature introduction and the absence of subtype covering it also discus the condition under which each of these can be tractably restored in realistic grammar signature where they do not already exist 
the marriage of renyi entropy with parzen density estimation ha been shown to be a viable tool in learning discriminative feature transforms however it suffers from computational complexity proportional to the square of the number of sample in the training data this set a practical limit to using large database we suggest immediate divorce of the two method and remarriage of renyi entropy with a semi parametric density estimation method such a a gaussian mixture model gmm this allows all of the computation to take place in the low dimensional target space and it reduces computational complexity proportional to square of the number of component in the mixture furthermore a convenient extension to hidden markov model a commonly used in speech recognition becomes possible 
we present a framework for d shape contour silhouette comparison that can account for stretching occlusion and region information topological change due to the original d scenario and articulation are also addressed to compare the degree of similarity between any two shape our approach is to represent each shape contour with a free tree structure derived from a shape axis sa model which we have recently proposed we then use a tree matching scheme to find the best approximate match and the matching cost to deal with articulation stretching and occlusion three local tree matching operation merge cut and merge and cut are introduced to yield optimally approximate match which can accommodate not only one to one but many to many mapping the optimization process give guaranteed globally optimal match efficiently experimental result on a variety of shape contour are provided 
the computer and telecommunication industry rely heavily on knowledge based expert system to manage the performance of their network these expert system are developed by knowledge engineer who must first interview domain expert to extract the pertinent knowledge this knowledge acquisition process is laborious and costly and typically is better at capturing qualitative knowledge than quantitative knowledge this is a liability especially for domain like the telecommunication domain where enormous amount of data are readily available for analysis data mining hold tremendous promise for the development of expert system for monitoring network performance since it provides a way of automatically identifying subtle yet important pattern in data this case study describes a project in which a temporal data mining system called timeweaver is used to identify faulty telecommunication equipment from log of network alarm message 
this paper address the problem of estimating time to collision from focal motion field measurement in the case of unconstrained relative rigid motion and surface orientation it is first observed that a long a time to collision is regarded a a scaled depth the above problem doe not admit a solution unless a narrow camera field of view is assumed by a careful generalization of the time to collision concept it is then expounded how to compute novel solution which hold however wide the field of view the formulation which reduces to known literature approach in the narrow field of view case extends the applicability range of time to collision based technique in area such a mobile robotics and visual surveillance the experimental validation of the main theoretical result includes a comparison of narrowand wide field of view time to collision approach using both dense and sparse motion estimate 
abstract we discus the relationship between probabilis tic logic and cm given a set of logical sen tences and their probability of being true the outcome of a probabilistic logic system con sists of lower and upper bound on the prob ability of an additional sentence to be true these bound are computed using a linear pro gramming formulation in cm system the outcome is defined by the probability of the support and the plausibility with the assump tions being independent after a first phase which consists of computing the prime implicants depending only on the variable of the assumption we propose to reformulate a cm system without independence condition on the assumption using the linear program ming framework of probabilisticlogic and show how to exploit it particular structure to solve it efficiently when an independence condition is imposed on the assumption the two system give different result comparison are made on small problem using the assumption basedev idential language program abel of anrig et a and the psat program of jaumard eta 
in constraint satisfaction problem csps value belonging to variable domain should be completely known before the constraint propagation process start in many application however the acquisition of domain value is a computational expensive process or some domain value could not be available at the beginning of the computation for this purpose we introduce an interactive constraint satisfaction problem icsp model a extension of the widely used csp model the variable domain value can be acquired when needed during the resolution process by mean of interactive constraint which retrieve possibly consistent information experimental result on randomly generated csps and for d object recognition show the effectiveness of the proposed approach 
this paper present a first investigation on the structure from motion problem from the combination of full and weak perspective image this problem arises in multiresolution object modeling where multiple zoomed in or close up view are combined with wider or distant reference view the narrow field of view fov image from the zoomed in or closeup view can be approximated a weak perspective projection using a full perspective projection model for the narrow fov image although more accurate actually lead to instability during the estimation process due to the non linearity in the imaging model the weak perspective approximation lead to more stable estimation algorithm although at the cost of a small amount of modeling inaccuracy previous work in structure from motion focused either on two or more perspective image or on a set of weak perspective more generally affine image the main contribution of this paper is the study of the sfm problem for the much neglected case of one perspective and one or more weak perspective image we show that in contrast to the case of a pair of weak perspective image there is adequate information to recover euclidean structure from a single perspective and a single weak perspective image the epipolar geometry is simpler than with two perspective image leading to simpler and more stable estimation algorithm computer simulation show that more stable result can be obtained with the technique presented in this paper than if two image are both considered to be full perspective 
we show that we can effectively and automatically fit a complex facial animation model to uncalibrated image sequence our approach is based on model driven bundleadjustment followed by least square fitting it take advantage of three complementary source of information stereo data silhouette edge and d feature point in this way complete head model can be acquired with a cheap and entirely passive sensor such a an ordinary video camera they can then be fed to existing animation software to produce synthetic sequence 
in this paper we show decidability of a rather expressive fragment of the situation calculus we allow second order quantification over finite and infinite set of situation we do not impose a domain closure assumption on action therefore infinite and even uncountable domain are allowed the decision procedure is based on automaton accepting infinite tree 
this paper present a control structure for general purpose image understanding that address both the high level of uncertainty in local hypothesis and the computational complexity of image interpretation the control of vision algorithm is performed by an independent subsystem that us bayesian network and utility theory to compute the marginal value of information provided by alternative operator and selects the one with the highest value we have implemented and tested this control structure with several aerial image data set the result show that the knowledge base used by the system can be acquired using standard learning technique and that the value driven approach to the selection of vision algorithm lead to performance gain moreover the modular system architecture simplifies the addition of both control knowledge and new vision algorithm 
this paper introduces a new framework for extending consistent domain of numeric csp the aim is to offer the greatest possible freedom of choice for one variable to the designer of a cad application thus we provide here an efficient and incremental algorithm which computes the maximal extension of the domain of one variable the key point of this framework is the definition for each inequality of an univariate extremum function which computes the left most and right most solution of a selected variable in a space delimited by the domain of the other variable we show how these univariate extremum function can be implemented efficiently the capability of this approach are illustrated on a ballistic example 
in this paper the score distribution of a number of text search engine are modeled it is shown empirically that the score distribution on a per query basis may be fitted using an exponential distribution for the set of non relevant document and a normal distribution for the set of relevant document experiment show that this model fit trec and trec data for not only probabilistic search engine like inquery but also vector space search engine like smart for english we have also used this model to fit the output of other search engine like lsi search engine and search engine indexing other language like chinese it is then shown that given a query for which relevance information is not available a mixture model consisting of an exponential and a normal distribution can be fitted to the score distribution these distribution can be used to map the score of a search engine to probability we also discus how the shape of the score distribution arise given certain assumption about word distribution in document we hypothesize that all good text search engine operating on any language have similar characteristic this model ha many possible application for example the output of different search engine can be combined by averaging the probability optimal if the search engine are independent or by using the probability to select the best engine for each query result show that the technique performs a well a the best current combination technique 
we consider a problem central in aerial visual surveil lance application detection and tracking of small independently moving object in long and noisy video sequence we directly use spatiotemporal image inten sity gradient measurement to compute an exact model of background motion this allows the creation of ac curate mosaic over many frame and the de nition of a constraint violation function which act a an in dicator of independent motion a novel temporal in tegration method maintains con dence measure over long subsequence without computing the optic ow re quiring object model or using a kalman lter the mosaic act a a stable feature frame allowing pre cise localization of the independently moving object we present a statistical analysis of the e ect of image noise on the constraint violation measure and nd a good match between the predicted probability distribu tion function and the measured sample frequency in a test sequence 
we present an algorithm for identifying linear mixture of a specified set of material in m airborne imaging spectrometer data the algorithm is invariant to the illumination and atmospheric condition and the relative amount of the specified material within a pixel only the spectral reflectance function for the specified material are required by the algorithm invariance over illumination and atmosphere condition is achieved by incorporating a physical model for scene variability in the constrained optimization formulation the algorithm also computes estimate of the amount of the specified material in identified mixture we demonstrate the effectiveness of the algorithm using real and synthetic hydice imagery acquired over a range of condition and altitude 
the mystery of belief propagation bp decoder especially of the turbo decoding is studied from information geometrical viewpoint the loopy belief network bn of turbo code make it difficult to obtain the true belief by bp and the characteristic of the algorithm and it equilibrium are not clearly understood our study give an intuitive understanding of the mechanism and a new framework for the analysis based on the framework we reveal basic property of the turbo decoding 
this paper present a novel method of generating and applying hierarchical dynamic topic based language model it proposes and evaluates new cluster generation hierarchical smoothing and adaptive topic probability estimation technique these combined model help capture long distance lexical dependency experiment on the broadcast news corpus show significant improvement in perplexity overall and on target vocabulary 
this article explores the advantage and one potential implementation of a new style of computation in which multiple line of symbolic processing are pursued at different speed within a hybrid multi agent system the cognitive architecture dual consists of small hybrid computational entity called dual agent each agent ha a symbolic processor capable of simple symbol manipulation there is also an activation level associated with each agent activation spread according to connectionist rule the speed of each symbolic processor is proportional to the activation level of the corresponding dual agent and varies dynamically thus multiple candidate solution to a given problem can be explored in parallel more computational resource are dedicated to the more promising candidate and the degree of promise is reevaluated dynamically this allows for flexible and efficient behavior of the system a a whole the exact relationship between symbolic speed and connectionist activation is based on an energetic analogy the symbolic processor is conceptualized a a machine converting connectionist activation into symbolic work a language for implementing variable speed symbol manipulation using delayed evaluation is introduced s llsp a small example from a dual based cognitive model illustrates variable speed marker passing in a semantic network 
we have designed and fabricated a vlsi synapse that can learn a conditional probability or correlation between spike based input and feedback signal the synapse is low power compact provides nonvolatile weight storage and can perform simultaneous multiplication and adaptation we can calibrate array of synapsis to ensure uniform adaptation characteristic finally adaptation in our synapse doe not necessarily depend on the signal used for computation consequently our synapse can implement learning rule that correlate past and present synaptic activity we provide analysis and experimental chip result demonstrating the operation in learning and calibration mode and show how to use our synapse to implement various learning rule in silicon 
we present an extension to the mixture of expert me model where the individual expert are gaussian process gp regression model using an input dependent adaptation of the dirichlet process we implement a gating network for an infinite number of expert inference in this model may be done efficiently using a markov chain relying on gibbs sampling the model allows the effective covariance function to vary with the input and may handle large datasets thus potentially overcoming two of the biggest hurdle with gp model simulation show the viability of this approach 
previous study considered quality optimization of anytime algorithm by taking into account the quality of the final result the problem we are interested in is the maximization of the average quality of a contract algorithm over a time interval we first informally illustrate and motivate this problem with few concrete situation then we prove that the problem is nphard but quadratic if the time interval is large enough eventually we give empirical result 
extracting rule from rbfs is not a trivial task because of nonlinear function or high input dimensionality in such case some of the hidden unit of the rbf network have a tendency to be shared across several output class or even may not contribute 
many state of the art heuristic planner derive their heuristic function by relaxing the planning task at hand where the relaxation is to assume that all delete list are empty looking at a collection of planning benchmark we measure topological property of state space with respect to that relaxation the result suggest that given the heuristic based on the relaxation many planning benchmark are simple in structure this shed light on the recent success of heuristic planner employing local search 
gt y sign ha x i where ha x i is the posterior mean ha x i ea x q ti p y i ja x i eq ti p y i ja x i e is the expectation over the gp prior and t is the number of example exact parametrisationmean and covariance of the posterior can be expressed a ha x i txi k x x lt f 
condensation a form of likelihood weighted particle filte ring ha been successfully used to infer the shape of highly constrained active contour in video sequence however when the contour are highly flexible e g for tracking finger of a hand a computationally burdensom e number of particle is needed to successfully approximate the c ontour distribution we show how the metropolis algorithm can be used to update a particle set representing a distribution over contour at e ach frame in a video sequence we compare this method to condensation using a video sequence that requires highly flexible contour and show th at the new algorithm performs dramatically better that the condensat ion algorithm we discus the incorporation of this method into the active contour framework where a shape subspace is used constrain shape variation 
a pulsed ladar based object recognition system with application to automatic target recognition atr is presented the approach used is to fit the sensed range image to range template extracted through a laser physic based simulation applied to geometric target model a projection based prescreener filter out more than of candidate template for recognition an m of n pixel matching scheme for internal shape matching is combined with a silhouette matching scheme the system wa trained on synthetic data obtained from the simulation and ha been blind tested on a data set containing real ladar image of military vehicle at various orientation and range successful blind testing on real imagery demonstrates the utility of synthetic imagery for training of recognizers operating on ladar imagery 
this paper considers the hypothesis that system learning aspect of visual perception may benefit from the use of suitably designed developmental progression during training we report the result of simulation in which three different artificial neural network model were trained to detect binocular disparity in pair of visual image two of the model were developmental model in the sense that the nature of their training input changed during the course of training either a 
the traditional representation of game using the extensive form or the strategic normal form obscure much of the structure that is present in real world game in this paper we propose a new representation language for general multiplayer game multi agent influence diagram maid this representation extends graphical model for probability distribution to a multi agent decision making context maid explicitly encode structure involving the dependence relationship among variable a a consequence we can define a notion of strategic relevance of one decision variable to another is strategically relevant to if to optimize the decision rule at the decision maker need to take into consideration the decision rule at we provide a sound and complete graphical criterion for determining strategic relevance we then show how strategic relevance can be used to detect structure in game allowing a large game to be broken up into a set of interacting smaller game which can be solved in sequence we show that this decomposition can lead to substantial saving in the computational cost of finding nash equilibrium in these game 
the conventional wisdom is that backprop net with excess hidden unit generalize poorly we show that net with excess capacity generalize well when trained with backprop and early stopping experiment suggest two reason for this overfitting can vary significant ly in different region of the model excess capacity allows better fit to reg ion of high non linearity and backprop often avoids overfitting the re gions of low non linearity regardless of size net learn task subco mponents in similar sequence big net pas through stage similar to th ose learned by smaller net early stopping can stop training the large n et when it generalizes comparably to a smaller net we also show that conjugate gradient can yield worse generalization because it overfits region of low non linearity when learning to fit region of high non linea rity 
a new method is described for automatically reconstructing d planar face from multiple image of a scene the novelty of the approach lie in the use of inter image homographies to validate and best estimate the plane and in the minimal initialization requirement only a single d line with a textured neighbourhood is required to generate a plane hypothesis the planar facet enable line grouping and also the construction of part of the wireframe which were missed due to the inevitable shortcoming of feature detection and matching the method allows a piecewise planar model of a scene to be built completely automatically with no user intervention at any stage given only the image and camera projection matrix a input the robustness and reliability of the method are illustrated on several example from both aerial and interior view 
in this paper we present an object recognition framework integrating several recognition paradigm and context information from the scene history to recognize elementary part contained in assembly we use a symbolic approach to detect action based on the object change in the scene to monitor the construction process the information about the element used to construct a new assembly serf a additional source of information for recognition process knowledge is exploited also for selecting the best interpre tation out of several alternative for a single scene which result from contradiction and uncertainty during integ ration of the different cue 
it ha long been known that lateral inhibition in neural network can lead to a winner take all competition so that only a single neuron is active at a steady state here we show how to organize lateral inhibition so that group of neuron compete to be active given a collection of potentially overlapping group the inhibitory connectivity is set by a formula that can be interpreted a arising from a simple learning rule our analysis demonstrates that such inhibition generally result in 
extracting rule from rbfs is not a trivial task because of nonlinear function or high input dimensionality in such case some of the hidden unit of the rbf network have a tendency to be shared across several output class or even may not contribute 
abstract collocational word similarity is considered a source of text cohesion that is hard to measure and quantify the work presented here explores the use of information from a training corpus in measuring word similarity and evaluates the method in the text segmentation task an implementation the vectile system produce similarity curve over text using pre compiled vector representation of the contextual behavior of word the performance of this system is shown to improve over that of the purely string based texttiling algorithm hearst background the notion of text cohesion rest on the intuition 
this paper present an extended system epdl of propositional dynamic logic by allowing a proposition a a modality for representing and specifying direct and indirect effect of action in a unified logical structure a set of causal logic based on the framework are proposed to model causal propagation through logical relevancy and iterated effect of causation it is shown that these logic capture the basic property of causal reasoning 
drawing on the correspondence between the graph laplacian thelaplace beltrami operator on a manifold and the connection tothe heat equation we propose a geometrically motivated algorithmfor constructing a representation for data sampled from a low dimensionalmanifold embedded in a higher dimensional space thealgorithm provides a computationally efficient approach to nonlineardimensionality reduction that ha locality preserving propertiesand a natural connection to 
a good decoding algorithm is critical to the success of any statistical machine translation system the decoder s job is to find the translation that is most likely according to set of previously learned parameter and a formula for combining them since the space of possible translation is extremely large typical decoding algorithm are only able to examine a portion of it thus risking to miss good solution in this paper we compare the speed and output quality of a traditional stack based decoding algorithm with two new decoder a fast greedy decoder and a slow but optimal decoder that treat decoding a an integer programming optimization problem 
this paper deal with the way dual genetic algorithm dga an extension of the standard one explore the search space after a brief introduction presenting genetic algorithm and dualism the fitness distance correlation is discussed in the context of dualism from this discussion a conjecture is made about the genetic heuristic used by dual genetic algorithm to explore the search space this conjecture is reinforced by the visualization of the population centroid trajectory in the plane fitness distance these trajectory help to point out leg up behavior which allow the dual genetic algorithm to reach the global optimum from walk on deceptive path 
in many type of technical text meaning is embedded in noun compound a language understanding program need to be able to interpret these in order to ascertain sentence meaning we explore the possibility of using an existing lexical hierarchy for the purpose of placing word from a noun compound into category and then using this category membership to determine the relation that hold between the noun in this paper we present the result of an analysis of this method on two word noun compound from the biomedical domain obtaining classification accuracy of approximately since lexical hierarchy are not necessarily ideally suited for this task we also pose the question how far down the hierarchy must the algorithm descend before all the term within the subhierarchy behave uniformly with respect to the semantic relation in question we find that the topmost level of the hierarchy yield an accurate classification thus providing an economic way of assigning relation to noun compound 
a new method for computing precise depth map estimate of d shape of a moving object is proposed d shape recovery in motion stereo is formulated a a matching optimization problem of multiple stereo image the proposed method is a heuristic modification of dynamic programming applied to two dimensional optimization problem d shape recovery using real motion stereo image demonstrates a good performance of the algorithm in term of reconstruction accuracy 
we present a new model for studying multitasklearning linking theoretical result topractical simulation in our model all tasksare combined in a single feedforward neuralnetwork learning is implemented in abayesian fashion in this bayesian frameworkthe hidden to output weight beingspecific to each task play the role of modelparameters the input to hidden weight which are shared between all task aretreated a hyperparameters other hyperparametersdescribe error 
recently tremendous advance have been made in the performance of ai planning system however increased performance is only one of the prerequisite for bringing planning into the realm of real application advance in the scope of problem that can be represented and solved must also be made in this paper we address two important representational feature concurrently executable action with varying duration and metric quantity like resource both essential for modeling real application we show how the forward chaining approach to planning can be extended to allow it to solve planning problem with these two feature forward chaining using heuristic or domain specific information to guide search ha shown itself to be a very promising approach to planning and it is sensible to try to build on this success in our experiment we utilize the tlplan approach to planning in which declaratively represented control knowledge is used to guide search we show that this extra knowledge can be intuitive and easy to obtain and that with it impressive planning performance can be achieved 
we present a syntax based statistical translation model our model transforms a source language parse tree into a target language string by applying stochastic operation at each node these operation capture linguistic difference such a word order and case marking model parameter are estimated in polynomial time using an em algorithm the model produce word alignment that are better than those produced by ibm model 
a hybrid system is described which combine the strength of manual rule writing and statistical learning obtaining result superior to both method if applied separately the combination of a rule based system and a statistical one is not parallel but serial the rule based system performing partial disambiguation with recall close to is applied first and a trigram hmm tagger run on it result an experiment in czech tagging ha been performed with encouraging result 
we explore how active learning with support vector machine work well for a non trivial task in natural language processing we use japanese word segmentation a a test case in particular we discus how the size of a pool affect the learning curve it is found that in the early stage of training with a larger pool more labeled example are required to achieve a given level of accuracy than those with a smaller pool in addition we propose a novel technique to use a large number of unlabeled example effectively by adding them gradually to a pool the experimental result show that our technique requires le labeled example than those with the technique in previous research to achieve accuracy the proposed technique need of labeled example that are required when using the previous technique and only of labeled example with random sampling 
web document hierarchical classification approach often rely on textual feature alone even though web page include multimedia data we propose a new hierarchical integrated web classification approach that combine image based and text based approach instead of using a flat classifier to combine text and image classification we perform classification on a hierarchy differently on different level of the tree using text for branch and image only at leaf the result of our experiment show that the use of the hierarchical structure improved web document classification performance significantly 
we introduce a novel method of constructing language model which avoids some of the problem associated with recurrent neuralnetworks the method of creating a prediction fractal machine pfm is briefly described and some experiment are presentedwhich demonstrate the suitability of pfms for language modeling pfms distinguish reliably between minimal pair and their behavioris consistent with the hypothesis that wellformedness is graded not absolute a discussion of 
signature can be acquired with a camera basedsystem with enough resolution to perform verification this paper present the performance of avisual acquisition signature verification system emphasizingon the importance of the parameterizationof the signature in order to achieve good classificationresults a technique to overcome the lack ofexamples in order to estimate the generalization errorof the algorithm is also described introduction and motivationone of the research area 
bayesian belief network bbns have become accepted and used widely to model uncertain reasoning and causal relationship we have developed an interactive visualization tool visnet that allows student and or teacher to inspect bbns using visnet it is possible to experiment with concept such a marginal probability change in probability probability propagation and cause effect relationship in bbns using visualization technique vismod visualization of bayesian student model an extended version of visnet open the internal representation of the student s knowledge to teacher and or student interested in knowing more about the knowledge about them represented in the system both visnet and vismod aim to support reflection process in learning environment that rely on the use of bayesian model 
the use of the human hand a a natural interface device serf a a motivating force for research in the modeling analysis and capture of the motion of an articulated hand model based hand motion capture can be formulated a a large nonlinear programming problem but this approach is plagued by local minimum an alternative way is to use analysis by synthesis by searching a huge space but the result are rough and the computation expensive in this paper articulated hand motion is decoupled a new two step iterative model based algorithm is proposed to capture articulated human hand motion and a proof of convergence of this iterative algorithm is also given in our proposed work the decoupled global hand motion and local finger motion are parameterized by the d hand pose and the state of the hand respectively hand pose determination is formulated a a least median of square lm problem rather than the nonrobust least square l problem so that d hand pose can be reliably calculated even if there are outlier local finger motion is formulated a an inverse kinematics problem a genetic algorithm based method is proposed to find a sub optimal solution of the inverse kinematics effectively our algorithm and the l based algorithm are compared in several experiment both algorithm converge when local finger motion between consecutive frame is small when large finger motion is present the l based method fails but our algorithm can still estimate the global and local finger motion well 
this paper present a method for inducing translation lexicon based on transduction model of cognate pair via bridge language bilingual lexicon within language family are induced using probabilistic string edit distance model translation lexicon for arbitrary distant language pair are then generated by a combination of these intra family translation model and one or more cross family on line dictionary up to exact match accuracy is achieved on the target vocabulary of inter family test pair thus substantial portion of translation lexicon can be generated accurately for language where no bilingual dictionary or parallel corpus may exist 
this is a survey of some theoretical result onboosting obtained from an analogous treatmentof some regression and classificationboosting algorithm some related paper include j and j a b c d which is a setof mutually overlapping paper concerningthe assumption of weak hypothesis behaviorof generalization error in the large time limitand during the process of boosting comparisonto the optimal bayes error in noisy situation overfitting and regularization 
image based and model based method are two representative rendering method for generating virtual image of object from their real image extensive research on these two method ha been made in cv and cg community however both method still have several drawback when it come to applying them to the mixed reality where we integrate such virtual image with real background image to overcome these difficulty we propose a new method which we refer to a the eigen texture method the proposed method sample appearance of a real object under various illumination and viewing condition and compress them in the d coordinate system defined on the d model surface the d model is generated from a sequence of range image the eigen texture method is practical because it doe not require any detailed reflectance analysis of the object surface and ha great advantage due to the accurate d geometric model this paper describes the method and report on it implementation 
competitive learning is a technique for training classification and clustering network we have designed and fabricated an transistor primitive that we term an automaximizing bump circuit that implement competitive learning dynamic the circuit performs a similarity computation affords nonvolatile storage and implement simultaneous local adaptation and computation we show that our primitive is suitable for implementing competitive learning in vlsi and demonstrate it effectiveness in a standard clustering task 
in this paper we derive a probabilistic model for recognition based on local descriptor and spatial relation between these descriptor our model take into account the variability of local descriptor their saliency a well a the probability of spatial configuration it is structured to clearly separate the probability of point wise correspondence from the spatial coherence of set of correspondence for each descriptor of the query image several correspondence in the image database 
we describe a method for learning an overcomplete set of basisfunctions for the purpose of modeling sparse structure in image the sparsity of the basis function coefficient is modeled with amixture of gaussians distribution one gaussian capture nonactivecoefficients with a small variance distribution centered atzero while one or more other gaussians capture active coefficientswith a large variance distribution we show that when the prior isin such a form there exist 
this paper present a new system called the asystem performing abductive reasoning within the framework of abductive logic programming it is based on a hybrid computational model that implement the abductive search in term of two tightly coupled process a reduction process of the highlevel logical representation to a lower level constraint store and a lower level constraint solving process a set of initial proof of principle experiment demonstrate the versatility of the approach stemming from it declarative representation of problem and the good underlying computational behaviour of the system the approach offer a general methodology of declarative problem solving in ai where an incremental and modular refinement of the high level representation with extra domain knowledge can improve and scale the computational performance of the framework 
recently there have been a number of algorithm proposed for analyzing hypertext link structure so a to determine the best authority for a given topic or query while such analysis is usually combined with content analysis there is a sense in which some algorithm are deemed to be more balanced and others more focused we undertake a comparative study of hypertext link analysis algorithm guided by some experimental query we propose some formal criterion for evaluating and comparing link analysis algorithm 
function approximation is essential to reinforcement learning but the standard approach of approximating a value function and determining a policy from it ha so far proven theoretically intractable in this paper we explore an alternative approach in which the policy is explicitly represented by it own function approximator independent of the value function and is updated according to the gradient of expected reward with respect to the policy parameter williams s reinforce method and actor critic method are example of this approach our main new result is to show that the gradient can be written in a form suitable for estimation from experience aided by an approximate action value or advantage function using this result we prove for the first time that a version of policy iteration with arbitrary differentiable function approximation is convergent to a locally optimal policy large application of reinforcement learning rl require the use of generalizing function approximators such neural network decision tree or instance based method the dominant approach for the last decade ha been the value function approach in which all function approximation effort go into estimating a value function with the action selection policy represented implicitly a the greedy policy with respect to the estimated value e g a the policy that selects in each state the action with highest estimated value the value function approach ha worked well in many application but ha several limitation first it is oriented toward finding deterministic policy whereas the optimal policy is often stochastic selecting different action with specific probability e g see singh jaakkola and jordan second an arbitrarily small change in the estimated value of an action can cause it to be or not be selected such discontinuous change have been identified a a key obstacle to establishing convergence assurance for algorithm following the value function approach bertsekas and tsitsiklis for example q learning sarsa and dynamic programming method have all been shown unable to converge to any policy for simple mdps and simple function approximators gordon baird tsitsiklis and van roy bertsekas and tsitsiklis this can occur even if the best approximation is found at each step before changing the policy and whether the notion of best is in the mean squared error sense or the slightly different sens of residual gradient temporal difference and dynamic programming method in this paper we explore an alternative approach to function approximation in rl 
in this paper we describe an approach to model selection in unsupervised learning thisapproach determines both the feature set and the number of cluster to this end we first derivean objective function that explicitly incorporates this generalization we then evaluate twoschemes for model selection one using this objective function a bayesian estimation schemethat selects the best model structure using the marginal or integrated likelihood and the secondbased on a technique using a 
when designing a two alternative classifier one ordinarily aim to maximize the classifier s ability to discriminate between member of the two class we describe a situation in a real world business application of machine learning prediction in which an additional constraint is placed on the nature of the solution that the classifier achieve a specified correct acceptance or correct rejection rate i e that it achieve a fixed accuracy on member of one class or the other our domain is predicting churn in the telecommunication industry churn refers to customer who switch from one service provider to another we propose four algorithm for training a classifier subject to this domain constraint and present result showing that each algorithm yield a reliable improvement in performance although the improvement is modest in magnitude it is nonetheless impressive given the difficulty of the problem and the financial return that it achieves to the service provider when designing a classifier one must specify an objective measure by which the classifier s performance is to be evaluated one simple objective measure is to minimize the number of misclassifications if the cost of a classification error depends on the target and or response class one might utilize a risk minimization framework to reduce the expected loss a more general approach is to maximize the classifier s ability to discriminate one class from another class e g chang lippmann an roc curve green swets can be used to visualize the discriminative performance of a two alternative classifier that output class posterior to explain the roc curve a classifier can be thought of a making a positive negative judgement a to whether an input is a member of some class two different accuracy measure can be obtained from the classifier the accuracy of correctly identifying an input a a member of the class a correct acceptance or ca and the accuracy of correctly identifying an input a a nonmember of the class a correct rejection or cr to evaluate the ca and cr rate it is necessary to pick a threshold above which the classifier s probability estimate is interpreted a an accept and below which is interpreted a a reject call this the criterion the roc curve plot ca against cr rate for various criterion figure a note that a the threshold is lowered the ca rate increase and the cr rate decrease for a criterion of the ca rate approach and the cr rate for a criterion of the ca rate approach 
human have an innate ability to perceive symmetry but it is not obvious how to automatethis powerful insight in this paper the mathematical theory of frieze and wallpaper group isused to extract visually meaningful building block motif from a repeated pattern a novelpeak detection algorithm based on quot region of dominance quot is used to automatically detect theunderlying translational lattice of a repeated pattern following automatic classification of thepattern s symmetry group 
coreference resolution involves finding antecedent for anaphoric discourse entity such a definite noun phrase but many definite noun phrase are not anaphoric because their meaning can be understood from general world knowledge e g the white house or the news medium we have developed a corpus based algorithm for automatically identifying definite noun phrase that are non anaphoric which ha the potential to improve the efficiency and accuracy of coreference resolution system our algorithm generates list of non anaphoric noun phrase and noun phrase pattern from a training corpus and us them to recognize non anaphoric noun phrase in new text using muc terrorism news article a the training corpus our approach achieved recall and precision at identifying such noun phrase in text document 
in this paper we extend the rao blackwellised particle filtering method to more complex hybrid model consisting of gaussian latent variable and discrete observation this is accomplished by augmenting the model with artificial variable that enable u to apply rao blackwellisation other improvement include the design of an optimal importance proposal distribution and being able to swap the sampling an selection step to handle outlier we focus on sequential binary classifier that 
we present three way of combining linear programming with the kernel trick to find value function approximation for reinforcement learning one formulation is based on svm regression the second is based on the bellman equation and the third seek only to ensure that good move have an advantage over bad move all formulation attempt to minimize the number of support vector while fitting the data experiment in a difficult synthetic maze problem show that all three formulation give 
this paper examines the use of generic summary for indexing in information retrieval our main observation are that with or without pseudo relevance feedback a summary index may be a effective a the corresponding fulltext index forprecision oriented search of highly relevant document but a reasonably sophisticated summarizer using a compression ratio of is desirable for this purpose in pseudo relevance feedback using a summary index at initial search and a fulltext index at final search is possibly effective for precision oriented search regardless of relevance level this strategy is significantly more effective than the one using the summary index only and probably more effective than using summary a mere term selection filter the use of summary a mere term selection filter the summary quality is probably not a critical factor for this strategy for this strategy the summary quality is probably not a critical factor and a compression ratio of appears best 
domain specific search engine are becoming increasingly popular because they offer increased accuracy and extra feature not possible with general web wide search engine unfortunately they are also difficult and time consuming to maintain this paper proposes the use of machine learning technique to greatly automate the creation and maintenance of domain specific search engine we describe new research in reinforcement learning text classification and information extraction that enables efficient spidering populates topic hierarchy and identifies informative text segment using these technique we have built a demonstration system a search engine for computer science research paper available at www cora justrcsettrch com 
we combine the replica approach from statistical physic with a variational approach to analyze learning curve analytically we apply the method to gaussian process regression a a main result we derive approximative relation between empirical error measure the generalization error and the posterior variance 
we present a method to automatically generate a concise summary by identifying and synthesizing similar element across related text from a set of multiple document our approach is unique in it usage of language generation to reformulate the wording of the summary 
based on an abstract framework for nonmonotonic reasoning bondarenko et at have extended the logic programming semantics of admissible and preferred argument to other nonmonotonic formalism such a circumscription autoepisternic logic and default logic although the new semantics have been tacitly assumed to mitigate the computational problem of nonmonotonic reasoning under the standard semantics of stable extension it seems questionable whether they improve the worst case behaviour a a matter of fact we show that credulous reasoning under the new semantics in propositional logic programming and prepositional default logic ha the same computational complexity a under the standard semantics furthermore sceptical reasoning under the admissibility semantics is easier since it is trivialised to monotonic reasoning finally sceptical reasoning under the preferability semantics is harder than under the standard semantics 
in packet switch packet queue at switch input and contend for output the contention arbitration policy directly affect switch performance the best policy depends on the current state of the switch and current traffic pattern this problem is hard because the state space possible transition and set of action all grow exponentially with the size of the switch we present a reinforcement learning formulation of the problem that decomposes the value function into many small independent value function and enables an efficient action selection 
everybody know that neural network need more than a single layer of nonlinear unit to compute interesting function we show that this is false if one employ winner take all a nonlinear unit any boolean function can be computed by a single winner takeall unit applied to weighted sum of the input variable any continuous function can be approximated arbitrarily well by a single soft winner take all unit applied to weighted sum of the input variable only positive weight are needed in these linear weighted sum this may be of interest from the point of view of neurophysiology since only of the synapsis in the cortex are inhibitory in addition it is widely believed that there are special microcircuit in the cortex that compute winner take all our result support the view that winner take all is a very useful basic computational unit in neural vlsi it is wellknown that winner take all of input variable can be computed very efficiently with transistor and a total wire length and area that is linear in in analog vlsi lazzaro et al we show that winner take all is not just useful for special purpose computation but may serve a the only nonlinear unit for neural circuit with universal computational power we show that any multi layer perceptron need quadratically in many gate to compute winner take all for input variable hence winner take all provides a substantially more powerful computational unit than a perceptron at about the same cost of implementation in analog vlsi 
in linguistics the semantic relation between word in a sentence are accounted for inter alia a the assignment of thematic role e g agent instrument etc a in predicate logic simple linguistic expression are decomposed into one predicate often the verb and it argument the predicate assigns thematic role to the argument so that each sentence ha a thematic grid a structure with all thematic role assigned by the predicate in order to reveal the thematic grid of a sentence a system called htrp hybrid thematic role processor is proposed in which the connectionist architecture ha a input a featural representation of the word of a sentence and a output it thematic grid both a random initial weight version riw and a biased initial weight version biw are proposed to account for system without and with initial knowledge respectively in biw initial connection weight reflect symbolic rule for thematic role for both version after supervised training a set of final symbolic rule is extracted which is consistently correlated to linguistic symbolic knowledge in the case of biw this amount to a revision of the initial rule in riw symbolic rule seem to be induced from the connectionist architecture and training 
the core experiment ce shape for shape descriptor performed for the mpeg standard gave a unique opportunity to compare various shape descriptor for non rigid shape with a single closed contour there are two main difference with respect to other comparison result reported in the literature for each shape descriptor the experiment were carried out by an institute that is in favor of this descriptor this implies that the parameter for each system were optimally determined and the implementation were throughly tested it wa possible to compare the performance of shape descriptor based on totally different mathematical approach a more theoretical comparison of these descriptor seems to be extremely hard in this paper we report on the mpeg core experiment ce shape 
in proc of ieee int l conf on computer vision vancouver canada visual tracking could be treated a a parameter estimation problem of target representation based on observation in image sequence a richer target representation would incur better chance of successful tracking in cluttered and dynamic environment however the dimensionality of target s state space also increase making tracking a formidable estimation problem in this paper the problem of tracking and integrating multiple cue is formulated in a probabilistic framework and represented by a factorized graphical model structured variational analysis of such graphical model factorizes different modality and suggests a co inference process among these modality a sequential monte carlo algorithm is proposed to give an efficient approximation of the co inference based on the importance sampling technique this algorithm is implemented in real time at around hz specifically tracking both position shape and color distribution of a target is investigated in this paper our extensive experiment show that the proposed algorithm performs robustly in a large variety of tracking scenario the approach presented in this paper ha the potential to solve other sensor fusion problem 
many parameter estimation method used in computer vision are able to utilise covariance information describing the uncertainty of data measurement this paper considers the value of this information to the estimation process when applied to measured image point location covariance matrix are first described and a procedure is then outlined whereby covariance may be associated with image feature located via a measurement process an empirical study is made of the condition under which covariance information enables generation of improved parameter estimate also explored is the extent to which the noise should be anisotropic and inhomogeneous if improvement are to be obtained over covariance free method critical in this is the devising of synthetic experiment under which noise condition can be precisely controlled given that covariance information is in itself subject to estimation error test are also undertaken to determine the impact of imprecise covariance information upon the quality of parameter estimate finally an experiment is carried out to ass the value of covariance in estimating the fundamental matrix from real image 
parametric ordinary differential equation arise in many area of science and engineering since some of the data is uncertain and given by interval traditional numerical method do not apply interval method provide a way to approach these problem but they often suffer from a loss in precision and high computation cost this paper present a constraint satisfaction approach that enhances interval method with a pruning step based on a global relaxation of the problem theoretical and experimental evaluation show that the approach produce significant improvement in accurracy and or efficiency over the best interval method 
recently we presented a new approach to the classification problem arising in data mining it is based on the regularization network approach but in contrast to other method which employ ansatz function associated to data point we use a grid in the usually high dimensional feature space for the minimization process to cope with the curse of dimensionality we employ sparse grid thus only o hn nd instead of o hn d grid point and unknown are involved here d denotes the dimension of the feature space and hn n give the mesh size we use the sparse grid combination technique where the classification problem is discretized and solved on a sequence of conventional grid with uniform mesh size in each dimension the sparse grid solution is then obtained by linear combination in contrast to our former work where d linear function were used we now apply linear basis function based on a simplicial discretization this allows to handle more dimension and the algorithm need le operation per data point we describe the sparse grid combination technique for the classification problem give implementational detail and discus the complexity of the algorithm it turn out that the method scale linearly with the number of given data point finally we report on the quality of the classifier built by our new method on data set with up to dimension it turn out that our new method achieves correctness rate which are competitive to that of the best existing method 
domain specific search engine are becoming increasingly popular because they offer increased accuracy and extra feature not possible with general web wide search engine unfortunately they are also difficult and time consuming to maintain this 
polarized dependency pd grammar are proposed a a mean of efficient treatment of discontinuous construction pd grammar describe two kind of dependency local explicitly derived by the rule and long implicitly specified by negative and positive valency of word if in a pd grammar the number of non saturated valency in derived structure is bounded by a constant then it is weakly equivalent to a cf grammar and ha a o n time parsing algorithm it happens that such bounded pd grammar are strong enough to express such phenomenon a unbounded raising extraction and extraposition 
in memory consolidation declarative memory which initially require the hippocampus for their recall ultimately become independent of it consolidation ha been the focus of numerous experimental and qualitative modeling study but only little quantitative exploration we present a consolidation model in which hierarchical connection in the cortex that initially instantiate purely semantic information acquired through probabilistic unsupervised learning come to instantiate episodic information a well the hippocampus is responsible for helping complete partial input pattern before consolidation is complete while also training the cortex to perform appropriate completion by itself 
full panoramic image covering degree can be created either by using panoramic camera or by mosaicing together many regular image creating panoramic view in stereo where one panorama is generated for the left eye and another panorama is generated for the right eye is more problematic earlier attempt to mosaic image from a rotating pair of stereo camera faced severe problem of parallax and of scale change a new family of multiple viewpoint image projection the circular projection is developed two panoramic image taken using such projection can serve a a panoramic stereo pair a system is described to generates a stereo panoramic image using circular projection from image or video taken by asingle rotating camera the system work in real time on a pc it should be noted that the stereo image are created without computation of d structure and the depth effect are created only in the viewer s brain 
in order to increase retrieval precision some new search engine provide manually verified answer to frequently asked query faq an underlying task is the identification of faq this paper describes our attempt to cluster similar query according to their content a well a user log our preliminary result show that the resulting cluster provide useful information for faq identification 
a new model named boolean latent semantic indexing model based on the singular value decomposition and boolean query formulation is introduced while the singular value decomposition alleviates the problem of lexical matching in the traditional information retrieval model boolean query formulation can help user to make precise representation of their information search need retrieval experiment on a number of test collection seem to show that the proposed model achieves substantial performance gain over the latent semantic indexing model 
this paper describes a new method for visualizing complex information space a painted image scientific visualization convert data into picture that allow viewer to see trend relationship and pattern we introduce a formal definition of the correspondence between traditional visualization technique and painterly style from the impressionist art movement this correspondence allows u to apply perceptual guideline from visualization to control the presentation of information in a computer generated painting the result is an image that is visually engaging but that also allows viewer to rapidly and accurately explore and analyze the underlying data value we conclude by applying our technique to a collection of environmental and weather reading to demonstrate it viability in a practical real world visualization environment 
in an immersive tele presence environment a d remote real scene is projected from the viewpoint of the local user this d world is acquired through stereo reconstruction at the remote site in this paper we start a performance analysis of stereo algorithm with respect to the task of immersive visualization a opposed to usual monocular image based rendering we are also interested in the depth error in novel view because our rendering is stereoscopic we describe an evaluation test bed which provides a world wide first available set of registered dense ground truth laser data and image data from multiple view we establish metric for novel depth view that reflect discrepancy both in the image and in d space it is well known that stereo performance is affected by both erroneous matching a well a incorrect depth triangulation we experimentally study the effect of occlusion and low texture on the distribution of the error metric then we algebraically predict the behavior of depth and novel projection error a a function of the camera set up and the error in the disparity these are first step towards building a laboratory for psychophysical judgement of depth estimate which is the ultimate performance test of tele presence stereo 
this paper address the rule based po tagging method of brill and question the importance of rule interaction to it performance adopting two assumption that serve to exclude rule interaction during tagging and training we arrive at some variant of brill s approach that are instance of decision list model these model allow for both rapid training on large data set and rapid tagger execution giving tagging accuracy that is comparable to or better than the brill method 
we describe a new algorithm for computing a nash equilibrium in graphical game a compact representation for multi agent system that we introduced in previous work the algorithm is the first to compute equilibrium both efficiently and exactly for a non trivial class of graphical game 
current computer based design tool for mechanical engineer are not tailored to the early stage of design most design start a pencil and paper sketch and are entered into cad system only when nearly complete our goal is to create a kind of magic paper capable of bridging the gap between these two stage we want to create a computer based sketching environment that feel a natural a sketching on paper but unlike paper understands a mechanical engineer s sketch a it is drawn one important step toward realizing this goal is resolving ambiguity in the sketch determining for example whether a circle is intended to indicate a wheel or a pin joint and doing this a the user draw so that it doesn t interfere with the design process we present a method and an implemented program that doe this for freehand sketch of simple d mechanical device 
image taken with wide angle camera tend to have severe distortion which pull point towards the optical center this paper proposes a method for recovering the disrortion parameter without the use of any calibration object the distortion cause straight line in the scene to appear a curve in the image our algorithm seek tojind the distortion parameter that would map the image curve to straight line the user selects a small set of point along the image curve recovery of the parameter is formulated a the minimization of an objective function which is designed to explicitly account for noise in the selected image point experimental result are presented for synthetic data with difserent noise level a well a for real image once calibrated the image stream from these camera can be undistorted in real time using look up table we also present an application of this calibration method for wide angle camera cluster which we call polycameras we apply our distortion correction technique to a polycamera with four wide angle camera to create a high resolution degree panorama in real time 
this paper describes a supervised learning method to automatically select from a set of noun phrase embedding proper name of different semantic class their most distinctive feature the result of the learning process is a decision tree which classifies an unknown proper name on the basis of it context of occurrence this classifier is used to estimate the probability distribution of an out of vocabulary proper name over a tagset this probability distribution is itself used to estimate the parameter of a stochastic part of speech tagger 
we present a method of searching text collection that take advantage of hierarchrical information within document and integrates search of structured and unstructured data we show that multidimensional database mdb designed for accessing data along hierarchical dimension are effective for information retrieval we demonstrate a method of using on line analytic processing olap technique on a text collection this combine traditional information retrieval and the slicing dicing drill down and roll up of olap we demonstrate use of a prototype for searching document from the trec collection 
particle filter are used for hidden state estimation with nonlinear dynamical system the inference of d human motion is a natural application given the nonlinear dynamic of the body and the nonlinear relation between state and image observation however the application of particle filter ha been limited to case where the number of state variable is relatively small because the number of sample needed with high dimensional problem can be prohibitive we describe a filter that us hybrid monte carlo hmc to obtain sample in high dimensional space it us multiple markov chain that use posterior gradient to rapidly explore the state space yielding fair sample from the posterior we find that the hmc filter is several thousand time faster than a conventional particle filter on a d people tracking problem 
we present a system to detect passenger car in aerial image where car appear a small object we pose this a a d object recognition problem to account for the variation in viewpoint and the shadow we started from psychological test to find important feature for human detection of car based on these observation we selected the boundary of the car body the boundary of the front windshield and the shadow a the feature some of these feature are affected by the intensity of the car and whether or not there is a shadow along it this information is represented in the structure of the bayesian network that we use to integrate all feature experiment show very promising result even on some very challenging image 
this paper present a statistical method for fingerprinting text in a large collection of independently written document each text is associated with a fingerprint which should be different from all the others if fingerprint are too close then it is suspected that passage of copied or similar text occur in two document our method exploit the characteristic distribution of word trigram and measure to determine similarity are based on set theoretic principle the system wa developed using a corpus of broadcast news report and ha been successfully used to detect plagiarism in student work it can find small section that are similar a well a those that are identical the method is very simple and effective but seems not to have been used before 
in this paper we derive a second order mean field theory for di rected graphical probability model by using an information theoretic argument it is shown how this can be done in the absense of a partition function this method is a direct generalisation of the well known tap approximation for boltzmann machine in a numerical example it is shown that the method greatly improves the first order mean fie ld approximation for a restricted class of graphical model so called single overlap graph the second order method ha comparable complexity to the first order method for sigmoid belief network the meth od is shown to be particularly fast and effective 
the selection of kernel parameter is an open problem in the training of nonlinear support vector machine the usual selection criterion is the quotient of the radius of the smallest sphere enclosing the training feature and the margin width empirical study on real world data using gaussian and polynomial kernel show that the test error due to this criterion is often much larger than the minimum test error in other word this criterion can be suboptimal or inadequate hence we propose augmenting the usual criterion with a traditional measure of class separability in statistical feature selection this measure employ the within class and betweenclass scatter in feature space which is equivalent to computing the pooled covariance matrix trace and the distance between class mean we show empirically that the new criterion result in improved generalization 
extracting rule from rbfs is not a trivial task because of nonlinear function or high input dimensionality in such case some of the hidden unit of the rbf network have a tendency to be shared across several output class or even may not contribute 
this paper present an algorithmic approach to the problem of detecting independently moving object in d scene that are viewed under camera motion there are two fundamental constraint that can be exploited for the problem two multiview camera motion constraint for instance the epipolar trilinear constraint and shape constancy constraint previous approach to the problem either use only partial constraint or rely on dense correspondence or flow we employ both the fundamental constraint in an algorithm that doe not demand a priori availability of correspondence or flow our approach us the plane plus parallax decomposition to enforce the two constraint it is also demonstrated that for a class of scene called sparse d scene in which genuine parallax and independent motion may be confounded how the plane plus parallax decomposition allows progressive introduction and verification of the fundamental constraint result of the algorithm on some difficult sparse d scene are promising 
this paper examines the application of reinforcement learning to a wireless communication problem the problem requires that channel utility be maximized while simultaneously minimizing battery usage we present a solution to this multi criterion problem that is able to significantly reduce power consumption the solution us a variable discount factor to capture the effect of battery usage 
we present a generalization of an incremental statistical parsing algorithm that allows for the re scoring of lattice of word hypothesis for use by a speech recognizer this approach contrast with other lattice parsing algorithm which either do not provide score for string in the lattice i e they just produce parse tree or use search technique e g a star to find the best path through the lattice without re scoring every arc we show that a very large efficiency gain can be had in processing best list without reducing word accuracy when the list are encoded in lattice instead of tree further this allows for processing arbitrary lattice without n best extraction this can lead to more interesting method of combination with other model both acoustic and language through for example adaptation or confusion matrix 
outlier detection is an important task in data mining with numerous application including credit card fraud detection video surveillance etc a recent work on outlier detection ha introduced a novel notion of local outlier in which the degree to which an object is outlying is dependent on the density of it local neighborhood and each object can be assigned a local outlier factor lof which represents the likelihood of that object being an outlier although the concept of local outlier is a useful one the computation of lof value for every data object requires a large number of kgr nearest neighbor search and can be computationally expensive since most object are usually not outlier it is useful to provide user with the option of finding only n most outstanding local outlier i e the top n data object which are most likely to be local outlier according to their lofs however if the pruning is not done carefully finding top n outlier could result in the same amount of computation a finding lof for all object in this paper we propose a novel method to efficiently find the top n local outlier in large database the concept of micro cluster is introduced to compress the data an efficient micro cluster based local outlier mining algorithm is designed based on this concept a our algorithm can be adversely affected by the overlapping in the micro cluster we proposed a meaningful cut plane solution for overlapping data the formal analysis and experiment show that this method can achieve good performance in finding the most outstanding local outlier 
extracting rule from rbfs is not a trivial task because of nonlinear function or high input dimensionality in such case some of the hidden unit of the rbf network have a tendency to be shared across several output class or even may not contribute 
this article compare search effectiveness when using query based internet search via the google search engine directory based search via yahoo and phrase based query reformulation assisted search via the hyperindex browser by mean of a controlled user based experimental study the focus wa to evaluate aspect of the search process cognitive load wa measured using a secondary digit monitoring task to quantify the effort of the user in various search state independent relevance judgement were employed to gauge the quality of the document accessed during the search process time wa monitored in various search state result indicated the directory based search doe not offer increased relevance over the query based search with or without query formulation assistance and also take longer query reformulation doe significantly improve the relevance of the document through which the user must trawl versus standard query based internet search however the improvement in document relevance come at the cost of increased search time and increased cognitive load 
kb consistency form the class of strong consistency used in interval constraint programming we survey prove and give theoretical motivation to some technical improvement to a naive kbconsistency algorithm our contribution is twofold on the one hand we introduce an optimal bconsistency algorithm whose time complexity of o md n improves the known bound by a factor n m is the number of constraint n is the number of variable and d is the maximal size of the interval of the box on the other hand we prove that improved bound on time complexity can effectively be reached for higher value of k these result are obtained with very affordable overhead in term of space complexity 
the human figure exhibit complex and rich dynamic behavior that is both nonlinear and time varying however most work on tracking and synthesizing figure motion ha employed either simple generic dynamic model or highly specific hand tailored one recently a broad class of learning and inference algorithm for time series model have been successfully cast in the framework of dynamic bayesian network dbns this paper describes a novel dbn based switching linear dynamic system slds model and present it application to figure motion analysis a key feature of our approach is an approximate viterbi inference technique for overcoming the intractability of exact inference in mixed state dbns we present experimental result for learning figure dynamic from video data and show promising initial result for tracking interpolation synthesis and classification using learned model 
abstract this paper is concerned with the problem of detecting outlier from unlabeled data in prior work we have developed smartsifter which is an on line outlier detection algorithm based on unsupervised learning from data on the basis of smartsifter this paper yield a new framework for outlier filtering using both supervised and unsupervised learning technique iteratively in order to make the detection process more effective and more understandable the outline of the framework is a follows in the first round for an initial dataset we run smartsifter to give each data a score with a high score indicating a high possibility of being an outlier next giving positive label to a number of higher scored data and negative label to a number of lower scored data we create labeled example then we construct an outlier filtering rule by supervised learning from them here the rule is generated based on the principle of minimizing extended stochastic complexity in the second round for a new dataset we filter the data using the constructed rule then among the filtered data we run smartsifter again to evaluate the data in order to update the filtering rule applying of our framework to the network intrusion detection we demonstrate that it can significantly improve the accuracy of smartsifter and outlier filtering rule can help 
we consider the problem of how an agent creates a discrete spatial representation from it continuous interaction with the environment such representation will be the minimal one that explains the experience of the agent in the environment in this 
redundancy reduction on the basis of the second order statisticsof natural image ha been very successful in accounting for thepsychophysics of low level vision here we study the second orderstatistics of natural sound ensemble using principal componentanalysis pca their eigen spectrum exhibit a finite size scalingbehavior a a function of the window size with universality afterthe millisecond range in contrast with natural scene auditory spectrum do not universally 
this paper present a method for designing bundle in a combinatorial auction protocol that is robust against false name bid internet auction have become an integral part of electronic commerce and a promising field for applying ai technology however the possibility of a new type of cheating called a false name bid i e a bid submitted under a fictitious name ha been pointed out a protocol called leveled division set lds protocol that is robust against false name bid ha been developed however this protocol requires the auctioneer to define a leveled division set a leveled division set is a series of division set where a division set is a set of division and a division is a combination of bundle of good we need to solve a very complicated optimization problem to construct a leveled division set in order to obtain a good social surplus we have developed a heuristic method for overcoming this problem in this method we first find a good division with a winner determination algorithm and then construct a leveled division set by using this division a a seed through a simulation we showthat our method can obtain a social surplus that is very close to optimal 
background subtraction is a method typically used to segment moving region in image sequence taken from a static camera by comparing each new frame to a model of the scene background we present a novel non parametric background model and a background subtraction approach the model can handle situation where the background of the scene is cluttered and not completely static but contains small motion such a tree branch and bush the model estimate the probability of observing pixel intensity value based on a sample of intensity value for each pixel the model adapts quickly to change in the scene which enables very sensitive detection of moving target we also show how the model can use color information to suppress detection of shadow the implementation of the model run in real time for both gray level and color imagery evaluation show that this approach achieves very sensitive detection with very low false alarm rate 
this paper describes a language independent method for alignment of parallel text that make use of homograph token for each pair of language in order to filter out token that may cause misalignment we use confidence band of linear regression line instead of heuristic which are not theoretically supported this method wa originally inspired on work done by pascale fung and kathleen mckeown and melamed providing the statistical support those author could not claim 
although several attempt have been made to introduce natural language processing nlp technique in information retrieval most one failed to prove their effectiveness in increasing performance in this paper text classification tc ha been taken a the ir task and the effect of linguistic capability of the underlying system have been studied a novel model for tc extending a well know statistical model i e rocchio s formula ittner et al and applied to linguistic feature ha been defined and experimented the proposed model represents an effective feature selection methodology all the experiment result in a significant improvement with respect to other purely statistical method e g yang thus stressing the relevance of the available linguistic information moreover the derived classifier reach the performance about of the best known model i e support vector machine svm and k nearest neighbour knn characterized by an higher computational complexity for training and processing 
a method to compute motion model in real time from point to line correspondence using linear programming is presented point to line correspondence are the most reliable motion measurement given the aperture effect and it is shown how they can approximate other motion measurement a well using an l error measure for image alignment based on point to line correspondence and minimizing this measure using linear programming achieves result which are more robust than the commonly used l metric while estimator based on l are not theoretically robust experiment show that the proposed method is robust enough to allow accurate motion recovery in hundred of consecutive frame the entire computation is performed in real time on a pc with no special hardware 
large calibrated datasets of random natural image have recently become available these make possible precise and intensive statistical study of the local nature of image we report result ranging from the simplest single pixel intensity to joint distribution of haar wavelet response some of these statistic shed light on old issue such a the near scale invariance of image statistic and some are entirely new we fit mathematical model to some of the statistic and explain others in term of local image feature 
this paper introduces a method to calibrate a wide area system of unsynchronized camera with respect to a single global coordinate system the method is simple and doe not require the physical construction of a large calibration object the user need only wave an identifiable point i n front of all camera the method generates a rough estimate of camera po se by first performing pa ir wise structure from motion on observed point and then combining the pair wise registration into a single c oordinate frame using the initial camera pose the moving point can be tracked in world space the path o f t he point defines a virtual calibration ob ject which can b e used to improve the initial estimate of camera po se iterating the above process yield a more precise estimate of both camera pose and the point path experimental result s how that it performs a well a calibration from a ph ysical t arget in case where all camera s hare some c ommon working volume we then demonstrate it effectiveness in wide area setting by calibrating a system of camera in a configuration where traditional method cannot be applied directly 
the feature quantity a quantitative representation of specificity introduced in this paper is based on an information theoretic perspective of co occurrence event between term and document mathematically the feature quantity is defined a a product of probability and information and maintains a good correspondence with the tfidf like measure popularly used in today s ir system in this paper we present a formal description of the feature quantity a well a some illustrative example of applying such a quantity to different type of information retrieval task representative term selection and text categorization 
we present a model based monitoring method for dynamic system that exhibit both discrete and continuous behavior mimic dvorak and kuiper us qualitative and semiquantitative model to monitor dynamic system even with incomplete knowledge recent advance have improved the quality of semi quantitative behavior prediction used observation to refine static envelope around monotonic function and provided a semiquantitative system identification method using these we reformulate and extend mimic to handle discontinuous change between model each hypothesis being monitored is embodied a a tracker which us the observation stream to refine it behavioral prediction it underlying model and the time uncertainty of any discontinuous transition 
this paper investigates the relationship between resolution and tableau proof system for the satisfiability of general knowledge base in the description logic alc we show that resolution proof system can polynornially simulate their tableau counterpart our resolution proof system is based on a selection refinement and utilises standard redundancy elimination criterion to ensure termination 
abstract there ha been much recent interest in retrieval of time series data earlier work ha used a fixed similarity metric e g euclidean distance to determine the similarity between a userspecified query and item in the database here we describe a novel approach to retrieval of time series data by using relevance feedback from the user to adjust the similarity metric this is important because the euclidean distance metric doe not capture many notion of similarity between time series in particular euclidean distance is sensitive to various distortion such a offset translation amplitude scaling etc depending on the domain and the user one may wish a query to be sensitive or insensitive to these distortion to varying degree this paper address this problem by introducing a profile that encodes the user s subjective notion of similarity in a domain these profile can be learned continuously from interaction with the user we further show how the user profile may be embedded in a system that us relevance feedback to modify the query in a manner analogous to the familiar text retrieval algorithm keywords time series multimedia data relevance feedback modeling user subjectivity 
we introduce total wire length a salient complexity measure for an analysis of the circuit complexity of sensory processing in biological neural system and neuromorphic engineering furthermore we introduce a set of basic computational problem that apparently need to be solved by circuit for translationand scale invariant sensory processing finally we exhibit a number of circuit design strategy for these new benchmark function that can be implemented within realistic complexity bound in particular with linear or almost linear total wire length 
we present a general method for proving tractability of reasoning over disjunction of jointly exhaustive and pairwise disjoint relation example of these kind of relation are allen s temporal interval relation and their spatial counterpart the r cc relation by randell cui and colin applying this method doe not require detailed knowledge about the considered relation instead it is rather sufficient to have a subset of the considered set of relation for which path consistency is known to decide consistency using this method we give a complete classification of tractability of reasoning over rcc by identifying two large new maximal tractable subset and show that these two subset together with h the already known maximal tractable subset are the only such set for rcc that contain all base relation we also apply our method to allen s interval algebra and derive the known maximal tractable subset 
hard disk drive failure are rare but are often costly the ability to predict failure is important to consumer drive manufacturer and computer system manufacturer alike in this paper we investigate the ability of two bayesian method to predict disk drive failure based on measurement of drive internal condition we firstview the problem from an anomaly detection stance we introduce a mixture model of naive bayes submodels i e cluster that is trained using expectation maximization the second method is a naive bayes classifier a supervised learning approach both method are tested on realworld data concerning drive the predictive accuracy of both algorithm is far higher than the accuracy of thresholding method used in the disk drive industry today 
recovering the projection geometry of an x ray system or an augmented reality video see through head mounted display hmd are mathematically quite similar recent work in both medical imaging and augmented reality use external optical sensor in order to recover the motion of the imaging system in this paper we take the example of the recovery of an x ray projection geometry we show that the mathematical problem which need to be solved is equivalent to the hand eye calibration well studied in both computer vision and robotics community we present a comparative study for the recovery of the motion and therefore projection geometry using ve dierent hand eye calibration method proposed in the literature we compare the motion estimation result using expensive external stereo based tracking system with one obtained by using an integrated optical camera the paper concludes by showing that even if the motion estimation is more accurate when using an external sensor the projection geometry is better estimated by the integrated optical camera these result are of crucial importance to both medical imaging and augmented reality community 
this paper present a method for obtaining class membership probability estimate for multiclass classification problem by coupling the probability estimate produced by binary classifier this is an extension for arbitrary code matrix of a method due to hastie and tibshirani for pairwise coupling of probability estimate experimental result with boosted naive bayes show that our method produce calibrated class membership probability estimate while having similar classification accuracy a loss based decoding a method for obtaining the most likely class that doe not generate probability estimate 
we propose a statistical method that find the maximum probability segmentation of a given text this method doe not require training data because it estimate probability from the given text therefore it can be applied to any text in any domain an experiment showed that the method is more accurate than or at least a accurate a a state of the art text segmentation system 
we present a dynamic programming approach for the solution of first order markov decision process this technique us an mdp whose dynamic is represented in a variant of the situation calculus allowing for stochastic action it produce a logical description of the optimal value function and policy by constructing a set of first order formula that minimally partition state space according to distinction made by the value function and policy this is achieved through the use of an operation known a decision theoretic regression in effect our algorithm performs value iteration without explicit enumeration of either the state or action space of the mdp this allows problem involving relational fluents and quantification to be solved without requiring explicit state space enumeration or conversion to propositional form 
we introduce static index pruning method that significantly reduce the index size in information retrieval system we investigate uniform and term based method that each remove selected entry from the index and yet have only a minor effect on retrieval result in uniform pruning there is a fixed cutoff threshold and all index entry whose contribution to relevance score is bounded above by a given threshold are removed from the index in term based pruning the cutoff threshold is determined for each term and thus may vary from term to term we give experimental evidence that for each level of compression term based pruning outperforms uniform pruning under various measure of precision we present theoretical and experimental evidence that under our term based pruning scheme it is possible to prune the index greatly and still get retrieval result that are almost a good a those based on the full index 
we consider the problem of designing a linear transformation of rank which project the feature of a classifier onto such a to achieve minimum bayes error or probability of misclassification two avenue will be explored the first is to maximize the average divergence between the class density and the second is to minimize the union bhattacharyya bound in the range of while both approach yield similar performance in practice they outperform standard lda feature and show a relative improvement in the word error rate over state of the art cepstral feature on a large vocabulary telephony speech recognition task 
computer modeling and simulation are indispensable for understanding the functioning of an organism on a molecular level we present an implemented method for the qualitative simulation of large and complex genetic regulatory network the method allows a broad range of regulatory interaction between gene to be represented and ha been applied to the analysis of a real network of biological interest the network controlling the inititation of sporulation in the bacterium b subtilis 
the question of whether the nervous system produce movement through the combination of a few discrete element ha long been central to the study of motor control muscle synergy i e coordinated pattern of muscle activity have been proposed a possible building block here we propose a model based on combination of muscle synergy with a specific amplitude and temporal structure time varying synergy provide a realistic basis for the decomposition of the complex pattern observed in natural behavior to extract time varying synergy from simultaneous recording of emg activity we developed an algorithm which extends existing non negative matrix factorization technique 
we use well established result in biological vision to construct a novel vision model for handwritten digit recognition we show empirically that the feature extracted by our model are linearly separable over a large training set mnist using only a linear classifier on these feature our model is relatively simple yet outperforms other model on the same data set 
this paper address the problem of building an interruptible real time system using contract algorithm contract algorithm offer a trade off between computation time and quality of result but their run time must be determined when they are activated many ai technique provide useful contract algorithm that are not interruptible we show how to optimally sequence contract algorithm to create the best interruptible system with or without stochastic information about the deadline these result extend the foundation of real time problem solving and provide useful guidance for embedding contract algorithm in application 
we describe an empirical study of an adaptive hierarchical vision system using a simplevision task requiring both low level andhigh level processing we examined how threeschemes of feedback for on line learning affectedthe true positive rate the number ofinstances used for learning and the need foruser feedback the rst scheme used forlearning those instance for which the userprovided feedback the second used all instance assuming that no feedback meantcorrect 
condensation is a popular algorithm for sequential inference that resamples a sampled representation of the posterior the algorithm is known to be asymptotically correct a the number of sample tends to infinity however the resampling phase involves a loss of information the sequence of representation produced by the algorithm is a markov chain which is usually inhomogeneous we show simple discrete example where this chain is homogeneous and ha absorbing state in these example the representation move to one of these state in time apparently linear in the number of sample and remains there this phenomenon appears in the continuous case a well where the algorithm tends to produce clumpy representation in practice this mean that different run of a tracker on the same data can give very different answer while a particular run of the tracker will look stable furthermore the state of the tracker can collapse to a single peak which ha non zero probability of being the wrong peak within time linear in the number of sample and the tracker can appear to be following tight peak in the posterior even in the absence of any meaningful measurement this mean that if theoretical lower bound on the number of sample are not available experiment must be very carefully designed to avoid these effect 
we present a new approach to the supervised learning of lateral interaction for the competitive layer model clm dynamic feature binding architecture the method is based on consistency condition which were recently shown to characterize the attractor state of this linear threshold recurrent network for a given set of training example the learning problem is formulated a a convex quadratic optimization problem in the lateral interaction weight an efficient dimension reduction of the learning problem can be achieved by using a linear superposition of basis interaction we show the successful application of the method to a medical image segmentation problem of fluorescence microscope cell image 
a fundamental unsolved vision problem is to distinguish image intensity variation caused by surface normal variation from those caused by reflectance change ie to tell shading from paint a solution to this problem is necessary for machine to interpret image a people do and could have many application the labelling allows u to reconstruct bandpassed image containing only those part of the input image caused by shading effect and a separate image containing only those part caused by reflectance change the resulting classification compare well with human psychophysical performance on a test set of image and show good result for test photograph 
we formulate stereo matching a an extremal surface extraction problem this is made possible by embedding the disparity surface inside a volume where the surface is composed of voxels with locally maximal similarity value this formulation naturally implement the coherence principle and allows u to incorporate most known global constraint time efficiency is achieved by executing the algorithm in a coarse to fine fashion and only populating the full volume at the coarsest level to make the system more practical we present a rectification algorithm based on the fundamental matrix avoiding full camera calibration we present result on standard stereo pair and on our own data set the result are qualitatively evaluated in term of both the generated disparity map and the d model 
we propose a method of image segmentation by integrating pairwise attraction and directional repulsion derived from local grouping and figure ground cue these two kind of pairwise relationship are encoded in the real and imaginary part of an hermitian graph weight matrix through which we can directly generalize the normalized cut criterion with bi graph construction this method can be readily extended to handle nondirectional repulsion that capture dissimilarity we demonstrate the use of repulsion in image segmentation with relative depth cue which allows segmentation and figure ground segregation to be computed simultaneously a a general mechanism to represent the dual measure of attraction and repulsion this method can also be employed to solve other constraint satisfaction and optimization problem 
i present empirical comparison between a linear combination of standard statistical language and translation model and an equivalent maximum entropy minimum divergence memd model using several different method for automatic feature selection the memd model significantly outperforms the standard model in test corpus perplexity even though it ha far fewer parameter 
ontology have been established for knowledge sharing and are widely used a a mean for conceptually structuring domain of interest with the growing usage of ontology the problem of overlapping knowledge in a common domain becomes critical we propose the new method fca merge for merging ontology following a bottom up approach which offer a structural description of the merging process the method is guided by application specific instance of the given source ontology that are to be merged we apply technique from natural language processing and formal concept analysis to derive a lattice of concept a a structural result of fca merge the generated result is then explored and transformed into the merged ontology with human interaction 
creating novel view by interpolating prestored image or view morphing ha many application in visual simulation we present in this paper a new method of automatically interpolating two image which tackle two most difficult problem of morphing due to the lack of depth information pixel matching and visibility handling we first describe a quasi dense matching algorithm based on region growing with the best first strategy for match propagation then we describe a robust construction of matched planar patch using local geometric constraint encoded by a homography after that we introduce a novel representation joint view triangulation for visible and half occluded patch in two image to handle their visibility during the creation of new view finally we demonstrate these technique on real image pair 
we propose a simple general purpose method that take a input a set of instance and a collection of web page and produce a set of new feature defined over the given instance each generated feature corresponds either to a word from an html header that appears to modify an instance or else to a quot position quot in an html document that appears to contain an instance in learning to classify musical artist for example the generated feature g classical might be true for all instance that 
our goal is to exploit human motion and object context to perform action recognition and object classification towards this end we introduce a framework for recognizing action and object by measuring image objectand action based information from video hidden markov model are combined with object context to classify hand action which are aggregated by a bayesian classifier to summarize activity we also use bayesian method to differentiate the class of unknown object by evaluating detected action along with lowlevel extracted object feature our approach is appropriate for locating and classifying object under a variety of condition including full occlusion we show experiment where both familiar and previously unseen object are recognized using action and context information this paper proposes a novel approach to human activity recognition that us context information of particular object in the scene we define class that contain object specific information including associated property appearance based description and action object provide a mean to focus attention on an individual interaction while maintaining awareness of other interaction in the scene by tracking hand contact with known object is detected once contact ha been established context is used to suggest specific hidden markov model hmms if any that may provide more explicit description of action associated with the object interaction captured over time are aggregated using bayesian statistic producing summary of activity additionally we show that the relationship between human action and object can be exploited to detect and classify object object classification is inferred in part by detecting learned action prior knowledge about object category and image analysis provides additional discrimination 
we present deterministic greedy algorithm for building sparse nonlinear regression model from observational data our objective is to develop ecient numerical scheme for reducing the training and runtime complexity of nonlinear regression technique applied to massive datasets in the spirit of natarajan s greedy algorithm natarajan we iteratively minimize a loss function subject to a speci ed constraint on the degree of sparsity required of the nal model or an upper bound on the 
successfully managing information mean being able to find relevant new information and to correctly integrate it with pre existing knowledge much information is nowadays stored a multilingual textual data therefore advanced classification system are currently considered a strategic component for effective knowledge management we describe an experience integrating different innovative ai technology such a hierarchical pattern matching and information extraction to provide flexible multilingual classification adaptable to user need pattern matching produce fairly accurate and fast categorisation over a large number of class while information extraction provides fine grained classification for a reduced number of class the resulting system wa adopted by the main italian financial news agency providing a pay to view service 
this paper extends the recovery of structure and motion to image sequence with several independently moving object the motion structure and camera calibration are all a priori unknown the fundamental constraint that we introduce is that multiple motion must share the same camera parameter existing work on independent motion ha not employed this constraint and therefore ha not gained over independent static scene reconstruction we show how this constraint lead to several new result in structure and motion recovery where euclidean reconstruction becomes possible in the multibody case when it wa underconstrained for a static scene we show how to combine motion of high relief low relief and planar object additionally we show that structure and motion can be recovered from just point in the uncalibrated fixed camera case experiment on real and synthetic imagery demonstrate the validity of the theory and the improvement in accuracy obtained using multibody analysis 
this paper present empirical study and closely corresponding theoretical model of the performance of a chart parser exhaustively parsing the penn treebank with the treebank s own cfg grammar we show how performance is dramatically affected by rule representation and tree transformation but little by top down v bottom up strategy we discus grammatical saturation including analysis of the strongly connected component of the phrasal nonterminals in the treebank and model how a sentence length increase the effective grammar rule size increase a region of the grammar are unlocked yielding super cubic observed time behavior in some configuration 
in d object detection and recognition an object of interest is subject to change in view a well a in illumination and shape for image classification purpose it is desirable to derive a representation in which intrinsic characteristic of the object are captured in a low dimensional space while effect due to artifact are reduced in this paper we propose a method for view based unsupervised learning of object appearance first view subspace are learned from a view unlabeled data set of multi view appearance using independent subspace analysis isa a learned viewsubspace provides a representation of appearance at that view regardless of illumination effect a measure called view subspace activity is calculated thereby to provide a metric for view based classification view based clustering is then performed by using maximum view subspace activity mvsa criterion this work is to the best of our knowledge the first devoted research on view based clustering of image 
we present a novel approach to real time structured light range scanning after an analysis of the underlying assumption of existing structured light technique we derive a new set of illumination pattern based on coding the boundary between projected stripe these stripe boundary code allow range scanning of moving object with only modest assumption about scene continuity and reflectance we describe an implementation that integrates these new code with real time algorithm for tracking stripe boundary and determining depth our system us a standard video camera and dlp projector and produce dense range image at hz with m accuracy over a cm working volume a an application we demonstrate the creation of complete model of rigid object the object are rotated in front of the scanner by hand and successive range image are automatically aligned 
this paper address the issue of designing embodied conversational agent that exhibit appropriate posture shift during dialogue with human user previous research ha noted the importance of hand gesture eye gaze and head nod in conversation between embodied agent and human we present an analysis of human monologue and dialogue that suggests that postural shift can be predicted a a function of discourse state in monologue and discourse and conversation state in dialogue on the basis of these finding we have implemented an embodied conversational agent that us collagen in such a way a to generate postural shift 
sequence of event are an important type of data arising in various application including telecommunication bio statistic web access analysis etc a basic approach to modeling such sequence is to find the underlying intensity function describing the expected number of event per time unit typically the intensity function are assumed to be piecewise constant we therefore consider different way of fitting intensity model to event sequence data we start by considering a bayesian approach using markov chain monte carlo mcmc method with varying number of piece these method can be used to produce posterior distribution on the intensity function and they can also accomodate covariates the drawback is that they are computationally intensive and thus are not very suitable for data mining application in which large number of intensity function have to be estimated we consider dynamic programming approach to finding the change point in the intensity function these method can find the maximum likelihood intensity function in o n k time for a sequence of n event and k different piece of intensity we show that simple heuristic can be used to prune the number of potential change point yielding speedup of several order of magnitude the result of the improved dynamic programming method correspond very closely with the posterior average produced by the mcmc method 
this paper present a new method of analyzing japanese noun phrase of the form n no n the japanese postposition no roughly corresponds to of but it ha much broader usage the method exploit a definition of n in a dictionary for example rugby no coach can be interpreted a a person who teach technique in rugby we illustrate the effectiveness of the method by the analysis of test noun phrase 
imitation is actively being studied a an effective mean of learning in multi agent environment it allows an agent to learn how to act well perhaps optimally by passively observing the action of cooperative teacher or other more experienced agent it environment we propose a straightforward imitation mechanism called model extraction that can be integrated easily into standard model based reinforcement learning algorithm roughly by observing a mentor with similar capability an agent can extract information about it own capability in unvisited part of state space the extracted information can accelerate learning dramatically we illustrate the benefit of model extraction by integrating it with prioritized sweeping and demonstrating improved performance and convergence through observation of single and multiple mentor though we make some stringent assumption regarding observability possible interaction and common ability we briefly comment on extension of the model that relax these 
previous algorithm that recover camera motion from image velocity suffer from both bias and excessive variance in the result we propose a robust estimator of camera motion that is statistically consistent when image noise is isotropic consistency mean that the estimated motion converges in probability to the true value a the number of image point increase an algorithm based on reweighted gauss newton iteration handle velocity measurement in about millisecond on a workstation 
this paper describes automatic technique for mapping semantically classified english verb to wordnetsenses the verb were initially grouped into semantic class based on syntactic category they werethen mapped into wordnet sens according to three piece of information prior probability of wordnetsenses semantic similarity of wordnet sens for verb within the same category and probabilisticcorrelations between wordnet relationship and verb frame data 
this paper study the structural sensitivity of line pattern recognition using shape graph we compare the recognition performance for four different algorithm each algorithm us a set of pair wise geometric attribute and a neighborhood graph to represent the structure of the line pattern the first algorithm us a pair wise geometric histogram the second us a relational histogram on the edge of the shape graph the third compare the set of attribute on the edge of the shape graph and the final algorithm compare the arrangement of line correspondence using graph matching the different algorithm are compared under line deletion line addition line fragmentation and line end point measurement error it is the graph matching algorithm which prof to be the most effective 
visitor who browse the web from wireless pda cell phone and pager are frequently stymied by web interface optimized for desktop pc simply replacing graphic with text and reformatting table doe not solve the problem because deep link structure can still require minute to traverse in this paper we develop an algorithm minpath that automatically improves wireless web navigation by suggesting useful shortcut link in real time minpath find shortcut by using a learned model of web visitor behavior to estimate the saving of shortcut link and suggests only the few best link we explore a variety of predictive model including na ve bayes mixture model and mixture of markov model and report empirical evidence that minpath find useful shortcut that save substantial navigational effort 
this paper describes how we used regression rule to improve upon a result previously published in the earth science literature in such a scienti c application of machine learning it is crucially important for the learned model to be understandable and communicable we recount how we selected a learning algorithm to maximize communicability and then describe two visualization technique that we developed to aid in understanding the model by exploiting the spatial nature of the data we also 
this paper present a new geometric relation between a solid bounded by a smooth surface and it silhouette in image formed under weak perspective projection the relation ha the potential to be used for recognizing complex d object from a single image object are modeled by showing them to a camera without any knowledge of their motion the main idea is to consider the dual of the d surface and the family of dual curve of the silhouette over all viewing direction occluding contour correspond to planar slice of the dual surface we introduce an affine invariant representation of this surface that can constructed from a sequence of image and allows an object to be recognized from arbitrary viewing direction we illustrate the proposed object representation scheme through synthetic example and image contour detected in real image 
abstract we calculate lower bound on the size of sigmoidal neural network thatapproximate continuous function in particular we show that for the approximationof polynomial the network size ha to grow a omega gamma k where ki the degree of the polynomial this bound is valid for any input dimension i e independently of the number of variable the result is obtained by introducinga new method employing upper bound on the vapnik chervonenkisdimension for proving lower 
this paper describes how to automaticallyextract grounding feature and segment adialogue into discourse unit once thedialogue ha been annotated with the dribackwardand forward looking tag suchan approach eliminates the need forseparate annotation of grounding makingdialogue annotation quicker and removinga possible source of error a preliminarytest of the mapping against a humanannotator is presented introductionthe annotation scheme ac developed by thediscourse 
a syntax tree or standard semantic representation can be represented a a set of indexed constraint this paper describes how this idea can be used in task oriented dialogue system to provide interpretation rule which incorporate structural and contextual constraint where available and degrade gracefully on ungrammatical input 
this paper present a mathematical framework for visual learning that integrates two popular statistical learning paradigm in the literature i descriptive learning such a markov random field and minimax entropy learning and ii generative learning such a pca ica tca image coding and hmm we apply this integrated learning framework to texton modeling and we assume that an observed texture image is generated by multiple layer of hidden stochastic texton process with each texton being a window function like a mini template or a wavelet under affine transformation the spatial arrangement of the textons are characterized by minimax entropy model the texton process generate image by occlusion or linear addition thus given a raw input image the learning framework achieves four goal i computing the appearance of the textons ii inferring the hidden stochastic texton process iii learning gibbs model for each texton process and iv verifying the learnt textons and gibbs model through random sampling and texture synthesis the integrated framework subsumes the minimax entropy learning paradigm and creates a richer class of probability model for visual pattern which are suited for middle level vision representation furthermore we show that the integration of description and generative method yield a natural and general framework of visual learning we demonstrate the proposed framework and algorithm on many real image 
in this paper we describe a type of data fusion involving the combination of evidence derived from multiple document representation our aim is to investigate if a composite representation can improve the online detection of novel event in a stream of broadcast news story this classification process otherwise known a first story detection fsd or in the topic detection and tracking pilot study a online new event detection is one of three main classification task defined by the tdt initiative our composite document representation consists of a semantic representation based on the lexical chain derived from a text and a syntactic representation using proper noun using the tdt evaluation methodology we evaluate a number of document representation combination using these document classifier 
dictionary have often been used for query translation in cross language information retrieval clir however we are faced with the problem of translation ambiguity i e multiple translation are stored in a dictionary for a word in addition a word by word query translation is not precise enough in this paper we explore several method to improve the previous dictionary based query translation first a many a possible noun phrase are recognized and translated a a whole by using statistical model and phrase translation pattern second the best word translation are selected based on the cohesion of the translation word our experimental result on trec english chinese clir collection show that these technique result in significant improvement over the simple dictionary approach and achieve even better performance than a high quality machine translation system 
we introduce and discus a local methodto learn one step ahead predictor for iteratedtime series forecasting for each singleone step ahead prediction our methodselects among different alternative a localmodel representation on the basis of a localcross validation procedure in the literature local learning is generally used forfunction estimation task which do not taketemporal behavior into account our techniqueextends this approach to the problem oflong horizon 
we describe preliminary result on combining depth information from a laser range nder and color and texture image cue to train classiers to segment illstructured dirt gravel and asphalt road a input to an autonomous road following system a large number of registered laser and camera image were captured at frame rate on a variety of rural road allowing laser feature such a d height and smoothness to be correlated with image feature such color histogram and gabor lter response a small set of road model were generated by training separate neural network on labeled feature vector clustered by road type by rst classifying the type of a novel road image an appropriate second stage classier wa selected to segment individual pixel achieving a high degree of accuracy on arbitrary image from the dataset 
most approach to cross language information retrieval assume that resource providing a direct translation between the query and document language exist this paper present research examining the situation where such an assumption is false here an intermediate or pivot language provides a mean of transitive translation of the query language to that of the document via the pivot at the cost however of introducing much error the paper report the novel approach of translating in parallel across multiple intermediate language and fusing the result such a technique remove the error raising the effectiveness of the tested retrieval system up to and possibly above the level expected had a direct translation route existed across a number of retrieval situation and combination of language the approach prof to be highly effective 
in intonational phonology and speech synthesis research it ha been suggested that the relative informativeness of a word can be used to predict pitch prominence the more information conveyed by a word the more likely it will be accented but there are others who express doubt about such a correlation in this paper we provide some empirical evidence to support the existence of such a correlation by employing two widely accepted measure of informativeness our experiment show that there is a positive correlation between the informativeness of a word and it pitch accent assignment they also show that informativeness enables statistically significant improvement in pitch accent prediction the computation of word informativeness is inexpensive and can be incorporated into speech synthesis system easily 
suppose you are given some dataset drawn from an underlying probability distribution and you want to estimate a simple subset of input space such that the probability that a test point drawn from lie outside of equal some a priori specified between and we propose a method to approach this problem by trying to estimate a function which is positive on and negative on the complement the functional form of is given by a kernel expansion in term of a potentially small subset of the training data it is regularized by controlling the length of the weight vector in an associated feature space we provide a theoretical analysis of the statistical performance of our algorithm the algorithm is a natural extension of the support vector algorithm to the case of unlabelled data 
determining the relationship between the activity of a single nervecell to that of an entire population is a fundamental question thatbears on the basic neural computation paradigm in this paperwe apply an information theoretic approach to quantify the levelof cooperative activity among cell in a behavioral context it ispossible to discriminate between synergetic activity of the cell v redundant activity depending on the difference between the informationthey provide when measured 
decision theoretic reasoning and planning algorithmsare increasingly being used for mobilerobot navigation due to the significantuncertainty accompanying the robot perceptionand action such algorithm requiredetailed probabilistic model of the environmentof the robot and it is very desirable toautomate the process of compiling such modelsby mean of autonomous learning algorithm this paper compare experimentallyfour learning method in combination withfour heuristic 
this paper present a new method for building domain specific web search engine previous method eliminate irrelevant document from the page accessed using heuristic based on human knowledge about the domain in question accordingly they are hard to build and can not be applied to other domain the keyword spice method in contrast improves search performance by adding domain specific keywords called keyword spice to the user s input query the modified query is then forwarded to a general purpose search engine keyword spice can be effectively discovered automatically from web document allowing u to build high quality domain specific search engine in various domain without requiring the collection of heuristic knowledge we describe a machine learning algorithm which is a type of decision tree learning algorithm that can extract keyword spice to demonstrate the value of the proposed approach we conduct experiment in the domain of cooking the result confirm the excellent performance of our method in term of both precision and recall 
this paper proposes a novel pattern classification approach called the nearest linear combination nlc approach for eigenface based face recognition assume that multiple prototypical vector are available per class each vector being a point in an eigenface space a linear combination of prototypical vector belonging to a face class is used to define a measure of distance from the query vector to the class the measure being defined a the euclidean distance from the query to the linear combination nearest to the query vector hence nlc this contrast to the nearest neighbor nn classification where a query vector is compared with each prototypical vector individually using a linear combination of prototypical vector instead of each of them individually extends the representational capacity of the prototype by generalization through interpolation and extrapolation experiment show that it lead to better result than existing classification method 
this paper examines the problem of image retrieval from large heterogeneous image database we present a technique that fulfills several need identified by surveying recent research in the field this technique fairly integrates a diverse and expandable set of image property for example color texture and location in a retrieval framework and allows end user substantial control over their use we propose a novel set of evaluation method in addition to applying established test for image retrieval our technique prof competitive with state of the art method in these test and doe better on certain task furthermore it improves on many standard image retrieval algorithm by supporting query based on subsection of image for certain query this capability significantly increase the relevance of the image retrieved and further expands the user s control over the retrieval process 
zebra crossing are useful road feature for outdoor navigation in mobility aid for the partially sighted in this paper zebra crossing are detected by looking for group of concurrent line edge are then partitioned using intensity variation information in order to tackle the ambiguity of the detection algorithm in distinguishing zebra crossing and stair case pose information is sought three method are developed to estimate the pose homography search approach using an a priori model nding normal using the vanishing line computed from equally spaced line and with two vanishing point these algorithm have been applied to real image with promising result and they are also useful in some other shape from texture application 
the traditional kalman filter can be viewed a a recursive stochastic algorithm that approximates an unknown function via a linear combination of prespecified basis function given a sequence of noisy sample in this paper we generalize the algorithm to one that approximates the fixed point of an operator that is known to be a euclidean norm contraction instead of noisy sample of the desired fixed point the algorithm update parameter based on noisy sample of function generated by application of the operator in the spirit of robbins monro stochastic approximation the algorithm is motivated by temporal difference learning and our development lead to a possibly more efficient variant of temporal difference learning we establish convergence of the algorithm and explore efficiency gain through computational experiment involving optimal stopping and queueing problem 
a motion segmentation algorithm based on factorization method and discriminant criterion is proposed this method us a feature with the most useful similarity for grouping selected using motion information calculated by factorization method and discriminant criterion a group is extracted based on discriminant analysis for the selected feature s similarity the same procedure is applied recursively to the remaining feature to extract other group this grouping is robust against noise and outlier because feature with no useful information are automatically rejected numerical computation is simple and stable no prior knowledge is needed on the number of object experimental result are shown for synthetic data and real image sequence 
objective the objective of this study is to compare how a general terminological system wordnet and a domain specific one umls represent linguistic and knowledge phenomenon at three different level term concept and semantic class method for one general class animal and one domain specific class health disorder the set of concept corresponding to the class wa established then for each semantic class the corresponding term were mapped from one system to the other both way result only of the domain specific concept from umls were found in wordnet but of the domain specific concept from wordnet were found in the umls concept overlap between the two system varies from to discussion missing term in both system are discussed a well a granularity and knowledge organization issue 
we developed a novel language model for japanese based on grapheme phoneme tuples which is one order of magnitude smaller than word based model we also developed an alignment algorithm of grapheme and phoneme for both ordinary text and ocr output we show by experiment that the combination of the grapheme phoneme tuple ngram model and the grapheme phoneme alignment algorithm significantly improve character recognition accuracy if both grapheme and phoneme representation are given 
the creation of a complex web site is a thorny problem in user interface design in ijcai we challenged the ai community to address this problem by creating adaptive web site in response we investigate the problem of index page synthesis the automatic creation of page that facilitate a visitor s navigation of a web site previous work ha employed statistical method to generate candidate index page that are of limited value because they do not correspond to concept or topic that are intuitive to people in this paper we formalize index page synthesis a a conceptual clustering problem and introduce a novel approach which we call conceptual cluster mining we search for a small number of cohesive cluster that correspond to concept in a given concept description language l next we present sgml an algorithm schema that combine a statistical clustering algorithm with a concept learning algorithm the clustering algorithm is used to generate seed cluster and the concept learning algorithm to describe these seed cluster using expression in l finally we offer preliminary experimental evidence that instantiation of sgml outperform existing algorithm e g cobweb in this domain 
the image of an object can vary dramatically depending on lighting specularities reflection and shadow it is often advantageous to separate these incidental variation from the intrinsic aspect of an image this paper describes how the statistical tool of independent component analysis can be used to separate some of these incidental component we describe the detail of this method and show it efficacy with example of separating reflection off glass and separating the relative contribution of individual light source 
this paper present an efficient shape based object detectionmethod based on distance transforms and describesits use for real time vision on board vehicle the method us a template hierarchy to capture thevariety of object shape efficient hierarchy can begenerated offline for given shape distribution usingstochastic optimization technique i e simulated annealing online matching involves a simultaneouscoarse to fine approach over the shape hierarchy andover the 
this paper present an approach for establishing correspondence in time and in space between two different video sequence of the same dynamic scene recorded by stationary uncalibrated video camera the method simultaneously estimate both spatial alignment a well a temporal synchronization temporal alignment between the two sequence using all available spatio temporal information temporal variation between image frame such a moving object or change in scene illumination are powerful cue for alignment which cannot be exploited by standard image to image alignment technique we show that by folding spatial and temporal cue into a single alignment framework situation which are inherently ambiguous for traditional image to image alignment method are often uniquely resolved by sequence to sequence alignment we also present a direct method for sequence tosequence alignment the algorithm simultaneously estimate spatial and temporal alignment parameter directly from measurable sequence quantity without requiring prior estimation of point correspondence frame correspondence or moving object detection result are shown on real image sequence taken by multiple video camera 
in this paper we study how to compute a dense depth map with panoramic field of view e g degree from multiperspective panorama a dense sequence of multiperspective panorama is used for better accuracy and reduced ambiguity by taking advantage of significant data redundancy to speed up the reconstruction we derive an approximate epipolar plane image that is associated with the planar sweeping camera setup and use one dimensional window for efficient matching to address the aperture problem introduced by one dimensional window matching we keep a set of possible depth candidate from matching score these candidate are then passed to a novel two pas tensor voting scheme to select the optimal depth by propagating the continuity and uniqueness constraint noniteratively in the voting process our method produce highquality reconstruction result even when significant occlusion is present experiment on challenging synthetic and real scene demonstrate the effectiveness and efficacy of our method 
this paper present a new multi unit auction protocol ir protocol that is robust against false name bid internet auction have become an integral part of electronic commerce and a promising field for applying agent and artificial intelligence technology although the internet provides an excellent infrastructure for executing auction the possibility of a new type of cheating called false name bid ha been pointed out a false name bid is a bid submitted under a fictitious name a protocol called lds ha been developed for combinatorial auction of multiple different item and ha proven to be robust against false name bid although we can modify the lds protocol to handle multi unit auction in which multiple unit of an identical item are auctioned the protocol is complicated and requires the auctioneer to carefully predetermine the combination of bundle to obtain a high social surplus or revenue for the auctioneer our newly developed ir protocol is easier to use than the lds since the combination of bundle is automatically determined in a flexible manner according to the declared evaluation value of agent the evaluation result show that the ir protocol can obtain a better social surplus than that obtained by the lds protocol 
learning a complex task can be significantly facilitated by defining a hierarchy of subtasks an agent can learn to choose between various temporally abstract action each solving an assigned subtask to accomplish the overall task in this paper we study hierarchical learning using the framework of option we argue that to take full advantage of hierarchical structure one should perform option specific state abstraction and that if this is to scale to larger task state abstraction should be automated we adapt mccallum s u tree algorithm to automatically build option specific representation of the state feature space and we illustrate the resulting algorithm using a simple hierarchical task result suggest that automated option specific state abstraction is an attractive approach to making hierarchical learning system more effective 
we consider the problem of how an agent creates a discrete spatial representation from it continuous interaction with the environment such representation will be the minimal one that explains the experience of the agent in the environment in this 
this paper investigates the formation of color category and color naming in a population of agent the agent perceive and categorize color stimulus and try to communicate about these perceived stimulus while doing so they adapt their internal representation to be more successful at conveying color meaning in future interaction the agent have no access to global information or to the representation of other agent they only exchange word form the factor driving the population coherence are the shared environment and the interaction the experiment show how agent can form a coherent lexicon of color term and particularly how a coherent color categorization emerges through these linguistic interaction the result are interpreted in the light of theory describing and explaining universal tendency in human color categorization and color naming at the same time the experiment confirm the view that certain aspect of language act a a complex dynamic system arising from self organization and cultural interaction 
application that interpret video data need to track object a they move in a scene tracking method that estimate the state trajectory of object a they change over time e g kalman filter have difficulty a the number of object and clutter increase we present an alternative call ed statistical tracking that is based on the concept of network tomography a scene is modeled a a network of interconnected region statistical tracking estimate the number of trip made from one region to another based on interregion boundary traffic count accumulated over time it is not necessary to track an object through a scene just to determine when an object cross a boundary something that is generally easier than estimating a continuous traje ctory in achieving this simplicity statistical tracking g ives up the ability to determine an object s state a the motion occurs instead it determines mean traffic intensity based on statistic accumulated over a period of time in spite of this limitation there are several application for whic h statistical tracking is useful we demonstrate the applica tion of the method to a large sample of video traffic surveillance data the method doe not require any data association which ha some important implication concerning personal privacy 
in this contribution we transfer a customer purchase incidence model for consumer product which is based on ehrenberg s repeat buying theory to web based information product ehrenberg s repeat buying theory successfully describes regularity on a large number of consumer product market we show that these regularity exist in electronic market for information good too and that purchase incidence model provide a well founded theoretical base for recommender and alert service the article consists of two part in the first part ehrenberg s repeat buying theory and it assumption are reviewed and adapted for web based information market second we present the empirical validation of the model based on data collected from the information market of the virtual university of the vienna university of economics and business administration at http vu wu wien ac at from september to may i introduction in this article we concentrate on an anonymous recommender service of the correlation type made famous by amazon com applied to an information broker it is based on consumption pattern for information good web site from market basket web browser session which we treat a consumer purchase history with unobserved consumer identity in resnick and varian s design space this recommender service is characterized a the content of a recommendation consists of link to web site it is an implicit service based on observed user behav 
the earley deduction algorithm is extended for the processing of ot syntax based on feature grammar due to faithfulness violation infinitely many candidate must be compared with the reasonable assumption i that ot constraint are description denoting bounded structure and ii that every rule recursion in the base grammar incurs some constraint violation a chart algorithm can be devised interleaving parsing and generation permit the application of generation based optimization even in the parsing task i e for a string input 
temporal aliasing artifact are common in both computer generated and natural motion sequence one of the most striking manifestation of temporal aliasing is the apparent reversal of motion commonly referred to a the wagon wheel effect in this paper we examine temporal aliasing from the standpoint of joint spatiotemporal spatiotemporal frequency representation we show that apparent motion reversal can be explained using these representation and demonstrate that a motion estimation algorithm based on such a representation the d gabor transform can accurately predict this illusion 
this paper address the derivation of likelihood function and confidence bound for problem involving overdetermined linear system with noise in all measurement often referred to a total least square tl it ha been shown previously that tl provides maximum likelihood estimate but rather than being a function solely of the variable of interest the associated likelihood function increase in dimensionality with the number of equation this ha made it difficult to derive suitable confidence bound and impractical to use these probability function with bayesian belief propagation or bayesian tracking this paper derives likelihood function that are defined only on the parameter of interest this ha two main advantage first the likelihood function are much easier to use within a bayesian framework and second it is straightforward to obtain a reliable confidence bound on the estimate we demonstrate the accuracy of our confidence bound in relation to others that have been proposed also we use our theoretical result to obtain likelihood function for estimating the direction of d camera translation 
automatic grouping and segmentation of image remains a challenging problem in computer vision recently a number of author have demonstrated good performance on this task using method that are based on eigenvectors of the affinity matrix these approach are extremely attractive in that they are based on simple eigen decomposition algorithm whose stability is well understood nevertheless the use of eigen decomposition in the context of segmentation is far from well understood in this paper we give a unified treatment of these algorithm and show the close connection between them while highlighting their distinguishing feature we then prove result on eigenvectors of block matrix that allow u to analyze the performance of these algorithm in simple grouping setting finally we use our analysis to motivate a variation on the existing method that combine aspect from different eigenvector segmentation algorithm we illustrate our analysis with result on real and synthetic image 
a method for reliably detecting change in the d shape of object that are well modeled a single value function zf x y is presented it us an estimate of the accuracy of the d model derived from a set of image taken simultaneously this accuracy estimate is used to distinguish between significant and insignificant change in d model derived from different image set the accuracy of the d model is estimated using a general methodology called self consistency for estimating the accuracy of computer vision algorithm which doe not require prior establishment of ground truth a novel image matching measure based on minimum description length mdl theory allows u to estimate the accuracy of individual element of the d model experiment to demonstrate the utility of the procedure are presented 
game site on the world wide web draw people from around the world with specialized interest skill and knowledge data from the game often reflects the player expertise and will to win we extract probabilistic forecast from data obtained from three online game the hollywood stock exchange hsx the foresight exchange fx and the formula one pick six f p competition we find that all three yield accurate forecast of uncertain future event in particular price of so called movie stock on hsx are good indicator of actual box office return price of hsx security in oscar emmy and grammy award correlate well with observed frequency of winning fx price are reliable indicator of future development in science and technology collective prediction from player in the f competition serve a good forecast of true race outcome in some case forecast induced from game data are more reliable than expert opinion we argue that web game naturally attract well informed and well motivated player and thus offer a valuable and oft overlooked source of high quality data with significant predictive value 
we present evidence that several higher order statistical propertiesof natural image and signal can be explained by a stochasticmodel which simply varies scale of an otherwise stationary gaussianprocess we discus two interesting consequence the rstis that a variety of natural signal can be related through a commonmodel of spherically invariant random process which havethe attractive property that the joint density can be constructedfrom the one dimensional marginal 
this paper proposes a new front propagation method to deal accurately with the challenging problem of tracking non rigid moving object this is obtained by employing a geodesic active region model where the designed objective function is composed of boundary and region based term and optimizes the curve position with respect to motion and intensity property the main novelty of our approach is that we deal with the motion estimation linear model are assumed and the tracking problem simultaneously in other word the optimization problem contains a coupled set of unknown variable the curve position and the corresponding motion model the designed objective function is minimized using a gradient descent method the curve is propagated towards the object boundary under the influence of boundary intensity and motion based force using a pde while given the curve position an incremental analytical solution is obtained for the motion model besides this pde is implemented using a level set approachwhere topological change are naturally handled very promising experimental result are provided using real video sequence 
while it ha recently become possible to build spoken dialogue system that interact with user in real time in a range of domain system that support conversational natural language are still subject to a large number of spoken language understanding slu error endowing such system with the ability to reliably distinguish slu error from correctly understood utterance might allow them to correct some error automatically or to interact with user to repair them thereby improving the system s overall performance we report experiment on learning to automatically distinguish slu error in spoken utterance collected in a field trial of at t s how may i help yousystem interacting with live customer traffic we apply the automatic classifier ripper cohen to train an slu classifier using feature that are automatically obtainable in real time the classifer achieves accuracy on this task an improvement of over the majority class baseline we show that the most important feature are those that the natural language understanding module can compute suggesting that integrating the trained classifier into the nlu module of the how may i help you system should be straightforward 
we consider the problem of wrapping around an object of which two view are available a reference surface and recovering the resulting parametric flow using direct computation via spatio temporal derivative the well known example are affine flow model and parameter flow model both describing a flow field of a planar reference surface we extend those classic flow model to deal with a quadric reference surface and work out the explicit parametric form of the flow field a a result we derive a simple warping algorithm that map between two view and leaf a residual flow proportional to the d deviation of the surface from a virtual quadric surface the application include image morphing model building image stabilization and disparate view correspondence 
we present an information geometric measure to systematicallyinvestigate neuronal firing pattern taking account not only ofthe second order but also of higher order interaction we beginwith the case of two neuron for illustration and show how to testwhether or not any pairwise correlation in one period is significantlydifferent from that in the other period in order to test such a hypothesisof different firing rate the correlation term need to besingled out orthogonally 
we introduce a new shape descriptor the shape context for correspondence recovery and shape based object recognition the shape context at a point capture the distribution over relative position of other shape point and thus summarizes global shape in a rich local descriptor shape context greatly simplify recovery of correspondence between point of two given shape moreover the shape context lead to a robust score for measuring shape similarity once shape are aligned the shape 
this paper describes the mashgreen dm prototype following three goal the identification and implementation of the task related to urban traffic control that use deep reasoning mechanism a main tool for their resolution the chosen model for explaining urban traffic behaviour is a qualitative model which includes among it main feature their low temporal and spatial computational cost the definition of a functional architecture with soft real time constraint that integrates the developed task a specilized component named agent composed by four functional module executes every task these module are the following one communication protocol method agent specilization data space and control this architecture verifies that the execution performance of every agent are not compromised by the inclusion of new agent in the system or by the interaction due to the active system agent the implementation of a prototype in a high performance computational architecture a beowulf computer system 
this paper proposes a question biased text summarization qbts approach that is useful for question answering system qbts is an extension of query biased text summarization in the sense that summarization is biased not only by the question which corresponds to the query but also by the prospective answer to the question we conducted text summarization experiment based on qa task and confirmed the effectiveness of our method in obtaining short summary 
most corpus based approach to natural language processing suffer from lack of training data this is because acquiring a large number of labeled data is expensive this paper describes a learning method that exploit unlabeled data to tackle data sparseness problem the method us committee learning to predict the label of unlabeled data that augment the existing training data our experiment on word sense disambiguation show that predictive accuracy is significantly improved by using additional unlabeled data 
in this paper we present a new bayesian framework for partially occluded object recognition with one to one correspondence we introduce two different statistical model for occlusion one model assumes that each feature in the model can be occluded independent of whether any other feature are occluded whereas the second model us spatially correlated occlusion to represent the extent of occlusion using these model the object recognition problem reduces to finding the object hypothesis with largest generalized likelihood we develop fast algorithm for finding the optimal one to one correspondence between scene feature and object model feature to compute the generalized likelihood we evaluate our algorithm using example extracted from synthetic aperture radar imagery and illustrate the performance advantage of our approach over alternative algorithm proposed by others 
distributional similarity is a useful notion in estimating the probability of rare joint event it ha been employed both to cluster event according to their distribution and to directly compute average of estimate for distributional neighbor of a target event here we examine the tradeoff between model size and prediction accuracy for cluster based and nearest neighbor distributional model of unseen event 
we present two language model based upon an immediate head parser our name for a parser that condition all event below a constituent c upon the head of c while all of the most accurate statistical parser are of the immediate head variety no previous grammatical language model us this technology the perplexity for both of these model significantly improve upon the trigram model base line a well a the best previous grammar based language model for the better of our two model these improvement are and respectively we also suggest that improvement of the underlying parser should significantly improve the model s perplexity and that even in the near term there is a lot of potential for improvement in immediate head language model 
symmetry often appears in real world constraint satisfaction problem but strategy for exploiting it are only beginning to be developed here a rationale for exploiting symmetry within depth first search is proposed leading to an heuristic for variable selection and a domain pruning procedure these strategy are then applied to a highly symmetric combinatorial problem namely the generation of balanced incomplete block design experimental result show that these strategy achieve a reduction of up to two order of magnitude in computational effort interestingly two previously developed strategy are shown to be particular instance of this approach 
this paper present a novel recognition framework which is based on matching shock graph of d shape outline where the distance between two shape is defined to be the cost of the least action path deforming one shape to another three key idea render the implementation of this framework practical first the shape space is partitioned by defining an equivalence class on shape where two shape with the same shock graph topology are considered to be equivalent second the space of deformation is discretiz ed by defining all deformation with the same sequence of shock graph transition a equivalent shock transition a re point along the deformation where the shock graph topology change third we employ a graph edit distance algorithm that search in the space of all possible transitio n sequence and find the globally optimal sequence in polynomial time the effectiveness of the proposed technique in the presence of a variety of visual transformation including occlusion articulation and deformation of part shadow and highlight viewpoint variation and boundary perturbation is demonstrated indexing into two separate database of roughly shape result in accuracy for top three match and for the next three match 
this paper present an algorithm for constructing object representation suitable for recognition the system automatically selects a representative subset of the view of the object while constructing the eigenspace basis these view are actively located for object identification and pose determination all processing is performed on line the camera is actively positioned during both representation and recognition when tested with view for each of seven object the system achieves accurate object recognition and pose determination these result are shown to degrade gracefully a condition deteriorate 
abstract a new paradigm is proposed for sorting spike in multi electrode data using ratio of transfer function between cell and electrode it is assumed that for every cell and electrode there is a stable linear relation these are dictated by the property of the tissue the electrode and their relative geometry the main advantage of the method is that it is insensitive to variation in the shape and amplitude of a spike spike sorting is carried out in two separate step first template describing the statistic of each spike type are generated by clustering transfer function ratio then spike are detected in the data using the spike statistic these technique were applied to data generated in the escape response system of the cockroach 
in order to understand cognitive aspect of autonomous robot it is fruitful to develop a mechanism by which the robot autonomously analyzes physical sensor data and construct a state space this paper proposes a coherent approach to constructing such a robot oriented state space by statistically analyzing sensor pattern and reward given a the result of task execution in the state space construction the robot creates sensor pattern classifier called empirically obtained perceiver eops which when combined represent internal state of the robot a novel feature of this method is that the eop directs attention to select necessary information and the state space is obtained with the attention control mechanism using eops we have confirmed that the robot can effectively construct state space through it vision sensor and execute a navigation task with the obtained state space in a complicated simulated world 
we present a new technique that improves upon existing structure from motion sfm method we propose a sfm algorithm that is both recursive and optimal our method incorporates innovative information from new frame into an existing solution without optimizing every camera pose and scene structure parameter to do this we incrementally optimize larger subset of parameter until the error is minimized these additional parameter are included in the optimization by tracing connection between point and frame in many case the complexity of adding a frame is much smaller than full bundle adjustment of all the parameter our algorithm is best described a incremental bundle adjustment a it allows new information to be added to an existing non linear least square solution 
many modeling task in computer vision e g structure from motion shape reflectance from shading filter synthesis have a low dimensional intrinsicstructure even though the dimension of the input datacan be relatively large we propose a simple but surprisinglyeffective iterative randomized algorithm thatdrastically cut down the time required for recoveringthe intrinsic structure the computational cost dependsonly on the intrinsic dimension of the structureof the task it is based on 
a simple algorithm for tracking the pose of articulated object in real time range image sequence is proposed this method model each target segment a a planar patch bounded by the convex hull of two circle and utilizes both edge like and region like information in matching the mode l to the target it us hard constraint for joint attachment and is designed to be robust to occlusion and missing data experimental result are presented in which a human arm is successfully tracked over frame of real video rate range imagery 
in many data mining domain misclassification cost are different for different example in the same way that class membership probability are example dependent in these domain both cost and probability are unknown for test example so both cost estimator and probability estimator must be learned after discussing how to make optimal decision given cost and probability estimate we present decision tree and naive bayesian learning method for obtaining well calibrated probability estimate we then explain how to obtain unbiased estimator for example dependent cost taking into account the difficulty that in general probability and cost are not independent random variable and the training example for which cost are known are not representative of all example the latter problem is called sample selection bias in econometrics our solution to it is based on nobel prize winning work due to the economist james heckman we show that the method we propose perform better than metacost and all other known method in a comprehensive experimental comparison that us the well known large and challenging dataset from the kdd data mining contest 
this paper present predictive gain scheduling a technique for simplify ing reinforcement learning problem by decomposition link admission control of self similar call traf fic is used to demonstrate the technique the control problem is decomposed into on line prediction of near future call arrival rate and precomputation of policy for poisson call ar rival process at decision time the prediction are used to select among the policy simulation show that this technique result in sig nificantly faster learning without any performance loss compared to a reinforcement learning controller that doe not decompose the problem 
this paper describes an efficient method to obtain d information by using spatio temporal analysis of omni image for outdoor navigation and map making in the intelligent transportation system it application two type of omni directional camera are employed to make a spatio temporal volume which is a sequence of omni image stacked in the spatio temporal space for the spatio temporal analysis of an omni image we define several different cross section in such spatio temporal volume and examine characteristic of the trace of image feature on the cross section we determine that the vertical straight line in the real world are preserved a straight line on these cross section and that the degree of this slope represents the quotient of the velocity of the camera motion and the depth of the object to acquire d information using these characteristic we propose a hybrid method of the epipolar plane image epi analysis and the model based analysis to demonstrate the effectiveness of this method we present some experimental result and the it application using an omni directional video camera to obtain image in outdoor environment 
we present an improved bound on the difference between training and test error for voting classiers this improved averaging bound provides a theoretical justication for popular averaging technique such a bayesian classic ation maximum entropy discrimination winnow and bayes point machine and ha implication for learning algorithm design 
this paper introduces how to eliminate redundant search space for forward chaining theorem proving a much a possible we consider how to keep on minimal useful consequent atom set for necessary branch in a proof tree in the most case an unnecessary non horn clause used for forward chaining will be split only once the increase of the search space by invoking unnecessary forward chaining clause will be nearly linear not exponential anymore in a certain sense we unsearch more than necessary we explain the principle of our method and provide an example to show that our approach is powerful for forward chaining theorem proving 
a new approach to characterizing the performance of point correspondence algorithm is presented instead of relying on any ground truth it us the self consistency of the output of an algorithm independently applied to different set of view of a static scene it allows one to evaluate algorithm for a given class of scene a well a to estimate the accuracy of every element of the output of the algorithm for a given set of view experiment to demonstrate the usefulness of the methodology are presented 
this paper present an application of perceptual grouping rule for content based image retrieval the semantic interrelationship between dier ent primitive image feature are exploited by perceptual grouping to detect the presence of manmade structure a methodology based on these principle in a bayesian framework for the retrieval of building image and the result obtained are presented the image database consists of monocular grayscale outdoor image taken from a ground level camera discrete primitive image feature the grouping principle proposed by gestalt psychologist embodied such concept a grouping by proximity similarity continuation closure and symmetry the grouping of low level feature provides a higher level structure these higher level structure may be further combined to yield another level of higher level structure the process may be repeated until a meaningful semantic representation is achieved that may be used by a 
an approach for tracking the motion of a rigidobject using parameterized flow model and acompact structure constraint is proposed whilepolynomial parameterized flow model have beenshown to be effective in tracking the rigid motionof planar object these model are inappropriatefor tracking moving object that change appearancerevealing their d structure we extendthese model by adding a structure compactnessconstraint that account for image motion that deviatesfrom a planar 
in this paper we present a new method for estimating confidence and curvature of d curvilinear structure the gradient structure tensor gst model shift invariance the eigenstructure of the tensor allows estimation of local dimensionality orientation and the corresponding confidence value local rotational invariance which occurs often in image cause a lower confidence estimate this underestimation can be corrected for by a parabolic deformation of the data in such a way that it becomes translational invariant we show that the optimal deformation can be found analytically and yield a local curvature estimate a a valuable by product we tested our new method on synthetic image and applied it to the detection of channel in d seismic data 
this paper offer computational theory and an algorithmicframework for perceptual organization of imagecontours arising from static occluding surface ofconstant lightness we articulate constraint and biasesunderlying the inference of such physical eventsas visible surface overlap and invisible modal andamodal surface boundary from ambiguous visualevidence including visible contrast edge and l typeand t type junction for any given scene an energyor cost function is 
there are many hierarchical clustering algorithm available but these lack a firm statistical basis here we set up a hierarchical probabilistic mixture model where data is generated in a hierarchical tree structured manner markov chain monte carlo mcmc method are demonstrated which can be used to sample from the posterior distribution over tree containing variable number of hidden unit 
we show that discourse structure need not bear the full burden of conveying discourse relation by showing that many of them can be explained nonstructurally in term of the grounding of anaphoric presupposition van der sandt this simplifies discourse structure while still allowing the realisation of a full range of discourse relation this is achieved using the same semantic machinery used in deriving clause level semantics 
hebbian learning rule are generally formulated a static rule under changing condition e g neuromodulation input statistic most rule are sensitive to parameter in particular recent work ha focused on two different formulation of spike timing dependent plasticity rule additive stdp is remarkably versatile but also very fragile whereas multiplicative stdp is more robust but lack attractive feature such a synaptic competition and rate stabilization here we address 
for many supervised learning task it is very costly to produce training data with class label active learning acquires data incrementally at each stage using the model learned so far to help identify especially useful additional data for labeling existing empirical active learning approach have focused on learning classifier however many application require estimation of the probability of class membership or score that can be used to rank new case we present a new active learning method for class probability estimation cpe and ranking bootstrap lv selects new data for labeling based on the variance in probability estimate a determined by learning multiple model from bootstrap sample of the existing labeled data we show empirically that the method reduces the number of data item that must be labeled across a wide variety of data set we also compare bootstrap lv with uncertainty sampling an existing active learning method designed to maximize classification accuracy the result show that bootstrap lv dominates for cpe surprisingly it also often is preferable for accelerating simple accuracy maximization 
in the future web of unmanned air and space vehicle will act together to robustly perform elaborate mission in uncertain environment we coordinate these system by introducing a reactive model based programming language rmpl that combine within a single unified representation the flexibility of embedded programming and reactive execution language and the deliberative reasoning power of temporal planner the kirk planning system take a input a problem expressed a a rmpl program and compiles it into a temporal plan network tpn similar to those used by temporal planner but extended for symbolic constraint and decision this intermediate representation clarifies the relation between temporal planning and causal link planning and permit a single task model to be used for planning and execution such a unified model ha been described a a holy grail for autonomous agent by the designer of the remote agent muscettola et al b 
building intelligent system that are capable of extracting high level representation from high dimensional data lie at the core of solving many ai related task including visual object or pattern recognition speech perception and language understanding theoretical and biological argument strongly suggest that building such system requires deep architecture that involve many layer of nonlinear processing many existing learning algorithm use shallow architecture including neural network with only one hidden layer support vector machine kernel logistic regression and many others the internal representation learned by such system are necessarily simple and are incapable of extracting some type of complex structure from high dimensional input in the past few year researcher across many different community from applied statistic to engineering computer science and neuroscience have proposed several deep hierarchical model that are capable of extracting meaningful high level representation an important property of these model is that they can extract complex statistical dependency from data and efficiently learn high level representation by re using and combining intermediate concept allowing these model to generalize well across a wide variety of task the learned high level representation have been shown to give state of the art result in many challenging learning problem and have been successfully applied in a wide variety of application domain including visual object recognition information retrieval natural language processing and speech perception a few notable example of such model include deep belief network deep boltzmann machine deep autoencoders and sparse coding based method the goal of the tutorial is to introduce the recent development of various deep learning method to the kdd community the core focus will be placed on algorithm that can learn multi layer hierarchy of representation emphasizing their application in information retrieval object recognition and speech perception 
we present an original method for tracking in an image sequence complex object which can be approximately modeled by a polyhedral shape the approach relies on the estimation of the d object image motion along with the computation of the d object pose the proposed method fulfills real time constraint along with reliability and robustness requirement real tracking experiment and result concerning a visual servoing positioning task are presented 
slanted surface pose a problem for correspondence algorithm utilizing search because of the greatly increased number of possibility when compared with fronto parallel surface in this paper we propose an algorithm to compute correspondence between stereo image or between frame of a motion sequence by minimizing an energy functional that account for slanted surface the energy is minimized in a greedy strategy that alternate between segmenting the image into a number of non overlapping region using the multiway cut algorithm of boykov veksler and zabih and finding the affine parameter describing the displacement function of each region a follow up step enables the algorithm to escape local minimum due to oversegmentation experiment on real image show the algorithm s ability to find an accurate segmentation and displacement map a well a discontinuity and crease from a wide variety of stereo and motion imagery 
we formulate a model for probability distribution on image space we show that any distribution of image can be factored exactly into conditional distribution of feature vector at one resolution pyramid level conditioned on the image information at lower resolution we would like to factor this over position in the pyramid level to make it tractable but such factoring may miss long range dependency to capture long range dependency we introduce hidden class label at each pixel in the pyramid the result is a hierarchical mixture of conditional probability similar to a hidden markov model on a tree the model parameter can be found with maximum likelihood estimation using the em algorithm we have obtained encouraging preliminary result on the problem of detecting various object in sar image and target recognition in optical aerial image 
we present an incremental refinement proof in acl which demonstrates the reduction ofthe observable behavior of a concurrent program to those of a much simpler program inparticular we document the proof of correctness of a concurrent program which implementsthe operation of a double ended queue in the application of a work stealing algorithm thedemonstration is carried out by proving a refinement from the implementation to a specificationvia an intermediate model we document 
design pattern describe micro architecture that solve recurrent architectural problem in objectoriented programming language it is important to identify these micro architecture during the maintenance of object oriented program but these micro architecture often appear distorted in the source code we present an application of explanation based constraint programming for identifying these distorted micro architecture 
we present a hybrid text mining method for finding abbreviation and their definition in free format text to deal with the problem this method employ pattern based abbreviation rule in addition to text marker and cue word the pattern based rule describe how abbreviation are formed from definition rule can be generated automatically and or manually and can be augmented when the system process new document the proposed method ha the advantage of high accuracy high flexibility wide coverage and fast recognition 
