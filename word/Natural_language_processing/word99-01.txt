i show that the equational treatment of ellipsis proposed in dalrymple et al can further be viewed a modeling the effect of parallelism on semantic interpretation i illustrate this claim by showing that the account straightforwardly extends to a general treatment of sloppy identity on the one hand and to deaccented focus on the other i also briefly discus the result obtained in a prototype implementation 
context is used in many nlp system a an indicator of a term s syntactic and semantic function the accuracy of the system is dependent on the quality and quantity of contextual information available to describe each term however the quantity variable is no longer fixed by limited corpus resource given fixed training time and computational resource it make sense for system to invest time in extracting high quality contextual information from a fixed corpus however with an effectively limitless quantity of text available extraction rate and representation size need to be considered we use thesaurus extraction with a range of context extracting tool to demonstrate the interaction between context quantity time and size on a corpus of million word 
we present a corpus based study of the sequential ordering among premodifiers in noun phrase this information is important for the fluency of generated text in practical application we propose and evaluate three approach to identify sequential order among premodifiers direct evidence transitive closure and clustering our implemented system can make over of such ordering decision correctly a evaluated on a large previously unseen test corpus 
for ambiguous sentence traditional semantics construction produce large number of higher order formula which must then be reduced individually underspecified version can produce compact description of all reading but it is not known how to perform reduction on these description we show how to do this using reduction constraint in the constraint language for structure clls 
fourteen indicator that measure the frequency of lexico syntactic phenomenon linguistically related to aspectual class are applied to aspectual classification this group of indicator is shown to improve classification performance for two aspectual distinction stativity and completedness i e telicity over unrestricted set of verb from two corpus several of these indicator have not previously been discovered to correlate with aspect 
we propose a novel co training method for statistical parsing the algorithm take a input a small corpus sentence annotated with parse tree a dictionary of possible lexicalized structure for each word in the training set and a large pool of unlabeled text the algorithm iteratively label the entire data set with parse tree using empirical result based on parsing the wall street journal corpus we show that training a statistical parser on the combined labeled and unlabeled data strongly out performs training only on the labeled data 
since kimball parsing preference principle such a right association ra and minimal attachment ma are often formulated with respect to constituent tree we present preference principle based on derivation tree within the framework of ltags we argue they remedy some shortcoming of the former approach and account for widely accepted heuristic e g argument modifier idiom 
this paper we discus an adaptive method ofhandling fragmented user utterance to a speech basedmultimodal dialogue system inserted silent pausesbetween fragment present the following problem doe the current silence indicate that the user hascompleted her utterance or is the silence just a pausebetween two fragment so that the system should waitfor more input our system incrementally classifiesuser utterance a either closing more input isunlikely to come or non closing 
this paper is concerned with using a semantic hierarchy to estimate the frequency with which a word sense appears a a given argument of a verb assuming the data is not sense disambiguated the standard approach is to split the count for any noun appearing in the data equally among the alternative sens of the noun this can lead to inaccurate estimate we describe a reestimation process which us the accumulated count of hypernym of the alternative sens in order to redistribute the count in order to choose a hypernym for each alternative sense we employ a novel technique which us a x test to measure the homogeneity of set of concept in the hierarchy 
we describe a distributed modular architecture for platform independent natural language system it feature automatic interface generation and self organization adaptive and non adaptive voting mechanism are used for integrating discrete module the architecture is suitable for rapid prototyping and product delivery 
this paper focus on the analysis and prediction of so called defined a turn where a user of a spoken dialogue system first becomes aware that the system ha made a speech recognition error we describe statistical comparison of feature of these aware site in a train timetable spoken dialogue corpus which reveal significant prosodic difference between such turn compared with turn that correct speech recognition error a well a with normal turn that are neither aware site nor correction we then present machine learning result in which we show how prosodic feature in combination with other automatically available feature can predict whether or not a user turn wa a normal turn a correction and or an aware site 
this paper present a restricted version of set local multi component tag weir which retains the strong generative capacity of tree local multi component tag i e produce the same derived structure but ha a greater derivational generative capacity i e can derive those structure in more way this formalism is then applied a a framework for integrating dependency and constituency based linguistic representation 
the theoretical study of the range concatenation grammar rcg formalism ha revealed many attractive property which may be used in nlp in particular range concatenation language rcl can be parsed in polynomial time and many classical grammatical formalism can be translated into equivalent rcgs without increasing their worst case parsing time complexity for example after translation into an equivalent rcg any tree adjoining grammar can be parsed in o n time in this paper we study a parsing technique whose purpose is to improve the practical efficiency of rcl parser the non deterministic parsing choice of the main parser for a language l are directed by a guide which us the shared derivation forest output by a prior rcl parser for a suitable superset of l the result of a practical evaluation of this method on a wide coverage english grammar are given 
this paper proposes a hidden markov model hmm and an hmm based chunk tagger from which a named entity ne recognition ner system is built to recognize and classify name time and numerical quantity through the hmm our system is able to apply and integrate four type of internal and external evidence simple deterministic internal feature of the word such a capitalization and digitalization internal semantic feature of important trigger internal gazetteer feature external macro context feature in this way the ner problem can be resolved effectively evaluation of our system on muc and muc english ne task achieves f measure of and respectively it show that the performance is significantly better than reported by any other machine learning system moreover the performance is even consistently better than those based on handcrafted rule 
pattern based machine translation is one of the machine translation method which performs syntactic analysis and structure transfer at the same time using bilingual pattern pbmt is used to expand the length of pattern up to sentence length in order to reduce ambiguity in translation but it brought out the problem of rapidly increased pattern we propose a model which shortens the length of pattern to phrase length and reduces ambiguity in translation by using two level translation pattern selection method in the first level the proper translation pattern are selected by using a hybrid method of exact example matching and semantic constraint by thesaurus in the second level the most natural translation pattern for the verb phrase is selected among the selected translation pattern category by using statistic information of the target language by using this proposed model we could shorten the length of pattern without raising the ambiguity in translation 
in this paper we propose adding long term grammatical information in a whole sentence maximun entropy language model wsme in order to improve the performance of the model the grammatical information wa added to the wsme model a feature and were obtained from a stochastic context free grammar finally experiment using a part of the penn treebank corpus were carried out and significant improvement were acheived 
i show that the equational treatment of ellipsis proposed in dalrymple et al can further be viewed a modeling the effect of parallelism on semantic interpretation i illustrate this claim by showing that the account straightforwardly extends to a general treatment of sloppy identity on the one hand and to deaccented focus on the other i also briefly discus the result obtained in a prototype implementation 
context is used in many nlp system a an indicator of a term s syntactic and semantic function the accuracy of the system is dependent on the quality and quantity of contextual information available to describe each term however the quantity variable is no longer fixed by limited corpus resource given fixed training time and computational resource it make sense for system to invest time in extracting high quality contextual information from a fixed corpus however with an effectively limitless quantity of text available extraction rate and representation size need to be considered we use thesaurus extraction with a range of context extracting tool to demonstrate the interaction between context quantity time and size on a corpus of million word 
we present a corpus based study of the sequential ordering among premodifiers in noun phrase this information is important for the fluency of generated text in practical application we propose and evaluate three approach to identify sequential order among premodifiers direct evidence transitive closure and clustering our implemented system can make over of such ordering decision correctly a evaluated on a large previously unseen test corpus 
for ambiguous sentence traditional semantics construction produce large number of higher order formula which must then be reduced individually underspecified version can produce compact description of all reading but it is not known how to perform reduction on these description we show how to do this using reduction constraint in the constraint language for structure clls 
fourteen indicator that measure the frequency of lexico syntactic phenomenon linguistically related to aspectual class are applied to aspectual classification this group of indicator is shown to improve classification performance for two aspectual distinction stativity and completedness i e telicity over unrestricted set of verb from two corpus several of these indicator have not previously been discovered to correlate with aspect 
we propose a novel co training method for statistical parsing the algorithm take a input a small corpus sentence annotated with parse tree a dictionary of possible lexicalized structure for each word in the training set and a large pool of unlabeled text the algorithm iteratively label the entire data set with parse tree using empirical result based on parsing the wall street journal corpus we show that training a statistical parser on the combined labeled and unlabeled data strongly out performs training only on the labeled data 
since kimball parsing preference principle such a right association ra and minimal attachment ma are often formulated with respect to constituent tree we present preference principle based on derivation tree within the framework of ltags we argue they remedy some shortcoming of the former approach and account for widely accepted heuristic e g argument modifier idiom 
this paper we discus an adaptive method ofhandling fragmented user utterance to a speech basedmultimodal dialogue system inserted silent pausesbetween fragment present the following problem doe the current silence indicate that the user hascompleted her utterance or is the silence just a pausebetween two fragment so that the system should waitfor more input our system incrementally classifiesuser utterance a either closing more input isunlikely to come or non closing 
this paper is concerned with using a semantic hierarchy to estimate the frequency with which a word sense appears a a given argument of a verb assuming the data is not sense disambiguated the standard approach is to split the count for any noun appearing in the data equally among the alternative sens of the noun this can lead to inaccurate estimate we describe a reestimation process which us the accumulated count of hypernym of the alternative sens in order to redistribute the count in order to choose a hypernym for each alternative sense we employ a novel technique which us a x test to measure the homogeneity of set of concept in the hierarchy 
we describe a distributed modular architecture for platform independent natural language system it feature automatic interface generation and self organization adaptive and non adaptive voting mechanism are used for integrating discrete module the architecture is suitable for rapid prototyping and product delivery 
this paper focus on the analysis and prediction of so called defined a turn where a user of a spoken dialogue system first becomes aware that the system ha made a speech recognition error we describe statistical comparison of feature of these aware site in a train timetable spoken dialogue corpus which reveal significant prosodic difference between such turn compared with turn that correct speech recognition error a well a with normal turn that are neither aware site nor correction we then present machine learning result in which we show how prosodic feature in combination with other automatically available feature can predict whether or not a user turn wa a normal turn a correction and or an aware site 
this paper present a restricted version of set local multi component tag weir which retains the strong generative capacity of tree local multi component tag i e produce the same derived structure but ha a greater derivational generative capacity i e can derive those structure in more way this formalism is then applied a a framework for integrating dependency and constituency based linguistic representation 
the theoretical study of the range concatenation grammar rcg formalism ha revealed many attractive property which may be used in nlp in particular range concatenation language rcl can be parsed in polynomial time and many classical grammatical formalism can be translated into equivalent rcgs without increasing their worst case parsing time complexity for example after translation into an equivalent rcg any tree adjoining grammar can be parsed in o n time in this paper we study a parsing technique whose purpose is to improve the practical efficiency of rcl parser the non deterministic parsing choice of the main parser for a language l are directed by a guide which us the shared derivation forest output by a prior rcl parser for a suitable superset of l the result of a practical evaluation of this method on a wide coverage english grammar are given 
this paper proposes a hidden markov model hmm and an hmm based chunk tagger from which a named entity ne recognition ner system is built to recognize and classify name time and numerical quantity through the hmm our system is able to apply and integrate four type of internal and external evidence simple deterministic internal feature of the word such a capitalization and digitalization internal semantic feature of important trigger internal gazetteer feature external macro context feature in this way the ner problem can be resolved effectively evaluation of our system on muc and muc english ne task achieves f measure of and respectively it show that the performance is significantly better than reported by any other machine learning system moreover the performance is even consistently better than those based on handcrafted rule 
pattern based machine translation is one of the machine translation method which performs syntactic analysis and structure transfer at the same time using bilingual pattern pbmt is used to expand the length of pattern up to sentence length in order to reduce ambiguity in translation but it brought out the problem of rapidly increased pattern we propose a model which shortens the length of pattern to phrase length and reduces ambiguity in translation by using two level translation pattern selection method in the first level the proper translation pattern are selected by using a hybrid method of exact example matching and semantic constraint by thesaurus in the second level the most natural translation pattern for the verb phrase is selected among the selected translation pattern category by using statistic information of the target language by using this proposed model we could shorten the length of pattern without raising the ambiguity in translation 
in this paper we propose adding long term grammatical information in a whole sentence maximun entropy language model wsme in order to improve the performance of the model the grammatical information wa added to the wsme model a feature and were obtained from a stochastic context free grammar finally experiment using a part of the penn treebank corpus were carried out and significant improvement were acheived 
this paper present several technique for performing automatic coreference annotation and performance result for each of them to demonstrate that they can be applied to real world data we have built a simple question answering system which us the technique a system using coreference is compared to a baseline system with the result that the addition of the coreference annotation improves performance 
in an effort to develop measure of discourse level management strategy this study examines a measure of the degree to which decision making interaction consist of sequence of utterance function that are linked in a decision making routine the measure is applied to dyadic interaction elicited in both face to face and computer mediated environment with systematic variation of task complexity and message window size every utterance in the interaction is coded according to a system that identifies decision making function and other routine function of utterance markov analysis of the coded utterance make it possible to measure the relative frequency with which sequence of and utterance trace a path in a markov model of the decision routine these proportion suggest that interaction in all condition adhere to the model although we find greater conformity in the computer mediated environment which is probably due to increased processing and attentional demand for greater efficiency the result suggest that measure based on markov analysis of coded interaction can provide useful measure for comparing discourse level property for correlating discourse feature with other textual feature and for analysis of discourse management strategy 
this article describes the construction and performance of granska a surface oriented system for grammar checking of swedish text with the use of carefully constructed error detection rule the system can detect and suggest correction for a number of grammatical error in swedish text in this article we specifically focus on how erroneously split compound and noun phrase disagreement are handled in the rule the system combine probabilistic and rule based method to achieve high efficiency and robustness this is a necessary prerequisite for a grammar checker that will be used in real time in direct interaction with user we hope to show that the granska system with higher efficiency can achieve the same or better result than system that use rule based parsing alone part of this work were presented at nodalida domeij et al in this article another grammar checker for swedish is presented this grammar checker called granska ha been developed at kth for about four year we will first present the structure of granska and then in more detail describe four important part of the system the part of speech tagging module the construction of error detection rule the algorithm for rule matching and the generation of error correction finally we describe the performance of the tagging error detection and np recognition 
we present a general architecture for efficient and deterministic morphological analysis based on memory based learning and apply it to morphological analysis of dutch the system make direct mapping from letter in context to rich category that encode morphological boundary syntactic class label and spelling change both precision and recall of labeled morpheme are over on held out dictionary test word and estimated to be over in free text 
we study distributional similarity measure for the purpose of improving probability estimation for unseen cooccurrences our contribution are three fold an empirical comparison of a broad range of measure a classification of similarity function based on the information that they incorporate and the introduction of a novel function that is superior at evaluating potential proxy distribution 
we propose a mixed language query disambiguation approach by using co occurrence information from monolingual data only a mixed language query consists of word in a primary language and a secondary language our method translates the query into monolingual query in either language two novel feature for disambiguation namely contextual word voting and best contextual word are introduced and compared to a baseline feature the nearest neighbor average query translation accuracy for the two feature are and compared to the baseline accuracy of 
in this paper we present and compare various single word based alignment model for statistical machine translation we discus the five ibm alignment model the hidden markov alignment model smoothing technique and various modification we present different method to combine alignment a evaluation criterion we use the quality of the resulting viterbi alignment compared to a manually produced reference alignment we show that model with a first order dependence and a fertility model lead to significantly better result than the simple model ibm or ibm which are not able to go beyond zero order dependency 
in this paper we address the problem of extracting key piece of information from voicemail message such a the identity and phone number of the caller this task differs from the named entity task in that the information we are interested in is a subset of the named entity in the message and consequently the need to pick the correct subset make the problem more difficult also the caller s identity may include information that is not typically associated with a named entity in this work we present three information extraction method one based on hand crafted rule one based on maximum entropy tagging and one based on probabilistic transducer induction we evaluate their performance on both manually transcribed message and on the output of a speech recognition system 
this paper describes an approach to machine translation that place linguistic information at it foundation the difficulty of translation from english to japanese is illustrated with data that show the influence of various linguistic contextual factor next a method for natural language transfer is presented that integrates translation example represented a typed feature structure with source target index with linguistic rule and constraint the method ha been implemented and the result of an evaluation are presented 
the incremental algorithm introduced in dale and reiter for producing distinguishing description doe not always generate a minimal description in this paper i show that when generalised to set of individual and disjunctive property this approach might generate unnecessarily long and ambiguous and or epistemically redundant description i then present an alternative constraint based algorithm and show that it build on existing related algorithm in that i it produce minimal description for set of individual using positive negative and disjunctive property ii it straightforwardly generalises to n ary relation and iii it is integrated with surface realisation 
natural language generation from flat semantics is an np complete problem this make it necessary to develop algorithm that run with reasonable efficiency in practice despite the high worst case complexity we show how to convert tag generation problem into dependency parsing problem which is useful because optimization in recent dependency parser based on constraint programming tackle exactly the combinatorics that make generation hard indeed initial experiment display promising runtimes 
the use of pipeline is an important technique in contemporary hardwaredesign particularly at the level of register transfer logic rtl earlierformal analysis e g using the acl theorem prover showed correctnessof pipelined floating point rtl this paper extends that workby considering a notion of a conditional pipeline essentially the result ofsharing hardware among several distinct pipeline we have employed apipeline tool written in acl but completely unverified 
inverse document frequency idf is a popular measure of a word s importance the idf invariably appears in a host of heuristic measure used in information retrieval however so far the idf ha itself been a heuristic in this paper we show idf to be optimal in a principled sense we show that idf is the optimal weight of a word with respect to minimization of a kullback leibler distance suitably generalized to nonnegative function which need not be probability distribution this optimization problem is closely related to maximum entropy problem we show that the idf is the optimal weight associated with a word feature in an information retrieval setting where we treat each document a the query that retrieves itself that is idf is optimal for document self retrieval 
we introduce a new categorial formalism based on intuitionistic linear logic this formalism which derives from current type logical grammar is abstract in the sense that both syntax and semantics are handled by the same set of primitive a a consequence the formalism is reversible and provides different computational paradigm that may be freely composed together 
most of the current work on corpus annotation is concentrated on morphemics lexical semantics and sentence structure however it becomes more and more obvious that attention should and can be also paid to phenomenon that reflect the link between a sentence and it context i e the discourse anchoring of utterance if conceived in this way an annotated corpus can be used a a resource for linguistic research not only within the limit of the sentence but also with regard to discourse pattern thus the application of the research to issue of information retrieval and extraction may be made more effective also application in new domain become feasible be it to serve for inner linguistic and literary aim such a text segmentation specification of topic of part of a discourse or for other discipline 
the possibility for data mining from large text collection are virtually untapped text express a vast rich range of information but encodes this information in a form that is difficult to decipher automatically perhaps for this reason there ha been little work in text data mining to date and most people who have talked about it have either conflated it with information access or have not made use of text directly to discover heretofore unknown information in this paper i will first define data mining information access and corpus based computational linguistics and then discus the relationship of these to text data mining the intent behind these contrast is to draw attention to exciting new kind of problem for computational linguist i describe example of what i consider to be real text data mining effort and briefly outline recent idea about how to pursue exploratory data analysis over text 
we offer a computational analysis of the resolution of ellipsis in certain case of dialogue clarification we show that this go beyond standard technique used in anaphora and ellipsis resolution and requires operation on highly structured linguistically heterogeneous representation we characterize these operation and the representation on which they operate we offer an analysis couched in a version of head driven phrase structure grammar combined with a theory of information state is in dialogue we sketch an algorithm for the process of utterance integration in i which lead to grounding or clarification 
co training is a weakly supervised learning paradigm in which the redundancy of the learning task is captured by training two classifier using separate view of the same data this enables bootstrapping from a small set of labeled training data via a large set of unlabeled data this study examines the learning behavior of co training on natural language processing task that typically require large number of training instance to achieve usable performance level using base noun phrase bracketing a a case study we find that co training reduces by the difference in error between co trained classifier and fully supervised classifier trained on a labeled version of all available data however degradation in the quality of the bootstrapped data arises a an obstacle to further improvement to address this we propose a moderately supervised variant of cotraining in which a human corrects the mistake made during automatic labeling our analysis suggests that corrected co training and similar moderately supervised method may help cotraining scale to large natural language learning task 
we describe a novel method for coping with ungrammatical input based on the use of chart like data structure which permit anytime processing priority is given to deep syntactic analysis should this fail the best partial analysis are selected according to a shortest path algorithm and assembled in a robust processing phase the method ha been applied in a speech translation project with large hpsg grammar 
an easy way of translating query in one language to the other for cross language information retrieval ir is to use a simple bilingual dictionary because of the general purpose nature of such dictionary however this simple method yield a severe translation ambiguity problem this paper describes the degree to which this problem arises in korean english cross language ir and suggests a relatively simple yet effective method for disambiguation using mutual information statistic obtained only from the target document collection in this method mutual information is used not only to select the best candidate but also to assign a weight to query term in the target language our experimental result based on the trec collection show that this method can achieve up to of the monolingual retrieval case and of the manual disambiguation case 
grammatical relationship grs form an important level of natural language processing but different set of grs are useful for different purpose therefore one may often only have time to obtain a small training corpus with the desired gr annotation to boost the performance from using such a small training corpus on a transformation rule learner we use existing system that find related type of annotation 
we introduce a new model of selectional preference induction unlike previous approach we provide a stochastic generation model for the word that appear a argument of a predicate more specifically we define a hidden markov model with the general shape of a given semantic class hierarchy this model ha a number of attractive feature among them that selectional preference can be seen a distribution over word initial result are promising however unsupervised parameter estimation ha proven problematic a central problem is word sense ambiguity in the training corpus we describe attempt to modify the forward backward algorithm an em algorithm to handle such disambiguation although these attempt were unsuccessful at improving performance we believe they give insight into the nature of the bottleneck and into the behavior of the em algorithm 
of the various problem that natural language processing ha revealed polysemy is probably the most frustrating people deal with polysemy so easily that potential abiguities are overlooked whereas computer must work hard to do far le well a linguistic approach generally involves a parser a lexicon and some ad hoc rule for using linguistic context to identify the context appropriate sense a statistical approach generally involves the use of word co occurrence statistic to create a semantic hyperspace where each word regardless of it polysemy is represented a a single vector each approach ha strength and limitation some combination is often proposed various possibility will be discussed in term of their psychological plausibility computational linguistics is generally considered to be the branch of engineering that us computer to do useful thing with linguistic signal but it can also be viewed a an extended test of computational theory of human cognition it is this latter perspective that psychologist find most interesting language provides a critical test for the hypothesis that physical symbol system are adequate to perform all human cognitive function a yet no adequate system for natural language processing ha approached human level of performance 
object such a event action and propo sitions can all serve a antecedent of auaphoric ex pressions we are not aware of any reliability resultsfor this type of annotation but the lt seg gt elementcan be used to identify the antecedent in this typeof anaphora if desired the annotator could use asecond attribute type to specify the type of objectintroduced by the lt seg element type would havevalues event prop and action 
we describe a set of supervised machine learning experiment centering on the construction of statistical model of wh question these model which are built from shallow linguistic feature of question are employed to predict target variable which represent a user s informational goal we report on different aspect of the predictive performance of our model including the influence of various training and testing factor on predictive performance and examine the relationship among the target variable 
we present a rule based shallow parser compiler which allows to generate a robust shallow parser for any language even in the absence of training data by resorting to a very limited number of rule which aim at identifying constituent boundary we contrast our approach to other approach used for shallow parsing i e finite state and probabilistic method we present an evaluation of our tool for english penn treebank and for french newspaper corpus lemonde for several task np chunking deeper parsing 
dominance constraint are logical description of tree that are widely used in computational linguistics their general satisfiability problem is known to be np complete here we identify the natural fragment of normal dominance constraint and show that it satisfiability problem is in deterministic polynomial time 
we present a generative distributional model for the unsupervised induction of natural language syntax which explicitly model constituent yield and context parameter search with em produce higher quality analysis than previously exhibited by unsupervised system giving the best published un supervised parsing result on the atis corpus experiment on penn treebank sentence of comparable length show an even higher f of on non trivial bracket we compare distributionally induced and actual part of speech tag a input data and examine extension to the basic model we discus error made by the system compare the system to previous model and discus upper bound lower bound and stability for this task 
it is necessary to have a large annotated corpus to build a statistical parser acquisition of such a corpus is costly and time consuming this paper present a method to reduce this demand using active learning which selects what sample to annotate instead of annotating blindly the whole training corpus sample selection for annotation is based upon representativeness and usefulness a model based distance is proposed to measure the difference of two sentence and their most likely parse tree based on this distance the active learning process analyzes the sample distribution by clustering and calculates the density of each sample to quantify it representativeness further more a sentence is deemed a useful if the existing model is highly uncertain about it par where uncertainty is measured by various entropy based score experiment are carried out in the shallow semantic parser of an air travel dialog system our result show that for about the same parsing accuracy we only need to annotate a third of the sample a compared to the usual random selection method 
building a bilingual dictionary for transfer in a machine translation system is conventionally done by hand and is very time consuming in order to overcome this bottleneck we propose a new mechanism for lexical transfer which is simple and suitable for learning from bilingual corpus it exploit a vector space model developed in information retrieval research we present a preliminary result from our computational experiment 
we present two measure for comparingcorpora based on information theorystatistics such a gain ratio a wellas simple term class frequency count 
this paper describes new and improved technique which help a unification based parser to process input efficiently and robustly in combination these method result in a speed up in parsing time of more than an order of magnitude the method are correct in the sense that none of them rule out legal rule application 
answer validation is an emerging topic in question answering where open domain system are often required to rank huge amount of candidate answer we present a novel approach to answer validation based on the intuition that the amount of implicit knowledge which connects an answer to a question can be quantitatively estimated by exploiting the redundancy of web information experiment carried out on the trec judged answer collection show that the approach achieves a high level of performance success rate the simplicity and the efficiency of this approach make it suitable to be used a a module in question answering system 
string similarity metric are used for several purpose in text processing one task is the extraction of cognate from bilingual text in this paper three approach to the automatic generation of language dependent string matching function are presented 
in automatic speech recognition asr enabled application for medical dictation corpus of literal transcription of speech are critical for training both speaker independent and speaker adapted acoustic model obtaining these transcription is both costly and time consuming non literal transcription on the other hand are easy to obtain because they are generated in the normal course of a medical transcription operation this paper present a method of automatically generating text that can take the place of literal transcription for training acoustic and language model atrs is an automatic transcription reconstruction system that can produce near literal transcription with almost no human labor we will show that i adapted acoustic model trained on atrs data perform a well a or better than adapted acoustic model trained on literal transcription a measured by recognition accuracy and ii language model trained on atrs data have lower perplexity than language model trained on non literal data 
in this paper we present part of speech tagger based on hidden markov model which adopt a le strict markov assumption to consider rich context in model whose parameter are very specific like lexicalized one sparse data problem is very serious and also conditional probability tend to be estimated unreliably to overcome data sparseness a simplified version of the well known back off smoothing method is used to mitigate unreliable estimation problem our model assume joint independence instead of conditional independence because joint probability have the same degree of estimation reliability in experiment for the brown corpus model with rich context achieve relatively high accuracy and some model assuming joint independence show better result than the corresponding hmms 
this paper introduces new learning algorithm for natural language processing based on the perceptron algorithm we show how the algorithm can be efficiently applied to exponential sized representation of parse tree such a the all subtrees dop representation described by bod or a representation tracking all sub fragment of a tagged sentence we give experimental result showing significant improvement on two task parsing wall street journal text and named entity extraction from web data 
many people are now routinely building grammar based language model for interactive spoken language application these language model are typically ad hoc semantic grammar which ignore many standard linguistic constraint in particular grammatical agreement we describe a series of experiment in which we took three cfg based language model from non trivial implemented system and in each case contrasted the performance of a version which included agreement constraint against a version which ignored them our finding suggest that inclusion of agreement constraint significantly improves performance in term of both word error rate and semantic error rate 
we study the impact of richer syntactic dependency on the performance of the structured language model slm along three dimension parsing accuracy lp lr perplexity ppl and word error rate wer n best re scoring we show that our model achieve an improvement in lp lr ppl and or wer over the reported baseline result using the slm on the upenn treebank and wall street journal wsj corpus respectively analysis of parsing performance show correlation between the quality of the parser a measured by precision recall and the language model performance ppl and wer a remarkable fact is that the enriched slm outperforms the baseline gram model in term of wer by when used in isolation a a second pas n best re scoring language model 
log linear model provide a statistically sound framework for stochastic unification based grammar subgs and stochastic version of other kind of grammar we describe two computationally tractable way of estimating the parameter of such grammar from a training corpus of syntactic analysis and apply these to estimate a stochastic version of lexical functional grammar 
this paper present an algorithm for text summarization using the thematic hierarchy of a text the algorithm is intended to generate a one page summary for the user thereby enabling the user to skim large volume of an electronic book on a computer display the algorithm first detects the thematic hierarchy of a source text with lexical cohesion measured by term repetition then it identifies boundary sentence at which a topic of appropriate grading probably start finally it generates a structured summary indicating the outline of the thematic hierarchy this paper mainly describes and evaluates the part for boundary sentence identification in the algorithm and then briefly discus the readability of one page summary 
non compositional expression present a special challenge to nlp application we present a method for automatic identification of non compositional expression using their statistical property in a text corpus our method is based on the hypothesis that when a phrase is non composition it mutual information differs significantly from the mutual information of phrase obtained by substituting one of the word in the phrase with a similar word 
this paper proposes a method for packing feature structure which automatically collapse equivalent part of lexical phrasal feature structure of hpsg into a single packed feature structure this method avoids redundant repetition of unification of those part preliminary experiment show that this method can significantly improve a unification speed in parsing 
many real world text contain table in order to process these text correctly and extract the information contained within the table it is important to identify the presence and structure of table in this paper we present a new approach that learns to recognize table in free text including the boundary row and column of table when tested on wall street journal news document our learning approach outperforms a deterministic table recognition algorithm that identifies table recognition algorithm that identifies table based on a fixed set of condition our learning approach is also more flexible and easily adaptable to text in different domain with different table characteristic 
in this paper we present a machine learning approachto question answering the task is answeringfactual question where the answer are to be foundin document in a large text database we trainedour system on question from the remedia corpus a well a trec development question we then evaluated our system on question ofthe trec question answering task although ourlearning approach only us feature we are ableto achieve quite competitive accuracy the 
this paper describes and extensively evaluates a system for the automatic routing of submitted paper to reviewer and area committee without the need for any human annotation from the reviewer or the program chair routing is based on a profile of previous writing obtainable on line for the reviewer pool a generally stable and reusable resource that requires no manual adaptation for new submission stream the paper explores a wide set of variation and extension on the core model and achieves system accuracy approaching that of several human judge on the same task 
in this paper we present an analysis of st ylistic variation that us a factor analytic technique to group the variable responsible for the bulk of the linguistic variation found in a corpus of pharmaceutica l leaflet two main factor of variation were found and analysed in more detail they also were compared with other two analysis 
the gram model at the same model size our analysis show that the high performance of the acm lie in the asymmetry of the model 
we present work on the automatic generation of short indicative informative abstract of scientific and technical article the indicative part of the abstract identifies the topic of the document while the informative part of the abstract elaborate some topic according to the reader s interest by motivating the topic describing entity and defining concept we have defined our method of automatic abstracting by studying a corpus professional abstract the method also considers the reader s interest a essential in the process of abstracting 
the main aim of this paper is to analyse the effect of applying pronominal anaphora resolution to question answering qa system for this task a complete qa system ha been implemented system evaluation measure performance improvement obtained when information that is referenced anaphorically in document is not ignored 
we present a parsing system built from a handwritten lexicon and grammar and trained on a selection of the brown corpus on the sentence it can parse the parser performs a well a purely corpus based parser it advantage lie in the fact that it syntactic analysis readily support semantic interpretation moreover the system s hand written foundation allows for a more fully lexicalized probabilistic model i e one sensitive to co occurrence of lexical head of phrase constituent 
in this paper we outline a theory of referential accessibility called vein theory vt we show how vt address the problem of left satellite currently a problem for stack based model and show that vt can be used to significantly reduce the search space for antecedent we also show that vt provides a better model for determining domain of referential accessibility and discus how vt can be used to address various issue of structural ambiguity 
in this paper we present a thorough evaluation of a corpus resource for portuguese cetemp blico a million word newspaper corpus free for r d in portuguese processing we provide information that should be useful to those using the resource and to considerable improvement for later version in addition we think that the procedure presented can be of interest for the larger nlp community since corpus evaluation and description is unfortunately not a common exercise 
in this paper brill s rule based po tagger is tested and adapted for hungarian it i s s hown that t he present system doe not obtain a high accuracy for hungarian a it doe for english and other germanic language because of the structural difference between these language hungarian un like english ha rich morphology is agglutinative with some inflectional characteristic and h a fairly free word order the tagger ha the greatest difficulty with part of speech belonging to open class because of their complicated morphological structure it is shown that the accuracy of tagging can be increased from approximately to by simply changing the rule generating mechanism namely the lexical template in the lexical training module 
we present result of machine learning experiment designed to identify user correction of speech recognition error in a corpus collected from a train information spoken dialogue system we investigate the predictive power of feature automatically computable from the prosody of the turn the speech recognition process experimental condition and the dialogue history our best performing feature reduce classification error from baseline of to 
previous research ha shown that the plausibility of an adjective noun combination is correlated with it corpus co occurrence frequency in this paper we estimate the co occurrence frequency of adjective noun pair that fail to occur in a million word corpus using smoothing technique and compare them to human plausibility rating both class based smoothing and distance weighted averaging yield frequency estimate that are significant predictor of rated plausibility which provides independent evidence for the validity of these smoothing technique 
broad coverage corpus annotated with semantic role or argument structure information are becoming available for the first time statistical system have been trained to automatically label semantic role from the output of statistical parser on unannotated text in this paper we quantify the effect of parser accuracy on these system performance and examine the question of whether a flatter chunked representation of the input can be a effective for the purpose of semantic role identification 
traditionally coreference is resolved by satisfying a combination of salience syntactic semantic and discourse constraint the acquisition of such knowledge is time consuming difficult and error prone therefore we present a knowledge minimalist methodology of mining coreference rule from annotated text corpus semantic consistency evidence which is a form of knowledge required by coreference is easily retrieved from wordnet additional consistency knowledge is discovered by a meta bootstrapping algorithm applied to unlabeled text 
best first chart parsing utilises a figure of merit fom to efficiently guide a parse by first attending to those edge judged better in the past it ha usually been static this paper will show that with some extra information a parser can compensate for fom flaw which otherwise slow it down our result are faster than the prior best by a factor of and the speedup is won with no significant decrease in parser accuracy 
this paper proposes a method for incrementally understanding user utterance whose semantic boundary are not known and responding in real time even before boundary are determined it is an integrated parsing and discourse processing method that update the partial result of understanding word by word enabling response based on the partial result this method incrementally find plausible sequence of utterance that play crucial role in the task execution of dialogue and utilizes beam search to deal with the ambiguity of boundary a well a syntactic and semantic ambiguity the result of a preliminary experiment demonstrate that this method understands user utterance better than an understanding method that assumes pause to be semantic boundary 
we propose index to measure the difficulty of the named entity ne task by looking at test corpus based on expression inside and outside the ne these index are intended to estimate the difficulty of each task without actually using an ne system and to be unbiased towards a specific system the value of the index are compared with the system performance in japanese document we also discus the difference between ne class with the index and show useful clue which will make it easier to recognize ne 
we present an approach tolanguage specific query basedsampling which given a singledocument in a target language can find many more example ofdocuments in that language byautomatically constructing queriesto access such document on theworld wide web we propose a numberof method for building searchqueries to quickly obtain documentsin the target language they performaccurately and efficiently forbuilding a corpus of document intagalog starting from a single 
recently statistical machine translation model have begun to take advantage of higher level linguistic structure such a syntactic dependency underlying these model is an assumption about the directness of translational correspondence between sentence in the two language however the extent to which this assumption is valid and useful is not well understood in this paper we present an empirical study that quantifies the degree to which syntactic dependency are preserved when par are projected directly from english to chinese our result show that although the direct correspondence assumption is often too restrictive a small set of principled elementary linguistic transformation can boost the quality of the projected chinese par by relative to the unimproved baseline 
this paper present an open domain textual question answering system that us several feedback loop to enhance it performance these feedback loop combine in a new way statistical result with syntactic semantic or pragmatic information derived from text and lexical database the paper present the contribution of each feedback loop to the overall performance of human assessed precise answer 
we describe a case study in which a memory based learning algorithm is trained to simultaneously chunk sentence and assign grammatical function tag to these chunk we compare the algorithm s performance on this parsing task with varying training set size yielding learning curve and different input representation in particular we compare input consisting of word only a variant that includes word form information for low frequency word gold standard po only and combination of these the word based shallow parser display an apparently log linear increase in performance and surpasses the flatter po based curve at about sentence of training data the low frequency variant performs even better and the combination is best comparative experiment with a real po tagger produce lower result we argue that we might not need an explicit intermediate po tagging step for parsing when a sufficient amount of training material is available and word form information is used for low frequency word 
introductionanaphora resolution is a complicated problem in natural language processing and hasattracted the attention of many researcher the approach developed traditional from purely syntactic one to highly semantic and pragmatic one alternative statistic uncertainty reasoning etc or knowledge poor offer only approximatesolutions the paper is an introduction to anaphora resolution offering a brief survey of the majorworks in the field basic notion and 
in a language generation system a content planner embodies one or more plan that are usually hand crafted sometimes through manual analysis of target text in this paper we present a system that we developed to automatically learn element of a plan and the ordering constraint among them a training data we use semantically annotated transcript of domain expert performing the task our system is designed to mimic given the large degree of variation in the spoken language of the transcript we developed a novel algorithm to find parallel between transcript based on technique used in computational genomics our proposed methodology wa evaluated two fold the learning and generalization capability were quantitatively evaluated using cross validation obtaining a level of accuracy of a qualitative evaluation is also provided 
we present a new chart parsing method for lambek grammar inspired by a method for d tree grammar parsing the formula of a lambek sequent are firstly converted into rule of an indexed grammar formalism which are used in an earley style predictive chart algorithm the method is non polynomial but performs well for practical purpose much better than previous chart method for lambek grammar 
this paper describes a lexicon organized around systematic polysemy a set of word sens that are related in systematic and predictable way the lexicon is derived by a fully automatic extraction method which utilizes a clustering technique called tree cut we compare our lexicon to wordnet cousin and the inter annotator disagreement observed between wordnet semcor and dso corpus 
it is widely recognized that the proliferation of annotation scheme run counter to the need to re use language resource and that standard for linguistic annotation are becoming increasingly mandatory to answer this need we have developed a representation framework comprised of an abstract model for a variety of different annotation type e g morpho syntactic tagging syntactic annotation co reference annotation etc which can be instantiated in different way depending on the annotator s approach and goal in this paper we provide an overview of our representation framework and demonstrate it applicability to syntactic annotation we show how the framework can contribute to comparative evaluation and merging of parser output and diverse syntactic annotation scheme 
we present an architecture for the integration of shallow and deep nlp component which is aimed at flexible combination of different language technology for a range of practical current and future application in particular we describe the integration of a high level hpsg parsing system with different high performance shallow component ranging from named entity recognition to chunk parsing and shallow clause recognition the nlp component enrich a representation of natural language text with layer of new xml meta information using a single shared data structure called the text chart we describe detail of the integration method and show how information extraction and language checking application for realworld german text benefit from a deep grammatical analysis 
this article present a dependency parsing scheme using an extended finite state approach the parser augments input representation with channel so that link representing syntactic dependency relation among word can be accommodated and iterates on the input a number of time to arrive at a fixed point intermediate configuration violating various constraint of projective dependency representation such a no crossing link and no independent item except sentential head are filtered via finite state filter we have applied the parser to dependency parsing of turkish 
choosing the best lexeme to realize a meaning in natural language generation is a hard task we investigate different tree based stochastic model for lexical choice because of the difficulty of obtaining a sense tagged corpus we generalize the notion of synonymy we show that a tree based model can achieve a word bag based accuracy of representing an improvement over the baseline 
to verify hardware design by model checking circuit specification are commonly expressed in the temporal logic ctl automatic conversion of english to ctl requires the definition of an appropriately restricted subset of english we show how the limited semantic expressibility of ctl can be exploited to derive a hierarchy of subset our strategy avoids potential difficulty with approach that take existing computational semantic analysis of english a their starting point such a the need to ensure that all sentence in the subset posse a ctl translation 
this paper describes how a machine learning named entity recognizer ner on upper case text can be improved by using a mixed case ner and some unlabeled text the mixed case ner can be used to tag some unlabeled mixed case text which are then used a additional training material for the upper case ner we show that this approach reduces the performance gap between the mixed case ner and the upper case ner substantially by for muc and for muc named entity test data our method is thus useful in improving the accuracy of ners on upper case text such a transcribed text from automatic speech recognizers where case information is missing 
the paper describes the application of k mean a standard clustering technique to the task of inducing semantic class for german verb using probability distribution over verb subcategorisation frame we obtained an intuitively plausible clustering of verb into class the automatic clustering wa evaluated against independently motivated hand constructed semantic verb class a series of post hoc cluster analysis explored the influence of specific frame and frame group on the coherence of the verb class and supported the tight connection between the syntactic behaviour of the verb and their lexical meaning component 
we present a new method to automatically merge lexicon that employ different incompatible po category such incompatibility have hindered effort to combine lexicon to maximize coverage with reasonable human effort given an original lexicon our method is able to merge lexeme from an additional lexicon into the original lexicon converting lexeme from the additional lexicon with about precision this level of precision is achieved with the aid of a device we introduce called an anti lexicon which neatly summarizes all the essential information we need about the co occurrence of tag and lemma our model is intuitive fast easy to implement and doe not require heavy computational resource nor training corpus lemma i tag 
in human sentence processing cognitive load can be defined many way this report considers a definition of cognitive load in term of the total probability of structural option that have been disconfirmed at some point in a sentence the surprisal of word wi given it prefix wo i on a phrase structural language model these load can be efficiently calculated using a probabilistic earley parser stolcke which is interpreted a generating prediction about reading time on a word by word basis under grammatical assumption supported by corpus frequency data the operation of stolcke s probabilistic earley parser correctly predicts processing phenomenon associated with garden path structural ambiguity and with the subject object relative asymmetry 
this paper describes articulatory adaptation in humanto human communication a computational model of such adaption by human communicator and the application of this work intended primarily for the design of communication aid but with potential application to multimodal communicative agent background communicator must adapt their strategy not only in response to the communicative act performed by other interlocutor but also in consideration of their own mean for the perception and articulation of act of communication the impact of the latter the focus of our current research is especially relevant to individual who are affected by expressive communication disorder past research ha addressed adaptation by a communicative agent to uncertainty with respect to it own percept paek and horvitz in contrast our work concern adaptation by a communicator to uncertainty with respect to his or her own embodiment and the affordances of that embodiment for performing act of communication and while past research concern the design of communicative agent for use in dialogue system our work concern the development of communicative agent for use in computational simulation individual with expressive communication disorder must adapt their strategy in response to constraint arising from physical disorder which can impair the individual s ability to perform certain act of communication and can have intermittent or chronic effect e g neuromuscular dysfunction resulting in spasticity atonicity or excessive fatigue for instance the intelligibility of an individual s speech or gesture might vary over the course of an interaction from somewhat intelligible to not at all when functional communication is not possible clinical intervention possibly in the form of a voice output communication aid a type of augmentative and alternative communication aac device can provide interlocutor with an additional aided mode of communication in the form of synthesized speech and while these device can be useful they are slow and extremely costly to use in term of physical exertion in addition the interface of these device often compete with or subvert the communicator s adaptive strategy for instance an interlocutor anticipating her next conversational turn must look down at the display of the device in order to compose a spoken utterance precisely at the point where eye gaze is important function in regulating turn taking such communicator must also adapt to their interlocutor whose level of familiarity with these device can signal which communicative strategy are likely to be fruitful a familiar partner is likely to understand a strategy of using abbreviation or gesture which is effort saving but requires a pre existing shared understanding while an unfamiliar partner is not an poorly chosen strategy can result in great expenditure of effort to signal and to repair any resulting misunderstanding often more effort than would have been exerted had a more costly strategy been chosen baljko b so in addition to perceptual uncertainty aided communicator also face articulatory uncertainty that is uncertainty about which mode of communication aided or unaided are most appropriate the strategy for mode use must be chosen with care a conservative choice can lead to unnecessary exertion while a risky choice can lead to the breakdown of communication compounding the difficulty the choice of strategy depends on communicator s uncertain perception of his own possibly shifting expressive capability and of the common ground accumulating with his communication partner 
current alternative for language modeling are statistical technique based on large amount of training data and hand crafted context free or finite state grammar that are difficult to build and maintain one way to address the problem of the grammar based approach is to compile recognition grammar from grammar written in a more expressive formalism while theoretically straight forward the compilation process can exceed memory and time bound and might not always result in accurate and efficient speech recognition we will describe and evaluate two approach to this compilation problem we will also describe and evaluate additional technique to reduce the structural ambiguity of the language model 
spoken dialogue manager have benefited from using stochastic planner such a markov decision process mdps however so far mdps do not handle well noisy and ambiguous speech utterance we use a partially observable markov decision process pomdp style approach to generate dialogue strategy by inverting the notion of dialogue state the state represents the user s intention rather than the system state we demonstrate that under the same noisy condition a pomdp dialogue manager make fewer mistake than an mdp dialogue manager furthermore a the quality of speech recognition degrades the pomdp dialogue manager automatically adjusts the policy 
named entity phrase are some of the most difficult phrase to translate because new phrase can appear from nowhere and because many are domain specific not to be found in bilingual dictionary we present a novel algorithm for translating named entity phrase using easily obtainable monolingual and bilingual resource we report on the application and evaluation of this algorithm in translating arabic named entity to english we also compare our result with the result obtained from human translation and a commercial system for the same task 
this paper describes a method of translating a predicate argument structure of a verb into that of an equivalent verb which is a core component of the dictionary based paraphrasing our method grasp several usage of a headword and those of the def head a a form of their case frame and aligns those case frame which mean the acquisition of word sense disambiguation rule and the detection of the appropriate equivalent and case marker transformation 
while paraphrasing is critical both for interpretation and generation of natural language current system use manual or semi automatic method to collect paraphrase we present an unsupervised learning algorithm for identification of paraphrase from a corpus of multiple english translation of the same source text our approach yield phrasal and single word lexical paraphrase a well a syntactic paraphrase 
this paper investigates the potential for projecting linguistic annotation including part of speech tag and base noun phrase bracketings from one language to another via automatically word aligned parallel corpus first experiment ass the accuracy of unmodified direct transfer of tag and bracket from the source language english to the target language french and chinese both for noisy machine aligned sentence and for clean hand aligned sentence performance is then substantially boosted over both of these baseline by using training technique optimized for very noisy data yielding core french part of speech tag accuracy and french bracketing f measure for stand alone monolingual tool trained without the need for any human annotated data in the given language 
this paper present a novel statistical model for automatic identification of english basenp it us two step the n best part of speech po tagging and basenp identification given the n best po sequence unlike the other approach where the two step are separated we integrate them into a unified statistical framework our model also integrates lexical information finally viterbi algorithm is applied to make global search in the entire sentence allowing u to obtain linear complexity for the entire process compared with other method using the same testing set our approach achieves in precision and in recall the result is comparable with or better than the previously reported result 
arabic is the language of million of people all over the world y et a publicly available grammatically tagged corpus of arabic still doe not exist in this paper i describe some of the initial finding in the development of an arabic part of speech tagger for this tagger i have c ompiled a tagset containing tag that is derived from traditional arabic grammatical theory i have used this tagger to manually tag a c orpus and i have e xtracted a lexicon from this corpus because arabic is a morphologically complex language some preprocessing is necessary before automatic tagging can take place the result i have obtained so far highlight some of the characteristic of the arabic language that need to be addressed to improve tagging 
this paper describes named entity ne extraction based on a maximum entropy m e model and transformation rule there are two type of named entity when focusing on the relationship between morpheme and ne a defined in the ne task of the irex competition held in each ne consists of one or more morpheme or includes a substring of a morpheme we extract the former type of ne by using the m e model we then extract the latter type of ne by applying transformation rule to the text 
reducing language model lm size is a critical issue when applying a lm to realistic application which have memory constraint in this paper three measure are studied for the purpose of lm pruning they are probability rank and entropy we evaluated the performance of the three pruning criterion in a real application of chinese text input in term of character error rate cer we first present an empirical comparison showing that rank performs the best in most case we also show that the high performance of rank lie in it strong correlation with error rate we then present a novel method of combining two criterion in model pruning experimental result show that the combined criterion consistently lead to smaller model than the model pruned using either of the criterion separately at the same cer 
this paper proposes a description of german word order including phenomenon considered a complex such a scrambling partial vp fronting and verbal pied piping our description relates a syntactic dependency structure directly to a topological hierarchy without resorting to movement or similar mechanism 
in this paper we discus our approach toward establishing a model of the acquisition of english grammatical structure by user of our english language tutoring system which ha been designed for deaf user of american sign language we explore the correlation between a corpus of error tagged text and their holistic proficiency score assigned by expert in order to draw initial conclusion about what language error typically occur at different level of proficiency in this population since error made at lower level and not at higher level presumably represent construction acquired before those on which error are found only at higher level this should provide insight into the order of acquisition of english grammatical form 
human evaluation of machine translation are extensive but expensive human evaluation can take month to finish and involve human labor that can not be reused we propose a method of automatic machine translation evaluation that is quick inexpensive and language independent that correlate highly with human evaluation and that ha little marginal cost per run we present this method a an automated understudy to skilled human judge which substitute for them when there is need for quick or frequent evaluation 
multi processor system are becoming more commonplace and affordable based on analysis of actual parsings we argue that to exploit the capability of such machine unification based grammar parser should distribute work at the level of individual unification operation we present a generic approach to parallel chart parsing that meet this requirement and show that an implementation of this technique for lingo achieves considerable speedup 
strand resnik is a language independent system for automatic discovery of text in parallel translation on the world wide web this paper extends the preliminary strand result by adding automatic language identification scaling up by order of magnitude and formally evaluating performance the most recent end product is an automatically acquired parallel corpus comprising english french document pair approximately million word per language 
in a headed tree each terminal word can be uniquely labeled with a governing word and grammatical relation this labeling is a summary of a syntactic analysis which eliminates detail reflects aspect of semantics and for some grammatical relation such a subject of finite verb is nearly uncontroversial we define a notion of expected governor markup which sum vector indexed by governor and scaled by probabilistic tree weight the quantity is computed in a parse forest representation of the set of tree analysis for a given sentence using vector sum and scaling by inside probability and flow 
a central problem in part of speech tagging especially for new language for which limited annotated resource are available is estimating the distribution of lexical probability for unknown word this paper introduces a new paradigmatic similarity measure and present a minimally supervised learning approach combining effective selection and weighting method based on paradigmatic and contextual similarity measure populated from large quantity of inexpensive raw text data this approach is highly language independent and requires no modification to the algorithm or implementation to shift between language such a french and english 
a great deal of work ha been done demonstrating the ability of machine learning algorithm to automatically extract linguistic knowledge from annotated corpus very little work ha gone into quantifying the difference in ability at this task between a person and a machine this paper is a first step in that direction 
the order of prenominal adjectival modifier in english is governed by complex and difficult to describe constraint which straddle the boundary between competence and performance this paper describes and compare a number of statistical and machine learning technique for ordering sequence of adjective in the context of a natural language generation system 
educator are interested in essay evaluation system that include feedback about writing feature that can facilitate the essay revision process for instance if the thesis statement of a student s essay could be automatically identified the student could then use this information to reflect on the thesis statement with regard to it quality and it relationship to other discourse element in the essay using a relatively small corpus of manually annotated data we use bayesian classification to identify thesis statement this method yield result that are much closer to human performance than the result produced by two baseline system 
we have written hardware simulator in acl in order to unify highspeed simulator and formal analysis model the technique used for these simulator extend to other kind of software which we demonstrate in this paper by implementing a much faster version of an algorithm for graph path nding previously verified by j moore using acl this exercise also highlight a weakness in acl the occasional need to add computational complexity to function in order to admit them to the 
an approximate word matching algorithm for chinese is presented based on this algorithm an effective approach to chinese spelling error detection and correction is implemented with a word tri gram language model the optimal string is searched from all possible derivation of the input sentence using operation of character substitution insertion and deletion comparing the original sentence with the optimal string spelling error detection and correction is realized simultaneously 
rambow wier and vijay shanker rambow et al point out the difference between tag derivation structure and semantic or predicate argument dependency and joshi and vijay shanker joshi and vijay shanker describe a monotonic compositional semantics based on attachment order that represents the desired dependency of a derivation without underspecifying predicate argument relationship at any stage in this paper we apply the joshi and vijay shanker conception of compositional semantics to the problem of preserving semantic dependency in synchronous tag translation shieber and schabes abeill et al in particular we describe an algorithm to obtain the semantic dependency on a tag parse forest and construct a target derivation forest with isomorphic or locally non isomorphic dependency in o n time 
hypernym link acquired through an information extraction procedure are projected on multi word term through the recognition of semantic variation the quality of the projected link resulting from corpus based acquisition is compared with projected link extracted from a technical thesaurus 
the paper develops a constraint based theory of prosodic phrasing and prominence based on an hpsg framework with an implementation in ale prominence and juncture are represented by n ary branching metrical tree the general aim is to define prosodic structure recursively in parallel with the definition of syntactic structure we address a number of prima facie problem arising from the discrepancy between syntactic and prosodic structure 
this paper describes a system for unsupervised learning of morphological affix from text or word list the system is composed of a generative probability model and a search algorithm experiment on the wall street journal and the hansard corpus french and english demonstrate the effectiveness of this approach the result suggest that more integrated system for learning both affix and morphographemic adjustment rule may be feasible in addition several definition and a theorem are developed so that our search algorithm can be formalized in term of the lattice formed by subset of suffix under inclusion this formalism is expected to be useful for investigating alternative search strategy over the same morphological hypothesis space 
in recent year statistical approach on atr automatic term recognition have achieved good result however there are scope to improve the performance in extracting term still further for example domain dictionary can improve the performance in atr this paper focus on a method for extracting term using a dictionary hierarchy our method produce relatively good result for this task 
the aim of this study is a systematic evaluationand comparison of four state of the art datadrivenlearning algorithm applied to part ofspeech tagging of swedish the algorithm includedin this study are hidden markov model maximum entropy memory based learning and transformation based learning the systemsare evaluated from several aspect boththe eects of tag set and the eects of the sizeof training data are examined the accuracy iscalculated a well a the error rate for 
extractive summarization technique cannot generate document summary shorter than a single sentence something that is often required an ideal summarization system would understand each document and generate an appropriate summary directly from the result of that understanding a more practical approach to this problem result in the use of an approximation viewing summarization a a problem analogous to statistical machine translation the issue then becomes one of generating a target document in a more concise language from a source document in a more verbose language this paper present result on experiment using this approach in which statistical model of the term selection and term ordering are jointly applied to produce summary in a style learned from a training corpus 
we present a noun phrase coreference system that extends the work of soon et al and to our knowledge produce the best result to date on the muc and muc coreference resolution data set f measure of and respectively improvement arise from two source extra linguistic change to the learning framework and a large scale expansion of the feature set to include more sophisticated linguistic knowledge 
algorithm for the alignment of word in translated text are well established however only recently new approach have been proposed to identify word translation from non parallel or even unrelated text this task is more difficult because most statistical clue useful in the processing of parallel text cannot be applied to non parallel text whereas for parallel text in some study up to of the word alignment have been shown to be correct the accuracy for non parallel text ha been around up to now the current study which is based on the assumption that there is a correlation between the pattern of word co occurrence in corpus of different language make a significant improvement to about of word translation identified correctly 
the problem of machine translation can be viewed a consisting of two subproblems a lexical selection and b lexical reordering we propose stochastic finite state model for these two subproblems in this paper stochastic finite state model are efficiently learnable from data effective for decoding and are associated with a calculus for composing model which allows for tight integration of constraint from various level of language processing we present a method for learning stochastic finite state model for lexical choice and lexical reordering that are trained automatically from pair of source and target utterance we use this method to develop model for english japanese translation and present the performance of these model for translation on speech and text we also evaluate the efficacy of such a translation model in the context of a call routing task of unconstrained speech utterance 
there are many challenging problem forvietnamese language processing it will be along time before these challenge are met evensome apparently simple problem such asspelling correction are quite difficult and havenot been approached systematically yet in thispaper we will discus one aspect of this type ofwork designing the so called vietools to detectand correct spelling of vietnamese text byusing a spelling database based on telexcode vietools is also extended to serve 
the stop system which generates personalised smoking cessation letter wa evaluated by a randomised controlled clinical trial we believe this is the largest and perhaps most rigorous task effectiveness evaluation ever performed on an nlg system the detailed result of the clinical trial have been presented elsewhere in the medical literature in this paper we discus the clinical trial itself it structure and cost what we did and did not learn from it especially considering that the trial showed that stop wa not effective and how it compare to other nlg evaluation technique 
using finite state automaton for the text analysis component in a text to speech system is problematic in several respect the rewrite rule from which the automaton are compiled are difficult to write and maintain and the resulting automaton can become very large and therefore inefficient converting the knowledge represented explicitly in rewrite rule into a more efficient format is difficult we take an indirect route learning an efficient decision tree representation from data and tapping information contained in existing rewrite rule which increase performance compared to learning exclusively from a pronunciation lexicon 
we show that discourse structure need not bear the full burden of conveying discourse relation by showing that many of them can be explained nonstructurally in term of the grounding of anaphoric presupposition van der sandt this simplifies discourse structure while still allowing the realisation of a full range of discourse relation this is achieved using the same semantic machinery used in deriving clause level semantics 
this paper present a revision learning method that achieves high performance with small computational cost by combining a model with high generalization capacity and a model with small computational cost this method us a high capacity model to revise the output of a small cost model we apply this method to english part of speech tagging and japanese morphological analysis and show that the method performs well 
the noisy channel model ha been applied to a wide range of problem including spelling correction these model consist of two component a source model and a channel model very little research ha gone into improving the channel model for spelling correction this paper describes a new channel model for spelling correction based on generic string to string edits using this model give significant performance improvement compared to previously proposed model 
word alignment problem between parallel corpus is based on the similar characteristic of two aligned word in two language we investigate linguistic knowledge based word similarity measure while other previous work heavily rely on statistical information and their limit will be discussed linguistic knowledge is acquired from linguistic comparison of all layer between two language for chinese and korean in this paper 
filled pause are characteristic of spontaneous speech and can present considerable problem for speech recognition by being often recognized a short word an um can be recognized a thumb or arm if the recognizer s language model doe not adequately represent fp s recognition of quasi spontaneous speech medical dictation is subject to this problem a well result from medical dictation by family practice physician show that using an fp model trained on the corpus populated with fp s produce overall better result than a model trained on a corpus that excluded fp s or a corpus that had random fp s 
we seek a knowledge free method for inducing multiword unit from text corpus for use a machine readable dictionary headword we provide two major evaluation of nine existing collocation finder and illustrate the continuing need for improvement we use latent semantic analysis to make modest gain in performance but we show the significant challenge encountered in trying this approach 
we develop a framework for formalizing semantic construction within grammar expressed in typed feature structure logic including hpsg the approach provides an alternative to the lambda calculus it maintains much of the desirable flexibility of unification based approach to composition while constraining the allowable operation in order to capture basic generalization and improve maintainability 
this paper introduces a new approach to morpho syntactic analysis through humor high speed unification morphology a reversible and unification based morphological analyzer which ha already been integrated with a variety of industrial application humor successfully cope with problem of agglutinative e g hungarian turkish estonian and other highly inflectional language e g polish czech german very effectively the author conclude the paper by arguing that the approach used in humor is general enough to be well suitable for a wide range of language and can serve a basis for higher level linguistic operation such a shallow parsing 
stochastic unification based grammar subgs define exponential distribution over the par generated by a unification based grammar ubg existing algorithm for parsing and estimation require the enumeration of all of the par of a string in order to determine the most likely one or in order to calculate the statistic needed to estimate a grammar from a training corpus this paper describes a graph based dynamic programming algorithm for calculating these statistic from the packed ubg parse representation of maxwell and kaplan which doe not require enumerating all par like many graphical algorithm the dynamic programming algorithm s complexity is worst case exponential but is often polynomial the key observation is that by using maxwell and kaplan packed representation the required statistic can be rewritten a either the max or the sum of a product of function this is exactly the kind of problem which can be solved by dynamic programming over graphical model 
this paper present an unsupervised method for choosing the correct translation of a word in context it learns disambiguation information from nonparallel bilingum corpus preferably in the same domain free from tagging our method combine two existing unsupervised disambiguation algorithm a word sense disambiguation algorithm based on distributional clustering and a translation disambiguation algorithm using target language corpus for the given word in context the former algorithm identifies it meaning a one of a number of predefined usage class derived by clustering a large amount of usage in the source language corpus the latter algorithm is responsible for associating each usage class i e cluster with a target word that is most relevant to the usage this paper also show preliminary result of translation experiment 
in this paper we use various method for multiple neural network combination in task of prepositional phrase attachment experiment with aggregation function such a unweighted and weighted average owa operator choquet integral and stacked generalization demonstrate that combining multiple network improve the estimation of each individual neural network using the ratnaparkhi data set the complete training set and the complete test set we obtained an accuracy score of in spite of the high cost in computational time of neural net training the response time in test mode is faster than others method 
this paper describes a decoding algorithm for a syntax based translation model yamada and knight the model ha been extended to incorporate phrasal translation a presented here in contrast to a conventional word to word statistical model a decoder for the syntax based model build up an english parse tree given a sentence in a foreign language a the model size becomes huge in a practical setting and the decoder considers multiple syntactic structure for each word alignment several pruning technique are necessary we tested our decoder in a chinese to english translation system and obtained better result than ibm model we also discus issue concerning the relation between this decoder and a language model 
typically the lexicon model used in statistical machine translation system do not include any kind of linguistic or contextual information which often lead to problem in performing a correct word sense disambiguation one way to deal with this problem within the statistical framework is to use maximum entropy method in this paper we present how to use this type of information within a statistical machine translation system we show that it is possible to significantly decrease training and test corpus perplexity of the translation model in addition we perform a rescoring of n best list using our maximum entropy model and thereby yield an improvement in translation quality experimental result are presented on the so called verbmobil task 
we describe a new framework for dependency grammar with a modular decomposition of immediate dependency and linear precedence our approach distinguishes two orthogonal yet mutually constraining structure a syntactic dependency tree and a topological dependency tree the syntax tree is nonprojective and even non ordered while the topological tree is projective and partially ordered 
in this paper we present an exercise in compiler source level verification actually the source and target language and the compiler have already been used in our article for the acl case study book goe a where we prove that source level correctness is not at all sufficient to prove compiler executables correct however the proof is interesting for itself and the fact that the compiler used in goe a is indeed proved correct is essential for the message of that article so we want 
a two tier model for the description of morphological syntactic and semantic variation of multi word term is presented it is applied to term normalization of french and english corpus in the medical and agricultural domain five differenct source of morphological and semantic knowledge are exploited multext celex agrovoc wordnet and microsoft word thesaurus 
this paper address recent progress in speaker independent large vocabulary continuous speech recognition which ha opened up a wide range of near and mid term application one rapidly expanding application area is the processing of broadcast audio for information access at limsi broadcast news transcription system have been developed for english french german mandarin and portuguese and system for other language are under development audio indexation must take into account the specificity of audio data such a needing to deal with the continuous data stream and an imperfect word transcription some near term application area are audio data mining selective dissemination of information and medium monitoring 
the definition of the basic concept rule and constraint of centering theory involve underspecified notion such a previous utterance realization and ranking we attempted to find the best way of defining each such notion among those that can be annotated reliably and using a corpus of text in two domain of practical interest our main result is that trying to reduce the number of utterance without a backward looking center cb result in an increased number of case in which some discourse entity but not the cb get pronominalized and viceversa 
native and non native use of language differs depending on the proficiency of the speaker in clear and quantifiable way it ha been shown that customizing the acoustic and language model of a natural language understanding system can significantly improve handling of non native input in order to make such a switch however the nativeness status of the user must be known in this paper we show that naive bayes classification can be used to identify non native utterance of english the advantage of our method is that it relies on text not on acoustic feature and can be used when the acoustic source is not available we demonstrate that both read and spontaneous utterance can be classified with high accuracy and that classification of errorful speech recognizer hypothesis is accurate than classification of perfect transcription we also characterize part of speech sequence that play a role in detecting non native speech 
in a question answering system the question canbe classied based on their degree of diculty asthe level of diculty increase question answeringsystems need to rely on richer semantic ontologiesand larger knowledge base this paper is concernedwith question whose answer are spread across severaldocuments and thus require answer fusion tond such answer the system need to develop domainspecic ontology a method is presented foron line acquisition of ontological 
experiment are presented which measure the perplexity reduction derived from incorporating into the predictive model utilised in a standard tag n gram part of speech tagger contextual information from previous sentence of a document the tagset employed is the roughly tag atr general english tagset whose tag are both syntactic and semantic in nature the kind of extrasentential information provided to the tagger is semantic and consists in the occurrence or non occurrence within the past sentence of the document being tagged of word tagged with particular tag from the tagset and of boolean combination of such condition in some case these condition are combined with the requirement that the word being tagged belong to a particular set of word thought most likely to benefit from the extrasentential information they are being conjoined with the baseline model utilized is a maximum entropy based tag ngram tagging model embodying a standard tag n gram approach to tagging i e constraint for tag trigram bigram and and the word tag occurrence frequency of the specific word being tagged form the basis of prediction added into to this baseline tagging model is the extrasentential semantic information just indicated the performance of the tagging model with and without the added contextual knowledge is contrasted training from the word atr general english treebank and testing on the accompanying word test treebank result are that a significant reduction in testset perplexity is achieved via the added semantic extrasentential information of the richer model the model with both long range tag trigger and more complex linguistic constraint achieved a perplexity reduction of 
abstract we present a constancy rate principle governing language generation we show that this principle implies that local measure of entropy ignoring context should increase with the sentence number we demonstrate that this is indeed the case by measuring entropy in three dierent way we also show that this eect ha both lexical which word are used and non lexical how the word are used cause 
previous work ha shown that automatic method can be used in building semantic lexicon this work go a step further by automatically creating not just cluster of related word but a hierarchy of noun and their hypernym akin to the hand built hierarchy in wordnet 
in this paper i discus issue pertinent to the design of a task based evaluation methodology for a spoken machine translation mt system processing human to human communication rather than human to machine communication i claim that system mediated human to human communication requires new evaluation criterion and metric based on goal complexity and the speaker s prioritization of goal 
developing dialogue system is a complex process in particular designing efficient dialogue management strategy is often difficult a there are no precise guideline to develop them and no sure test to validate them several suggestion have been made recently to use reinforcement learning to search for the optimal management strategy for specific dialogue situation these approach have produced interesting result including application involving real world dialogue system however reinforcement learning suffers from the fact that it is state based in other word the optimal strategy is expressed a a decision table specifying which action to take in each specific state it is therefore difficult to see whether there is any generality across state this limit the analysis of the optimal strategy and it potential for re use in other dialogue situation in this paper we tackle this problem by learning rule that generalize the state based strategy these rule are more readable than the underlying strategy and therefore easier to explain and re use we also investigate the capability of these rule in directing the search for the optimal strategy by looking for generalization whilst the search proceeds 
while the generative view of language processing build bigger unit out of smaller one by mean of rewriting step the axiomatic view eliminates invalid linguistic structure out of a set of possible structure by mean of well formedness principle we present a generator based on the axiomatic view and argue that when combined with a tag like grammar and a flat semantics this axiomatic view permit avoiding drawback known to hold either of top down or of bottom up generator 
we have developed a system that generates evaluative argument that are tailored to the user properly arranged and concise we have also developed an evaluation framework in which the effectiveness of evaluative argument can be measured with real user this paper present the result of a formal experiment we have performed in our framework to verify the influence of argument conciseness on argument effectiveness 
this paper examines the use of an unsupervised statistical model for determining the attachment of ambiguous coordinate phrase cp of the form n p n cc n the model presented here is based on ar an unsupervised model for determining prepositional phrase attachment after training on unannotated wall street journal text the model performs at accuracy on a development set from section through of the wsj treebank msm 
this paper present a bottom up generator that make use of information retrieval technique to rank potential generation candidate by comparing them to a data base of stored instance we introduce two general technique to address the search problem expectation driven search and dynamic grammar rule selection and present the architecture of an implemented generation system called igen our approach us a domain specific generation grammar that is automatically derived from a semantically tagged treebank we then evaluate the efficiency of our system 
this paper describes a program which revise a draft text by aggregating together description of discourse entity in addition to deleting extraneous information in contrast to knowledge rich sentence aggregation approach explored in the past this approach exploit statistical parsing and robust coreference detection in an evaluation involving revision of topic related summary using informativeness measure from the tipster summac evaluation the result show gain in informativeness without compromising readability 
previous comparison of document and query translation suffered difficulty due to differing quality of machine translation in these two opposite direction we avoid this difficulty by training identical statistical translation model for both translation direction using the same training data we investigate information retrieval between english and french incorporating both translation direction into both document translation and query translation based information retrieval a well a into hybrid system we find that hybrid of document and query translation based system out perform query translation system even human quality query translation system 
we present a shift reduce rhetorical parsing algorithm that learns to construct rhetorical structure of text from a corpus of discourse parse action sequence the algorithm exploit robust lexical syntactic and semantic knowledge source 
this paper describes the application of the paradise evaluation framework to the corpus of human computer dialogue collected in the june darpa communicator data collection we describe result based on the standard logfile metric a well a result based on additional qualitative metric derived using the date dialogue act tagging scheme we show that performance model derived via using the standard metric can account for of the variance in user satisfaction and that the addition of date metric improved the model by an absolute 
the emergence of internet a a global information repository where information of all kind is stored requires intelligent information processing tool i e computer application to help the information seeker to retrieve the stored information to build these intelligent information processing tool we need to build computer application that understand human language since most of those information is represented in human language this is where computational linguistics becomes important especially for country like indonesia that host more than million people we need to develop a systematic understanding of the bahasa indonesia the indonesian national language to enable u develop the needed computer application that will help u manage information intelligently 
we describe a biographical multi document summarizer that summarizes information about people described in the news the summarizer us corpus statistic along with linguistic knowledge to select and merge description of people from a document collection removing redundant description the summarization component have been extensively evaluated for coherence accuracy and non redundancy of the description produced 
we exploit and extend the generative lexicon theory to develop a formal description of adnominal constituent in a lexicon which can deal with linguistic phenomenon found in japanese adnominal constituent we classify the problematic behavior into static disambiguation and dynamic disambiguation task static disambiguation can be done using lexical information in a dictionary whereas dynamic disambiguation requires inference at the knowledge representation level 
we describe the use of xml tokenisation tagging and mark up tool to prepare a corpus for parsing our technique are generally applicable but here we focus on parsing medline abstract with the anlt wide coverage grammar hand crafted grammar inevitably lack coverage but many coverage failure are due to inadequacy of their lexicon we describe a method of gaining a degree of robustness by interfacing po tag information with the existing lexicon we also show that xml tool provide a sophisticated approach to pre processing helping to ameliorate the messiness in real language data and improve parse performance 
we present a document compression system that us a hierarchical noisy channel model of text production our compression system first automatically derives the syntactic structure of each sentence and the overall discourse structure of the text given a input the system then us a statistical hierarchical model of text production in order to drop non important syntactic and discourse constituent so a to generate coherent grammatical document compression of arbitrary length the system outperforms both a baseline and a sentence based compression system that operates by simplifying sequentially all sentence in a text our result support the claim that discourse knowledge play an important role in document summarization 
this paper present a corpus based algorithm capable of inducing inflectional morphological analysis of both regular and highly irregular form such a brought bring from distributional pattern in large monolingual text with no direct supervision the algorithm combine four original alignment model based on relative corpus frequency contextual similarity weighted string similarity and incrementally retrained inflectional transduction probability starting with no paired example for training and no prior seeding of legal morphological transformation accuracy of the induced analysis of past tense test case in english exceeds for the set with currently over accuracy on the most highly irregular form and accuracy on form exhibiting non concatenative suffixation 
this paper describes an extension to the hidden markov model for part of speech tagging using second order approximation for both contextual and lexical probability this model increase the accuracy of the tagger to state of the art level these approximation make use of more contextual information than standard statistical system new method of smoothing the estimated probability are also introduced to address the sparse data problem 
in this paper we propose a new method of text categorization based on feature space restructuring for svms in our method independent component of document vector are extracted using ica and concatenated with the original vector this restructuring make it possible for svms to focus on the latent semantic space without losing information given by the original feature space using this method we achieved high performance in text categorization both with small number and large number of labeled data 
we present a system for identifying the semantic relationship or filled by constituent of a sentence within a semantic frame various lexical and syntactic feature are derived from parse tree and used to derive statistical classifier from hand annotated training data 
we investigate a novel approach to solve the problem of sparse data through dimension reduction linear algebraic technique called lsa svd is used to find co relationship of sparse word three variant estimation method are suggested and they are evaluated for estimating unseen noun verb co occurrence probability the model show possibility to be alternative probability smoothing method 
this paper considers statistical parsing of czech which differs radically from english in at least two respect it is a highly inflected language and it ha relatively free word order these difference are likely to pose new problem for technique that have been developed on english we describe our experience in building on the parsing model of collins our final result dependency accuracy represent good progress towards the accuracy of the parser on english wall street journal text 
the paper show that movement or equivalent computational structure changing operation of any kind at the level of logical form can be dispensed with entirely in capturing quantifier scope ambiguity it offer a new semantics whereby the effect of quantifier scope alternation can be obtained by an entirely monotonic derivation without type changing rule the paper follows fodor fodor and sag and park in viewing many apparent scope ambiguity a arising from referential category rather than true generalized quantifier 
in this work we use a large text corpus toorder noun by their level of specificity thissemantic information can for most noun bedetermined with over accuracy usingsimple statistic from a text corpus withoutusing any additional source of semanticknowledge this kind of semantic informationcan be used to help in automaticallyconstructing or augmenting a lexicaldatabase such a wordnet introductionlarge lexical database such a wordnet see fellbaum are in common 
this paper present a method that assist in maintaining a rule based named entity recognition and classification system the underlying idea is to use a separate system constructed with the use of machine learning to monitor the performance of the rule based system the training data for the second system is generated with the use of the rule based system thus avoiding the need for manual tagging the disagreement of the two system act a a signal for updating the rule based system the generality of the approach is illustrated by applying it to large corpus in two different language greek and french the result are very encouraging showing that this alternative use of machine learning can assist significantly in the maintenance of rule based system 
over the year many proposal have been made to incorporate assorted type of feature in language model however discrepancy between training set evaluation criterion algorithm and hardware environment make it difficult to compare the model objectively in this paper we take an information theoretic approach to select feature type in a systematic manner we describe a quantitative analysis of the information gain and the information redundancy for various combination of feature type inspired by both dependency structure and bigram structure using a chinese treebank and taking word prediction a the object the experiment yield several conclusion on the predictive value of several feature type and feature type combination for word prediction which are expected to provide guideline for feature type selection in language modeling 
in this article we show how a bilingual texttranslation alignment method can be adapted to deal with more than two version of a text experiment on a trilingual corpus demonstrate that this method yield better bilingual alignment than can be obtained with bilingual textalignment method moreover for a given number of text the computational complexity of the multilingual method is the same a for bilingual alignment 
this paper describes automatic technique for mapping entry in a database of english verb to wordnet sens the verb were initially grouped into class based on syntactic feature mapping these verb into wordnet sens provides a resource that support disambiguation in multilingual application such a machine translation and cross language information retrieval our technique make use of a training set of disambiguated entry representing verb entry from class word sense probability from frequency count in a tagged corpus semantic similarity of wordnet sens for verb within the same class probabilistic correlation between wordnet data and attribute of the verb class the best result achieved precision and recall versus a lower bound of precision and recall for assigning the most frequently occurring wordnet sense and an upper bound of precision and recall for human judgment 
this paper describes how we annotated and analysed the np modifier in a corpus of museum description to discover rule for the selection and realisation of such modifier in particular non referring one we implemented the regularity into an extension of the ilex system to generate complex np capable of serving multiple communicative goal 
in this paper we describe a systematic approach for creating a dialog management system based on a construct algebra a collection of relation and operation on a task representation these relation and operation are analytical component for building higher level abstraction called dialog motivator the dialog manager consisting of a collection of dialog motivator is entirely built using the construct algebra 
in this paper we discus the use of cascaded finite state transducer for machine translation a number of small dedicated transducer is applied to convert sentence pair from a bilingual corpus into generalized translation pattern these pattern together with the transducer are then used a a hierarchical translation memory for fully automatic translation result on the german english verbmobil corpus are given 
we consider the question how much strong generative power can be squeezed out of a formal system without increasing it weak generative power and propose some theoretical and practical constraint on this problem we then introduce a formalism which under these constraint maximally squeeze strong generative power out of context free grammar finally we generalize this result to formalism beyond cfg 
in what sense is a grammar the union of it rule this paper adapts the notion of composition well developed in the context of programming language to the domain of linguistic formalism we study alternative definition for the semantics of such formalism suggesting a denotational semantics that we show to be compositional and fully abstract this facilitates a clear mathematically sound way for defining grammar modularity 
we apply support vector machine svms to identify english base phrase chunk svms are known to achieve high generalization performance even with input data of high dimensional feature space furthermore by the kernel principle svms can carry out training with smaller computational overhead independent of their dimensionality we apply weighted voting of svms based system trained with distinct chunk representation experimental result show that our approach achieves higher accuracy than previous approach 
existing algorithm for pronoun resolution typically cast the problem into a coreference task which mean they simply identify an antecedent noun phrase for each pronoun selection of the antecedent is usually based on a calculation of salience or focus this simplified approach is unable to account for pronoun without noun phrase antecedent example are abstract referent such a event proposition and speech act that might appear in the linguistic surface form a sentential complement verbal construction or entire sentence a well a consequence or outcome that don t appear in the surface form at all this paper contains a survey of current method of pronoun resolution for natural language understanding it then proposes a strategy for resolving pronominal reference to abstract entity that incorporates semantic information in addition to salience calculation preliminary experiment are described that show the strategy to perform well above baseline on a collection of spoken task oriented dialog 
most work in statistical parsing ha focused on a single corpus the wall street journal portion of the penn treebank while this ha allowed for quantitative comparison of parsing technique it ha left open the question of how other type of text might aect parser performance and how portable parsing model are across corpus we examine these question by comparing result for the brown and wsj corpus and also consider which part of the parser s probability model are particularly tuned to the corpus on which it wa trained this lead u to a technique for pruning parameter to reduce the size of the parsing model 
categorial grammar ha traditionally used the calculus to represent meaning we present an alternative dependency based perspective on linguistic meaning and situate it in the computational setting this perspective is formalized in term of hybrid logic and ha a rich yet perspicuous propositional ontology that enables a wide variety of semantic phenomenon to be represented in a single meaning formalism finally we show how we can couple this formalization to combinatory categorial grammar to produce interpretation compositionally 
this paper present an approach to automatically build a semantic perceptron net spn for topic spotting it us context at the lower layer to select the exact meaning of key word and employ a combination of context co occurrence statistic and thesaurus to group the distributed but semantically related word within a topic to form basic semantic node the semantic node are then used to infer the topic within an input document experiment on reuters data set demonstrate that spn is able to capture the semantics of topic and it performs well on topic spotting task 
in this paper we compare the relative effect of segment order segmentation and segment contiguity on the retrieval performance of a translation memory system we take a selection of both bag of word and segment order sensitive string comparison method and run each over both character and word segmented data in combination with a range of local segment contiguity model in the form of n gram over two distinct datasets we find that indexing according to simple character bigram produce a retrieval accuracy superior to any of the tested word n gram model further in their optimum configuration bag of word method are shown to be equivalent to segment order sensitive method in term of retrieval accuracy but much faster we also provide evidence that our finding are scalable 
this paper introduces a statistical model for query relevant summarization succinctly characterizing the relevance of a document to a query learning parameter value for the proposed model requires a large collection of summarized document which we do not have but a a proxy we use a collection of faq frequently asked question document taking a learning approach enables a principled quantitative evaluation of the proposed system and the result of some initial experiment on a collection of usenet faq and on a faq like set of customer submitted question to several large retail company suggest the plausibility of learning for summarization 
existing software system for automated essay scoring can provide nlp researcher with opportunity to test certain theoretical hypothesis including some derived from centering theory in this study we employ ets s e rater essay scoring system to examine whether local discourse coherence a defined by a measure of rough shift transition might be a significant contributor to the evaluation of essay our positive result indicate that rough shift do indeed capture a source of incoherence one that ha not been closely examined in the centering literature these result not only justify rough shift a a valid transition type but they also support the original formulation of centering a a measure of discourse continuity even in pronominal free text 
this paper examines the extent to which verb diathesis alternation are empirically attested in corpus data we automatically acquire alternating verb from large balanced corpus by using partialparsing method and taxonomic information and discus how corpus data can be used to quantify linguistic generalization we estimate the productivity of an alternation and the typicality of it member using type and token frequency 
named entity ne recognition is a task in which proper noun and numerical information in a document are detected and classified into category such a person organization location and date ne recognition play an essential role in information extraction system and question answering system it is well known that hand crafted system with a large set of heuristic rule are difficult to maintain and corpus based statistical approach are expected to be more robust and require le human intervention several statistical approach have been reported in the literature in a recent japanese ne workshop a maximum entropy me system outperformed decision tree system and most hand crafted system here we propose an alternative method based on a simple rule generator and decision tree learning our experiment show that it performance is comparable to the me approach we also found that it can be trained more efficiently with a large set of training data and that it improves readability 
identifying and classifying personal geographic institutional or other name in a text is an important task for numerous application this paper describes and evaluates a language independent bootstrapping algorithm based on iterative learning and re estimation of contextual and morphological pattern captured in hierarchically smoothed trie model the algorithm learns from unannotated text and achieves competitive performance when trained on a very short labelled name list with no other required language specific information tokenizers or tool 
both probabilistic context free grammar pcfgs and shift reduce probabilistic pushdown automaton ppdas have been used for language modeling and maximum likelihood parsing we investigate the precise relationship between these two formalism showing that while they define the same class of probabilistic language they appear to impose different inductive bias 
the end user of medical digital library need quick access to information that is specic to the patient under their care we present a summarization system that nd and extract result from multiple medical journal article returned by a search lters result that match the patient and merges and order the remaining fact for the summary our approach feature an integration of text categorization information extraction information fusion and text reformulation for the summarization task 
we introduce an annotation scheme for temporal expression and describe a method for resolving temporal expression in print and broadcast news the system which is based on both hand crafted and machine learnt rule achieves an accuracy f measure against hand annotated data some initial step towards tagging event chronology are also described 
this paper investigates several refinement of a generic tabular parser for tree adjoining grammar the resulting parser is simpler and more efficient in practice even though the worst case complexity is not optimal 
several recent stochastic parser use bilexical grammar where each word type idiosyncratically prefers particular complement with particular head word we present o n parsing algorithm for two bilexical formalism improving the prior upper bound of o n for a common special case that wa known to allow o n parsing eisner we present an o n algorithm with an improved grammar constant 
this paper describes a statistical methodology ibr automatically retrieving collocation from po tagged korean text using interrupted bigram the free order of korean make it hard to identify collocation we devised four statistic frequency randomness condensation and correlation to account for the more flexible word order property of korean collocation we extracted meaningful bigram using an evaluation ihnction and extended the bigram to n gram collocation by generating equivalence set a cover we view a modeling problem for n gram collocation a that for clustering of cohesive word 
in term of both speed and memory consumption graph unification remains the most expensive component of unification based grammar parsing we present a technique to reduce the memory usage of unification algorithm considerably without increasing execution time also the proposed algorithm is thread safe providing an efficient algorithm for parallel processing a well 
this paper deal with translation ambiguity and target polysemy problem together two monolingual balanced corpus are employed to learn word co occurrence for translation ambiguity resolution and augmented translation restriction for target polysemy resolution experiment show that the model achieves of monolingual information retrieval and is addition to the select all model combining the target polysemy resolution the retrieval performance is about increase to the model resolving translation ambiguity only 
an algorithm for semantic interpretation is explained the algorithm is based on predicate defined for wordnet verb class the algorithm is driven by the definition of these predicate whose thematic role are linked to the wordnet ontology for noun and to the syntactic relation that realize them the algorithm ha been tested in the identification of the meaning of the verb thematic role and temporal and spatial adjunct 
this paper introduces a new unsupervised algorithm for noun phrase coreference resolution it differs from existing method in that it view corererence resolution a a clustering task in an evaluation on the muc coreference resolution corpus the algorithm achieves an f measure of placing it firmly between the worst and best system in the muc evaluation more importantly the clustering approach outperforms the only muc system to treat coreference resolution a a learning problem the clustering algorithm appears to provide a flexible mechanism for coordinating the application of context independent and context dependent constraint and preference for accurate partitioning of noun phrase into coreference equivalence class 
weighted finite state transducer suffer from the lack of a training algorithm training is even harder for transducer that have been assembled via finite state operation such a composition minimization union concatenation and closure a this yield tricky parameter tying we formulate a parameterized fst paradigm and give training algorithm for it including a general bookkeeping trick expectation semirings that cleanly and efficiently computes expectation and gradient 
in this paper a new language model the multi class composite n gram is proposed to avoid a data sparseness problem for spoken language in that it is difficult to collect training data the multi class composite n gram maintains an accurate word prediction capability and reliability for sparse data with a compact model size based on multiple word cluster called multi class in the multi class the statistical connectivity at each position of the n gram is regarded a word attribute and one word cluster each is created to represent the positional attribute furthermore by introducing higher order word n gram through the grouping of frequent word succession multi class n gram are extended to multi class composite n gram in experiment the multi class composite n gram result in lower perplexity and a lower word error rate in speech recognition with a smaller parameter size than conventional word gram 
this paper report the first part of a project that aim to develop a knowledge extraction and knowledge discovery system that extract causal knowledge from textual database in this initial study we develop a method to identify and extract cause effect information that is explicitly expressed in medical abstract in the medline database a set of graphical pattern were constructed that indicate the presence of a causal relation in sentence and which part of the sentence represents the cause and which part represents the effect the pattern are matched with the syntactic parse tree of sentence and the part of the parse tree that match with the slot in the pattern are extracted a the cause or the effect 
abstract this paper show that linguistic techniquesalong with machine learningcan extract high quality noun phrasesfor the purpose of providing the gistor summary of email message wedescribe a set of comparative experimentsusing several machine learningalgorithms for the task of salient nounphrase extraction three main conclusionscan be drawn from this study i the modifier of a noun phrase can besemantically a important a the head for the task of gisting ii linguistic 
we present a new composite similarity metric that combine information from multiple linguistic indicator to measure semantic distance between pair of small textual unit several potential feature are investigated and an optireal combination is selected via machine learning we discus a more restrictive definition of similarity than traditional document level and information retrieval oriented notion of similarity and motivate it by showing it relevance to the multi document text summarization problem result from our system are evaluated against standard information retrieval technique establishing that the new method is more effective in identifying closely related textual unit 
the paper proposes an information theory based method for feature type analysis in probabilistic evaluation modelling for statistical parsing the basic idea is that we use entropy and conditional entropy to measure whether a feature type grasp some of the information for syntactic structure prediction our experiment quantitatively analyzes several feature type power for syntactic structure prediction and draw a series of interesting conclusion 
the paper proposes an information theory based method for feature type analysis in probabilistic evaluation modelling for statistical parsing the basic idea is that we use entropy and conditional entropy to measure whether a feature type grasp some of the information for syntactic structure prediction our experiment quantitatively analyzes several feature type power for syntactic structure prediction and draw a series of interesting conclusion 
i present a method of identifying cognate in the vocabulary of related language i show that a measure of phonetic similarity based on multivalued feature performs better than orthographic measure such a the longest common subsequence ratio lcsr or dice s coefficient i introduce a procedure for estimating semantic similarity of gloss that employ keyword selection and wordnet test performed on vocabulary of four algonquian language indicate that the method is capable of discovering on average nearly percent of cognate at precision 
this paper present a pronoun resolution algorithm that adheres to the constraint and rule of centering theory grosz et al and is an alternative to brennan et al s algorithm the advantage of this new model the left right centering algorithm lrc lie in it incremental processing of utterance and in it low computational overhead the algorithm is compared with three other pronoun resolution method hobbs syntax based algorithm strube s s list approach and the bfp centering algorithm all four method were implemented in a system and tested on an annotated subset of the treebank corpus consisting of pronoun the noteworthy result were that hobbs and lrc performed the best 
we present a method for extracting part of object from whole e g speedometer from car given a very large corpus our method find part word with ranked by the system the part list could be scanned by an end user and added to an existing ontology such a wordnet or used a a part of a rough semantic lexicon 
the paper proposes and empirically motivates an integration of supervised learning with unsupervised learning to deal with human bias in summarization in particular we explore the use of probabilistic decision tree within the clustering framework to account for the variation a well a regularity in human created summary the corpus of human created extract is created from a newspaper corpus and used a a test set we build probabilistic decision tree of different flavor and integrate each of them with the clustering framework experiment with the corpus demonstrate that the mixture of the two paradigm generally give a significant boost in performance compared to case where either of the two is considered alone 
we perform a linguistic analysis of document during indexing for information retrieval by eliminating index term that occur only in subordinate clause index size is reduced by approximately without adversely affecting precision or recall these result hold for two corpus a sample of the world wide web and an electronic encyclopedia 
in this paper a computational approach for resolving zero pronoun in spanish text is proposed our approach ha been evaluated with partial parsing of the text and the result obtained show that these pronoun can be resolved using similar technique that those used for pronominal anaphora compared to other well known baseline on pronominal anaphora resolution the result obtained with our approach have been consistently better than the rest 
we propose a distribution based pruning of n gram backoff language model instead of the conventional approach of pruning n gram that are infrequent in training data we prune n gram that are likely to be infrequent in a new document our method is based on the n gram distribution i e the probability that an n gram occurs in a new document experimental result show that our method performed word perplexity reduction better than conventional cutoff method 
in this paper we present an approach to the disambiguation of capitalized word when they are used in the position where capitalization is expected such a the first word in a sentence or after a period quote etc such word can act a proper name or can be just capitalized variant of common word the main feature of our approach is that it us a minimum of prebuilt resource and tire to dynamically infer the disambiguation clue from the entire document the approach wa thoroughly tested and achieved about accuracy on unseen text from the new york time corpus 
the paper present a new approach to identifying discourse relation which make use of a particular sampling method called committeebased sampling cbs in the committee based sampling multiple learning model are generated to measure the utility of an input example in classification if it is judged a not useful then the example will be ignored the method ha the effect of reducing the amount of data required for training in the paper we extend cbs for decision tree classifier with an additional extension called error feedback it is found that the method achieves an increased accuracy a well a a substantial reduction in the amount of data for training classifier 
mobile interface need to allow the user and system to adapt their choice of communication mode according to user preference the task at hand and the physical and social environment we describe a multimodal application architecture which combine finite state multimodal language processing a speech act based multimodal dialogue manager dynamic multimodal output generation and user tailored text planning to enable rapid prototyping of multimodal interface with flexible input and adaptive output our testbed application match multimodal access to city help provides a mobile multimodal speech pen interface to restaurant and sub way information for new york city 
we present in this paper a formalization of multiset relation in acl and we show how multisets can be used to prove non trivial termination property in acl intuitively multisets are set that admit multiple occurrence of element 
following a dependency based framework that admits no intermediate phrasal node and allows no crossing of syntactic dependency link we discus how chinese sentence are analysed and annotated using an sgml based scheme issue related to tolerance of error at various level of analysis and compatibility with other syntactic framework are addressed 
recent contribution to statistical language modeling for speech recognition have shown that probabilistically parsing a partial word sequence aid the prediction of the next word leading to structured language model that have the potential to outperform n gram existing approach to structured language modeling construct node in the partial parse tree after all of the underlying word have been predicted this paper present a different approach based on probabilistic left corner grammar plcg parsing that extends a partial parse both from the bottom up and from the top down leading to a more focused and more accurate though somewhat le robust search of the parse space at the core of our new structured language model is a fast context sensitive and lexicalized plcg parsing algorithm that us dynamic programming preliminary perplexity and word accuracy result appear to be competitive with previous one while speed is increased 
selecting the most appropriate sense for an ambiguous word in a sentence is a central problem in natural language processing in this paper we present a method that attempt to disambiguate all the noun verb adverb and adjective in a text using the sens provided in wordnet the sens are ranked using two source of information the internet for gathering statistic for word word cooccurrences and wordnet for measuring the semantic density for a pair of word we report an average accuracy of for the first ranked sense and for the first two ranked sens extension of this method for larger window of more than two word are considered 
in this paper a hybrid disambiguation method for the prepositional phrase pp attachment and interpretation problem is presented the data needed semantic pp interpretation rule and an annotated corpus is described first then the three major step of the disambiguation method are explained cross validated evaluation result for german correct for binary attachment ambiguity correct for interpretation ambiguity show that disambiguation method combining interpretation rule and statistical method might yield significantly better result than nonhybrid disambiguation method 
levin s taxonomy of verb and their class is a widely used resource for lexical semantics in her framework some verb such a give exhibit no class ambiguity but other verb such a write can inhabit more than one class in some of these ambiguous case the appropriate class for a particular token of a verb is immediately obvious from inspection of the surrounding context in others it is not and an application which want to recover this information will be forced to rely on some more or le elaborate process of inference we present a simple statistical model of verb class ambiguity and show how it can be used to carry out such inference 
this paper discus the supervised learning of morphology using stochastic transducer trained using the expectation maximization em algorithm two approach are presented first using the transducer directly to model the process and secondly using them to define a similarity measure related to the fisher kernel method jaakkola and haussler and then using a memory based learning mbl technique these are evaluated and compared on data set from english german slovene and arabic 
we propose a statistical dialogue analysis model to determine discourse structure a well a speech act using maximum entropy model the model can automatically acquire probabilistic discourse knowledge from a discourse tagged corpus to resolve ambiguity we propose the idea of tagging discourse segment boundary to represent the structural information of discourse using this representation we can effectively combine speech act analysis and discourse structure analysis in one framework 
we address the issue of on line detection of communication problem in spoken dialogue system the usefulness is investigated of the sequence of system question type and the word graph corresponding to the respective user utterance by applying both rule induction and memory based learning technique to data obtained with a dutch train time table information system the current paper demonstrates that the aforementioned feature indeed lead to a method for problem detection that performs significantly above baseline the result are interesting from a dialogue perspective since they employ feature that are present in the majority of spoken dialogue system and can be obtained with little or no computational overhead the result are interesting from a machine learning perspective since they show that the rule based method performs significantly better than the memory based method because the former is better capable of representing interaction between feature 
prepositional phrase attachment is a common source of ambiguity in natural language processing we present an unsupervised corpus based approach to prepositional phrase attachment that achieves similar performance to supervised method unlike previous unsupervised approach in which training data is obtained by heuristic extraction of unambiguous example from a corpus we use an iterative process to extract training data from an automatically parsed corpus attachment decision are made using a linear combination of feature and low frequency event are approximated using contextually similar word 
this paper discus a decision tree approach to the problem of assigning probability to word following a given text in contrast with previous decision tree language model attempt an algorithm for selecting nearly optimal question is considered the model is to be tested on a standard task the wall street journal allowing a fair comparison with the well known trigram model 
in this paper we investigate polysemous adjective whose meaning varies depending on the noun they modify e g fast we acquire the meaning of these adjective from a large corpus and propose a probabilistic model which provides a ranking on the set of possible interpretation we identify lexical semantic information automatically by exploiting the consistent correspondence between surface syntactic cue and lexical meaning we evaluate our result against paraphrase judgment elicited experimentally from human and show that the model s ranking of meaning correlate reliably with human intuition meaning that are found highly probable by the model are also rated a plausible by the subject 
in application such a translation and paraphrase operation are carried out on grammar at the meta level this paper show how a meta grammar defining structure at the meta level is useful in the case of such operation in particular how it solves problem in the current definition of synchronous tag shieber caused by ignoring such structure in mapping between grammar for application such a translation moreover essential property of the formalism remain unchanged 
this paper describes algorithm which rerank the top n hypothesis from a maximum entropy tagger the application being the recovery of named entity boundary in a corpus of web data the first approach us a boosting algorithm for ranking problem the second approach us the voted perceptron algorithm both algorithm give comparable significant improvement over the maximum entropy baseline the voted perceptron algorithm can be considerably more efficient to train at some cost in computation on test example 
this paper present a formal analysis for a large class of word called alternative marker which includes other than such a and besides these word appear frequently enough in dialog to warrant serious attention yet present natural language search engine perform poorly on query containing them i show that the performance of a search engine can be improved dramatically by incorporating an approximation of the formal analysis that is compatible with the search engine s operational semantics the value of this approach is that a the operational semantics of natural language application improve even larger improvement are possible 
we examine the benefit of using multiple agent to produce explanation in particular we identify the ability to construct prior plan a a key issue constraining the effectiveness of a single agent approach we describe an implemented system that us multiple agent to tackle a problem for which prior planning is particularly impractical real time soccer commentary our commentary system demonstrates a number of the advantage of decomposing an explanation task among several agent most notably it show how individual agent can benefit from following different discourse strategy further it illustrates that discourse issue such a controlling interruption abbreviation and maintaining consistency can also be decomposed rather than considering them at the single level of one linear explanation they can also be tackled separately within each individual agent we evaluate our system s output and show that it closely compare to the speaking pattern of a human commentary team 
in this paper we present our recent work on the development of a scalable personalized web based multi document summarization and recommendation system webinessence webinessence is designed to help end user effectively search for useful information and automatically summarize selected document based on the user personal profile we address some of the design issue to improve the scalability and readability of our multi document summarizer included in webinessence some evaluation result with different configuration are also presented 
this paper describes a system for segmenting chinese text into word using the mbdp algorithm mbdp is a knowledge free segmentation algorithm that bootstrap it own lexicon which start out empty experiment on chinese and english corpus show that mbdp reliably outperforms the best previous algorithm when the available hand segmented training corpus is small a the size of the hand segmented training corpus grows the performance of mbdp converges toward that of the best previous algorithm the fact that mbdp can be used with a small corpus is expected to be useful not only for the rare event of adapting to a new language but also for the common event of adapting to a new genre within the same language 
the rag proposal for generic specification of nlg system includes a detailed account of data representation but only an outline view of processing aspect in this paper we introduce a modular processing architecture with a concrete implementation which aim to meet the rag goal of transparency and reusability we illustrate the model with the rich system a generation system built from simple linguistically motivated module 
we present a clustering algorithm for arabic word sharing the same root root based cluster can substitute dictionary in indexing for ir modifying adamson and boreham our two stage algorithm applies light stemming before calculating word pair similarity coefficient using technique sensitive to arabic morphology test show a successful treatment of infix and accurate clustering to up to for unedited arabic text sample without the use of dictionary 
chunk parsing ha focused on the recognition of partial constituent structure at the level of individual chunk little attention ha been paid to the question of how such partial analysis can be combined into larger structure for complete utterance such larger structure are not only desirable for a deeper syntactic analysis they also constitute a necessary prerequisite for assigning function argument structure the present paper offer a similarity based algorithm for assigning functional label such a subject object head complement etc to complete syntactic structure on the basis of prechunked input the evaluation of the algorithm ha concentrated on measuring the quality of functional label it wa performed on a german and an english treebank using two different annotation scheme at the level of function argument structure the result of correct functional label for german and for english validate the general approach 
in this paper we present hidden markov model for korean part of speech tagging which consider korean characteristic such a high agglutinativity word spacing and high lexical correlativity in order ot consider rich information in context the model adopt a le strict markov assumption in the model sparse data problem is very serious and their parameter tend to be estimated unreliably because they have a large number of parameter to overcome sparse data problem our model us a simplified version of the well known back off smoothing method to mitigate unreliable estimation problem our model assume joint independence instead of conditional independence because joint probability have the same degree of estimation reliability experimental result show that model with rich context perform even better than standard hmms and that joint independent assumption is effective in some model 
we provide a logical definition of minimalist grammar that are stabler s formalization of chomsky s minimalist program our logical definition lead to a neat relation to categorial grammar yielding a treatment of montague semantics a parsing a deduction in a resource sensitive logic and a learning algorithm from structured data based on a typing algorithm and type unification here we emphasize the connection to montague semantics which can be viewed a a formal computation of the logical form 
the process through which reader evoke mental representation of phonological form from print constitute a hotly debated and controversial issue in current psycholinguistics in this paper we present a computational analysis of the grapho phonological system of written french and an empirical validation of some of the obtained descriptive statistic the result provide direct evidence demonstrating that both grapheme frequency and grapheme entropy influence performance on pseudoword naming we discus the implication of those finding for current model of phonological coding in visual word recognition 
transformation based learning ha been successfully employed to solve many natural language processing problem it achieves state of the art performance on many natural language processing task and doe not overtrain easily however it doe have a serious drawback the training time is often intorelably long especially on the large corpus which are often used in nlp in this paper we present a novel and realistic method for speeding up the training time of a transformation based learner without sacrificing performance the paper compare and contrast the training time needed and performance achieved by our modified learner with two other system a standard transformation based learner and the ica system hepple the result of these experiment show that our system is able to achieve a significant improvement in training time while still achieving the same performance a a standard transformation based learner this is a valuable contribution to system and algorithm which utilize transformation based learning at any part of the execution 
a an application of nlp to computer assisted language learning call we propose a diagnostic processing of japanese being able to detect error and inappropriateness of sentence composed by the student in the given situation and the context of the exercise text using ltag lexicalized tree adjoining grammar formalism we have implemented a prototype of such a diagnostic parser a a component of a call system being developed 
we present a set of algorithm that enable u to translate natural language sentence by exploiting both a translation memory and a statistical based translation model our result show that an automatically derived translation memory can be used within a statistical framework to often find translation of higher probability than those found using solely a statistical model the translation produced using both the translation memory and the statistical model are significantly better than translation produced by two commercial system our hybrid system translated perfectly of the sentence in a test collection while the commercial system translated perfectly only of them 
wordnet is a rich source of world knowledge from which formal axiom can be derived in this paper we present a method for transforming the wordnet gloss into logic form and further into axiom the transformation of wordnet gloss into logic form is useful for theorem proving and other application the paper demonstrates the utility of the wordnet axiom in a question answering system to rank and extract answer 
we demonstrate the benefit of a multilingual approach to automatic lexical semantic verb classification based on statistical analysis of corpus in multiple language our research incorporates two interrelated thread in one we exploit the across language in the syntactic expression of semantic property to show that complementary information about english verb can be extracted from their translation in a second language chinese the use of multilingual feature improves classification performance of the english verb achieving an accuracy of baseline 
this paper describes a simple pattern matching algorithm for recovering empty node and identifying their co indexed antecedent in phrase structure tree that do not contain this information the pattern are minimal connected tree fragment containing an empty node and all other node co indexed with it this paper also proposes an evaluation procedure for empty node recovery procedure which is independent of most of the detail of phrase structure which make it possible to compare the performance of empty node recovery on parser output with the empty node annotation in a gold standard corpus evaluating the algorithm on the output of charniak s parser charniak and the penn treebank marcus et al show that the pattern matching algorithm doe surprisingly well on the most frequently occuring type of empty node given it simplicity 
the amount of readily available on line text ha reached hundred of billion of word and continues to grow yet for most core natural language task algorithm continue to be optimized tested and compared after training on corpus consisting of only one million word or le in this paper we evaluate the performance of different learning method on a prototypical natural language disambiguation task confusion set disambiguation when trained on order of magnitude more labeled data than ha previously been used we are fortunate that for this particular application correctly labeled training data is free since this will often not be the case we examine method for effectively exploiting very large corpus when labeled data come at a cost 
we propose an algorithm to automatically induce the morphology of inflectional language using only text corpus and no human input our algorithm combine cue from orthography semantics and syntactic distribution to induce morphological relationship in german dutch and english using celex a a gold standard for evaluation we show our algorithm to be an improvement over any knowledge free algorithm yet proposed 
human understanding of spoken language appears to integrate the use of contextual expectation with acoustic level perception in a tightly coupled sequential fashion yet computer speech understanding system typically pas the transcript produced by a speech recognizer into a natural language parser with no integration of acoustic and grammatical constraint one reason for this is the complexity of implementing that integration to address this issue we have created a robust semantic parser a a single finite state machine fsm a such it run time action is le complex than other robust parser that are based on either chart or generalized left right glr architecture therefore we believe it is ultimately more amenable to direct integration with a speech decoder 
chinese input is one of the key challenge for chinese pc user this paper proposes a statistical approach to pinyin based chinese input this approach us a trigram based language model and a statistically based segmentation also to deal with real input it also includes a typing model which enables spelling correction in sentence based pinyin input and a spelling model for english which enables modeless pinyin input 
we are developing corpus based technique for identifying semantic relation at an intermediate level of description more specific than those used in case frame but more general than those used in traditional knowledge representation system in this paper we describe a classification algorithm for identifying relationship between two word noun compound we find that a very simple approach using a machine learning algorithm and a domain specific lexical hierarchy successfully generalizes from training instance performing better on previously unseen word than a baseline consisting of training on the word themselves 
we present a stochastic parsing system consisting of a lexical functional grammar lfg a constraint based parser and a stochastic disambiguation model we report on the result of applying this system to parsing the upenn wall street journal wsj treebank the model combine full and partial parsing technique to reach full grammar coverage on unseen data the treebank annotation are used to provide partially labeled data for discriminative statistical estimation using exponential model disambiguation performance is evaluated by measuring match of predicate argument relation on two distinct test set on a gold standard of manually annotated f structure for a subset of the wsj treebank this evaluation reach f score an evaluation on a gold standard of dependency relation for brown corpus data achieves f score 
an approach to automatic detection of syllable boundary is presented we demonstrate the use of several manually constructed grammar trained with a novel algorithm combining the advantage of treebank and bracketed corpus training we investigate the effect of the training corpus size on the performance of our system the evaluation show that a hand written grammar performs better on finding syllable boundary than doe a treebank grammar 
in this paper we explore the power of surface text pattern for open domain question answering system in order to obtain an optimal set of pattern we have developed a method for learning such pattern automatically a tagged corpus is built from the internet in a bootstrapping process by providing a few hand crafted example of each question type to altavista pattern are then automatically extracted from the returned document and standardized we calculate the precision of each pattern and the average precision for each question type these pattern are then applied to find answer to new question using the trec question set we report result for two case answer determined from the trec corpus and from the web 
most document are about more than one subject but many nlp and ir technique implicitly assume document have just one topic we describe new clue that mark shift to new topic novel algorithm for identifying topic boundary and the us of such boundary once identified we report topic segmentation performance on several corpus a well a improvement on an ir task that benefit from good segmentation 
we present the multilingual summarization functionality for verb mobil a speech translation system we reuse resource of the system to create a summary after content extraction we interpret the result in the dialog context a summary generator provides the input to generation a first evaluation indicates the feasibility of the approach 
we present a simple architecture for parsing transcribed speech in which an edited word detector first remove such word from the sentence string and then a standard statistical parser trained on transcribed speech par the remaining word the edit detector achieves a misclassification rate on edited word of the null model which mark everything a not edited ha an error rate of to evaluate our parsing result we introduce a new evaluation metric the purpose of which is to make evaluation of a parse tree relatively indifferent to the exact tree position of edited node by this metric the parser achieves precision and recall 
this paper present a new formalization of a unificationor join preserving encoding of partially ordered set that more essentially capture what it mean for an encoding to preserve join generalizing the standard definition in ai research it then show that every statically typable ontology in the logic of typed feature structure can be encoded in a data structure of fixed size without the need for resizing or additional union find operation this is important for any grammar implementation or development system based on typed feature structure a it significantly reduces the overhead of memory management and reference pointer chasing during unification 
we describe a speedup for training conditional maximum entropy model the algorithm is a simple variation on generalized iterative scaling but converges roughly an order of magnitude faster depending on the number of constraint and the way speed is measured rather than attempting to train all model parameter simultaneously the algorithm train them sequentially the algorithm is easy to implement typically us only slightly more memory and will lead to improvement for most maximum entropy problem 
spoken dialog manager have benefited from stochastic planner such a mdps however so far mdps do not handle well noisy and ambiguous utterance from the user we address this problem by inverting the notion of dialog state the state represents the user s intention rather than the system state this approach allows for simple and intuitive dialog description at the sacrifice of state observability we use a pomdp style approach to generate dialog policy however the intractability of pomdp solution requires an approximate solution we instead augment the state representation of the mdp by providing the system with the maximum likelihood state and a compressed representation of the belief state in this way the system can approximate the optimal pomdp solution but at mdp like speed 
sentence planning is a set of inter related but distinct task one of which is sentence scoping i e the choice of syntactic structure for elementary speech act and the decision of how to combine them into one or more sentence in this paper we present spot a sentence planner and a new methodology for automatically training spot on the basis of feedback provided by human judge we reconceptualize the task into two distinct phase first a very simple randomized sentence plan generator spg generates a potentially large list of possible sentence plan for a given text plan input second the sentence plan ranker spr rank the list of output sentence plan and then selects the top ranked plan the spr us ranking rule automatically learned from training data we show that the trained spr learns to select a sentence plan whose rating on average is only worse than the top human ranked sentence plan 
we propose a formal system for representing the available reading of sentence displaying quantifier scope ambiguity in which partial scope may be expressed we show that using a theory of scope availability based upon the function argument structure of a sentence allows a deterministic polynomial time test for the availability of a reading while solving the same problem within theory based on the well formedness of sentence in the meaning language ha been shown to be np hard 
while previous work suggests that multiple goal can be addressed by a nominal expression there is no systematic work describing what goal in addition to identification might be relevant and how speaker can use nominal expression to achieve them in this paper we first hypothesize a number of communicative goal that could be addressed by nominal expression in task oriented dialogue we then describe the intentional influence model for nominal expression generation that attempt to simultaneously address the identification goal and these additional goal with a single nominal expression our evaluation result show that the intentional influence model fit the nominal expression in the coconut corpus a well a previous account that focus solely on the identification goal 
many machine learning method have recently been applied to natural language processing task among them the winnow algorithm ha been argued to be particularly suitable for nlp problem due to it robustness to irrelevant feature however in theory winnow may not converge for non separable data to remedy this problem a modification called regularized winnow ha been proposed in this paper we apply this new method to text chunking we show that this method achieves state of the art performance with significantly le computation than previous approach 
this paper describes extensionsto commandtalk to support spoken dialogue while we make no theoretical claim about thenature and structure of dialogue we are influencedby the theoretical work of grosz andsidner and will use terminology fromthat tradition when appropriate we also follow chu carroll and brown in distinguishing task initiative and dialogue initiative 
accurate nominal compound analysis is crucial for in application of natural language processing such a information retrieval and extraction a well a nominal compound interpretation i n the nominal compound analysis area some corpus based approach have reported successful result by using statistal cooccurrences of noun but a nominal compound often ha the similar structure to a simple sentence e g the complement predicate structure a well a representing compound meaning with several noun combined due to the grammarical characteristic of nominal compound the fi amework based only on statistcal association between noun often fails to analyze their structure accurately especially in korean this pcper present a new model for korean nominal compound analysis on the basis of linguistic and statistical knowledge the syntactic relation often have an effect on determining the structure of nominal compound and we analyzed million word corpus in order to acquire syntactic and s tatistical knowledge the structure of a nominal compound is analyzed based on the linguistic lexical information extracted by experiment it is shown that our method is effective for accurate analysis of korean nominal compound 
this paper present a case study of analyzing and improving intercoder reliability in discourse tagging using statistical technique bias corrected tag are formulated and successfully used to guide a revision of the coding manual and develop an automatic classifier 
we present a statistical model of japanese unknown word consisting of a set of length and spelling model classified by the character type that constitute a word the point is quite simple different character set should be treated differently and the change between character type are very important because japanese script ha both ideogram like chinese kanji and phonogram like english katakana both word segmentation accuracy and part of speech tagging accuracy are improved by the proposed model the model can achieve tagging accuracy if unknown word are correctly segmented 
a fundamental function of any task oriented dialogue system is the ability to generate nominal expression that describe object in the task domain in this paper we report result from using machine learning to train and test a nominal expression generator on a set of nominal description from the coconut corpus of task oriented design dialogue result show that we can achieve a match to human performance a opposed to a baseline for just guessing the most frequent type of nominal expression in the coconut corpus to our surprise our result indicate that many of the central feature of previously proposed selection model did not improve the performance of the learned nominal expression generator 
this paper describes recent progress and the author s perspective of speech recognition technology application of speech recognition technology can be classified into two main area dictation and human computer dialogue system in the dictation domain the automatic broadcast news transcription is now actively investigated especially under the darpa project the broadcast news dictation technology ha recently been integrated with information extraction and retrieval technology and many application system such a automatic voice document indexing and retrieval system are under development in the human computer interaction domain a variety of experimental system for information retrieval through spoken dialogue are being investigated in spite of the remarkable recent progress we are still behind our ultimate goal of understanding free conversational speech uttered by any speaker under any environment this paper also describes the most important research issue that we should attack in order to advance to our ultimate goal of fluent speech recognition 
we present a contrastive analysis of the syntactic realization of possessive and partitive in hebrew and english and conclude by presenting an input specification for complex np which is slightly more abstract than the one used in surge we define two main feature possessor and ref set and discus how the grammar handle complex syntactic co occurrence phenomenon based on this input we conclude by evaluating how the resulting input specification language is appropriate for both language syntactic realization grammar have traditionally attempted to accept input with the highest possible level of abstraction in order to facilitate the work of the component sentence planner preparing the input recently the search for higher abstraction ha been however challenged elhadad and robin lavoie and rambow busemann and horacek in this paper we contribute to the issue of selecting the ideal abstraction level in the input to syntactic realization grammar by considering the case of partitive and possessive in a bilingual hebrew english generation grammar in the case of bilingual generation the ultimate goal is to provide a single input structure where only the openclass lexical entry are specific to the language in that case the minimal abstraction required must cover the different syntactic constraint of the two language 
in this paper a memory based parsing method is extended for handling compositional structure the method is oriented for learning to parse any selected subset of target syntactic structure it is local yet can handle also compositional structure part of speech a well a embedded instance are being used simultaneously the output is a partial parse in which instance of the target structure are marked 
previous algorithm for the generation of referring expression have been developed specifically for this purpose here we introduce an alternative approach based on a fully generic aggregation method also motivated for other generation task we argue that the alternative contributes to a more integrated and uniform approach to content determination in the context of complete noun phrase generation 
in optimality theoretic syntax optimization with unrestricted expressive power on the side of the ot constraint is undecidable this paper provides a proof for the decidability of optimization based on constraint expressed with reference to local subtrees which is in the spirit of ot theory the proof build on kaplan and wedekind s construction showing that lfg generation produce context free language 
text normalization is an important aspect of successful information retrieval from medical document such a clinical note radiology report and discharge summary in the medical domain a significant part of the general problem of text normalization is abbreviation and acronym disambiguation numerous abbreviation are used routinely throughout such text and knowing their meaning is critical to data retrieval from the document in this paper i will demonstrate a method of automatically generating training data for maximum entropy me modeling of abbreviation and acronym and will show that using me modeling is a promising technique for abbreviation and acronym normalization i report on the result of an experiment involving training a number of me model used to normalize abbreviation and acronym on a sample of rheumatology note with accuracy 
this paper present a grammatical and processing framework for handling the repair hesitation and other interruption in natural human dialog the proposed framework ha proved adequate for a collection of human human task oriented dialog both in a full manual examination of the corpus and in test with a parser capable of parsing some of that corpus this parser can also correct a pre parser speech repair identifier resulting in a increase in recall 
the billion base pair sequence of the human genome is now available and attention is focusing on annotating it to extract biological meaning i will discus what we have obtained and the method that are being used to analyse biological sequence in particular i will discus approach using stochastic grammar analogous to those used in computational linguistics both for gene finding and protein family classification 
this paper present a corpus based approach to word sense disambiguation where a decision tree assigns a sense to an ambiguous word based on the bigram that occur nearby this approach is evaluated using the sense tagged corpus from the senseval word sense disambiguation exercise it is more accurate than the average result reported for of word and is more accurate than the best result for of word 
we present condition under which verb phrase are elided based on a corpus of positive and negative example factor that affect verb phrase ellipsis include the distance between antecedent and ellipsis site the syntactic relation between antecedent and ellipsis site and the presence or absence of adjunct building on these result we examine where in the generation architecture a trainable algorithm for vp ellipsis should be located we show that the best performance is achieved when the trainable module is located after the realizer and ha access to surface oriented feature error rate of 
writing english is a big barrier for most chinese user to build a computer aided system that help chinese user not only on spelling checking and grammar checking but also on writing in the way of native english is a challenging task although machine translation is widely used for this purpose how to find an efficient way in which human collaborates with computer remains an open issue in this paper based on the comprehensive study of chinese user requirement we propose an approach to machine aided english writing system which consists of two component a statistical approach to word spelling help and an information retrieval based approach to intelligent recommendation by providing suggestive example sentence both component work together in a unified way and highly improve the productivity of english writing we also developed a pilot system namely pen perfect english system preliminary experiment show very promising result 
this paper describes a new efficient speech act type tagging system this system cover the task of segmenting a turn into the optimal number of speech act unit sa unit and assigning a speech act type tag sa tag to each sa unit our method is based on a theoretically clear statistical model that integrates linguistic acoustic and situational information we report tagging experiment on japanese and english dialogue corpus manually labeled with sa tag we then discus the performance difference between the two language we also report on some translation experiment on positive response expression using sa tag 
the dialogue strategy used by a spoken dialogue system strongly influence performance and user satisfaction an ideal system would not use a single fixed strategy but would adapt to the circumstance at hand to do so a system must be able to identify dialogue property that suggest adaptation this paper focus on identifying situation where the speech recognizer is performing poorly we adopt a machine learning approach to learn rule from a dialogue corpus for identifying these situation our result show a significant improvement over the baseline and illustrate that both lower level acoustic feature and higher level dialogue feature can affect the performance of the learning algorithm 
we discus the advantage of lexicalized tree adjoining grammar a an alternative to lexicalized pcfg for statistical parsing describing the induction of a probabilistic ltag model from the penn treebank and evaluating it parsing performance we find that this induction method is an improvement over the em based method of hwa and that the induced model yield result comparable to lexicalized pcfg 
this is a paper that describes computational linguistic activity on philippine language the philippine is an archipelago with vast number of island and numerous language the task of understanding representing and implementing these language require enormous work an extensive amount of work ha been done on understanding at least some of the major philippine language but little ha been done on the computational aspect majority of the latter ha been on the purpose of machine translation 
we present two method for learning the structure of personal name from unlabeled data the first simply us a few implicit constraint governing this structure to gain a toehold on the problem e g descriptor come before first name which come before middle name etc the second model also us possible coreference information we found that coreference constraint on name improve the performance of the model from to we are interested in this problem in it own right but also a a possible way to improve named entity recognition by recognizing the structure of different kind of name and a a way to improve noun phrase coreference determination 
pitch accent placement is a major topic in intonational phonology research and it application to speech synthesis what factor influence whether or not a word is made intonationally prominent or not is an open question in this paper we investigate how one aspect of a word s local context it collocation with neighboring word influence whether it is accented or not result of experiment on two transcribed speech corpus in a medical domain show that such collocation information is a useful predictor of pitch accent placement 
the distinction between achievement and accomplishment is known to be an empirically important but subtle one it is argued here to depend on the atomicity rather than punctuality of event and to be strongly related to incrementality i e to event object mapping function a computational treatment of incrementality and atomicity is discussed in the paper and a number or related empirical problem considered notably lexical polysemy in verb argument relationship 
boosting is a machine learning algorithm that is not well known in computational linguistics we apply it to part of speech tagging and prepositional phrase attachment performance is very encouraging we also show how to improve data quality by using boosting to identify annotation error 
in this paper we show how machine learningtechniques for constructing and combining severalclassifiers can be applied to improve theaccuracy of an existing english po tagger m arquez and rodr iguez additionally the problem of data sparseness is also addressedby applying a technique of generating convexpseudo data breiman experimental resultsand a comparison to other state of the art tagger are reported keywords po tagging corpus based modeling decision 
language model for speech recognition concentrate solely on recognizing the word that were spoken in this paper we advocate redefining the speech recognition problem so that it goal is to find both the best sequence of word and their po tag and thus incorporate po tagging to use po tag effectively we use clustering and decision tree algorithm which allow generalization between po tag and word to be effectively used in estimating the probability distribution we show that our po model give a reduction in word error rate and perplexity for the train corpus in comparison to word and class based approach by using the wall street journal corpus we show that this approach scale up when more training data is available 
this paper present the architecture operation and result obtained with the lasso question answering system developed in the natural language processing laboratory at smu to find answer the system relies on a combination of syntactic and semantic technique the search for the answer is based on a novel form of indexing called paragraph indexing a score of for short answer and for long answer wa achieved at the trec competition 
this paper describes initial work on deep read an automated reading comprehension system that accepts arbitrary text input a story and answer question about it we have acquired a corpus of development and test story of rd to th grade material each story is followed by short answer question an answer key wa also provided we used these to construct and evaluate a baseline system that us pattern matching bag of word technique augmented with additional automated linguistic processing stemming name identification semantic class identification and pronoun resolution this simple system retrieves the sentence containing the answer of the time 
a number of previous experiment on the role of lexical ambiguity in information retrieval are reproduced on the ir semcor test collection derived from semcor where both query and document are hand tagged with phrase part of speech and wordnet sens our result indicate that a word sense disambiguation can be more beneficial to information retrieval than the experiment of sanderson with artificially ambiguous pseudo word suggested b partof speech tagging doe not seem to help improving retrieval even if it is manually annotated c using phrase a indexing term is not a good strategy if no partial credit is given to the phrase component 
an approach to automatic detectionof syllable structure is presented wedemonstrate a novel application ofem based clustering to multivariatedata exemplied by the inductionof and dimensional probabilisticsyllable class the qualitativeevaluation show that the methodyields phonologically meaningful syllableclasses we then propose anovel approach to grapheme to phonemeconversion and show that syllablestructure represents valuableinformation for pronunciation 
in this paper we revisit pustejovsky s proposal to treat ontologically complex word meaning by so called dotted pair we use a higher order feature logic based on ohori s record calculus to model the semantics of word like book and library in particular their behavior in the context of quantification and cardinality statement 
we show that it is possible to learn the context for linguistic operation which map a semantic representation to a surface syntactic tree in sentence realization with high accuracy we cast the problem of learning the context for the linguistic operation a classification task and apply straightforward machine learning technique such a decision tree learning the training data consist of linguistic feature extracted from syntactic and semantic representation produced by a linguistic analysis system the target feature are extracted from link to surface syntax tree our evidence consists of four example from the german sentence realization system code named case assignment assignment of verb position feature extraposition and syntactic aggregation 
we present a machine learning approach to evaluating the well formedness of output of a machine translation system using classifier that learn to distinguish human reference translation from machine translation this approach can be used to evaluate an mt system tracking improvement over time to aid in the kind of failure analysis that can help guide system development and to select among alternative output string the method presented is fully automated and independent of source language target language and domain 
this paper present method for a qualitative unbiased comparison of lexical association measure and the result we have obtained for adjective noun pair and preposition noun verb triple extracted from german corpus in our approach we compare the entire list of candidate sorted according to the particular measure to a reference set of manually identified true positive we also show how estimate for the very large number of hapaxlegomena and double occurrence can be inferred from random sample 
in this paper we argue that comparative evaluation in anaphora resolution ha to be performed using the same pre processing tool and on the same set of data the paper proposes an evaluation environment for comparing anaphora resolution algorithm which is illustrated by presenting the result of the comparative evaluation of three method on the basis of several evaluation measure 
in this paper we describe improved alignmentmodels for statistical machine translation thestatistical translation approach us two typesof information a translation model and a lan guage model the language model used is abigram or general m gram model the translationmodel is decomposed into a lexical and analignment model we describe two different approachesfor statistical translation and presentexperimental result the first approach isbased on dependency between 
this paper argues that developmental pattern in child language be taken seriously in computational model of language acquisition and proposes a formal theory that meet this criterion we first present developmental fact that are problematic for statistical learning approach which assume no prior knowledge of grammar and for traditional learnability model which assume the learner move from one ug defined grammar to another in contrast we view language acquisition a a population of grammar associated with weight that compete in a darwinian selectionist process selection is made possible by the variational property of individual grammar specifically their differential compatibility with the primary linguistic data in the environment in addition to a convergence proof we present empirical evidence in child language development that a learner is best modeled a multiple grammar in co existence and competition 
technique for automatically training module of a natural language generator have recently been proposed but a fundamental concern is whether the quality of utterance produced with trainable component can compete with hand crafted template based or rule based approach in this paper we experimentally evaluate a trainable sentence planner for a spoken dialogue system by eliciting subjective human judgment in order to perform an exhaustive comparison we also evaluate a hand crafted template based generation component two rule based sentence planner and two baseline sentence planner we show that the trainable sentence planner performs better than the rule based system and the baseline and a well a the hand crafted system 
this paper describes a method of alignment between the noun part of wordnet and goi taikei s ontology the result of the automatic alignment show that matching word in japanese a well a in english is highly efficient in extracting correct match analysis of correct alignment by hand illustrate that in ontology alignment it is not always possible to preserve relative relation between concept such a hyponymy or antonymy 
we present a statistical question answering system developed for trec in detail the system is an application of maximum entropy classification for question answer type prediction and named entity marking we describe our system for information retrieval which did document retrieval from a local encyclopedia and then expanded the query word and finally did passage retrieval from the trec collection we will also discus the answer selection algorithm which determines the best sentence given both the question and the occurrence of a phrase belonging to the answer class desired by the question a new method of analyzing system performance via a transition matrix is shown 
this paper considers three assumption conventionally made about signature in typed feature logic that are in potential disagreement with current practice among grammar developer and linguist working within feature based framework such a hpsg meet semi latticehood unique feature introduction and the absence of subtype covering it also discus the condition under which each of these can be tractably restored in realistic grammar signature where they do not already exist 
this paper present a novel method of generating and applying hierarchical dynamic topic based language model it proposes and evaluates new cluster generation hierarchical smoothing and adaptive topic probability estimation technique these combined model help capture long distance lexical dependency experiment on the broadcast news corpus show significant improvement in perplexity overall and on target vocabulary 
abstract collocational word similarity is considered a source of text cohesion that is hard to measure and quantify the work presented here explores the use of information from a training corpus in measuring word similarity and evaluates the method in the text segmentation task an implementation the vectile system produce similarity curve over text using pre compiled vector representation of the contextual behavior of word the performance of this system is shown to improve over that of the purely string based texttiling algorithm hearst background the notion of text cohesion rest on the intuition 
a good decoding algorithm is critical to the success of any statistical machine translation system the decoder s job is to find the translation that is most likely according to set of previously learned parameter and a formula for combining them since the space of possible translation is extremely large typical decoding algorithm are only able to examine a portion of it thus risking to miss good solution in this paper we compare the speed and output quality of a traditional stack based decoding algorithm with two new decoder a fast greedy decoder and a slow but optimal decoder that treat decoding a an integer programming optimization problem 
in many type of technical text meaning is embedded in noun compound a language understanding program need to be able to interpret these in order to ascertain sentence meaning we explore the possibility of using an existing lexical hierarchy for the purpose of placing word from a noun compound into category and then using this category membership to determine the relation that hold between the noun in this paper we present the result of an analysis of this method on two word noun compound from the biomedical domain obtaining classification accuracy of approximately since lexical hierarchy are not necessarily ideally suited for this task we also pose the question how far down the hierarchy must the algorithm descend before all the term within the subhierarchy behave uniformly with respect to the semantic relation in question we find that the topmost level of the hierarchy yield an accurate classification thus providing an economic way of assigning relation to noun compound 
we present a syntax based statistical translation model our model transforms a source language parse tree into a target language string by applying stochastic operation at each node these operation capture linguistic difference such a word order and case marking model parameter are estimated in polynomial time using an em algorithm the model produce word alignment that are better than those produced by ibm model 
a hybrid system is described which combine the strength of manual rule writing and statistical learning obtaining result superior to both method if applied separately the combination of a rule based system and a statistical one is not parallel but serial the rule based system performing partial disambiguation with recall close to is applied first and a trigram hmm tagger run on it result an experiment in czech tagging ha been performed with encouraging result 
we explore how active learning with support vector machine work well for a non trivial task in natural language processing we use japanese word segmentation a a test case in particular we discus how the size of a pool affect the learning curve it is found that in the early stage of training with a larger pool more labeled example are required to achieve a given level of accuracy than those with a smaller pool in addition we propose a novel technique to use a large number of unlabeled example effectively by adding them gradually to a pool the experimental result show that our technique requires le labeled example than those with the technique in previous research to achieve accuracy the proposed technique need of labeled example that are required when using the previous technique and only of labeled example with random sampling 
this paper present a method for inducing translation lexicon based on transduction model of cognate pair via bridge language bilingual lexicon within language family are induced using probabilistic string edit distance model translation lexicon for arbitrary distant language pair are then generated by a combination of these intra family translation model and one or more cross family on line dictionary up to exact match accuracy is achieved on the target vocabulary of inter family test pair thus substantial portion of translation lexicon can be generated accurately for language where no bilingual dictionary or parallel corpus may exist 
coreference resolution involves finding antecedent for anaphoric discourse entity such a definite noun phrase but many definite noun phrase are not anaphoric because their meaning can be understood from general world knowledge e g the white house or the news medium we have developed a corpus based algorithm for automatically identifying definite noun phrase that are non anaphoric which ha the potential to improve the efficiency and accuracy of coreference resolution system our algorithm generates list of non anaphoric noun phrase and noun phrase pattern from a training corpus and us them to recognize non anaphoric noun phrase in new text using muc terrorism news article a the training corpus our approach achieved recall and precision at identifying such noun phrase in text document 
we present a method to automatically generate a concise summary by identifying and synthesizing similar element across related text from a set of multiple document our approach is unique in it usage of language generation to reformulate the wording of the summary 
polarized dependency pd grammar are proposed a a mean of efficient treatment of discontinuous construction pd grammar describe two kind of dependency local explicitly derived by the rule and long implicitly specified by negative and positive valency of word if in a pd grammar the number of non saturated valency in derived structure is bounded by a constant then it is weakly equivalent to a cf grammar and ha a o n time parsing algorithm it happens that such bounded pd grammar are strong enough to express such phenomenon a unbounded raising extraction and extraposition 
this paper address the rule based po tagging method of brill and question the importance of rule interaction to it performance adopting two assumption that serve to exclude rule interaction during tagging and training we arrive at some variant of brill s approach that are instance of decision list model these model allow for both rapid training on large data set and rapid tagger execution giving tagging accuracy that is comparable to or better than the brill method 
this paper describes a supervised learning method to automatically select from a set of noun phrase embedding proper name of different semantic class their most distinctive feature the result of the learning process is a decision tree which classifies an unknown proper name on the basis of it context of occurrence this classifier is used to estimate the probability distribution of an out of vocabulary proper name over a tagset this probability distribution is itself used to estimate the parameter of a stochastic part of speech tagger 
this paper present a statistical method for fingerprinting text in a large collection of independently written document each text is associated with a fingerprint which should be different from all the others if fingerprint are too close then it is suspected that passage of copied or similar text occur in two document our method exploit the characteristic distribution of word trigram and measure to determine similarity are based on set theoretic principle the system wa developed using a corpus of broadcast news report and ha been successfully used to detect plagiarism in student work it can find small section that are similar a well a those that are identical the method is very simple and effective but seems not to have been used before 
we present a generalization of an incremental statistical parsing algorithm that allows for the re scoring of lattice of word hypothesis for use by a speech recognizer this approach contrast with other lattice parsing algorithm which either do not provide score for string in the lattice i e they just produce parse tree or use search technique e g a star to find the best path through the lattice without re scoring every arc we show that a very large efficiency gain can be had in processing best list without reducing word accuracy when the list are encoded in lattice instead of tree further this allows for processing arbitrary lattice without n best extraction this can lead to more interesting method of combination with other model both acoustic and language through for example adaptation or confusion matrix 
this paper describes a language independent method for alignment of parallel text that make use of homograph token for each pair of language in order to filter out token that may cause misalignment we use confidence band of linear regression line instead of heuristic which are not theoretically supported this method wa originally inspired on work done by pascale fung and kathleen mckeown and melamed providing the statistical support those author could not claim 
we propose a statistical method that find the maximum probability segmentation of a given text this method doe not require training data because it estimate probability from the given text therefore it can be applied to any text in any domain an experiment showed that the method is more accurate than or at least a accurate a a state of the art text segmentation system 
i present empirical comparison between a linear combination of standard statistical language and translation model and an equivalent maximum entropy minimum divergence memd model using several different method for automatic feature selection the memd model significantly outperforms the standard model in test corpus perplexity even though it ha far fewer parameter 
this paper present empirical study and closely corresponding theoretical model of the performance of a chart parser exhaustively parsing the penn treebank with the treebank s own cfg grammar we show how performance is dramatically affected by rule representation and tree transformation but little by top down v bottom up strategy we discus grammatical saturation including analysis of the strongly connected component of the phrasal nonterminals in the treebank and model how a sentence length increase the effective grammar rule size increase a region of the grammar are unlocked yielding super cubic observed time behavior in some configuration 
this paper address the issue of designing embodied conversational agent that exhibit appropriate posture shift during dialogue with human user previous research ha noted the importance of hand gesture eye gaze and head nod in conversation between embodied agent and human we present an analysis of human monologue and dialogue that suggests that postural shift can be predicted a a function of discourse state in monologue and discourse and conversation state in dialogue on the basis of these finding we have implemented an embodied conversational agent that us collagen in such a way a to generate postural shift 
this paper present a new method of analyzing japanese noun phrase of the form n no n the japanese postposition no roughly corresponds to of but it ha much broader usage the method exploit a definition of n in a dictionary for example rugby no coach can be interpreted a a person who teach technique in rugby we illustrate the effectiveness of the method by the analysis of test noun phrase 
this paper describes automatic technique for mapping semantically classified english verb to wordnetsenses the verb were initially grouped into semantic class based on syntactic category they werethen mapped into wordnet sens according to three piece of information prior probability of wordnetsenses semantic similarity of wordnet sens for verb within the same category and probabilisticcorrelations between wordnet relationship and verb frame data 
this paper describes how to automaticallyextract grounding feature and segment adialogue into discourse unit once thedialogue ha been annotated with the dribackwardand forward looking tag suchan approach eliminates the need forseparate annotation of grounding makingdialogue annotation quicker and removinga possible source of error a preliminarytest of the mapping against a humanannotator is presented introductionthe annotation scheme ac developed by thediscourse 
a syntax tree or standard semantic representation can be represented a a set of indexed constraint this paper describes how this idea can be used in task oriented dialogue system to provide interpretation rule which incorporate structural and contextual constraint where available and degrade gracefully on ungrammatical input 
in intonational phonology and speech synthesis research it ha been suggested that the relative informativeness of a word can be used to predict pitch prominence the more information conveyed by a word the more likely it will be accented but there are others who express doubt about such a correlation in this paper we provide some empirical evidence to support the existence of such a correlation by employing two widely accepted measure of informativeness our experiment show that there is a positive correlation between the informativeness of a word and it pitch accent assignment they also show that informativeness enables statistically significant improvement in pitch accent prediction the computation of word informativeness is inexpensive and can be incorporated into speech synthesis system easily 
objective the objective of this study is to compare how a general terminological system wordnet and a domain specific one umls represent linguistic and knowledge phenomenon at three different level term concept and semantic class method for one general class animal and one domain specific class health disorder the set of concept corresponding to the class wa established then for each semantic class the corresponding term were mapped from one system to the other both way result only of the domain specific concept from umls were found in wordnet but of the domain specific concept from wordnet were found in the umls concept overlap between the two system varies from to discussion missing term in both system are discussed a well a granularity and knowledge organization issue 
we developed a novel language model for japanese based on grapheme phoneme tuples which is one order of magnitude smaller than word based model we also developed an alignment algorithm of grapheme and phoneme for both ordinary text and ocr output we show by experiment that the combination of the grapheme phoneme tuple ngram model and the grapheme phoneme alignment algorithm significantly improve character recognition accuracy if both grapheme and phoneme representation are given 
the earley deduction algorithm is extended for the processing of ot syntax based on feature grammar due to faithfulness violation infinitely many candidate must be compared with the reasonable assumption i that ot constraint are description denoting bounded structure and ii that every rule recursion in the base grammar incurs some constraint violation a chart algorithm can be devised interleaving parsing and generation permit the application of generation based optimization even in the parsing task i e for a string input 
this paper proposes a question biased text summarization qbts approach that is useful for question answering system qbts is an extension of query biased text summarization in the sense that summarization is biased not only by the question which corresponds to the query but also by the prospective answer to the question we conducted text summarization experiment based on qa task and confirmed the effectiveness of our method in obtaining short summary 
most corpus based approach to natural language processing suffer from lack of training data this is because acquiring a large number of labeled data is expensive this paper describes a learning method that exploit unlabeled data to tackle data sparseness problem the method us committee learning to predict the label of unlabeled data that augment the existing training data our experiment on word sense disambiguation show that predictive accuracy is significantly improved by using additional unlabeled data 
distributional similarity is a useful notion in estimating the probability of rare joint event it ha been employed both to cluster event according to their distribution and to directly compute average of estimate for distributional neighbor of a target event here we examine the tradeoff between model size and prediction accuracy for cluster based and nearest neighbor distributional model of unseen event 
we present two language model based upon an immediate head parser our name for a parser that condition all event below a constituent c upon the head of c while all of the most accurate statistical parser are of the immediate head variety no previous grammatical language model us this technology the perplexity for both of these model significantly improve upon the trigram model base line a well a the best previous grammar based language model for the better of our two model these improvement are and respectively we also suggest that improvement of the underlying parser should significantly improve the model s perplexity and that even in the near term there is a lot of potential for improvement in immediate head language model 
we show that discourse structure need not bear the full burden of conveying discourse relation by showing that many of them can be explained nonstructurally in term of the grounding of anaphoric presupposition van der sandt this simplifies discourse structure while still allowing the realisation of a full range of discourse relation this is achieved using the same semantic machinery used in deriving clause level semantics 
we present an incremental refinement proof in acl which demonstrates the reduction ofthe observable behavior of a concurrent program to those of a much simpler program inparticular we document the proof of correctness of a concurrent program which implementsthe operation of a double ended queue in the application of a work stealing algorithm thedemonstration is carried out by proving a refinement from the implementation to a specificationvia an intermediate model we document 
we present a hybrid text mining method for finding abbreviation and their definition in free format text to deal with the problem this method employ pattern based abbreviation rule in addition to text marker and cue word the pattern based rule describe how abbreviation are formed from definition rule can be generated automatically and or manually and can be augmented when the system process new document the proposed method ha the advantage of high accuracy high flexibility wide coverage and fast recognition 
