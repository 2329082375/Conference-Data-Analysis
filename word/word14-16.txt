the phealth conference is the th in a series of scientific event bringing together expertise from medical technological political administrative and social domain and even from philosophy or linguistics it open a new chapter in the success story of the series of international conference on wearable or implantable micro and nano technology for personalized medicine by presenting keynote invited talk oral presentation and short poster presentation provided by close to author from country from various part of the world starting in with personal health management system phealth conference have evolved to truly interdisciplinary and global event by covering technological and biomedical facility legal ethical social and organizational requirement and impact a well a necessary basic research for enabling future proof care paradigm thereby it combine medical service with public health prevention social and elderly care wellness and personal fitness to establish participatory predictive personalized preventive and effective care setting by this way it ha attracted scientist developer and practitioner from various technology medical and health discipline legal affair politics and administration from all over the world the conference brought together health service vendor and provider institution payer organization governmental department academic institution professional body but also patient and citizen representative smart mobile system such a microsystems smart textile smart implant sensor controlled medical device and innovative sensor and actuator principle and technique a well a related body local and wide area network up to cloud service have become important enablers for telemedicine and ubiquitous pervasive health a the next generation health service social medium and gamification have added even further knowledge to phealth a an eco system oecd ha defined four basic area to be managed in the new care model address the big data challenge foster meaningful innovation understand and address the potential new risk and support concerted effort to un silo community for a virtual care future the multilateral benefit of phealth technology for all stakeholder community including patient citizen health professional politician healthcare establishment and company from the biomedical technology pharmaceutical and telecommunication domain give enormous potential not only for medical quality improvement and industrial competitiveness but also for managing health care cost the phealth conference thankfully benefit from the experience and the lesson learned from the organizing committee of previous phealth event particularly in oslo in berlin in lyon in porto in tallinn and vienna the conference brought up the interesting idea of having special session focusing on a particular topic and being organized by a mentor moderator the berlin event in initiated workshop on particular topic prior to the official kick off of the conference lyon in initiated the launch of socalled dynamic demonstration allowing the participant to dynamically show software and hardware solution on the fly without needing a booth implementing preconference event the phealth in porto gave attendee a platform for presenting and discussing recent development and provocative idea that helped to animate the session highlight of phealth in tallinn wa the special session on european project success story but also presentation on the newest paradigm change and challenge coming up with big data analytics translational and nano medicine etc vienna in focused on lesson learned from international and national r amp d activity and practical solution and especially from the new eu framework program for research and innovation horizon beside report about technology transfer support and building ecosystem and value chain to ensure better time to market and higher impact of knowledge based technology the acceptability of solution especially considering security and privacy aspect have been presented and deeply discussed the phealth conference address mobile technology knowledge driven application and computer assisted decision support but also apps designed to support elderly a well a chronic patient in their daily and possibly independent living invited contribution consider fundamental scientific and methodological challenge of adaptive autonomous and intelligent phealth approach the new role of patient a consumer and active party with growing autonomy and related responsibility but also requirement and solution for mhealth in lowand medium income country it aim at sharing experience and result and opening up for the future the phealth presentation are complemented by demonstration of practical artifact and solution a well a by a student poster competition more insight in the phealth challenge are provided by a satellite conference of novamedtech embedded sensor system for health novamedtech and hl sweden but following a long term tradition also the working group electronic health record ehr personal portable device ppd and security safety and ethic sse of the european federation for medical informatics efmi have been actively involved in the preparation and realization of the phealth conference this proceeding volume cover keynote and specially invited talk but also oral presentation selected from more than submission to the phealth conference and poster presentation all submission have been carefully and critically reviewed by at least two independent expert from other than the author home country and additionally by at least one member of the scientific program committee the performed highly selective review process resulted in a full paper rejection rate of more than thereby guaranteeing a high scientific level of the accepted and finally published paper the editor are indebted to the acknowledged and highly experienced reviewer for having essentially contributed to the quality of the conference and the book at hand both the phealth conference and the publication of the phealth proceeding at io press would not have been possible without the supporter and sponsor health level international hl international embedded sensor system for health novamedtech and european federation for medical informatics efmi the editor are also grateful to the dedicated effort of the local organizing committee member and their supporter for carefully and smoothly preparing and operating the conference they especially thank all team member from the school of innovation design and engineering m lardalen university v ster s sweden for their dedication to the event bernd blobel maria lind n mobyen uddin ahmed editor 
this paper address the problem of learning control policy in very high dimensional state space we propose a linear dimensionality reduction algorithm that discovers predictive projection projection in which accurate prediction of future state can be made using simple nearest neighbor style learning the goal of this work is to extend the reach of existing reinforcement learning algorithm to domain where they would otherwise be inapplicable without extensive engineering of feature the approach is demonstrated on a synthetic pendulum balancing domain a well a on a robot domain requiring visually guided control 
topic model are a useful and ubiquitous tool for understanding large corpus however topic model are not perfect and for many user in computational social science digital humanity and information study who are not machine learning expert existing model and framework are often a take it or leave it proposition this paper present a mechanism for giving user a voice by encoding user feedback to topic model a correlation between word into a topic model this framework interactive topic modeling itm allows untrained user to encode their feedback easily and iteratively into the topic model because latency in interactive system is crucial we develop more efficient inference algorithm for tree based topic model we validate the framework both with simulated and real user 
we study the problem of learning personalized user model from rich user interaction in particular we focus on learning from clustering feedback i e grouping recommended item into cluster which enables user to express similarity or redundancy between different item we propose and study a new machine learning problem for personalization which we call collaborative clustering analogous to collaborative filtering in collaborative clustering the goal is to leverage how existing user cluster or group item in order to predict similarity model for other user clustering task we propose a simple yet effective latent factor model to learn the variability of similarity function across a user population we empirically evaluate our approach using data collected from a clustering interface we developed for a goal oriented data exploration or sensemaking task asking user to explore and organize attraction in paris we evaluate using several realistic use case and show that our approach learns more effective user model than conventional clustering and metric learning approach 
personalized recommendation system are used in a wide variety of application such a electronic commerce social network web search and more collaborative filtering approach to recommendation system typically assume that the rating matrix e g movie rating by viewer is low rank in this paper we examine an alternative approach in which the rating matrix is locally low rank concretely we assume that the rating matrix is low rank within certain neighborhood of the metric space defined by user item pair we combine a recent approach for local low rank approximation based on the frobenius norm with a general empirical risk minimization for ranking loss our experiment indicate that the combination of a mixture of local low rank matrix each of which wa trained to minimize a ranking loss outperforms many of the currently used state of the art recommendation system moreover our method is easy to parallelize making it a viable approach for large scale real world rank based recommendation system 
the world wide web www ha become an indispensable part of the modern life providing many benefit in diverse way for instance the huge amount of information from the web offer people unprecedented level of opportunity for education entertainment social activity productivity improvement and business the web however ha also become perilous with many danger such a privacy violation and security breach and therein exist many villain who would like to turn into victim scrupulous a well a casual user in this regard www ha become almost like wild wild west where wonderful opportunity and great peril co existed tizen www tizen org is a web centric open source standard based software platform for smart device such a smartphones smart tv ivi in vehicle infotainment and other consumer device like camera printer and more tizen is web centric in that it directly support web apps application apps written in html and javascript even outside the web browser and provides seamless support for the web a such tizen not only share the benefit and peril of the web with other platform but also ha the additional burden to meet the performance of non web platform platform that directly support only conventional programming language in this talk we present tizen s approach to taming the web to maximize it benefit while minimizing the risk of it peril we also describe various optimization of tizen that enable delivering web app performance on par with that of non web platform 
a surrogate function of norm many nonconvex penalty function have been proposed to enhance the sparse vector recovery it is easy to extend these nonconvex penalty function on singular value of a matrix to enhance low rank matrix recovery however different from convex optimization solving the nonconvex low rank minimization problem is much more challenging than the nonconvex sparse minimization problem we observe that all the existing nonconvex penalty function are concave and monotonically increasing on thus their gradient are decreasing function based on this property we propose an iteratively reweighted nuclear norm irnn algorithm to solve the nonconvex nonsmooth low rank minimization problem irnn iteratively solves a weighted singular value thresholding wsvt problem by setting the weight vector a the gradient of the concave penalty function the wsvt problem ha a closed form solution in theory we prove that irnn decrease the objective function value monotonically and any limit point is a stationary point extensive experiment on both synthetic data and real image demonstrate that irnn enhances the low rank matrix recovery compared with state of the art convex algorithm 
when a website hosting user generated content asks user a straightforward question wa this content helpful with one yes and one no button a the two possible answer one might expect to get a straightforward answer in this paper we explore how user respond to this question and find that their response are not quite straightforward after all using data from amazon product review we present evidence that user do not make absolute independent voting decision based on individual review quality alone rather whether user vote at all a well a the polarity of their vote for any given review depends on the context in which they view it review receive a larger overall number of vote when they are misranked and the polarity of vote becomes more positive negative when the review is ranked lower higher than it deserves we distill these empirical finding into a new probabilistic model of rating behavior that includes the dependence of rating decision on context understanding and formally modeling voting behavior is crucial for designing learning mechanism and algorithm for review ranking and we conjecture that many of our finding also apply to user behavior in other online content rating setting 
we revisit a pioneer unsupervised learning technique called archetypal analysis which is related to successful data analysis method such a sparse coding and non negative matrix factorization since it wa proposed archetypal analysis did not gain a lot of popularity even though it produce more interpretable model than other alternative because no efficient implementation ha ever been made publicly available it application to important scientific problem may have been severely limited our goal is to bring back into favour archetypal analysis we propose a fast optimization scheme using an active set strategy and provide an efficient open source implementation interfaced with matlab r and python then we demonstrate the usefulness of archetypal analysis for computer vision task such a codebook learning signal classification and large image collection visualization 
using random graph to model network ha a rich history in this paper we analyze and improve the multifractal network generator mfng introduced by palla et al we provide a new result on the probability of subgraphs existing in graph generated with mfng this allows u to quickly compute moment of an important set of graph property such a the expected number of edge star and clique for graph generated using mfng specifically we show how to compute these moment in time complexity independent of the size of the graph and the number of recursive level in the generative model we leverage this theory to propose a new method of moment algorithm for fitting mfng to large network empirically this new approach effectively simulates property of several social and information network in term of matching subgraph count our method outperforms similar algorithm used with the stochastic kronecker graph model furthermore we present a fast approximation algorithm to generate graph instance following the multifractal structure the approximation scheme is an improvement over previous method which ran in time complexity quadratic in the number of vertex combined our method of moment and fast sampling scheme provide the first scalable framework for effectively modeling large network with mfng 
on many social networking web site such a facebook and twitter resharing or reposting functionality allows user to share others content with their own friend or follower a content is reshared from user to user large cascade of reshares can form while a growing body of research ha focused on analyzing and characterizing such cascade a recent parallel line of work ha argued that the future trajectory of a cascade may be inherently unpredictable in this work we develop a framework for addressing cascade prediction problem on a large sample of photo reshare cascade on facebook we find strong performance in predicting whether a cascade will continue to grow in the future we find that the relative growth of a cascade becomes more predictable a we observe more of it reshares that temporal and structural feature are key predictor of cascade size and that initially breadth rather than depth in a cascade is a better indicator of larger cascade this prediction performance is robust in the sense that multiple distinct class of feature all achieve similar performance we also discover that temporal feature are predictive of a cascade s eventual shape observing independent cascade of the same content we find that while these cascade differ greatly in size we are still able to predict which end up the largest 
given a large dataset of user rating of movie what is the best model to accurately predict which movie a person will like and how can we prevent spammer from tricking our algorithm into suggesting a bad movie is it possible to infer structure between movie simultaneously in this paper we describe a unified bayesian approach to collaborative filtering that accomplishes all of these goal it model the discrete structure of rating and is flexible to the often non gaussian shape of the distribution additionally our method find a co clustering of the user and item which improves the model s accuracy and make the model robust to fraud we offer three main contribution we provide a novel model and gibbs sampling algorithm that accurately model the quirk of real world rating such a convex rating distribution we provide proof of our model s robustness to spam and anomalous behavior we use several real world datasets to demonstrate the model s effectiveness in accurately predicting user s rating avoiding prediction skew in the face of injected spam and finding interesting pattern in real world rating data 
we consider the problem of resolving duplicate in a database of place where a place is defined a any entity that ha a name and a physical location when other auxiliary attribute like phone and full address are not available deduplication based solely on name and approximate location becomes an exceptionally challenging problem that requires both domain knowledge a well an local geographical knowledge for example the pair newpark mall gap outlet and newpark mall sears outlet have a high string similarity but determining that they are different requires the domain knowledge that they represent two different store name in the same mall similarly in most part of the world a local business called central park cafe might simply be referred to by central park except in new york where the keyword cafe in the name becomes important to differentiate it from the famous park in the city in this paper we present a language model that can encapsulate both domain knowledge a well a local geographical knowledge we also present unsupervised technique that can learn such a model from a database of place finally we present deduplication technique based on such a model and we demonstrate using real datasets that our technique are much more effective than simple tf idf based model in resolving duplicate our technique are used in production at facebook for deduplicating the place database 
one of the most important innovation of social networking website is the notion of a feed a sequence of news item presented to the user a a stream that expands a the user scroll down the common method for monetizing such stream is to insert ad in between news item in this paper we model this setting and observe that allocation and pricing of ad insertion in a stream pose interesting algorithmic and mechanism design challenge in particular we formulate an optimization problem that capture a typical stream ad placement setting we give an approximation algorithm for this problem that provably achieves a value close to the optimal and show how this algorithm can be turned into an incentive compatible mechanism finally we conclude with a simple practical algorithm that make the allocation decision in an online fashion we prove this algorithm to be approximately welfare maximizing and show that it also ha good incentive property 
support vector machine svm ha been one of the most popular learning algorithm with the central idea of maximizing the minimum margin i e the smallest distance from the instance to the classification boundary recent theoretical result however disclosed that maximizing the minimum margin doe not necessarily lead to better generalization performance and instead the margin distribution ha been proven to be more crucial in this paper we propose the large margin distribution machine ldm which try to achieve a better generalization performance by optimizing the margin distribution we characterize the margin distribution by the firstand second order statistic i e the margin mean and variance the ldm is a general learning approach which can be used in any place where svm can be applied and it superiority is verified both theoretically and empirically in this paper 
given a large graph like who call whom or who like whom what behavior is normal and what should be surprising possibly due to fraudulent activity how do graph evolve over time how doe influence news virus propagate over time we focus on three topic a anomaly detection in large static graph b pattern and anomaly in large time evolving graph and c cascade and immunization for the first we present a list of static and temporal law including advance pattern like eigenspokes we show how to use them to spot suspicious activity in on line buyer and seller setting in facebook in twitter like network for the second we show how to handle time evolving graph a tensor how to handle large tensor in map reduce environment a well a some discovery such setting for the third we show that for virus propagation a single number is enough to characterize the connectivity of graph and thus we show how to do efficient immunization for almost any type of virus si no immunity sir lifetime immunity etc we conclude with some open research question for graph mining 
we introduce a new problem the online selective anomaly detection osad to model a specific scenario emerging from research in sleep science scientist have segmented sleep into several stage and stage two is characterized by two pattern or anomaly in the eeg time series recorded on sleep subject these two pattern are sleep spindle s and k complex the osad problem wa introduced to design a residual system where all anomaly known and unknown are detected but the system only trigger an alarm when non s anomaly appear the solution of the osad problem required u to combine technique from both data mining and control theory experiment on data from real subject attest to the effectiveness of our approach 
online social network have become ubiquitous to today s society and the study of data from these network ha improved our understanding of the process by which relationship form research in statistical relational learning focus on method to exploit correlation among the attribute of linked node to predict user characteristic with greater accuracy concurrently research on generative graph model ha primarily focused on modeling network structure without attribute producing several model that are able to replicate structural characteristic of network such a power law degree distribution or community structure however there ha been little work on how to generate network with real world structural property and correlated attribute in this work we present the attributed graph model agm framework to jointly model network structure and vertex attribute our framework learns the attribute correlation in the observed network and exploit a generative graph model such a the kronecker product graph model kpgm and chung lu graph model cl to compute structural edge probability agm then combine the attribute correlation with the structural probability to sample network conditioned on attribute value while keeping the expected edge probability and degree of the input graph model we outline an efficient method for estimating the parameter of agm a well a a sampling method based on accept reject sampling to generate edge with correlated attribute we demonstrate the efficiency and accuracy of our agm framework on two large real world network showing that agm scale to network with hundred of thousand of vertex a well a having high attribute correlation 
an incisive understanding of personal psychological trait is not only essential to many scientific discipline but also ha a profound business impact on online recommendation recent study in psychology suggest that novelty seeking trait is highly related to consumer behavior in this paper we focus on understanding individual novelty seeking trait embodied at different level and across heterogeneous domain unlike the questionnaire based method widely adopted in the past we first present a computational framework novel seeking model nsm for exploring the novelty seeking trait implied by observable activity then we explore the novelty seeking trait in two heterogeneous domain check in behavior in location based social network which reflects mobility pattern in the physical world and online shopping behavior on e commerce site which reflects consumption concept in economic activity to demonstrate the effectiveness of nsm we conducted extensive experiment with a large dataset covering the two domain activity for hundred of thousand of individual our result suggest that nsm offer a powerful paradigm for presenting an effective measurement of a personality trait that can explicitly explain the deviation of individual from the habit of individual and crowd uncovering the correlation of novelty seeking trait at different level and across heterogeneous domain the proposed method provides emerging implication for personalized cross domain recommendation and targeted advertising 
we present a system that demonstrates how the compositional structure of event in concert with the compositional structure of language can interplay with the underlying focusing mechanism in video action recognition providing a medium for top down and bottom up integration a well a multi modal integration between vision and language we show how the role played by participant noun their characteristic adjective the action performed verb the manner of such action adverb and changing spatial relation between participant preposition in the form of whole sentence description mediated by a grammar guide the activity recognition process further the utility and expressiveness of our framework is demonstrated by performing three separate task in the domain of multi activity video sentence guided focus of attention generation of sentential description and query based search simply by leveraging the framework in different manner 
e commerce web site such a ebay a well a advertising exchange adx such a doubleclick s rightmedia or adecn work a intermediary who sell item e g page view on behalf of a seller e g a publisher to buyer on the opposite side of the market e g advertiser these platform often use fixed percentage sharing scheme according to which i the platform run an auction amongst buyer and ii give the seller a constant fraction e g of the auction proceeds in these setting the platform face asymmetric information regarding both the valuation of buyer for the item a in a standard auction environment a well a about the seller s opportunity cost of selling the item moreover platform often face intense competition from similar market place and such competition is likely to favor auction rule that secure high payoff to seller in such an environment what selling mechanism should platform employ our goal in this paper is to study optimal mechanism design in setting plagued by competition and two sided asymmetric information and identify condition under which the current practice of employing constant cut is indeed optimal in particular we first show that for a large class of competition game platform behave in equilibrium a if they maximize a a convex combination of seller s payoff and platform s revenue with weight on the seller s payoff which is proxy for the intensity of competition in the market we generalize the analysis of myerson and satterthwaite and derive the optimal direct revelation mechanism for each a expected the optimal mechanism applies a reserve price which is decreasing in next we present an indirect implementation based on sharing scheme we show that constant cut are optimal if and only if the opportunity cost of the seller ha a power form distribution and derive a simple formula for computing the optimal constant cut a a function of the seller distribution of opportunity cost and the market competition proxy finally for completeness we study the case of a seller s optimal auction with a fixed profit for the platform and derive the optimal direct and indirect implementation in this setting 
it ha been suggested that online search and retrieval contributes to the intellectual isolation of user within their preexisting ideology where people s prior view are strengthened and alternative viewpoint are infrequently encountered this so called filter bubble phenomenon ha been called out a especially detrimental when it come to dialog among people on controversial emotionally charged topic such a the labeling of genetically modified food the right to bear arm the death penalty and online privacy we seek to identify and study information seeking behavior and access to alternative versus reinforcing viewpoint following shocking emotional and large scale news event we choose for a case study to analyze search and browsing on gun control right a strongly polarizing topic for both citizen and leader of the united state we study the period of time preceding and following a mass shooting to understand how it occurrence follow on discussion and debate may have been linked to change in the pattern of searching and browsing we employ information theoretic measure to quantify the diversity of web domain of interest to user and understand the browsing pattern of user we use these measure to characterize the influence of news event on these web search and browsing pattern 
the web is rapidly evolving into the web of the world where people place thing and their relationship are all digitally represented this evolution open up unparalleled opportunity to organize this vast digital universe for even greater human purpose in this talk dr lu will share an outline of microsoft s quest and aspiration to organize the digital universe with a pervasive computational fabric of digital information digital service and digital experience that empower every human being on the planet to accomplish more and enrich their life dr lu will discus high level computational structure and present specific example across bing window and other product and service to illustrate microsoft s approach to delivering end user value and accelerating the pace of innovation for the industry a a whole 
learning fine grained image similarity is a challenging task it need to capture between class and within class image difference this paper proposes a deep ranking model that employ deep learning technique to learn similarity metric directly from image it ha higher learning capability than model based on hand crafted feature a novel multiscale network structure ha been developed to describe the image effectively an efficient triplet sampling algorithm is also proposed to learn the model with distributed asynchronized stochastic gradient extensive experiment show that the proposed algorithm outperforms model based on hand crafted visual feature and deep classification model 
we describe a completely automated large scale visual recommendation system for fashion our focus is to efficiently harness the availability of large quantity of online fashion image and their rich meta data specifically we propose two class of data driven model in the deterministic fashion recommenders dfr and stochastic fashion recommenders sfr for solving this problem we analyze relative merit and pitfall of these algorithm through extensive experimentation on a large scale data set and baseline them against existing idea from color science we also illustrate key fashion insight learned through these experiment and show how they can be employed to design better recommendation system the industrial applicability of proposed model is in the context of mobile fashion shopping finally we also outline a large scale annotated data set of fashion image fashion k that can be exploited for future research in data driven visual fashion 
discovering and tracking topic shift in news constitutes a new challenge for application nowadays topic evolve emerge and fade making it more difficult for the journalist or the press consumerto decrypt the news for instance the current syrian chemical crisis ha been the starting point of the un russian initiative and also the revival of the u france alliance a topical mapping representing how the topic evolve in time would be helpful to contextualize information a far a we know few topic tracking system can provide such temporal topic connection in this paper we introduce a novel framework inspired from collective factorization for online topic discovery able to connect topic between different time slot the framework learns jointly the topic evolution and their time dependency it offer the user the ability to control through one unique hyper parameter the tradeoff between the past accumulated knowledge and the current observed data we show on semi synthetic datasets and on yahoo news article that our method is competitive with state of the art technique while providing a simple way to monitor topic evolution including emerging and disappearing topic 
motivated by multi distribution divergence which originate in information theory we propose a notion of multi point kernel and study their application we study a class of kernel based on jensen type divergence and show that these can be extended to measure similarity among multiple point we study tensor flattening method and develop a multi point kernel spectral clustering msc method we further emphasize on a special case of the proposed kernel which is a multi point extension of the linear dot product kernel and show the existence of cubic time tensor flattening algorithm in this case finally we illustrate the usefulness of our contribution using standard data set and image segmentation task 
local search user today decide what business to visit solely based on distance information and business rating that can be sparse or stale we believe that when user search for local business such a bar or restaurant they need to know more about the ambience of each business such a how crowded it is how loud and of what type the music it play is a well a how loud the human chatter in the business is unfortunately this information doesn t exist today in this paper we propose to automatically crowdsource such rich local business ambience metadata through real user check in event every time a user check into a business the phone is in user s hand and the phone s sensor can sense the business environment we leverage the phone s microphone during this time to infer the occupancy and human chatter level the music type a well a the music and noise level in the business a people check in to business throughout the day business metadata can be automatically updated over time enabling a new generation of local search experience using approximately audio trace collected from real business of various type over a period of month we show that by properly extracting the temporal and frequency signature of the audio signal it is feasible to train model that can simultaneously infer occupancy human chatter music and noise level in a business with higher than accuracy 
traditional search system generally present a ranked list of document a answer to user query in aggregated search system result from different and increasingly diverse vertical image video news etc are returned to user for instance many such search engine return to user both image and web document a answer to the query flower aggregated search ha become a very popular paradigm in this paper we go one step further and study a different search paradigm composite retrieval rather than returning and merging result from different vertical a is the case with aggregated search we propose to return to user a set of bundle where a bundle is composed of cohesive result from several vertical for example for the query london olympic one bundle per sport could be returned each containing result extracted from news video image or wikipedia composite retrieval can promote exploratory search in a way that help user understand the diversity of result available for a specific query and decide what to explore in more detail in this paper we propose and evaluate a variety of approach to construct bundle that are relevant cohesive and diverse compared with three baseline traditional general web only ranking federated search ranking and aggregated search our evaluation result demonstrate significant performance improvement for a highly heterogeneous web collection 
there is a growing interest in intelligent assistant for a variety of application from sorting email to helping people with disability to do their daily chore in this paper we formulate the problem of intelligent assistance in a decision theoretic framework and present both theoretical and empirical result we first introduce a class of pomdps called hidden goal mdps hgmdps which formalizes the problem of interactively assisting an agent whose goal is hidden and whose action are observable in spite of it restricted nature we show that optimal action selection for hgmdps is pspace complete even for deterministic dynamic we then introduce a more restricted model called helper action mdps hamdps which are sufficient for modeling many real world problem we show class of hamdps for which efficient algorithm are possible more interestingly for general hamdps we show that a simple myopic policy achieves a near optimal regret compared to an oracle assistant that know the agent s goal we then introduce more sophisticated version of this policy for the general case of hgmdps that we combine with a novel approach for quickly learning about the agent being assisted we evaluate our approach in two game like computer environment where human subject perform task and in a real world domain of providing assistance during folder navigation in a computer desktop environment the result show that in all three domain the framework result in an assistant that substantially reduces user effort with only modest computation 
web application security is an important problem in today s internet a major cause of this status is that many programmer do not have adequate knowledge about secure coding so they leave application with vulnerability an approach to solve this problem is to use source code static analysis to find these bug but these tool are known to report many false positive that make hard the task of correcting the application this paper explores the use of a hybrid of method to detect vulnerability with le false positive after an initial step that us taint analysis to flag candidate vulnerability our approach us data mining to predict the existence of false positive this approach reach a trade off between two apparently opposite approach human coding the knowledge about vulnerability for taint analysis versus automatically obtaining that knowledge with machine learning for data mining given this more precise form of detection we do automatic code correction by inserting fix in the source code the approach wa implemented in the wap tool and an experimental evaluation wa performed with a large set of open source php application 
massive open online course moocs one of the latest internet revolution have engendered hope that constant iterative improvement and economy of scale may cure the cost disease of higher education while scalable in many way providing feedback for homework submission particularly open ended one remains a challenge in the online classroom in course where the student teacher ratio can be ten thousand to one or worse it is impossible for instructor to personally give feedback to student or to understand the multitude of student approach and pitfall organizing and making sense of massive collection of homework solution is thus a critical web problem despite the challenge the dense solution space sampling in highly structured homework for some moocs suggests an elegant solution to providing quality feedback to student on a massive scale we outline a method for decomposing online homework submission into a vocabulary of code phrase and based on this vocabulary we architect a queryable index that allows for fast search into the massive dataset of student homework submission to demonstrate the utility of our homework search engine we index over a million code submission from user worldwide in stanford s machine learning mooc and a semi automatically learn shared structure amongst homework submission and b generate specific feedback for student mistake codewebs is a tool that leverage the redundancy of densely sampled highly structured homework in order to force multiply teacher effort giving articulate instant feedback is a crucial component of the online learning process and thus by building a homework search engine we hope to take a step towards higher quality free education 
the past year have seen a great improvement in the rigor of information retrieval experimentation due primarily to two factor high quality public portable test collection such a those produced by trec the text retrieval conference and the increased practice of statistical hypothesis testing to determine whether measured improvement can be ascribed to something other than random chance together these create a very useful standard for reviewer program committee and journal editor work in information retrieval ir increasingly cannot be published unless it ha been evaluated using a well constructed test collection and shown to produce a statistically significant improvement over a good baseline but a the saying go any tool sharp enough to be useful is also sharp enough to be dangerous statistical test of significance are widely misunderstood most researcher treat them a a black box evaluation result go in and a p value come out because significance is such an important factor in determining what research direction to explore and what is published using p value obtained without thought can have consequence for everyone doing research in ir ioannidis ha argued that the main consequence in the biomedical science is that most published research finding are false could that be the case in ir a well 
statistical model with constrained probability distribution are abundant in machine learning some example include regression model with norm constraint e g lasso probit model many copula model and latent dirichlet allocation lda model bayesian inference involving probability distribution confined to constrained domain could be quite challenging for commonly used sampling algorithm for such problem we propose a novel markov chain monte carlo mcmc method that provides a general and computationally efficient framework for handling boundary condition our method first map the d dimensional constrained domain of parameter to the unit ball formula see text then augments it to a d dimensional sphere s d such that the original boundary corresponds to the equator of s d this way our method handle the constraint implicitly by moving freely on the sphere generating proposal that remain within boundary when mapped back to the original space to improve the computational efficiency of our algorithm we divide the dynamic into several part such that the resulting split dynamic ha a partial analytical solution a a geodesic flow on the sphere we apply our method to several example including truncated gaussian bayesian lasso bayesian bridge regression and a copula model for identifying synchrony among multiple neuron our result show that the proposed method can provide a natural and efficient framework for handling several type of constraint on target distribution 
recommendation system have been widely used in e commerce site social network etc one of the core task in recommendation system is to predict the user rating on item although many model and algorithm have been proposed how to make accurate prediction for new user with extremely few rating record still remains a big challenge which is called the cold start problem many existing method utilize additional information such a social graph to cope with the cold start problem however the side information may not always be available in contrast to such method we propose a more general solution to address the cold start problem based on the observed user rating record only specifically we define a random walk on a bipartite graph of user and item to simulate the preference propagation among user in order to alleviate the data sparsity problem for cold start user then we propose a monte carlo algorithm to estimate the similarity between different user this algorithm take a precomputation approach and thus can efficiently compute the user similarity given any new user for rating prediction in addition our algorithm can easily handle dynamic update and can be parallelized naturally which are crucial for large recommendation system theoretical analysis is presented to demonstrate the efficiency and effectiveness of our algorithm and extensive experiment also confirm our theoretical finding 
we describe the design implementation and evaluation of ember an automated x continuous system for forecasting civil unrest across country of latin america using open source indicator such a tweet news source blog economic indicator and other data source unlike retrospective study ember ha been making forecast into the future since nov which have been and continue to be evaluated by an independent t e team mitre of note ember ha successfully forecast the june protest in brazil and feb violent protest in venezuela we outline the system architecture of ember individual model that leverage specific data source and a fusion and suppression engine that support trading off specific evaluation criterion ember also provides an audit trail interface that enables the investigation of why specific prediction were made along with the data utilized for forecasting through numerous evaluation we demonstrate the superiority of ember over baserate method and it capability to forecast significant societal happening 
query auto completion qac is a common interactive feature that assist user in formulating query by providing completion suggestion a they type in order for qac to minimise the user s cognitive and physical effort it must i suggest the user s intended query after minimal input keystroke and ii rank the user s intended query highly in completion suggestion typically qac approach rank completion suggestion by their past popularity accordingly qac is usually very effective for previously seen and consistently popular query user are increasingly turning to search engine to find out about unpredictable emerging and ongoing event and phenomenon often using previously unseen or unpopular query consequently qac must be both robust and time sensitive that is able to sufficiently rank both consistently and recently popular query in completion suggestion to address this trade off we propose several practical completion suggestion ranking approach including i a sliding window of query popularity evidence from the past day ii the query popularity distribution in the last n query observed with a given prefix and iii short range query popularity prediction based on recently observed trend using real time simulation experiment we extensively investigated the parameter necessary to maximise qac effectiveness for three openly available query log datasets with prefix of character msn and aol both english and sogou chinese optimal parameter vary for each query log capturing the differing temporal dynamic and querying distribution result demonstrate consistent and language independent improvement of up to over a non temporal qac baseline for all query log with prefix length of character this work is an important step towards more effective qac approach 
we consider a single buyer with a combinatorial preference that would like to purchase related product and service from different vendor where each vendor supply exactly one product we study the general case where subset of product can be substitute a well a complementary and analyze the game that is induced on the vendor where a vendor s strategy is the price that he asks for his product this model generalizes both bertrand competition where vendor are perfect substitute and nash bargaining where they are perfect complement and capture a wide variety of scenario that can appear in complex crowd sourcing or in automatic pricing of related product we study the equilibrium of such game and show that a pure efficient equilibrium always exists in the case of submodular buyer preference we fully characterize the set of pure nash equilibrium essentially showing uniqueness for the even more restricted substitute buyer preference we also prove uniqueness over mixed equilibrium finally we begin the exploration of natural generalization of our setting such a when service have cost when there are multiple buyer or uncertainty about the the buyer s valuation and when a single vendor supply multiple product 
the development of semantic web rdf brings new requirement for data analytics tool and method going beyond querying to semantics rich analytics through warehouse style tool in this work we fully redesign from the bottom up core data analytics concept and tool in the context of rdf data leading to the first complete formal framework for warehouse style rdf analytics notably we define i analytical schema tailored to heterogeneous semantics rich rdf graph ii analytical query which beyond relational cube allow flexible querying of the data and the schema a well a powerful aggregation and iii olap style operation experiment on a fully implemented platform demonstrate the practical interest of our approach 
browser based defense have recently been advocated a an effective mechanism to protect web application against the threat of session hijacking fixation and related attack in existing approach all such defense ultimately rely on client side heuristic to automatically detect cooky containing session information to then protect them against theft or otherwise unintended use while clearly crucial to the effectiveness of the resulting defense mechanism these heuristic have not a yet undergone any rigorous assessment of their adequacy in this paper we conduct the first such formal assessment based on a gold set of cooky we collect from popular website of the alexa ranking to obtain the gold set we devise a semi automatic procedure that draw on a novel notion of authentication token which we introduce to capture multiple web authentication scheme we test existing browser based defense in the literature against our gold set unveiling several pitfall both in the heuristic adopted and in the method used to ass them we then propose a new detection method based on supervised learning where our gold set is used to train a binary classifier and report on experimental evidence that our method outperforms existing proposal interestingly the resulting classification together with our hand on experience in the construction of the gold set provides new insight on how web authentication is implemented in practice 
sustainable energy system of the future could no longer rely on the current paradigm that energy supply follows demand since many of the renewable energy resource do not produce power on demand there is a need for new market structure that motivate sustainable behavior by participant the power trading agent competition power tac is a new annual competition that focus on the design and operation of future retail power market specifically in smart grid environment with renewable energy production smart metering and autonomous agent acting on behalf of customer and retailer it us a rich open source simulation platform that is based on real world data and state of the art customer model it purpose is to help researcher understand the dynamic of customer and retailer decision making a well a the robustness of proposed market design this research contributes to the former by introducing tactex the champion agent from the inaugural competition in tactex learns and adapts to the environment in which it operates by heavily relying on reinforcement learning and prediction method we formalize the complex decision making problem that tactex face and approximate it solution in tactex s constituent component we examine the success of the complete agent through analysis of competition result 
driven by outstanding success story of internet startup such a facebook and the huffington post recent study have thoroughly described their growth these highly visible online success story however overshadow an untold number of similar venture that fail the study of website popularity is ultimately incomplete without general mechanism that can describe both success and failure in this work we present six year of the daily number of user dau of twenty two membership based website encompassing online social network grassroots movement online forum and membership only internet store well balanced between success and failure we then propose a combination of reaction diffusion decay process whose resulting equation seem not only to describe well the observed dau time series but also provide mean to roughly predict their evolution this model allows an approximate automatic dau based classification of website into self sustainable v s unsustainable and whether the startup growth is mostly driven by marketing medium campaign or word of mouth adoption 
online controlled experiment also called a b testing have been established a the mantra for data driven decision making in many web facing company a b testing support decision making by directly comparing two variant at a time it can be used for comparison between two candidate treatment and a candidate treatment and an established control in practice one typically run an experiment with multiple treatment together with a control to make decision for both purpose simultaneously this is known to have two issue first having multiple treatment increase false positive due to multiple comparison second the selection process cause an upward bias in estimated effect size of the best observed treatment to overcome these two issue a two stage process is recommended in which we select the best treatment from the first screening stage and then run the same experiment with only the selected best treatment and the control in the validation stage traditional application of this two stage design often focus only on result from the second stage in this paper we propose a general methodology for combining the first screening stage data together with validation stage data for more sensitive hypothesis testing and more accurate point estimation of the treatment effect our method is widely applicable to existing online controlled experimentation system 
given the heterogeneity of the data one can find on the linked data cloud being able to trace back the provenance of query result is rapidly becoming a must have feature of rdf system while provenance model have been extensively discussed in recent year little attention ha been given to the efficient implementation of provenance enabled query inside data store this paper introduces tripleprov a new system extending a native rdf store to efficiently handle such query tripleprov implement two different storage model to physically co locate lineage and instance data and for each of them implement algorithm for tracing provenance at two granularity level in the following we present the overall architecture of our system it different lineage storage model and the various query execution strategy we have implemented to efficiently answer provenance enabled query in addition we present the result of a comprehensive empirical evaluation of our system over two different datasets and workload 
user provided rating data about product and service is one key feature of website such a amazon tripadvisor or yelp since these rating are rather static but might change over time a temporal analysis of rating distribution provides deeper insight into the evolution of a product quality given a time series of rating distribution in this work we answer the following question how to detect the base behavior of user regarding a product s evaluation over time how to detect point in time where the rating distribution differs from this base behavior e g due to attack or spontaneous change in the product s quality to achieve these goal we model the base behavior of user regarding a product a a latent multivariate autoregressive process this latent behavior is mixed with a sparse anomaly signal finally leading to the observed data we propose an efficient algorithm solving our objective and we present interesting finding on various real world datasets 
micro finance organization provide non profit lending opportunity to mitigate poverty by financially supporting impoverished yet skilled entrepreneur who are in desperate need of an institution that lends to them in kiva org a widely used crowd funded micro financial service a vast amount of micro financial activity are done by lending team and thus understanding their diverse characteristic is crucial in maintaining a healthy micro finance ecosystem a the first step for this goal we model different lending team by using a maximum entropy distribution approach based on a wealthy set of heterogeneous information regarding micro financial transaction available at kiva based on this approach we achieved a competitive performance in predicting the lending activity for the top team furthermore we provide deep insight about the characteristic of lending team by analyzing the resulting team specific lending model we found that lending team are generally more careful in selecting loan by a loan s geo location a borrower s gender a field partner s reliability etc when compared to lender without team affiliation in addition we identified interesting lending behavior of different lending team based on lender background and interest such a their ethnic religious linguistic educational regional and occupational aspect finally using our proposed model we tackled a novel problem of lending team recommendation and showed it promising performance result 
team of mobile robot often need to divide up subtasks efficiently in spatial domain a key criterion for doing so may depend on distance between robot and the subtasks location this research considers a specific such criterion namely how to assign interchangeable robot to a set of target location such that the makespan time for all robot to reach their target location is minimized while also preventing collision among robot we provide an overview of a scalable multiagent dynamic role assignment system known a scram scalable collision avoiding role assignment with minimal makespan scram us a graph theoretic approach to map agent to target location such that our objective for both minimizing the makespan and avoiding agent collision are met scram scale to thousand of agent a role assignment algorithm run in polynomial time 
we develop a method for optimization in shape space i e set of surface modulo re parametrization unlike previously proposed gradient flow we achieve superlinear convergence rate through an approximation of the shape hessian which is generally hard to compute and suffers from a series of degeneracy our analysis highlight the role of mean curvature motion in comparison with first order scheme instead of surface area our approach penalizes deformation either by it dirichlet energy or total variation and hence doe not suffer from shrinkage the latter regularizer spark the development of an alternating direction method of multiplier on triangular mesh therein a conjugate gradient solver enables u to bypass formation of the gaussian normal equation appearing in the course of the overall optimization we combine all of these idea in a versatile geometric variation regularized levenberg marquardt type method applicable to a variety of shape functionals depending on intrinsic property of the surface such a normal field and curvature a well a it embedding into space promising experimental result are reported 
service composition us existing service based application a component to achieve a business goal the composite service operates in a highly dynamic environment hence it can fail at any time due to the failure of component service service composition language such a bpel provide a compensation mechanism to rollback the error but such a compensation mechanism ha several issue for instance it cannot guarantee the functional property of the composite service after compensation in this work we propose an automated approach based on a genetic algorithm to calculate the recovery plan that could guarantee the satisfaction of functional property of the composite service after recovery given a composite service with large state space the proposed method doe not require exploring the full state space of the composite service therefore it allows efficient selection of recovery plan in addition the selection of recovery plan is based on their quality of service qos a qos optimal recovery plan allows effective recovery from the state of failure our approach ha been evaluated on real world case study and ha shown promising result 
we study the power of fractional allocation of resource to maximize our influence in a network this work extends in a natural way the well studied model by kleinberg kempe and tardos where a designer selects a small seed set of node in a social network to influence directly this influence cascade when other node reach certain threshold of neighbor influence and the goal is to maximize the final number of influenced node despite extensive study from both practical and theoretical viewpoint this model limit the designer to a binary choice for each node with no chance to apply intermediate level of influence this model capture some setting precisely such a exposure to an idea or pathogen but it fails to capture very relevant concern in others for example a manufacturer promoting a new product by distributing five off coupon instead of giving away a single free product while fractional version of problem tend to be easier to solve than integral version for influence maximization we show that the two version have essentially the same computational complexity on the other hand the two version can have vastly different solution the added flexibility of fractional allocation can lead to significantly improved influence our main theoretical contribution is to show how to adapt the major positive result from the integral case to the fractional case specifically mossel and roch used the submodularity of influence to obtain their integral result we introduce a new notion of continuous submodularity and use this to obtain matching fractional result we conclude that we can achieve the same greedy e approximation for the fractional case a the integral case and that other heuristic are likely to carry over a well in practice we find that the fractional model performs substantially better than the integral model according to simulation on real world social network data 
homophily is a phenomenon observed very frequently in social network and is related with the inclination of people to be involved with others that exhibit similar characteristic the root of homophily can be subtle and are mainly traced back to two mechanism i social selection and ii peer influence decomposing the effect of each of these mechanism requires analysis of longitudinal data this ha been a burden to similar study in traditional social science due to the hardness of collecting such information however the proliferation of online social medium ha enabled the collection of massive amount of information related with human activity in this work we are interested in examining the force of the above mechanism in the context of the location visited by people for our study we use a longitudinal dataset collected from gowalla a location based social network lbsn lbsns unlike other online social medium bond user online interaction with their activity in real world physical location prior work on lbsns ha focused on the influence of geographical constraint on the formation of social tie on the contrary in this paper we perform a microscopic study of the peer influence and social selection mechanism in lbsns our analysis indicates that while the similarity of friend spatial trail at a geographically global scale cannot be attributed to peer influence the latter can explain up to of the geographically localized similarity between friend moreover this percentage depends on the type of location we examine and it can be even higher for specific category e g nightlife spot finally we find that the social selection mechanism is only triggered by place that exhibit specific network characteristic we believe that our work can have significant implication on obtaining a deeper understanding of the way that people create friendship act and move in real space which can further facilitate and enhance application such a recommender system trip planning and marketing 
the web ha enabled one of the most visible recent development in education the deployment of massive open online course with their global reach and often staggering enrollment moocs have the potential to become a major new mechanism for learning despite this early promise however moocs are still relatively unexplored and poorly understood in a mooc each student s complete interaction with the course material take place on the web thus providing a record of learner activity of unprecedented scale and resolution in this work we use such trace data to develop a conceptual framework for understanding how user currently engage with moocs we develop a taxonomy of individual behavior examine the different behavioral pattern of highand low achieving student and investigate how forum participation relates to other part of the course we also report on a large scale deployment of badge a incentive for engagement in a mooc including randomized experiment in which the presentation of badge wa varied across sub population we find that making badge more salient produced increase in forum engagement 
teamwork and care coordination are of increasing importance to health care delivery and patient safety and health this thesis aim at developing agent that are able to make intelligent information sharing decision to support a diverse evolving team of care provider in constructing and maintaining a shared plan that operates in uncertain environment and over a long time horizon 
online advertising is an essential part of the internet and the main source of revenue for many web centric firm such a search engine social network and online publisher a key component of online advertising is the auction mechanism which selects and price the set of winning ad this work is inspired by one of the biggest practical drawback of the widely popular vickrey clarke grove vcg mechanism which is the unique incentive compatible mechanism that maximizes social welfare it is known that vcg lack a desired property of revenue monotonicity a natural notion which state that the revenue of a mechanism shouldn t go down a the number of bidder increase or if the bidder increase their bid most firm which depend on online advertising revenue have a large sale team to attract more bidder on their inventory a the general belief is that more bidder will increase competition and hence revenue however the lack of revenue monotonicity of vcg conflict with this general belief and can be strategically confusing for the firm s business in this work we seek incentive compatible mechanism that are revenue monotone this natural property come at the expense of social welfare one can show that it is not possible to get incentive compatibility revenue monotonicity and optimal social welfare simultaneously in light of this we introduce the notion of price of revenue monotonicity porm to capture the loss in social welfare of a revenue monotone mechanism we further study revenue monotonicity for two important online advertising scenario first one is the text v image ad auction where in an ad slot one can either show a single image ad or a few text ad second one is the video pod auction where we have a video advertising slot of k second which can be filled with multiple video ad for the image text auction we give a mechanism that satisfy both rm and ic and achieve porm of i k i ln k we also show that the porm of our mechanism is the best possible by proving a matching lower bound of i k i on the porm of any deterministic mechanism under some mild assumption for the video pod auction we give a mechanism that achieves a porm of log k ln k 
we consider a model of repeated online auction in which an ad with an uncertain click through rate face a random distribution of competing bid in each auction and there is discounting of payoff we formulate the optimal solution to this explore exploit problem a a dynamic programming problem and show that efficiency is maximized by making a bid for each advertiser equal to the advertiser s expected value for the advertising opportunity plus a term proportional to the variance in this value divided by the number of impression the advertiser ha received thus far we then use this result to illustrate that the value of incorporating active exploration into a machine learning system in an auction environment is exceedingly small 
r rml is used to specify transformation of data available in relational database into materialised or virtual rdf datasets sparql query evaluated against virtual datasets are translated into sql query according to the r rml mapping so that they can be evaluated over the underlying relational database engine in this paper we describe an extension of a well known algorithm for sparql to sql translation originally formalised for rdbms backed triple store that take into account r rml mapping we present the result of our implementation using query from a synthetic benchmark and from three real use case and show that sparql query can be in general evaluated a fast a the sql query that would have been generated by sql expert if no r rml mapping had been used 
the heat kernel is a type of graph diffusion that like the much used personalized pagerank diffusion is useful in identifying a community nearby a starting seed node we present the first deterministic local algorithm to compute this diffusion and use that algorithm to study the community that it produce our algorithm is formally a relaxation method for solving a linear system to estimate the matrix exponential in a degree weighted norm we prove that this algorithm stay localized in a large graph and ha a worst case constant runtime that depends only on the parameter of the diffusion not the size of the graph on large graph our experiment indicate that the community produced by this method have better conductance than those produced by pagerank although they take slightly longer to compute on a real world community identification task the heat kernel community perform better than those from the pagerank diffusion 
petabyte of data about human movement transaction and communication pattern are being generated by everyday technology such a mobile phone credit card this unprecedented volume of information facilitates a novel set of research question applicable to a wide range of development issue in collaboration involving mobile phone operator across country jana s mobile technology platform can instantly poll and compensate billion active mobile subscription this talk will discus how insight gained from living in kenya became the genesis of a technology company currently working with global client in over country including p g google unilever danone general mill nestle johnson johnson microsoft the world bank and the united nation after providing an overview of the mobile and social medium landscape in emerging market we discus a system that implement poll mobile subscription compensation the presentation will conclude by emphasizing the value of consumer data in underserved and understudied region of the world 
understanding the group characteristic in mmorpgs is important in user behavior study since people tend to gather together and form group due to their inherent nature in this paper we analyze the group activity of user in aion one of the largest mmorpgs based on the record of the activity of user in particular we focus on i how social interaction within a group differ from the one across group ii what make a group rise sustain or fall iii how group member join and leave a group and iv what make a group end we first find that structural pattern of social interaction within a group are more likely to be close knit and reciprocative than the one across group we also observe that member in a rising group i e the number of member increase are more cohesive and communicate with more evenly within the group than the one in other group our analysis further reveals that if a group is not cohesive not actively communicating or not evenly communicating among member member of the group tend to leave 
in large scale query by example retrieval embedding image signature in a binary space offer two benefit data compression and search efficiency while most embedding algorithm binarize both query and database signature it ha been noted that this is not strictly a requirement indeed asymmetric scheme that binarize the database signature but not the query still enjoy the same two benefit but may provide superior accuracy in this work we propose two general asymmetric distance that are applicable to a wide variety of embedding technique including locality sensitive hashing lsh locality sensitive binary code lsbc spectral hashing sh pca embedding pcae pcae with random rotation pcae rr and pcae with iterative quantization pcae itq we experiment on four public benchmark containing up to m image and show that the proposed asymmetric distance consistently lead to large improvement over the symmetric hamming distance for all binary embedding technique 
supervised hashing aim to map the original feature to compact binary code that are able to preserve label based similarity in the hamming space non linear hash function have demonstrated their advantage over linear one due to their powerful generalization capability in the literature kernel function are typically used to achieve non linearity in hashing which achieve encouraging retrieval performance at the price of slow evaluation and training time here we propose to use boosted decision tree for achieving non linearity in hashing which are fast to train and evaluate hence more suitable for hashing with high dimensional data in our approach we first propose sub modular formulation for the hashing binary code inference problem and an efficient graphcut based block search method for solving large scale inference then we learn hash function by training boosted decision tree to fit the binary code experiment demonstrate that our proposed method significantly outperforms most state of the art method in retrieval precision and training time especially for highdimensional data our method is order of magnitude faster than many method in term of training time 
reducing energy consumption of climate control system is important in order to reduce human environmental foot print we consider a method for an automated agent to provide advice to driver which will motivate them to reduce the energy consumption of their climate control unit our approach take into account both the energy consumption of the climate control system and the expected comfort level of the driver we therefore build two model one for assessing the energy consumption of the climate control system a a function of the system s setting and the other model human comfort level a a function of the climate control system s setting using these model the agent provides advice to the driver considering how to set the climate control system the agent advises setting which try to preserve a high level of comfort while consuming a little energy a possible we empirically show that driver equipped with our agent which provides them with advice signicantly save energy a compared to driver not equipped with our agent 
an important task in the analysis of multiagent system is to understand how group of selfish player can form coalition i e work together in team in this paper we study the dynamic of coalition formation under bounded rationality we consider setting where each team s profit is given by a concave function and propose three profit sharing scheme each of which is based on the concept of marginal utility the agent are assumed to be myopic i e they keep changing team a long a they can increase their payoff by doing so we study the property such a closeness to nash equilibrium or total profit of the state that result after a polynomial number of such move and prove bound on the price of anarchy and the price of stability of the corresponding game 
pattern and texture are key characteristic of many natural object a shirt can be striped the wing of a butterfly can be veined and the skin of an animal can be scaly aiming at supporting this dimension in image understanding we address the problem of describing texture with semantic attribute we identify a vocabulary of forty seven texture term and use them to describe a large dataset of pattern collected in the wild the resulting describable texture dataset dtd is a basis to seek the best representation for recognizing describable texture attribute in image we port from object recognition to texture recognition the improved fisher vector ifv and deep convolutional network activation feature decaf and show that surprisingly they both outperform specialized texture descriptor not only on our problem but also in established material recognition datasets we also show that our describable attribute are excellent texture descriptor transferring between datasets and task in particular combined with ifv and decaf they significantly outperform the state of the art by more than on both fmd and kth tip b benchmark we also demonstrate that they produce intuitive description of material and internet image 
a number of psychological and physiological evidence suggest that early visual attention work in a coarse to fine way which lay a basis for the reverse hierarchy theory rht this theory state that attention propagates from the top level of the visual hierarchy that process gist and abstract information of input to the bottom level that process local detail inspired by the theory we develop a computational model for saliency detection in image first the original image is downsampled to different scale to constitute a pyramid then saliency on each layer is obtained by image super resolution reconstruction from the layer above which is defined a unpredictability from this coarse to fine reconstruction finally saliency on each layer of the pyramid is fused into stochastic fixation through a probabilistic model where attention initiate from the top layer and propagates downward through the pyramid extensive experiment on two standard eye tracking datasets show that the proposed method can achieve competitive result with state of the art model 
with the rapid growth of web service in the past decade the issue of qos aware web service recommendation is becoming more and more critical since the web service qos information collection work requires much time and effort and is sometimes even impractical the service qos value is usually missing there are some work to predict the missing qos value using traditional collaborative filtering method based on user service static model however the qos value is highly related to the invocation context e g qos value are various at different time by considering the third dynamic context information a temporal qos aware web service recommendation framework is presented to predict missing qos value under various temporal context further we formalize this problem a a generalized tensor factorization model and propose a non negative tensor factorization ntf algorithm which is able to deal with the triadic relation of user service time model extensive experiment are conducted based on our real world web service qos dataset collected on planet lab which is comprised of service invocation response time and throughput value from user on web service at time period the comprehensive experimental analysis show that our approach achieves better prediction accuracy than other approach 
researcher have introduced the dynamic distributed constraint optimization problem dynamic dcop formulation to model dynamically changing multi agent coordination problem where a dynamic dcop is a sequence of static canonical dcops each partially different from the dcop preceding it existing work typically assumes that the problem in each time step is decoupled from the problem in other time step which might not hold in some application in this paper we introduce a new model called markovian dynamic dcops md dcops where a dcop is a function of the value assignment in the preceding dcop we also introduce a distributed reinforcement learning algorithm that balance exploration and exploitation to solve md dcops in an online manner 
we propose sparfa trace a new machine learning based framework for time varying learning and content analytics for educational application we develop a novel message passing based blind approximate kalman filter for sparse factor analysis sparfa that jointly trace learner concept knowledge over time analyzes learner concept knowledge state transition induced by interacting with learning resource such a textbook section lecture video etc or the forgetting effect and estimate the content organization and difficulty of the question in assessment these quantity are estimated solely from binary valued correct incorrect graded learner response data and the specific action each learner performs e g answering a question or studying a learning resource at each time instant experimental result on two online course datasets demonstrate that sparfa trace is capable of tracing each learner s concept knowledge evolution over time analyzing the quality and content organization of learning resource and estimating the question concept association and the question difficulty moreover we show that sparfa trace achieves comparable or better performance in predicting unobserved learner response compared to existing collaborative filtering and knowledge tracing method 
knowledge base kb s contain data about a large number of people organization and other entity however this knowledge can never be complete due to the dynamic of the ever changing world new company are formed every day new song are composed every minute and become of interest for addition to a kb to keep up with the real world s entity the kb maintenance process need to continuously discover newly emerging entity in news and other web stream in this paper we focus on the most difficult case where the name of new entity are ambiguous this raise the technical problem to decide whether an observed name refers to a known entity or represents a new entity this paper present a method to solve this problem with high accuracy it is based on a new model of measuring the confidence of mapping an ambiguous mention to an existing entity and a new model of representing a new entity with the same ambiguous name a a set of weighted keyphrases the method can handle both wikipedia derived entity that typically constitute the bulk of large kb s a well a entity that exist only in other web source such a online community about music or movie experiment show that our entity discovery method outperforms previous method for coping with out of kb entity called unlinkable in entity linking 
in the bag of word bow model the vocabulary is of key importance typically multiple vocabulary are generated to correct quantization artifact and improve recall however this routine is corrupted by vocabulary correlation i e overlapping among different vocabulary vocabulary correlation lead to an over counting of the indexed feature in the overlapped area or the intersection set thus compromising the retrieval accuracy in order to address the correlation problem while preserve the benefit of high recall this paper proposes a bayes merging approach to down weight the indexed feature in the intersection set through explicitly modeling the correlation problem in a probabilistic view a joint similarity on both imageand feature level is estimated for the indexed feature in the intersection set we evaluate our method on three benchmark datasets albeit simple bayes merging can be well applied in various merging task and consistently improves the baseline on multi vocabulary merging moreover bayes merging is efficient in term of both time and memory cost and yield competitive performance with the state of the art method 
network are characterized by node and edge while there ha been a spate of recent work on estimating the number of node in a network the edge estimation question appears to be largely unaddressed in this work we consider the problem of estimating the average degree of a large network using efficient random sampling where the number of node is not known to the algorithm we propose a new estimator for this problem that relies on access to node sample under a prescribed distribution next we show how to efficiently realize this ideal estimator in a random walk setting our estimator ha a natural and simple implementation using random walk we bound it performance in term of the mixing time of the underlying graph we then show that our estimator are both provably and practically better than many natural estimator for the problem our work contrast with existing theoretical work on estimating average degree which assume that a uniform random sample of node is available and the number of node is known 
recent study have demonstrated advantage of information fusion based on sparsity model for multimodal classification among several sparsity model tree structured sparsity provides a flexible framework for extraction of cross correlated information from different source and for enforcing group sparsity at multiple granularity however the existing algorithm only solves an approximated version of the cost functional and the resulting solution is not necessarily sparse at group level this paper reformulates the tree structured sparse model for multimodal classification task an accelerated proximal algorithm is proposed to solve the optimization problem which is an efficient tool for feature level fusion among either homogeneous or heterogeneous source of information in addition a fuzzy set theoretic possibilistic scheme is proposed to weight the available modality based on their respective reliability in a joint optimization problem for finding the sparsity code this approach provides a general framework for quality based fusion that offer added robustness to several sparsity based multimodal classification algorithm to demonstrate their efficacy the proposed method are evaluated on three different application multiview face recognition multimodal face recognition and target classification 
community detection ha arisen a one of the most relevant topic in the field of graph mining principally for it application in domain such a social or biological network analysis different community detection algorithm have been proposed during the last decade approaching the problem from different perspective however existing algorithm are in general based on complex and expensive computation making them unsuitable for large graph with million of vertex and edge such a those usually found in the real world in this paper we propose a novel disjoint community detection algorithm called scalable community detection scd by combining different strategy scd partition the graph by maximizing the weighted community clustering wcc a recently proposed community detection metric based on triangle analysis using real graph with ground truth overlapped community we show that scd outperforms the current state of the art proposal even those aimed at finding overlapping community in term of quality and performance scd provides the speed of the fastest algorithm and the quality in term of nmi and f score of the most accurate state of the art proposal we show that scd is able to run up to two order of magnitude faster than practical existing solution by exploiting the parallelism of current multi core processor enabling u to process graph of unprecedented size in short execution time 
we introduce a new algorithm for off policy temporal difference learning with function approximation that ha lower variance and requires le knowledge of the behavior policy than prior method we develop the notion of a recognizer a filter on action that distorts the behavior policy to produce a related target policy with low variance importance sampling correction we also consider target policy that are deviation from the state distribution of the behavior policy such a potential temporally abstract option which further reduces variance this paper introduces recognizers and their potential advantage then develops a full algorithm for linear function approximation and prof that it update are in the same direction a on policy td update which implies asymptotic convergence even though our algorithm is based on importance sampling we prove that it requires absolutely no knowledge of the behavior policy for the case of state aggregation function approximators 
personalization or customizing the experience of each individual user is seen a a useful way to navigate the huge variety of choice on the web today a key tenet of personalization is the capacity to model user preference the paradigm ha shifted from that of individual preference whereby we look at a user s past activity alone to that of shared preference whereby we model the similarity in preference between pair of user e g friend people with similar interest however shared preference are still too granular because it assumes that a pair of user would share preference across all item we therefore postulate the need to pay attention to context which refers to the specific item on which the preference between two user are to be estimated in this paper we propose a generative model for contextual agreement in preference for every triplet consisting of two user and an item the model estimate both the prior probability of agreement between the two user a well a the posterior probability of agreement with respect to the item at hand the model parameter are estimated from rating data to extend the model to unseen rating we further propose several matrix factorization technique focused on predicting agreement rather than rating experiment on real life data show that our model yield context specific similarity value that perform better on a prediction task than model relying on shared preference 
we consider discrete pairwise energy minimization problem weighted constraint satisfaction max sum labeling and method that identify a globally optimal partial assignment of variable when finding a complete optimal assignment is intractable determining optimal value for a part of variable is an interesting possibility existing method are based on different sufficient condition we propose a new sufficient condition for partial optimality which is verifiable in polynomial time invariant to reparametrization of the problem and permutation of label and includes many existing sufficient condition a special case it is derived by using a relaxation technique coherent with the relaxation for energy minimization we pose the problem of finding the maximum optimal partial assignment identifiable by the new sufficient condition a polynomial method is proposed which is guaranteed to assign same or larger part of variable find the same or larger part of optimal assignment than several existing approach the core of the method is a specially constructed linear program that identifies persistent assignment in an arbitrary multi label setting 
massive online open course have the potential to revolutionize higher education with their wide outreach and accessibility but they require instructor to come up with scalable alternate to traditional student evaluation peer grading having student ass each other is a promising approach to tackling the problem of evaluation at scale since the number of grader naturally scale with the number of student however student are not trained in grading which mean that one cannot expect the same level of grading skill a in traditional setting drawing on broad evidence that ordinal feedback is easier to provide and more reliable than cardinal feedback it is therefore desirable to allow peer grader to make ordinal statement e g project x is better than project y and not require them to make cardinal statement e g project x is a b thus in this paper we study the problem of automatically inferring student grade from ordinal peer feedback a opposed to existing method that require cardinal peer feedback we formulate the ordinal peer grading problem a a type of rank aggregation problem and explore several probabilistic model under which to estimate student grade and grader reliability we study the applicability of these method using peer grading data collected from a real class with instructor and ta grade a a baseline and demonstrate the efficacy of ordinal feedback technique in comparison to existing cardinal peer grading method finally we compare these peer grading technique to traditional evaluation technique 
the vast majority of real world classification problem are imbalanced meaning there are far fewer data from the class of interest the positive class than from other class we propose two machine learning algorithm to handle highly imbalanced classification problem the classifier are disjunction of conjunction and are created a union of parallel axis rectangle around the positive example and thus have the benefit of being interpretable the first algorithm us mixed integer programming to optimize a weighted balance between positive and negative class accuracy regularization is introduced to improve generalization performance the second method us an approximation in order to assist with scalability specifically it follows a textit characterize then discriminate approach where the positive class is characterized first by box and then each box boundary becomes a separate discriminative classifier this method ha the computational advantage that it can be easily parallelized and considers only the relevant region of feature space 
a simple and inexpensive low power and low bandwidth modification is made to a conventional off the shelf color video camera from which we recover multiple color frame for each of the original measured frame and each of the recovered frame can be focused at a different depth the recovery of multiple frame for each measured frame is made possible via high speed coding manifested via translation of a single coded aperture the inexpensive translation is constituted by mounting the binary code on a piezoelectric device to simultaneously recover depth information a liquid lens is modulated at high speed via a variable voltage consequently during the aforementioned coding process the liquid lens allows the camera to sweep the focus through multiple depth in addition to designing and implementing the camera fast recovery is achieved by an anytime algorithm exploiting the group sparsity of wavelet dct coefficient 
influence maximization fundamental for word of mouth marketing and viral marketing aim to find a set of seed node maximizing influence spread on social network early method mainly fall into two paradigm with certain benefit and drawback greedy algorithm selecting seed node one by one give a guaranteed accuracy relying on the accurate approximation of influence spread with high computational cost heuristic algorithm estimating influence spread using efficient heuristic have low computational cost but unstable accuracy we first point out that greedy algorithm are essentially finding a self consistent ranking where node rank are consistent with their ranking based marginal influence spread this insight motivates u to develop an iterative ranking framework i e imrank to efficiently solve influence maximization problem under independent cascade model starting from an initial ranking e g one obtained from efficient heuristic algorithm imrank find a self consistent ranking by reordering node iteratively in term of their ranking based marginal influence spread computed according to current ranking we also prove that imrank definitely converges to a self consistent ranking starting from any initial ranking furthermore within this framework a last to first allocating strategy and a generalization of this strategy are proposed to improve the efficiency of estimating ranking based marginal influence spread for a given ranking in this way imrank achieves both remarkable efficiency and high accuracy by leveraging simultaneously the benefit of greedy algorithm and heuristic algorithm a demonstrated by extensive experiment on large scale real world social network imrank always achieves high accuracy comparable to greedy algorithm while the computational cost is reduced dramatically about time faster than other scalable heuristic 
feature tracking in video is a crucial task in computer vision usually the tracking problem is handled one feature at a time using a single feature tracker like the kanade lucas tomasi algorithm or one of it derivative while this approach work quite well when dealing with high quality video and strong feature it often falter when faced with dark and noisy video containing low quality feature we present a framework for jointly tracking a set of feature which enables sharing information between the different feature in the scene we show that our method can be employed to track feature for both rigid and non rigid motion possibly of few moving body even when some feature are occluded furthermore it can be used to significantly improve tracking result in poorly lit scene where there is a mix of good and bad feature our approach doe not require direct modeling of the structure or the motion of the scene and run in real time on a single cpu core 
we present deepwalk a novel approach for learning latent representation of vertex in a network these latent representation encode social relation in a continuous vector space which is easily exploited by statistical model deepwalk generalizes recent advancement in language modeling and unsupervised feature learning or deep learning from sequence of word to graph deepwalk us local information obtained from truncated random walk to learn latent representation by treating walk a the equivalent of sentence we demonstrate deepwalk s latent representation on several multi label network classification task for social network such a blogcatalog flickr and youtube our result show that deepwalk outperforms challenging baseline which are allowed a global view of the network especially in the presence of missing information deepwalk s representation can provide f score up to higher than competing method when labeled data is sparse in some experiment deepwalk s representation are able to outperform all baseline method while using le training data deepwalk is also scalable it is an online learning algorithm which build useful incremental result and is trivially parallelizable these quality make it suitable for a broad class of real world application such a network classification and anomaly detection 
revisioned text content is present in numerous collaboration platform on the web most notably wikis to track authorship of text token in such system ha many potential application the identification of main author for licensing reason or tracing collaborative writing pattern over time to name some in this context two main challenge arise first it is critical for such an authorship tracking system to be precise in it attribution to be reliable for further processing second it ha to run efficiently even on very large datasets such a wikipedia a a solution we propose a graph based model to represent revisioned content and an algorithm over this model that tackle both issue effectively we describe the optimal implementation and design choice when tuning it to a wiki environment we further present a gold standard of token from english wikipedia article annotated with their origin this gold standard wa created manually and confirmed by multiple independent user of a crowdsourcing platform it is the first gold standard of this kind and quality and our solution achieves an average of precision on this data set we also perform a first ever precision evaluation of the state of the art algorithm for the task exceeding it by over on average our approach outperforms the execution time of the state of the art by one order of magnitude a we demonstrate on a sample of over english wikipedia article we argue that the increased size of an optional materialization of our result by about compared to the baseline is a favorable trade off given the large advantage in runtime performance 
a topic propagating in a social network reach it tipping point if the number of user discussing it in the network exceeds a critical threshold such that a wide cascade on the topic is likely to occur in this paper we consider the task of selecting initial seed user of a topic with minimum size so that em with a guaranteed probability the number of user discussing the topic would reach a given threshold we formulate the task a an optimization problem called em seed minimization with probabilistic coverage guarantee sm pcg this problem departs from the previous study on social influence maximization or seed minimization because it considers influence coverage with em probabilistic guarantee instead of guarantee on em expected influence coverage we show that the problem is not submodular and thus is harder than previously studied problem based on submodular function optimization we provide an approximation algorithm and show that it approximates the optimal solution with both a multiplicative ratio and an additive error the multiplicative ratio is tight while the additive error would be small if influence coverage distribution of certain seed set are well concentrated for one way bipartite graph we analytically prove the concentration condition and obtain an approximation algorithm with an o log n multiplicative ratio and an o sqrt n additive error where n is the total number of node in the social graph moreover we empirically verify the concentration condition in real world network and experimentally demonstrate the effectiveness of our proposed algorithm comparing to commonly adopted benchmark algorithm 
emerging trend and product pose a challenge to modern search engine since they must adapt to the constantly changing need and interest of user for example vertical search engine such a amazon ebay walmart yelp and yahoo local provide business category hierarchy for people to navigate through million of business listing the category information also provides important ranking feature that can be used to improve search experience however category hierarchy are often manually crafted by some human expert and they are far from complete manually constructed category hierarchy cannot handle the ever changing and sometimes long tail user information need in this paper we study the problem of how to expand an existing category hierarchy for a search navigation system to accommodate the information need of user more comprehensively we propose a general framework for this task which ha three step detecting meaningful missing category modeling the category hierarchy using a hierarchical dirichlet model and predicting the optimal tree structure according to the model reorganizing the corpus using the complete category structure i e associating each webpage with the relevant category from the complete category hierarchy experimental result demonstrate that our proposed framework generates a high quality category hierarchy and significantly boost the retrieval performance 
we propose a method for human pose estimation based on deep neural network dnns the pose estimation is formulated a a dnn based regression problem towards body joint we present a cascade of such dnn regressors which result in high precision pose estimate the approach ha the advantage of reasoning about pose in a holistic fashion and ha a simple but yet powerful formulation which capitalizes on recent advance in deep learning we present a detailed empirical analysis with state ofart or better performance on four academic benchmark of diverse real world image 
we consider the problem of offline pool based active semi supervised learning on graph this problem is important when the labeled data is scarce and expensive whereas unlabeled data is easily available the data point are represented by the vertex of an undirected graph with the similarity between them captured by the edge weight given a target number of node to label the goal is to choose those node that are most informative and then predict the unknown label we propose a novel framework for this problem based on our recent result on sampling theory for graph signal a graph signal is a real valued function defined on each node of the graph a notion of frequency for such signal can be defined using the spectrum of the graph laplacian matrix the sampling theory for graph signal aim to extend the traditional nyquist shannon sampling theory by allowing u to identify the class of graph signal that can be reconstructed from their value on a subset of vertex this approach allows u to define a criterion for active learning based on sampling set selection which aim at maximizing the frequency of the signal that can be reconstructed from their sample on the set experiment show the effectiveness of our method 
online recommendation site are valuable information source that people contribute to and often use to choose restaurant however little is known about the dynamic behind participation in these online community and how the recommendation in these community are formed in this work we take a first look at online restaurant recommendation community to study what endogenous i e related to entity being reviewed and exogenous factor influence people s participation in the community and to what extent we analyze an online community corpus of k restaurant and their m associated review from to spread across every u s state we construct model for number of review and rating by community member based on several dimension of endogenous and exogenous factor we find that while endogenous factor such a restaurant attribute e g meal price service affect recommendation surprisingly exogenous factor such a demographic e g neighborhood diversity education and weather e g temperature rain snow season also exert a significant effect on review we find that many of the effect in online community can be explained using offline theory from experimental psychology our study is the first to look at exogenous factor and how it related to online online restaurant review it ha implication for designing online recommendation site and in general social medium and online community 
document level relevance judgment are a major component in the calculation of effectiveness metric collecting high quality judgment is therefore a critical step in information retrieval evaluation however the nature of and the assumption underlying relevance judgment collection have not received much attention in particular relevance judgment are typically collected for each document in isolation although user read each document in the context of other document in this work we aim to investigate the nature of relevance judgment collection we collect relevance label in both isolated and conditional setting and ask for judgment in various dimension of relevance a well a overall relevance then we compare the relevance metric based on various type of judgment with other metric of quality such a user preference our analysis illuminate how these setting for judgment collection affect the quality and the characteristic of the judgment we also find that the metric based on conditional judgment show higher correlation with user preference than isolated judgment 
we performed controlled experiment of human participant in a continuous sequence of ad auction similar to those used by internet company the goal of the research wa to understand user strategy in making bid we studied the behavior under two auction type the generalized second price gsp auction and the vickrey clarke grove vcg payment rule and manipulated also the participant knowledge condition explicitly given valuation and payoff information from which valuation could be deduced we found several interesting behavior among them are no convergence to equilibrium wa detected moreover the frequency with which participant modified their bid increased with time we can detect explicit better response behavior rather than just mixed bidding while bidder in gsp auction do strategically shade their bid they tend to bid higher than theoretically predicted by the standard vcg like equilibrium of gsp bidder who are not explicitly given their valuation but can only deduce them from their gain behave a little le precisely than those with such explicit knowledge but mostly during an initial learning phase vcg and gsp yield approximately the same high social welfare but gsp tends to give higher revenue 
we study the problem of computing similarity ranking in large scale multi categorical bipartite graph where the two side of the graph represent actor and item and the item are partitioned into an arbitrary set of category the problem ha several real world application including identifying competing advertiser and suggesting related query in an online advertising system or finding user with similar interest and suggesting content to them in these setting we are interested in computing on the fly ranking of similar actor given an actor and an arbitrary subset of category of interest two main challenge arise first the bipartite graph are huge and often lopsided e g the system might receive billion of query while presenting only million of advertiser second the sheer number of possible combination of category prevents the pre computation of the result for all of them we present a novel algorithmic framework that address both issue for the computation of several graph theoretical similarity measure including common neighbor and personalized pagerank we show how to tackle the imbalance in the graph to speed up the computation and provide efficient real time algorithm for computing ranking for an arbitrary subset of category finally we show experimentally the accuracy of our approach with real world data using both public graph and a very large dataset from google adwords 
statement about rdf statement or meta triple provide additional information about individual triple such a the source the occurring time or place or the certainty integrating such meta triple into semantic knowledge base would enable the querying and reasoning mechanism to be aware of provenance time location or certainty of triple however an efficient rdf representation for such meta knowledge of triple remains challenging the existing standard reification approach allows such meta knowledge of rdf triple to be expressed using rdf by two step the first step is representing the triple by a statement instance which ha subject predicate and object indicated separately in three different triple the second step is creating assertion about that instance a if it is a statement while reification is simple and intuitive this approach doe not have formal semantics and is not commonly used in practice a described in the rdf primer in this paper we propose a novel approach called singleton property for representing statement about statement and provide a formal semantics for it we explain how this singleton property approach fit well with the existing syntax and formal semantics of rdf and the syntax of sparql query language we also demonstrate the use of singleton property in the representation and querying of meta knowledge in two example of semantic web knowledge base yago and bkr our experiment on the bkr show that the singleton property approach give a decent performance in term of number of triple query length and query execution time compared to existing approach this approach which is also simple and intuitive can be easily adopted for representing and querying statement about statement in other knowledge base 
hundred of thousand of photograph are uploaded to the internet every minute through various social networking and photo sharing platform while some image get million of view others are completely ignored even from the same user different photograph receive different number of view this begs the question what make a photograph popular can we predict the number of view a photograph will receive even before it is uploaded these are some of the question we address in this work we investigate two key component of an image that affect it popularity namely the image content and social context using a dataset of about million image from flickr we demonstrate that we can reliably predict the normalized view count of image with a rank correlation of using both image content and social cue in this paper we show the importance of image cue such a color gradient deep learning feature and the set of object present a well a the importance of various social cue such a number of friend or number of photo uploaded that lead to high or low popularity of image 
after a decade long approval process multiple rejection and an independent review icann approved the xxx tld for inclusion in the domain name system to begin general availability on december it sponsoring registry proposed it a an expansion of the name space a well a a way to separate adult from child appropriate content many independent group including trademark holder political group and the adult entertainment industry itself were concerned that it would primarily generate value through defensive and speculative registration without actually serving a real need this paper measure the validity of these concern using data gathered from icann whois and web request we use this information to characterize each xxx domain and infer the registrant s most likely intent we find that at most of xxx domain host or redirect to potentially legitimate web content with the rest generally serving either defensive or speculative purpose indeed registrant spent roughly m up front to defend existing brand and trademark within the xxx tld and an additional m over the course of the first year additional evidence suggests that over of annual domain registration are for purely defensive purpose and do not even resolve 
the performance of web browser ha become a major bottleneck when dealing with complex webpage many calculation redundancy exist when processing similar webpage thus it is possible to cache and reuse previously calculated intermediate result to improve web browser performance significantly in this paper we propose a similarity based optimization approach to improve webpage processing performance of web browser through caching and reusing of style property calculated previously we are able to eliminate the redundancy caused by processing similar webpage from the same website we propose a tree structured architecture to store style property to facilitate efficient caching and reuse experiment on webpage of various website show that the proposed technique can speed up the webpage loading process by up to and reduce the redundant style calculation by up to for the first visit to a webpage with almost negligible overhead 
with more than million article the largest collaborative knowledge resource never sleep experiencing several article edits every second over one fifth of these article describes individual people the majority of which are still alive such article are by their nature prone to corruption and vandalism manual quality assurance by expert can barely cope with this massive amount of data can it be effectively replaced by feedback from the crowd can we provide meaningful support for quality assurance with automated text processing technique which property of the article should then play a key role in the machine learning algorithm and why in this paper we study the user perceived quality of wikipedia article based on a novel wikipedia user feedback dataset in contrast to previous work on quality assessment which mostly relied on judgement of active wikipedia author we analyze rating of ordinary wikipedia user along four quality dimension complete well written trustworthy and objective we first present an empirical analysis of the novel dataset with over million wikipedia article rating we then select a subset of biographical article and perform classification experiment to predict their quality rating along each of the dimension exploring multiple linguistic surface and network property of the rated article additionally we study the classification performance and difference for the biography of living and dead people a well a those for men and woman we demonstrate the effectiveness of our approach by the f score of and for the dimension complete well written trustworthy and objective based on the result we believe that the quality assessment of big textual data can be effectively supported by current text classification and language processing tool 
worker reliability is a longstanding issue in crowdsourcing and the automatic discovery of high quality worker is an important practical problem most previous work on this problem mainly focus on estimating the quality of each individual worker jointly with the true answer of each task however in practice for some task worker quality could be associated with some explicit characteristic of the worker such a education level major and age so the following question arises how do we automatically discover related worker attribute for a given task and further utilize the finding to improve data quality in this paper we propose a general crowd targeting framework that can automatically discover for a given task if any group of worker based on their attribute have higher quality on average and target such group if they exist for future work on the same task our crowd targeting framework is complementary to traditional worker quality estimation approach furthermore an advantage of our framework is that it is more budget efficient because we are able to target potentially good worker before they actually do the task experiment on real datasets show that the accuracy of final prediction can be improved significantly for the same budget or even le budget in some case our framework can be applied to many real word task and can be easily integrated in current crowdsourcing platform 
we study the pattern by which a user consumes the same item repeatedly over time in a wide variety domain ranging from check in at the same business location to re watch of the same video we find that recency of consumption is the strongest predictor of repeat consumption based on this we develop a model by which the item from t timesteps ago is reconsumed with a probability proportional to a function of t we study theoretical property of this model develop algorithm to learn reconsumption likelihood a a function of t and show a strong fit of the resulting inferred function via a power law with exponential cutoff we then introduce a notion of item quality show that it alone underperforms our recency based model and develop a hybrid model that predicts user choice based on a combination of recency and quality we show how the parameter of this model may be jointly estimated and show that the resulting scheme outperforms other alternative 
online service rely on unique identifier of machine to tailor offering to their user an implicit assumption is made that each machine identifier map to an individual however shared ma chine are common leading to interwoven search history and noisy signal for application such a personalized search and ad vertising we present method for attributing search activity to individual searcher using ground truth data for a sample of almost four million u s web searcher containing both machine identifier and person identifier we show that over half of the machine identifier comprise the query of multiple people we characterize variation in feature of topic time and other aspect such a the complexity of the information sought per the number of searcher on a machine and show significant difference in all measure based on these insight we develop model to accurately estimate when multiple people contribute to the log ascribed to a single machine identifier we also develop model to cluster search behavior on a machine allowing u to attribute historical data accurately and automatically assign new search activity to the correct searcher the finding have implication for the design of application such a personalized search and advertising that rely heavily on machine identifier to custom tailor their service 
in online social medium system user are not only posting consuming and resharing content but also creating new and destroying existing connection in the underlying social network while each of these two type of dynamic ha individually been studied in the past much le is known about the connection between the two how doe user information posting and seeking behavior interact with the evolution of the underlying social network structure here we study way in which network structure reacts to user posting and sharing content we examine the complete dynamic of the twitter information network where user post and reshare information while they also create and destroy connection we find that the dynamic of network structure can be characterized by steady rate of change interrupted by sudden burst information diffusion in the form of cascade of post re sharing often creates such sudden burst of new connection which significantly change user local network structure we also explore the effect of the information content on the dynamic of the network and find evidence that the appearance of new topic and real world event can lead to significant change in edge creation and deletion lastly we develop a model that quantifies the dynamic of the network and the occurrence of these burst a a function of the information spreading through the network the model can successfully predict which information diffusion event will lead to burst in network dynamic 
url shortening service facilitate the need of exchanging long url using limited space by creating compact url alias that redirect user to the original url when followed some of these service show advertisement ad to link clicking user and pay a commission of their advertising earnings to link shortening user in this paper we investigate the ecosystem of these increasingly popular ad based url shortening service even though traditional url shortening service have been thoroughly investigated in previous research we argue that due to the monetary incentive and the presence of third party advertising network ad based url shortening service and their user are exposed to more hazard than traditional shortening service by analyzing the service themselves the advertiser involved and their user we uncover a series of issue that are actively exploited by malicious advertiser and endanger the user moreover next to documenting the ongoing abuse we suggest a series of defense mechanism that service and user can adopt to protect themselves 
web search involves voluminous data stream that record million of user interaction with the search engine recently latent topic in web search data have been found to be critical for a wide range of search engine application such a search personalization and search history warehousing however the existing method usually discover latent topic from web search data in an offline and retrospective fashion hence they are increasingly ineffective in the face of the ever increasing web search data that accumulate in the format of online stream in this paper we propose a novel probabilistic topic model the web search stream model wssm which is delicately calibrated for handling two salient feature of the web search data it is in the format of stream and in massive volume we further propose an efficient parameter inference method the stream parameter inference spi to efficiently train wssm with massive web search stream based on a large scale search engine query log we conduct extensive experiment to verify the effectiveness and efficiency of wssm and spi we observe that wssm together with spi discovers latent topic from web search stream faster than the state of the art method while retaining a comparable topic modeling accuracy 
event sequence such a patient medical history or user sequence of product review trace how individual progress over time identifying common pattern or progression stage in such event sequence is a challenging task because not every individual follows the same evolutionary pattern stage may have very different length and individual may progress at different rate in this paper we develop a model based method for discovering common progression stage in general event sequence we develop a generative model in which each sequence belongs to a class and sequence from a given class pas through a common set of stage where each sequence evolves at it own rate we then develop a scalable algorithm to infer class of sequence while also segmenting each sequence into a set of stage we evaluate our method on event sequence ranging from patient medical history to online news and navigational trace from the web the evaluation show that our methodology can predict future event in a sequence while also accurately inferring meaningful progression stage and effectively grouping sequence based on common progression pattern more generally our methodology allows u to reason about how event sequence progress over time by discovering pattern and category of temporal evolution in large scale datasets of event 
sampling is a standard approach in big graph analytics the goal is to efficiently estimate the graph property by consulting a sample of the whole population a perfect sample is assumed to mirror every property of the whole population unfortunately such a perfect sample is hard to collect in complex population such a graph e g web graph social network where an underlying network connects the unit of the population therefore a good sample will be representative in the sense that graph property of interest can be estimated with a known degree of accuracy while previous work focused particularly on sampling scheme to estimate certain graph property e g triangle count much le is known for the case when we need to estimate various graph property with the same sampling scheme in this paper we propose a generic stream sampling framework for big graph analytics called graph sample and hold gsh which sample from massive graph sequentially in a single pas one edge at a time while maintaining a small state in memory we use a horvitz thompson construction in conjunction with a scheme that sample arriving edge without adjacency to previously sampled edge with probability p and hold edge with adjacency with probability q our sample and hold framework facilitates the accurate estimation of subgraph pattern by enabling the dependence of the sampling process to vary based on previous history within our framework we show how to produce statistically unbiased estimator for various graph property from the sample given that the graph analytics will run on a sample instead of the whole population the runtime complexity is kept under control moreover given that the estimator are unbiased the approximation error is also kept under control finally we test the performance of the proposed framework gsh on various type of graph showing that from a sample with k edge it produce estimate with relative error 
realistic multi agent team application often feature dynamic environment with soft deadline that penalize late execution of task this put a premium on quickly allocating task to agent but finding the optimal allocation is np hard because task must be executed sequentially by agent we propose a novel task allocation algorithm that find allocation that are fair envy free balancing the load and sharing important task between agent and efficient pareto optimal by using a fisher market based on a simplified problem model such allocation can be easily sequenced to yield high quality solution a shown empirically on problem inspired by real police log 
sparse support vector machine svm is a robust predictive model that can effectively remove noise and preserve signal like lasso it can efficiently learn a solution path based on a set of predefined parameter and therefore provides strong support for model selection sparse svm ha been successfully applied in a variety of data mining application including text mining bioinformatics and image processing the emergence of big data analysis pose new challenge for model selection with large scale data that consist of ten of million sample and feature in this paper a novel screening technique is proposed to accelerate model selection for l regularized l svm and effectively improve it scalability this technique can precisely identify inactive feature in the optimal solution of a l regularized l svm model and remove them before training the technique make use of the variational inequality and provides a closed form solution for screening inactive feature in different situation every feature that is removed by the screening technique is guaranteed to be inactive in the optimal solution therefore when l regularized l svm us the feature selected by the technique it achieves exactly the same result a when it us the full feature set because the technique can remove a large number of inactive feature it can greatly increase the efficiency of model selection for l regularized l svm experimental result on five high dimensional benchmark data set demonstrate the power of the proposed technique 
stack overflow is the most popular community based question answering cqa website for programmer on the web with m user m question and m answer stack overflow ha explicit detailed guideline on how to post question and an ebullient moderation community despite these precise communication and safeguard question posted on stack overflow can be extremely off topic or very poor in quality such question can be deleted from stack overflow at the discretion of experienced community member and moderator we present the first study of deleted question on stack overflow we divide our study into two part i characterization of deleted question over year of data ii prediction of deletion at the time of question creation our characterization study reveals multiple insight on question deletion phenomenon we find that it take substantial time to vote a question to be deleted but once voted the community take swift action we also see that question author delete their question to salvage reputation point we notice some instance of accidental deletion of good quality question but such question are voted back to be undeleted quickly we discover a pyramidal structure of question quality on stack overflow and find that deleted question lie at the bottom lowest quality of the pyramid we also build a predictive model to detect the deletion of question at the creation time we experiment with feature based on user profile community generated question content and syntactic style and report an accuracy of our finding reveal important suggestion for content quality maintenance on community based question answering website to the best of our knowledge this is the first large scale study on poor quality deleted question on stack overflow 
web search are increasingly formulated a natural language question rather than keyword query retrieving answer to such question requires a degree of understanding of user expectation an important step in this direction is to automatically infer the type of answer implied by the question e g factoid statement on a topic instruction review etc answer type taxonomy currently exist for factoid style question but not for open domain question building taxonomy for non factoid question is a harder problem since these question can come from a very broad semantic space a few attempt have been made to develop taxonomy for non factoid question but these tend to be too narrow or domain specific in this paper we address this problem by modeling the answer type a a latent variable that is learned in a data driven fashion allowing the model to be more adaptive to new domain and data set we propose approach that detect the relevance of candidate answer to a user question by jointly clustering question according to the hidden variable and modeling relevance conditioned on this hidden variable in this paper we propose new model a logistic regression mixture lrm b glocal logistic regression mixture g lrm and c mixture glocal logistic regression mixture mg lrm that automatically learn question cluster and cluster specific relevance model all three model perform better than a baseline relevance model that us explicit answer type category predicted by a supervised answer type classifier on a newsgroups dataset our model also perform better than a baseline relevance model that doe not use any answer type information on a blog dataset 
we describe quizz a gamified crowdsourcing system that simultaneously ass the knowledge of user and acquires new knowledge from them quizz operates by asking user to complete short quiz on specific topic a a user answer the quiz question quizz estimate the user s competence to acquire new knowledge quizz also incorporates question for which we do not have a known answer the answer given by competent user provide useful signal for selecting the correct answer for these question quizz actively try to identify knowledgeable user on the internet by running advertising campaign effectively leveraging the targeting capability of existing publicly available ad placement service quizz quantifies the contribution of the user using information theory and sends feedback to the advertisingsystem about each user the feedback allows the ad targeting mechanism to further optimize ad placement our experiment which involve over ten thousand user confirm that we can crowdsource knowledge curation for niche and specialized topic a the advertising network can automatically identify user with the desired expertise and interest in the given topic we present controlled experiment that examine the effect of various incentive mechanism highlighting the need for having short term reward a goal which incentivize the user to contribute finally our cost quality analysis indicates that the cost of our approach is below that of hiring worker through paid crowdsourcing platform while offering the additional advantage of giving access to billion of potential user all over the planet and being able to reach user with specialized expertise that is not typically available through existing labor marketplace 
i present a novel solution for social choice problem with missing preference information using machine learning to impute the missing data based on latent pattern in the vote that are provided my technique offer a new conceptualization of the problem with the potential for building a strong connection between social choice theory and machine learning and with excellent performance in practice 
existing method on video based action recognition are generally view dependent i e performing recognition from the same view seen in the training data we present a novel multiview spatio temporal and or graph mst aog representation for cross view action recognition i e the recognition is performed on the video from an unknown and unseen view a a compositional model mst aog compactly represents the hierarchical combinatorial structure of cross view action by explicitly modeling the geometry appearance and motion variation this paper proposes effective method to learn the structure and parameter of mst aog the inference based on mst aog enables action recognition from novel view the training of mst aog take advantage of the d human skeleton data obtained from kinect camera to avoid annotating enormous multi view video frame which is error prone and time consuming but the recognition doe not need d information and is based on d video input a new multiview action d dataset ha been created and will be released extensive experiment have demonstrated that this new action representation significantly improves the accuracy and robustness for cross view action recognition on d video 
estimating set similarity is a central problem in many computer application in this paper we introduce the odd sketch a compact binary sketch for estimating the jaccard similarity of two set the exclusive or of two sketch equal the sketch of the symmetric difference of the two set this mean that odd sketch provide a highly space efficient estimator for set of high similarity which is relevant in application such a web duplicate detection collaborative filtering and association rule learning the method extends to weighted jaccard similarity relevant e g for tf idf vector comparison we present a theoretical analysis of the quality of estimation to guarantee the reliability of odd sketch based estimator our experiment confirm this efficiency and demonstrate the efficiency of odd sketch in comparison with b bit minwise hashing scheme on association rule learning and web duplicate detection task 
topic modeling ha been proved to be an effective method for exploratory text mining it is a common assumption of most topic model that a document is generated from a mixture of topic in real world scenario individual document usually concentrate on several salient topic instead of covering a wide variety of topic a real topic also adopts a narrow range of term instead of a wide coverage of the vocabulary understanding this sparsity of information is especially important for analyzing user generated web content and social medium which are featured a extremely short post and condensed discussion in this paper we propose a dual sparse topic model that address the sparsity in both the topic mixture and the word usage by applying a spike and slab prior to decouple the sparsity and smoothness of the document topic and topic word distribution we allow individual document to select a few focused topic and a topic to select focused term respectively experiment on different genre of large corpus demonstrate that the dual sparse topic model outperforms both classical topic model and existing sparsity enhanced topic model this improvement is especially notable on collection of short document 
monitoring web browsing behavior ha benefited many data mining application such a top k discovery and anomaly detection however releasing private user data to the greater public would concern web user about their privacy especially after the incident of aol search log release where anonymization wa not correctly done in this paper we adopt differential privacy a strong provable privacy definition and show that differentially private aggregate of web browsing activity can be released in real time while preserving the utility of shared data our proposed algorithm utilize the rich correlation of the time series of aggregated data and adopt a state space approach to estimate the underlying true aggregate from the perturbed value by the differential privacy mechanism we evaluate our algorithm with real world web browsing data utility evaluation with three metric demonstrate that the quality of the private released data by our solution closely resembles that of the original unperturbed aggregate 
linked open data lod comprises an unprecedented volume of structured data on the web however these datasets are of varying quality ranging from extensively curated datasets to crowdsourced or extracted data of often relatively low quality we present a methodology for test driven quality assessment of linked data which is inspired by test driven software development we argue that vocabulary ontology and knowledge base should be accompanied by a number of test case which help to ensure a basic level of quality we present a methodology for assessing the quality of linked data resource based on a formalization of bad smell and data quality problem our formalization employ sparql query template which are instantiated into concrete quality test case query based on an extensive survey we compile a comprehensive library of data quality test case pattern we perform automatic test case instantiation based on schema constraint or semi automatically enriched schema and allow the user to generate specific test case instantiation that are applicable to a schema or dataset we provide an extensive evaluation of five lod datasets manual test case instantiation for five schema and automatic test case instantiation for all available schema registered with linked open vocabulary lov one of the main advantage of our approach is that domain specific semantics can be encoded in the data quality test case thus being able to discover data quality problem beyond conventional quality heuristic 
in the paper we consider the problem of link prediction in time evolving graph we assume that certain graph feature such a the node degree follow a vector autoregressive var model and we propose to use this information to improve the accuracy of prediction our strategy involves a joint optimization procedure over the space of adjacency matrix and var matrix on the adjacency matrix it take into account both sparsity and low rank property and on the var it encodes the sparsity the analysis involves oracle inequality that illustrate the trade offs in the choice of smoothing parameter when modeling the joint effect of sparsity and low rank the estimate is computed efficiently using proximal method and evaluated through numerical experiment 
in recent year location aware music recommendation is increasing in popularity a more and more user consume music on the move in this demonstration we present an intelligent system called just for me to facilitate accurate music recommendation based on where user present our system is developed based on a novel probabilistic generative model which can effectively integrate the location context and global music popularity trend this approach allows u to gain more comprehensive modeling on user preference and thus significantly enhances the music recommendation performance 
we propose two dynamic indexing scheme for shortest path and distance query on large time evolving graph which are useful in a wide range of important application such a real time network aware search and network evolution analysis to the best of our knowledge these method are the first practical exact indexing method to efficiently process distance query and dynamic graph update we first propose a dynamic indexing scheme for query on the last snapshot the scalability and efficiency of it offline indexing algorithm and query algorithm are competitive even with previous static method meanwhile the method is dynamic that is it can incrementally update index a the graph change over time then we further design another dynamic indexing scheme that can also answer two kind of historical query with regard to not only the latest snapshot but also previous snapshot through extensive experiment on real and synthetic evolving network we show the scalability and efficiency of our method specifically they can construct index from large graph with million of vertex answer query in microsecond and update index in millisecond 
this paper address the problem of extracting accurate label from crowdsourced datasets a key challenge in crowdsourcing prior work ha focused on modeling the reliability of individual worker for instance by way of confusion matrix and using these latent trait to estimate the true label more accurately however this strategy becomes ineffective when there are too few label per worker to reliably estimate their quality to mitigate this issue we propose a novel community based bayesian label aggregation model communitybcc which assumes that crowd worker conform to a few different type where each type represents a group of worker with similar confusion matrix we assume that each worker belongs to a certain community where the worker s confusion matrix is similar to a perturbation of the community s confusion matrix our model can then learn a set of key latent feature i the confusion matrix of each community ii the community membership of each user and iii the aggregated label of each item we compare the performance of our model against established aggregation method on a number of large scale real world crowdsourcing datasets our experimental result show that our communitybcc model consistently outperforms state of the art label aggregation method requiring on average le data to pas the accuracy mark 
compact and discriminative visual codebooks are preferred in many visual recognition task in the literature a number of work have taken the approach of hierarchically merging visual word of an initial large sized codebook but implemented this approach with different merging criterion in this work we propose a single probabilistic framework to unify these merging criterion by identifying two key factor the function used to model class conditional distribution and the method used to estimate the distribution parameter more importantly by adopting new distribution function and or parameter estimation method our framework can readily produce a spectrum of novel merging criterion three of them are specifically focused in this work in the first criterion we adopt the multinomial distribution with bayesian method in the second criterion we integrate gaussian distribution with maximum likelihood parameter estimation in the third criterion which show the best merging performance we propose a max margin based parameter estimation method and apply it with multinomial distribution extensive experimental study is conducted to systematically analyse the performance of the above three criterion and compare them with existing one a demonstrated the best criterion obtained in our framework achieves the overall best merging performance among the comparable merging criterion developed in the literature 
a they compete for developer mobile app ecosystem have been exposing a growing number of apis through their software development kit many of these apis involve accessing sensitive functionality and or user data and require approval by user android for instance allows developer to select from over possible permission expecting user to review and possibly adjust setting related to these permission ha proven unrealistic in this paper we report on the result of a study analyzing people s privacy preference when it come to granting permission to different mobile apps our result suggest that while people s mobile app privacy preference are diverse a relatively small number of profile can be identified that offer the promise of significantly simplifying the decision mobile user have to make specifically our result are based on the analysis of setting of million smartphone user of a mobile security and privacy platform the platform relies on a rooted version of android where user are allowed to choose between granting denying or requesting to be dynamically prompted when it come to granting different android permission to mobile apps they have downloaded 
in bag of word bow based image retrieval the sift visual word ha a low discriminative power so false positive match occur prevalently apart from the information loss during quantization another cause is that the sift feature only describes the local gradient distribution to address this problem this paper proposes a coupled multi index c mi framework to perform feature fusion at indexing level basically complementary feature are coupled into a multi dimensional inverted index each dimension of c mi corresponds to one kind of feature and the retrieval process vote for image similar in both sift and other feature space specifically we exploit the fusion of local color feature into c mi while the precision of visual match is greatly enhanced we adopt multiple assignment to improve recall the joint cooperation of sift and color feature significantly reduces the impact of false positive match extensive experiment on several benchmark datasets demonstrate that c mi improves the retrieval accuracy significantly while consuming only half of the query time compared to the baseline importantly we show that c mi is well complementary to many prior technique assembling these method we have obtained an map of and n s score of on holiday and ukbench datasets respectively which compare favorably with the state of the art 
how doe one develop a new online community that is highly engaging to each user and promotes social interaction a number of website offer friend finding feature that help user bootstrap social network on the website by copying link from an established network like facebook or twitter this paper quantifies the extent to which such social bootstrapping is effective in enhancing a social experience of the website first we develop a stylised analytical model that suggests that copying tends to produce a giant connected component i e a connected community quickly and preserve property such a reciprocity and clustering up to a linear multiplicative factor second we use data from two website pinterest and last fm to empirically compare the subgraph of link copied from facebook to link created natively we find that the copied subgraph ha a giant component higher reciprocity and clustering and confirm that the copied connection see higher social interaction however the need for copying diminishes a user become more active and influential such user tend to create link natively on the website to user who are more similar to them than their facebook friend our finding give new insight into understanding how bootstrapping from established social network can help engage new user by enhancing social interactivity 
clustering web item i e web resource like video image into semantic group benefit many application such a organizing item generating meaningful tag and improving web search in this paper we systematically investigate how user generated comment can be used to improve the clustering of web item in our preliminary study of last fm we find that the two data source extracted from user comment the textual comment and the commenting user provide complementary evidence to the item intrinsic feature these source have varying level of quality but we importantly we find that incorporating all three source improves clustering to accommodate such quality imbalance we invoke multi view clustering in which each data source represents a view aiming to best leverage the utility of different view to combine multiple view under a principled framework we propose conmf co regularized non negative matrix factorization which extends nmf for multi view clustering by jointly factorizing the multiple matrix through co regularization under our conmf framework we devise two paradigm pair wise conmf and cluster wise conmf and propose iterative algorithm for their joint factorization experimental result on last fm and yelp datasets demonstrate the effectiveness of our solution in last fm conmf better k mean with a statistically significant f increase of while achieving comparable performance with the state of the art multi view clustering method cosc co regularized spectral clustering on a yelp dataset conmf outperforms the best baseline cosc with a statistically significant performance gain of 
although criticized for some of it limitation modularity remains a standard measure for analyzing social network quantifying the statistical surprise in the arrangement of the edge of the network ha led to simple and powerful algorithm however relying solely on the distribution of edge instead of more complex structure such a path limit the extent of modularity indeed recent study have shown restriction of optimizing modularity for instance it resolution limit we introduce here a novel formal and well defined modularity measure based on random walk we show how this modularity can be computed from path induced by the graph instead of the traditionally used edge we argue that by computing modularity on path instead of edge more informative feature can be extracted from the network we verify this hypothesis on a semi supervised classification procedure of the node in the network where we show that under the same setting the feature of the random walk modularity help to classify better than the feature of the usual modularity additionally the proposed approach outperforms the classical label propagation procedure on two data set of labeled social network 
we propose a method for inferring human attribute such a gender hair style clothes style expression action from image of people under large variation of viewpoint pose appearance articulation and occlusion convolutional neural net cnn have been shown to perform very well on large scale object recognition problem in the context of attribute classification however the signal is often subtle and it may cover only a small part of the image while the image is dominated by the effect of pose and viewpoint discounting for pose variation would require training on very large labeled datasets which are not presently available part based model such a poselets and dpm have been shown to perform well for this problem but they are limited by shallow low level feature we propose a new method which combine part based model and deep learning by training pose normalized cnns we show substantial improvement v state of the art method on challenging attribute classification task in unconstrained setting experiment confirm that our method outperforms both the best part based method on this problem and conventional cnns trained on the full bounding box of the person 
one problem facing player of competitive game is negative or toxic behavior league of legend the largest esport game us a crowdsourcing platform called the tribunal to judge whether a reported toxic player should be punished or not the tribunal is a two stage system requiring report from those player that directly observe toxic behavior and human expert that review aggregated report while this system ha successfully dealt with the vague nature of toxic behavior by majority rule based on many vote it naturally requires tremendous cost time and human effort in this paper we propose a supervised learning approach for predicting crowdsourced decision on toxic behavior with large scale labeled data collection over million user report involved in million toxic player and corresponding crowdsourced decision our result show good performance in detecting overwhelmingly majority case and predicting crowdsourced decision on them we demonstrate good portability of our classifier across region finally we estimate the practical implication of our approach potential cost saving and victim protection 
a weakly supervised method acquires fine grained class label that do not occur verbatim in the input data or underlying text collection the method generates more specific class label gold mining company listed on the toronto stock exchange that capture the semantics of the underlying class out of pair of input class label company listed on the toronto stock exchange gold mining company available for an instance golden star resource when applied to wikipedia article and their category the method generates new category for existing article and expands existing category with additional article 
user attribute such a occupation education and location are important for many application in this paper we study the problem of profiling user attribute in social network to capture the correlation between attribute and social connection we present a new insight that social connection are discriminatively correlated with attribute via a hidden factor relationship type for example a user s colleague are more likely to share the same employer with him than other friend based on the insight we propose to co profile user attribute and relationship type of their connection to achieve co profiling we develop an efficient algorithm based on an optimization framework our algorithm capture our insight effectively it iteratively profile attribute by propagation via certain type of connection and profile type of connection based on attribute and the network structure we conduct extensive experiment to evaluate our algorithm the result show that our algorithm profile various attribute accurately which improves the state of the art method by 
named entity recognition ner play an important role in a variety of online information management task including text categorization document clustering and faceted search while recent ner system can achieve near human performance on certain document like news article they still remain highly domain specific and thus cannot effectively identify entity such a original technical concept in scientific document in this work we propose novel approach for ner on distinctive document collection such a scientific article based on n gram inspection and classification we design and evaluate several entity recognition feature ranging from well known part of speech tag to n gram co location statistic and decision tree to classify candidate in addition we show how the use of external knowledge base either specific like dblp or generic like dbpedia can be leveraged to improve the effectiveness of ner for idiosyncratic collection we evaluate our system on two test collection created from a set of computer science and physic paper and compare it against state of the art supervised method experimental result show that a careful combination of the feature we propose yield up to ner accuracy over scientific collection and substantially outperforms state of the art approach such a those based on maximum entropy 
a novel method to induce wide coverage combinatory categorial grammar ccg resource for japanese is proposed in this article for some language including english the availability of large annotated corpus and the development of data based induction of lexicalized grammar have enabled deep parsing i e parsing based on lexicalized grammar however deep parsing for japanese ha not been widely studied this is mainly because most japanese syntactic resource are represented in chunk based dependency structure while previous method for inducing grammar are dependent on tree corpus to translate syntactic information presented in chunk based dependency to phrase structure a accurately a possible integration of annotation from multiple dependency based corpus is proposed our method first integrates dependency structure and predicate argument information and convert them into phrase structure tree the tree are then transformed into ccg derivation in a similar way to previously proposed method the quality of the conversion is empirically evaluated in term of the coverage of the obtained ccg lexicon and the accuracy of the parsing with the grammar while the transforming process used in this study is specialized for japanese the framework of our method would be applicable to other language for which dependency based analysis ha been regarded a more appropriate than phrase structure based analysis due to morphosyntactic feature 
deep convolutional neural network have recently achieved state of the art performance on a number of image recognition benchmark including the imagenet large scale visual recognition challenge ilsvrc the winning model on the localization sub task wa a network that predicts a single bounding box and a confidence score for each object category in the image such a model capture the whole image context around the object but cannot handle multiple instance of the same object in the image without naively replicating the number of output for each instance in this work we propose a saliency inspired neural network model for detection which predicts a set of class agnostic bounding box along with a single score for each box corresponding to it likelihood of containing any object of interest the model naturally handle a variable number of instance for each class and allows for cross class generalization at the highest level of the network we are able to obtain competitive recognition performance on voc and ilsvrc while using only the top few predicted location in each image and a small number of neural network evaluation 
we propose a method for knowledge transfer between semantically related class in imagenet by transferring knowledge from the image that have bounding box annotation to the others our method is capable of automatically populating imagenet with many more bounding box the underlying assumption that object from semantically related class look alike is formalized in our novel associative embedding ae representation ae recovers the latent low dimensional space of appearance variation among image window the dimension of ae space tend to correspond to aspect of window appearance e g side view close up background we model the overlap of a window with an object using gaussian process gp regression which spread annotation smoothly through ae space the probabilistic nature of gp allows our method to perform self assessment i e assigning a quality estimate to it own output it enables trading off the amount of returned annotation for their quality a large scale experiment on class and million image demonstrates that our method outperforms state of the art method and baseline for object localization using self assessment we can automatically return bounding box annotation for of all image with high localization accuracy i e average overlap with ground truth 
eli pariser coined the term filter bubble to describe the potential for online personalization to effectively isolate people from a diversity of viewpoint or content online recommender system built on algorithm that attempt to predict which item user will most enjoy consuming are one family of technology that potentially suffers from this effect because recommender system have become so prevalent it is important to investigate their impact on user in these term this paper examines the longitudinal impact of a collaborative filtering based recommender system on user to the best of our knowledge it is the first paper to measure the filter bubble effect in term of content diversity at the individual level we contribute a novel metric to measure content diversity based on information encoded in user generated tag and we present a new set of method to examine the temporal effect of recommender system on the user experience we do find that recommender system expose user to a slightly narrowing set of item over time however we also see evidence that user who actually consume the item recommended to them experience lessened narrowing effect and rate item more positively 
we analyze symmetric protocol to rationally coordinate on an asymmetric efficient allocation in an infinitely repeated n agent c resource allocation problem where the resource are all homogeneous bhaskar proposed one way to achieve this in agent resource game agent start by symmetrically randomizing their action and a soon a they each choose different action they start to follow a potentially asymmetric convention that prescribes their action from then on we extend the concept of convention to the general case of infinitely repeated resource allocation game with n agent and c resource we show that for any convention there exists a symmetric subgameperfect equilibrium which implement it we present two convention bourgeois where agent stick to the first allocation and market where agent pay for the use of resource and observe a global coordination signal which allows them to alternate between different allocation we define price of anonymity of a convention a a ratio between the maximum social payoff of any asymmetric strategy profile and the expected social payoff of the subgame perfect equilibrium which implement the convention we show that while the price of anonymity of the bourgeois convention is infinite the market convention decrease this price by reducing the conflict between the agent 
over the past few year massive amount of world knowledge have been accumulated in publicly available knowledge base such a freebase nell and yago yet despite their seemingly huge size these knowledge base are greatly incomplete for example over of people included in freebase have no known place of birth and have no known ethnicity in this paper we propose a way to leverage existing web search based question answering technology to fill in the gap in knowledge base in a targeted way in particular for each entity attribute we learn the best set of query to ask such that the answer snippet returned by the search engine are most likely to contain the correct value for that attribute for example if we want to find frank zappa s mother we could ask the query who is the mother of frank zappa however this is likely to return the mother of invention which wa the name of his band our system learns that it should in this case add disambiguating term such a zappa s place of birth in order to make it more likely that the search result contain snippet mentioning his mother our system also learns how many different query to ask for each attribute since in some case asking too many can hurt accuracy by introducing false positive we discus how to aggregate candidate answer across multiple query ultimately returning probabilistic prediction for possible value for each attribute finally we evaluate our system and show that it is able to extract a large number of fact with high confidence 
together with the sign positive or negative and strength strong or weak the directionality is also an important property of social tie though usually ignored in undirected social network for it invisibility however we believe most social tie are natively directed and the awareness of directionality can improve our understanding about the network structure and further benefit social network analysis and mining task thus it s appealing to study whether there exist interesting pattern about directionality in social network and whether we can learn the direction for undirected network based on these pattern in this study we engage in the investigation of directionality pattern on real world directed social network and summarize our finding using four consistency hypothesis based on these hypothesis we propose redirect an optimization framework which make it possible to infer the hidden direction of undirected social tie based on the network topology only this general framework can incorporate various predictive model under specific scenario furthermore we show how to improve redirect by introducing semi self supervision in the framework and how to construct the self labeled training data using simple but effective heuristic experimental result show that even without external information our approach can recover the direction of network effectively moreover we re quite surprising to find that redirect can benefit predictive task remarkably with a case study of link prediction in experiment the redirected network inferred using redirect are proven much more informative than original undirected one and can improve the prediction performance significantly it convinces u that redirect can be a beneficial general data preprocess tool for various network analysis and mining task by uncovering the hidden direction of undirected social network 
conformity influence is the inclination of a person to be influenced by others in this paper we study how the conformity tendency of a person change with her role a defined by her structural property in a social network we first formalize conformity influence using a utility function based on the conformity theory from social psychology and then propose a proba bilistic graphical model referred to a role conformity model rcm for modeling the role aware conformity influence between user by incorporating the utility function we apply the proposed rcm to several academic research network and discover that people with higher degree and lower clustering coefficient are more likely to conform to others we also evaluate rcm through the task of word usage prediction in academic publication and show significant improvement over baseline model 
estimating similarity between vertex is a fundamental issue in network analysis across various domain such a social network and biological network method based on common neighbor and structural context have received much attention however both category of method are difficult to scale up to handle large network with billion of node in this paper we propose a sampling method that provably and accurately estimate the similarity between vertex the algorithm is based on a novel idea of random path and an extended method is also presented to enhance the structural similarity when two vertex are completely disconnected we provide theoretical proof for the error bound and confidence of the proposed algorithm we perform extensive empirical study and show that our algorithm can obtain top k similar vertex for any vertex in a network approximately faster than state of the art method we also use identity resolution and structural hole spanner finding two important application in social network to evaluate the accuracy of the estimated similarity our experimental result demonstrate that the proposed algorithm achieves clearly better performance than several alternative method 
citation recommendation is an interesting but challenging research problem most existing study assume that all paper adopt the same criterion and follow the same behavioral pattern in deciding relevance and authority of a paper however in reality paper have distinct citation behavioral pattern when looking for different reference depending on paper content author and target venue in this study we investigate the problem in the context of heterogeneous bibliographic network and propose a novel cluster based citation recommendation framework called cluscite which explores the principle that citation tend to be softly clustered into interest group based on multiple type of relationship in the network therefore we predict each query s citation based on related interest group each having it own model for paper authority and relevance specifically we learn group membership for object and the significance of relevance feature for each interest group while also propagating relative authority between object by solving a joint optimization problem experiment on both dblp and pubmed datasets demonstrate the power of the proposed approach with improvement in recall and growth in mrr over the best performing baseline 
the speed of optical flow algorithm is crucial for many video editing task such a slow motion synthesis selection propagation tone adjustment propagation and so on variational coarse to fine optical flow algorithm can generally produce high quality result but cannot fulfil the speed requirement of many practical application besides large motion in real world video also pose a difficult problem to coarse to fine variational approach we in this paper present a fast optical flow algorithm that can handle large displacement motion our algorithm is inspired by recent success of local method in visual correspondence searching a well a approximate nearest neighbor field algorithm the main novelty is a fast randomized edge preserving approximate nearest neighbor field algorithm which propagates self similarity pattern in addition to offset experimental result on public optical flow benchmark show that our method is significantly faster than state of the art method without compromising on quality especially when scene contain large motion finally we show some demo application by applying our technique into real world video editing task 
we introduce a simple modification of local image descriptor such a sift based on pooling gradient orientation across different domain size in addition to spatial location the resulting descriptor which we call dsp sift outperforms other method in wide baseline matching benchmark including those based on convolutional neural network despite having the same dimension of sift and requiring no training 
in this paper we provide an extensive evaluation of fixation prediction and salient object segmentation algorithm a well a statistic of major datasets our analysis identifies serious design flaw of existing salient object benchmark called the dataset design bias by over emphasising the stereotypical concept of saliency the dataset design bias doe not only create the discomforting disconnection between fixation and salient object segmentation but also misleads the algorithm designing based on our analysis we propose a new high quality dataset that offer both fixation and salient object segmentation ground truth with fixation and salient object being presented simultaneously we are able to bridge the gap between fixation and salient object and propose a novel method for salient object segmentation finally we report significant benchmark progress on existing datasets of segmenting salient object 
in this paper we propose novel method for completion from limited sample and de noising of multilinear tensor data and a an application consider d and d color video data completion and de noising we exploit the recently proposed tensor singular value decomposition t svd based on t svd the notion of multilinear rank and a related tensor nuclear norm wa proposed in to characterize informational and structural complexity of multilinear data we first show that video with linear camera motion can be represented more efficiently using t svd compared to the approach based on vectorizing or flattening of the tensor since efficiency in representation implies efficiency in recovery we outline a tensor nuclear norm penalized algorithm for video completion from missing entry application of the proposed algorithm for video recovery from missing entry is shown to yield a superior performance over existing method we also consider the problem of tensor robust principal component analysis pca for de noising d video data from sparse random corruption we show superior performance of our method compared to the matrix robust pca adapted to this setting a proposed in 
state of the art patch based image representation involve a pooling operation that aggregate statistic computed from local descriptor standard pooling operation include sumand max pooling sum pooling lack discriminability because the resulting representation is strongly influenced by frequent yet often uninformative descriptor but only weakly influenced by rare yet potentially highly informative one max pooling equalizes the influence of frequent and rare descriptorsbut is only applicable to representation that rely on count statistic such a the bag of visual word bov and it softand sparse coding extension we propose a novel pooling mechanism that achieves the same effect a max pooling but is applicable beyond the bov and especially to the state of the art fisher vector hence the name generalized max pooling gmp it involves equalizing the similarity between each patch and the pooled representation which is shown to be equivalent to re weighting the per patch statistic we show on five public image classification benchmark that the proposedgmp can lead to significant performance gain with respect toheuristic alternative 
dictionary learning algorithm or supervised deep convolution network have considerably improved the efficiency of predefined feature representation such a sift we introduce a deep scattering convolution network with complex wavelet filter over spatial and angular variable this representation brings an important improvement to result previously obtained with predefined feature over object image database such a caltech and cifar the resulting accuracy is comparable to result obtained with unsupervised deep learning and dictionary based representation this show that refining image representation by using geometric prior is a promising direction to improve image classification and it understanding 
human pose estimation is a key step to action recognition we propose a method of estimating d human pose from a single image which work in conjunction with an existing d pose joint detector d pose estimation is challenging because multiple d pose may correspond to the same d pose after projection due to the lack of depth information moreover current d pose estimator are usually inaccurate which may cause error in the d estimation we address the challenge in three way i we represent a d pose a a linear combination of a sparse set of base learned from d human skeleton ii we enforce limb length constraint to eliminate anthropomorphically implausible skeleton iii we estimate a d pose by minimizing the norm error between the projection of the d pose and the corresponding d detection the norm loss term is robust to inaccurate d joint estimation we use the alternating direction method adm to solve the optimization problem efficiently our approach outperforms the state of the art on three benchmark datasets 
to train good supervised and semi supervised object classifier it is critical that we not waste the time of the human expert who are providing the training label existing active learning strategy can have uneven performance being efficient on some datasets but wasteful on others or inconsistent just between run on the same dataset we propose perplexity based graph construction and a new hierarchical subquery evaluation algorithm to combat this variability and to release the potential of expected error reduction under some specific circumstance expected error reduction ha been one of the strongest performing informativeness criterion for active learning until now it ha also been prohibitively costly to compute for sizeable datasets we demonstrate our highly practical algorithm comparing it to other active learning measure on classification datasets that vary in sparsity dimensionality and size our algorithm is consistent over multiple run and achieves high accuracy while querying the human expert for label at a frequency that match their desired time budget 
we present a simple efficient model for learning boundary detection based on a random forest classifier our approach combine efficient clustering of training example based on a simple partitioning of the space of local edge orientation and scale dependent calibration of individual tree output probability prior to multiscale combination the resulting model outperforms published result on the challenging bsds boundary detection benchmark further on large datasets our model requires substantially le memory for training and speed up training time by a factor of over the structured forest model 
we introduce a multi scale framework for low level vision where the goal is estimating physical scene value from image data such a depth from stereo image pair the framework us a dense overlapping set of image region at multiple scale and a local model such a a slanted plane model for stereo disparity that is expected to be valid piecewise across the visual field estimation is cast a optimization over a dichotomous mixture of variable simultaneously determining which region are inliers with respect to the local model binary variable and the correct co ordinate in the local model space for each inlying region continuous variable when the region are organized into a multi scale hierarchy optimization can occur in an efficient and parallel architecture where distributed computational unit iteratively perform calculation and share information through sparse connection between parent and child the framework performs well on a standard benchmark for binocular stereo and it produce a distributional scene representation that is appropriate for combining with higher level reasoning and other low level cue 
the notion of creativity a opposed to related concept such a beauty or interestingness ha not been studied from the perspective of automatic analysis of multimedia content meanwhile short online video shared on social medium platform or micro video have arisen a a new medium for creative expression in this paper we study creative micro video in an effort to understand the feature that make a video creative and to address the problem of automatic detection of creative content defining creative video a those that are novel and have aesthetic value we conduct a crowdsourcing experiment to create a dataset of over micro video labelled a creative and non creative we propose a set of computational feature that we map to the component of our definition of creativity and conduct an analysis to determine which of these feature correlate most with creative video finally we evaluate a supervised approach to automatically detect creative video with promising result showing that it is necessary to model both aesthetic value and novelty to achieve optimal classification accuracy 
the present article serf a an erratum to our paper of the same title which wa presented and published in the kdd conference in that article we claimed falsely that the objective function defined in section is non monotone submodular we are deeply indebted to debmalya mandal jean pouget abadie and yaron singer for bringing to our attention a counter example to that claim subsequent to becoming aware of the counter example we have shown that the objective function is in fact np hard to approximate to within a factor of o n for any in an attempt to fix the record the present article combine the problem motivation model and experimental result section from the original incorrect article with the new hardness result we would like reader to only cite and use this version which will remain an unpublished note instead of the incorrect conference version 
we tackle the problem of optimizing over all possible positive definite radial kernel on riemannian manifold for classification kernel method on riemannian manifold have recently become increasingly popular in computer vision however the number of known positive definite kernel on manifold remain very limited furthermore most kernel typically depend on at least one parameter that need to be tuned for the problem at hand a poor choice of kernel or of parameter value may yield significant performance drop off here we show that positive definite radial kernel on the unit n sphere the grassmann manifold and kendall s shape manifold can be expressed in a simple form whose parameter can be automatically optimized within a support vector machine framework we demonstrate the benefit of our kernel learning algorithm on object face action and shape recognition 
this paper aim at developing an integrated system of clothing co parsing in order to jointly parse a set of clothing image unsegmented but annotated with tag into semantic configuration we propose a data driven framework consisting of two phase of inference the first phase referred a image co segmentation iterates to extract consistent region on image and jointly refines the region over all image by employing the exemplar svm esvm technique in the second phase i e region colabeling we construct a multi image graphical model by taking the segmented region a vertex and incorporate several context of clothing configuration e g item location and mutual interaction the joint label assignment can be solved using the efficient graph cut algorithm in addition to evaluate our framework on the fashionista dataset we construct a dataset called ccp consisting of high resolution street fashion photo to demonstrate the performance of our system we achieve segmentation accuracy and recognition rate on the fashionista and the ccp datasets respectively which are superior compared with state of the art method 
detecting object becomes difficult when we need to deal with large shape deformation occlusion and low resolution we propose a novel approach to i handle large deformation and partial occlusion in animal a example of highly deformable object ii describe them in term of body part and iii detect them when their body part are hard to detect e g animal depicted at low resolution we represent the holistic object and body part separately and use a fully connected model to arrange template for the holistic object and body part our model automatically decouples the holistic object or body part from the model when they are hard to detect this enables u to represent a large number of holistic object and body part combination to better deal with different detectability pattern caused by deformation occlusion and or low resolution we apply our method to the six animal category in the pascal voc dataset and show that our method significantly improves state of the art by ap and provides a richer representation for object during training we use annotation for body part e g head torso etc making use of a new dataset of fully annotated object part for pascal voc which provides a mask for each part 
we study the interplay between a dynamic process and the structure of the network on which it is defined specifically we examine the impact of this interaction on the quality measure of network cluster and node centrality this enables u to effectively identify network community and important node participating in the dynamic a the first step towards this objective we introduce an umbrella framework for defining and characterizing an ensemble of dynamic process on a network this framework generalizes the traditional laplacian framework to continuous time biased random walk and also allows u to model some epidemic process over a network for each dynamic process in our framework we can define a function that measure the quality of every subset of node a a potential cluster or community with respect to this process on a given network this subset quality function generalizes the traditional conductance measure for graph partitioning we partially justify our choice of the quality function by showing that the classic cheeger s inequality which relates the conductance of the best cluster in a network with a spectral quantity of it laplacian matrix can be extended from the laplacian conductance setting to this more general setting 
deformable part model and convolutional network each have achieved notable performance in object detection yet these two approach find their strength in complementary area dpms are well versed in object composition modeling fine grained spatial relationship between part likewise convnets are adept at producing powerful image feature having been discriminatively trained directly on the pixel in this paper we propose a new model that combine these two approach obtaining the advantage of each we train this model using a new structured loss function that considers all bounding box within an image rather than isolated object instance this enables the non maximal suppression nm operation previously treated a a separate post processing stage to be integrated into the model this allows for discriminative training of our combined convnet dpm nm model in end to end fashion we evaluate our system on pascal voc and datasets achieving competitive result on both benchmark 
collaborative filtering with implicit feedback ha been steadily receiving more attention since the abundant implicit feedback are more easily collected while explicit feedback are not necessarily always available several recent work address this problem well utilizing pairwise ranking method with a fundamental assumption that a user prefers item with positive feedback to the item without observed feedback which also implies that the item without observed feedback are treated equally without distinction however user have their own preference on different item with different degree which can be modeled into a ranking relationship in this paper we exploit this prior information of a user s preference from the nearest neighbor set by the neighbor implicit feedback which can split item into different item group with specific ranking relation we propose a novel prigp personalized ranking with item group based pairwise preference learning algorithm to integrate item based pairwise preference and item group based pairwise preference into the same framework experimental result on three real world datasets demonstrate the proposed method outperforms the competitive baseline on several ranking oriented evaluation metric 
for many language that use non roman based indigenous script e g arabic greek and indic language one can often find a large amount of user generated transliterated content on the web in the roman script such content creates a monolingual or multi lingual space with more than one script which we refer to a the mixed script space ir in the mixed script space is challenging because query written in either the native or the roman script need to be matched to the document written in both the script moreover transliterated content feature extensive spelling variation in this paper we formally introduce the concept of mixed script ir and through analysis of the query log of bing search engine estimate the prevalence and thereby establish the importance of this problem we also give a principled solution to handle the mixed script term matching and spelling variation where the term across the script are modelled jointly in a deep learning architecture and can be compared in a low dimensional abstract space we present an extensive empirical analysis of the proposed method along with the evaluation result in an ad hoc retrieval setting of mixed script ir where the proposed method achieves significantly better result increase in mrr and increase in map compared to other state of the art baseline 
cross medium hashing which conduct cross medium retrieval by embedding data from different modality into a common low dimensional hamming space ha attracted intensive attention in recent year the existing cross medium hashing approach only aim at learning hash function to preserve the intra modality and inter modality correlation but do not directly capture the underlying semantic information of the multi modal data we propose a discriminative coupled dictionary hashing dcdh method in this paper in dcdh the coupled dictionary for each modality is learned with side information e g category a a result the coupled dictionary not only preserve the intra similarity and inter correlation among multi modal data but also contain dictionary atom that are semantically discriminative i e the data from the same category is reconstructed by the similar dictionary atom to perform fast cross medium retrieval we learn hash function which map data from the dictionary space to a low dimensional hamming space besides we conjecture that a balanced representation is crucial in cross medium retrieval we introduce multi view feature on the relatively weak modality into dcdh and extend it to multi view dcdh mv dcdh in order to enhance their representation capability the experiment on two real world data set show that our dcdh and mv dcdh outperform the state of the art method significantly on cross medium retrieval 
passage based retrieval model have been studied for some time and have been shown to have some benefit for document ranking finding passage that are not only topically relevant but are also answer to the user question would have a significant impact in application such a mobile search to develop model for answer passage retrieval we need to have appropriate test collection and evaluation measure making annotation at the passage level is however expensive and can have poor coverage in this paper we describe the advantage of document summarization measure for evaluating answer passage retrieval and show that these measure have high correlation with existing measure and human judgment 
we study time critical search where user have urgent information need in the context of an acute problem a example user may need to know how to stem a severe bleed help a baby who is choking on a foreign object or respond to an epileptic seizure while time critical situation and action have been studied in the realm of decision support system little ha been done with time critical search and retrieval and little direct support is offered by search system critical challenge with time critical search include accurately inferring when user have urgent need and providing relevant information that can be understood and acted upon quickly we leverage survey and search log data from a large mobile search provider to a characterize the use of search engine for time critical situation and b develop predictive model to accurately predict urgent information need given a query and a diverse set of feature spanning topical temporal behavioral and geospatial attribute the method and finding highlight opportunity for extending search and retrieval to consider the urgency of query 
text based social medium channel such a twitter produce torrent of opinionated data about the most diverse topic and entity the analysis of such data aka sentiment analysis is quickly becoming a key feature in recommender system and search engine a prominent approach to sentiment analysis is based on the application of classification technique that is content is classified according to the attitude of the writer a major challenge however is that twitter follows the data stream model and thus classifier must operate with limited resource including labeled data and time for building classification model also challenging is the fact that sentiment distribution may change a the stream evolves in this paper we address these challenge by proposing algorithm that select relevant training instance at each time step so that training set are kept small while providing to the classifier the capability to suit itself to and to recover itself from different type of sentiment drift simultaneously providing capability to the classifier however is a conflicting objective problem and our proposed algorithm employ basic notion of economics in order to balance both capability we performed the analysis of event that reverberated on twitter and the comparison against the state of the art reveals improvement both in term of error reduction up to and reduction of training resource by order of magnitude 
multi tree ensemble model have been proven to be effective for document ranking using a large number of tree can improve accuracy but it take time to calculate ranking score of matched document this paper investigates data traversal method for fast score calculation with a large ensemble we propose a d blocking scheme for better cache utilization with simpler code structure compared to previous work the experiment with several benchmark show significant acceleration in score calculation without loss of ranking accuracy 
we propose to enhance proximity based probabilistic retrieval model with more contextual information a term pair with higher contextual relevance of term proximity is assigned a higher weight several measure are proposed to estimate the contextual relevance of term proximity we assume the top ranked document from a basic weighting model are more relevant to the query and calculate the contextual relevance of term proximity using the top ranked document we propose a context sensitive proximity model and the experimental result on standard trec data set show the effectiveness of our proposed model 
one of the fundamental problem in image search is to rank image document according to a given textual query existing search engine highly depend on surrounding text for ranking image or leverage the query image pair annotated by human labelers to train a series of ranking function however there are two major limitation the surrounding text are often noisy or too few to accurately describe the image content and the human annotation are resourcefully expensive and thus cannot be scaled up we demonstrate in this paper that the above two fundamental challenge can be mitigated by jointly exploring the cross view learning and the use of click through data the former aim to create a latent subspace with the ability in comparing information from the original incomparable view i e textual and visual view while the latter explores the largely available and freely accessible click through data i e crowdsourced human intelligence for understanding query specifically we propose a novel cross view learning method for image search named click through based cross view learning ccl by jointly minimizing the distance between the mapping of query and image in the latent subspace and preserving the inherent structure in each original space on a large scale click based image dataset ccl achieves the improvement over support vector machine based method by in term of relevance while reducing the feature dimension by several order of magnitude e g from thousand to ten moreover the experiment also demonstrate the superior performance of ccl to several state of the art subspace learning technique 
the potential impact of a scientific article ha a significant correlation with it ability to establish novel connection between pre existing knowledge discovering hidden connection between the existing scientific literature is an interesting yet highly challenging information retrieval problem literature based discovery lbd us computational algorithm to discover potential hidden connection between previously disconnected set of literature most of the current lbd method focus on analyzing latent semantic feature in text but are usually computationally demanding in particular they do not aim at predicting novel discovery link between cluster of literature combining latent semantic and structural feature of literature is a promising yet unexplored lbd approach this approach is potentially scalable and effective for example incorporating structural feature of web page ha increased the effectiveness of many large scale ir system the bibliographic structure of scientific paper make it possible to view a corpus of literature a a complex network of node article and link citation relationship in which recognizable community or cluster can be observed each representing a distinct research field consequently potential hidden connection between disparate field might be found from among non overlapping cluster that do not have any existing link between their member yet exhibit a high propensity to converge in the future this work approach lbd a a cluster link prediction problem we view disjoint literature set a disjoint cluster in citation network our method search for hidden connection between disjoint cluster whose member node show high probability in forming future link to this end we address two research problem the first problem is to group paper into cluster of distinct research area we compare the accuracy of well known community detection algorithm such a louvain and infomap in detecting research field cluster from citation network of physic literature we evaluate the quality of these cluster using purity rand index f measure and normalized mutual information since ground truth community are usually unknown we also propose using alternative textual coherence measure such a jensen shannon divergence the second problem is to predict the future formation of link between the node in previously disconnected cluster we introduce a novel algorithm latent domain similarity lds which us combination of semantic feature e g distribution of technical term in title and abstract and structural feature e g cited reference citing article of two or more article in order to infer shared latent domain between them we assume that while two set of literature could have been published separately in two seemingly unrelated field it is possible that they share many similar domain previously unknown to researcher in each field the goal is to explore whether these shared latent domain correlate with the probability of previously disconnected cluster to form future citation link with each other 
all research project begin with a goal for instance to describe search behavior to predict when a person will enter a second query or to discover which ir system performs the best different research goal suggest different research approach ranging from field study to lab study to online experimentation this tutorial will provide an overview of the different type of research goal common evaluation approach used to address each type and the constraint each approach entail participant will come away with a broad perspective of research goal and approach in ir and an understanding of the benefit and limitation of these research approach the tutorial will take place in two independent but interrelated part each focusing on a unique set of research approach but with the same intended tutorial outcome these outcome will be accomplished by deconstructing and analyzing our own published research paper with further illustration of each technique using the broader literature by using our own research a anchor we will provide insight about the research process revealing the difficult choice and trade offs researcher make when designing and conducting ir study 
microblogs such a twitter are important source for spreading vital information at high speed they also reflect the general people s reaction and opinion towards major event or story with information traveling so quickly it is helpful to be able to apply unsupervised learning technique to discover topic for information extraction and analysis although graphical model have been traditionally used for topic discovery in microblogs and text stream previous work may not be a efficient because of the diverse and noisy nature of microblogs in this paper we demonstrate the application of the author topic and the author recipient topic model to microblogs we extensively compare these model under different setting to an lda baseline our result show that the author recipient topic model extract the most coherent topic establishing that joint modeling on author recipient pair and on the content of tweet lead to quantitatively better topic discovery this paper also address the problem of topic modeling on short text by using clustering technique this technique help in boosting the performance of our model our study reveals interesting trait about twitter message user and their interaction 
we propose a two stage lossless compression approach on large scale rdf data our approach exploit both representation compression and component compression technique to support query and dynamic operation directly on the compressed data 
religious belief play an important role in how people behave influencing how they form preference interpret event around them and develop relationship with others traditionally the religion label of user population are obtained by conducting a large scale census study such an approach is both high cost and time consuming in this paper we study the problem of predicting user religion label using their microblogging data we formulate religion label prediction a a classification task and identify content structure and aggregate feature considering their self and social variant for representing a user we introduce the notion of representative user to identify user who are important in the religious user community we further define feature using representative user we show that svm classifier using our proposed feature can accurately assign christian and muslim label to a set of twitter user with known religion label 
a large number of web query are related to product entity studying evolution of product entity can help analyst understand the change in particular attribute value for these product however studying the evolution of a product requires u to be able to link various version of a product together in a temporal order while it is easy to temporally link recent version of product in a few domain manually solving the problem in general is challenging the ability to temporally order and link various version of a single product can also improve product search engine in this paper we tackle the problem of finding the previous version predecessor of a product entity given a repository of product entity we first parse the product name using a crf model after identifying entity corresponding to a single product we solve the problem of finding the previous version of any given particular version of the product for the second task we leverage innovative feature with a na ve bayes classifier our method achieve a precision of in identifying the product version from product entity name and a precision of in identifying the predecessor 
we examine the effect of expanding a judged set of sentence with their duplicate from a corpus including new sentence that are exact duplicate of the previously judged sentence may allow for better estimation of performance metric and enhance the reusability of a test collection we perform experiment in context of the temporal summarization track at trec we find that adding duplicate sentence to the judged set doe not significantly affect relative system performance however we do find statistically significant change in the performance of nearly half the system that participated in the track we recommend adding exact duplicate sentence to the set of relevance judgement in order to obtain a more accurate estimate of system performance 
this research fall in the area of enhancing the quality of tag based item recommendation system it aim to achieve this by employing a multi dimensional user profile approach and by analyzing the semantic aspect of tag tag based recommender system have two characteristic that need to be carefully studied in order to build a reliable system firstly the multi dimensional correlation called a tag assignment user item tag should be appropriately modelled in order to create the user profile secondly the semantics behind the tag should be considered properly a the flexibility with their design can cause semantic problem such a synonymy and polysemy this research proposes to address these two challenge for building a tag based item recommendation system by employing tensor modeling a the multi dimensional user profile approach and the topic model a the semantic analysis approach the first objective is to optimize the tensor model reconstruction and to improve the model performance in generating quality recommendation a novel tensor based recommendation using probabilistic ranking trpr method ha been developed result show this method to be scalable for large datasets and outperforming the benchmarking method in term of accuracy the memory efficient loop implement the n mode block striped matrix product for tensor reconstruction a an approximation of the initial tensor the probabilistic ranking calculates the probability of user to select candidate item using their tag preference list based on the entry generated from the reconstructed tensor the second objective is to analyse the tag semantics and utilize the outcome in building the tensor model this research proposes to investigate the problem using topic model approach to keep the tag nature a the social vocabulary for the tag assignment data topic can be generated from the occurrence of tag given for an item however there is only limited amount of tag available to represent item a collection of topic since an item might have only been tagged by using several tag consequently the generated topic might not able to represent the item appropriately furthermore given that each tag can belong to any topic with various probability score the occurrence of tag cannot simply be mapped by the topic to build the tensor model a standard weighting technique will not appropriately calculate the value of tagging activity since it will define the context of an item using a tag instead of a topic 
in this work we investigate approach to engineer better topic set in information retrieval test collection by recasting the trec evaluation exercise from one of building more effective system to an exercise in building better topic we present two possible approach to quantify topic goodness topic ease and topic set predictivity a novel interpretation of a well known result and a twofold analysis of data from several trec edition lead to a result that ha been neglected so far both topic ease and topic set predictivity have changed significantly across the year sometimes in a perhaps undesirable way 
the task of author verification is concerned with the question whether or not someone is the author of a given piece of text algorithm that extract writing style feature from text are used to determine how close in style different document are currently evaluation of author verification algorithm are restricted to small scale corpus with usually le than one hundred test case in this work we present a methodology to derive a large scale author verification corpus based on wikipedia talkpages we create a corpus based on english wikipedia which is significantly larger than existing corpus we investigate two dimension on this corpus which so far have not received sufficient attention the influence of topic and the influence of time on author verification accuracy 
there are many existing study of user behavior in simple task e g navigational and informational search within a short duration of query however we know relatively little about user behavior especially browsing and clicking behavior for longer search session solving complex search task in this paper we characterize and compare user behavior in relatively long search session minute about query for search task of four different type the task differ in two dimension the user is locating fact or is pursuing intellectual understanding of a topic the user ha a specific task goal or ha an ill defined and undeveloped goal we analyze how search behavior a well a browsing and clicking pattern change during a search session in these different task our result indicate that user behavior in the four type of task differ in various aspect including search activeness browsing style clicking strategy and query reformulation a a search session progress we note that user shift their interest to focus le on the top result but more on result ranked at lower position in browsing we also found that result eventually become le and le attractive for the user the reason vary and include downgraded search performance of query decreased novelty of search result and decaying persistence of user in browsing our study highlight the lack of long session support in existing search engine and suggests different strategy of supporting longer session according to different task type 
term relevance prediction from brain signal trpb is proposed to automatically detect relevance of text information directly from brain signal an experiment with forty participant wa conducted to record neural activity of participant while providing relevance judgment to text stimulus for a given topic high precision scientific equipment wa used to quantify neural activity across electroencephalography eeg channel a classifier based on a multi view eeg feature representation showed improvement up to in relevance prediction based on brain signal alone relevance wa also associated with brain activity with significant change in certain brain area consequently trpb is based on change identified in specific brain area and doe not require user specific training or calibration hence relevance prediction can be conducted for unseen content and unseen participant a an application of trpb we demonstrate a high precision variant of the classifier that construct set of relevant term for a given unknown topic of interest our research show that detecting relevance from brain signal is possible and allows the acquisition of relevance judgment without a need to observe any other user interaction this suggests that trpb could be used in combination or a an alternative for conventional implicit feedback signal such a dwell time or click through activity 
state of the art question answering qa system employ passage retrieval based on bag of word similarity model with respect to a query and a passage we propose a combination of a traditional bag of word similarity model and an annotation similarity model to improve passage ranking the proposed annotation similarity model is generic enough to process annotation of arbitrary type historical fact validation is a subtask to determine whether a given sentence tell u historically correct information which is important for a qa task on world history experimental result show that the combined model gain up to and improvement in historical fact validation in term of precision at rank and mean reciprocal rank respectively 
we present a novel unsupervised approach to re ranking an initially retrieved list the approach is based on the cross entropy method applied to permutation of the list and relies on performance prediction using pseudo predictor we establish a lower bound on the prediction quality that is required so a to have our approach significantly outperform the original retrieval our experiment serve a a proof of concept demonstrating the considerable potential of the proposed approach a case in point only a tiny fraction of the huge space of permutation need to be explored to attain significant improvement over the original retrieval 
caching query result is an efficient technique for web search engine admission policy can prevent infrequent query from taking space of more frequent query in the cache in this paper we present two novel admission policy tailored for query result cache these policy are based on query result prefetching information we also propose a demote operation for the query result cache to improve the cache hit ratio we then use a trace of over million query to evaluate our admission policy a well a traditional policy experimental result show that our prefetch aware admission policy can achieve hit ratio better than state of the art admission policy 
all pair similarity search used in many data mining and information retrieval application is a time consuming process although a partition based approach accelerates this process by simplifying parallelism management and avoiding unnecessary i o and comparison it is still challenging to balance the computation load among parallel machine with a distributed architecture this is mainly due to the variation in partition size and irregular dissimilarity relationship in large datasets this paper present a two stage heuristic algorithm to improve the load balance and shorten the overall processing time we analyze the optimality and competitiveness of the proposed algorithm and demonstrates it effectiveness using several datasets we also describe a static partitioning algorithm to even out the partition size while detecting more dissimilar pair the evaluation result show that the proposed scheme outperforms a previously developed solution by up to in the tested case 
prior art stay at the foundation for future work in academic research however the increasingly large amount of publication make it difficult for researcher to effectively discover the most important previous work to the topic of their research in this paper we study the automatic discovery of the core paper for a research area we propose a collective topic model on three type of object paper author and published venue we model any of these object a bag of citation based on probabilistic latent semantic analysis plsa authorship published venue and citation relation are used for quantifying paper importance our method discus milestone paper discovery in different case of input object experiment on the acl anthology network ann indicate that our model is superior in milestone paper discovery when compared to a previous model which considers only paper 
existing work on collaborative filtering cf is often based on the overall rating the item have received however in many case understanding how a user rate each aspect of an item may reveal more detailed information about her preference and thus may lead to more effective cf prior work ha studied extracting quantizing sentiment on different aspect from the review based on which the unknown overall rating are inferred however in that work all the aspect are treated equally while in reality different user tend to place emphasis on difference aspect when reaching the overall rating for example user may give a high rating to a movie just for it plot despite it mediocre performance this emphasis on aspect varies for different user and different item in this paper we propose a method that us tensor factorization to automatically infer the weight of different aspect in forming the overall rating the main idea is to learn through constrained optimization a compact representation of a weight tensor indexed by three dimension for user item and aspect respectively overall rating can then be predicted using the obtained weight experiment on a movie dataset show that our method compare favorably with three baseline method 
for large scale category system such a directory mozilla which consist of ten of thousand category it ha been empirically verified in earlier study that the distribution of document among category can be modeled a a power law distribution it implies that a significant fraction of category referred to a rare category have very few document assigned to them this characteristic of the data make it harder for learning algorithm to learn effective decision boundary which can correctly detect such category in the test set in this work we exploit the distribution of document among category to i derive an upper bound on the accuracy of any classifier and ii propose a ranking based algorithm which aim to maximize this upper bound the empirical evaluation on publicly available large scale datasets demonstrate that the proposed method not only achieves higher accuracy but also much higher coverage of rare category a compared to state of the art method 
to use math expression in search current search engine require knowing expression name or using a structure editor or string encoding e g latex for mathematical non expert this can lead to an intention gap between the query they wish to express and what the interface will allow them to express min is a search interface that support drawing expression on a canvas using mouse touch keyboard and image we present a user study examining whether min change search behavior for mathematical non expert and to identify real world usage scenario for multimodal math search interface participant found query by expression using hand drawn input useful and identified scenario in which they would like to use system like min such a for locating editing and sharing complex expression e g with many greek letter and working on complex math problem 
score safe index processing ha received a great deal of attention over the last two decade by pre calculating maximum term impact during indexing the number of scoring operation can be minimized and the top k document for a query can be located efficiently however these method often ignore the importance of the effectiveness gain possible when using sequential dependency model we present a hybrid approach which leverage score safe processing and suffix based self indexing structure in order to provide efficient and effective top k document retrieval 
a the scale of available on line data grows ever larger individual and business must cope with increasing complexity in decision making process which utilize large volume of unstructured semi structured and or structured data to satisfy multiple interrelated information need which contribute to an overall decision traditional decision support system ds have been developed to address this need but such system are typically expensive to build and are purpose built for a particular decision making scenario making them difficult to extend or adapt to new decision scenario in this paper we propose a novel decision representation which allows decision maker to formulate and organize natural language question or assertion into an analytic hierarchy which can be evaluated a part of an ad hoc decision process or a a documented repeatable analytic process we then introduce a new decision support framework quad which take advantage of automatic question answering qa technology to automatically understand and process a decision representation producing a final decision by gathering and weighting answer to individual question using a bayesian learning and inference process an open source framework implementation is presented and applied to two real world application target validation a fundamental decision making task for the pharmaceutical industry and product recommendation from review text an everyday decision making situation faced by on line consumer in both application we implemented and compared a number of decision synthesis algorithm and present experimental result which demonstrate the performance of the quad approach versus other baseline approach 
search satisfaction is a property of a user s search process understanding it is critical for search provider to evaluate the performance and improve the effectiveness of search engine existing method model search satisfaction holistically at the search task level ignoring important dependency between action level satisfaction and overall task satisfaction we hypothesize that searcher latent action level satisfaction i e whether they believe they were satisfied with the result of a query or click influence their observed search behavior and contributes to overall search satisfaction we conjecture that by modeling search satisfaction at the action level we can build more complete and more accurate predictor of search task satisfaction to do this we develop a latent structural learning method whereby rich structured feature and dependency relation unique to search satisfaction prediction are explored using in situ search satisfaction judgment provided by searcher we show that there is significant value in modeling action level satisfaction in search task satisfaction prediction in addition experimental result on large scale log from bing com demonstrate clear benefit from using inferred action satisfaction label for other application such a document relevance estimation and query suggestion 
the statistical machine translation smt component of cross lingual information retrieval clir system is often regarded a black box that is optimized for translation quality independent from the retrieval task in recent work smt ha been tuned for retrieval by training a reranker on k best translation ordered according to their retrieval performance in this paper we propose a decomposable proxy for retrieval quality that obviates the need for costly intermediate retrieval furthermore we explore the full search space of the smt decoder by directly optimizing decoder parameter under a retrieval based objective experimental result for patent retrieval show our approach to be a promising alternative to the standard pipeline approach 
a query log is a key asset in a commercial search engine everyday million of user rely on search engine to find information on the web by entering a few keywords on a simple search interface those query represent a subset of user behavioral data which is used to mine and discover search pattern for improving the overall end user experience while query are very useful it is not always possible to capture precisely what the user wa looking for when the intent is not that clear we explore a different alternative based on human computation to gather a bit more information from user and show the type of query log that would be possible to construct 
hashtags have been widely used to annotate topic in tweet short post on twitter com in this paper we study the problem of real time prediction of bursting hashtags will a hashtag burst in the near future if it will how early can we predict it and how popular will it become based on empirical analysis of data collected from twitter we propose solution to these challenging problem the performance of different feature and possible solution are evaluated 
we address the problem of retrieving chess game position similar to a given query position from a collection of archived chess game we investigate this problem from an information retrieval ir perspective the advantage of our proposed ir based approach is that it allows using the standard inverted organization of stored chess position leading to an efficient retrieval moreover in contrast to retrieving exactly identical board position the ir based approach is able to provide approximate search functionality in order to define the similarity between two chess board position we encode each game state with a textual representation this textual encoding is designed to represent the position reachability and the connectivity between chess piece due to the absence of a standard ir dataset that can be used for this search task a new evaluation benchmark dataset wa constructed comprising of document chess position from a freely available chess game archive experiment conducted on this dataset demonstrate that our proposed method of similarity computation which take into account a combination of the mobility and the connectivity between the chess piece performs well on the search task achieving map and ndcg value of and respectively 
many domain specific search task are initiated by document length query e g patent invalidity search aim to find prior art related to a new query patent we call this type of search query document search in this type of search the initial query document is typically long and contains diverse aspect or sub topic user tend to issue many query based on the initial document to retrieve relevant document to help user in this situation we propose a method to suggest diverse query that can cover multiple aspect of the query document we first identify multiple query aspect and then provide diverse query suggestion that are effective for retrieving relevant document a well being related to more query aspect in the experiment we demonstrate that our approach is effective in comparison to previous query suggestion method 
the elia fano representation of monotone sequence ha been recently applied to the compression of inverted index showing excellent query performance thanks to it efficient random access and search operation while it space occupancy is competitive with some state of the art method such a gamma delta golomb code and pfordelta it fails to exploit the local clustering that inverted list usually exhibit namely the presence of long subsequence of close identifier in this paper we describe a new representation based on partitioning the list into chunk and encoding both the chunk and their endpoint with elia fano hence forming a two level data structure this partitioning enables the encoding to better adapt to the local statistic of the chunk thus exploiting clustering and improving compression we present two partition strategy respectively with fixed and variable length chunk for the latter case we introduce a linear time optimization algorithm which identifies the minimum space partition up to an arbitrarily small approximation factor we show that our partitioned elia fano index offer significantly better compression than plain elia fano while preserving their query time efficiency furthermore compared with other state of the art compressed encoding our index exhibit the best compression ratio query time trade off 
the development and evaluation of information retrieval and recommender system ha traditionally focused on the relevance and accuracy of retrieved document and recommendation respectively however there is an increasing realization that accuracy alone might be a sub optimal strategy for a successful user experience property such a novelty and diversity have been explored in both field for assessing and enhancing the usefulness of search result and recommendation in this doctoral research we study the assessment and enhancement of both property in the confluence of information retrieval and recommender system 
we address the task of recipient recommendation for emailing in enterprise we propose an intuitive and elegant way of modeling the task of recipient recommendation which us both the communication graph i e who are most closely connected to the sender and the content of the email additionally the model can incorporate evidence a prior probability experiment on two enterprise email collection show that our model achieves very high score and that it outperforms two variant that use either the communication graph or the content in isolation 
the semantics of mathematical formula depend on their spatial structure and they usually exist in layout presentation such a pdf latex and presentation mathml which challenge previous text index and retrieval method this paper proposes an innovative mathematics retrieval system along with the novel algorithm which enables efficient formula index and retrieval from both webpage and pdf document unlike prior study which require user to manually input formula markup language a query the new system enables user to copy formula query directly from pdf document furthermore by using a novel indexing and matching model the system is aimed at searching for similar mathematical formula based on both textual and spatial similarity a hierarchical generalization technique is proposed to generate sub tree from the semi operator tree of formula and support substructure match and fuzzy match experiment based on massive wikipedia and citeseer repository show that the new system along with novel algorithm comparing with two representative mathematics retrieval system provides more efficient mathematical formula index and retrieval while simplifying user query input for pdf document 
rating prediction is to predict the preference rating of a user to an item that she ha not rated before using the business review data from yelp in this paper we study business rating prediction a business here can be a restaurant a shopping mall or other kind of business different from most other type of item that have been studied in various recommender system e g movie song book a business physically exists at a geographical location and most business have geographical neighbor within walking distance when a user visit a business there is a good chance that she walk by it neighbor through data analysis we observe that there exists weak positive correlation between a business s rating and it neighbor rating regardless of the category of business based on this observation we assume that a user s rating to a business is determined by both the intrinsic characteristic of the business and the extrinsic characteristic of it geographical neighbor using the widely adopted latent factor model for rating prediction in our proposed solution we use two kind of latent factor to model a business one for it intrinsic characteristic and the other for it extrinsic characteristic the latter encodes the neighborhood influence of this business to it geographical neighbor in our experiment we show that by incorporating geographical neighborhood influence much lower prediction error is achieved than the state of the art model including biased mf svd and social mf the prediction error is further reduced by incorporating influence from business category and review content 
finding way to help user ass relevance when they search using math expression is critical for making mathematical information retrieval mir system easier to use we designed a study where participant completed search task involving mathematical expression using two different summary style and measured response time and relevance assessment accuracy the control summary style used google s regular hit formatting where expression are presented a text e g in latex while the second summary style render the math expression participant were undergraduate and graduate student participant in the rendered summary style n had on average a higher assessment accuracy than those in the non rendered summary style n with no significant difference in response time participant in the rendered condition reported having fewer problem reading hit than participant in the control condition this suggests that user will benefit from search engine that properly render math expression in their hit summary 
social tag are known to be a valuable source of information for image retrieval and organization however contrary to the conventional document retrieval rich tag frequency information in social sharing system such a flickr is not available thus we cannot directly use the tag frequency analogous to the term frequency in a document to represent the relevance of tag many heuristic approach have been proposed to address this problem among which the well known neighbor voting based approach are the most effective method the basic assumption of these method is that a tag is considered a relevant to the visual content of a target image if this tag is also used to annotate the visual neighbor image of the target image by lot of different user the main limitation of these approach is that they treat the voting power of each neighbor image either equally or simply based on it visual similarity in this paper we cast the social tag relevance learning problem a an adaptive teleportation random walk process on the voting graph in particular we model the relationship among image by constructing a voting graph and then propose an adaptive teleportation random walk in which a confidence factor is introduced to control the teleportation probability on the voting graph through this process direct and indirect relationship among image can be explored to cooperatively estimate the tag relevance to quantify the performance of our approach we compare it with state of the art method on two publicly available datasets nu wide and mir flickr the result indicate that our method achieves substantial performance gain on these datasets 
query auto completion qac facilitates faster user query input by predicting user intended query most qac algorithm take a learning based approach to incorporate various signal for query relevance prediction however such model are trained on simulated user input from query log data the lack of real user interaction data in the qac process prevents them from further improving the qac performance in this work for the first time we collect a high resolution qac query log that record every keystroke in a qac session based on this data we discover two user behavior namely the horizontal skipping bias and vertical position bias which are crucial for relevance prediction in qac in order to better explain them we propose a novel two dimensional click model for modeling the qac process with emphasis on these behavior extensive experiment on our qac data set from both pc and mobile device demonstrate that our proposed model can accurately explain the user behavior in interacting with a qac system and the resulting relevance model significant improves the qac performance over existing click model furthermore the learned knowledge about the skipping behavior can be effectively incorporated into existing learning based model to further improve their performance 
due to it low storage cost and fast query speed hashing ha been widely adopted for approximate nearest neighbor search in large scale datasets traditional hashing method try to learn the hash code in an unsupervised way where the metric euclidean structure of the training data is preserved very recently supervised hashing method which try to preserve the semantic structure constructed from the semantic label of the training point have exhibited higher accuracy than unsupervised method in this paper we propose a novel supervised hashing method called latent factor hashing lfh to learn similarity preserving binary code based on latent factor model an algorithm with convergence guarantee is proposed to learn the parameter of lfh furthermore a linear time variant with stochastic learning is proposed for training lfh on large scale datasets experimental result on two large datasets with semantic label show that lfh can achieve superior accuracy than state of the art method with comparable training time 
the fast growth of technology ha driven the advancement of our society it is often necessary to quickly grab the evolution of technology in order to better understand the technology trend the availability of huge volume of granted patent document provides a reasonable basis for analyzing technology evolution in this paper we propose a unified framework named patentline to generate a technology evolution tree for a given topic or a classification code related to granted patent the framework integrates different type of patent information including patent content citation of patent temporal relation etc and provides a concise yet comprehensive evolution summary the generated summary enables a variety of patent related analysis such a identifying relevant prior art and detecting technology gap a case study on a collection of u patent demonstrates the efficacy of our proposed framework 
hashing technique have been extensively investigated to boost similarity search for large scale high dimensional data most of the existing approach formulate the their objective a a pair wise similarity preserving problem in this paper we consider the hashing problem from the perspective of optimizing a list wise learning to rank problem and propose an approach called list wise supervised hashing lwh in lwh the hash function are optimized by employing structural svm in order to explicitly minimize the ranking loss of the whole list wise permutation instead of merely the point wise or pair wise supervision we evaluate the performance of lwh on two real world data set experimental result demonstrate that our method obtains a significant improvement over the state of the art hashing approach due to both structural large margin and list wise ranking pursuing in a supervised manner 
the purpose of this study is to investigate the extent to which two theory information scent and need for cognition explain people s search behavior when interacting with search engine result page serps information scent the perception of the value of information source wa manipulated by varying the number and distribution of relevant result on the first serp need for cognition nfc a personality trait that measure the extent to which a person enjoys cognitively effortful activity wa measured by a standardized scale a laboratory experiment wa conducted with forty eight participant who completed six open ended search task result showed that while interacting with serps containing more relevant document participant examined more document and clicked deeper in the search result list when interacting with serps that contained the same number of relevant result distributed across different rank participant were more likely to abandon their query when relevant document appeared later on the serp with respect to nfc participant with higher nfc paginated le frequently and paid le attention to result at lower rank than those with lower nfc the interaction between nfc and the number of relevant result on the serp affected the time spent on searching and a participant s likelihood to reformulate paginate and stop our finding suggest evaluating system effectiveness based on the first page of result even for task that require the user to view multiple document and varying interface feature based on nfc 
medical information is accessible from diverse source including the general web social medium journal article and hospital record information searcher can be patient and their family researcher practitioner and clinician challenge in medical information retrieval include diversity of user and user knowledge and expertise variation in the format reliability and quality of biomedical and medical information the multi modal nature of much of the data and the need for accuracy and reliability of medical information the aim of the workshop is to bring together researcher interested in medical information search with the goal of identifying specific challenge that need to be addressed to advance the state of the art 
million of people search the web each day a a consequence the ranking algorithm employed by web search engine have a profound influence on which page user visit characterizing this influence and informing user when different engine favor certain site or point of view enables more transparent access to the web s information we present paw a platform for analyzing difference among web search engine paw measure content emphasis the degree to which difference across search engine ranking correlate with feature of the ranked content including point of view e g positive or negative orientation toward their company s product and advertisement we propose an approach for identifying the orientation in search result at scale through a novel technique that minimizes the expected number of human judgment required we apply paw to news search on google and bing and find no evidence that the engine emphasize result that express positive orientation toward the engine company s product we do find that the engine emphasize particular news site and that they also favor page containing their company s advertisement a opposed to competitor advertisement 
entity are centric to a large number of real world application wikipedia show entity infoboxes for a large number of entity however not much structured information is available about character entity in book automatic discovery of character from book can help in effective summarization such a structured summary which not just introduces character in the book but also provides a high level relationship between them can be of critical importance for buyer this task involves the following challenging novel problem automatic discovery of important character given a book automatic social graph construction relating the discovered character automatic summarization of text most related to each of the character and automatic infobox extraction from such summarized text for each character a part of this demo we design mechanism to address these challenge and experiment with publicly available book 
abstract using a novel evaluation toolkit that simulates a human reviewer in the loop we compare the effectiveness of three machine learning protocol for technology assisted review a used in document review for discovery in legal proceeding our comparison address a central question in the deployment of technology assisted review should training document be selected at random or should they be selected using one or more non random method such a keyword search or active learning on eight review task four derived from the trec legal track and four derived from actual legal matter recall wa measured a a function of human review effort the result show that entirely non random training method in which the initial training document are selected using a simple keyword search and subsequent training document are selected by active learning require substantially and significantly le human review effort p 
one of the main target of any search engine is to make every user fully satisfied with her search result for this reason lot of effort are being paid to improving ranking model in order to show the best result to user however there is a class of document on the web which can spoil all effort being shown to the user when user receive result which are not only irrelevant but also completely out of the picture of their expectation they can get really frustrated so we attempted to find a method to determine such document and reduce their negative impact upon user and a a consequence on search engine in general 
finding relevant web service and composing them into value added application is becoming increasingly important in cloud and service based marketplace the key problem with current approach to finding relevant web service is that most of them only provide search over a discrete set of feature using exact keyword matching we demonstrate in this paper that by utilizing well known indexing scheme such a inverted file and r tree index over web service attribute the earth mover s distance emd algorithm can be used efficiently to find partial match between a query and a database of web service 
supervised text classifier require extensive human expertise and labeling effort in this paper we propose a weakly supervised text classification algorithm based on the labeling of latent dirichlet allocation lda topic our algorithm is based on the generative property of lda in our algorithm we ask an annotator to assign one or more class label to each topic based on it most probable word we classify a document based on it posterior topic proportion and the class label of the topic we also enhance our approach by incorporating domain knowledge in the form of labeled word we evaluate our approach on four real world text classification datasets the result show that our approach is more accurate in comparison to semi supervised technique from previous work a central contribution of this work is an approach that delivers effectiveness comparable to the state of the art supervised technique in hard to classify domain with very low overhead in term of manual knowledge engineering 
collaborative filtering cf based recommendation algorithm such a latent factor model lfm work well in term of prediction accuracy however the latent feature make it difficulty to explain the recommendation result to the user fortunately with the continuous growth of online user review the information available for training a recommender system is no longer limited to just numerical star rating or user item feature by extracting explicit user opinion about various aspect of a product from the review it is possible to learn more detail about what aspect a user care which further shed light on the possibility to make explainable recommendation in this work we propose the explicit factor model efm to generate explainable recommendation meanwhile keep a high prediction accuracy we first extract explicit product feature i e aspect and user opinion by phrase level sentiment analysis on user review then generate both recommendation and disrecommendations according to the specific product feature to the user s interest and the hidden feature learned besides intuitional feature level explanation about why an item is or is not recommended are generated from the model offline experimental result on several real world datasets demonstrate the advantage of our framework over competitive baseline algorithm on both rating prediction and top k recommendation task online experiment show that the detailed explanation make the recommendation and disrecommendations more influential on user s purchasing behavior 
measurement are fundamental to any empirical science and similarly search evaluation is a vital part of information retrieval ir evaluation ensures the progressive development of approach tool and method studied in this field apart from the scientific perspective the evaluation approach are also important from the practical perspective indeed the evaluation experiment enable commercial search engine to make data driven decision while developing new feature and working on the quality of the user experience thus it is not surprising that evaluation ha gained a huge attention from the research community and such an interest span almost fifty year of research the cranfield experiment evolved into the widely used offline system evaluation approach despite it convenience and popularity the offline evaluation approach ha several limitation these limitation resulted in the development and recent growth in popularity of the online user based evaluation approach such a interleaving and a b testing 
with recent advance in radio frequency identification rfid wireless sensor network and web based service physical thing are becoming an integral part of the emerging ubiquitous web in this paper we focus on the thing recommendation problem in internet of thing iot in particular we propose a unified probabilistic based framework by fusing information across relationship between user i e user social network and thing i e thing correlation to make more accurate recommendation the proposed approach not only inherits the advantage of the matrix factorization but also exploit the merit of social relationship and thing thing correlation we validate our approach based on an internet of thing platform and the experimental result demonstrate it feasibility and effectiveness 
many technique in information retrieval produce count from a sample and it is common to analyse these count a proportion of the whole term frequency are a familiar example proportion carry only relative information and are not free to vary independently of one another for the proportion of one term to increase one or more others must decrease these constraint are hallmark of compositional data while there ha long been discussion in other field of how such data should be analysed to our knowledge compositional data analysis coda ha not been considered in ir in this work we explore compositional data in ir through the lens of distance measure and demonstrate that common measure naive to composition have some undesirable property which can be avoided with composition aware measure a a practical example these measure are shown to improve clustering 
this paper describes an advanced search engine that support user in querying document by mean of keywords entity and category user simply type word which are automatically mapped onto appropriate suggestion for entity and category based on named entity disambiguation the search engine return document containing the query s entity and prominent entity from the query s category 
similarity search method based on hashing for effective and efficient cross modal retrieval on large scale multimedia database with massive text and image have attracted considerable attention the core problem of cross modal hashing is how to effectively construct correlation between multi modal representation which are heterogeneous intrinsically in the process of hash function learning analogous to canonical correlation analysis cca most existing cross modal hash method embed the heterogeneous data into a joint abstraction space by linear projection however these method fail to bridge the semantic gap more effectively and capture high level latent semantic information which ha been proved that it can lead to better performance for image retrieval to address these challenge in this paper we propose a novel latent semantic sparse hashing lssh to perform cross modal similarity search by employing sparse coding and matrix factorization in particular lssh us sparse coding to capture the salient structure of image and matrix factorization to learn the latent concept from text then the learned latent semantic feature are mapped to a joint abstraction space moreover an iterative strategy is applied to derive optimal solution efficiently and it help lssh to explore the correlation between multi modal representation efficiently and automatically finally the unified hashcodes are generated through the high level abstraction space by quantization extensive experiment on three different datasets highlight the advantage of our method under cross modal scenario and show that lssh significantly outperforms several state of the art method 
a common problem in unstructured peer to peer p p information retrieval is the need to compute global statistic of the full collection when only a small subset of the collection is visible to a peer without accurate estimate of these statistic the effectiveness of modern retrieval model can be reduced we show that for the case of a probably approximately correct p p architecture and using either the bm retrieval model or a language model with dirichlet smoothing very close approximation of the required global statistic can be estimated with very little overhead and a small extension to the protocol however through theoretical modeling and simulation we demonstrate this technique also greatly increase the ability for adversarial peer to manipulate search result we show an adversary controlling fewer than of peer can censor or increase the rank of document or disrupt overall search result a a defense we propose a simple modification to the extension and show global statistic estimation is viable even when up to of peer are adversarial 
cripts e g arabic greek and indic language one can often find a large amount of user generated transliterated content on the web in the roman script such content creates a monolingual or cross lingual space with more than one script which is referred a mixed script space and information retrieval in this space is referred a mixed script information retrieval msir in mixed script space the document and query may either be in the native script and or the roman transliterated script for a language mono lingual scenario there can be further extension of msir such a multi lingual msir in which term can be in multiple script in multiple language since there are no standard way of spelling a word in a non native script transliteration content almost always feature extensive spelling variation this phenomenon present a non trivial term matching problem for search engine to match the native script or roman transliterated query with the document in multiple script taking into account the spelling variation this problem although prevalent inweb search for user of many language around the world ha received very little attention till date very recently we have formally defined the problem of msir and presented the quantitative study on it through bing query log analysis 
recommender system usually need to compare a large number of item before user most preferred one can be found this process can be very costly if recommendation are frequently made on large scale datasets in this paper a novel hashing algorithm named preference preserving hashing pph is proposed to speed up recommendation hashing ha been widely utilized in large scale similarity search e g similar image search and the search speed with binary hashing code is significantly faster than that with real valued feature however one challenge of applying hashing to recommendation is that recommendation concern user preference over item rather than their similarity to address this challenge pph contains two novel component that work with the popular matrix factorization mf algorithm in mf user preference over item are calculated a the inner product between the learned real valued user item feature the first component of pph constrains the learning process so that user preference can be well approximated by user item similarity the second component which is a novel quantization algorithm generates the binary hashing code from the learned real valued user item feature finally recommendation can be achieved efficiently via fast hashing code search experiment on three real world datasets show that the recommendation speed of the proposed pph algorithm can be hundred of time faster than original mf with real valued feature and the recommendation accuracy is significantly better than previous work of hashing for recommendation 
cold start is one of the most challenging problem in recommender system in this paper we tackle the cold start problem by proposing a context aware semi supervised co training method named csel specifically we use a factorization model to capture fine grained user item context then in order to build a model that is able to boost the recommendation performance by leveraging the context we propose a semi supervised ensemble learning algorithm the algorithm construct different weak prediction model using example with different context and then employ the co training strategy to allow each weak prediction model to learn from the other prediction model the method ha several distinguished advantage over the standard recommendation method for addressing the cold start problem first it defines a fine grained context that is more accurate for modeling the user item preference second the method can naturally support supervised learning and semi supervised learning which provides a flexible way to incorporate the unlabeled data the proposed algorithm are evaluated on two real world datasets the experimental result show that with our method the recommendation accuracy is significantly improved compared to the standard algorithm and the cold start problem is largely alleviated 
traditionally the efficiency and effectiveness of search system have both been of great interest to the information retrieval community however an in depth analysis on the interplay between the response latency of web search system and user search experience ha been missing so far in order to fill this gap we conduct two separate study aiming to reveal how response latency affect the user behavior in web search first we conduct a controlled user study trying to understand how user perceive the response latency of a search system and how sensitive they are to increasing delay in response this study reveals that when artificial delay are introduced into the response the user of a fast search system are more likely to notice these delay than the user of a slow search system the introduced delay become noticeable by the user once they exceed a certain threshold value second we perform an analysis using a large scale query log obtained from yahoo web search to observe the potential impact of increasing response latency on the click behavior of user this analysis demonstrates that latency ha an impact on the click behavior of user to some extent in particular given two content wise identical search result page we show that the user are more likely to perform click on the result page that is served with lower latency 
previous study of online user attention during information seeking task have mainly focused on analyzing searcher behavior in the web search setting while these study enabled better understanding of search result examination their finding might not generalize for the task and search interface in other domain such a shopping or social medium in this paper we present to best of our knowledge the first cross domain comparison of search examination behavior and pattern of aggregated attention across web search news shopping and social network domain we investigate how domain of the search and the scope of the information need affect search examination and find significant difference beyond those arising from natural disparity between individual for example we find that the mean fixation duration a common indicator of cognitive load varies significantly across domain e g mean fixation duration in the social network domain exceeds that of general web search by over we also find large difference in the aggregate pattern of user attention on the screen especially in the shopping and social network domain compared to the web search domain emphasizing the need for domain specific user model and evaluation metric 
we examine the spatial keyword search problem to retrieve object of interest that are ranked based on both their spatial proximity to the query location a well a the textual relevance of the object s keywords existing solution for the problem are based on either using a combination of textual and spatial index or using specialized hybrid index that integrate the indexing of both textual and spatial attribute value in this paper we propose a new approach that is based on modeling the problem a a top k aggregation problem which enables the design of a scalable and efficient solution that is based on the ubiquitous inverted list index our performance study demonstrates that our approach outperforms the state of the art hybrid method by a wide margin 
it is rare for a new user interface to break through and become successful especially in information intensive task like search coming to consensus or building up knowledge most complex interface end up going unused often the successful solution lie in a previously unexplored part of the interface design space that is simple in a new way that work just right in this talk i will give example of such success in the information intensive interface design space and attempt to provide stimulating idea for future research direction 
a large number of mainstream application like temporal search event detection and trend identification assume knowledge of the timestamp of every document in a given textual collection in many case however the required timestamps are either unavailable or ambiguous a characteristic instance of this problem emerges in the context of large repository of old digitized document for such document the timestamp may be corrupted during the digitization process or may simply be unavailable in this paper we study the task of approximating the timestamp of a document so called document dating we propose a contentbased method and use recent advance in the domain of term burstiness which allow it to overcome the drawback of previous document dating method e g the fix time partition strategy we use an extensive experimental evaluation on different datasets to validate the efficacy and advantage of our methodology showing that our method outperforms the state of the art method on document dating 
recommender system ha become an important component in modern ecommerce recent research on recommender system ha been mainly concentrating on improving the relevance or profitability of individual recommended item but in reality user are usually exposed to a set of item and they may buy multiple item in one single order thus the relevance or profitability of one item may actually depend on the other item in the set in other word the set of recommendation is a bundle with item interacting with each other in this paper we introduce a novel problem called the bundle recommendation problem brp by solving the brp we are able to find the optimal bundle of item to recommend with respect to preferred business objective however brp is a large scale np hard problem we then show that it may be sufficient to solve a significantly smaller version of brp depending on property of input data this allows u to solve brp in real world application with million of user and item both offline and online experimental result on a walmart com demonstrate the incremental value of solving brp across multiple baseline model 
we present a generic method for augmenting unsupervised query segmentation by incorporating part of speech po sequence information to detect meaningful but rare n gram our initial experiment with an existing english po tagger employing two different po tagsets and an unsupervised po induction technique specifically adapted for query show that po information can significantly improve query segmentation performance in all these case 
we propose a family of new evaluation measure called markov precision mp which exploit continuous time and discrete time markov chain in order to inject user model into precision continuous time mp behaves like time calibrated measure bringing the time spent by the user into the evaluation of a system discrete time mp behaves like traditional evaluation measure being part of the same markovian framework the time based and rank based version of mp produce value that are directly comparable we show that it is possible to re create average precision using specific user model and this help in providing an explanation of average precision ap in term of user model more realistic than the one currently used to justify it we also propose several alternative model that take into account different possible behavior in scanning a ranked result list finally we conduct a thorough experimental evaluation of mp on standard trec collection in order to show that mp is a reliable a other measure and we provide an example of calibration of it time parameter based on click log from yandex 
adequacy of citation is very important for a scientific paper however it is not an easy job to find appropriate citation for a given context especially for citation in different language in this paper we define a novel task of cross language context aware citation recommendation which aim at recommending english citation for a given context of the place where a citation is made in a chinese paper this task is very challenging because the context and citation are written in different language and there exists a language gap when matching them to tackle this problem we propose the bilingual context citation embedding algorithm i e blsrec i which can learn a low dimensional joint embedding space for both context and citation moreover two advanced algorithm named blsrec ii and blsrec iii are proposed by enhancing blsrec i with translation result and abstract information respectively we evaluate the proposed method based on a real dataset that contains chinese context and english citation the result demonstrate that our proposed algorithm can outperform a few baseline and the blsrec ii and blsrec iii method can outperform the blsrec i method 
we present a study of the correlation between the extent to which the cluster hypothesis hold a measured by various test and the relative effectiveness of cluster based retrieval with respect to document based retrieval we show that the correlation can be affected by several factor such a the size of the result list of the most highly ranked document that is analyzed we further show that some cluster hypothesis test are often negatively correlated with one another moreover in several setting some of the test are also negatively correlated with the relative effectiveness of cluster based retrieval 
a person often us a single search engine for very different task for example an author editing a manuscript may use the same academic search engine to find the latest work on a particular topic or to find the correct citation for a familiar article the author s tolerance for latency and accuracy may vary according to task however search engine typically employ a consistent approach for processing all query in this paper we explore how a range of search need and expectation can be supported within a single search system using differential search we introduce citesight a system that provides personalized citation recommendation to author group that vary based on task citesight present cached recommendation instantaneously for online task e g active paper writing and refines these recommendation in the background for offline task e g future literature review we develop an active cache warming process to enhance the system a the author work and context coupling a technique for augment sparse citation network by evaluating the quality of the recommendation and collecting user feedback we show that differential search can provide a high level of accuracy for different task on different time scale we believe that differential search can be used in many situation where the user s tolerance for latency and desired response vary dramatically based on use 
in recent year researcher have investigated search result diversification through a variety of approach in such situation information retrieval system need to consider both aspect of relevance and diversity for those retrieved document on the other hand previous research ha demonstrated that data fusion is useful for improving performance when we are only concerned with relevance however it is not clear if it help when both relevance and diversity are both taken into consideration in this short paper we propose a few data fusion method to try to improve performance when both relevance and diversity are concerned experiment are carried out with group of top ranked result submitted to the trec web diversity task we find that data fusion is still a useful approach to performance improvement for diversity a for relevance previously 
all research project begin with a goal for instance to describe search behavior to predict when a person will enter a second query or to discover which ir system performs the best different research goal suggest different research approach ranging from field study to lab study to online experimentation this tutorial will provide an overview of the different type of research goal common evaluation approach used to address each type and the constraint each approach entail participant will come away with a broad perspective of research goal and approach in ir and an understanding of the benefit and limitation of these research approach the tutorial will take place in two independent but interrelated part each focusing on a unique set of research approach but with the same intended tutorial outcome these outcome will be accomplished by deconstructing and analyzing our own published research paper with further illustration of each technique using the broader literature by using our own research a anchor we will provide insight about the research process revealing the difficult choice and trade offs researcher make when designing and conducting ir study 
current approach for contextual sentiment lexicon construction in phrase level sentiment analysis assume that the numerical star rating of a review represents the overall sentiment orientation of the review text although widely adopted we find through user rating analysis that this is not necessarily true in this paper we attempt to bridge the gap between phrase level and review document level sentiment analysis by leveraging the result given by review level sentiment classification to boost phrase level sentiment polarity labeling in contextual sentiment lexicon construction task using a novel constrained convex optimization framework experimental result on both english and chinese review show that our framework improves the precision of sentiment polarity labeling by up to which is a significant improvement from current approach 
social medium such a twitter ha come to reflect the reaction of the general public to major event since post are short and noisy it is hard to extract reliable event based on word frequency even though an event term appears in a particularly low frequency a long a at least one reliable user mention the term it should be extracted this paper proposes an event extraction method which combine user reliability and timeline analysis the latent dirichlet allocation lda topic model is adapted with the weight of event term on timeline and reliable user to extract social event the reliable user are detected on twitter according to their tweeting behavior socially well known user and active user reliable and low frequency event can be detected based on reliable user in order to see the effectiveness of the proposed method experiment are conducted on a korean tweet collection the proposed model achieved in precision this show that the lda with timeline and reliable user is effective for extracting event on the twitter test collection 
web search engine are optimized to reduce the high percentile response time to consistently provide fast response to almost all user query this is a challenging task because the query workload exhibit large variability consisting of many short running query and a few long running query that significantly impact the high percentile response time with modern multicore server parallelizing the processing of an individual query is a promising solution to reduce query execution time but it give limited benefit compared to sequential execution since most query see little or no speedup when parallelized the root of this problem is that short running query which dominate the workload do not benefit from parallelization they incur a large parallelization overhead taking scarce resource from long running query on the other hand parallelization substantially reduces the execution time of long running query with low overhead and high parallelization efficiency motivated by these observation we propose a predictive parallelization framework with two part predicting long running query and selectively parallelizing them for the first part prediction should be accurate and efficient for accuracy we study a comprehensive feature set covering both term feature reflecting dynamic pruning efficiency and query feature reflecting query complexity for efficiency to keep overhead low we avoid expensive feature that have excessive requirement such a large memory footprint for the second part we use the predicted query execution time to parallelize long running query and process short running query sequentially we implement and evaluate the predictive parallelization framework in microsoft bing search our measurement show that under moderate to heavy load the predictive strategy reduces the th percentile response time by from m to m compared with prior approach that parallelize all query 
presence of hyperlink in a tweet is a strong indication of tweet being more informative in this paper we study the problem of hashtag recommendation for hyperlinked tweet i e tweet containing link to web page by recommending hashtags to hyperlinked tweet we argue that the function of hashtags such a providing the right context to interpret the tweet tweet categorization and tweet promotion can be extended to the linked document the proposed solution for hashtag recommendation consists of two phase in the first phase we select candidate hashtags through five scheme by considering the similar tweet the similar document the named entity contained in the document and the domain of the link in the second phase we formulate the hashtag recommendation problem a a learning to rank problem and adopt ranksvm to aggregate and rank the candidate hashtags our experiment on a collection of million tweet show that the proposed solution achieves promising result 
personal expertise or interest often evolve over time despite much work on expertise retrieval in the recent year very little work ha studied the dynamic of personal expertise in this paper we propose a probabilistic model to characterize how people change or stick with their expertise specifically three factor are taken into consideration in whether an expert will choose a new expertise area the personality of the expert in exploring new area the similarity between the new area and the expert s current area the popularity of the new area these three factor are integrated into a unified generative process a predictive language model is derived to estimate the distribution of the expert s word in her future publication in addition kl divergence is defined on the predictive language model to quantify and forecast the change of expertise we conduct the experiment on a testbed of academic publication and the initial result demonstrate the effectiveness of the proposed approach 
authorship attribution aa aim to identify the author of a set of document traditional study in this area often assume that there are a large set of labeled document available for training however in the real life it is hard or expensive to collect a large set of labeled data for example in the online review domain most reviewer author only write a few review which are not enough to serve a the training data for accurate classification in this paper we present a novel two view co training framework to iteratively identify the author of a few unlabeled data to augment the training set the key idea is to first represent each document a several distinct view and then a co training technique is adopted to exploit the large amount of unlabeled document starting from training text per author we systematically evaluate the effectiveness of co training for authorship attribution with limited labeled data two method and three view are investigated logistic regression lr and support vector machine svm method and character lexical and syntactic view the experimental result show that lr is particularly effective for improving co training in aa and the lexical view performs the best among three view when combined with a lr classifier furthermore the co training framework doe not make much difference between one classifier from two view and two classifier from one view instead it is the learning approach and the view that play a critical role 
query performance prediction qpp is the estimation of the retrieval success for a query without explicit knowledge about relevant document qpp is especially interesting in the context of automatic query expansion aqe based on pseudo relevance feedback prf prf based aqe is known to produce unreliable result when the initial set of retrieved document is poor theoretically a good predictor would allow to selectively apply prf based aqe when performance of the initial result set is good enough thus enhancing the overall robustness of the system qpp would be of great benefit in the context of microblog retrieval a aqe wa the most widely deployed technique for enhancing retrieval performance at trec in this work we study the performance of the state of the art predictor under microblog retrieval condition a well a introducing our own predictor our result show how our proposed predictor outperform the baseline significantly 
a popular strategy for search result diversification is to first retrieve a set of document utilizing a standard retrieval method and then rerank the result we adopt a different perspective on the problem based on data fusion starting from the hypothesis that data fusion can improve performance in term of diversity metric we examine the impact of standard data fusion method on result diversification we take the output of a set of ranker optimized for diversity or not and find that data fusion can significantly improve state of the art diversification method we also introduce a new data fusion method called diversified data fusion which infers latent topic of a query using topic modeling without leveraging outside information our experiment show that data fusion method can enhance the performance of diversification and ddf significantly outperforms existing data fusion method in term of diversity metric 
user frequently interact with web search system on their mobile device via multiple modality including touch and speech these interaction mode are substantially different from the user experience on desktop search a a result system designer have new challenge and question around understanding the intent on these platform in this paper we study the query reformulation pattern in mobile log we group query reformulations based on their input method into four category text text text voice voice text and voice voice we discus the unique characteristic of each of these group by comparing them against each other and desktop log we also compare the distribution of reformulation type e g adding dropping word against desktop log and show that there are new class of reformulations that are caused by error in speech recognition our result suggest that user do not tend to switch between different input type e g voice and text voice to text switch are largely caused by speech recognition error and text to voice switch are unlikely to be about the same intent 
relevation is a system for performing relevance judgement for information retrieval evaluation relevation is web based fully configurable and expandable it allows researcher to effectively collect assessment and additional qualitative data the system is easily deployed allowing assessor to smoothly perform their relevance judging task even remotely relevation is available a an open source project at http ielab github io relevation 
entity centric document filtering is the task of analyzing a time ordered stream of document and emitting those that are relevant to a specified set of entity e g people place organization this task is exemplified by the trec knowledge base acceleration kba track and ha broad applicability in other modern ir setting in this paper we present a simple yet effective approach based on learning high quality boolean query that can be applied deterministically during filtering we call these boolean statement sufficient query we argue that using deterministic query for entity centric filtering can reduce confounding factor seen in more familiar score then threshold filtering method experiment on two standard datasets show significant improvement over state of the art baseline model 
with the rise in popularity of smart phone there ha been a recent increase in the number of image taken at large social e g festival and world e g natural disaster event which are uploaded to image sharing website such a flickr a with all online image they are often poorly annotated resulting in a difficult retrieval scenario to overcome this problem many photo tag recommendation method have been introduced however these method all rely on historical flickr data which is often problematic for a number of reason including the time lag problem i e in our collection user upload image on average day after taking them meaning training data is often out of date in this paper we develop an image annotation model which exploit textual content from related twitter and wikipedia data which aim to overcome the discussed problem the result of our experiment show and highlight the merit of exploiting social medium data for annotating event image where we are able to achieve recommendation accuracy comparable with a state of the art model 
this study investigated query formulation by user with it cognitive search intent csis which are user need for the cognitive characteristic of document to be retrieved em e g comprehensibility subjectivity and concreteness our four main contribution are summarized a follows i we proposed an example based method of specifying search intent to observe query formulation by user without biasing them by presenting a verbalized task description ii we conducted a questionnaire based user study and found that about half our subject did not input any keywords representing csis even though they were conscious of csis iii our user study also revealed that over of subject occasionally had experience with search with csis while our evaluation demonstrated that the performance of a current web search engine wa much lower when we not only considered user topical search intent but also csis and iv we demonstrated that a machine learning based query expansion could improve the performance for some type of csis our finding suggest user over adapt to current web search engine and create opportunity to estimate csis with non verbal user input 
online review are immensely valuable for customer to make informed purchase decision and for business to improve the quality of their product and service however customer review grow exponentially while varying greatly in quality it is generally very tedious and difficult if not impossible for user to read though the huge amount of review data fortunately review quality evaluation enables a system to select the most helpful review for user decision making previous study predict only the overall review utility about a product and often focus on developing different data feature to learn a quality function for addressing the problem in this paper we aim to select the most helpful review not only at the product level but also at a fine grained product aspect level we propose a novel supervised joint aspect and sentiment model sjasm which is a probabilistic topic modeling framework that jointly discovers aspect and sentiment guided by a review helpfulness metric one key advantage of sjasm is it ability to infer the underlying aspect and sentiment which are indicative of the helpfulness of a review we validate sjasm using publicly available review data and our experimental result demonstrate the superiority of sjasm over several competing model 
evaluation is a fundamental part of information retrieval and in the conventional cranfield evaluation paradigm set of relevance assessment are a fundamental part of test collection this workshop revisits how relevance assessment can be efficiently created seeking to provide a forum for discussion and exploration of the topic 
a large number of image are continuously uploaded to popular photo sharing website and online social community in this demonstration we show a novel application which automatically classifies image in a live photo stream according to their attractiveness for the community based on a number of visual and textual feature the system effectively introduces an additional facet to browse and explore photo collection by highlighting the most attractive photograph and demoting the least attractive 
people often use more than one query when searching for information they revisit search result to re find information and build an understanding of their search need through iterative exploration of query formulation these task are not well supported by search interface and web browser we designed and built searchpanel a chrome browser extension that support people in their ongoing information seeking this extension combine document and process metadata into an interactive representation of the retrieved document that can be used for sense making navigation and re finding document in a real world deployment spanning over two month result show that searchpanel appears to have been primarily used for complex information need in search session with long duration and high number of query when process metadata wa present in the ui searcher in explorative search session submitted more and longer query and interacted more with the serp these result indicate that the process metadata feature in searchpanel seem to be of particular importance for exploratory search 
with the rapid expansion of online social network social network based recommendation ha become a meaningful and effective way of suggesting new item or activity to user in this paper we propose two method to improve the performance of the state of art social network based recommender system snrs which is based on a probabilistic model our first method classifies the correlation between pair of user rating the other is making the system robust to sparse data i e few immediate friend having few common rating with the target user our experimental study demonstrates that our technique significantly improve the accuracy of snrs 
compression of collection such a text database can both reduce space consumption and increase retrieval efficiency through better caching and better exploitation of the memory hierarchy a promising technique is relative lempel ziv coding in which a sample of material from the collection serf a a static dictionary in previous work this method demonstrated extremely fast decoding and good compression ratio while allowing random access to individual item however there is a trade off between dictionary size and compression ratio motivating the search for a compact yet similarly effective dictionary in previous work it wa observed that since the dictionary is generated by sampling some of it selected substring may be discarded with little loss in compression unfortunately simple dictionary pruning approach are ineffective we develop a formal model of our approach based on generating an optimal dictionary for a given collection within a memory bound we generate measure for identification of low value substring in the dictionary and show on a variety of size of text collection that halving the dictionary size lead to only marginal loss in compression ratio this is a dramatic improvement on previous approach 
the query performance prediction task is to estimate retrieval effectiveness with no relevance judgment pre retrieval prediction method operate prior to retrieval time hence these predictor are often based on analyzing the query and the corpus upon which retrieval is performed we propose a em corpus independent approach to pre retrieval prediction which relies on information extracted from wikipedia specifically we present wikipedia based feature that can attest to the effectiveness of retrieval performed in response to a query em regardless of the corpus upon which search is performed empirical evaluation demonstrates the merit of our approach a a case in point integrating the wikipedia based feature with state of the art pre retrieval predictor that analyze the corpus yield prediction quality that is consistently better than that of using the latter alone 
trending search suggestion is leading a new paradigm of image search where user s exploratory search experience is facilitated with the automatic suggestion of trending query existing image search engine however only provide general suggestion and hence cannot capture user s personal interest in this paper we move one step forward to investigate personalized suggestion of trending image search according to user search behavior to this end we propose a learning based framework including two novel component the first component i e trending aware weight regularized matrix factorization ta wrmf is able to suggest personalized trending search query by learning user preference from many user a well a auxiliary common search the second component associate the most representative and trending image with each suggested query the personalized suggestion of image search consists of a trending textual query and it associated trending image the combined textual visual query not only are trending bursty and personalized to user s search preference but also provide the compelling visual aspect of these query we evaluate our proposed learning based framework on a large scale search log with million user and million query in two week from a commercial image search engine the evaluation demonstrate that our system achieve about gain compared with state of the art in term of query prediction accuracy 
recently some recommendation method try to relieve the data sparsity problem of collaborative filtering by exploiting data from user multiple type of behavior however most of the exist method mainly consider to model the correlation between different behavior and ignore the heterogeneity of them which may make improper information transferred and harm the recommendation result to address this problem we propose a novel recommendation model named group latent factor model glfm which attempt to learn a factorization of latent factor space into subspace that are shared across multiple behavior and subspace that are specific to each type of behavior thus the correlation and heterogeneity of multiple behavior can be modeled by these shared and specific latent factor experiment on the real world dataset demonstrate that our model can integrate user multiple type of behavior into recommendation better 
in this study we analyze an educational search engine log for shedding light on k student search behavior in a learning environment we specially focus on query session user and click characteristic and compare the trend to the finding in the literature for general web search engine our analysis help understanding how student search with the purpose of learning in an educational vertical and reveals new direction to improve the search performance in the education domain 
click dwell time is the amount of time that a user spends on a clicked search result many previous study have shown that click dwell time is strongly correlated with result level satisfaction and document relevance accurate estimate of dwell time are therefore important for application such a search satisfaction prediction and result ranking however dwell time can be estimated in different way according to the information available about the search process for example a result reached for the query garfield may involve s of server side dwell time observable to the search engine and s of client side dwell time observable from the browser since search engine can only observe server side action i e activity on the search engine result page server side dwell time are estimated by measuring the time between a search result click and the next search event click or query conversely more detailed information about page dwell time can be obtained via client side method such a web browser toolbars the client side information enables the estimation of more accurate dwell time by measuring the amount of time that a user spends on page of interest either the landing page or page on the full navigation trail in this paper we define three different dwell time i e server side client side and trail dwell time and examine their effectiveness for predicting click satisfaction for this we collect toolbar and search engine log from real user and provide an analysis of dwell time for improving prediction performance moreover we show further improvement in predicting click level satisfaction by combining dwell time with other query feature e g query clarity 
this paper examines the space time performance of in memory conjunctive list intersection algorithm a used in search engine where integer represent document identifier we demonstrate that the combination of bitvectors large skip delta compressed list and url ordering produce superior result to using skip or bitvectors alone we define semi bitvectors a new partial bitvector data structure that store the front of the list using a bitvector and the remainder using skip and delta compression to make it particularly effective we propose that document be ordered so a to skew the posting list to have dense region at the front this can be accomplished by grouping document by their size in a descending manner and then reordering within each group using url ordering in each list the division point between bitvector and delta compression can occur at any group boundary we explore the performance of semi bitvectors using the gov dataset for various number of group resulting in significant space time improvement over existing approach semi bitvectors do not directly support ranking indeed bitvectors are not believed to be useful for ranking based search system because frequency and offset cannot be included in their structure to refute this belief we propose several approach to improve the performance of ranking based search system using bitvectors and leave their verification for future work these proposal suggest that bitvectors and more particularly semi bitvectors warrant closer examination by the research community 
web search engine utilize behavioral signal to develop search experience tailored to individual user to be effective such personalization relies on access to sufficient information about each user s interest and intention for new user or new query profile information may be sparse or non existent to handle these case and perhaps also improve personalization for those with profile search engine can employ signal from user who are similar along one or more dimension i e those in the same cohort in this paper we describe a characterization and evaluation of the use of such cohort modeling to enhance search personalization we experiment with three pre defined cohort topic location and top level domain preference independently and in combination and also evaluate method to learn cohort dynamically we show via extensive experimentation with large scale log from a commercial search engine that leveraging cohort behavior can yield significant relevance gain when combined with a production search engine ranking algorithm that us similar class of personalization signal but at the individual searcher level additional experiment show that our gain can be extended when we dynamically learn cohort and target easily identifiable class of ambiguous or unseen query 
unlike in general recommendation scenario where a user ha only a single role user in trust rating network e g epinions are associated with two different role simultaneously a a truster and a a trustee with different role user can show distinct preference for rating item which the previous approach do not involve moreover based on explicit single link between two user existing method can not capture the implicit correlation between two user who are similar but not socially connected in this paper we propose to learn dual role preference truster trustee specific preference for trust aware recommendation by modeling explicit interaction e g rating and trust and implicit interaction in particular local link structure of trust network are exploited a two regularization term to capture the implicit user correlation in term of truster trustee specific preference using a real world and open dataset we conduct a comprehensive experimental study to investigate the performance of the proposed model rorec the result show that rorec outperforms other trust aware recommendation approach in term of prediction accuracy 
this paper present expertime a web based system for tracking expertise over time we visualize a person s expertise profile on a timeline where we detect and characterize change in the focus or topic of expertise it is possible to zoom in on a given time period in order to examine the underlying data that is used a supporting evidence it is also possible to perform visual and quantitative comparison of two arbitrarily selected time period in a highly interactive environment we invite profile owner to evaluate and fine tune their profile and to leave feedback 
recommender system using collaborative filtering technique are capable of make personalized prediction however these system are highly vulnerable to profile injection attack group attack are attack that target a group of item instead of one and there are common attribute among these item such profile will have a good probability of being similar to a large number of user profile making them hard to detect we propose a novel technique for identifying group attack profile which us an improved metric based on degree of similarity with top neighbor degsim and rating deviation from mean agreement rdma we also extend our work with a detailed analysis of target item rating pattern experiment show that the combined method can improve detection rate in user based recommender system 
hierarchical multi label classification assigns a document to multiple hierarchical class in this paper we focus on hierarchical multi label classification of social text stream concept drift complicated relation among class and the limited length of document in social text stream make this a challenging problem our approach includes three core ingredient short document expansion time aware topic tracking and chunk based structural learning we extend each short document in social text stream to a more comprehensive representation via state of the art entity linking and sentence ranking strategy from document extended in this manner we infer dynamic probabilistic distribution over topic by dividing topic into dynamic global topic and local topic for the third and final phase we propose a chunk based structural optimization strategy to classify each document into multiple class extensive experiment conducted on a large real world dataset show the effectiveness of our proposed method for hierarchical multi label classification of social text stream 
with the rise in popularity of smart phone taking and sharing photograph ha never been more openly accessible further photo sharing website such a flickr have made the distribution of photograph easy resulting in an increase of visual content uploaded online due to the laborious nature of annotating image however a large percentage of these image are unannotated making their organisation and retrieval difficult therefore there ha been a recent research focus on the automatic and semi automatic process of annotating these image despite the progress made in this field however annotating image automatically based on their visual appearance often result in unsatisfactory suggestion and a a result these model have not been adopted in photo sharing website many method have therefore looked to exploit new source of evidence for annotation purpose such a image context for example in this demonstration we instead explore the scenario of annotating image taken at a large scale event where evidence can be extracted from a wealth of online textual resource specifically we present a novel tag recommendation system for image taken at a popular music festival which allows the user to select relevant tag from related tweet and wikipedia content thus reducing the workload involved in the annotation process 
the query performance prediction task ha been described a estimating retrieval effectiveness in the absence of relevance judgment the expectation throughout the year were that improved prediction technique would translate to improved retrieval approach however this ha not yet happened herein we provide an in depth analysis of why this is the case to this end we formalize the prediction task in the most general probabilistic term using this formalism we draw novel connection between task and method used to address these task in federated search fusion based retrieval and query performance prediction furthermore using formal argument we show that the ability to estimate the probability of effective retrieval with no relevance judgment i e to predict performance implies knowledge of how to perform effective retrieval we also explain why the expectation that using previously proposed query performance predictor would help to improve retrieval effectiveness wa not realized this is due to a misalignment with the actual goal for which these predictor were devised ranking query based on the presumed effectiveness of using them for retrieval over a corpus with a specific retrieval method focusing on this specific prediction task namely query ranking by presumed effectiveness we present a novel learning to rank based approach that us markov random field the resultant prediction quality substantially transcends that of state of the art predictor 
a query considered in isolation provides limited information about the searcher s interest previous work ha considered various type of user behavior e g click and dwell time to obtain a better understanding of the user s intent we consider the searcher s search and page view history using search log from a commercial search engine we i investigate the impact of feature derived from user behavior on reranking a generic ranked list ii optimally integrate the contribution of user behavior and candidate document by learning their relative importance per query based on similar user we use dwell time on clicked url when estimating the relevance of document for a query and perform bayesian probabilistic matrix factorization a smoothing to predict the relevance considering user behavior achieves better ranking than non personalized ranking aggregation of user behavior and query document feature with a user dependent adaptive weight outperforms combination with a fixed uniform value 
in the current web era the popularity of web resource fluctuates ephemerally based on trend and social interest a a result content based relevance signal are insufficient to meet user constantly evolving information need in searching for web item incorporating future popularity into ranking is one way to counter this however predicting popularity a a third party a in the case of general search engine is difficult in practice due to their limited access to item view history to enable popularity prediction externally without excessive crawling we propose an alternative solution by leveraging user comment which are more accessible than view count due to the sparsity of comment traditional solution that are solely based on view history do not perform well to deal with this sparsity we mine comment to recover additional signal such a social influence by modeling comment a a time aware bipartite graph we propose a regularization based ranking algorithm that account for temporal social influence and current popularity factor to predict the future popularity of item experimental result on three real world datasets crawled from youtube flickr and last fm show that our method consistently outperforms competitive baseline in several evaluation task 
topic model have been widely used for text analysis previous topic model have enjoyed great success in mining the latent topic structure of text document with many effort made on endowing the resulting document topic distribution with different motivation however none of these model have paid any attention on the resulting topic word distribution since topic word distribution also play an important role in the modeling performance topic model which emphasize only the resulting document topic representation but pay le attention to the topic term distribution are limited in this paper we propose the orthogonalized topic model otm which imposes an orthogonality constraint on the topic term distribution we also propose a novel model fitting algorithm based on the generalized expectation maximization algorithm and the newthon raphson method quantitative evaluation of text classification demonstrates that otm outperforms other baseline model and indicates the important role played by topic orthogonalizing 
it is crucial for query auto completion to accurately predict what a user is typing given a query prefix and it context e g previous query conventional context aware approach often produce relevant query to the context the purpose of this paper is to investigate the feasibility of exploiting the context to learn user reformulation behavior for boosting prediction performance we first conduct an in depth analysis of how the user reformulate their query based on the analysis we propose a supervised approach to query auto completion where three kind of reformulation related feature are considered including term level query level and session level feature these feature carefully capture how the user change preceding query along the query session extensive experiment have been conducted on the large scale query log of a commercial search engine the experimental result demonstrate a significant improvement over competitive baseline 
we investigate the application of a light weight approach to result list clustering for the purpose of diversifying search result we introduce a novel post retrieval approach which is independent of external information or even the full text content of retrieved document only the retrieval score of a document is used our experiment show that this novel approach is beneficial to effectiveness albeit only on certain baseline system the fact that the method work indicates that the retrieval score is potentially exploitable in diversity 
axiomatic approach provides a systematic way to think about heuristic identify the weakness of existing method and optimize the existing method accordingly this tutorial aim to promote axiomatic thinking that can benefit not only the study of ir model but also the method for many ir application 
retrievability is an important and interesting indicator that can be used in a number of way to analyse information retrieval system and document collection rather than focusing totally on relevance retrievability examines what is retrieved how often it is retrieved and whether a user is likely to retrieve a document or not this is important because a document need to be retrieved before it can be judged for relevance in this tutorial we shall explain the concept of retrievability along with a number of retrievability measure how it can be estimated and how it can be used for analysis since retrieval precedes relevance we shall also provide an overview of how retrievability relates to effectiveness describing some of the insight that researcher have discovered so far we shall also show how retrievability relates to efficiency and how the theory of retrievability can be used to improve both effectiveness and efficiency then we shall provide an overview of the different application of retrievability such a search engine bias corpus profiling etc before wrapping up with challenge and opportunity the final session of the day will look at example problem and way to analyse and apply retrievability to other problem and domain 
information retrieval test collection traditionally use a combination of automatic and manual run to create a pool of document to be judged the quality of the final judgment produced for a collection is a product of the variety across each of the run submitted and the pool depth in this work we explore fully automated approach to generating a pool by combining a simple voting approach with machine learning from document retrieved by automatic run we are able to identify a large portion of relevant document that would normally only be found through manual run our initial result are promising and can be extended in future study to help test collection curator ensure proper judgment coverage is maintained across complete document collection 
with the development of microblog service ten of thousand of message are produced every day and recommending useful message according to user interest is recognized a an effective way to overcome the information overload problem collaborative filtering which rooted from recommender system ha been utilized for microblog recommendation where social relationship information can help improve the recommendation performance however most of existing method only consider the static relationship i e the following relationship which totally ignores the relationship conveyed by user repost behavior to explore the effect of behavior based relationship on recommendation we propose an interaction based collaborative filtering ibcf approach specifically we first use topic model to analyze user interactive behavior and measure the topic specific relationship strength then we incorporate the relationship factor into the matrix factorization framework experimental result show that compared to the current popular social recommendation method ibcf can achieve better performance on the map and ndcg evaluation measure and have better interpretability for the recommended result 
online search evaluation metric are typically derived based on implicit feedback from the user for instance computing the number of page click number of query or dwell time on a search result in a recent paper dupret and lalmas introduced a new metric called absence time which us the time interval between successive session of user to measure their satisfaction with the system they evaluated this metric on a version of yahoo answer in this paper we investigate the effectiveness of absence time in evaluating new feature in a web search engine such a new ranking algorithm or a new user interface we measured the variation of absence time to the effect of experiment performed on a search engine our finding show that the outcome of absence time agreed with the judgement of human expert performing a thorough analysis of a wide range of online and offline metric in out of these case we also investigated the relationship between absence time and a set of commonly used covariates feature such a the number of query and click in the session our result suggest that user are likely to return to the search engine sooner when their previous session ha more query and more click 
this paper investigates the temporal cluster hypothesis in search task where time play an important role do relevant document tend to cluster together in time we explore this question in the context of tweet search and temporal feedback starting with an initial set of result from a baseline retrieval model we estimate the temporal density of relevant document which is then used for result reranking our contribution lie in a method to characterize this temporal density function using kernel density estimation with and without human relevance judgment and an approach to integrating this information into a standard retrieval model experiment on trec datasets confirm that our temporal feedback formulation improves search effectiveness thus providing support for our hypothesis our approach out performs both a standard baseline and previous temporal retrieval model temporal feedback improves over standard lexical feedback with and without human judgment illustrating that temporal relevance signal exist independently of document content 
in many online news service user often write comment towards news in subjective emotion such a sadness happiness or anger knowing such emotion can help understand the preference and perspective of individual user and therefore may facilitate online publisher to provide more relevant service to user although building emotion classifier is a practical task it highly depends on sufficient training data that is not easy to be collected directly and the manually labeling work of comment can be quite labor intensive also online news ha different domain which make the problem even harder a different word distribution of the domain require different classifier with corresponding distinct training data this paper address the task of emotion tagging for comment of cross domain online news the cross domain task is formulated a a transfer learning problem which utilizes a small amount of labeled data from a target news domain and abundant labeled data from a different source domain this paper proposes a novel framework to transfer knowledge across different news domain more specifically different approach have been proposed when the two domain share the same set of emotion category or use different category an extensive set of experimental result on four datasets from popular online news service demonstrates the effectiveness of our proposed model in cross domain emotion tagging for comment of online news in both the scenario of sharing the same emotion category or having different category in the source and target domain 
session search is a complex search task that involves multiple search iteration triggered by query reformulations we observe a markov chain in session search user s judgment of retrieved document in the previous search iteration affect user s action in the next iteration we thus propose to model session search a a dual agent stochastic game the user agent and the search engine agent work together to jointly maximize their long term reward the framework which we term win win search is based on partially observable markov decision process we mathematically model dynamic in session search including decision state query change click and reward a a cooperative game between the user and the search engine the experiment on trec and session datasets show a statistically significant improvement over the state of the art interactive search and session search algorithm 
while many multidimensional model of relevance have been posited prior study have been largely exploratory rather than confirmatory lacking a methodological framework to quantify the relationship among factor or measure model fit to observed data many past model could not be empirically tested or falsified to enable more positivist experimentation xu and chen proposed a psychometric framework for multidimensional relevance modeling however we show their framework exhibit several methodological limitation which could call into question the validity of finding drawn from it in this work we identify and address these limitation scale their methodology via crowdsourcing and describe quality control method from psychometrics which stand to benefit crowdsourcing ir study in general methodology we describe for relevance judging is expected to benefit both human centered and system centered ir 
in this tutorial we will present review and compare the most popular evaluation metric for some of the most salient information related task covering i information retrieval ii clustering and iii filtering the tutorial will make a special emphasis on the specification of constraint for suitable metric in each of the three task and on the systematic comparison of metric according to such constraint the last part of the tutorial will investigate the challenge of combining and weighting metric 
click log provide a unique and highly valuable source of human judgment on ad relevance however click are heavily biased by lot of factor two main factor that are widely acknowledged to be the most influential one are neighboring ad and presentation order the latter is referred to a positional effect a popular practice to recover the ad quality cleaned from positional bias is to adopt click model based on examination or cascade hypothesis originally developed for organic search in this paper we show the strong evidence that this practice is far from perfection when considering the top ad block on a search engine result page serp we show that cascade hypothesis is the most questionable one because of important difference between organic and sponsored search result that may encourage user to analyze the whole ad block before clicking additionally we design a testing setup for an unbiased evaluation of click model prediction accuracy 
modeling user behavior on a search engine result page is important for understanding the user and supporting simulation experiment a result page become more complex click model evolve a well in order to capture additional aspect of user behavior in response to new form of result presentation we propose a method for evaluating the intuitiveness of vertical aware click model namely the ability of a click model to capture key aspect of aggregated result page such a vertical selection item selection result presentation and vertical diversity this method allows u to isolate model component and therefore give a multi faceted view on a model s performance we argue that our method can be used in conjunction with traditional click model evaluation metric such a log likelihood or perplexity in order to demonstrate the power of our method in situation where result page can contain more than one type of vertical e g image and news we extend the previously studied federated click model such that it model user click on such page our evaluation method yield non trivial yet interpretable conclusion about the intuitiveness of click model highlighting their strength and weakness 
work on using relevance feedback for retrieval ha focused on the single retrieved list setting that is an initial document list is retrieved in response to the query and feedback for the most highly ranked document is used to perform a second search we address a setting wherein the list for which feedback is provided result from fusing several intermediate retrieved list accordingly we devise method that utilize the feedback while exploiting the special characteristic of the fusion setting specifically the feedback serf two different yet complementary purpose the first is to directly rank the pool of document in the intermediate list the second is to estimate the effectiveness of the intermediate list for improved re fusion in addition we present a meta fusion method that us the feedback for these two purpose simultaneously empirical evaluation demonstrates the merit of our approach a a case in point the retrieval performance is substantially better than that of using the relevance feedback a in the single list setting the performance also substantially transcends that of a previously proposed approach to utilizing relevance feedback in fusion based retrieval 
understanding how people interact when searching is central to the study of interactive information retrieval iir most of the prior work ha either been conceptual observational or empirical while this ha led to numerous insight and finding regarding the interaction between user and system the theory ha lagged behind in this paper we extend the recently proposed search economic theory to make the model more realistic we then derive eight interaction based hypothesis regarding search behaviour to validate the model we explore whether the search behaviour of thirty six participant from a lab based study is consistent with the theory our analysis show that observed search behaviour are in line with predicted search behaviour and that it is possible to provide credible explanation for such behaviour this work describes a concise and compact representation of search behaviour providing a strong theoretical basis for future iir research 
circumlocution is when many word are used to describe what could be said with fewer e g a machine that take moisture out of the air instead of dehumidifier web search is a perfect backdrop for circumlocution where people struggle to name what they seek in some domain not knowing the correct term can have a significant impact on the search result that are retrieved we study the medical domain where professional medical term are not commonly known and where the consequence of not knowing the correct term can impact the accuracy of surfaced information a well a escalation of anxiety and ultimately the medical care sought given a free form colloquial health search query our objective is to find the underlying professional medical term the problem is complicated by the fact that people issue quite varied query to describe what they have machine learning algorithm can be brought to bear on the problem but there are two key complexity creating high quality training data and identifying predictive feature to our knowledge no prior work ha been able to crack this important problem due to the lack of training data we give novel solution and demonstrate their efficacy via extensive experiment greatly improving over the prior art 
we present a novel approach to the cluster labeling task using fusion method the core idea of our approach is to weigh label suggested by any labeler according to the estimated labeler s decisiveness with respect to each of it suggested label we hypothesize that a cluster labeler s labeling choice for a given cluster should remain stable even in the presence of a slightly incomplete cluster data using state of the art cluster labeling and data fusion method evaluated over a large data collection of cluster we demonstrate that overall the cluster labeling fusion method that further consider the labeler s decisiveness provide the best labeling performance 
in the age of big data automatic method for creating summary of document become increasingly important in this paper we propose a novel unsupervised method for multi document summarization in an unsupervised and language independent fashion this approach relies on the strength of word association in the set of document to be summarized the summary are generated by picking sentence which cover the most specific word association of the document s we measure the performance on the duc dataset our experiment indicate that the proposed method is the best performing unsupervised summarization method in the state of the art that make no use of human curated knowledge base 
web search ha seen two big change recently rapid growth in mobile search traffic and an increasing trend towards providing answer like result for relatively simple information need e g weather today such result display the answer or relevant information on the search page itself without requiring a user to click while click on organic search result have been used extensively to infer result relevance and search satisfaction click on answer like result are often rare or meaningless making it challenging to evaluate answer quality together these call for better measurement and understanding of search satisfaction on mobile device in this paper we studied whether tracking the browser viewport visible portion of a web page on mobile phone could enable accurate measurement of user attention at scale and provide good measurement of search satisfaction in the absence of click focusing on answer like result in web search we designed a lab study to systematically vary answer presence and relevance to the user s information need obtained satisfaction rating from user and simultaneously recorded eye gaze and viewport data a user performed search task using this ground truth we identified increased scrolling past answer and increased time below answer a clear measurable signal of user dissatisfaction with answer while the viewport may contain three to four result at any given time we found strong correlation between gaze duration and viewport duration on a per result basis and that the average user attention is focused on the top half of the phone screen suggesting that we may be able to scalably and reliably identify which specific result the user is looking at from viewport data alone 
we study the problem of linking information between different idiomatic usage of the same language for example colloquial and formal language we propose a novel probabilistic topic model called multi idiomatic lda milda it modeling principle follow the intuition that certain word are shared between two idiom of the same language while other word are non shared that is idiom specific we demonstrate the ability of our model to learn relation between cross idiomatic topic in a dataset containing product description and review we intrinsically evaluate our model by the perplexity measure following that a an extrinsic evaluation we present the utility of the new milda topic model in a recently proposed ir task of linking pinterest pin given in colloquial english on the user side to online webshops given in formal english on the retailer side we show that our multi idiomatic model outperforms the standard monolingual lda model and the pure bilingual lda model both in term of perplexity and map score in the ir task 
reputation management expert have to monitor among others twitter constantly and decide at any given time what is being said about the entity of interest a company organization personality solving this reputation monitoring problem automatically a a topic detection task is both essential manual processing of data is either costly or prohibitive and challenging topic of interest for reputation monitoring are usually fine grained and suffer from data sparsity we focus on a solution for the problem that i learns a pairwise tweet similarity function from previously annotated data using all kind of content based and twitter based feature ii applies a clustering algorithm on the previously learned similarity function our experiment indicate that i twitter signal can be used to improve the topic detection process with respect to using content signal only ii learning a similarity function is a flexible and efficient way of introducing supervision in the topic detection clustering process the performance of our best system is substantially better than state of the art approach and get close to the inter annotator agreement rate a detailed qualitative inspection of the data further reveals two type of topic detected by reputation expert reputation alert issue which usually spike in time and organizational topic which are usually stable across time 
most existing tag based social image search engine present search result a a ranked list of image which cannot be consumed by user in a natural and intuitive manner in this paper we present a novel concept preserving image search result summarization algorithm named prism prism exploit both visual feature and tag of the search result to generate high quality summary which not only break the result into visually and semantically coherent cluster but it also maximizes the coverage of the summary w r t the original search result it first construct a visual similarity graph where the node are image in the search result and the edge represent visual similarity between pair of image this graph is optimally decomposed and compressed into a set of concept preserving subgraphs based on a set of summarization objective image in a concept preserving subgraph are visually and semantically cohesive and are described by a minimal set of tag or concept lastly one or more exemplar image from each subgraph is selected to form the exemplar summary of the result set through empirical study we demonstrate the effectiveness of prism against state of the art image summarization and clustering algorithm 
the aim of risk sensitive evaluation is to measure when a given information retrieval ir system doe not perform worse than a corresponding baseline system for any topic this paper argues that risk sensitive evaluation is akin to the underlying methodology of the student s t test for matched pair hence we introduce a risk reward tradeoff measure trisk that generalises the existing urisk measure a used in the trec web track s risk sensitive task while being theoretically grounded in statistical hypothesis testing and easily interpretable in particular we show that trisk is a linear transformation of the t statistic which is the test statistic used in the student s t test this inherent relationship between trisk and the t statistic turn risk sensitive evaluation from a descriptive analysis to a fully fledged inferential analysis specifically we demonstrate using past trec data that by using the inferential analysis technique introduced in this paper we can decide whether an observed level of risk for an ir system is statistically significant and thereby infer whether the system exhibit a real risk and determine the topic that individually lead to a significant level of risk indeed we show that the latter permit a state of the art learning to rank algorithm lambdamart to focus on those topic in order to learn effective yet risk averse ranking system 
online service rely on machine identifier to tailor service such a personalized search and advertising to individual user the assumption made is that each identifier comprises the behavior of a single person however shared machine usage is common and in these case the activity of multiple user may be generated under a single identifier creating a potentially noisy signal for application such a search personalization we propose enhancing web search personalization with method that can disambiguate among different user of a machine thus connecting the current query with the appropriate search history using log containing both person and machine identifier and log from a popular commercial search engine we learn model that accurately assign observed search behavior to each of different user this information is then used to augment existing personalization method that are currently based only on machine identifier we show that this new capability to infer user can be used to improve the performance of existing personalization method the early finding of our research are promising and have implication for search personalization 
when applying learning to rank algorithm in real search application noise in human labeled training data becomes an inevitable problem which will affect the performance of the algorithm previous work mainly focused on studying how noise affect ranking algorithm and how to design robust ranking algorithm in our work we investigate what inherent characteristic make training data robust to label noise the motivation of our work come from an interesting observation that a same ranking algorithm may show very different sensitivity to label noise over different data set we thus investigate the underlying reason for this observation based on two typical kind of learning to rank algorithm i e pairwise and listwise method and three different public data set i e ohsumed td and mslr web k we find that when label noise increase in training data it is the emph document pair noise ratio i e emph pnoise rather than emph document noise ratio i e emph dnoise that can well explain the performance degradation of a ranking algorithm 
with the prevalence of the geo position enabled device and service a rapidly growing amount of tweet are associated with geo tag consequently the real time search on geo tagged twitter stream ha attracted great attention in this paper we advocate the significance of the co occurrence of keywords for the geo tagged tweet data analytics which is overlooked by existing study particularly we formally introduce the problem of identifying local frequent keyword co occurrence pattern over the geo tagged twitter stream namely lfp xspace query to accommodate the high volume and the rapid update of the twitter stream we develop an inverted kmv sketch ik xspace sketch for short structure to capture the co occurrence of keywords in limited space then efficient algorithm are developed based on ik xspace sketch to support lfp xspace query a well a it variant the extensive empirical study on real twitter dataset confirms the effectiveness and efficiency of our approach 
twitter a one of the most popular social medium platform provides a convenient way for people to communicate and interact with each other it ha been well recognized that influence exists during user interaction some pioneer study on finding influential user have been reported in the literature but they do not distinguish different influence role which are of great value for various marketing purpose in this paper we move a step forward trying to further distinguish influence role of twitter user in a certain topic by defining three view of feature relating to topic sentiment and popularity respectively we propose a multi view influence role clustering mirc algorithm to group twitter user into five category experimental result show the effectiveness of the proposed approach in inferring influence role 
twitter is a popular platform for sharing activity plan and opinion through tweet user often reveal their location information and short term visiting plan in this paper we are interested in extracting fine grained location mentioned in tweet with temporal awareness more specifically we like to extract each point of interest poi mention in a tweet and predict whether the user ha visited is currently at or will soon visit this poi our proposed solution named petar consists of two main component a poi inventory and a time aware poi tagger the poi inventory is built by exploiting the crowd wisdom of foursquare community it contains not only the formal name of poi but also the informal abbreviation the poi tagger based on conditional random field crf model is designed to simultaneously identify the poi and resolve their associated temporal awareness in our experiment we investigated four type of feature i e lexical grammatical geographical and bilou schema feature for time aware poi extraction with the four type of feature petar achieves promising extraction accuracy and outperforms all baseline method 
using the inferred measure framework is a popular choice for constructing test collection when the target document set is too large for pooling to be a viable option within the framework different amount of assessing effort is placed on different region of the ranked list a defined by a sampling strategy the sampling strategy is critically important to the quality of the resultant collection but there is little published guidance a to the important factor this paper address this gap by examining the effect on collection quality of different sampling strategy within the inferred measure framework the quality of a collection is measured by how accurately it distinguishes the set of significantly different system pair top k pooling is competitive though not the best strategy because it cannot distinguish topic with large relevant set size incorporating a deep very sparsely sampled stratum is a poor choice strategy that include a top pool create better collection than those that do not a well a allow precision score to be directly computed 
pseudo relevance feedback is an effective technique to improve the performance of ad hoc information retrieval traditionally the expansion term are extracted either according to the term distribution in the feedback document or according to both the term distribution in the feedback document and in the whole document collection however most of the existing model employ a single term frequency normalization mechanism or criterion that cannot take into account various aspect of a term s saliency in the feedback document in this paper we propose a simple and heuristic but effective model in which three term frequency transformation technique are integrated to capture the saliency of a candidate term associated with the original query term in the feedback document through evaluation and comparison on six trec collection we show that our proposed model is effective and generally superior to the recent progress of relevance feedback model 
music information retrieval and music recommendation are seeing a paradigm shift towards method that incorporate user context aspect however structured experiment on a standardized music dataset to investigate the effect of doing so are scarce in this paper we compare performance of various combination of collaborative filtering and geospatial a well a cultural user model for the task of music recommendation to this end we propose a geospatial model that us gps coordinate and a cultural model that us semantic location continent country and state of the user we conduct experiment on a novel standardized music collection the million musical tweet dataset of listening event extracted from microblogs overall we find that modeling listener location via gaussian mixture model and computing similarity from these outperforms both cultural user model and collaborative filtering 
vocabulary mismatch ha long been recognized a one of the major issue affecting search effectiveness ineffective query usually fail to incorporate important term and or incorrectly include inappropriate keywords however in this paper we show another cause of reduced search performance sometimes user issue reasonable query term but system cannot identify the correct property of those term and take advantage of the property specifically we study two distinct type of term that exist in all search query necessary term for which term occurrence alone is indicative of document relevance and frequent term for which the relative term frequency is indicative of document relevance within the set of document where the term appears we evaluate these two property of query term in a dataset result show that only of the term are both necessary and frequent while another only hold one of the property and the final third do not hold any of the property however existing retrieval model do not clearly distinguish term with the two property and consider them differently we further show the great potential of improving retrieval model by treating term with distinct property differently 
evaluation a a service eaas is a new methodology that enables community wide evaluation and the construction of test collection on document that cannot be distributed the basic idea is that evaluation organizer provide a service api through which the evaluation task can be completed however this concept violates some of the premise of traditional pool based collection building and thus call into question the quality of the resulting test collection in particular the service api might restrict the diversity of run that contribute to the pool this might hamper innovation by researcher and lead to incomplete judgment pool that affect the reusability of the collection this paper show that the distinctiveness of the retrieval run used to construct the first test collection built using eaas the trec microblog collection is not substantially different from that of the trec ad hoc collection a high quality collection built using traditional pooling further analysis using the leave out uniques test suggests that pool from the microblog collection are le complete than those from trec although both collection benefit from the presence of distinctive and effective manual run although we cannot yet generalize to all eaas implementation our analysis reveal no obvious flaw in the test collection built using the methodology in the trec microblog track 
extensive previous research ha shown that searcher often require assistance with query formulation and refinement yet it is not clear what kind of assistance is most useful and how effective it is both objectively e g in term of task success and subjectively e g in term of searcher perception of the search difficulty this work describes the result of a controlled user study comparing the effect of providing specific v generic search hint on search success and satisfaction our result indicate that specific search hint tend to effectively improve searcher success rate and reduce perceived effort while generic one can be detrimental in both search effectiveness and user satisfaction the result of this study are an important step towards the design of future search system that could effectively assist and guide the user in accomplishing complex search task 
linkedin is the world s largest professional network with over million member one of the primary activity on the site is people search for which linkedin member are both the user and the corpus this paper present insight about people search behavior on linkedin based on a log analysis and a user study in particular it examines the role that network distance play in name search and non name search for name search user primarily click on only one of the result and closer network distance lead to higher click through rate in contrast for non name search user are more likely to click on multiple result that are not in their existing connection but with whom they have shared connection the result show that while network distance contributes significantly to linkedin search engagement in general it role varies dramatically depending on the type of search query 
people often read summary of news article in order to get reliable information about an event or a topic however the information expressed in news article is not always certain and some sentence contain uncertain information about the event existing summarization system do not consider whether a sentence in news article is certain or not in this paper we propose a novel system called ctsum to incorporate the new factor of information certainty into the summarization task we first analyze the sentence in news article and automatically predict the certainty level of sentence by using the support vector regression method with a few useful feature the predicted certainty score are then incorporated into a summarization system with a graph based ranking algorithm experimental result on a manually labeled dataset verify the effectiveness of the sentence certainty prediction technique and experimental result on the duc dataset show that our new summarization system cannot only produce summary with better content quality but also produce summary with higher certainty 
obesity and it associated health consequence such a high blood pressure and cardiac disease affect a significant proportion of the world s population at the same time the popularity of location based service lb and recommender system is continually increasing with improvement in mobile technology we observe that the health domain lack a suggestion system that focus on healthy lifestyle choice we introduce the mobile application fityou which dynamically generates recommendation according to the user s current location and health condition a a real time lb it utilizes preference determined from user history and health information from a biometric profile the system wa developed upon a top performing contextual suggestion system in both trec and contextual suggestion track 
influence maximization is the problem of finding a set of seed node in social network for maximizing the spread of influence traditionally researcher view influence propagation a a stochastic process and formulate the influence maximization problem a a discrete optimization problem thus most previous work focus on finding efficient and effective heuristic algorithm within the greedy framework in this paper we view the influence maximization problem from the perspective of data reconstruction and propose a novel framework named textsl data reconstruction for influence maximization drim in our framework we first construct an influence matrix each row of which is the influence of a node to other node then we select k most informative row to reconstruct the matrix and the corresponding node are the seed node which could maximize the influence spread finally we evaluate our framework on two real world data set and the result show that drim is at least a effective a the traditional greedy algorithm 
while microblogging ha emerged a an important information sharing and communication platform it ha also become a convenient venue for spammer to overwhelm other user with unwanted content currently spammer detection in microblogging focus on using social networking information but little on content analysis due to the distinct nature of microblogging message first label information is hard to obtain second the text in microblogging are short and noisy a we know spammer detection ha been extensively studied for year in various medium e g email sm and the web motivated by abundant resource available in the other medium we investigate whether we can take advantage of the existing resource for spammer detection in microblogging while people accept that text in microblogging are different from those in other medium there is no quantitative analysis to show how different they are in this paper we first perform a comprehensive linguistic study to compare spam across different medium inspired by the finding we present an optimization formulation that enables the design of spammer detection in microblogging using knowledge from external medium we conduct experiment on real world twitter datasets to verify whether email sm and web spam resource help and how different medium help for spammer detection in microblogging 
can the activity pattern of page use during information search session discriminate between different type of information seeking task we model sequence of interaction with search result and content page during information search session two representation are created the sequence of page use and a cognitive representation of page interaction the cognitive representation is based on logged eye movement pattern of textual information acquisition via the reading process page sequence action from task session n in a user study are analyzed the study task differed from one another in basic dimension of complexity specificity level and the type of information product intellectual or factual the result show that difference in task type can be measured at both the level of observation of page type sequence and at the level of cognitive activity on the page we discus the implication for personalization of search system measurement of task similarity and the development of user centered information system that can support the user s current and expected search intention 
existing recommender system usually model item a static unchanging in attribute description and feature however in domain such a mobile apps a version update may provide substantial change to an app a update reflected by an increment in it version number may attract a consumer s interest for a previously unappealing version version description constitute an important recommendation evidence source a well a a basis for understanding the rationale for a recommendation we present a novel framework that incorporates feature distilled from version description into app recommendation we use a semi supervised topic model to construct a representation of an app s version a a set of latent topic from version metadata and textual description we then discriminate the topic based on genre information and weight them on a per user basis to generate a version sensitive ranked list of apps for a target user incorporating our version feature with state of the art individual and hybrid recommendation technique significantly improves recommendation quality an important advantage of our method is that it target particular version of apps allowing previously disfavored apps to be recommended when user relevant feature are added 
information retrieval ir and information privacy security are two fast growing computer science discipline there are many synergy and connection between these two discipline however there have been very limited effort to connect the two important discipline on the other hand due to lack of mature technique in privacy preserving ir concern about information privacy and security have become serious obstacle that prevent valuable user data to be used in ir research such a study on query log social medium tweet session and medical record retrieval this privacy preserving ir workshop aim to spur research that brings together the research field of ir and privacy security and research that mitigates privacy threat in information retrieval by constructing novel algorithm and tool that enable web user to better understand associated privacy risk 
microblogging service have emerged a an essential way to strengthen the communication among individual one of the most important feature of microblog over traditional social network is the extensive proliferation in information diffusion a the outbreak of information diffusion often brings in valuable opportunity or devastating effect it will be beneficial if a mechanism can be provided to predict whether a piece of information will become viral and which part of the network will participate in propagating this information in this work we define three type of influence namely interest oriented influence social oriented influence and epidemic oriented influence that will affect a user s decision on whether to perform a diffusion action we propose a diffusion targeted influence model to differentiate and quantify various type of influence further we model the problem of diffusion prediction by factorizing a user s intention to transmit a microblog into these influence the learned prediction model is then used to predict the future diffusion state of any new microblog we conduct experiment on a real world microblogging dataset to evaluate our method and the result demonstrate the superiority of the proposed framework a compared to the state of the art approach 
context aware recommendation car can lead to significant improvement in the relevance of the recommended item by modeling the nuanced way in which context influence preference the dominant approach in context aware recommendation ha been the multidimensional latent factor approach in which user item and context variable are represented a latent feature in low dimensional space an interaction between a user item and a context variable is typically modeled a some linear combination of their latent feature however given the many possible type of interaction between user item and contextual variable it may seem unrealistic to restrict the interaction among them to linearity to address this limitation we develop a novel and powerful non linear probabilistic algorithm for context aware recommendation using gaussian process the method which we call gaussian process factorization machine gpfm is applicable to both the explicit feedback setting e g numerical rating a in the netflix dataset and the implicit feedback setting i e purchase click we derive stochastic gradient descent optimization to allow scalability of the model we test gpfm on five different benchmark contextual datasets experimental result demonstrate that gpfm outperforms state of the art context aware recommendation method 
spelling correction in the s wa all about algorithm and small dictionary this century it is about mining vast data set of past user behavior simple algorithm and using those to correct mistake the large internet giant are data driven enterprise that use data to transform and continually improve user experience in this talk hugh williams share story about data and how it is used to build internet product and explains why he belief data will transform business a we know them every major company is becoming a data driven company and hugh share example of transformation occurring in health aviation farming and telecommunication he recently joined pivotal a company that is assembling the toolkit that exists in only a few consumer internet company and making that toolkit open and available to every industry including big data platform development framework and an open cloud independent platform a a service he will conclude by sharing detail about pivotal the pivotal vision and roadmap hugh e williams ha been senior vice president of research development at pivotal since january his team build big data technology and development framework and service including pivotal s hadoop spring java framework and greenplum database offering most recently he spent four and a half year a an executive with ebay where he wa responsible for the team that conceived designed and built ebay s user experience search engine big data technology and platform prior to joining ebay he managed an r d team at microsoft s bing for four and a half year spent over ten year researching and developing search technology and ran his own startup and consultancy for several year he ha published over work mostly in the field of information retrieval including two book for o reilly medium inc he hold u s patent with many more pending he ha a phd from rmit university in australia 
how assessor and end user judge the relevance of image ha been studied in information science and information retrieval for a considerable time the criterion by which assessor judge relevance ha been intensively studied and there ha been a large amount of work which ha investigated how relevance judgment for test collection can be more cheaply generated such a through crowd sourcing relatively little work ha investigated the process individual assessor go through to judge the relevance of an image in this paper we focus on the process by which relevance is judged for image and in particular the degree of effort a user must expend to judge relevance for different topic result suggest that topic difficulty and how semantic visual a topic is impact user performance and perceived effort 
many query are submitted to search engine by right clicking the marked text i e the query in web browser because the document being read by the searcher often provides sufficient contextual information for the query search engine could provide much more relevant search result if the query is augmented by the contextual information captured from the source document how to extract the right contextual information from the source document is the main focus of this study to this end we evaluate text component extraction scheme and feature extraction scheme the former determines from which text component e g title meta data or paragraph containing the selected query to extract contextual information the latter determines which word or phrase to extract in total combination are evaluated and our evaluation result show that noun phrase extracted from all paragraph that contain the query word is the best option 
this paper address the problem of identifying local expert in social medium system like twitter local expert in contrast to general topic expert have specialized knowledge focused around a particular location and are important for many application including answering local information need and interacting with community expert and yet identifying these expert is difficult hence in this paper we propose a geo spatial driven approach for identifying local expert that leverage the fine grained gps coordinate of million of twitter user we propose a local expertise framework that integrates both user topical expertise and their local authority concretely we estimate a user s local authority via a novel spatial proximity expertise approach that leverage over million geo tagged twitter list we estimate a user s topical expertise based on expertise propagation over million geo tagged social connection on twitter we evaluate the proposed approach across query coupled with over individual judgment from amazon mechanical turk we find significant improvement over both general non local expert approach and comparable local expert finding approach 
recent research ha shown that the performance of search engine can be improved by enriching a user s personal profile with information about other user with shared interest in the existing approach group of similar user are often statically determined e g based on the common document that user clicked however these static grouping method are query independent and neglect the fact that user in a group may have different interest with respect to different topic in this paper we argue that common interest group should be dynamically constructed in response to the user s input query we propose a personalisation framework in which a user profile is enriched using information from other user dynamically grouped with respect to an input query the experimental result on query log from a major commercial web search engine demonstrate that our framework improves the performance of the web search engine and also achieves better performance than the static grouping method 
we consider a scenario where a searcher requires both high precision and high recall from an interactive retrieval process such scenario are very common in real life exemplified by medical search legal search market research and literature review when access to the entire data set is available an active learning loop could be used to ask for additional relevance feedback label in order to refine a classifier when data is accessed via search service however only limited subset of the corpus can be considered subset defined by query in that setting relevance feedback ha been used in a query enhancement loop that update a query we describe and demonstrate the effectiveness of req rec requery reclassify a double loop retrieval system that combine iterative expansion of a query set with iterative refinement of a classifier this permit a separation of concern where the query selector s job is to enhance recall while the classifier s job is to maximize precision on the item that have been retrieved by any of the query so far the overall process alternate between the query enhancement loop to increase recall and the classifier refinement loop to increase precision the separation allows the query enhancement process to explore larger part of the query space our experiment show that this distribution of work significantly outperforms previous relevance feedback method that rely on a single ranking function to balance precision and recall 
majority of the study on modeling the evolution of a social network using spectral graph kernel do not consider temporal effect while estimating the kernel parameter a a result such kernel fail to capture structural property of the evolution over the time in this paper we propose temporal spectral graph kernel of four popular graph kernel namely path counting triangle closing exponential and neumann their response in predicting future growth of the network have been investigated in detail using two large datasets namely facebook and dblp it is evident from various experimental setup that the proposed temporal spectral graph kernel outperform all of their non temporal counterpart in predicting future growth of the network 
state of the art method for product recommendation encounter significant performance drop in category where a user ha no purchase history this problem need to be addressed since current online retailer are moving beyond single category and attempting to be diversified in this paper we investigate the challenge problem of product recommendation in unexplored category and discover that the price a factor transferrable across category can improve the recommendation performance significantly through our investigation we address four research question progressively what is the impact of unexplored category on recommendation performance how to represent the price factor from the recommendation point of view what doe price factor across category mean to recommendation how to utilize price factor across category for recommendation in unexplored category based on a series of experiment and analysis conducted on a dataset collected from a leading e commerce website we discover valuable finding for the above four question first unexplored category cause performance drop by relatively for current recommendation system second the price factor can be represented a either a quantity for a product or a distribution for a user to improve performance third consumer behavior with respect to price factor across category is complicated and need to be carefully modeled finally and most importantly we propose a new method which encodes the two perspective of the price factor the proposed method significantly improves the recommendation performance in unexplored category over the state of the art baseline system and shortens the performance gap by relatively 
in this paper we describe virlab a novel web based virtual laboratory for information retrieval ir unlike existing command line based ir toolkits the virlab system provides a more interactive tool that enables easy implementation of retrieval function with only a few line of code simplified evaluation process over multiple data set and parameter setting and straightforward result analysis interface through operational search engine and pair wise comparison these feature make virlab a unique and novel tool that can help teaching ir model improving the productivity for doing ir model research a well a promoting controlled experimental study of ir model 
recent work introduced a probabilistic framework that measure search engine performance information theoretically this allows for novel meta evaluation measure such a information difference which measure the magnitude of the difference between search engine in their ranking of document for which we have relevance information using information difference we can compare the behavior of search engine which document the search engine prefers a well a search engine performance how likely the search engine is to satisfy a hypothetical user in this work we a extend this probabilistic framework to precision oriented context b show that information difference can be used to detect similar search engine at shallow rank and c demonstrate the utility of the information difference methodology by showing that well tuned search engine employing different retrieval model are more similar than a well tuned and a poorly tuned implementation of the same retrieval model 
social recommender system ha become an emerging research topic due to the prevalence of online social networking service during the past few year in this paper aiming at providing fundamental support to the research of social recommendation problem we conduct an in depth analysis on the correlation between social friend relation and user interest similarity when evaluating interest similarity without distinguishing different friend a user ha we surprisingly observe that social friend relation generally cannot represent user interest similarity a user s average similarity on all his her friend is even correlated with the average similarity on some other randomly selected user however when measuring interest similarity using a finer granularity we find that the similarity between a user and his her friend are actually controlled by the network structure in the friend network factor that affect the interest similarity include subgraph topology connected component number of co friend etc we believe our analysis provides substantial impact for social recommendation research and will benefit ongoing research in both recommender system and other social application 
recently significant progress ha been made in research on what we call semantic matching sm in web search question answering online advertisement cross language information retrieval and other task advanced technology based on machine learning have been developed let u take web search a example of the problem that also pervades the other task when comparing the textual content of query and document web search still heavily relies on the term based approach where the relevance score between query and document are calculated on the basis of the degree of matching between query term and document term this simple approach work rather well in practice partly because there are many other signal in web search hypertext user log etc that complement it however when considering the long tail of web search it can suffer from data sparseness e g trenton doe not match new jersey capital query document mismatch occur when searcher and author use different term representation and this phenomenon is prevalent due to the nature of human language 
how can a search engine with a relatively weak relevance ranking function compete with a search engine that ha a much stronger ranking function this dual challenge which to the best of our knowledge ha not been addressed in previous work entail an interesting bi modal utility function for the weak search engine that is the goal is to produce in response to a query a document result list whose effectiveness doe not fall much behind that of the strong search engine and which is quite different than that of the strong engine we present a per query algorithmic approach that leverage fundamental retrieval principle such a pseudo feedback based relevance modeling we demonstrate the merit of our approach using trec data 
log based document re ranking is a special form of session search the task re rank document from search engine result page serp according to the search log in which both the search activity from other user and personalized query log for a user are available the purpose of re ranking is to provide the user with a new and better ordering of the initial retrieved document we test the system on the wscd dataset in which the actual content of the query and document are not available due to privacy concern the challenge is to perform effective re ranking purely based on user behavior such a click and query reformulations rather than document content in this paper we propose to model log based document re ranking a a partially observable markov decision process pomdp experiment on the document re ranking task show that our approach is effective and outperforms the baseline ranking provided by a commercial search engine 
many national and international heritage institute realize the importance of archiving the web for future culture heritage web archiving is currently performed either by harvesting a national domain or by crawling a pre defined list of website selected by the archiving institution in either method crawling result in more information being harvested than just the website intended for preservation which could be used to reconstruct impression of page that existed on the live web of the crawl date but would have been lost forever we present a method to create representation of what we will refer to a a web collection s aura the web document that were not included in the archived collection but are known to have existed due to their mention on page that were included in the archived web collection to create representation of these unarchived page we exploit the information about the unarchived url that can be derived from the crawl by combining crawl date distribution anchor text and link structure we illustrate empirically that the size of the aura can be substantial in the dutch web archive contained m unique page while we uncover reference to m additional unarchived page 
enterprise computer network are filled with user performing a variety of task ranging from business critical task to personal interest browsing due to this multi modal distribution of behavior it is non trivial to automatically discern which behavior are business relevant and which are not additionally it is difficult to infer community of interest within the enterprise even given an organizational mapping in this work we present a two step framework for classifying user behavior within an enterprise in a data driven way a a first step we use a latent topic model on active search query to identify type of behavior and topic of interest associated with a given user we then leverage the information about user s assigned role within the organization to extract relevant topic which are most reflective of self organizing community of interest we demonstrate that our framework is able to identify rich community of interest that are better representation of how user interact and assemble in an enterprise setting 
usage of mobile device for web search grows rapidly in recent year the common tendency is that user want to receive information immediately result in incorporating rich snippet and vertical result into search engine result page serps and in increasing of good abandonment this article provides an offline metric for quality evaluation of mobile web search which take good abandonment rate into consideration the metric is the dbn click model that allows the probability to be satisfied directly on the serp the model parameter are estimated from the mobile search log of a controlled experiment the new metric outperforms traditional err metric in term of the validation dataset built using a serp degradation technique 
query auto completion qac is a popular feature of web search engine that aim to assist user to formulate query faster and avoid spelling mistake by presenting them with possible completion a soon a they start typing however despite the wide adoption of auto completion in search system there is little published on how user interact with such service in this paper we present the first large scale study of user interaction with auto completion based on query log of bing a commercial search engine our result confirm that lower ranked auto completion suggestion receive substantially lower engagement than those ranked higher we also observe that user are most likely to engage with auto completion after typing about half of the query and in particular at word boundary interestingly we also noticed that the likelihood of using auto completion varies with the distance of query character on the keyboard overall we believe that the result reported in our study provide valuable insight for understanding user engagement with auto completion and are likely to inform the design of more effective qac system 
text reuse is a common phenomenon in a variety of user generated content along with the quick expansion of social medium reuses of local text are occurring much more frequently than ever before the task of detecting these local reuses serf a an essential step for many application it ha attracted extensive attention in recent year however semantic level similarity have not received consideration in most previous work in this paper we introduce a novel method to efficiently detect local reuses at the semantic level for large scale problem we propose to use continuous vector representation of word to capture the semantic level similarity between short text segment in order to handle ten of billion of document method based on information geometry and hashing method are introduced to aggregate and map text segment presented by word embeddings to binary hash code experimental result demonstrate that the proposed method achieve significantly better performance than state of the art approach in all six document collection belonging to four different category at some recall level the precision of the proposed method are even time higher than previous method moreover the efficiency of the proposed method is comparable to or better than that of some other hashing method 
it is becoming increasingly difficult to stay aware of the state of the art in any research field due to the exponential increase in the number of academic publication this problem effect author and reviewer of submission to academic journal and conference who must be able to identify which portion of an article are novel and which are not therefore having a process to automatically judge the flow of novelty though a document would assist academic in their quest for truth in this article we propose the concept of within document novelty location a method of identifying location of novelty and non novelty within a given document in this preliminary investigation we examine if a second order statistical model ha any benefit in term of accuracy and confidence over a simpler first order model experiment on text sequence taken from three academic article showed that the second order model provided a significant increase in novelty location accuracy for two of the three document there wa no significant difference in accuracy for the remaining document which is likely to be due to the absence of context analysis 
we present a study to understand the effect that negated term e g no fever and family history e g family history of diabetes have on searching clinical record our analysis is aimed at devising the most effective mean of handling negation and family history in doing so we explicitly represent a clinical record according to it different content type negated family history and normal content the retrieval model weight each of these separately empirical evaluation show that overall the presence of negation harm retrieval effectiveness while family history ha little effect we show negation is best handled by weighting negated content rather than the common practise of removing or replacing it however we also show that many query benefit from the inclusion of negated content and that negation is optimally handled on a per query basis additional evaluation show that adaptive handing of negated and family history content can have significant benefit 
online community within the enterprise offer their leader an easy and accessible way to attract engage and influence others our research study the recommendation of social medium content to leader owner of online community within the enterprise we developed a system that suggests to owner new content from outside the community which might interest the community member a online community are taking a central role in the pervasion of social medium to the enterprise sharing such recommendation can help owner create a more lively and engaging community we compared seven different method for generating recommendation including content based member based and hybridization of the two for member based recommendation we experimented with three group owner active member and regular member our evaluation is based on a survey in which community owner rated a total of recommended content item we analyzed the quality of the different recommendation method and examined the effect of different community characteristic such a type and size 
we tackle the problem of improving microblog retrieval algorithm by proposing a robust structural representation of query tweet pair we employ these structure in a principled kernel learning framework that automatically extract and learns highly discriminative feature we test the generalization power of our approach on the trec microblog and task we find that relational syntactic feature generated by structural kernel are effective for learning to rank l r and can easily be combined with those of other existing system to boost their accuracy in particular the result show that our l r approach improves on almost all the participating system at trec only using their raw score a a single feature our method yield an average increase of in retrieval effectiveness and position in system rank 
similarity search is an important problem in many large scale application such a image and text retrieval hashing method ha become popular for similarity search due to it fast search speed and low storage cost recent research ha shown that hashing quality can be dramatically improved by incorporating supervised information e g semantic tag label into hashing function learning however most existing supervised hashing method can be regarded a passive method which assume that the labeled data are provided in advance but in many real world application such supervised information may not be available this paper proposes a novel active hashing approach active hashing with joint data example and tag selection ah jdets which actively selects the most informative data example and tag in a joint manner for hashing function learning in particular it first identifies a set of informative data example and tag for user to label based on the selection criterion that both the data example and tag should be most uncertain and dissimilar with each other then this labeled information is combined with the unlabeled data to generate an effective hashing function an iterative procedure is proposed for learning the optimal hashing function and selecting the most informative data example and tag extensive experiment on four different datasets demonstrate that ah jdets achieves good performance compared with state of the art supervised hashing method but requires much le labeling cost which overcomes the limitation of passive hashing method furthermore experimental result also indicate that the joint active selection approach outperforms a random non active selection method and active selection method only focusing on either data example or tag 
online health seeking ha transformed the way of health knowledge exchange and reusability the existing general and vertical health search engine however just routinely return list of matched document or question answer qa pair which may overwhelm the seeker or not sufficiently meet the seeker expectation instead our multilingual system is able to return one multi faceted answer that is well structured and precisely extracted from multiple heterogeneous healthcare source further should the seeker not be satisfied with the returned search result our system can automatically route the unsolved question to the professional with relevant expertise 
due to the fast query speed and low storage cost hashing based approximate nearest neighbor search method have attracted much attention recently many state of the art method are based on eigenvalue decomposition in these approach the information caught in different dimension is unbalanced and generally most of the information is contained in the top eigenvectors we demonstrate that this lead to an unexpected phenomenon that longer hashing code doe not necessarily yield better performance in this work we introduce a random subspace strategy to address this limitation at first a small fraction of the whole feature space is randomly sampled to train the hashing algorithm each time and only the top eigenvectors are kept to generate one piece of short code this process will be repeated several time and then the obtained many piece of short code are concatenated into one piece of long code theoretical analysis and experiment on two benchmark confirm the effectiveness of the proposed strategy for hashing 
in some jurisdiction party to a lawsuit can request document from each other but document subject to a claim of privilege may be withheld the trec legal track developed what is presently the only public test collection for evaluating privilege classification this paper examines the reliability and reusability of that collection for reliability the key question is the extent to which privilege judgment correctly reflect the opinion of the senior litigator whose judgment is authoritative for reusability the key question is the degree to which system whose result contributed to creation of the test collection can be fairly compared with other system that use those privilege judgment in the future these correspond to measurement error and sampling error respectively the result indicate that measurement error is the larger problem 
data retrieved from community question answering cqa site such a content and user assessment of content is commonly used for expertise estimation related task one such task in which the received vote are directly used a graded relevance assessment value is ranking reply of a question even though these available assessment value are very practical for evaluation purpose they may not always reflect the correct assessment value of the content due to the possible temporal or presentation bias introduced by the cqa system during voting process this paper analyzes a very commonly used cqa data collection in term of these introduced bias and their effect on the experimental evaluation of approach a more bias free test set construction approach which ha correlated result with the manual assessment is also proposed in this paper 
web search query without hyperlink click are often referred to a abandoned query understanding the reason for abandonment is crucial for search engine in evaluating their performance abandonment can be categorized a good or bad depending on whether user information need are satisfied by result page content previous research ha sought to understand abandonment rationale via user survey or ha developed model to predict those rationale using behavioral pattern however these model ignore important contextual factor such a the relationship between the abandoned query and prior abandonment instance we propose more advanced method for modeling and predicting abandonment rationale using contextual information from user search session by analyzing search engine log and discover dependency between abandoned query and user behavior we leverage these dependency signal to build a sequential classifier using a structured learning framework designed to handle such signal our experimental result show that our approach is more accurate than the state of the art abandonment rationale classifier going beyond prediction we leverage the prediction result to significantly improve relevance using instance of predicted good and bad abandonment 
several researcher have found that a user s mouse position give an indication of the user s gaze during web search and other task a part of a user study that involved relevance judging of document summary and full document we recorded user mouse movement we found that in a large number of case the user did nothing more with their mouse than move it to the button used for recording the relevance decision in addition we found that different search topic can result in large difference in the amount of mouse movement that is indicative of user attention for simple reading task such a short document summary mouse tracking doe not appear to be an effective mean of discerning user attention while more complex task may allow mouse movement to provide information regarding user attention on average indication of user attention existed in only of the relevance judgment made for full document 
web archive already hold together more than billion file and this number continues to grow a new initiative arise searching on all version of these file acquired throughout time is challenging since user expect a fast and precise answer from web archive a the one provided by current web search engine this work study for the first time how to improve the search effectiveness of web archive including the creation of novel temporal feature that explore the correlation found between web document persistence and relevance the persistence wa analyzed over year of web snapshot additionally we propose a temporal dependent ranking framework that exploit the variance of web characteristic over time influencing ranking model based on the assumption that closer period are more likely to hold similar web characteristic our framework learns multiple model simultaneously each tuned for a specific period experimental result show significant improvement over the search effectiveness of single model that learn from all data independently of it time thus our approach represents an important step forward on the state of the art ir technology usually employed in web archive 
although searcher often click on more than one result following a query little is known about how they interact with search result after their first click using large scale query log analysis we characterize what people do when they return to a result page after having visited an initial result we find that the initial click provides insight into the searcher s subsequent behavior with short initial dwell time suggesting more future interaction and later click occurring close in rank to the first although user think of a search result list a static when people return to a result list following a click there is the opportunity for the list to change potentially providing additional relevant content such change however can be confusing leading to increased abandonment and slower subsequent click we explore the risk and opportunity of changing search result during use observing for example that when result change above a user s initial click that user is le likely to find new content whereas change below correlate with increased subsequent interaction our result can be used to improve people s search experience during the course of a single query by seamlessly providing new more relevant content a the user interacts with a search result page helping them find what they are looking for without having to issue a new query 
most of the previous approach surrounding collaborative information retrieval cir provide either a user based mediation in which the system only support user collaborative activity or a system based mediation in which the system play an active part in balancing user role re ranking result and distributing them to optimize overall retrieval performance in this paper we propose to combine both of these approach by a role mining methodology that learns from user action about the retrieval strategy they adapt this hybrid method aim at showing how user are different and how to use these difference for suggesting role the core of the method is expressed a an algorithm that monitor user action in a cir setting discovers difference among the collaborator along certain dimension and suggests appropriate role to make the most out of individual skill and optimize ir performance our approach is empirically evaluated and relies on two different laboratory study involving pair of user our experiment show promising result that highlight how role mining could optimize the collaboration within a search session the contribution of this work include a new algorithm for mining user role in collaborative ir an evaluation methodology and a new approach to improve ir performance with the operationalization of user driven system mediated collaboration 
relevance feedback ha been shown to improve retrieval for a broad range of retrieval model it is the most common way of adapting a retrieval model for a specific query in this work we expand this common way by focusing on an approach that enables u to do query specific modification of a retrieval model for learning to rank problem our approach is based on using feedback document in two way to improve the retrieval model directly and to identify a subset of training query that are more predictive than others experiment with the gov collection show that this approach can obtain statistically significant improvement over two baseline learning to rank svm rank with no feedback and learning to rank with standard relevance feedback 
volunteer are extremely crucial to nonprofit organization npos to sustain their continuing operation on the other hand many talent are looking for appropriate volunteer opportunity to realize their dream of making an impact on the world with their expertise this is a typical supply and demand matching issue fortunately user profiling and the discovery of user volunteering tendency can benefit from user continuous enthusiasm and active participation in diverse online social network osns and the huge amount of publicly available user generated content ugcs in this work we aim to bridge the gap between the supply of talent with volunteering tendency and the demand of social enterprise and enhance the social welfare this is done by incorporating volunteering tendency into user profiling across multiple osns consequently this interdisciplinary research open a new window for both computer science and social science to the best of our knowledge this is the first attempt to tackle the problem of volunteer matching for social enterprise based on publicly available ugcs first we explain the definition of the main concept with example second we propose a system architecture for addressing the problem of volunteerism matching that includes three component profile collection profile enrichment and profile matching finally we identify the major challenge encountered in our current research work this paper discus our design and progress in this research 
identifying and targeting visitor on an e commerce website with personalized content in real time is extremely important to marketer although such targeting exists today it is based on demographic attribute of the visitor we show that dynamic visitor attribute extracted from their click stream provide much better predictive capability of visitor intent in this demonstration we showcase an interactive real time user interface for marketer to visualize and target visitor segment our dashboard not only provides the marketer understanding of their visitor click pattern but also let them target individual or group of visitor with offer and promotion 
the use of sampling randomized algorithm or training based on the unpredictable input of user in information retrieval often lead to non deterministic output evaluating the effectiveness of system incorporating these method can be challenging since each run may produce different effectiveness score current ir evaluation technique do not address this problem using the context of distributed information retrieval a a case study for our investigation we propose a solution based on multivariate linear modeling we show that the approach provides a consistent and reliable method to compare the effectiveness of non deterministic ir algorithm and explain how statistic can safely be used to show that two ir algorithm have equivalent effectiveness 
we present a post hoc analysis of a benchmarking activity for information retrieval ir in the medical domain to determine if performance for query with different level of complexity can be associated with different ir method or technique our analysis is based on data and run for task of the clef ehealth lab which provided patient query and a large medical document collection for patient centred medical information retrieval technique development we categorise the query based on their complexity which is defined a the number of medical concept they contain we then show how query complexity affect performance of run submitted to the lab and provide suggestion for improving retrieval quality for this complex retrieval task and similar ir evaluation task 
this paper present a project called knowing camera for real time recognizing and annotating place of interest poi in smartphone photo with the availability of online geotagged image of such place we propose a spatial visual s v framework which consists of a probabilistic field of view model in the spatial phase and sparse coding similarity metric in the visual phase to recognize phone captured poi moreover we put forward an offline collaborative salient area costar mining algorithm to detect common visual feature called costars among the noisy photo geotagged on each poi thus to clean the geotagged image database the mining result can be utilized to annotate the region of interest on the query image during the online query processing besides this mining procedure further improves the efficiency and accuracy of the s v framework our experiment in the real world and oxford k datasets show promising recognition and annotation performance of the proposed approach and that the proposed costar mining technique outperforms state of the art approach 
we combine search in triple store with full text search into what we call emph semantic full text search we provide a fully functional web application that allows the incremental construction of complex query on the english wikipedia combined with the fact from freebase the user is guided by context sensitive suggestion of matching word instance class and relation after each keystroke we also provide a powerful api which may be used for research task or a a back end e g for a question answering system our web application and public api are available under url http broccoli c uni freiburg de 
location model built on social medium have been shown to be an important step toward understanding place in query current search technology focus on predicting broad region such a city hyperlocal scenario are important because of the increasing prevalence of smartphones and mobile search and recommendation user expect the system to recognize their location and provide information about their immediate surroundings in this work we propose an algorithm for constructing hyperlocal model of place that are a small a half a city block we show that dynamic location model dlms are computationally efficient and provide better estimate of the language model of hyperlocal place than the standard method of segmenting the globe into approximately equal grid square we evaluate the model using a repository of million geotagged public image from flickr we show that the index produced by dlms have a larger vocabulary and smaller average document length than their fixed grid counterpart for index with an equivalent number of location this produce location model that are more robust to retrieval parameter and more accurate in predicting location in text 
word segmentation is a challenging issue and the corresponding algorithm can be used in many application of natural language processing this paper address the problem of vietnamese word segmentation proposes a probabilistic ensemble learning pel framework and design a novel pel based word segmentation pelws algorithm supported by the data structure of syllable syllable frequency index the pelws algorithm combine multiple weak segmenters to form a strong segmenter within the pel framework the experimental result show that the pelws algorithm can achieve the state of the art performance in the vietnamese word segmentation task 
the vast amount of real time and social content in microblogs result in an information overload for user when searching microblog data given the user s search query delivering content that is relevant to her interest is a challenging problem traditional method for personalized web search are insufficient in the microblog domain because of the diversity of topic sparseness of user data and the highly social nature in particular social interaction between user need to be considered in order to accurately model user s interest alleviate data sparseness and tackle the cold start problem in this paper we therefore propose a novel framework for collaborative personalized twitter search at it core we develop a collaborative user model which exploit the user s social connection in order to obtain a comprehensive account of her preference we then propose a novel user model structure to manage the topical diversity in twitter and to enable semantic aware query disambiguation our framework integrates a variety of information about the user s preference in a principled manner a thorough evaluation is conducted using two personalized twitter search query log demonstrating a superior ranking performance of our framework compared with state of the art baseline 
reputation analysis is naturally linked to a sentiment analysis task of the targeted entity this analysis leverage on a sentiment lexicon that includes general sentiment word and domain specific jargon however in most case target entity are themselves part of the sentiment lexicon creating a loop from which it is difficult to infer an entity reputation sometimes the entity became a reference in the domain and is vastly cited a an example of a highly reputable entity for example in the movie domain it is not uncommon to see review citing batman or anthony hopkins a esteemed reference in this paper we describe an unsupervised method for performing a simultaneous analysis of the reputation of multiple named entity our method jointly extract named entity reputation and a domain specific sentiment lexicon the objective is two fold named entity are naturally ranked by our method and we can build a reputation graph of the domain s named entity this framework ha immediate application in term of visualization or search by reputation 
the web is an important resource for understanding and diagnosing medical condition based on exposure to online content people may develop undue health concern believing that common and benign symptom are explained by serious illness in this paper we investigate potential strategy to mine query and searcher history for clue that could help search engine choose the most appropriate information to present in response to exploratory medical query to do this we performed a longitudinal study of health search behavior using the log of a popular web search engine we found that query variation which might appear innocuous e g bad headache v severe headache may hold valuable information about the searcher which could be used by search engine to improve performance furthermore we investigated how medically concerned user respond differently to search engine result page serps and find that their disposition for clicking on concerning page is pronounced potentially leading to a self reinforcement of concern finally we studied to which degree variation in the serp impact future search and real world healthseeking behavior and obtained some surprising result e g viewing concerning page may lead to a short term reduction of in world healthcare utilization 
in recent year social medium ha become one of the most popular tool for discovering and following breaking news and ongoing event however tool and interface have lagged behind user expectation with current tool making it difficult to discover new event and failing to provide a solution to the problem of information overload we have developed an interactive interface for visualizing event backed by a state of the art event detection approach which is able to detect track and summarize event in real time our interface provides up to the second information about ongoing event in an easy to understand manner including category information temporal distribution and location information all of which wa previously unobtainable in real time 
we propose a framework which can perform web page segmentation with a structured prediction approach it formulates the segmentation task a a structured labeling problem on a transformed web page segmentation graph wps graph wps graph model the candidate segmentation boundary of a page and the dependency relation among the adjacent segmentation boundary each labeling scheme on the wps graph corresponds to a possible segmentation of the page the task of finding the optimal labeling of the wps graph is transformed into a binary integer linear programming problem which considers the entire wps graph a a whole to conduct structured prediction a learning algorithm based on the structured output support vector machine framework is developed to determine the feature weight which is capable to consider the inter dependency among candidate segmentation boundary furthermore we investigate it efficacy in supporting the development of automatic web page classification 
simrank is an attractive structural context measure of similarity between two object in a graph it recursively follows the intuition that two object are similar if they are referenced by similar object the best known matrix based method for calculating simrank however implies an assumption that the graph is non singular it adjacency matrix is invertible in reality non singular graph are very rare such an assumption in is too restrictive in practice in this paper we provide a treatment of by supporting similarity assessment on non invertible adjacency matrix assume that a singular graph g ha n node with r kr time for k iteration in contrast the only known matrix based algorithm that support singular graph need o r n time the experimental result on real and synthetic datasets demonstrate the superiority of invsr on singular graph against it baseline 
different important study in web search result clustering have recently shown increasing performance motivated by the use of external resource following this trend we present a new algorithm called dual c mean which provides a theoretical background for clustering in different representation space it originality relies on the fact that external resource can drive the clustering process a well a the labeling task in a single step to validate our hypothesis a series of experiment are conducted over different standard datasets and in particular over a new dataset built from the trec web track to take into account query log information the comprehensive empirical evaluation of the proposed approach demonstrates it significant advantage over traditional clustering and labeling technique 
we address the core challenge of the entity retrieval task ranking entity in response to a query by their presumed relevance to the information need that the query represents a an initial research direction we explored two model for entity ranking that were evaluated using the inex entity ranking dataset and which posted promising performance a natural future direction to explore is how to generalize these model to address various type of information need that are associated with entity 
we address the query performance prediction task for entity retrieval that is retrieval effectiveness is estimated with no relevance judgement first we show how to adapt state of the art query performance predictor proposed for document retrieval to the entity retrieval domain we then present a novel predictor that is based on the cluster hypothesis evaluation performed with the inex entity ranking track collection show that our predictor can often outperform the most effective predictor we experimented with 
understanding a text which wa written some time ago can be compared to translating a text from another language complete interpretation requires a mapping in this case a kind of time travel translation between present context knowledge and context knowledge at time of text creation in this paper we study time aware re contextualization the challenging problem of retrieving concise and complementing information in order to bridge this temporal context gap we propose an approach based on learning to rank technique using sentence level context information extracted from wikipedia the employed ranking combine relevance complimentarity and time awareness the effectiveness of the approach is evaluated by contextualizing article from a news archive collection using more than manually judged relevance pair to this end we show that our approach is able to retrieve a significant number of relevant context information for a given news article 
conventional approach to road hazard detection involve manual inspection of road by government transportation agency these approach are usually expensive to execute and sometimes are not able to capture the most recent hazard moreover they often only focus on major highway due to a lack of sufficient manpower consequently many hazard on minor road get ignored which may pose serious danger to driver in this paper we demonstrate an application of twitter to atomically determining road hazard by building language model based on twitter user online communication our system aim at pinpointing potential road hazard that pose driving risk the likelihood of poor driving condition can then be exposed via map overlay to warn driver about potentially dangerous driving condition in their locale or on current route thereby significantly reducing the chance of an accident occurring to the best of our knowledge this is the first work demonstrating the utility of social medium to automatically detect road hazard we conduct experiment on a testbed of tweet discussing road condition and the initial result demonstrate the effectiveness of our approach 
the somera workshop target cutting edge research from all field of retrieval recommendation and browsing in social medium a well a the analysis of user s multifaceted trace therein submission to the workshop cover a broad range of topic including multimedia retrieval and exploration user aware recommender system network analysis event detection and computational linguistics 
search result diversification ha gained attention a a way to tackle the ambiguous or multi faceted information need of user most existing method on this problem utilize a heuristic predefined ranking function where limited feature can be incorporated and extensive tuning is required for different setting in this paper we address search result diversification a a learning problem and introduce a novel relational learning to rank approach to formulate the task however the definition of ranking function and loss function for the diversification problem are challenging in our work we firstly show that diverse ranking is in general a sequential selection process from both empirical and theoretical aspect on this basis we define ranking function a the combination of relevance score and diversity score between the current document and those previously selected and loss function a the likelihood loss of ground truth based on plackett luce model which can naturally model the sequential generation of a diverse ranking list stochastic gradient descent is then employed to conduct the unconstrained optimization and the prediction of a diverse ranking list is provided by a sequential selection process based on the learned ranking function the experimental result on the public trec datasets demonstrate the effectiveness and robustness of our approach 
over the last two decade the information retrieval landscape ha changed dramatically twenty year ago there were fewer than k web site and the earliest web search engine indexed approximately k page today search engine index billion of web page image video news music social medium book etc and have become the main entry point for a wide range of information service communication and entertainment despite these tremendous accomplishment we still have a long way to go many search are unsuccessful and even those that succeed are often harder than they should be to address these challenge we need to extend our evaluation method to handle the diversity of searcher task and interactivity that characterize information system today i will discus recent work on user modeling and temporal dynamic of information system to illustrate the power of utilizing converging line of evidence from laboratory panel and large scale log technique to understand and support searcher 
we make the suggestion that instead of implementing custom index structure and query evaluation algorithm ir researcher should simply store document representation in a column oriented relational database and implement ranking model using sql for rapid prototyping this is particularly advantageous since researcher can explore new scoring function and feature by simply issuing sql query without needing to write imperative code we demonstrate the feasibility of this approach by an implementation of conjunctive bm using two modern column store experiment on a web collection show that a retrieval engine built in this manner achieves effectiveness and efficiency on par with custom built retrieval engine but provides many additional advantage including cleaner query semantics a simpler architecture built in support for error analysis and the ability to exploit advance in database technology for free 
user often reformulate or modify their query when they engage in searching information particularly when the search task is complex and exploratory this paper investigates query reformulation behavior in collaborative tourism information searching on the web a user study wa conducted with pair of participant and each pair worked a a team collaboratively on an exploratory travel search task in two scenario we analyzed user collaborative query cq reformulation behavior in two dimension firstly cq reformulation strategy and secondly the effect of individual query and chat log on cq reformulation the finding show that individual query and chat log were two major source of query term in cq reformulation the statistical result demonstrate the significant effect of individual query on cq reformulation we also found that five operation were performed to reformulate the cqs namely addition modification reordering addition and modification and addition and reordering these finding have implication for the design of query suggestion that could be offered to user during search using collaborative search tool 
searching for scene in team sport video is a task that recurs very often in game analysis and other related activity performed by coach in most case query are formulated on the basis of specific motion characteristic the user remembers from the video providing sketching interface for graphically specifying query input is thus a very natural user interaction for a retrieval application however the quality of the query the sketch heavily depends on the memory of the user and her ability to accurately formulate the intended search query by transforming this d memory of the known item s into a d sketch query in this paper we present an auto suggest search feature that harness spatiotemporal data of team sport video to suggest potential direction containing relevant data during the formulation of a sketch based motion query user can intuitively select the direction of the desired motion query on the fly using the displayed visual clue thus relaxing the need for relying heavily on memory to formulate the query at the same time this significantly enhances the accuracy of the result and the speed at which they appear a first evaluation ha shown the effectiveness and efficiency of our approach 
the tremendous increase of multimedia data in recent year ha heightened the need for system that not only allow to search with keywords but that also support content based retrieval in order to effectively and efficiently query large collection in this paper we introduce adam a system that is able to store and retrieve multimedia object by seamlessly combining aspect from database and information retrieval adam is able to work with both structured and unstructured data and to jointly provide boolean retrieval and similarity search to efficiently handle large volume of data it make use of a signature based indexing and the distribution of the collection to multiple shard that are queried in a mapreduce style we present adam in the setting of a sketch based image retrieval application using the imagenet collection containing million image 
can we learn the influence of a set of people in a social network from cascade of information diffusion this question is often addressed by a two stage approach first learn a diffusion model and then calculate the influence based on the learned model thus the success of this approach relies heavily on the correctness of the diffusion model which is hard to verify for real world data in this paper we exploit the insight that the influence function in many diffusion model are coverage function and propose a novel parameterization of such function using a convex combination of random basis function moreover we propose an efficient maximum likelihood based algorithm to learn such function directly from cascade data and hence bypass the need to specify a particular diffusion model in advance we provide both theoretical and empirical analysis for our approach showing that the proposed approach can provably learn the influence function with low sample complexity be robust to the unknown diffusion model and significantly outperform existing approach in both synthetic and real world data 
it ha been shown that graphical model can be used to leverage the dependence in large scale multiple testing problem with significantly improved performance sun amp amp cai liu et al these graphical model are fully parametric and require that we know the parameterization of f the density function of the test statistic under the alternative hypothesis however in practice f is often heterogeneous and cannot be estimated with a simple parametric distribution we propose a novel semiparametric approach for multiple testing under dependence which estimate f adaptively this semiparametric approach exactly generalizes the local fdr procedure efron et al and connects with the bh procedure benjamini amp amp hochberg a variety of simulation show that our semiparametric approach outperforms classical procedure which assume independence and the parametric approach which capture dependence 
the recent blossom of social network and communication service in both public and corporate setting have generated a staggering amount of network data of all kind unlike the bio network and the chemical compound graph data often used in traditional network mining and analysis the new network data grown out of the social application are characterized by their rich attribute high heterogeneity enormous size and complex pattern of various semantic meaning all of which have posed significant research challenge to the graph network mining community in this tutorial we aim to examine some recent advance in network mining and analysis for social application covering a diverse collection of methodology and application from the perspective of event relationship collaboration and network pattern we would present the problem setting the challenge the recent research advance and some future direction for each perspective topic include but are not limited to correlation mining iceberg finding anomaly detection relationship discovery information flow task routing and pattern mining 
in recent year with the widespread usage of web technique crowdsourcing play an important role in offering human intelligence in various service website such a yahoo answer and quora with the increasing amount of crowd oriented service data an important task is to analyze latest hot topic and track topic evolution over time however the existing technique in text mining cannot effectively work due to the unique structure of crowd oriented service data task response pair which consists of the task and it corresponding response in particular existing approach become ineffective with the ever increasing crowd oriented service data that accumulate along the time in this paper we first study the problem of discovering topic over crowd oriented service data then we propose a new probabilistic topic model the topic crowd service model tc model to effectively discover latent topic from massive crowd oriented service data in particular in order to train tc efficiently we design a novel parameter inference algorithm the bucket parameter estimation bpe which utilizes belief propagation and a new sketching technique called pairwise sketch psketch finally we conduct extensive experiment to verify the effectiveness and efficiency of the tc model and the bpe algorithm 
in this paper we study networked bandit a new bandit problem where a set of interrelated arm varies over time and given the contextual information that selects one arm invokes other correlated arm this problem remains under investigated in spite of it applicability to many practical problem for instance in social network an arm can obtain payoff from both the selected user and it relation since they often share the content through the network we examine whether it is possible to obtain multiple payoff from several correlated arm based on the relationship in particular we formalize the networked bandit problem and propose an algorithm that considers not only the selected arm but also the relationship between arm our algorithm is optimism in face of uncertainty style in that it decides an arm depending on integrated confidence set constructed from historical data we analyze the performance in simulation experiment and on two real world offline datasets the experimental result demonstrate our algorithm s effectiveness in the networked bandit setting 
topic modeling ha been widely used to mine topic from document however a key weakness of topic modeling is that it need a large amount of data e g thousand of document to provide reliable statistic to generate coherent topic however in practice many document collection do not have so many document given a small number of document the classic topic model lda generates very poor topic even with a large volume of data unsupervised learning of topic model can still produce unsatisfactory result in recently year knowledge based topic model have been proposed which ask human user to provide some prior domain knowledge to guide the model to produce better topic our research take a radically different approach we propose to learn a human do i e retaining the result learned in the past and using them to help future learning when faced with a new task we first mine some reliable prior knowledge from the past learning modeling result and then use it to guide the model inference to generate more coherent topic this approach is possible because of the big data readily available on the web the proposed algorithm mine two form of knowledge must link meaning that two word should be in the same topic and cannot link meaning that two word should not be in the same topic it also deal with two problem of the automatically mined knowledge i e wrong knowledge and knowledge transitivity experimental result using review document from product domain show that the proposed approach make dramatic improvement over state of the art baseline 
the explosive growth in sharing and consumption of the video content on the web creates a unique opportunity for scientific advance in video retrieval recommendation and discovery in this paper we focus on the task of video suggestion commonly found in many online application the current state of the art video suggestion technique are based on the collaborative filtering analysis and suggest video that are likely to be co viewed with the watched video in this paper we propose augmenting the collaborative filtering analysis with the topical representation of the video content to suggest related video we propose two novel method for topical video representation the first method us information retrieval heuristic such a tf idf while the second method learns the optimal topical representation based on the implicit user feedback available in the online scenario we conduct a large scale live experiment on youtube traffic and demonstrate that augmenting collaborative filtering with topical representation significantly improves the quality of the related video suggestion in a live setting especially for category with fresh and topically rich video content such a news video in addition we show that employing user feedback for learning the optimal topical video representation can increase the user engagement by more than over the standard information retrieval representation when compared to the collaborative filtering baseline 
in this talk drew will examine data science through the lens of the social scientist he will discus how the various skill and discipline combine into data science drew will also present a motivating example directly from his work a a senior advisor to nyc s mayor s office of analytics 
the diffusion of information rumor and disease are assumed to be probabilistic process over some network structure an event start at one node of the network and then spread to the edge of the network in most case the underlying network structure that generates the diffusion process is unobserved and we only observe the time at which each node is altered influenced by the process this paper proposes a probabilistic model for inferring the diffusion network which we call probabilistic latent network visualization plnv it is based on cascade data a record of observed time of node influence an important characteristic of our approach is to infer the network by embedding it into a low dimensional visualization space we assume that each node in the network ha latent coordinate in the visualization space and diffusion is more likely to occur between node that are placed close together our model us maximum a posteriori estimation to learn the latent coordinate of node that best explain the observed cascade data the latent coordinate of node in the visualization space can enable the system to suggest network layout most suitable for browsing and lead to high accuracy in inferring the underlying network when analyzing the diffusion process of new or rare information rumor and disease 
kernel based regression represents an important family of learning technique for solving challenging regression task with non linear pattern despite being studied extensively most of the existing work suffers from two major drawback i they are often designed for solving regression task in a batch learning setting making them not only computationally inefficient and but also poorly scalable in real world application where data arrives sequentially and ii they usually assume a fixed kernel function is given prior to the learning task which could result in poor performance if the chosen kernel is inappropriate to overcome these drawback this paper present a novel scheme of online multiple kernel regression omkr which sequentially learns the kernel based regressor in an online and scalable fashion and dynamically explore a pool of multiple diverse kernel to avoid suffering from a single fixed poor kernel so a to remedy the drawback of manual heuristic kernel selection the omkr problem is more challenging than regular kernel based regression task since we have to on the fly determine both the optimal kernel based regressor for each individual kernel and the best combination of the multiple kernel regressors in this paper we propose a family of omkr algorithm for regression and discus their application to time series prediction task we also analyze the theoretical bound of the proposed omkr method and conduct extensive experiment to evaluate it empirical performance on both real world regression and time series prediction task 
balanced edge partition ha emerged a a new approach to partition an input graph data for the purpose of scaling out parallel computation which is of interest for several modern data analytics computation platform including platform for iterative computation machine learning problem and graph database this new approach stand in a stark contrast to the traditional approach of balanced vertex partition where for given number of partition the problem is to minimize the number of edge cut subject to balancing the vertex cardinality of partition in this paper we first characterize the expected cost of vertex and edge partition with and without aggregation of message for the commonly deployed policy of placing a vertex or an edge uniformly at random to one of the partition we then obtain the first approximation algorithm for the balanced edge partition problem which for the case of no aggregation match the best known approximation ratio for the balanced vertex partition problem and show that this remains to hold for the case with aggregation up to factor that is equal to the maximum in degree of a vertex we report result of an extensive empirical evaluation on a set of real world graph which quantifies the benefit of edgevs vertex partition and demonstrates efficiency of natural greedy online assignment for the balanced edge partition problem with and with no aggregation 
discovering temporal dependence structure from multivariate time series ha established it importance in many application we observe that when we look in reversed order of time the temporal dependence structure of the time series is usually preserved after switching the role of cause and effect inspired by this observation we create a new time series by reversing the time stamp of original time series and combine both time series to improve the performance of temporal dependence recovery we also provide theoretical justification for the proposed algorithm for several existing time series model we test our approach on both synthetic and real world datasets the experimental result confirm that this surprisingly simple approach is indeed effective under various circumstance 
human emotional state are not independent but rather proceed along systematic path governed by both internal cognitive factor and external social one for example anxiety often transition to disappointment which is likely to sink to depression before rising to happiness and relaxation and these state are conditioned by the state of others in our community modeling these complex dependency can yield insight into human emotion and support more powerful sentiment technology we develop a theory of conditional dependency between emotional state in which emotion are characterized not only by valence polarity and arousal intensity but also by the role they play in state transition and social relationship we implement this theory using conditional random field crfs that synthesize textual information with information about previous emotional state and the emotional state of others to ass the power of affective transition we evaluate our model in a collection of mood update from the experience project to ass the power of social factor we use a corpus of product review from a website in which the community dynamic encourage reviewer to be influenced by each other in both setting our model yield improvement of statistical and practical significance over one that classify each text independently of it emotional or social context 
shapelets are discriminative sub sequence of time series that best predict the target variable for this reason shapelet discovery ha recently attracted considerable interest within the time series research community currently shapelets are found by evaluating the prediction quality of numerous candidate extracted from the series segment in contrast to the state of the art this paper proposes a novel perspective in term of learning shapelets a new mathematical formalization of the task via a classification objective function is proposed and a tailored stochastic gradient learning algorithm is applied the proposed method enables learning near to optimal shapelets directly without the need to try out lot of candidate furthermore our method can learn true top k shapelets by capturing their interaction extensive experimentation demonstrates statistically significant improvement in term of win and rank against baseline over time series datasets 
social sensing is based on the idea that community or group of people can provide a set of information similar to those obtainable from a sensor network emergency management is a candidate field of application for social sensing in this work we describe the design implementation and deployment of a decision support system for the detection and the damage assessment of earthquake in italy our system exploit the message shared in real time on twitter one of the most popular social network in the world data mining and natural language processing technique are employed to select meaningful and comprehensive set of tweet we then apply a burst detection algorithm in order to promptly identify outbreaking seismic event detected event are automatically broadcasted by our system via a dedicated twitter account and by email notification in addition we mine the content of the message associated to an event to discover knowledge on it consequence finally we compare our result with official data provided by the national institute of geophysics and volcanology ingv the authority responsible for monitoring seismic event in italy the ingv network detects shaking level produced by the earthquake but can only model the damage scenario by using empirical relationship this scenario can be greatly improved with direct information site by site result show that the system ha a great ability to detect event of a magnitude in the region of with relatively low occurrence of false positive earthquake detection mostly occurs within second of the event and far earlier than the notification shared by ingv or by other official channel thus we are able to alert interested party promptly information discovered by our system can be extremely useful to all the government agency interested in mitigating the impact of earthquake a well a the news agency looking for fresh information to publish 
this paper instantly infers the gas consumption and pollution emission of vehicle traveling on a city s road network in a current time slot using gps trajectory from a sample of vehicle e g taxicab the knowledge can be used to suggest cost efficient driving route a well a identifying road segment where gas ha been wasted significantly the instant estimation of the emission from vehicle can enable pollution alert and help diagnose the root cause of air pollution in the long run in our method we first compute the travel speed of each road segment using the gps trajectory received recently a many road segment are not traversed by trajectory i e data sparsity we propose a travel speed estimation tse model based on a context aware matrix factorization approach tse leverage feature learned from other data source e g map data and historical trajectory to deal with the data sparsity problem we then propose a traffic volume inference tvi model to infer the number of vehicle passing each road segment per minute tvi is an unsupervised bayesian network that incorporates multiple factor such a travel speed weather condition and geographical feature of a road given the travel speed and traffic volume of a road segment gas consumption and emission can be calculated based on existing environmental theory we evaluate our method based on extensive experiment using gps trajectory generated by over taxi in beijing over a period of two month the result demonstrate the advantage of our method over baseline validating the contribution of it component and finding interesting discovery for the benefit of society 
for decade large corporation a well a labor placement service have maintained extensive yet static resume databanks online professional network like linkedin have taken these resume databanks to a dynamic constantly updated and massive scale professional profile dataset spanning career record from hundred of industry million of company and hundred of million of people worldwide using this professional profile dataset this paper attempt to model profile of individual a a sequence of position held by them a a time series of node each of which represents one particular position or job experience in the individual s career trajectory these career trajectory model can be employed in various utility application including career trajectory planning for student in school university using knowledge inferred from real world career outcome they can also be employed for decoding sequence to uncover path leading to certain professional milestone from a user s current professional status we deploy the proposed technique to ascertain professional similarity between two individual by developing a similarity measure simcareers similar career path the measure employ sequence alignment between two career trajectory to quantify professional similarity between career path to the best of our knowledge simcareers is the first framework to model professional similarity between two people taking account their career trajectory information we posit that using the temporal and structural feature of a career trajectory for modeling profile similarity is a far more superior approach than using similarity measure on semi structured attribute representation of a profile for this application we validate our hypothesis by extensive quantitative evaluation on a gold dataset of similar profile generated from recruiting activity log from actual recruiter using linkedin in addition we show significant improvement in engagement by running an a b test on a real world application called similar profile on linkedin world s largest online professional network 
chronic disease such a alzheimer s disease diabetes and chronic obstructive pulmonary disease usually progress slowly over a long period of time causing increasing burden to the patient their family and the healthcare system a better understanding of their progression is instrumental in early diagnosis and personalized care modeling disease progression based on real world evidence is a very challenging task due to the incompleteness and irregularity of the observation a well a the heterogeneity of the patient condition in this paper we propose a probabilistic disease progression model that address these challenge a compared to existing disease progression model the advantage of our model is three fold it learns a continuous time progression model from discrete time observation with non equal interval it learns the full progression trajectory from a set of incomplete record that only cover short segment of the progression it learns a compact set of medical concept a the bridge between the hidden progression process and the observed medical evidence which are usually extremely sparse and noisy we demonstrate the capability of our model by applying it to a real world copd patient cohort and deriving some interesting clinical insight 
given a social network can we quickly zoom out of the graph is there a smaller equivalent representation of the graph that preserve it propagation characteristic can we group node together based on their influence property these are important problem with application to influence analysis epidemiology and viral marketing application in this paper we first formulate a novel graph coarsening problem to find a succinct representation of any graph while preserving key characteristic for diffusion process on that graph we then provide a fast and effective near linear time in node and edge algorithm coarsenet for the same using extensive experiment on multiple real datasets we demonstrate the quality and scalability of coarsenet enabling u to reduce the graph by in some case without much loss of information finally we also show how our method can help in diverse application like influence maximization and detecting pattern of propagation at the level of automatically created group on real cascade data 
modeling the movement of information within social medium outlet like twitter is key to understanding to how idea spread but quantifying such movement run into several difficulty two specific area that elude a clear characterization are i the intrinsic random nature of individual to potentially adopt and subsequently broadcast a twitter topic and ii the dissemination of information via non twitter source such a news outlet and word of mouth and it impact on twitter propagation these distinct yet inter connected area must be incorporated to generate a comprehensive model of information diffusion we propose a bispace model to capture propagation in the union of exclusively twitter and non twitter environment to quantify the stochastic nature of twitter topic propagation we combine principle of geometric brownian motion and traditional network graph theory we apply poisson process function to model information diffusion outside of the twitter mention network we discus technique to unify the two sub model to accurately model information dissemination we demonstrate the novel application of these technique on real twitter datasets related to mass protest adoption in social community 
it s not often we get to peer into the detail of how big corporates use predictive modeling in practice in this talk sprint s head of predictive modeling tracey de poalo will talk about the process she developed using sa and logistic regression to build a wide range of model jeremy howard will discus his experience a a consultant at sprint comparing r and random forest to the existing process and will show the pro and con of each approach the talk will also cover the real world issue that tracey ha dealt with in creating a reusable process including data mart development sampling testing reporting and implementation with internal customer sprint s existing process is the best that jeremy ha seen in industry and show a lot of best practice in data structure documentation testing automation and customer interaction and modeling 
singular value decomposition svd is computationally costly and therefore a naive implementation doe not scale to the need of scenario where data evolves continuously while there are various on line analysis and incremental decomposition technique these may not accurately represent the data or may be slow for the need of many application to address these challenge in this paper we propose a low rank windowed incremental svd lwi svd algorithm which a leverage efficient and accurate low rank approximation to speed up incremental svd update and b us a window based approach to aggregate multiple incoming update insertion or deletion of row and column and thus reduces online processing cost we also present an lwi svd with restarts lwi svd algorithm which leverage a novel highly efficient partial reconstruction based change detection scheme to support timely refreshing of the decomposition with significant change in the data and prevent accumulation of error over time experiment result including comparison to other state of the art technique on different data set and under different parameter setting confirm that lwi svd and lwi svd are both efficient and accurate in maintaining decomposition 
we introduce a novel graphical model the collaborative score topic model cstm for personal recommendation of textual document cstm s chief novelty lie in it learned model of individual library or set of document associated with each user overall cstm is a joint directed probabilistic model of user item score rating and the textual side information in the user library and the item creating a generative description of score and the text allows cstm to perform well in a wide variety of data regime smoothly combining the side information with observed rating a the number of rating available for a given user range from none to many experiment on real world datasets demonstrate cstm s performance we further demonstrate it utility in an application for personal recommendation of poster which we deployed at the nip conference 
community detection is an important task for social network which help u understand the functional module on the whole network among different community detection method based on graph structure modularity based method are very popular recently but suffer a well known resolution limit problem this paper connects modularity based method with correlation analysis by subtly reformatting their math formula and investigates how to fully make use of correlation analysis to change the objective function of modularity based method which provides a more natural and effective way to solve the resolution limit problem in addition a novel theoretical analysis on the upper bound of different objective function help u understand their bias to different community size and experiment are conducted on both real life and simulated data to validate our finding 
the u department of defense dictionary of military term defines the information environment a the aggregate of individual organization and system that collect process disseminate or act on information the decision and action we take both a individual and collectively simultaneously shape and are shaped by the information environment in which we live the nature of our interaction with the information environment is rapidly evolving and old model are becoming irrelevant faster than we can develop new one this result in uncertainty that leave u exposed to dangerous influence without proper defense the purpose of this talk is to help frame a new science of information environment security y whose goal is to create and apply the tool needed to discover and maintain fundamental model of our ever changing information environment and to defend u in that environment both a individual and collectively against intentional a well a unintentional attempt to deceive misinform and otherwise manipulate u y is an interdisciplinary science that will require bringing together expert working in area such a cognitive science computer science social science security marketing political campaigning public policy and psychology to develop a theoretical a well a an applied engineering methodology for managing the full spectrum of information environment security issue 
we consider the problem of open domain question answering open qa over massive knowledge base kb existing approach use either manually curated kb like freebase or kb automatically extracted from unstructured text in this paper we present oqa the first approach to leverage both curated and extracted kb a key technical challenge is designing system that are robust to the high variability in both natural language question and massive kb oqa achieves robustness by decomposing the full open qa problem into smaller sub problem including question paraphrasing and query reformulation oqa solves these sub problem by mining million of rule from an unlabeled question corpus and across multiple kb oqa then learns to integrate these rule by performing discriminative training on question answer pair using a latent variable structured perceptron algorithm we evaluate oqa on three benchmark question set and demonstrate that it achieves up to twice the precision and recall of a state of the art open qa system 
social scientist increasingly criticize the use of machine learning technique to understand human behavior criticism include they are atheoretical and hence of limited scientific value they do not address causality and are hence of limited policy value and they are uninterpretable and hence of limited generalizability value outside context very narrowly similar to the training dataset these criticism i argue miss the enormous opportunity offered by ml technique to fundamentally improve the practice of empirical social science yet each criticism doe contain a grain of truth and overcoming them will require innovation to existing methodology some of these innovation are being developed today and some are yet to be tackled i will in this talk sketch what these innovation look like or should look like why they are needed and the technical challenge they raise i will illustrate my point using a set of application that range from financial market to social policy problem to computational model of basic psychological process this talk describes joint work with jon kleinberg and individual project with himabindu lakkaraju jure leskovec jens ludwig anuj shah chenhao tan mike yeoman and tom zimmerman 
rating data is ubiquitous on website such a amazon tripadvisor or yelp since rating are not static but given at various point in time a temporal analysis of rating data provides deeper insight into the evolution of a product s quality in this work we tackle the following question given the time stamped rating data for a product or service how can we detect the general rating behavior of user a well a time interval where the rating behave anomalous we propose a bayesian model that represents the rating data a sequence of categorical mixture model in contrast to existing method our method doe not require any aggregation of the input but it operates on the original time stamped data to capture the dynamic effect of the rating the categorical mixture are temporally constrained anomaly can occur in specific time interval only and the general rating behavior should evolve smoothly over time our method automatically determines the interval where anomaly occur and it capture the temporal effect of the general behavior by using a state space model on the natural parameter of the categorical distribution for learning our model we propose an efficient algorithm combining principle from variational inference and dynamic programming in our experimental study we show the effectiveness of our method and we present interesting discovery on multiple real world datasets 
in this paper we report the first empirical study and live test of the reserve price optimisation problem in the context of real time bidding rtb display advertising from an operational environment a reserve price is the minimum that the auctioneer would accept from bidder in auction and in a second price auction it could potentially uplift the auctioneer s revenue by charging winner the reserve price instead of the second highest bid a such it ha been used for sponsored search and been well studied in that context however comparing with sponsored search and contextual advertising this problem in the rtb context is le understood yet more critical for publisher because bidder have to submit a bid for each individual impression which mostly is associated with user data that is subject to change over time this coupled with practical constraint such a the budget campaign life time etc make the theoretical result from optimal auction theory not necessarily applicable and a further empirical study is required to confirm it optimality from the real world system in rtb an advertiser is facing nearly unlimited supply and the auction is almost done in last second which encourages spending le on the high cost ad placement this could imply the loss of bid volume over time if a correct reserve price is not in place in this paper we empirically examine several commonly adopted algorithm for setting up a reserve price we report our result of a large scale online experiment in a production platform the result suggest the our proposed game theory based oneshot algorithm performed the best and the superiority is significant in most case 
online health community are a valuable source of information for patient and physician however such user generated resource are often plagued by inaccuracy and misinformation in this work we propose a method for automatically establishing the credibility of user generated medical statement and the trustworthiness of their author by exploiting linguistic cue and distant supervision from expert source to this end we introduce a probabilistic graphical model that jointly learns user trustworthiness statement credibility and language objectivity we apply this methodology to the task of extracting rare or unknown side effect of medical drug this being one of the problem where large scale non expert data ha the potential to complement expert medical knowledge we show that our method can reliably extract side effect and filter out false statement while identifying trustworthy user that are likely to contribute valuable medical information 
large enterprise it information technology infrastructure component generate large volume of alert and incident ticket these are manually screened but it is otherwise difficult to extract information automatically from them to gain insight in order to improve operational efficiency we propose a framework to cluster alert and incident ticket based on the text in them using unsupervised machine learning this would be a step towards eliminating manual classification of the alert and incident which is very labor intense and costly our framework can handle the semi structured text in alert generated by it infrastructure component such a storage device network device server etc a well a the unstructured text in incident ticket created manually by operation support personnel after text pre processing and application of appropriate distance metric we apply different graph theoretic approach to cluster the alert and incident ticket based on their semi structured and unstructured text respectively for automated interpretation and read ability on semi structured text cluster we propose a method to visualize cluster that preserve the structure and human readability of the text data a compared to traditional word cloud where the text structure is not preserved for unstructured text cluster we find a simple way to define prototype of cluster for easy interpretation this framework for clustering and visualization will enable enterprise to prioritize the issue in their it infrastructure and improve the reliability and availability of their service 
clustering categorical data pose some unique challenge due to missing order and spacing among the category selecting a suitable similarity measure is a difficult task many existing technique require the user to specify input parameter which are difficult to estimate moreover many technique are limited to detect cluster in the full dimensional data space only few method exist for subspace clustering and they produce highly redundant result therefore we propose rocat relevant overlapping subspace cluster on categorical data a novel technique based on the idea of data compression following the minimum description length principle rocat automatically detects the most relevant subspace cluster without any input parameter the relevance of each cluster is validated by it contribution to compress the data optimizing the trade off between goodness of fit and model complexity rocat automatically determines a meaningful number of cluster to represent the data rocat is especially designed to detect subspace cluster on categorical data which may overlap in object and or attribute i e object can be assigned to different cluster in different subspace and attribute may contribute to different subspace containing cluster rocat naturally avoids undesired redundancy in cluster and subspace by allowing overlap only if it improves the compression rate extensive experiment demonstrate the effectiveness and efficiency of our approach 
short text clustering ha become an increasingly important task with the popularity of social medium like twitter google and facebook it is a challenging problem due to it sparse high dimensional and large volume characteristic in this paper we proposed a collapsed gibbs sampling algorithm for the dirichlet multinomial mixture model for short text clustering abbr to gsdmm we found that gsdmm can infer the number of cluster automatically with a good balance between the completeness and homogeneity of the clustering result and is fast to converge gsdmm can also cope with the sparse and high dimensional problem of short text and can obtain the representative word of each cluster our extensive experimental study show that gsdmm can achieve significantly better performance than three other clustering model 
a recent pandemic such a sars and the swine flu outbreak have shown disease spread very fast in today s interconnected world making public health an important research area some of the basic question are how can an outbreak be contained before it becomes an epidemic and what disease surveillance strategy should be implemented these problem have been studied traditionally using differential equation method which are amenable to analysis and closed form solution however these model are based on complete mixing assumption which do not hold for realistic population thereby limiting their utility in this tutorial we focus on an approach based on diffusion process on complex network this capture more realistic population but lead to novel mathematical and computational challenge the structure of the underlying network ha a significant impact on the dynamical property motivating the need for improved network model and efficient algorithm for computing network and dynamical property that scale to large network we provide an overview of the state of the art in computational epidemiology which is a multi disciplinary research area that overlap different area in computer science including data mining machine learning high performance computing and theoretical computer science a well a mathematics economics and statistic specifically we will discus mathematical and computational model problem of inference forecasting and state assessment and epidemic containment 
we consider the problem of collaborative permutation recovery i e recovering multiple permutation over object e g preference ranking over different option from limited pairwise comparison we tackle both the problem of how to recover multiple related permutation from limited observation and the active learning problem of which pairwise comparison query to ask so a to allow better recovery there ha been much work on recovering single permutation from pairwise comparison but we show that considering several related permutation jointly we can leverage their relatedness so a to reduce the number of comparison needed compared to reconstructing each permutation separately to do so we take a collaborative filtering matrix completion approach and use a trace norm or max norm regularized matrix learning model our approach can also be seen a a collaborative learning version of jamieson and nowak s recent work on constrained permutation recovery where instead of basing the recovery on known feature we learn the best feature de novo 
recent year have witnessed a proliferation of large scale knowledge graph such a freebase yago google s knowledge graph and microsoft s satori whereas there is a large body of research on mining homogeneous graph this new generation of information network are highly heterogeneous with thousand of entity and relation type and billion of instance of vertex and edge in this tutorial we will present the state of the art in constructing mining and growing knowledge graph the purpose of the tutorial is to equip newcomer to this exciting field with an understanding of the basic concept tool and methodology available datasets and open research challenge a publicly available knowledge base freebase will be used throughout the tutorial to exemplify the different technique 
in many application we have a social network of people and would like to identify the member of an interesting but unlabeled group or community we start with a small number of exemplar group member they may be follower of a political ideology or fan of a music genre and need to use those example to discover the additional member this problem give rise to the seed expansion problem in community detection given example community member how can the social graph be used to predict the identity of remaining hidden community member in contrast with global community detection graph partitioning or covering seed expansion is best suited for identifying community locally concentrated around node of interest a growing body of work ha used seed expansion a a scalable mean of detecting overlapping community yet despite growing interest in seed expansion there are divergent approach in the literature and there still isn t a systematic understanding of which approach work best in different domain here we evaluate several variant and uncover subtle trade offs between different approach we explore which property of the seed set can improve performance focusing on heuristic that one can control in practice a a consequence of this systematic understanding we have found several opportunity for performance gain we also consider an adaptive version in which request are made for additional membership label of particular node such a one find in field study of social community this lead to interesting connection and contrast with active learning and the trade offs of exploration and exploitation finally we explore topological property of community and seed set that correlate with algorithm performance and explain these empirical observation with theoretical one we evaluate our method across multiple domain using publicly available datasets with labeled ground truth community 
we consider a search task a a set of query that serve the same user information need analyzing search task from user query stream play an important role in building a set of modern tool to improve search engine performance in this paper we propose a probabilistic method for identifying and labeling search task based on the following intuitive observation query that are issued temporally close by user in many sequence of query are likely to belong to the same search task meanwhile different user having the same information need tend to submit topically coherent search query to capture the above intuition we directly model query temporal pattern using a special class of point process called hawkes process and combine topic model with hawkes process for simultaneously identifying and labeling search task essentially hawkes process utilize their self exciting property to identify search task if influence exists among a sequence of query for individual user while the topic model exploit query co occurrence across different user to discover the latent information needed for labeling search task more importantly there is mutual reinforcement between hawkes process and the topic model in the unified model that enhances the performance of both we evaluate our method based on both synthetic data and real world query log data in addition we also apply our model to query clustering and search task identification by comparing with state of the art method the result demonstrate that the improvement in our proposed approach is consistent and promising 
we often care about people s degree of belief about certain event e g causality between an action and the outcome odds distribution among the outcome of a horse race and so on it is well recognized that the best form to elicit opinion from human is probability distribution instead of simple voting because the form of distribution retains the delicate information that an opinion express in the past opinion elicitation ha relied on expert who are expensive and not always available more recently crowdsourcing ha gained prominence a an inexpensive way to get a great deal of human input however traditional crowdsourcing ha primarily focused on issuing very simple e g binary decision task to the crowd in this paper we study how to use crowd for opinion elicitation there are three major challenge to eliciting opinion information in the form of probability distribution how to measure the quality of distribution how to aggregate the distribution and how to strategically implement such a system to address these challenge we design and implement cope crowd powered opinion elicitation market cope model crowdsourced work a a trading market where the worker behave like trader to maximize their profit by presenting their opinion among the innovative feature in this system we design cope updating to combine the multiple elicited distribution following a bayesian scheme also to provide more flexibility while running cope we propose a series of efficient algorithm and a slope based strategy to manage the ending condition of cope we then demonstrate the implementation of cope and report experimental result running on real commercial platform to demonstrate the practical value of this system 
inferring phenotypic pattern from population scale clinical data is a core computational task in the development of personalized medicine one important source of data on which to conduct this type of research is patient electronic medical record emr however the patient emrs are typically sparse and noisy which creates significant challenge if we use them directly to represent patient phenotype in this paper we propose a data driven phenotyping framework called pacifier patient record densifier where we interpret the longitudinal emr data of each patient a a sparse matrix with a feature dimension and a time dimension and derive more robust patient phenotype by exploring the latent structure of those matrix specifically we assume that each derived phenotype is composed of a subset of the medical feature contained in original patient emr whose value evolves smoothly over time we propose two formulation to achieve such goal one is individual basis approach iba which assumes the phenotype are different for every patient the other is shared basis approach sba which assumes the patient population share a common set of phenotype we develop an efficient optimization algorithm that is capable of resolving both problem efficiently finally we validate pacifier on two real world emr cohort for the task of early prediction of congestive heart failure chf and end stage renal disease esrd our result show that the predictive performance in both task can be improved significantly by the proposed algorithm average auc score improved from to on chf and from to on esrd respectively on diagnosis group granularity we also illustrate some interesting phenotype derived from our data 
selecting an informative subset of feature ha important application in many data mining task especially for high dimensional data recently simultaneous selection of feature and feature group a k a bi level selection becomes increasingly popular since it not only reduces the number of feature but also unveils the underlying grouping effect in the data which is a valuable functionality in many application such a bioinformatics and web data mining one major challenge of bi level selection or even feature selection only is that computing a globally optimal solution requires a prohibitive computational cost to overcome such a challenge current research mainly fall into two category the first one focus on finding suitable continuous computational surrogate for the discrete function and this lead to various convex and nonconvex optimization model although efficient convex model usually deliver sub optimal performance while nonconvex model on the other hand require significantly more computational effort another direction is to use greedy algorithm to solve the discrete optimization directly however existing algorithm are proposed to handle single level selection only and it remains challenging to extend these method to handle bi level selection in this paper we fulfill this gap by introducing an efficient sparse group hard thresholding algorithm our main contribution are we propose a novel bi level selection model and show that the key combinatorial problem admits a globally optimal solution using dynamic programming we provide an error bound between our solution and the globally optimal under the rip restricted isometry property theoretical framework our experiment on synthetic and real data demonstrate that the proposed algorithm produce encouraging performance while keeping comparable computational efficiency to convex relaxation model 
internet display advertising is a critical revenue source for publisher and online content provider and is supported by massive amount of user and publisher data targeting display ad can be improved substantially with machine learning method but building many model on massive data becomes prohibitively expensive computationally this paper present a combination of strategy deployed by the online advertising firm dstillery for learning many model from extremely high dimensional data efficiently and without human intervention this combination includes i a method for simple yet effective transfer learning where a model learned from data that is relatively abundant and cheap is taken a a prior for bayesian logistic regression trained with stochastic gradient descent sgd from the more expensive target data ii a new update rule for automatic learning rate adaptation to support learning from sparse high dimensional data a well a the integration with adaptive regularization we present an experimental analysis across different ad campaign showing that the transfer learning indeed improves performance across a large number of them especially at the start of the campaign the combined hand free method need no fiddling with the sgd learning rate and we show that it is just a effective a using expensive grid search to set the regularization parameter for each campaign 
strategic planning and talent management in large enterprise composed of knowledge worker requires complete accurate and up to date representation of the expertise of employee in a form that integrates with business process like other similar organization operating in dynamic environment the ibm corporation strives to maintain such current and correct information specifically assessment of employee against job role and skill set from it expertise taxonomy in this work we deploy an analytics driven solution that infers the expertise of employee through the mining of enterprise and social data that is not specifically generated and collected for expertise inference we consider job role and specialty prediction and pose them a supervised classification problem we evaluate a large number of feature set predictive model and postprocessing algorithm and choose a combination for deployment this expertise analytics system ha been deployed for key employee population segment yielding large reduction in manual effort and the ability to continually and consistently serve up to date and accurate data for several business function this expertise management system is in the process of being deployed throughout the corporation 
web site owner from small web site to the largest property that include amazon facebook google linkedin microsoft and yahoo attempt to improve their web site optimizing for criterion ranging from repeat usage time on site to revenue having been involved in running thousand of controlled experiment at amazon booking com linkedin and multiple microsoft property we share seven rule of thumb for experimenter which we have generalized from these experiment and their result these are principle that we believe have broad applicability in web optimization and analytics outside of controlled experiment yet they are not provably correct and in some case exception are known to support these rule of thumb we share multiple real example most being shared in a public paper for the first time some rule of thumb have previously been stated such a speed matter but we describe the assumption in the experimental design and share additional experiment that improved our understanding of where speed matter more certain area of the web page are more critical this paper serf two goal first it can guide experimenter with rule of thumb that can help them optimize their site second it provides the kdd community with new research challenge on the applicability exception and extension to these one of the goal for kdd s industrial track 
chronic obstructive pulmonary disease copd is a lung disease characterized by airflow limitation usually associated with an inflammatory response to noxious particle such a cigarette smoke copd is currently the third leading cause of death in the united state and is the only leading cause of death that is increasing in prevalence it also represents an enormous financial burden to society costing ten of billion of dollar annually in the u s it is widely accepted by the medical community that copd is a heterogeneous disease with substantial evidence indicating that genetic variation contributes to varying level of disease susceptibility this heterogeneity make it difficult to predict health decline and develop targeted treatment for better patient care although researcher have made several attempt to discover disease subtypes result have been inconclusive in part because standard clustering method have not properly dealt with disease manifestation that may worsen with increased exposure in this paper we introduce a transformative way of looking at the copd subtyping task specifically we model the relationship between risk factor such a age and smoke exposure and manifestation of disease severity using gaussian process which allow u to represent so called disease trajectory we also posit that individual can be associated with multiple disease type latent cluster which we assume are influenced by genetics furthermore we predict that only subset of the numerous disease related quantitative feature are useful for describing each latent subtype we model these association using two separate beta process prior and we describe a variational inference approach to discover the most probable latent cluster assignment result are validated with association to genetic marker 
million of people use social network everyday to talk about a variety of subject publish opinion and share information understanding this data to infer user s topical interest is a challenging problem with application in various data powered product in this paper we present lasta large scale topic assignment a full production system used at klout inc which mine topical interest from five social network and assigns over topic to hundred of million of user on a daily basis the system continuously collect stream of user data and is reactive to fresh information updating topic for user a interest shift lasta generates over distinct feature derived from signal such a user generated post and profile user reaction such a comment and retweets user attribution such a list tag and endorsement a well a signal based on social graph connection we show that using this diverse set of feature lead to a better representation of a user s topical interest a compared to using only generated text or only graph based feature we also show that using cross network information for a user lead to a more complete and accurate understanding of the user s topic a compared to using any single network we evaluate lasta s topic assignment system on an internal labeled corpus of user topic label generated from real user 
the analysis of network connection diffusion process and cascade requires evaluating property of the diffusion network property of interest often involve variable that are not explicitly observed in real world diffusion connection strength in the network and diffusion path of infection over the network are example of such hidden variable these hidden variable therefore need to be estimated for these property to be evaluated in this paper we propose and study this novel problem in a bayesian framework by capturing the posterior distribution of these hidden variable given the observed cascade and computing the expectation of these property under this posterior distribution we identify and characterize interesting network diffusion property whose expectation can be computed exactly and efficiently either wholly or in part for property that are not nice in this sense we propose a gibbs sampling framework for monte carlo integration in detailed experiment using various network diffusion property over multiple synthetic and real datasets we demonstrate that the proposed approach is significantly more accurate than a frequentist plug in baseline we also propose a map reduce implementation of our framework and demonstrate that this can analyze cascade with million of infection in minute 
many clustering method partition the data group based on the input data similarity matrix thus the clustering result highly depend on the data similarity learning because the similarity measurement and data clustering are often conducted in two separated step the learned data similarity may not be the optimal one for data clustering and lead to the suboptimal result in this paper we propose a novel clustering model to learn the data similarity matrix and clustering structure simultaneously our new model learns the data similarity matrix by assigning the adaptive and optimal neighbor for each data point based on the local distance meanwhile the new rank constraint is imposed to the laplacian matrix of the data similarity matrix such that the connected component in the resulted similarity matrix are exactly equal to the cluster number we derive an efficient algorithm to optimize the proposed challenging problem and show the theoretical analysis on the connection between our method and the k mean clustering and spectral clustering we also further extend the new clustering model for the projected clustering to handle the high dimensional data extensive empirical result on both synthetic data and real world benchmark data set show that our new clustering method consistently outperforms the related clustering approach 
the hierarchical dirichlet process hdp is an intuitive and elegant technique to model data with latent group however it ha not been widely used for practical application due to the high computational cost associated with inference in this paper we propose an effective parallel gibbs sampling algorithm for hdp by exploring it connection with the gamma gamma poisson process specifically we develop a novel framework that combine bootstrap and reversible jump mcmc algorithm to enable parallel variable update we also provide theoretical convergence analysis based on gibbs sampling with asynchronous variable update experiment result on both synthetic datasets and two large scale text collection show that our algorithm can achieve considerable speedup a well a better inference accuracy for hdp compared with existing parallel sampling algorithm 
behavioral pattern discovery is increasingly being studied to understand human behavior and the discovered pattern can be used in many real world application such a web search recommender system and advertisement targeting traditional method usually consider the behavior a simple user and item connection or represent them with a static model in real world however human behavior are actually complex and dynamic they include correlation between user and multiple type of object and also continuously evolve along time these characteristic cause severe data sparsity and computational complexity problem which pose great challenge to human behavioral analysis and prediction in this paper we propose a flexible evolutionary multi faceted analysis fema framework for both behavior prediction and pattern mining fema utilizes a flexible and dynamic factorization scheme for analyzing human behavioral data sequence which can incorporate various knowledge embedded in different object domain to alleviate the sparsity problem we give approximation algorithm for efficiency where the bound of approximation loss is theoretically proved we extensively evaluate the proposed method in two real datasets for the prediction of human behavior the proposed fema significantly outperforms other state of the art baseline method by moreover fema is able to discover quite a number of interesting multi faceted temporal pattern on human behavior with good interpretability more importantly it can reduce the run time from hour to minute which is significant for industry to serve real time application 
this paper present an efficient active transductive approach for classification a common approach of active learning algorithm is to focus on querying point near the class boundary in order to refine it however for certain data distribution this approach ha been shown to lead to uninformative sample more recent approach consider combining data exploration with traditional refinement technique these technique typically require tuning sampling of unexplored region with refinement of detected class boundary they also involve significant computational cost for the exploration of informative query candidate we present a novel iterative active learning algorithm designed to overcome these shortcoming by using a linear running time active transductive learning approach that naturally switch from exploration to refinement the passive classifier employed in our algorithm build a random walk on the data graph based on a modified graph geometry that combine the data distribution with current label hypothesis while the query component us the uncertainty of the evolving hypothesis our supporting theory draw the link between the spectral property of our iteration matrix and a solution to the minimal cut problem for a fused hypothesis data graph experiment demonstrate computational complexity that is order of magnitude lower than state of the art and competitive result on benchmark data and real churn prediction data 
spreadsheet contain valuable data on many topic however spreadsheet are difficult to integrate with other data source converting spreadsheet data to the relational model would allow data analyst to use relational integration tool we propose a two phase semiautomatic system that extract accurate relational metadata while minimizing user effort based on an undirected graphical model our system enables downstream spreadsheet integration application first the automatic extractor us hint from spreadsheet graphical style and recovered metadata to extract the spreadsheet data a accurately a possible second the interactive repair identifies similar region in distinct spreadsheet scattered across large spreadsheet corpus allowing a user s single manual repair to be amortized over many possible extraction error our experiment show that a human can obtain the accurate extraction with just of the manual operation required by a standard classification based technique on two real world datasets 
recommendation and review site offer a wealth of information beyond rating for instance on imdb user leave review commenting on different aspect of a movie e g actor plot visual effect and expressing their sentiment positive or negative on these aspect in their review this suggests that uncovering aspect and sentiment will allow u to gain a better understanding of user movie and the process involved in generating rating the ability to answer question such a doe this user care more about the plot or about the special effect or what is the quality of the movie in term of acting help u to understand why certain rating are generated this can be used to provide more meaningful recommendation in this work we propose a probabilistic model based on collaborative filtering and topic modeling it allows u to capture the interest distribution of user and the content distribution for movie it provides a link between interest and relevance on a per aspect basis and it allows u to differentiate between positive and negative sentiment on a per aspect basis unlike prior work our approach is entirely unsupervised and doe not require knowledge of the aspect specific rating or genre for inference we evaluate our model on a live copy crawled from imdb our model offer superior performance by joint modeling moreover we are able to address the cold start problem by utilizing the information inherent in review our model demonstrates improvement for new user and movie 
in data mining application such a crowdsourcing and privacy preserving data mining one may wish to obtain consolidated prediction out of multiple model without access to feature of the data besides multiple model usually carry complementary predictive information model combination can potentially provide more robust and accurate prediction by correcting independent error from individual model various method have been proposed to combine prediction such that the final prediction are maximally agreed upon by multiple base model though this maximum consensus principle ha been shown to be successful simply maximizing consensus can lead to le discriminative prediction and overfit the inevitable noise due to imperfect base model we argue that proper regularization for model combination approach is needed to alleviate such overfitting effect specifically we analyze the hypothesis space of several model combination method and identify the trade off between model consensus and generalization ability we propose a novel model called regularized consensus maximization rcm which is formulated a an optimization problem to combine the maximum consensus and large margin principle we theoretically show that rcm ha a smaller upper bound on generalization error compared to the version without regularization experiment show that the proposed algorithm outperforms a wide spectrum of state of the art model combination method on task 
large scale data center network are complex comprising several thousand network device and several hundred thousand link and form the critical infrastructure upon which all higher level service depend on despite the built in redundancy in data center network performance issue and device or link failure in the network can lead to user perceived service interruption therefore determining and localizing user impacting availability and performance issue in the network in near real time is crucial traditionally both passive and active monitoring approach have been used for failure localization however data from passive monitoring is often too noisy and doe not effectively capture silent or gray failure whereas active monitoring is potent in detecting fault but limited in it ability to isolate the exact fault location depending on it scale and granularity our key idea is to use statistical data mining technique on large scale active monitoring data to determine a ranked list of suspect cause which we refine with passive monitoring signal in particular we compute a failure probability for device and link in near real time using data from active monitoring and look for statistically significant increase in the failure probability we also correlate the probabilistic output with other failure signal from passive monitoring to increase the confidence of the probabilistic analysis we have implemented our approach in the window azure production environment and have validated it effectiveness in term of localization accuracy precision and time to localization using known network incident over the past three month the correlated ranked list of device and link is surfaced a a report that is used by network operator to investigate current issue and identify probable root cause 
online social network offering various service have become ubiquitous in our daily life meanwhile user nowadays are usually involved in multiple online social network simultaneously to enjoy specific service provided by different network formally social network that share some common user are named a partially aligned network in this paper we want to predict the formation of social link in multiple partially aligned social network at the same time which is formally defined a the multi network link formation prediction problem in multiple partially aligned social network user can be extensively correlated with each other by various connection to categorize these diverse connection among user intra network social meta path and category of inter network social meta path are proposed in this paper these social meta path can cover a wide variety of connection information in the network some of which can be helpful for solving the multi network link prediction problem but some can be not to utilize useful connection a subset of the most informative social meta path are picked the process of which is formally defined a social meta path selection in this paper an effective general link formation prediction framework mli multi network link identifier is proposed in this paper to solve the multi network link formation prediction problem built with heterogenous topological feature extracted based on the selected social meta path in the multiple partially aligned social network mli can help refine and disambiguate the prediction result reciprocally in all aligned network extensive experiment conducted on real world partially aligned heterogeneous network foursquare and twitter demonstrate that mli can solve the multi network link prediction problem very well 
identifying interpretable discriminative high order feature interaction given limited training data in high dimension is challenging in both machine learning and data mining in this paper we propose a factorization based sparse learning framework termed fhim for identifying high order feature interaction in linear and logistic regression model and study several optimization method for solving them unlike previous sparse learning method our model fhim recovers both the main effect and the interaction term accurately without imposing tree structured hierarchical constraint furthermore we show that fhim ha oracle property when extended to generalized linear regression model with pairwise interaction experiment on simulated data show that fhim outperforms the state of the art sparse lear ning technique further experiment on our experimentally generated data from patient blood sample using a novel somamer slow off rate modified aptamer technology show that fhim performs blood based cancer diagnosis and bio marker discovery for renal cell carcinoma much better than other competing method and it identifies interpretable block wise high order gene interaction predictive of cancer stage of sample a literature survey show that the interaction identified by fhim play important role in cancer development 
purchasing decision in many product category are heavily influenced by the shopper s aesthetic preference it s insufficient to simply match a shopper with popular item from the category in question a successful shopping experience also identifies product that match those aesthetic the challenge of capturing shopper style becomes more difficult a the size and diversity of the marketplace increase at etsy an online marketplace for handmade and vintage good with over million diverse listing the problem of capturing taste is particularly important user come to the site specifically to find item that match their eclectic style in this paper we describe our method and experiment for deploying two new style based recommender system on the etsy site we use latent dirichlet allocation lda to discover trending category and style on etsy which are then used to describe a user s interest profile we also explore hashing method to perform fast nearest neighbor search on a map reduce framework in order to efficiently obtain recommendation these technique have been implemented successfully at very large scale substantially improving many key business metric 
core decomposition ha proven to be a useful primitive for a wide range of graph analysis one of it most appealing feature is that unlike other notion of dense subgraphs it can be computed linearly in the size of the input graph in this paper we provide an analogous tool for uncertain graph i e graph whose edge are assigned a probability of existence the fact that core decomposition can be computed efficiently in deterministic graph doe not guarantee efficiency in uncertain graph where even the simplest graph operation may become computationally intensive here we show that core decomposition of uncertain graph can be carried out efficiently a well we extensively evaluate our definition and method on a number of real world datasets and application such a influence maximization and task driven team formation 
solving the missing value mv problem with small estimation error in big data environment is a notoriously resource demanding task a datasets and their user community continuously grow the problem can only be exacerbated assume that it is possible to have a single machine godzilla which can store the massive dataset and support an ever growing community submitting mv imputation request is it possible to replace godzilla by employing a large number of cohort machine so that imputation can be performed much faster engaging cohort in parallel each of which access much smaller partition of the original dataset if so it would be preferable for obvious performance reason to access only a subset of all cohort per imputation in this case can we decide swiftly which is the desired subset of cohort to engage per imputation but efficiency and scalability is just one key concern is it possible to do the above while ensuring comparable or even better than godzilla s imputation estimation error in this paper we derive answer to these fundamental question and develop principled method and a framework which offer large performance speed ups and better or comparable error to that of godzilla independently of which missing value imputation algorithm is used our contribution involve pythia a framework and algorithm for providing the answer to the above question and for engaging the appropriate subset of cohort per mv imputation request pythia functionality rest on two pillar i dataset partition signature one per cohort and ii similarity notion and algorithm which can identify the appropriate subset of cohort to engage comprehensive experimentation with real and synthetic datasets showcase our efficiency scalability and accuracy claim 
the detection of abnormal moving object over high volume trajectory stream is critical for real time application ranging from military surveillance to transportation management yet this problem remains largely unexplored in this work we first propose class of novel trajectory outlier definition that model the anomalous behavior of moving object for a large range of real time application our theoretical analysis and empirical study on the beijing taxi and gmti ground moving target indicator datasets demonstrate it effectiveness in capturing abnormal moving object furthermore we propose a general strategy for efficiently detecting the new outlier class it feature three fundamental optimization principle designed to minimize the detection cost our comprehensive experimental study demonstrate that our proposed strategy drive the detection cost fold down into practical realm for application producing high volume trajectory stream to utilize 
e commerce ha largely been a pull model to date offline retailer have nailed discovery delight serendipity and impulse purchase in person with greater success than online commerce site however in an always on mobile first world company like groupon have the opportunity to push the frontier even further than offline retailer or comprehensive site due to the fact that our smartphones are always with u the challenge is to provide the right deal to the right user at the right time that involves learning about the user and their location their personal preference and predicting which deal are likely to delight them presenting diversity discovery and engaging ux to gather user preference and semantic graph approach for user deal matching this presentation will give insight into how groupon manages to grapple with these challenge via a data driven system in order to delight and surprise customer 
in the era of ehrs it is possible to examine the outcome of decision made by doctor during clinical practice to identify pattern of care generating evidence based on the collective practice of expert we will discus method that use unstructured patient data to monitor for adverse drug event profile specific drug identify off label drug usage uncover natural experiment and generate practice based evidence for difficult to test clinical hypothesis we will describe how to detect association among drug and their adverse event several year before an alert is issued a well a compute the true rate of drug drug interaction we will present approach to identify novel off label us of drug using the patient feature matrix along with prior knowledge about drug disease and known usage we will review a natural experiment where a subset of congestive heart failure patient who were prescribed cilostazol despite it black box warning and profile it safety we will discus the testing of a clinical hypothesis about an association between allergic condition and chronic uveitis in patient with juvenile idiopathic arthritis 
deep learning ha rapidly moved from a marginal approach in the machine learning community le than ten year ago to one that ha strong industrial impact in particular for high dimensional perceptual data such a speech and image but also natural language the demand for expert in deep learning is growing very fast faster than we can graduate phd thereby considerably increasing their market value deep learning is based on the idea of learning multiple level of representation with higher level computed a a function of lower level and corresponding to more abstract concept automatically discovered by the learner deep learning arose out of research on artificial neural network and graphical model and the literature on that subject ha considerably grown in recent year culminating in the creation of a dedicated conference iclr the tutorial will introduce some of the basic algorithm both on the supervised and unsupervised side a well a discus some of the guideline for successfully using them in practice finally it will introduce current research question regarding the challenge of scaling up deep learning to much larger model that can successfully extract information from huge datasets 
with the rapid development of online social network a growing number of people are willing to share their group activity e g having dinner with colleague and watching movie with spouse this motivates the study on group recommendation which aim to recommend item for a group of user group recommendation is a challenging problem because different group member have different preference and how to make a trade off among their preference for recommendation is still an open problem in this paper we propose a probabilistic model named com consensus model to model the generative process of group activity and make group recommendation intuitively user in a group may have different influence and those who are expert in topic relevant to the group are usually more influential in addition user in a group may behave differently a group member from a individual com is designed based on these intuition and is able to incorporate both user selection history and personal consideration of content factor when making recommendation com estimate the preference of a group to an item by aggregating the preference of the group member with different weight we conduct extensive experiment on four datasets and the result show that the proposed model is effective in making group recommendation and outperforms baseline method significantly 
the objective in extreme multi label classification is to learn a classifier that can automatically tag a data point with the most relevant subset of label from a large label set extreme multi label classification is an important research problem since not only doe it enable the tackling of application with many label but it also allows the reformulation of ranking problem with certain advantage over existing formulation our objective in this paper is to develop an extreme multi label classifier that is faster to train and more accurate at prediction than the state of the art multi label random forest mlrf algorithm and the label partitioning for sub linear ranking lpsr algorithm mlrf and lpsr learn a hierarchy to deal with the large number of label but optimize task independent measure such a the gini index or clustering error in order to learn the hierarchy our proposed fastxml algorithm achieves significantly higher accuracy by directly optimizing an ndcg based ranking loss function we also develop an alternating minimization algorithm for efficiently optimizing the proposed formulation experiment reveal that fastxml can be trained on problem with more than a million label on a standard desktop in eight hour using a single core and in an hour using multiple core 
processing large volume of streaming data in near real time is becoming increasingly important a the internet sensor network and network traffic grow online machine learning is a typical mean of dealing with streaming data since it allows the classification model to learn one instance of data at a time although many online learning method have been developed since the development of the perceptron algorithm existing online method assume that the number of class is available in advance of classification process however this assumption is unrealistic for large scale or streaming data set this work proposes an online chinese restaurant process crp algorithm which is an online and nonparametric algorithm to tackle this problem this work proposes a relaxing function a part of the prior and update the parameter with the likelihood function in term of the consistency between the true label information and predicted result this work present two gibbs sampling algorithm to perform posterior inference in the experiment the online crp is applied to three massive data set and compared with several online learning and batch learning algorithm one of the data set is obtained from wikipedia which comprises approximately two million document the experimental result reveal that the proposed online crp performs well and efficiently on massive data set finally this work proposes two method to update the hyperparameter alpha of the online crp the first method is based on the posterior distribution of alpha and the second exploit the property of online learning namely adapting to change to adjust alpha dynamically 
crime reduction and prevention strategy are essential to increase public safety and reduce the crime cost to society law enforcement agency have long realized the importance of analyzing co offending network network of offender who have committed crime together for this purpose although network structure can contribute significantly to co offence prediction research in this area is very limited here we address this important problem by proposing a framework for co offence prediction using supervised learning considering the available information about offender we introduce social geographic geo social and similarity feature set which are used for classifying potential negative and positive pair of offender similar to other social network co offending network also suffer from a highly skewed distribution of positive and negative pair to address the class imbalance problem we identify three type of criminal cooperation opportunity which help to reduce the class imbalance ratio significantly while keeping half of the co offence the proposed framework is evaluated on a large crime dataset for the province of british columbia canada our experimental evaluation of four different feature set show that the novel geo social feature are the best predictor overall we experimentally show the high effectiveness of the proposed co offence prediction framework we believe that our framework will not only allow law enforcement agency to improve their crime reduction and prevention strategy but also offer new criminological insight into criminal link formation between offender 
accurate knowledge of a patient s disease state and trajectory is critical in a clinical setting modern electronic healthcare record contain an increasingly large amount of data and the ability to automatically identify the factor that influence patient outcome stand to greatly improve the efficiency and quality of care we examined the use of latent variable model viz latent dirichlet allocation to decompose free text hospital note into meaningful feature and the predictive power of these feature for patient mortality we considered three prediction regime baseline prediction dynamic time varying outcome prediction and retrospective outcome prediction in each our prediction task differs from the familiar time varying situation whereby data accumulates since fewer patient have long icu stay a we move forward in time fewer patient are available and the prediction task becomes increasingly difficult we found that latent topic derived feature were effective in determining patient mortality under three timeline inhospital day post discharge and year post discharge mortality our result demonstrated that the latent topic feature important in predicting hospital mortality are very different from those that are important in post discharge mortality in general latent topic feature were more predictive than structured feature and a combination of the two performed best the time varying model that combined latent topic feature and baseline feature had auc that reached and for in hospital day post discharge and year post discharge mortality respectively our result agreed with other work suggesting that the first hour of patient information are often the most predictive of hospital mortality retrospective model that used a combination of latent topic feature and structured feature achieved auc of and for in hospital day and year mortality prediction our work focus on the dynamic time varying setting because model from this regime could facilitate an on going severity stratification system that help direct care staff resource and inform treatment strategy 
analyzing biomedical big data bbd is computationally expensive due to high dimensionality and large data volume performance and scalability issue of traditional database management system dbms often limit the usage of more sophisticated and complex data query and analytic model moreover in the conventional setting data management and analysis use separate software platform exporting and importing large amount of data across platform require a significant amount of computational and i o resource a well a potentially putting sensitive data at a security risk in this tutorial the participant will learn the difference between in memory dbms and traditional dbms through hand on exercise using sap s cloud based hana in memory dbms in conjunction with the multi parameter intelligent monitoring in intensive care mimic dataset mimic is an open access critical care ehr archive over tb in size and consists of structured unstructured and waveform data furthermore this tutorial will seek to educate the participant on how a combination of dynamic querying and in memory dbms may enhance the management and analysis of complex clinical data 
the rapid growth of information source on the web ha intensified the problem of data quality in particular the same real world entity may be described by different source in various way with overlapping information and possibly conflicting or even erroneous value in order to obtain a more complete and accurate picture for a real world entity we need to collate the data record that refer to the entity a well a correct any erroneous value we observe that these two task are often tightly coupled rectifying erroneous value will facilitate data collation while linking similar record provides u with a clearer view of the data and additional evidence for error correction in this paper we present a framework called comet that interleaf record linkage with error correction taking into consideration the source reliability on various attribute the proposed framework first utilizes confidence based matching to discriminate record in term of ambiguity and source reliability then it performs adaptive matching to reduce the impact of erroneous value experiment result demonstrate that comet outperforms the state of the art technique and is able to build complete and accurate profile for real world entity 
given a class of large number of student each exhibiting a different ability level how can we group them into section so that the overall gain for student is maximized this question ha been a topic of central concern and debate amongst social scientist and policy maker for a long time we propose a framework for rigorously studying this question taking a computational perspective we present a formal definition of the grouping problem and investigate some of it variant such variant are determined by the desired number of group a well a the definition of the gain for each student in the group we focus on two natural instantiation of the gain function and we show that for both of them the problem of identifying a single group of student that maximizes the gain among it member can be solved in polynomial time the corresponding partitioning problem where the goal is to partition the student into non overlapping group appear to be much harder however the algorithm for the single group version can be leveraged for solving the more complex partitioning problem our experiment with generated data coming from different distribution demonstrate that our algorithm is significantly better than the current strategy in vogue for dividing student in a class into section 
distance query are a basic tool in data analysis they are used for detection and localization of change for the purpose of anomaly detection monitoring or planning distance query are particularly useful when data set such a measurement snapshot of a system content traffic matrix and activity log are collected repeatedly random sampling which can be efficiently performed over streamed or distributed data is an important tool for scalable data analysis the sample constitutes an extremely flexible summary which naturally support domain query and scalable estimation of statistic which can be specified after the sample is generated the effectiveness of a sample a a summary however hinge on the estimator we have we derive novel estimator for estimating l p distance from sampled data our estimator apply with the most common weighted sampling scheme poisson probability proportional to size pps and it fixed sample size variant they also apply when the sample of different data set are independent or coordinated our estimator are admissible pareto optimal in term of variance and have compelling property we study the performance of our manhattan and euclidean distance p estimator on diverse datasets demonstrating scalability and accuracy even when a small fraction of the data is sampled our work for the first time facilitates effective distance estimation over sampled data 
people s interest are dynamically evolving often affected by external factor such a trend promoted by the medium or adopted by their friend in this work we model interest evolution through dynamic interest cascade we consider a scenario where a user s interest may be affected by a the interest of other user in her social circle a well a b suggestion she receives from a recommender system in the latter case we model user reaction through either attraction or aversion towards past suggestion we study this interest evolution process and the utility accrued by recommendation a a function of the system s recommendation strategy we show that in steady state the optimal strategy can be computed a the solution of a semi definite program sdp using datasets of user rating we provide evidence for the existence of aversion and attraction in real life data and show that our optimal strategy can lead to significantly improved recommendation over system that ignore aversion and attraction 
given a large collection of epidemiological data consisting of the count of d contagious disease for l location of duration n how can we find pattern rule and outlier for example the project tycho provides open access to the count infection for u s state from to for contagious disease e g measles influenza which include missing value possible recording error sudden spike or dive of infection etc so how can we find a combined model for all these disease location and time tick in this paper we present funnel a unifying analytical model for large scale epidemiological data a well a a novel fitting algorithm funnelfit which solves the above problem our method ha the following property a sense making it detects important pattern of epidemic such a periodicity the appearance of vaccine external shock event and more b parameter free our modeling framework free the user from providing parameter value c scalable funnelfit is carefully designed to be linear on the input size d general our model is general and practical which can be applied to various type of epidemic including computer virus propagation a well a human disease extensive experiment on real data demonstrate that funnelfit doe indeed discover important property of epidemic p disease seasonality e g influenza spike in january lyme disease spike in july and the absence of yearly periodicity for gonorrhea p disease reduction effect e g the appearance of vaccine p local state level sensitivity e g many measles case in ny p external shock event e g historical flu pandemic p detect incongruous value i e data reporting error 
this paper is concerned with the problem of personalized diversification of search result with the goal of enhancing the performance of both plain diversification and plain personalization algorithm in previous work the problem ha mainly been tackled by mean of unsupervised learning to further enhance the performance we propose a supervised learning strategy specifically we set up a structured learning framework for conducting supervised personalized diversification in which we add feature extracted directly from the token of document and those utilized by unsupervised personalized diversification algorithm and importantly those generated from our proposed user interest latent dirichlet topic model based on our proposed topic model whether a document can cater to a user s interest can be estimated in our learning strategy we also define two constraint in our structured learning framework to ensure that search result are both diversified and consistent with a user s interest we conduct experiment on an open personalized diversification dataset and find that our supervised learning strategy outperforms unsupervised personalized diversification method a well a other plain personalization and plain diversification method 
hashing ha enjoyed a great success in large scale similarity search recently researcher have studied the multi modal hashing to meet the need of similarity search across different type of medium however most of the existing method are applied to search across multi view among which explicit bridge information is provided given a heterogeneous medium search task we observe that abundant multi view data can be found on the web which can serve a an auxiliary bridge in this paper we propose a heterogeneous translated hashing hth method with such auxiliary bridge incorporated not only to improve current multi view search but also to enable similarity search across heterogeneous medium which have no direct correspondence hth simultaneously learns hash function embedding heterogeneous medium into different hamming space and translator aligning these space unlike almost all existing method that map heterogeneous data in a common hamming space mapping to different space provides more flexible and discriminative ability we empirically verify the effectiveness and efficiency of our algorithm on two real world large datasets one publicly available dataset of flickr and the other mirflickr yahoo answer dataset 
the rapidly increasing availability of electronic health record ehrs from multiple heterogeneous source ha spearheaded the adoption of data driven approach for improved clinical research decision making prognosis and patient management unfortunately ehr data do not always directly and reliably map to phenotype or medical concept that clinical researcher need or use existing phenotyping approach typically require labor intensive supervision from medical expert we propose marble a novel sparse non negative tensor factorization method to derive phenotype candidate with virtually no human supervision marble decomposes the observed tensor into two term a bias tensor and an interaction tensor the bias tensor represents the baseline characteristic common amongst the overall population and the interaction tensor defines the phenotype we demonstrate the capability of our proposed model on both simulated and patient data from a publicly available clinical database our result show that marble derived phenotype provide at least a reduction in the number of non zero element and also retains predictive power for classification purpose furthermore the resulting phenotype and baseline characteristic from real ehr data are consistent with known characteristic of the patient population thus it can potentially be used to rapidly characterize predict and manage a large number of disease thereby promising a novel data driven solution that can benefit very large segment of the population 
in this paper we propose a novel supervised learning method fast flux discriminant ffd for large scale nonlinear classification compared with other existing method ffd ha unmatched advantage a it attains the efficiency and interpretability of linear model a well a the accuracy of nonlinear model it is also sparse and naturally handle mixed data type it work by decomposing the kernel density estimation in the entire feature space into selected low dimensional subspace since there are many possible subspace we propose a submodular optimization framework for subspace selection the selected subspace prediction are then transformed to new feature on which a linear model can be learned besides since the transformed feature naturally expect non negative weight we only require smooth optimization even with the l regularization unlike other nonlinear model such a kernel method the ffd model is interpretable a it give importance weight on the original feature it training and testing are also much faster than traditional kernel model we carry out extensive empirical study on real world datasets and show that the proposed model achieves state of the art classification result with sparsity interpretability and exceptional scalability our model can be learned in minute on datasets with million of sample for which most existing nonlinear method will be prohibitively expensive in space and time 
deep learning ha catapulted to the front page of the new york time formed the core of the so called google brain and achieved impressive result in vision speech recognition and elsewhere yet researcher have offered simple conundrum that deep learning doesn t address for example consider the sentence the large ball crashed right through the table because it wa made of styrofoam what wa made of styrofoam the large ball or the table the answer is obviously the table but if we change the word styrofoam to steel the answer is clearly the large ball to automatically answer this type of question our computer require an extensive body of knowledge we believe that text mining can provide the requisite body of knowledge my talk will describe work at the new allen institute for ai towards building the next generation of text mining system 
food safety is an important health issue in singapore a the number of food poisoning case have increased significantly over the past few decade the national environment agency of singapore nea is the primary government agency responsible for monitoring and mitigating the food safety risk in an effort to pro actively monitor emerging food safety issue and to stay abreast with development related to food safety in the world nea track the world wide web a a source of news feed to identify food safety related article however such information gathering is a difficult and time consuming process due to information overload in this paper we present foodsis a system for end to end web information gathering for food safety foodsis improves efficiency of such focused information gathering process with the use of machine learning technique to identify and rank relevant content we discus the challenge in building such a system and describe how thoughtful system design and recent advance in machine learning provide a framework that synthesizes interactive learning with classification to provide a system that is used in daily operation we conduct experiment and demonstrate that our classification approach result in improving the efficiency by average compared to a conventional approach and the ranking approach lead to average improvement in elevating the rank of relevant article 
user on an online social network site generate a large number of heterogeneous activity ranging from connecting with other user to sharing content to updating their profile the set of activity within a user s network neighborhood form a stream of update for the user s consumption in this paper we report our experience with the problem of ranking activity in the linkedin homepage feed in particular we provide a taxonomy of social network activity describe a system architecture with a number of key component open sourced that support fast iteration in model development demonstrate a number of key factor for effective ranking and report experimental result from extensive online bucket test 
time sync video tagging aim to automatically generate tag for each video shot it can improve the user s experience in previewing a video s timeline structure compared to traditional scheme that tag an entire video clip in this paper we propose a new application which extract time sync video tag by automatically exploiting crowdsourced comment from video website such a nico nico douga where video are commented on by online crowd user in a time sync manner the challenge of the proposed application is that user with bias interact with one another frequently and bring noise into the data while the comment are too sparse to compensate for the noise previous technique are unable to handle this task well a they consider video semantics independently which may overfit the sparse comment in each shot and thus fail to provide accurate modeling to resolve these issue we propose a novel temporal and personalized topic model that jointly considers temporal dependency between video semantics user interaction in commenting and user preference a prior knowledge our proposed model share knowledge across video shot via user to enrich the short comment and peel off user interaction and user bias to solve the noisy comment problem log likelihood analysis and user study on large datasets show that the proposed model outperforms several state of the art baseline in video tagging quality case study also demonstrate our model s capability of extracting tag from the crowdsourced short and noisy comment 
this paper outline the approach developed together with the radio network strategy design department of a large european telecom operator in order to forecast the air interface load in their g network which is used for planning network upgrade and budgeting purpose it is based on large scale intelligent data analysis and modeling at the level of thousand of individual radio cell resulting in model per day it ha been embedded into a scenario simulation framework that is used by end user not experienced in data mining for studying and simulating the behavior of this complex networked system a an example of a systematic approach to the deployment step in the kdd process this system is already in use for two year in the country where it wa developed and it is a part of a standard business process in the last six month this national operator became a competence center for predictive modeling for micro simulation of g air interface load for four other operator of the same parent company 
visualization of high dimensional data such a text document is widely applicable the traditional mean is to find an appropriate embedding of the high dimensional representation in a low dimensional visualizable space a topic modeling is a useful form of dimensionality reduction that preserve the semantics in document recent approach aim for a visualization that is consistent with both the original word space a well a the semantic topic space in this paper we address the semantic visualization problem given a corpus of document the objective is to simultaneously learn the topic distribution a well a the visualization coordinate of document we propose to develop a semantic visualization model that approximates l normalized data directly the key is to associate each document with three representation a coordinate in the visualization space a multinomial distribution in the topic space and a directional vector in a high dimensional unit hypersphere in the word space we join these representation in a unified generative model and describe it parameter estimation through variational inference comprehensive experiment on real life text datasets show that the proposed method outperforms the existing baseline on objective evaluation metric for visualization quality and topic interpretability 
this paper address geospatial interpolation for meteorological measurement in which we estimate the value of climatic metric at unsampled site with existing observation providing climatological and meteorological condition covering a large region is potentially useful in many application such a smart grid however existing research work on interpolation either cause a large number of complex calculation or are lack of high accuracy we propose a bayesian compressed sensing based non parametric statistical model to efficiently perform the spatial interpolation task student t prior are employed to model the sparsity of unknown signal coefficient and the approximated variational inference avi method is provided for effective and fast learning the presented model ha been deployed at ibm targeting for aiding the intelligent management of smart grid the evaluation on two real world datasets demonstrate that our algorithm achieves state of the art performance in both effectiveness and efficiency 
over of column in hundred of million of web table contain numeric quantity table are a richer source of structured knowledge than free text we harness web table to answer query whose target is a quantity with natural variation such a net worth of zuckerburg battery life of ipad half life of plutonium and calorie in pizza our goal is to respond to such query with a ranked list of quantity distribution suitably represented apart from the challenge of informal schema and noisy extraction which have been known since table were used for non quantity information extraction we face additional problem of noisy number format a well a unit specification that are often contextual and ambiguous early hardening of extraction decision at a table level lead to poor accuracy instead we use a probabilistic context free grammar pcfg based unit extractor on the table and retain several top scoring extraction of quantity and numeral then we inject these into a new collective inference framework that make global decision about the relevance of candidate table snippet the interpretation of the query s target quantity type the value distribution to be ranked and presented and the degree of consensus that can be built to support the proposed quantity distribution experiment with over million web table and diverse query show robust large benefit from our quantity catalog unit extractor and collective inference 
advanced manufacturing such a aerospace semi conductor and flat display device often involves complex production process and generates large volume of production data in general the production data come from product with different level of quality assembly line with complex flow and equipment and processing craft with massive controlling parameter the scale and complexity of data is beyond the analytic power of traditional it infrastructure to achieve better manufacturing performance it is imperative to explore the underlying dependency of the production data and exploit analytic insight to improve the production process however few research and industrial effort have been reported on providing manufacturer with integrated data analytical solution to reveal potential and optimize the production process from data driven perspective in this paper we design implement and deploy an integrated solution named pdp miner which is a data analytics platform customized for process optimization in plasma display panel pdp manufacturing the system utilizes the latest advance in data mining technology and big data infrastructure to create a complete analytical solution besides our proposed system is capable of supporting automatically configuring and scheduling analysis task and balancing heterogeneous computing resource the system and the analytic strategy can be applied to other advanced manufacturing field to enable complex data analysis task since pdp miner ha been deployed a the data analysis platform of changhong coc by taking the advantage of our system the overall pdp yield rate ha increased from to the monthly production is boosted by panel which brings more than million rmb of revenue improvement per year 
how can one summarize a massive data set on the fly i e without even having seen it in it entirety in this paper we address the problem of extracting representative element from a large stream of data i e we would like to select a subset of say k data point from the stream that are most representative according to some objective function many natural notion of representativeness satisfy submodularity an intuitive notion of diminishing return thus such problem can be reduced to maximizing a submodular set function subject to a cardinality constraint classical approach to submodular maximization require full access to the data set we develop the first efficient streaming algorithm with constant factor approximation guarantee to the optimum solution requiring only a single pas through the data and memory independent of data size in our experiment we extensively evaluate the effectiveness of our approach on several application including training large scale kernel method and exemplar based clustering on million of data point we observe that our streaming method while achieving practically the same utility value run about time faster than previous work 
traditional data mining technique are designed to model a single type of heterogeneity such a multi task learning for modeling task heterogeneity multi view learning for modeling view heterogeneity etc recently a variety of real application emerged which exhibit dual heterogeneity namely both task heterogeneity and view heterogeneity example include insider threat detection across multiple organization web image classification in different domain etc existing method for addressing such problem typically assume that multiple task are equally related and multiple view are equally consistent which limit their application in complex setting with varying task relatedness and view consistency in this paper we advance state of the art technique by adaptively modeling task relatedness and view consistency via a nonparametric bayes model we model task relatedness using normal penalty with sparse covariance and view consistency using matrix dirichlet process based on this model we propose the noble algorithm using an efficient gibbs sampler experimental result on multiple real data set demonstrate the effectiveness of the proposed algorithm 
in this paper we study a variant of the social network maximum influence problem and it application to intelligently approaching individual gang member with incentive to leave a gang the goal is to identify individual who when influenced to leave gang will propagate this action we study this emerging application by exploring specific facet of the problem that must be addressed when modeling this particular situation we formulate a new influence maximization variant the social incentive influence sii problem and study it both formally and in the context of the law enforcement domain using new technique from unconstrained submodular maximization we develop an approximation algorithm for sii and present a suite of experimental result including test on real world police data from chicago 
location based data is increasingly prevalent with the rapid increase and adoption of mobile device in this paper we address the problem of learning spatial density model focusing specifically on individual level data modeling and predicting a spatial distribution for an individual is a challenging problem given both a the typical sparsity of data at the individual level and b the heterogeneity of spatial mobility pattern across individual we investigate the application of kernel density estimation kde to this problem using a mixture model approach that can interpolate between an individual s data and broader pattern in the population a a whole the mixture kde approach is evaluated on two large geolocation check in data set from twitter and gowalla with comparison to non kde baseline using both log likelihood and detection of simulated identity theft a evaluation metric our experimental result indicate that the mixture kde method provides a useful and accurate methodology for capturing and predicting individual level spatial pattern in the presence of noisy and sparse data 
in netflix announced a m prize competition to advance recommendation algorithm the recommendation problem wa simplified a the accuracy in predicting a user rating measured by the root mean squared error while that formulation helped get the attention of the research community in the area it may have put an excessive focus on what is simply one of possible approach to recommendation in this tutorial we will describe different component of modern recommender system such a personalized ranking similarity explanation context awareness or search a recommendation in the first part we will use the netflix use case a a driving example of a prototypical industrial scale recommender system we will also review the usage of modern algorithmic approach that include algorithm such a factorization machine restricted boltzmann machine simrank deep neural network or listwise learning to rank in the second part we will focus on the area of context aware recommendation where the two dimensional user item recommender problem is turned into an n dimensional space 
multi label classification of heterogeneous information network ha received renewed attention in social network analysis in this paper we present an activity edge centric multi label classification framework for analyzing heterogeneous information network with three unique feature first we model a heterogeneous information network in term of a collaboration graph and multiple associated activity graph we introduce a novel concept of vertex edge homophily in term of both vertex label and edge label and transform a general collaboration graph into an activity based collaboration multigraph by augmenting it edge with class label from each activity graph through activity based edge classification second we utilize the label vicinity to capture the pairwise vertex closeness based on the labeling on the activity based collaboration multigraph we incorporate both the structure affinity and the label vicinity into a unified classifier to speed up the classification convergence third we design an iterative learning algorithm aeclass to dynamically refine the classification result by continuously adjusting the weight on different activity based edge classification scheme from multiple activity graph while constantly learning the contribution of the structure affinity and the label vicinity in the unified classifier extensive evaluation on real datasets demonstrates that aeclass outperforms existing representative method in term of both effectiveness and efficiency 
deep learning well demonstrates it potential in learning latent feature representation recent year have witnessed an increasing enthusiasm for regularizing deep neural network by incorporating various side information such a user provided label or pairwise constraint however the effectiveness and parameter sensitivity of such algorithm have been major obstacle for putting them into practice the major contribution of our work is the exposition of a novel supervised deep learning algorithm which distinguishes from two unique trait first it regularizes the network construction by utilizing similarity or dissimilarity constraint between data pair rather than sample specific annotation such kind of side information is more flexible and greatly mitigates the workload of annotator secondly unlike prior work our proposed algorithm decouples the supervision information and intrinsic data structure we design two heterogeneous network each of which encodes either supervision or unsupervised data structure respectively specifically we term the supervision oriented network a auxiliary network since it is principally used for facilitating the parameter learning of the other one and will be removed when handling out of sample data the two network are complementary to each other and bridged by enforcing the correlation of their parameter we name the proposed algorithm supervision guided autoencoder sugar comparing prior work on unsupervised deep network and supervised learning sugar better balance numerical tractability and the flexible utilization of supervision information the classification performance on mnist digit and eight benchmark datasets demonstrates that sugar can effectively improve the performance by using the auxiliary network on both shallow and deep architecture particularly when multiple sugar are stacked the performance is significantly boosted on the selected benchmark ours achieve up to relative accuracy improvement compared to the state of the art model 
we study the problem of active learning for multilabel classification we focus on the real world scenario where the average number of positive relevant label per data point is small leading to positive label sparsity carrying out mutual information based near optimal active learning in this setting is a challenging task since the computational complexity involved is exponential in the total number of label we propose a novel inference algorithm for the sparse bayesian multilabel model of the benefit of this alternate inference scheme is that it enables a natural approximation of the mutual information objective we prove that the approximation lead to an identical solution to the exact optimization problem but at a fraction of the optimization cost this allows u to carry out efficient non myopic and near optimal active learning for sparse multilabel classification extensive experiment reveal the effectiveness of the method 
in this paper we study bid optimisation for real time bidding rtb based display advertising rtb allows advertiser to bid on a display ad impression in real time when it is being generated it go beyond contextual advertising by motivating the bidding focused on user data and it is different from the sponsored search auction where the bid price is associated with keywords for the demand side a fundamental technical challenge is to automate the bidding process based on the budget the campaign objective and various information gathered in runtime and in history in this paper the programmatic bidding is cast a a functional optimisation problem under certain dependency assumption we derive simple bidding function that can be calculated in real time our finding show that the optimal bid ha a non linear relationship with the impression level evaluation such a the click through rate and the conversion rate which are estimated in real time from the impression level feature this is different from previous work that is mainly focused on a linear bidding function our mathematical derivation suggests that optimal bidding strategy should try to bid more impression rather than focus on a small set of high valued impression because according to the current rtb market data compared to the higher evaluated impression the lower evaluated one are more cost effective and the chance of winning them are relatively higher aside from the theoretical insight offline experiment on a real dataset and online experiment on a production rtb system verify the effectiveness of our proposed optimal bidding strategy and the functional optimisation framework 
ambiguous query which are typical on search engine and recommendation system often return a large number of result from multiple interpretation given that many user often perform their search on limited size screen e g mobile phone an important problem is which result to display first recent work ha suggested displaying a set of result top k based on their relevance score with respect to the query and their diversity with respect to each other however previous work balance relevance and diversity mostly by a predefined fixed way in this paper we show that for different search task there is a different ideal balance of relevance and diversity we propose a principled method for adaptive diversification of query result that minimizes the user effort to find the desired result by dynamically balancing the relevance and diversity at each query step e g when refining the query or viewing the next page of result we introduce a navigation cost model a a mean to estimate the effort required to navigate the query result and show that the problem of estimating the ideal amount of diversification at each step is np hard we propose an efficient approximate algorithm to select a near optimal subset of the query result that minimizes the expected user effort finally we demonstrate the efficacy and efficiency of our solution in minimizing user effort compared to state of the art ranking method by mean of an extensive experimental evaluation and a comprehensive user study on amazon mechanical turk 
when data driven improvement involve personally identifiable data or even data that can be used to infer sensitive information about individual we face the dilemma that we potentially risk compromising privacy a we see increased emphasis on using data mining to effect improvement in a range of socially beneficial activity from improving matching of talented student to opportunity for higher education or improving allocation of fund across competing school program or reducing hospitalization time following surgery the dilemma can often be especially acute the data involved often is personally identifiable or revealing and sensitive and many of the institution that must be involved in gathering and maintaining custody of the data are not equipped to adequately secure the data raising the risk of privacy breach how should we approach this trade off can we ass the risk can we control or mitigate them can we develop guideline for when the risk is or is not worthwhile and for how best to handle data in different common scenario chair raghu ramakrishnan and geoffrey i webb bring this panel of leading data miner and privacy expert together to address these critical issue 
histogram construction is a fundamental problem in data management and a good histogram support numerous mining operation recent work ha extended histogram to probabilistic data however constructing histogram for probabilistic data can be extremely expensive and existing study suffer from limited scalability this work design novel approximation method to construct scalable histogram on probabilistic data we show that our method provide constant approximation compared to the optimal histogram produced by the state of the art in the worst case we also extend our method to parallel and distributed setting so that they can run gracefully in a cluster of commodity machine we introduced novel synopsis to reduce communication cost when running our method in such setting extensive experiment on large real data set have demonstrated the superb scalability and efficiency achieved by our method when compared to the state of the art method they also achieved excellent approximation quality in practice 
given a simple noun such a em apple and a question such a is it edible what process take place in the human brain more specifically given the stimulus what are the interaction between group of neuron also known a functional connectivity and how can we automatically infer those interaction given measurement of the brain activity furthermore how doe this connectivity differ across different human subject in this work we present a simple novel good enough brain model or gebm in short and a novel algorithm sparse sysid which are able to effectively model the dynamic of the neuron interaction and infer the functional connectivity moreover gebm is able to simulate basic psychological phenomenon such a habituation and priming whose definition we provide in the main text we evaluate gebm by using both synthetic and real brain data using the real data gebm produce brain activity pattern that are strikingly similar to the real one and the inferred functional connectivity is able to provide neuroscientific insight towards a better understanding of the way that neuron interact with each other a well a detect regularity and outlier in multi subject brain activity measurement 
given a directed graph of million of node how can we automatically spot anomalous suspicious node judging only from their connectivity pattern suspicious graph pattern show up in many application from twitter user who buy fake follower manipulating the social network to botnet member performing distributed denial of service attack disturbing the network traffic graph we propose a fast and effective method catchsync which exploit two of the tell tale sign left in graph by fraudsters a synchronized behavior suspicious node have extremely similar behavior pattern because they are often required to perform some task together such a follow the same user and b rare behavior their connectivity pattern are very different from the majority we introduce novel measure to quantify both concept synchronicity and normality and we propose a parameter free algorithm that work on the resulting synchronicity normality plot thanks to careful design catchsync ha the following desirable property a it is scalable to large datasets being linear on the graph size b it is parameter free and c it is side information oblivious it can operate using only the topology without needing labeled data nor timing information etc while still capable of using side information if available we applied catchsync on two large real datasets billion edge twitter social graph and billion edge tencent weibo social graph and several synthetic one catchsync consistently outperforms existing competitor both in detection accuracy by on twitter and on tencent weibo a well a in speed 
in classification if a small number of instance is added or removed incremental and decremental technique can be applied to quickly update the model however the design of incremental and decremental algorithm involves many consideration in this paper we focus on linear classifier including logistic regression and linear svm because of their simplicity over kernel or other method by applying a warm start strategy we investigate issue such a using primal or dual formulation choosing optimization method and creating practical implementation through theoretical analysis and practical experiment we conclude that a warm start setting on a high order optimization method for primal formulation is more suitable than others for incremental and decremental learning of linear classification 
graph clustering and graph outlier detection have been studied extensively on plain graph with various application recently algorithm have been extended to graph with attribute a often observed in the real world however all of these technique fail to incorporate the user preference into graph mining and thus lack the ability to steer algorithm to more interesting part of the attributed graph in this work we overcome this limitation and introduce a novel user oriented approach for mining attributed graph the key aspect of our approach is to infer user preference by the so called focus attribute through a set of user provided exemplar node in this new problem setting cluster and outlier are then simultaneously mined according to this user preference specifically our focusco algorithm identifies the focus extract focused cluster and detects outlier moreover focusco scale well with graph size since we perform a local clustering of interest to the user rather than global partitioning of the entire graph we show the effectiveness and scalability of our method on synthetic and real world graph a compared to both existing graph clustering and outlier detection approach 
on street parking just a any publicly owned utility is used inefficiently if access is free or priced very far from market rate this paper introduces a novel demand management solution using data from dedicated occupancy sensor an iteration scheme update parking rate to better match demand the new rate encourage parker to avoid peak hour and peak location and reduce congestion and underuse the solution is deliberately simple so that it is easy to understand easily seen to be fair and lead to parking policy that are easy to remember and act upon we study the convergence property of the iteration scheme and prove that it converges to a reasonable distribution for a very large class of model the algorithm is in use to change parking rate in over space in downtown los angeles since june a part of the la express park project initial result are encouraging with a reduction of congestion and underuse while in more location rate were decreased than increased 
online platform such a meetup and plancast have recently become popular for planning gathering and event organization however there is a surprising lack of study on how to effectively and efficiently organize social event for a large group of people through such platform in this paper we study the key computational problem involved in organization of social event to our best knowledge for the first time we propose the social event organization seo problem a one of assigning a set of event for a group of user to attend where the user are socially connected with each other and have innate level of interest in those event a a first step toward social event organization we introduce a formal definition of a restricted version of the problem and show that it is np hard and is hard to approximate we propose efficient heuristic algorithm that improve upon simple greedy algorithm by incorporating the notion of phantom event and by using look ahead estimation using synthetic datasets and three real datasets including those from the platform meetup and plancast we experimentally demonstrate that our greedy heuristic are scalable and furthermore outperform the baseline algorithm significantly in term of achieving superior social welfare 
advance in real time location system rtls solution have enabled u to collect massive amount of fine grained semantically rich location trace which provide unparalleled opportunity for understanding human activity and discovering useful knowledge this in turn delivers intelligence for real time decision making in various field such a workflow management indeed it is a new paradigm for workflow modeling by the knowledge discovery in location trace to that end in this paper we provide a focused study of workflow modeling by the integrated analysis of indoor location trace in the hospital environment in comparison with conventional workflow modeling based on passive workflow log one salient feature of our approach is that it can proactively unravel the workflow pattern hidden in the location trace by automatically constructing the workflow state and estimating parameter describing the transition pattern of moving object specifically to determine a meaningful granularity for the model the workflow state are first constructed a region associated with specific healthcare activity then we transform the original indoor location trace to the sequence of workflow state and model the workflow transition pattern by finite state machine furthermore we leverage the correlation in the location trace between related type of medical device to reinforce the modeling performance and enable more application the result show that the proposed framework can not only model the workflow pattern effectively but also have managerial application in workflow monitoring auditing and inspection of workflow compliance which are critical in the healthcare industry 
one response to the proliferation of large datasets ha been to develop ingenious way to throw resource at the problem using massive fault tolerant storage architecture parallel and graphical computation model such a mapreduce pregel and giraph however not all environment can support this scale of resource and not all query need an exact response this motivates the use of sampling to generate summary datasets that support rapid query and prolong the useful life of the data in storage to be effective sampling must mediate the tension between resource constraint data characteristic and the required query accuracy the state of the art in sampling go far beyond simple uniform selection of element to maximize the usefulness of the resulting sample this tutorial review progress in sample design for large datasets including streaming and graph structured data application are discussed to sampling network traffic and social network 
this paper proposes multi task copula mtc that can handle a much wider class of task than mean regression with gaussian noise in most former multi task learning mtl while former mtl emphasizes shared structure among model mtc aim at joint prediction to exploit inter output correlation given input the output of mtc are allowed to follow arbitrary joint continuous distribution mtc capture the joint likelihood of multi output by learning the marginal of each output firstly and then a sparse and smooth output dependency graph function while the former can be achieved by classical mtl learning graph dynamically varying with input is quite a challenge we address this issue by developing sparse graph regression spagraphr a non parametric estimator incorporating kernel smoothing maximum likelihood and sparse graph structure to gain fast learning algorithm it start from a few seed graph on a few input point and then update the graph on other input point by a fast operator via coarse to fine propagation due to the power of copula in modeling semi parametric distribution spagraphr can model a rich class of dynamic non gaussian correlation we show that mtc can address more flexible and difficult task that do not fit the assumption of former mtl nicely and can fully exploit their relatedness experiment on robotic control and stock price prediction justify it appealing performance in challenging mtl problem 
frequent itemset mining is a core data mining task and ha been studied extensively although by their nature frequent itemsets are aggregate over many individual and would not seem to pose a privacy threat an attacker with strong background information can learn private individual information from frequent itemsets this ha lead to differentially private frequent itemset mining which protects privacy by giving inexact answer we give an approach that first identifies top k frequent itemsets then us them to construct a compact differentially private fp tree once the noisy fp tree is built the privatized support of all frequent itemsets can be derived from it without access to the original data experimental result show that the proposed algorithm give substantially better result than prior approach especially for high level of privacy 
data glitch are unusual observation that do not conform to data quality expectation be they logical semantic or statistical by applying data integrity constraint potentially large section of data could be flagged a being noncompliant ignoring or repairing significant section of the data could fundamentally bias the result and conclusion drawn from analysis in the context of big data where large number and volume of feed from disparate source are integrated it is likely that significant portion of seemingly noncompliant data are actually legitimate usable data in this paper we introduce the notion of empirical glitch explanation concise multi dimensional description of subset of potentially dirty data and propose a scalable method for empirically generating such explanatory characterization the explanation could serve two valuable function provide a way of identifying legitimate data and releasing it back into the pool of clean data in doing so we reduce cleaning related statistical distortion of the data used to refine existing data quality constraint and generate and formalize domain knowledge we conduct experiment using real and simulated data to demonstrate the scalability of our method and the robustness of explanation in addition we use two real world example to demonstrate the utility of the explanation where we reclaim over of the suspicious data keeping data repair related statistical distortion close to 
we show how to programmatically model process that human use when extracting answer to query e g who invented typewriter list of washington national park from semi structured web page returned by a search engine this modeling enables various application including automating repetitive search task and helping search engine developer design micro segment of factoid question we describe the design and implementation of a domain specific language that enables extracting data from a webpage based on it structure visual layout and linguistic pattern we also describe an algorithm to rank multiple answer extracted from multiple webpage on query across micro segment obtained from bing log our system laseweb answered query with an average recall of also the desired answer s were present in top suggestion for case 
demographic are widely used in marketing to characterize different type of customer however in practice demographic information such a age gender and location is usually unavailable due to privacy and other reason in this paper we aim to harness the power of big data to automatically infer user demographic based on their daily mobile communication pattern our study is based on a real world large mobile network of more than user and over communication record call and sm we discover several interesting social strategy that mobile user frequently use to maintain their social connection first young people are very active in broadening their social circle while senior tend to keep close but more stable connection second female user put more attention on cross generation interaction than male user though interaction between male and female user are frequent third a persistent same gender triadic pattern over one s lifetime is discovered for the first time while more complex opposite gender triadic pattern are only exhibited among young people we further study to what extent user demographic can be inferred from their mobile communication a a special case we formalize a problem of double dependent variable prediction inferring user gender and age simultaneously we propose the whoami method a double dependent variable factor graph model to address this problem by considering not only the effect of feature on gender age but also the interrelation between gender and age our experiment show that the proposed whoami method significantly improves the prediction accuracy by up to compared with several alternative method 
speaker of more than language have access to internet and communication technology the majority of phone tablet and computer now ship with language enabled capability like speech recognition and intelligent auto correction and people increasingly interact with data intensive cloud based language technology like search engine and spam filter for both personal and large scale technology the service quality drop or disappears entirely outside of a handful of language speaker of low resource language correlate with lower access to healthcare education and higher vulnerability to disaster serving the broadest possible range of language is crucial to ensuring equitable participation in the global information economy i will present example of how natural language processing and distributed human computing are improving the life of speaker of all the world s language in area including education disaster response health and access to employment when applying natural language processing to the full diversity of the world s communication we need to go beyond simple keyword analysis and implement complex technology that require human in the loop processing to ensure usable accuracy in recent work where more than a million human judgment were collected on unstructured text and imagery data around natural disaster i will present observation that debunk recent over optimistic claim about the utility of social medium following disaster on the positive side i will share result that show how for profit technology are improving people s life by providing sustainable economic growth opportunity when they support more language aligning business objective with global diversity 
with the fast growth of smart device and social network a lot of computing system collect data that record different type of activity an important computational challenge is to analyze these data extract pattern and understand activity trend we consider the problem of mining activity network to identify interesting event such a a big concert or a demonstration in a city or a trending keyword in a user community in a social network we define an event to be a subset of node in the network that are close to each other and have high activity level we formalize the problem of event detection using two graph theoretic formulation the first one capture the compactness of an event using the sum of distance among all pair of the event node we show that this formulation can be mapped to the maxcut problem and thus it can be solved by applying standard semidefinite programming technique the second formulation capture compactness using a minimum distance tree this formulation lead to the prize collecting steiner tree problem which we solve by adapting existing approximation algorithm for the two problem we introduce we also propose efficient and effective greedy approach and we prove performance guarantee for one of them we experiment with the proposed algorithm on real datasets from a public bicycling system and a geolocation enabled social network dataset collected from twitter the result show that our method are able to detect meaningful event 
we describe an automated system for the large scale monitoring of web site that serve a online storefront for spam advertised good our system is developed from an extensive crawl of black market web site that deal in illegal pharmaceutical replica luxury good and counterfeit software the operational goal of the system is to identify the affiliate program of online merchant behind these web site the system itself is part of a larger effort to improve the tracking and targeting of these affiliate program there are two main challenge in this domain the first is that appearance can be deceiving web page that render very differently are often linked to the same affiliate program of merchant the second is the difficulty of acquiring training data the manual labeling of web page though necessary to some degree is a laborious and time consuming process our approach in this paper is to extract feature that reveal when web page linked to the same affiliate program share a similar underlying structure using these feature which are mined from a small initial seed of labeled data we are able to profile the web site of forty four distinct affiliate program that account collectively for hundred of million of dollar in illicit e commerce our work also highlight several broad challenge that arise in the large scale empirical study of malicious activity on the web 
since data is often multi faceted in it very nature it might not adequately be summarized by just a single clustering to better capture the data s complexity method aiming at the detection of multiple alternative clustering have been proposed independent of this research area semi supervised clustering technique have shown to substantially improve clustering result for single view clustering by integrating prior knowledge in this paper we join both research area and present a solution for integrating prior knowledge in the process of detecting multiple clustering we propose a bayesian framework modeling multiple clustering of the data by multiple mixture distribution each responsible for an individual set of relevant dimension in addition our model is able to handle prior knowledge in the form of instance level constraint indicating which object should or should not be grouped together since a priori the assignment of constraint to specific view is not necessarily known our technique automatically determines their membership for efficient learning we propose the algorithm smvc using variational bayesian method with experiment on various real world data we demonstrate smvc s potential to detect multiple clustering view and it capability to improve the result by exploiting prior knowledge 
the unintentional transport of invasive specie i e non native and harmful specie that adversely affect habitat and native specie through the global shipping network gsn cause substantial loss to social and economic welfare e g annual loss due to ship borne invasion in the laurentian great lake is estimated to be a high a usd million despite the huge negative impact management of such invasion remains challenging because of the complex process that lead to specie transport and establishment numerous difficulty associated with quantitative risk assessment e g inadequate characterization of invasion process lack of crucial data large uncertainty associated with available data etc have hampered the usefulness of such estimate in the task of supporting the authority who are battling to manage invasion with limited resource we present here an approach for addressing the problem at hand via creative use of computational technique and multiple data source thus illustrating how data mining can be used for solving crucial yet very complex problem towards social good by modeling implicit specie exchange a a network that we refer to a the specie flow network sfn large scale specie flow dynamic are studied via a graph clustering approach that decomposes the sfn into cluster of port and inter cluster connection we then exploit this decomposition to discover crucial knowledge on how pattern in gsn affect aquatic invasion and then illustrate how such knowledge can be used to devise effective and economical invasive specie management strategy by experimenting on actual gsn traffic data for year we have discovered crucial knowledge that can significantly aid the management authority 
in the past few year there ha been an explosion of social network in the online world user flock these network creating profile and linking themselves to other individual connecting online ha a small cost compared to the physical world leading to a proliferation of connection many of which carry little value or importance understanding the strength and nature of these relationship is paramount to anyone interesting in making use of the online social network data in this paper we use the principle of strong triadic closure to characterize the strength of relationship in social network the strong triadic closure principle stipulates that it is not possible for two individual to have a strong relationship with a common friend and not know each other we consider the problem of labeling the tie of a social network a strong or weak so a to enforce the strong triadic closure property we formulate the problem a a novel combinatorial optimization problem and we study it theoretically although the problem is np hard we are able to identify case where there exist efficient algorithm with provable approximation guarantee we perform experiment on real data and we show that there is a correlation between the labeling we obtain and empirical metric of tie strength and that weak edge act a bridge between different community in the network finally we study extension and variation of our problem both theoretically and experimentally 
for document scoring although learning to rank and domain adaptation are treated a two different problem in previous work we discover that they actually share the same challenge of adapting keyword contribution across different query or domain in this paper we propose to study the cross task document scoring problem where a task refers to a query to rank or a domain to adapt to a the first attempt to unify these two problem existing solution for learning to rank and domain adaptation either leave the heavy burden of adapting keyword contribution to feature designer or are difficult to be generalized to resolve such limitation we abstract the keyword scoring principle pointing out that the contribution of a keyword essentially depends on first it importance to a task and second it importance to the document for determining these two aspect of keyword importance we further propose the concept of feature decoupling suggesting using two type of easy to design feature meta feature and intra feature towards learning a scorer based on the decoupled feature we require that our framework fulfill inferred sparsity to eliminate the interference of noisy keywords and employ distant supervision to tackle the lack of keyword label we propose the tree structured boltzmann machine t rbm a novel two stage markov network a our solution experiment on three different application confirm the effectiveness of t rbm which achieves significant improvement compared with four state of the art baseline method 
mining phrase entity concept topic and hierarchy from massive text corpus is an essential problem in the age of big data text data in electronic form are ubiquitous ranging from scientific article to social network enterprise log news article social medium and general web page it is highly desirable but challenging to bring structure to unstructured text data uncover underlying hierarchy relationship pattern and trend and gain knowledge from such data in this tutorial we provide a comprehensive survey on the state of the art of data driven method that automatically mine phrase extract and infer latent structure from text corpus and construct multi granularity topical grouping and hierarchy of the underlying theme we study their principle methodology algorithm and application using several real datasets including research paper and news article and demonstrate how these method work and how the uncovered latent entity structure may help text understanding knowledge discovery and management 
network are prevalent and have posed many fascinating research question how can we spot similar user e g virtual identical twin in cleveland for a new yorker given a query disease how can we prioritize it candidate gene by incorporating the tissue specific protein interaction network of those similar disease in most if not all of the existing network ranking method the node are the ranking object with the finest granularity in this paper we propose a new network data model a network of network non where each node of the main network itself can be further represented a another domain specific network this new data model enables to compare the node in a broader context and rank them at a finer granularity moreover such an non model enables much more efficient search when the ranking target reside in a certain domain specific network we formulate ranking on non a a regularized optimization problem propose efficient algorithm and provide theoretical analysis such a optimality convergence complexity and equivalence extensive experimental evaluation demonstrate the effectiveness and the efficiency of our method 
it is traditionally a challenge for home buyer to understand compare and contrast the investment value of real estate while a number of estate appraisal method have been developed to value real property the performance of these method have been limited by the traditional data source for estate appraisal however with the development of new way of collecting estate related mobile data there is a potential to leverage geographic dependency of estate for enhancing estate appraisal indeed the geographic dependency of the value of an estate can be from the characteristic of it own neighborhood individual the value of it nearby estate peer and the prosperity of the affiliated latent business area zone to this end in this paper we propose a geographic method named clusranking for estate appraisal by leveraging the mutual enforcement of ranking and clustering power clusranking is able to exploit geographic individual peer and zone dependency in a probabilistic ranking model specifically we first extract the geographic utility of estate from geography data estimate the neighborhood popularity of estate by mining taxicab trajectory data and model the influence of latent business area via clusranking also we use a linear model to fuse these three influential factor and predict estate investment value moreover we simultaneously consider individual peer and zone dependency and derive an estate specific ranking likelihood a the objective function finally we conduct a comprehensive evaluation with real world estate related data and the experimental result demonstrate the effectiveness of our method 
the increasing sophistication of malicious software call for new defensive technique that are harder to evade and are capable of protecting user against novel threat we present aesop a scalable algorithm that identifies malicious executable file by applying aesop s moral that a man is known by the company he keep we use a large dataset voluntarily contributed by the member of norton community watch consisting of partial list of the file that exist on their machine to identify close relationship between file that often appear together on machine aesop leverage locality sensitive hashing to measure the strength of these inter file relationship to construct a graph on which it performs large scale inference by propagating information from the labeled file a benign or malicious to the preponderance of unlabeled file aesop attained early labeling of of benign file and of malicious file over a week before they are labeled by the state of the art technique with a true positive rate at flagging malware at false positive rate 
in the past few year the government and other agency have publicly released a prodigious amount of data that can be potentially mined to benefit the society at large however data such a health record are typically only provided at aggregated level e g per state per hospital referral region etc to protect privacy unfortunately aggregation can severely diminish the utility of such data when modeling or analysis is desired at a per individual basis so not surprisingly despite the increasing abundance of aggregate data there have been very few successful attempt in exploiting them for individual level analysis this paper introduces ludia a novel low rank approximation algorithm that utilizes aggregation constraint in addition to auxiliary information in order to estimate or reconstruct the original individual level value from aggregate data if the reconstructed data are statistically similar to the original individual level data off the shelf individual level model can be readily and reliably applied for subsequent predictive or descriptive analytics ludia is more robust to nonlinear estimate and random effect than other reconstruction algorithm it solves a sylvester equation and leverage multi level also known a hierarchical or mixed effect modeling approach efficiently a novel graphical model is also introduced to provide a probabilistic viewpoint of ludia experimental result using a texas inpatient dataset show that individual level data can be reasonably reconstructed from county hospital and zip code level aggregate data several factor affecting the reconstruction quality are discussed along with the implication of this work for current aggregation guideline 
tourism industry ha become a key economic driver for singapore understanding the behavior of tourist is very important for the government and private sector e g restaurant hotel and advertising company to improve their existing service or create new business opportunity in this joint work with singapore s land transport authority lta we innovatively apply machine learning technique to identity the tourist among public commuter using the public transportation data provided by lta on successful identification the travelling pattern of tourist are then revealed and thus allow further analysis to be carried out such a on their favorite destination region of stay etc technically we model the tourist identification a a classification problem and design an iterative learning algorithm to perform inference with limited prior knowledge and labeled data we show the superiority of our algorithm with performance evaluation and comparison with other state of the art learning algorithm further we build an interactive web based system for answering query regarding the moving pattern of the tourist which can be used by stakeholder to gain insight into tourist travelling behavior in singapore 
information network such a social medium and email network often contain sensitive information releasing such network data could seriously jeopardize individual privacy therefore we need to sanitize network data before the release in this paper we present a novel data sanitization solution that infers a network s structure in a differentially private manner we observe that by estimating the connection probability between vertex instead of considering the observed edge directly the noise scale enforced by differential privacy can be greatly reduced our proposed method infers the network structure by using a statistical hierarchical random graph hrg model the guarantee of differential privacy is achieved by sampling possible hrg structure in the model space via markov chain monte carlo mcmc we theoretically prove that the sensitivity of such inference is only o log n where n is the number of vertex in a network this bound implies le noise to be injected than those of existing work we experimentally evaluate our approach on four real life network datasets and show that our solution effectively preserve essential network structural property like degree distribution shortest path length distribution and influential node 
ideal point estimation that estimate legislator ideological position and understands their voting behavior ha attracted study from political science and computer science typically a legislator is assigned a global ideal point based on her voting or other social behavior however it is quite normal that people may have different position on different policy dimension for example some people may be more liberal on economic issue while more conservative on cultural issue in this paper we propose a novel topic factorized ideal point estimation model for a legislative voting network in a unified framework first we model the ideal point of legislator and bill for each topic instead of assigning them to a global one second the generation of topic are guided by the voting matrix in addition to the text information contained in bill a unified model that combine voting behavior modeling and topic modeling is presented and an iterative learning algorithm is proposed to learn the topic of bill a well a the topic factorized ideal point of legislator and bill by comparing with the state of the art ideal point estimation model our method ha a much better explanation power in term of held out log likelihood and other measure besides case study show that the topic factorized ideal point coincide with human intuition finally we illustrate how to use these topic factorized ideal point to predict voting result for unseen bill 
distance metric learning dml aim to learn a distance metric better than euclidean distance it ha been successfully applied to various task e g classification clustering and information retrieval many dml algorithm suffer from the over fitting problem because of a large number of parameter to be determined in dml in this paper we exploit the dropout technique which ha been successfully applied in deep learning to alleviate the over fitting problem for dml different from the previous study that only apply dropout to training data we apply dropout to both the learned metric and the training data we illustrate that application of dropout to dml is essentially equivalent to matrix norm based regularization compared with the standard regularization scheme in dml dropout is advantageous in simulating the structured regularizers which have shown consistently better performance than non structured regularizers we verify both empirically and theoretically that dropout is effective in regulating the learned metric to avoid the over fitting problem last we examine the idea of wrapping the dropout technique in the state of art dml method and observe that the dropout technique can significantly improve the performance of the original dml method 
it is often crucial for manufacturer to decide what product to produce so that they can increase their market share in an increasingly fierce market to decide which product to produce manufacturer need to analyze the consumer requirement and how consumer make their purchase decision so that the new product will be competitive in the market in this paper we first present a general distance based product adoption model to capture consumer purchase behavior using this model various distance metric can be used to describe different real life purchase behavior we then provide a learning algorithm to decide which set of distance metric one should use when we are given some historical purchase data based on the product adoption model we formalize the k most marketable product or k mmp selection problem and formally prove that the problem is np hard to tackle this problem we propose an efficient greedy based approximation algorithm with a provable solution guarantee using submodularity analysis we prove that our approximation algorithm can achieve at least of the optimal solution we apply our algorithm on both synthetic datasets and real world datasets tripadvisor com and show that our algorithm can easily achieve five or more order of speedup over the exhaustive search and achieve about of the optimal solution on average our experiment also show the significant impact of different distance metric on the result and how proper distance metric can improve the accuracy of product selection 
contextual advertising is a form of textual advertising usually displayed on third party web page one of the main problem with contextual advertising is determining how to select ad that are relevant to the page content and or the user information in order to achieve both effective advertising and a positive user experience typically the relevance of an ad to page content is indicated by a tf idf score that measure the word overlap between the page and the ad content so this problem is transformed into a similarity search in a vector space however such an approach is not useful if the vocabulary used on the page is expected to be different from that in the ad there have been study proposing the use of semantic category or hidden class to overcome this problem with these approach it is necessary to expand the ad retrieval system or build new index to handle the category or class and it is not always easy to maintain the number of category and class required for business need in this work we propose a translation method that learns the mapping of the contextual information to the textual feature of ad by using past click data the contextual information includes the user s demographic information and behavioral information a well a page content information the proposed method is able to retrieve more preferable ad while maintaining the sparsity of the inverted index and the performance of the ad retrieval system in addition it is easy to implement and there is no need to modify an existing ad retrieval system we evaluated this approach offline on a data set based on log from an ad network our method achieved better result than existing method we also applied our approach with a real ad serving system and compared the online performance using a b testing our approach achieved an improvement over the existing production system 
we present a clustering algorithm for discovering rare yet significant recurring class across a batch of sample in the presence of random effect we model each sample data by an infinite mixture of dirichlet process gaussian mixture model dpms with each dpm representing the noisy realization of it corresponding class distribution in a given sample we introduce dependency across multiple sample by placing a global dirichlet process prior over individual dpms this hierarchical prior introduces a sharing mechanism across sample and allows for identifying local realization of class across sample we use collapsed gibbs sampler for inference to recover local dpms and identify their class association we demonstrate the utility of the proposed algorithm processing a flow cytometry data set containing two extremely rare cell population and report result that significantly outperform competing technique the source code of the proposed algorithm is available on the web via the link http c iupui edu dundar aspire htm 
the pervasive use of social medium generates massive data in an unprecedented rate and the information overload problem becomes increasingly severe for social medium user recommendation ha been proven to be effective in mitigating the information overload problem demonstrated it strength in improving the quality of user experience and positively impacted the success of social medium new type of data introduced by social medium not only provide more information to advance traditional recommender system but also manifest new research possibility for recommendation in this tutorial we aim to provide a comprehensive overview of various recommendation task in social medium especially their recent advance and new frontier we introduce basic concept review state of the art algorithm and deliberate the emerging challenge and opportunity finally we summarize the tutorial with discussion on open issue and challenge about recommendation in social medium updated information about the tutorial can be found at url http www public asu edu jtang recommendation htm 
most current mutual information mi based feature selection technique are greedy in nature thus are prone to sub optimal decision potential performance improvement could be gained by systematically posing mi based feature selection a a global optimization problem a rare attempt at providing a global solution for the mi based feature selection is the recently proposed quadratic programming feature selection qpfs approach we point out that the qpfs formulation face several non trivial issue in particular how to properly treat feature self redundancy while ensuring the convexity of the objective function in this paper we take a systematic approach to the problem of global mi based feature selection we show how the resulting np hard global optimization problem could be efficiently approximately solved via spectral relaxation and semi definite programming technique we experimentally demonstrate the efficiency and effectiveness of these novel feature selection framework 
we are interested in organizing a continuous stream of sparse and noisy text known a tweet in real time into an ontology of hundred of topic with measurable and stringently high precision this inference is performed over a full scale stream of twitter data whose statistical distribution evolves rapidly over time the implementation in an industrial setting with the potential of affecting and being visible to real user made it necessary to overcome a host of practical challenge we present a spectrum of topic modeling technique that contribute to a deployed system these include non topical tweet detection automatic labeled data acquisition evaluation with human computation diagnostic and corrective learning and most importantly high precision topic inference the latter represents a novel two stage training algorithm for tweet text classification and a close loop inference mechanism for combining text with additional source of information the resulting system achieves precision at substantial overall coverage 
the gps technology and new form of urban geography have changed the paradigm for mobile service a such the abundant availability of gps trace ha enabled new way of doing taxi business indeed recent effort have been made on developing mobile recommender system for taxi driver using taxi gps trace these system can recommend a sequence of pick up point for the purpose of maximizing the probability of identifying a customer with the shortest driving distance however in the real world the income of taxi driver is strongly correlated with the effective driving hour in other word it is more critical for taxi driver to know the actual driving route to minimize the driving time before finding a customer to this end in this paper we propose to develop a cost effective recommender system for taxi driver the design goal is to maximize their profit when following the recommended route for finding passenger specifically we first design a net profit objective function for evaluating the potential profit of the driving route then we develop a graph representation of road network by mining the historical taxi gps trace and provide a brute force strategy to generate optimal driving route for recommendation however a critical challenge along this line is the high computational cost of the graph based approach therefore we develop a novel recursion strategy based on the special form of the net profit function for searching optimal candidate route efficiently particularly instead of recommending a sequence of pick up point and letting the driver decide how to get to those point our recommender system is capable of providing an entire driving route and the driver are able to find a customer for the largest potential profit by following the recommendation this make our recommender system more practical and profitable than other existing recommender system finally we carry out extensive experiment on a real world data set collected from the san francisco bay area and the experimental result clearly validate the effectiveness of the proposed recommender system 
the frequency and intensity of natural disaster ha significantly increased over the past decade and this trend is predicted to continue facing these possible and unexpected disaster accurately predicting human emergency behavior and their mobility will become the critical issue for planning effective humanitarian relief disaster management and long term societal reconstruction in this paper we build up a large human mobility database gps record of million user over one year and several different datasets to capture and analyze human emergency behavior and their mobility following the great east japan earthquake and fukushima nuclear accident based on our empirical analysis through these data we find that human behavior and their mobility following large scale disaster sometimes correlate with their mobility pattern during normal time and are also highly impacted by their social relationship intensity of disaster damage level government appointed shelter news reporting large population flow and etc on the basis of these finding we develop a model of human behavior that take into account these factor for accurately predicting human emergency behavior and their mobility following large scale disaster the experimental result and validation demonstrate the efficiency of our behavior model and suggest that human behavior and their movement during disaster may be significantly more predictable than previously thought 
correlation clustering is a basic primitive in data miner s toolkit with application ranging from entity matching to social network analysis the goal in correlation clustering is given a graph with signed edge partition the node into cluster to minimize the number of disagreement in this paper we obtain a new algorithm for correlation clustering our algorithm is easily implementable in computational model such a mapreduce and streaming and run in a small number of round in addition we show that our algorithm obtains an almost approximation to the optimal correlation clustering experiment on huge graph demonstrate the scalability of our algorithm and it applicability to data mining problem 
betweenness centrality measure the importance of a vertex by quantifying the number of time it act a a midpoint of the shortest path between other vertex this measure is widely used in network analysis in many application we wish to choose the k vertex with the maximum adaptive betweenness centrality which is the betweenness centrality without considering the shortest path that have been taken into account by already chosen vertex all previous method are designed to compute the betweenness centrality in a fixed graph thus to solve the above task we have to run these method k time in this paper we present a method that directly solves the task with an almost linear runtime no matter how large the value of k our method first construct a hypergraph that encodes the betweenness centrality and then computes the adaptive betweenness centrality by examining this graph our technique can be utilized to handle other centrality measure we theoretically prove that our method is very accurate and experimentally confirm that it is three order of magnitude faster than previous method relying on the scalability of our method we experimentally demonstrate that strategy based on adaptive betweenness centrality are effective in important application studied in the network science and database community 
with billion of database generated page on the web where consumer can readily add priced product offering to their virtual shopping cart several opportunity will become possible once we can automatically recognize what exactly is being offered for sale on each page we present a case study of a deployed data driven system that first chunk individual title into semantically classified sub segment and then us this information to improve a hyperlink insertion service to accomplish this process we propose an annotation structure that is general enough to apply to offering title from most e commerce industry while also being specific enough to identify useful semantics about each offer to automate the parsing task we apply the best practice approach of training a supervised conditional random field model and discover that creating separate prediction model for some of the industry along with the use of model ensemble achieves the best performance to date we further report on a real world application of the trained parser to the task of growing a lexical dictionary of product related term which critically provides background knowledge to an affiliate marketing hyperlink insertion service on a regular basis we apply the parser to offering title to produce a large set of labeled term from these candidate we select the most confidently predicted novel term for review by crowd sourced annotator the agreed on term are then added into a dictionary which significantly improves the performance of the link insertion service finally to continually improve system performance we retrain the model in an online fashion by performing additional annotation on title with incorrect prediction on each batch 
it is extremely important in many application domain to have transparency in predictive modeling domain expert do not tend to prefer black box predictive model model they would like to understand how prediction are made and possibly prefer model that emulate the way a human expert might make a decision with a few important variable and a clear convincing reason to make a particular prediction i will discus recent work on interpretable predictive modeling with decision list and sparse integer linear model i will describe several approach including an algorithm based on discrete optimization and an algorithm based on bayesian analysis i will show example of interpretable model for stroke prediction in medical patient and prediction of violent crime in young people raised in out of home care collaborator are ben letham berk ustun stefano traca siong thye goh tyler mccormick and david madigan 
team formation ha been long recognized a a natural way to acquire a diverse pool of useful skill by combining expert with complementary talent this allows organization to effectively complete beneficial project from different domain while also helping individual expert position themselves and succeed in highly competitive job market here we assume a collection of project ensuremath p where each project requires a certain set of skill and yield a different benefit upon completion we are further presented with a pool of expert ensuremath x where each expert ha his own skillset and compensation demand then we study the problem of hiring a cluster of expert t x so that the overall compensation cost doe not exceed a given budget b and the total benefit of the project that this team can collectively cover is maximized we refer to this a the clusterhire problem our work present a detailed analysis of the computational complexity and hardness of approximation of the problem a well a heuristic yet effective algorithm for solving it in practice we demonstrate the efficacy of our approach through experiment on real datasets of expert and demonstrate their advantage over intuitive baseline we also explore additional variant of the fundamental problem formulation in order to account for constraint and consideration that emerge in realistic cluster hiring scenario all variant considered in this paper have immediate application in the cluster hiring process a it emerges in the context of different organizational setting 
in many application classification label may not be associated with a single instance of record but may be associated with a data set of record the class behavior may not be possible to infer effectively from a single record but may be only be inferred by an aggregate set of record therefore in this problem the class label is associated with a set of instance both in the training and test data therefore the problem may be understood to be that of classifying a set of data set typically the classification behavior may only be inferred from the overall pattern of data distribution and very little information is embedded in any given record for classification purpose we refer to this problem a the setwise classification problem the problem can be extremely challenging in scenario where the data is received in the form of a stream and the record within any particular data set may not necessarily be received contiguously in this paper we present a first approach for real time and streaming classification of such data we present experimental result illustrating the effectiveness of the approach 
we study the problem of determining if an input matrix a rm x n can be well approximated by a low rank matrix specifically we study the problem of quickly estimating the rank or stable rank of a the latter often providing a more robust measure of the rank since we seek significantly sublinear time algorithm we cast these problem in the property testing framework in this framework a either ha low rank or stable rank or is far from having this property the algorithm should read only a small number of entry or row of a and decide which case a is in with high probability if neither case occurs the output is allowed to be arbitrary we consider two notion of being far a requires changing at least an fraction of it entry or a requires changing at least an fraction of it row we call the former the entry model and the latter the row model we show for testing if a matrix ha rank at most d in the entry model we improve the previous number of entry of a that need to be read from o d krauthgamer and sasson soda to o d our algorithm is the first to adaptively query the entry of a which for constant d we show is necessary to achieve o query for the important case of d we also give a new non adaptive algorithm improving the previous o query to o log for testing if a matrix ha rank at most d in the row model we prove an d lower bound on the number of row that need to be read even for adaptive algorithm our lower bound match a non adaptive upper bound of krauthgamer and sasson for testing if a matrix ha stable rank at most d in the row model or requires changing an d fraction of it row in order to have stable rank at most d we prove that reading d row is necessary and sufficient we also give an empirical evaluation of our rank and stable rank algorithm on real and synthetic datasets 
a tensor provide a natural representation for the higher order relation tensor factorization technique such a tucker decomposition and candecomp parafac decomposition have been applied to many field tucker decomposition ha strong capacity of expression but the time complexity is unpractical for the large scale real problem on the other hand candecomp parafac decomposition is linear in the feature dimensionality but the assumption is so strong that it abandon some important information besides both of td and cp decompose a tensor into several factor matrix however the factor matrix are not natural for the representation of the higher order relation to overcome these problem we propose a near linear tensor factorization approach which decompose a tensor into factor tensor in order to model the higher order relation without loss of important information in addition to reduce the time complexity and the number of the parameter we decompose each slice of the factor tensor into two smaller matrix we conduct experiment on both synthetic datasets and real datasets the experimental result on the synthetic datasets validate that our model ha strong capacity of expression the result on the real datasets show that our approach outperforms the state of the art tensor factorization method 
stochastic gradient descent sgd is a popular technique for large scale optimization problem in machine learning in order to parallelize sgd minibatch training need to be employed to reduce the communication cost however an increase in minibatch size typically decrease the rate of convergence this paper introduces a technique based on approximate optimization of a conservatively regularized objective function within each minibatch we prove that the convergence rate doe not decrease with increasing minibatch size experiment demonstrate that with suitable implementation of approximate optimization the resulting algorithm can outperform standard sgd in many scenario 
point of interest poi recommendation ha become an important mean to help people discover attractive location however extreme sparsity of user poi matrix creates a severe challenge to cope with this challenge viewing mobility record on location based social network lbsns a implicit feedback for poi recommendation we first propose to exploit weighted matrix factorization for this task since it usually serf collaborative filtering with implicit feedback better besides researcher have recently discovered a spatial clustering phenomenon in human mobility behavior on the lbsns i e individual visiting location tend to cluster together and also demonstrated it effectiveness in poi recommendation thus we incorporate it into the factorization model particularly we augment user and poi latent factor in the factorization model with activity area vector of user and influence area vector of poi respectively based on such an augmented model we not only capture the spatial clustering phenomenon in term of two dimensional kernel density estimation but we also explain why the introduction of such a phenomenon into matrix factorization help to deal with the challenge from matrix sparsity we then evaluate the proposed algorithm on a large scale lbsn dataset the result indicate that weighted matrix factorization is superior to other form of factorization model and that incorporating the spatial clustering phenomenon into matrix factorization improves recommendation performance 
targeted online advertising is a prime source of revenue for many internet company it is a common industry practice to use a generalized second price auction mechanism to rank advertisement at every opportunity of an impression this greedy algorithm is suboptimal for both advertiser and publisher when advertiser have a finite budget in a greedy mechanism high performing advertiser tend to drop out of the auction marketplace fast and that adversely affect both the advertiser experience and the publisher revenue we describe a method for improving such ad serving system by including a budget pacing component that serf ad by being aware of global supply pattern such a system is beneficial for both advertiser and publisher we demonstrate the benefit of this component using experiment we conducted on advertising at linkedin 
user recommender system are a key component in any on line social networking platform they help the user growing their network faster thus driving engagement and loyalty in this paper we study link prediction with explanation for user recommendation in social network for this problem we propose wtfw who to follow and why a stochastic topic model for link prediction over directed and node attributed graph our model not only predicts link but for each predicted link it decides whether it is a topical or a social link and depending on this decision it produce a different type of explanation a topical link is recommended between a user interested in a topic and a user authoritative in that topic the explanation in this case is a set of binary feature describing the topic responsible of the link creation a social link is recommended between user which share a large social neighborhood in this case the explanation is the set of neighbor which are more likely to be responsible for the link creation our experimental assessment on real world data confirms the accuracy of wtfw in the link prediction and the quality of the associated explanation 
the automatic evaluation of computer program is a nascent area of research with a potential for large scale impact extant program assessment system score mostly based on the number of test case passed providing no insight into the competency of the programmer in this paper we present a system to grade computer program automatically in addition to grading a program on it programming practice and complexity the key kernel of the system is a machine learning based algorithm which determines closeness of the logic of the given program to a correct program this algorithm us a set of highly informative feature derived from the abstract representation of a given program that capture the program s functionality these feature are then used to learn a model to grade the program which are built against evaluation done by expert we show that the regression model provide much better grading than the ubiquitous test case pas based grading and rival the grading accuracy of other open response problem such a essay grading we also show that our novel feature add significant value over and above basic keyword expression count feature in addition to this we propose a novel way of posing computer program grading a a one class modeling problem and report encouraging preliminary result we show the value of the system through a case study in a real world industrial deployment to the best of the author knowledge this is the first time a system using machine learning ha been developed and used for grading program the work is timely with regard to the recent boom in massively online open courseware moocs which promise to produce a significant amount of hand graded digitized data 
deep societal benefit will spring from advance in data availability and in computational procedure for mining insight and inference from large data set i will describe effort to harness data for making prediction and guiding decision touching on work in transportation healthcare online service and interactive system i will start with effort to learn and field predictive model that forecast flow of traffic in greater city region moving from the ground to the air i will discus fusing data from aircraft to make inference about atmospheric condition and using these result to enhance air transport i will then focus on experience with building and fielding predictive model in clinical medicine i will show how inference about outcome and intervention can provide insight and guide decision making moving beyond data captured by hospital i will discus the promise of transforming anonymized behavioral data drawn from web service into large scale sensor network for public health including effort to identify adverse effect of medication and to understand illness in population i will conclude by describing how we can use machine learning to leverage the complementarity of human and machine intellect to solve challenging problem in science and society 
with the globalisation of the world s economy and ever evolving financial structure fraud ha become one of the main dissipaters of government wealth and perhaps even a major contributor in the slowing down of economy in general although corporate residence fraud is known to be a major factor data availability and high sensitivity have caused this domain to be largely untouched by academia the current belgian government ha pledged to tackle this issue at large by using a variety of in house approach and cooperation with institution such a academia the ultimate goal being a fair and efficient taxation system this is the first data mining application specifically aimed at finding corporate residence fraud where we show the predictive value of using both structured and fine grained invoicing data we further describe the problem involved in building such a fraud detection system which are mainly data related e g data asymmetry quality volume variety and velocity and deployment related e g the need for explanation of the prediction made 
correlation clustering is arguably the most natural formulation of clustering given a set of object and a pairwise similarity measure between them the goal is to cluster the object so that to the best possible extent similar object are put in the same cluster and dissimilar object are put in different cluster a it just need a definition of similarity it broad generality make it applicable to a wide range of problem in different context and in particular make it naturally suitable to clustering structured object for which feature vector can be difficult to obtain despite it simplicity generality and wide applicability correlation clustering ha so far received much more attention from the algorithmic theory community than from the data mining community the goal of this tutorial is to show how correlation clustering can be a powerful addition to the toolkit of the data mining researcher and practitioner and to encourage discussion and further research in the area in the tutorial we will survey the problem and it most common variant with an emphasis on the algorithmic technique and key idea developed to derive efficient solution we will motivate the problem and discus real world application the scalability issue that may arise and the existing approach to handle them 
how can we optimize the topology of a networked system to bring a flu under control propel a video to popularity or stifle a network malware in it infancy previous work on information diffusion ha focused on modeling the diffusion dynamic and selecting node to maximize minimize influence only a paucity of recent study have attempted to address the network modification problem where the goal is to either facilitate desirable spread or curtail undesirable one by adding or deleting a small subset of network node or edge in this paper we focus on the widely studied linear threshold diffusion model and prove for the first time that the network modification problem under this model have supermodular objective function this surprising property allows u to design efficient data structure and scalable algorithm with provable approximation guarantee despite the hardness of the problem in question both the time and space complexity of our algorithm are linear in the size of the network which allows u to experiment with million of node and edge we show that our algorithm outperform an array of heuristic in term of their effectiveness in controlling diffusion process often beating the next best by a significant margin 
in topic modelling various alternative prior have been developed for instance asymmetric and symmetric prior for the document topic and topic word matrix respectively the hierarchical dirichlet process prior for the document topic matrix and the hierarchical pitman yor process prior for the topic word matrix for information retrieval language model exhibiting word burstiness are important indeed this burstiness effect ha been show to help topic model a well and this requires additional word probability vector for each document here we show how to combine these idea to develop high performing non parametric topic model exhibiting burstiness based on standard gibbs sampling experiment are done to explore the behavior of the model under different condition and to compare the algorithm with previously published the full non parametric topic model with burstiness are only a small factor slower than standard gibbs sampling for lda and require double the memory making them very competitive we look at the comparative behaviour of different model and present some experimental insight 
pattern discovery is a core data mining activity initial approach were dominated by the frequent pattern discovery paradigm only pattern that occur frequently in the data were explored having been thoroughly researched and it limitation now well understood this paradigm is giving way to a new one which can be called statistically sound pattern discovery in this paradigm the main impetus is to discover statistically significant pattern which are unlikely to have occurred by chance and are likely to hold in future data thus the new paradigm provides a strict control over false discovery and overfitting this tutorial cover both classic and cutting edge research topic on pattern discovery combined to statistical significance testing we start with an advanced introduction to the relevant form of statistical significance testing including different school and alternative model their underlying assumption practical issue and limitation we then discus their application to data mining specific problem including evaluation of nested pattern the multiple testing problem algorithmic strategy and real world consideration we present the current state of the art solution and explore in detail how this approach to pattern discovery can deliver efficient and effective discovery of small set of interesting pattern 
counting the number of distinct element in a large dataset is a common task in web application and database this problem is difficult in limited memory setting where storing a large hash table table is intractable this paper advance the state of the art in probabilistic method for estimating the number of distinct element in a streaming setting new streaming algorithm are given that provably beat the optimal error for min count and hyperloglog while using the same sketch this paper also contributes to the understanding and theory of probabilistic cardinality estimation introducing the concept of an area cutting process and the martingale estimator these idea lead to theoretical analysis of both old and new sketch and estimator and show the new estimator are optimal for several streaming setting while also providing accurate error bound that match those obtained via simulation furthermore the area cutting process provides a geometric intuition behind all method for counting distinct element which are not affected by duplicate this intuition lead to a new sketch discrete max count and the analysis of a class of sketch self similar area cutting decomposition that have attractive property and unbiased estimator for both streaming and non streaming setting together these contribution lead to multi faceted advance in sketch construction cardinality and error estimation the theory and intuition for the problem of approximate counting of distinct element for both the streaming and non streaming case 
a online service have more and more popular incident diagnosis ha emerged a a critical task in minimizing the service downtime and ensuring high quality of the service provided for most online service incident diagnosis is mainly conducted by analyzing a large amount of telemetry data collected from the service at runtime time series data and event sequence data are two major type of telemetry data technique of correlation analysis are important tool that are widely used by engineer for data driven incident diagnosis despite their importance there ha been little previous work addressing the correlation between two type of heterogeneous data for incident diagnosis continuous time series data and temporal event data in this paper we propose an approach to evaluate the correlation between time series data and event data our approach is capable of discovering three important aspect of event timeseries correlation in the context of incident diagnosis existence of correlation temporal order and monotonic effect our experimental result on simulation data set and two real data set demonstrate the effectiveness of the algorithm 
data stream mining ha gained growing attention due to it wide emerging application such a target marketing email filtering and network intrusion detection in this paper we propose a prototype based classification model for evolving data stream called syncstream which dynamically model time changing concept and make prediction in a local fashion instead of learning a single model on a sliding window or ensemble learning syncstream capture evolving concept by dynamically maintaining a set of prototype in a new data structure called the p tree the prototype are obtained by error driven representativeness learning and synchronization inspired constrained clustering to identify abrupt concept drift in data stream pca and statistic based heuristic approach are employed syncstream ha several attractive benefit a it is capable of dynamically modeling evolving concept from even a small set of prototype and is robust against noisy example b owing to synchronization based constrained clustering and the p tree it support an efficient and effective data representation and maintenance c gradual and abrupt concept drift can be effectively detected empirical result show that our method achieves good predictive performance compared to state of the art algorithm and that it requires much le time than another instance based stream mining algorithm 
in many diverse setting aggregated opinion of others play an increasingly dominant role in shaping individual decision making one key prerequisite of harnessing the crowd wisdom is the independency of individual opinion yet in real setting collective opinion are rarely simple aggregation of independent mind recent experimental study document that disclosing prior collective opinion distorts individual decision making a well a their perception of quality and value highlighting a fundamental disconnect from current modeling effort how to model social influence and it impact on system that are constantly evolving in this paper we develop a mechanistic framework to model social influence of prior collective opinion e g online product rating on subsequent individual decision making we find our method successfully capture the dynamic of rating growth helping u separate social influence bias from inherent value using large scale longitudinal customer rating datasets we demonstrate that our model not only effectively ass social influence bias but also accurately predicts long term cumulative growth of rating solely based on early rating trajectory we believe our framework will play an increasingly important role a our understanding of social process deepens it promotes strategy to untangle manipulation and social bias and provides insight towards a more reliable and effective design of social platform 
inferring diffusion network from trace of cascade ha been extensively studied to better understand information diffusion in many domain a widely used assumption in previous work is that the diffusion network is homogenous and diffusion process of cascade follow the same pattern however in social medium user may have various interest and the connection among them are usually multi faceted in addition different cascade normally diffuse at different speed and spread to diverse scale and hence show various diffusion pattern it is challenging for traditional model to capture the heterogeneous user interaction and diverse pattern of cascade in social medium in this paper we investigate a novel problem of inferring multi aspect diffusion network with multi pattern cascade in particular we study the effect of various diffusion pattern on the information diffusion process by analyzing user retweeting behavior on a microblogging dataset by incorporating aspect level user interaction and various diffusion pattern a new model for inferring multi aspect transmission rate between user using multi pattern cascade mmrate is proposed we also provide an expectation maximization algorithm to effectively estimate the parameter experimental result on both synthetic and microblogging datasets demonstrate the superior performance of our approach over the state of the art method in inferring multi aspect diffusion network 
given two homogeneous rating matrix with some overlapped user item whose mapping are unknown this paper aim at answering two question first can we identify the unknown mapping between the user and or item second can we further utilize the identified mapping to improve the quality of recommendation in either domain our solution integrates a latent space matching procedure and a refining process based on the optimization of prediction to identify the matching then we further design a transfer based method to improve the recommendation performance using both synthetic and real data we have done extensive experiment given different real life scenario to verify the effectiveness of our model the code and other material are available at http www csie ntu edu tw r matching 
multi task feature learning ha been proposed to improve the generalization performance by learning the shared feature among multiple related task and it ha been successfully applied to many real world problem in machine learning data mining computer vision and bioinformatics most existing multi task feature learning model simply assume a common noise level for all task which may not be the case in real application recently a calibrated multivariate regression cmr model ha been proposed which calibrates different task with respect to their noise level and achieves superior prediction performance over the non calibrated one a major challenge is how to solve the cmr model efficiently a it is formulated a a composite optimization problem consisting of two non smooth term in this paper we propose a variant of the calibrated multi task feature learning formulation by including a squared norm regularizer we show that the dual problem of the proposed formulation is a smooth optimization problem with a piecewise sphere constraint the simplicity of the dual problem enables u to develop fast dual optimization algorithm with low per iteration cost we also provide a detailed convergence analysis for the proposed dual optimization algorithm empirical study demonstrate that the dual optimization algorithm quickly converges and it is much more efficient than the primal optimization algorithm moreover the calibrated multi task feature learning algorithm with and without the squared norm regularizer achieve similar prediction performance and both outperform the non calibrated one thus the proposed variant not only enables u to develop fast optimization algorithm but also keep the superior prediction performance of the calibrated multi task feature learning over the non calibrated one 
image are often used to convey many different concept or illustrate many different story we propose an algorithm to mine multiple diverse relevant and interesting text snippet for image on the web our algorithm scale to all image on the web for each image all webpage that contain it are considered the top k text snippet selection problem is posed a combinatorial subset selection with the goal of choosing an optimal set of snippet that maximizes a combination of relevancy interestingness and diversity the relevancy and interestingness are scored by machine learned model our algorithm is run at scale on the entire image index of a major search engine resulting in the construction of a database of image with their corresponding text snippet we validate the quality of the database through a large scale comparative study we showcase the utility of the database through two web scale application a augmentation of image on the web a webpage are browsed and b an image browsing experience similar in spirit to web browsing that is enabled by interconnecting semantically related image which may not be visually related through shared concept in their corresponding text snippet 
cardiac disease is the leading cause of death around the world with ischemic heart disease alone claiming million life in this burden can be attributed in part to the absence of biomarkers that can reliably identify high risk patient and match them to treatment that are appropriate for them in recent clinical study we have demonstrated the ability of computation to extract information with substantial prognostic utility that is typically disregarded in time series data collected from cardiac patient of particular interest are subtle variation in long term electrocardiographic ecg data that are usually overlooked a noise but provide a useful assessment of myocardial instability in multiple clinical cohort we have developed the pathophysiological basis for studying probabilistic variation in long term ecg and demonstrated the ability of this information to effectively risk stratify patient at risk of dying following heart attack in this paper we extend this work and focus on the question of how to reduce it computational complexity for scalable use in large datasets or energy constrained embedded device our basic approach to uncovering pathological structure within the ecg focus on characterizing beat to beat time warped shape deformation of the ecg using a modified dynamic time warping dtw and lomb scargle periodogram based algorithm a part of our effort to scale this work up we explore a novel approach to address the quadratic runtime of dtw we achieve this by developing the idea of adaptive downsampling to reduce the size of the input presented to dtw and describe change to the dynamic programming problem underlying dtw to exploit adaptively downsampled ecg signal when evaluated on data from patient in the disperse timi trial our result show that high morphologic variability is associated with an to fold increased risk of death within day of a heart attack moreover the use of adaptive downsampling with a modified dtw formulation achieves a to almost fold reduction in runtime relative to dtw without a significant change in biomarker discrimination 
with the rapid growth of web a variety of content sharing service such a flickr youtube blogger and tripadvisor etc have become extremely popular over the last decade on these website user have created and shared with each other various kind of resource such a photo video and travel blog the sheer amount of user generated content varies greatly in quality which call for a principled method to identify a set of authority who created high quality resource from a massive number of contributor of content since most previous study only infer global authoritativeness of a user there is no way to differentiate the authoritativeness in different aspect of life topic in this paper we propose a novel model of topic specific authority analysis taa which address the limitation of the previous approach to identify authority specific to given query topic s on a content sharing service this model jointly leverage the usage data collected from the sharing log and the favorite log the parameter in taa are learned from a constructed training dataset for which a novel logistic likelihood function is specifically designed to perform bayesian inference for taa with the new logistic likelihood we extend typical gibbs sampling by introducing auxiliary variable thorough experiment with two real world datasets demonstrate the effectiveness of taa in topic specific authority identification a well a the generalizability of the taa generative model 
unconditional cash transfer to the extreme poor via mobile telephony represent a radical new approach to giving givedirectly is a non governmental organization ngo at the vanguard of delivering this proven and effective approach to reducing poverty in this work we streamline an important step in the operation of the ngo by developing and deploying a data driven system for locating village with extreme poverty in kenya and uganda using the type of roof of a home thatched or metal a a proxy for poverty we develop a new remote sensing approach for selecting extremely poor village to target for cash transfer we develop an analytics algorithm that estimate housing quality and density in patch of publicly available satellite imagery by learning a predictive model with sieve of template matching result combined with color histogram a feature we develop and deploy a crowdsourcing interface to obtain labeled training data we deploy the predictive model to construct a fine scale heat map of poverty and integrate this discovered knowledge into the process of givedirectly s operation aggregating estimate at the village level we produce a ranked list from which top village are included in givedirectly s planned distribution of cash transfer the automated approach increase village selection efficiency significantly 
recommender system have become very important for many online activity such a watching movie shopping for product and connecting with friend on social network user behavioral analysis and user feedback both explicit and implicit modeling are crucial for the improvement of any online recommender system widely adopted recommender system at linkedin such a people you may know and endorsement are evolving by analyzing user behavior on impressed recommendation item in this paper we address modeling impression discounting of recommended item that is how to model user s no action feedback on impressed recommended item the main contribution of this paper include large scale analysis of impression data from linkedin and kdd cup novel anti noise regression technique and it application to learn four different impression discounting function including linear decay inverse decay exponential decay and quadratic decay applying these impression discounting function to linkedin s people you may know and endorsement recommender system 
the nystrom method is an efficient approach to enabling large scale kernel method the nystrom method generates a fast approximation to any large scale symmetric positive semidefinete spsd matrix using only a few column of the spsd matrix however since the nystrom approximation is low rank when the spectrum of the spsd matrix decay slowly the nystrom approximation is of low accuracy in this paper we propose a variant of the nystrom method called the modified nystrom by spectral shifting s nystrom the s nystrom method work well no matter whether the spectrum of spsd matrix decay fast or slow we prove that our s nystrom ha a much stronger error bound than the standard and modified nystrom method and that s nystrom can be even more accurate than the truncated svd of the same scale in some case we also devise an algorithm such that the s nystrom approximation can be computed nearly a efficient a the modified nystrom approximation finally our s nystrom method demonstrates significant improvement over the standard and modified nystrom method on several real world datasets 
with the rapid prevalence of smart mobile device the number of mobile apps available ha exploded over the past few year to facilitate the choice of mobile apps existing mobile app recommender system typically recommend popular mobile apps to mobile user however mobile apps are highly varied and often poorly understood particularly for their activity and function related to privacy and security therefore more and more mobile user are reluctant to adopt mobile apps due to the risk of privacy invasion and other security concern to fill this crucial void in this paper we propose to develop a mobile app recommender system with privacy and security awareness the design goal is to equip the recommender system with the functionality which allows to automatically detect and evaluate the security risk of mobile apps then the recommender system can provide app recommendation by considering both the apps popularity and the user security preference specifically a mobile app can lead to security risk because insecure data access permission have been implemented in this app therefore we first develop the technique to automatically detect the potential security risk for each mobile app by exploiting the requested permission then we propose a flexible approach based on modern portfolio theory for recommending apps by striking a balance between the apps popularity and the user security concern and build an app hash tree to efficiently recommend apps finally we evaluate our approach with extensive experiment on a large scale data set collected from google play the experimental result clearly validate the effectiveness of our approach 
community question answering cqa site have become valuable platform to create share and seek a massive volume of human knowledge how can we spot an insightful question that would inspire massive further discussion in cqa site how can we detect a valuable answer that benefit many user the long term impact e g the size of the population a post benefit of a question answer post is the key quantity to answer these question in this paper we aim to predict the long term impact of question answer shortly after they are posted in the cqa site in particular we propose a family of algorithm for the prediction problem by modeling three key aspect i e non linearity question answer coupling and dynamic we analyze our algorithm in term of optimality correctness and complexity we conduct extensive experimental evaluation on two real cqa data set to demonstrate the effectiveness and efficiency of our algorithm 
success of manufacturing company largely depends on reliability of their product scheduled maintenance is widely used to ensure that equipment is operating correctly so a to avoid unexpected breakdown such maintenance is often carried out separately for every component based on it usage or simply on some fixed schedule however scheduled maintenance is labor intensive and ineffective in identifying problem that develop between technician s visit unforeseen failure still frequently occur in contrast predictive maintenance technique help determine the condition of in service equipment in order to predict when and what repair should be performed the main goal of predictive maintenance is to enable pro active scheduling of corrective work and thus prevent unexpected equipment failure 
event detection in social medium is an important but challenging problem most existing approach are based on burst detection topic modeling or clustering technique which cannot naturally model the implicit heterogeneous network structure in social medium a a result only limited information such a term and geographic location can be used this paper present non parametric heterogeneous graph scan nphgs a new approach that considers the entire heterogeneous network for event detection we first model the network a a sensor network in which each node sens it neighborhood environment and report an empirical p value measuring it current level of anomalousness for each time interval e g hour or day then we efficiently maximize a nonparametric scan statistic over connected subgraphs to identify the most anomalous network cluster finally the event represented by each cluster is summarized with information such a type of event geographical location time and participant a a case study we consider two application using twitter data civil unrest event detection and rare disease outbreak detection and present empirical evaluation illustrating the effectiveness and efficiency of our proposed approach 
in performance display advertising a key metric of a campaign effectiveness is it conversion rate the proportion of user who take a predefined action on the advertiser website such a a purchase predicting this conversion rate is thus essential for estimating the value of an impression and can be achieved via machine learning one difficulty however is that the conversion can take place long after the impression up to a month and this delayed feedback hinders the conversion modeling we tackle this issue by introducing an additional model that capture the conversion delay intuitively this probabilistic model help determining whether a user that ha not converted should be treated a a negative sample when the elapsed time is larger than the predicted delay or should be discarded from the training set when it is too early to tell we provide experimental result on real traffic log that demonstrate the effectiveness of the proposed model 
in the competitive environment of the internet retaining and growing one s user base is of major concern to most web service furthermore the economic model of many web service is allowing free access to most content and generating revenue through advertising this unique model requires securing user time on a site rather than the purchase of good which make it crucially important to create new kind of metric and solution for growth and retention effort for web service in this work we address this problem by proposing a new retention metric for web service by concentrating on the rate of user return we further apply predictive analysis to the proposed retention metric on a service a a mean for characterizing lost customer finally we set up a simple yet effective framework to evaluate a multitude of factor that contribute to user return specifically we define the problem of return time prediction for free web service our solution is based on the cox s proportional hazard model from survival analysis the hazard based approach offer several benefit including the ability to work with censored data to model the dynamic in user return rate and to easily incorporate different type of covariates in the model we compare the performance of our hazard based model in predicting the user return time and in categorizing user into bucket based on their predicted return time against several baseline regression and classification method and find the hazard based approach to be superior 
early classification of time series is prevalent in many time sensitive application such a but not limited to early warning of disease outcome and early warning of crisis in stock market textcolor black for example early diagnosis allows physician to design appropriate therapeutic strategy at early stage of disease however practical adaptation of early classification of time series requires an easy to understand explanation interpretability and a measure of confidence of the prediction result uncertainty estimate these two aspect were not jointly addressed in previous time series early classification study such that a difficult choice of selecting one of these aspect is required in this study we propose a simple and yet effective method to provide uncertainty estimate for an interpretable early classification method the question we address here is how to provide estimate of uncertainty in regard to interpretable early prediction in our extensive evaluation on twenty time series datasets we showed that the proposed method ha several advantage over the state of the art method that provides reliability estimate in early classification namely the proposed method is more effective than the state of the art method is simple to implement and provides interpretable result 
the analysis of social sentiment expressed on the web is becoming increasingly relevant to a variety of application and it is important to understand the underlying mechanism which drive the evolution of sentiment in one way or another in order to be able to predict these change in the future in this paper we study the dynamic of news event and their relation to change of sentiment expressed on relevant topic we propose a novel framework which model the behavior of news and social medium in response to event a a convolution between event s importance and medium response function specific to medium and event type this framework is suitable for detecting time and duration of event a well a their impact and dynamic from time series of publication volume these data can greatly enhance event analysis for instance they can help distinguish important event from unimportant or predict sentiment and stock market shift a an example of such application we extracted news event for a variety of topic and then correlated this data with the corresponding sentiment time series revealing the connection between sentiment shift and event dynamic 
inference in topic model typically involves a sampling step to associate latent variable with observation unfortunately the generative model loses sparsity a the amount of data increase requiring o k operation per word for k topic in this paper we propose an algorithm which scale linearly with the number of actually instantiated topic kd in the document for large document collection and in structured hierarchical model kd ll k this yield an order of magnitude speedup our method applies to a wide variety of statistical model such a pdp and hdp at it core is the idea that dense slowly changing distribution can be approximated efficiently by the combination of a metropolis hastings step use of sparsity and amortized constant time sampling via walker s alias method 
this paper target the problem of computing meaningful clustering from uncertain data set existing method for clustering uncertain data compute a single clustering without any indication of it quality and reliability thus decision based on their result are questionable in this paper we describe a framework based on possible world semantics when applied on an uncertain dataset it computes a set of representative clustering each of which ha a probabilistic guarantee not to exceed some maximum distance to the ground truth clustering i e the clustering of the actual but unknown data our framework can be combined with any existing clustering algorithm and it is the first to provide quality guarantee about it result in addition our experimental evaluation show that our representative clustering have a much smaller deviation from the ground truth clustering than existing approach thus reducing the effect of uncertainty 
we present a direct multi class boosting dmcboost method for classification with the following property i instead of reducing the multi class classification task to a set of binary classification task dmcboost directly solves the multi class classification problem and only requires very weak base classifier ii dmcboost build an ensemble classifier by directly optimizing the non convex performance measure including the empirical classification error and margin function without resorting to any upper bound or approximation a a non convex optimization method dmcboost show competitive or better result than state of the art convex relaxation boosting method and it performs especially well on the noisy case 
performance monitor software for data center typically generates a great number of alert sequence these alert sequence indicate abnormal network event given a set of observed alert sequence it is important to identify the most critical alert that are potentially the cause of others while the need for mining critical alert over large scale alert sequence is evident most alert analysis technique stop at modeling and mining the causal relation among the alert this paper study the critical alert mining problem given a set of alert sequence we aim to find a set of k critical alert such that the number of alert potentially triggered by them is maximized we show that the problem is intractable therefore we resort to approximation and heuristic algorithm first we develop an approximation algorithm that obtains a near optimal alert set in quadratic time and propose pruning technique to improve it runtime performance moreover we show a faster approximation exists when the alert follow certain causal structure second we propose two fast heuristic algorithm based on tree sampling technique on real life data these algorithm identify a critical alert from up to mined causal relation in second meanwhile they preserve more than of solution quality and are up to time faster than their approximation counterpart 
most semi supervised learning model propagate the label over the laplacian graph where the graph should be built beforehand however the computational cost of constructing the laplacian graph matrix is very high on the other hand when we do classification data point lying around the decision boundary boundary point are noisy for learning the correct classifier and deteriorate the classification performance to address these two challenge in this paper we propose an adaptive semi supervised learning model different from previous semi supervised learning approach our new model needn t construct the graph laplacian matrix thus our method avoids the huge computational cost required by previous method and achieves a computational complexity linear to the number of data point therefore our method is scalable to large scale data moreover the proposed model adaptively suppresses the weight of boundary point such that our new model is robust to the boundary point an efficient algorithm is derived to alternatively optimize the model parameter and class probability distribution of the unlabeled data such that the induction of classifier and the transduction of label are adaptively unified into one framework extensive experimental result on six real world data set show that the proposed semi supervised learning model outperforms other related method in most case 
linear regression is a widely used tool in data mining and machine learning in many application fitting a regression model with only linear effect may not be sufficient for predictive or explanatory purpose one strategy which ha recently received increasing attention in statistic is to include feature interaction to capture the nonlinearity in the regression model such model ha been applied successfully in many biomedical application one major challenge in the use of such model is that the data dimensionality is significantly higher than the original data resulting in the small sample size large dimension problem recently weak hierarchical lasso a sparse interaction regression model is proposed that produce sparse and hierarchical structured estimator by exploiting the lasso penalty and a set of hierarchical constraint however the hierarchical constraint make it a non convex problem and the existing method find the solution of it convex relaxation which need additional condition to guarantee the hierarchical structure in this paper we propose to directly solve the non convex weak hierarchical lasso by making use of the gist general iterative shrinkage and thresholding optimization framework which ha been shown to be efficient for solving non convex sparse formulation the key step in gist is to compute a sequence of proximal operator one of our key technical contribution is to show that the proximal operator associated with the non convex weak hierarchical lasso admits a closed form solution however a naive approach for solving each subproblem of the proximal operator lead to a quadratic time complexity which is not desirable for large size problem to this end we further develop an efficient algorithm for computing the subproblems with a linearithmic time complexity we have conducted extensive experiment on both synthetic and real data set result show that our proposed algorithm is much more efficient and effective than it convex relaxation 
social medium such a twitter or weblogs are a popular source for live textual data much of this popularity is due to the fast rate at which this data arrives and there are a number of global event such a the arab spring where twitter is reported to have had a major influence however existing method for emerging topic detection are often only able to detect event of a global magnitude such a natural disaster or celebrity death and can monitor user selected keywords or operate on a curated set of hashtags only interesting emerging topic may however be of much smaller magnitude and may involve the combination of two or more word that themselves are not unusually hot at that time our contribution to the detection of emerging trend are three fold first of all we propose a significance measure that can be used to detect emerging topic early long before they become hot tag by drawing upon experience from outlier detection secondly by using hash table in a heavy hitter type algorithm for establishing a noise baseline we show how to track even all keyword pair using only a fixed amount of memory finally we aggregate the detected co trend into larger topic using clustering approach a often a a single event will cause multiple word combination to trend at the same time 
collaborative network are composed of expert who cooperate with each other to complete specific task such a resolving problem reported by customer a task is posted and subsequently routed in the network from an expert to another until being resolved when an expert cannot solve a task his routing decision i e where to transfer a task is critical since it can significantly affect the completion time of a task in this work we attempt to deduce the cognitive process of task routing and model the decision making of expert a a generative process where a routing decision is made based on mixed routing pattern in particular we observe an interesting phenomenon that an expert tends to transfer a task to someone whose knowledge is neither too similar to nor too different from his own based on this observation an expertise difference based routing pattern is developed we formalize multiple routing pattern by taking into account both rational and random analysis of task and present a generative model to combine them for a held out set of task our model not only explains their real routing sequence very well but also accurately predicts their completion time under three different quality measure our method significantly outperforms all the alternative with more than accuracy gain in practice with the help of our model hypothesis on how to improve a collaborative network can be tested quickly and reliably thereby significantly easing performance improvement of collaborative network 
poor academic performance in k is often a precursor to unsatisfactory educational outcome such a dropout which are associated with significant personal and social cost hence it is important to be able to predict student at risk of poor performance so that the right personalized intervention plan can be initiated in this paper we report on a large scale study to identify student at risk of not meeting acceptable level of performance in one state level and one national standardized assessment in grade of a major u school district an important highlight of our study is it scale both in term of the number of student included the number of year and the number of feature which provide a very solid grounding to the research we report on our experience with handling the scale and complexity of data and on the relative performance of various machine learning technique we used for building predictive model our result demonstrate that it is possible to predict student at risk of poor assessment performance with a high degree of accuracy and to do so well in advance these insight can be used to pro actively initiate personalized intervention program and improve the chance of student success 
the effective analysis of social network and graph structured data is often limited by the privacy concern of individual whose data make up these network differential privacy offer individual a rigorous and appealing guarantee of privacy but while differentially private algorithm for computing basic graph property have been proposed most graph modeling task common in the data mining community cannot yet be carried out privately in this work we propose algorithm for privately estimating the parameter of exponential random graph model ergms we break the estimation problem into two step computing private sufficient statistic then using them to estimate the model parameter we consider specific alternating statistic that are in common use for ergm model and describe a method for estimating them privately by adding noise proportional to a high confidence bound on their local sensitivity in addition we propose an estimation algorithm that considers the noise distribution of the private statistic and offer better accuracy than performing standard parameter estimation using the private statistic 
in this paper we propose a citywide and real time model for estimating the travel time of any path represented a a sequence of connected road segment in real time in a city based on the gps trajectory of vehicle received in current time slot and over a period of history a well a map data source though this is a strategically important task in many traffic monitoring and routing system the problem ha not been well solved yet given the following three challenge the first is the data sparsity problem i e many road segment may not be traveled by any gps equipped vehicle in present time slot in most case we cannot find a trajectory exactly traversing a query path either second for the fragment of a path with trajectory they are multiple way of using or combining the trajectory to estimate the corresponding travel time finding an optimal combination is a challenging problem subject to a tradeoff between the length of a path and the number of trajectory traversing the path i e support third we need to instantly answer user query which may occur in any part of a given city this call for an efficient scalable and effective solution that can enable a citywide and real time travel time estimation to address these challenge we model different driver travel time on different road segment in different time slot with a three dimension tensor combined with geospatial temporal and historical context learned from trajectory and map data we fill in the tensor s missing value through a context aware tensor decomposition approach we then devise and prove an object function to model the aforementioned tradeoff with which we find the most optimal concatenation of trajectory for an estimate through a dynamic programming solution in addition we propose using frequent trajectory pattern mined from historical trajectory to scale down the candidate of concatenation and a suffix tree based index to manage the trajectory received in the present time slot we evaluate our method based on extensive experiment using gps trajectory generated by more than taxi over a period of two month the result demonstrate the effectiveness efficiency and scalability of our method beyond baseline approach 
this paper present a novel guided image filtering method using multipoint local polynomial approximation lpa with range guidance in our method the lpa is extended from a pointwise model into a multipoint model for reliable filtering and better preserving image spatial variation which usually contains the essential information in the input image in addition we develop a scheme with constant computational complexity invariant to the size of filtering kernel for generating a spatial adaptive support region around a point by using the hybrid of the local polynomial model and color intensity based range guidance the proposed method not only preserve edge but also doe a much better job in preserving spatial variation than existing popular filtering method our method prof to be effective in a number of application depth image upsampling joint image denoising detail enhancement and image abstraction experimental result show that our method produce better result than state of the art method and it is also computationally efficient 
video event detection allows intelligent indexing of video content based on event traditional approach extract feature from video frame or shot then quantize and pool the feature to form a single vector representation for the entire video though simple and efficient the final pooling step may lead to loss of temporally local information which is important in indicating which part in a long video signifies presence of the event in this work we propose a novel instance based video event detection approach we represent each video a multiple instance defined a video segment of different temporal interval the objective is to learn an instance level event detection model based on only video level label to solve this problem we propose a large margin formulation which treat the instance label a hidden latent variable and simultaneously infers the instance label a well a the instance level classification model our framework infers optimal solution that assume positive video have a large number of positive instance while negative video have the fewest one extensive experiment on large scale video event datasets demonstrate significant performance gain the proposed method is also useful in explaining the detection result by localizing the temporal segment in a video which is responsible for the positive detection 
modeling interaction of multiple co occurring object in a complex activity is becoming increasingly popular in the video domain the dynamic bayesian network dbn ha been applied to this problem in the past due to it natural ability to statistically capture complex temporal dependency however standard dbn structure learning algorithm are generatively learned require manual structure definition and or are computationally complex or restrictive we propose a novel structure learning solution that fuse the granger causality statistic a direct measure of temporal dependence with the adaboost feature selection algorithm to automatically constrain the temporal link of a dbn in a discriminative manner this approach enables u to completely define the dbn structure prior to parameter learning which reduces computational complexity in addition to providing a more descriptive structure we refer to this modeling approach a the granger constraint dbn gcdbn our experiment show how the gcdbn outperforms two of the most relevant state of the art graphical model in complex activity classification on handball video data surveillance data and synthetic data 
when building vision system that predict structured object such a image segmentation or human pose a crucial concern is performance under task specific evaluation measure e g jaccard index or average precision an ongoing research challenge is to optimize prediction so a to maximize performance on such complex measure in this work we present a simple meta algorithm that is surprisingly effective empirical min bayes risk embr take a input a pre trained model that would normally be the final product and learns three additional parameter so a to optimize performance on the complex high order task specific measure we demonstrate embr in several domain taking existing state of the art algorithm and improving performance up to simply with three extra parameter 
the underlying idea of multitask learning is that learning task jointly is better than learning each task individually in particular if only a few training example are available for each task sharing a jointly trained representation improves classification performance in this paper we propose a novel multitask learning method that learns a low dimensional representation jointly with the corresponding classifier which are then able to profit from the latent inter class correlation our method scale with respect to the original feature dimension and can be used with high dimensional image descriptor such a the fisher vector furthermore it consistently outperforms the current state of the art on the sun scene classification benchmark with varying amount of training data 
curvature ha received increasing attention a an important alternative to length based regularization in computer vision in contrast to length it preserve elongated structure and fine detail existing approach are either inefficient or have low angular resolution and yield result with strong block artifact we derive a new model for computing squared curvature based on integral geometry the model count response of straight line triple clique the corresponding energy decomposes into submodular and supermodular pairwise potential we show that this energy can be efficiently minimized even for high angular resolution using the trust region framework our result confirm that we obtain accurate and visually pleasing solution without strong artifact at reasonable runtimes 
in this work we describe a convolutional neural network cnn to accurately predict image quality without a reference image taking image patch a input the cnn work in the spatial domain without using hand crafted feature that are employed by most previous method the network consists of one convolutional layer with max and min pooling two fully connected layer and an output node within the network structure feature learning and regression are integrated into one optimization process which lead to a more effective model for estimating image quality this approach achieves state of the art performance on the live dataset and show excellent generalization ability in cross dataset experiment further experiment on image with local distortion demonstrate the local quality estimation ability of our cnn which is rarely reported in previous literature 
retinal image contain forest of mutually intersecting and overlapping venous and arterial vascular tree the geometry of these tree show adaptation to vascular disease including diabetes stroke and hypertension segmentation of the retinal vascular network is complicated by inconsistent vessel contrast fuzzy edge variable image quality medium opacity complex intersection and overlap this paper present a bayesian approach to resolving the configuration of vascular junction to correctly construct the vascular tree a probabilistic model of vascular joint terminal bridge and bifurcation and their configuration in junction is built and maximum a posteriori map estimation used to select most likely configuration the model is built using a reference set of joint extracted from the drive public domain vascular segmentation dataset and evaluated on joint from the drive test set demonstrating an accuracy of 
in this paper we introduce a new distance for robustly matching vector of d rotation a special representation of d rotation which we coin full angle quaternion faq allows u to express this distance a euclidean we apply the distance to the problem of d shape recognition from point cloud and d object tracking in color video for the former we introduce a hashing scheme for scale and translation which outperforms the previous state of the art approach on a public dataset for the latter we incorporate online subspace learning with the proposed faq representation to highlight the benefit of the new representation 
we have discovered that d reconstruction can be achieved from asingle still photographic capture due to accidental motion of thephotographer even while attempting to hold the camera still although these motion result in little baseline and therefore high depth uncertainty in theory we can combine many such measurement over the duration of the capture process a few second to achieve usable depth estimate wepresent a novel d reconstruction system tailored for this problemthat produce depth map from short video sequence from standard cameraswithout the need for multi lens optic active sensor or intentionalmotions by the photographer this result lead to the possibilitythat depth map of sufficient quality for rgb d photography application likeperspective change simulated aperture and object segmentation cancome for free for a significant fraction of still photographsunder reasonable condition 
reconstructing the shape of a d object from multi view image under unknown general illumination is a fundamental problem in computer vision and high quality reconstruction is usually challenging especially when high detail is needed this paper present a total variation tv based approach for recovering surface detail using shading and multi view stereo mv behind the approach are our two important observation the illumination over the surface of an object tends to be piecewise smooth and the recovery of surface orientation is not sufficient for reconstructing geometry which were previously overlooked thus we introduce tv to regularize the lighting and use visual hull to constrain partial vertex the reconstruction is formulated a a constrained tvminimization problem that treat the shape and lighting a unknown simultaneously an augmented lagrangian method is proposed to quickly solve the tv minimization problem a a result our approach is robust stable and is able to efficiently recover high quality of surface detail even starting with a coarse mv these advantage are demonstrated by the experiment with synthetic and real world example 
despite the fact that face detection ha been studied intensively over the past several decade the problem is still not completely solved challenging condition such a extreme pose lighting and occlusion have historically hampered traditional model based method in contrast exemplar based face detection ha been shown to be effective even under these challenging condition primarily because a large exemplar database is leveraged to cover all possible visual variation however relying heavily on a large exemplar database to deal with the face appearance variation make the detector impractical due to the high space and time complexity we construct an efficient boosted exemplar based face detector which overcomes the defect of the previous work by being faster more memory efficient and more accurate in our method exemplar a weak detector are discriminatively trained and selectively assembled in the boosting framework which largely reduces the number of required exemplar notably we propose to include non face image a negative exemplar to actively suppress false detection to further improve the detection accuracy we verify our approach over two public face detection benchmark and one personal photo album and achieve significant improvement over the state of the art algorithm in term of both accuracy and efficiency 
in this paper we present a novel online visual tracking method based on linear representation first we present a novel probability continuous outlier model pcom to depict the continuous outlier that occur in the linear representation model in the proposed model the element of the noisy observation sample can be either represented by a pca subspace with small guassian noise or treated a an arbitrary value with a uniform prior in which the spatial consistency prior is exploited by using a binary markov random field model then we derive the objective function of the pcom method the solution of which can be iteratively obtained by the outlier free least square and standard max flow min cut step finally based on the proposed pcom method we design an effective observation likelihood function and a simple update scheme for visual tracking both qualitative and quantitative evaluation demonstrate that our tracker achieves very favorable performance in term of both accuracy and speed 
interactive segmentation in which a user provides a bounding box to an object of interest for image segmentation ha been applied to a variety of application in image editing crowdsourcing computer vision and medical imaging the challenge of this semi automatic image segmentation task lie in dealing with the uncertainty of the foreground object within a bounding box here we formulate the interactive segmentation problem a a multiple instance learning mil task by generating positive bag from pixel of sweeping line within a bounding box we name this approach milcut we provide a justification to our formulation and develop an algorithm with significant performance and efficiency gain over existing state of the art system extensive experiment demonstrate the evident advantage of our approach 
a training process for facial expression recognition is usually performed sequentially in three individual stage feature learning feature selection and classifier construction extensive empirical study are needed to search for an optimal combination of feature representation feature set and classifier to achieve good recognition performance this paper present a novel boosted deep belief network bdbn for performing the three training stage iteratively in a unified loopy framework through the proposed bdbn framework a set of feature which is effective to characterize expression related facial appearance shape change can be learned and selected to form a boosted strong classifier in a statistical way a learning continues the strong classifier is improved iteratively and more importantly the discriminative capability of selected feature are strengthened a well according to their relative importance to the strong classifier via a joint fine tune process in the bdbn framework extensive experiment on two public database showed that the bdbn framework yielded dramatic improvement in facial expression analysis 
recent progress in salient object detection have exploited the boundary prior or background information to assist other saliency cue such a contrast achieving state of the art result however their usage of boundary prior is very simple fragile and the integration with other cue is mostly heuristic in this work we present new method to address these issue first we propose a robust background measure called boundary connectivity it characterizes the spatial layout of image region with respect to image boundary and is much more robust it ha an intuitive geometrical interpretation and present unique benefit that are absent in previous saliency measure second we propose a principled optimization framework to integrate multiple low level cue including our background measure to obtain clean and uniform saliency map our formulation is intuitive efficient and achieves state of the art result on several benchmark datasets 
we propose a robust and accurate method to extract the centerline and scale of tubular structure in d image and d volume existing technique rely either on filter designed to respond to ideal cylindrical structure which lose accuracy when the linear structure become very irregular or on classification which is inaccurate because location on centerline and location immediately next to them are extremely difficult to distinguish we solve this problem by reformulating centerline detection in term of a regression problem we first train regressors to return the distance to the closest centerline in scale space and we apply them to the input image or volume the centerline and the corresponding scale then correspond to the regressors local maximum which can be easily identified we show that our method outperforms state of the art technique for various d and d datasets 
in this paper we study the problem of estimating relative pose between two camera in the presence of radial distortion specifically we consider minimal problem where one of the camera ha no or known radial distortion there are three useful case for this setup with a single unknown distortion i fundamental matrix estimation where the two camera are uncalibrated ii essential matrix estimation for a partially calibrated camera pair iii essential matrix estimation for one calibrated camera and one camera with unknown focal length we study the parameterization of these three problem and derive fast polynomial solver based on gr bner basis method we demonstrate the numerical stability of the solver on synthetic data the minimal solver have also been applied to real imagery with convincing result 
most of the previous work on video action recognition use complex hand designed local feature such a sift hog and surf but these approach are implemented sophisticatedly and difficult to be extended to other sensor modality recent study discover that there are no universally best hand engineered feature for all datasets and learning feature directly from the data may be more advantageous one such endeavor is slow feature analysis sfa proposed by wiskott and sejnowski sfa can learn the invariant and slowly varying feature from input signal and ha been proved to be valuable in human action recognition it is also observed that the multi layer feature representation ha succeeded remarkably in widespread machine learning application in this paper we propose to combine sfa with deep learning technique to learn hierarchical representation from the video data itself specifically we use a two layered sfa learning structure with d convolution and max pooling operation to scale up the method to large input and capture abstract and structural feature from the video thus the proposed method is suitable for action recognition at the same time sharing the same merit of deep learning the proposed method is generic and fully automated our classification result on hollywood kth and ucf sport are competitive with previously published result to highlight some on the kth dataset our recognition rate show approximately improvement in comparison to state of the art method even without supervision or dense sampling 
ubiquitous image blur brings out a practically important question what are effective feature to differentiate between blurred and unblurred image region we address it by studying a few blur feature representation in image gradient fourier domain and data driven local filter unlike previous method which are often based on restoration mechanism our feature are constructed to enhance discriminative power and are adaptive to various blur scale in image to avail evaluation we build a new blur perception dataset containing thousand of image with labeled ground truth our result are applied to several application including blur region segmentation deblurring and blur magnification 
we consider the problem of deliberately manipulating the direct and indirect light flowing through a time varying fully general scene in order to simplify it visual analysis our approach rest on a crucial link between stereo geometry and light transport while direct light always obeys the epipolar geometry of a projector camera pair indirect light overwhelmingly doe not we show that it is possible to turn this observation into an imaging method that analyzes light transport in real time in the optical domain prior to acquisition this yield three key ability that we demonstrate in an experimental camera prototype producing a live indirect only video stream for any scene regardless of geometric or photometric complexity capturing image that make existing structured light shape recovery algorithm robust to indirect transport and turning them into one shot method for dynamic d shape capture 
in modern face recognition the conventional pipeline consists of four stage detect align represent classify we revisit both the alignment step and the representation step by employing explicit d face modeling in order to apply a piecewise affine transformation and derive a face representation from a nine layer deep neural network this deep network involves more than million parameter using several locally connected layer without weight sharing rather than the standard convolutional layer thus we trained it on the largest facial dataset to date an identity labeled dataset of four million facial image belonging to more than identity the learned representation coupling the accurate model based alignment with the large facial database generalize remarkably well to face in unconstrained environment even with a simple classifier our method reach an accuracy of on the labeled face in the wild lfw dataset reducing the error of the current state of the art by more than closely approaching human level performance 
the objective of this study is to reconstruct image from bag of visual word bovw which is the de facto standard feature for image retrieval and recognition bovw is defined here a a histogram of quantized descriptor extracted densely on a regular grid at a single scale despite it wide use no report describes reconstruction of the original image of a bovw this task is challenging for two reason bovw includes quantization error when local descriptor are assigned to visual word bovw lack spatial information of local descriptor when we count the occurrence of visual word to tackle this difficult task we use a large scale image database to estimate the spatial arrangement of local descriptor then this task creates a jigsaw puzzle problem with adjacency and global location cost of visual word solving this optimization problem is also challenging because it is known a an np hard problem we propose a heuristic but efficient method to optimize it to underscore the effectiveness of our method we apply it to bovws extracted from about different category and demonstrate that it can reconstruct the original image although the image feature lack spatial information and include quantization error 
recently the emergence of kinect system ha demonstrated the benefit of predicting an intermediate body part labeling for d human pose estimation in conjunction with rgb d imagery the availability of depth information play a critical role so an important question is whether a similar representation can be developed with sufficient robustness in order to estimate d pose from rgb image this paper provides evidence for a positive answer by leveraging a d human body part labeling in image b second order label sensitive pooling over dynamically computed region resulting from a hierarchical decomposition of the body and c iterative structured output modeling to contextualize the process based on d pose estimate for robustness and generalization we take advantage of a recent large scale d human motion capture dataset human m that also ha human body part labeling annotation available with image we provide extensive experimental study where alternative intermediate representation are compared and report a substantial error reduction over competitive discriminative baseline that regress d human pose against global hog feature 
many prevalent multi class classification approach can be unified and generalized by the output coding framework which usually consists of three phase coding learning binary classifier and decoding most of these approach focus on the first two phase and predefined distance function is used for decoding in this paper however we propose to perform learning in coding space for more adaptive decoding thereby improving overall performance ramp loss is exploited for measuring multi class decoding error the proposed algorithm ha uniform stability it is insensitive to data noise and scalable with large scale datasets generalization error bound and numerical result are given with promising outcome 
we argue that object subcategories follow a long tail distribution a few subcategories are common while many are rare we describe distributed algorithm for learning largemixture model that capture long tail distribution which are hard to model with current approach we introduce a generalized notion of mixture or subcategories that allow for example to be shared across multiple subcategories we optimize our model with a discriminative clustering algorithm that search over mixture in a distributed brute force fashion we used our scalable system to train ten of thousand of deformable mixture for voc object we demonstrate significant performance improvement particularly for object class that are characterized by large appearance variation 
a a convex relaxation of the low rank matrix factorization problem the nuclear norm minimization ha been attracting significant research interest in recent year the standard nuclear norm minimization regularizes each singular value equally to pursue the convexity of the objective function however this greatly restricts it capability and flexibility in dealing with many practical problem e g denoising where the singular value have clear physical meaning and should be treated differently in this paper we study the weighted nuclear norm minimization wnnm problem where the singular value are assigned different weight the solution of the wnnm problem are analyzed under different weighting condition we then apply the proposed wnnm algorithm to image denoising by exploiting the image nonlocal self similarity experimental result clearly show that the proposed wnnm algorithm outperforms many state of the art denoising algorithm such a bm d in term of both quantitative measure and visual perception quality 
most motion estimation algorithm optical flow layered model cannot handle large amount of occlusion in textureless region a motion is often initialized with no occlusion assumption despite that occlusion may be included in the final objective to handle such situation we propose a local layering model where motion and occlusion relationship are inferred jointly in particular the uncertainty of occlusion relationship are retained so that motion is inferred by considering all the possibility of local occlusion relationship in addition the local layering model handle articulated object with self occlusion we demonstrate that the local layering model can handle motion and occlusion well for both challenging synthetic and real sequence 
it is useful to automatically compare image based on their visual property to predict which image is brighter more feminine more blurry etc however comparative model are inherently more costly to train than their classification counterpart manually labeling all pairwise comparison is intractable so which pair should a human supervisor compare we explore active learning strategy for training relative attribute ranking function with the goal of requesting human comparison only where they are most informative we introduce a novel criterion that request a partial ordering for a set of example that minimizes the total rank margin in attribute space subject to a visual diversity constraint the setwise criterion help amortize effort by identifying mutually informative comparison and the diversity requirement safeguard against request a human viewer will find ambiguous we develop an efficient strategy to search for set that meet this criterion on three challenging datasets and experiment with live online annotator the proposed method outperforms both traditional passive learning a well a existing active rank learning method 
visual tracking is a challenging problem in computer vision most state of the art visual tracker either rely on luminance information or use simple color representation for image description contrary to visual tracking for object recognition and detection sophisticated color feature when combined with luminance have shown to provide excellent performance due to the complexity of the tracking problem the desired color feature should be computationally efficient and posse a certain amount of photometric invariance while maintaining high discriminative power this paper investigates the contribution of color in a tracking by detection framework our result suggest that color attribute provides superior performance for visual tracking we further propose an adaptive low dimensional variant of color attribute both quantitative and attribute based evaluation are performed on challenging benchmark color sequence the proposed approach improves the baseline intensity based tracker by in median distance precision furthermore we show that our approach outperforms state of the art tracking method while running at more than frame per second 
in this paper we propose a new methodology for segmenting non rigid visual object where the search procedure is onducted directly on a sparse low dimensional manifold guided by the classification result computed from a deep belief network our main contribution is the fact that we do not rely on the typical sub division of segmentation task into rigid detection and non rigid delineation instead the non rigid segmentation is performed directly where point in the sparse low dimensional can be mapped to an explicit contour representation in image space our proposal show significantly smaller search and training complexity given that the dimensionality of the manifold is much smaller than the dimensionality of the search space for rigid detection and non rigid delineation aforementioned and that we no longer require a two stage segmentation process we focus on the problem of left ventricle endocardial segmentation from ultrasound image and lip segmentation from frontal facial image using the extended cohn kanade ck database our experiment show that the use of sparse low dimensional manifold reduces the search and training complexity of current segmentation approach without a significant impact on the segmentation accuracy shown by state of the art approach 
hashing is very useful for fast approximate similarity search on large database in the unsupervised setting most hashing method aim at preserving the similarity defined by euclidean distance hash code generated by these approach only keep their hamming distance corresponding to the pairwise euclidean distance ignoring the local distribution of each data point this objective doe not hold for k nearest neighbor search in this paper we firstly propose a new adaptive similarity measure which is consistent with k nn search and prove that it lead to a valid kernel then we propose a hashing scheme which us binary code to preserve the kernel function using low rank approximation our hashing framework is more effective than existing method that preserve similarity over arbitrary kernel the proposed kernel function hashing framework and their combination have demonstrated significant advantage compared with several state of the art method 
psychophysical study show motion cue inform about shape even with unknown reflectance recent work in computer vision have considered shape recovery for an object of unknown brdf using light source or object motion this paper address the remaining problem of determining shape from the small or differential motion of the camera for unknown isotropic brdfs our theory derives a differential stereo relation that relates camera motion to depth of a surface with unknown isotropic brdf which generalizes traditional lambertian assumption under orthographic projection we show shape may not be constrained in general but two motion suffice to yield an invariant for several restricted still unknown brdfs exhibited by common material for the perspective case we show that three differential motion suffice to yield surface depth for unknown isotropic brdf and unknown directional lighting while additional constraint are obtained with restriction on brdf or lighting the limit imposed by our theory are intrinsic to the shape recovery problem and independent of choice of reconstruction method we outline with experiment how potential reconstruction method may exploit our theory we illustrate trend shared by theory on shape from motion of light object or camera relating reconstruction hardness to imaging complexity 
subspace clustering is a powerful technology for clustering data according to the underlying subspace representation based method are the most popular subspace clustering approach in recent year in this paper we analyze the grouping effect of representation based method in depth in particular we introduce the enforced grouping effect condition which greatly facilitate the analysis of grouping effect we further find that grouping effect is important for subspace clustering which should be explicitly enforced in the data self representation model rather than implicitly implied by the model a in some prior work based on our analysis we propose the smooth representation smr model we also propose a new affinity measure based on the grouping effect which prof to be much more effective than the commonly used one a a result our smr significantly outperforms the state of the art one on benchmark datasets 
given a static scene a human can trivially enumerate the myriad of thing that can happen next and characterize the relative likelihood of each in the process we make use of enormous amount of commonsense knowledge about how the world work in this paper we investigate learning this commonsense knowledge from data to overcome a lack of densely annotated spatiotemporal data we learn from sequence of abstract image gathered using crowdsourcing the abstract scene provide both object location and attribute information we demonstrate qualitatively and quantitatively that our model produce plausible scene prediction on both the abstract image a well a natural image taken from the internet 
low rank matrix recovery from a corrupted observation ha many application in computer vision conventional method address this problem by iterating between nuclear norm minimization and sparsity minimization however iterative nuclear norm minimization is computationally prohibitive for large scale data e g video analysis in this paper we propose a robust orthogonal subspace learning rosl method to achieve efficient low rank recovery our intuition is a novel rank measure on the low rank matrix that imposes the group sparsity of it coefficient under orthonormal subspace we present an efficient sparse coding algorithm to minimize this rank measure and recover the low rank matrix at quadratic complexity of the matrix size we give theoretical proof to validate that this rank measure is lower bounded by nuclear norm and it ha the same global minimum a the latter to further accelerate rosl to linear complexity we also describe a faster version rosl empowered by random sampling our extensive experiment demonstrate that both rosl and rosl provide superior efficiency against the state of the art method at the same level of recovery accuracy 
curse of dimensionality is a practical and challenging problem in image categorization especially in case with a large number of class multi class classification encounter severe computational and storage problem when dealing with these large scale task in this paper we propose hierarchical feature hashing to effectively reduce dimensionality of parameter space without sacrificing classification accuracy and at the same time exploit information in semantic taxonomy among category we provide detailed theoretical analysis on our proposed hashing method moreover experimental result on object recognition and scene classification further demonstrate the effectiveness of hierarchical feature hashing 
joint segmentation of image set is a challenging problem especially when there are multiple object with variable appearance shared among the image in the collection and the set of object present in each particular image is itself varying and unknown in this paper we present a novel method to jointly segment a set of image containing object from multiple class we first establish consistent functional map across the input image and introduce a formulation that explicitly model partial similarity across image instead of global consistency given the optimized map between pair of image multiple group of consistent segmentation function are found such that they align with segmentation cue in the image agree with the functional map and are mutually exclusive the proposed fully unsupervised approach exhibit a significant improvement over the state of the art method a shown on the co segmentation data set msrc flickr and pascal 
a k poselet is a deformable part model dpm with k part where each of the part is a poselet aligned to a specific configuration of keypoints based on ground truth annotation a separate template is used to learn the appearance of each part the part are allowed to move with respect to each other with a deformation cost that is learned at training time this model is richer than both the traditional version of poselets and dpms it enables a unified approach to person detection and keypoint prediction which barring contemporaneous approach based on cnn feature achieves state of the art keypoint prediction while maintaining competitive detection performance 
we describe an information driven active selection approach to determine which detector to deploy at which location in which frame of a video to minimize semantic class label uncertainty at every pixel with the smallest computational cost that ensures a given uncertainty bound we show minimal performance reduction compared to a paragon algorithm running all detector at all location in all frame at a small fraction of the computational cost our method can handle uncertainty in the labeling mechanism so it can handle both oracle manual annotation or noisy detector automated annotation 
the subspace segmentation problem is addressed in this paper by effectively constructing an exactly block diagonal sample affinity matrix the block diagonal structure is heavily desired for accurate sample clustering but is rather difficult to obtain most current state of the art subspace segmentation method such a ssc and lrr resort to alternative structural prior such a sparseness and low rankness to construct the affinity matrix in this work we directly pursue the block diagonal structure by proposing a graph laplacian constraint based formulation and then develop an efficient stochastic subgradient algorithm for optimization moreover two new subspace segmentation method the block diagonal ssc and lrr are devised in this work to the best of our knowledge this is the first research attempt to explicitly pursue such a block diagonal structure extensive experiment on face clustering motion segmentation and graph construction for semi supervised learning clearly demonstrate the superiority of our novelly proposed subspace segmentation method 
we present the first automatic method to remove shadow from single rgb d image using normal cue directly derived from depth we can remove hard and soft shadow while preserving surface texture and shading our key assumption is pixel with similar normal spatial location and chromaticity should have similar color a modified nonlocal matching is used to compute a shadow confidence map that localizes well hard shadow boundary thus handling hard and soft shadow within the same framework we compare our result produced using state of the art shadow removal on single rgb image and intrinsic image decomposition on standard rgb d datasets 
relative attribute learning aim to learn ranking function describing the relative strength of attribute most of current learning approach learn ranking function for each attribute independently without considering possible intrinsic relatedness among the attribute for a problem involving multiple attribute it is reasonable to assume that utilizing such relatedness among the attribute would benefit learning especially when the number of labeled training pair are very limited in this paper we proposed a relative multi attribute learning framework that integrates relative attribute into a multi task learning scheme the formulation allows u to exploit the advantage of the state of the art regularization based multi task learning for improved attribute learning in particular using joint feature learning a the case study we evaluated our framework with both synthetic data and two real datasets experimental result suggest that the proposed framework ha clear performance gain in ranking accuracy and zero shot learning accuracy over existing method of independent relative attribute learning and multi task learning 
we propose ordered subspace clustering osc to segment data drawn from a sequentially ordered union of subspace current subspace clustering technique learn the relationship within a set of data and then use a separate clustering algorithm such a ncut for final segmentation in contrast our technique under certain condition is capable of segmenting cluster intrinsically without providing the number of cluster a a parameter similar to sparse subspace clustering ssc we formulate the problem a one of finding a sparse representation but include a new penalty term to take care of sequential data we test our method on data drawn from infrared hyper spectral data video sequence and face image our experiment show that our method osc outperforms the state of the art method spatial subspace clustering spatsc low rank representation lrr and ssc 
recently there ha been a surge of interest to use branch and bound bnb optimisation for d point cloud registration while bnb guarantee globally optimal solution it is usually too slow to be practical a fundamental source of difficulty is the search for the rotation parameter in the d rigid transform in this work assuming that the translation parameter are known we focus on constructing a fast rotation search algorithm with respect to an inherently robust geometric matching criterion we propose a novel bounding function for bnb that allows rapid evaluation underpinning our bounding function is the usage of stereographic projection to precompute and spatially index all possible point match this yield a robust and global algorithm that is significantly faster than previous method to conduct full d registration the translation can be supplied by d feature matching or by another optimisation framework that provides the translation on various challenging point cloud including those taken out of lab setting our approach demonstrates superior efficiency 
the initial step of many computer vision algorithm are interest point extraction and matching in larger image set the pairwise matching of interest point descriptor between image is an important bottleneck for each descriptor in one image the approximate nearest neighbor in the other one ha to be found and checked against the second nearest neighbor to ensure the correspondence is unambiguous here we asked the question how to best decimate the list of interest point without losing match i e we aim to speed up matching by filtering out in advance those point which would not survive the matching stage it turn out that the best filtering criterion is not the response of the interest point detector which in fact is not surprising the goal of detection are repeatable and well localized point whereas the objective of the selection are point whose descriptor can be matched successfully we show that one can in fact learn to predict which descriptor are matchable and thus reduce the number of interest point significantly without losing too many match we show that this strategy a simple a it is greatly improves the matching success with the same number of point per image moreover we embed the prediction in a state of the art structure from motion pipeline and demonstrate that it also outperforms other selection method at system level 
in this paper we address the problem of jointly summarizing large set of flickr image and youtube video starting from the intuition that the characteristic of the two medium type are different yet complementary we develop a fast and easily parallelizable approach for creating not only high quality video summary but also novel structural summary of online image a storyline graph the storyline graph can illustrate various event or activity associated with the topic in a form of a branching network the video summarization is achieved by diversity ranking on the similarity graph between image and video frame the reconstruction of storyline graph is formulated a the inference of sparse time varying directed graph from a set of photo stream with assistance of video for evaluation we collect the datasets of outdoor activity consisting of m flickr image and k youtube video due to the large scale nature of our problem we evaluate our algorithm via crowdsourcing using amazon mechanical turk in our experiment we demonstrate that the proposed joint summarization approach outperforms other baseline and our own method using video or image only 
we introduce an asymmetric sparse approximate embedding optimized for fast kernel comparison operation arising in large scale visual search in contrast to other method that perform an explicit approximate embedding using kernel pca followed by a distance compression technique in ropf d which loses information at both step our method utilizes the implicit kernel representation directly in addition we empirically demonstrate that our method need no explicit training step and can operate with a dictionary of random exemplar from the dataset we evaluate our method on three benchmark image retrieval datasets sift m imagenet and m tinyimages 
in this paper higher order correlation clustering hocc is used for text line detection in natural image we treat text line detection a a graph partitioning problem where each vertex is represented by a maximally stable extremal region mser first weak hypothesis are proposed by coarsely grouping msers based on their spatial alignment and appearance consistency then higher order correlation clustering hocc is used to partition the msers into text line candidate using the hypothesis a soft constraint to enforce long range interaction we further propose a regularization method to solve the semidefinite programming problem in the inference finally we use a simple texton based texture classifier to filter out the non text area this framework allows u to naturally handle multiple orientation language and font experiment show that our approach achieves competitive performance compared to the state of the art 
we consider the design of a single vector representation for an image that embeds and aggregate a set of local patch descriptor such a sift more specifically we aim to construct a dense representation like the fisher vector or vlad though of small or intermediate size we make two contribution both aimed at regularizing the individual contribution of the local descriptor in the final representation the first is a novel embedding method that avoids the dependency on absolute distance by encoding direction the second contribution is a democratization strategy that further limit the interaction of unrelated descriptor in the aggregation stage these method are complementary and give a substantial performance boost over the state of the art in image search with short or mid size vector a demonstrated by our experiment on standard public image retrieval benchmark 
we present a new globally optimal algorithm for self calibrating a moving camera with constant parameter our method aim at estimating the dual absolute quadric daq under the rank and optionally camera center chirality constraint we employ the branch and prune paradigm and explore the space of only parameter pruning in our method relies on solving linear matrix inequality lmi feasibility and generalized eigenvalue gev problem that solely depend upon the entry of the daq these lmi and gev problem are used to rule out branch in the search tree in which a quadric not satisfying the rank and chirality condition on camera center is guaranteed not to exist the chirality lmi condition are obtained by relying on the mild assumption that the camera undergoes a rotation of no more than between consecutive view furthermore our method doe not rely on calculating bound on any particular cost function and hence can virtually optimize any objective while achieving global optimality in a very competitive running time 
computer vision algorithm make mistake in human centric application some mistake are more annoying to user than others in order to design algorithm that minimize the annoyance to user we need access to an annoyance or cost matrix that hold the annoyance of each type of mistake such matrix are not readily available especially for a wide gamut of human centric application where annoyance is tied closely to human perception to avoid having to conduct extensive user study to gather the annoyance matrix for all possible mistake we propose predicting the annoyance of previously unseen mistake by learning from example mistake and their corresponding annoyance we promote the use of attribute based representation to transfer this knowledge of annoyance our experimental result with face and scene demonstrate that our approach can predict annoyance more accurately than baseline we show that a a result our approach make le annoying mistake in a real world image retrieval application 
the essential matrix which encodes the epipolar constraint between point in two projective view is a cornerstone of modern computer vision previous work have proposed different characterization of the space of essential matrix a a riemannian manifold however they either do not consider the symmetric role played by the two view or do not fully take into account the geometric peculiarity of the epipolar constraint we address these limitation with a characterization a a quotient manifold which can be easily interpreted in term of camera pose while our main focus in on theoretical aspect we include experiment in pose averaging and show that the proposed formulation produce a meaningful distance between essential matrix 
example based texture synthesis ets ha been widely used to generate high quality texture of desired size from a small example however not all texture are equally well reproducible that way we predict how synthesizable a particular texture is by ets we introduce a dataset texture of which all image have been annotated in term of their synthesizability we design a set of texture feature such a textureness homogeneity repetitiveness and irregularity and train a predictor using these feature on the data collection this work is the first attempt to quantify this image property and we find that texture synthesizability can be learned and predicted we use this insight to trim image to part that are more synthesizable also we suggest which texture synthesis method is best suited to synthesise a given texture our approach can be seen a winner us all picking one method among several alternative ending up with an overall superior ets method such strategy could also be considered for other vision task rather than building an even stronger method choose from existing method based on some simple preprocessing 
we propose a very intuitive and simple approximation for the conventional spectral clustering method it effectively alleviates the computational burden of spectral clustering reducing the time complexity from o n to o n while capable of gaining better performance in our experiment specifically by involving a more realistic and effective distance and the k mean duality property our algorithm can handle datasets with complex cluster shape multi scale cluster and noise we also show it superiority in a series of it real application on task including digit clustering a well a image segmentation 
kurtosis of d projection provides important statistical characteristic of natural image in this work we first provide a theoretical underpinning to a recently observed phenomenon known a projection kurtosis concentration that the kurtosis of natural image over different band pas channel tend to concentrate around a typical value based on this analysis we further describe a new method to estimate the covariance matrix of correlated gaussian noise from a noise corrupted image using random band pas filter we demonstrate the effectiveness of our blind noise covariance matrix estimation method on natural image 
we present an accurate and efficient stereo matching method using locally shared label a new labeling scheme that enables spatial propagation in mrf inference using graph cut they give each pixel and region a set of candidate disparity label which are randomly initialized spatially propagated and refined for continuous disparity estimation we cast the selection and propagation of locallydefined disparity label a fusion based energy minimization the joint use of graph cut and locally shared label ha advantage over previous approach based on fusion move or belief propagation it produce submodular move deriving a subproblem optimality enables powerful randomized search help to find good smooth locally planar disparity map which are reasonable for natural scene allows parallel computation of both unary and pairwise cost our method is evaluated using the middlebury stereo benchmark and achieves first place in sub pixel accuracy 
attribute are widely used a mid level descriptor of object property in object recognition and retrieval mostly such attribute are manually pre defined based on domain knowledge and their number is fixed however pre defined attribute may fail to adapt to the property of the data at hand may not necessarily be discriminative and or may not generalize well in this work we propose a dictionary learning framework that flexibly adapts to the complexity of the given data set and reliably discovers the inherent discriminative middle level binary feature in the data we use sample relatedness information to improve the generalization of the learned dictionary to steer the structure of the attribute representation for discrimination and generalization we demonstrate that our framework is applicable to both object recognition and complex image retrieval task even with few training example moreover the learned dictionary also help classify novel object category experimental result on the animal with attribute ilsvrc and pascal voc datasets indicate that using relatedness information lead to significant performance gain over established baseline 
human are capable of perceiving a scene at a glance and obtain deeper understanding with additional time similarly visual recognition deployment should be robust to varying computational budget such situation require anytime recognition ability which is rarely considered in computer vision research we present a method for learning dynamic policy to optimize anytime performance in visual architecture our model sequentially order feature computation and performs subsequent classification crucially decision are made at test time and depend on observed data and intermediate result we show the applicability of this system to standard problem in scene and object recognition on suitable datasets we can incorporate a semantic back off strategy that give maximally specific prediction for a desired level of accuracy this provides a new view on the time course of human visual perception 
in this paper we investigate an approach for reconstructing storyline graph from large scale collection of internet image and optionally other side information such a friendship graph the storyline graph can be an effective summary that visualizes various branching narrative structure of event or activity recurring across the input photo set of a topic class in order to explore further the usefulness of the storyline graph we leverage them to perform the image sequential prediction task from which photo recommendation application can benefit we formulate the storyline reconstruction problem a an inference of sparse time varying directed graph and develop an optimization algorithm that successfully address a number of key challenge of web scale problem including global optimality linear complexity and easy parallelization with experiment on more than million of image of class and user study via amazon mechanical turk we show that the proposed algorithm improves other candidate method for both storyline reconstruction and image prediction task 
while approach based on bag of feature excel at low level action classification they are ill suited for recognizing complex event in video where concept based temporal representation currently dominate this paper proposes a novel representation that capture the temporal dynamic of windowed mid level concept detector in order to improve complex event recognition we first express each video a an ordered vector time series where each time step consists of the vector formed from the concatenated confidence of the pre trained concept detector we hypothesize that the dynamic of time series for different instance from the same event class a captured by simple linear dynamical system lds model are likely to be similar even if the instance differ in term of low level visual feature we propose a two part representation composed of fusing a singular value decomposition of block hankel matrix ssid s and a harmonic signature h computed from the corresponding eigen dynamic matrix the proposed method offer several benefit over alternate approach our approach is straightforward to implement directly employ existing concept detector and can be plugged into linear classification framework result on standard datasets such a nist s trecvid multimedia event detection task demonstrate the improved accuracy of the proposed method 
this paper present a highly efficient very accurate regression approach for face alignment our approach ha two novel component a set of local binary feature and a locality principle for learning those feature the locality principle guide u to learn a set of highly discriminative local binary feature for each facial landmark independently the obtained local binary feature are used to jointly learn a linear regression for the final output our approach achieves the state of the art result when tested on the current most challenging benchmark furthermore because extracting and regressing local binary feature is computationally very cheap our system is much faster than previous method it achieves over fps on a desktop or fps on a mobile phone for locating a few dozen of landmark 
existing method to learn visual attribute are prone to learning the wrong thing namely property that are correlated with the attribute of interest among training sample yet many proposed application of attribute rely on being able to learn the correct semantic concept corresponding to each attribute we propose to resolve such confusion by jointly learning decorrelated discriminative attribute model leveraging side information about semantic relatedness we develop a multi task learning approach that us structured sparsity to encourage feature competition among unrelated attribute and feature sharing among related attribute on three challenging datasets we show that accounting for structure in the visual attribute space is key to learning attribute model that preserve semantics yielding improved generalizability that help in the recognition and discovery of unseen object category 
global bundle adjustment usually converges to a non zero residual and produce sub optimal camera pose for local area which lead to loss of detail for highresolution reconstruction instead of trying harder to optimize everything globally we argue that we should live with the non zero residual and adapt the camera pose to local area to this end we propose a segment based approach to readjust the camera pose locally and improve the reconstruction for fine geometry detail the key idea is to partition the globally optimized structure from motion point into well conditioned segment for re optimization reconstruct their geometry individually and fuse everything back into a consistent global model this significantly reduces severe propagated error and estimation bias caused by the initial global adjustment the result on several datasets demonstrate that this approach can significantly improve the reconstruction accuracy while maintaining the consistency of the d structure between segment 
the prevalent approach to image based localization is matching interest point detected in the query image to a sparse d point cloud representing the known world the obtained correspondence are then used to recover a precise camera pose the state of the art in this field often ignores the availability of a set of d descriptor per d point for example by representing each d point by only it centroid in this paper we demonstrate that these set contain useful information that can be exploited by formulating matching a a discriminative classification problem since memory demand and computational complexity are crucial in such a setup we base our algorithm on the efficient and effective random fern principle we propose an extension which project feature to fern specific embedding space which yield improved matching rate in short runtime experiment first show that our novel formulation provides improved matching performance in comparison to the standard nearest neighbor approach and that we outperform related randomization method in our localization scenario 
this paper present a photometric stereo method that is purely pixelwise and handle general isotropic surface in a stable manner following the recently proposed sum of lobe representation of the isotropic reflectance function we constructed a constrained bivariate regression problem where the regression function is approximated by smooth bivariate bernstein polynomial the unknown normal vector wa separated from the unknown reflectance function by considering the inverse representation of the image formation process and then we could accurately compute the unknown surface normal by solving a simple and efficient quadratic programming problem extensive evaluation that showed the state of the art performance using both synthetic and real world image were performed 
we pose unseen view synthesis a a probabilistic tensor completion problem given image of people organized by their rough viewpoint we form a d appearance tensor indexed by image pose example viewpoint and image position after discovering the low dimensional latent factor that approximate that tensor we can impute it missing entry in this way we generate novel synthetic view of people even when they are observed from just one camera viewpoint we show that the inferred view are both visually and quantitatively accurate furthermore we demonstrate their value for recognizing action in unseen view and estimating viewpoint in novel image while existing method are often forced to choose between data that is either realistic or multi view our virtual view offer both thereby allowing greater robustness to viewpoint in novel image 
this paper describes a method of gait recognition from image sequence wherein a subject is accelerating or decelerating a a speed change occurs due to a change of pitch the first order derivative of a phase namely a gait stance and or stride we model this speed change using a cylindrical manifold whose azimuth and height corresponds to the phase and the stride respectively a radial basis function rbf interpolation framework is used to learn subject specific mapping matrix for mapping from manifold to image space given an input image sequence of speed transited gait of a test subject we estimate the mapping matrix of the test subject a well a the phase and stride sequence using an energy minimization framework considering the following three point fitness of the synthesized image to the input image sequence a well a to an eigenspace constructed by exemplar of training subject smoothness of the phase and the stride sequence and pitch and stride fitness to the pitch stride preference model using the estimated mapping matrix we synthesize a constant speed gait image sequence and extract a conventional period based gait feature from it for matching we conducted experiment using real speed transited gait image sequence with subject and demonstrated the effectiveness of the proposed method 
we propose a sequential optimization technique for segmenting a rectified image of a facade into semantic category our method retrieves a parsing which respect common architectural constraint and also return a certificate for global optimality contrasting the suggested method the considered facade labeling problem is typically tackled a a classification task or a grammar parsing both approach are not capable of fully exploiting the regularity of the problem therefore our technique very significantly improves the accuracy compared to the state of the art while being an order of magnitude faster in addition in of the test image we obtain a certificate for optimality 
establishing dense correspondence reliably between a pair of image is an important vision task with many application though significant advance ha been made towards estimating dense stereo and optical flow field for two image adjacent in viewpoint or in time building reliable dense correspondence field for two general image still remains largely unsolved for instance two given image sharing some content exhibit dramatic photometric and geometric variation or they depict different d scene of similar scene characteristic fundamental challenge to such an image or scene alignment task are often multifold which render many existing technique fall short of producing dense correspondence robustly and efficiently this paper present a novel approach called daisy filter flow dff to address this challenging task inspired by the recent patchmatch filter technique we leverage and extend a few established method daisy descriptor filter based efficient flow inference and the patchmatch fast search coupling and optimizing these module seamlessly with image segment a the bridge the proposed dff approach enables efficiently performing dense descriptor based correspondence field estimation in a generalized high dimensional label space which is augmented by scale and rotation experiment on a variety of challenging scene show that our dff approach estimate spatially coherent yet discontinuity preserving image alignment result both robustly and efficiently 
we introduce a method that can register challenging image from specular and poorly textured d environment on which previous approach fail we assume that a small set of reference image of the environment and a partial d model are available like previous approach we register the input image by aligning them with one of the reference image using the d information however these approach typically rely on the pixel intensity for the alignment which is prone to fail in presence of specularities or in absence of texture our main contribution is an efficient novel local descriptor that we use to describe each image location we show that we can rely on this descriptor in place of the intensity to significantly improve the alignment robustness at a minor increase of the computational cost and we analyze the reason behind the success of our descriptor 
robust multi object tracking by detection requires the correct assignment of noisy detection result to object trajectory we address this problem by proposing an online approach based on the observation that object detector primarily fail if object are significantly occluded in contrast to most existing work we only rely on geometric information to efficiently overcome detection failure in particular we exploit the spatio temporal evolution of occlusion region detector reliability and target motion prediction to robustly handle missed detection in combination with a conservative association scheme for visible object this allows for real time tracking of multiple object from a single static camera even in complex scenario our evaluation on publicly available multi object tracking benchmark datasets demonstrate favorable performance compared to the state of the art in online and offline multi object tracking 
recent year have seen a major push for face recognition technology due to the large expansion of image sharing on social network in this paper we consider the difficult task of determining parent offspring resemblance using deep learning to answer the question who do i look like although human can perform this job at a rate higher than chance it is not clear how they do it however recent study in anthropology have determined which feature tend to be the most discriminative in this study we aim to not only create an accurate system for resemblance detection but bridge the gap between study in anthropology with computer vision technique further we aim to answer two key question do offspring resemble their parent and do offspring resemble one parent more than the other we propose an algorithm that fuse the feature and metric discovered via gated autoencoders with a discriminative neural network layer that learns the optimal or what we call genetic feature to delineate parent offspring relationship we further analyze the correlation between our automatically detected feature and those found in anthropological study meanwhile our method outperforms the state of the art in kinship verification by depending on the relationship using specific father son mother daughter etc and generic model 
we present max margin boltzmann machine mmbms for object segmentation mmbms are essentially a class of conditional boltzmann machine that model the joint distribution of hidden variable and output label conditioned on input observation in addition to image to label connection we build direct image to hidden connection to facilitate global shape prediction and thus derive a simple iterated conditional mode algorithm for efficient maximum a posteriori inference we formulate a max margin objective function for discriminative training and analyze the effect of different margin function on learning we evaluate mmbms using three datasets against state of the art method to demonstrate the strength of the proposed algorithm 
face detection and facial point localization are interconnected task recently it ha been shown that solving these two task jointly with a mixture of tree of part mtp lead to state of the art result however mtp a most other method for facial point localization proposed so far requires a complete annotation of the training data at facial point level this is used to predefine the structure of the tree and to place the part correctly in this work we extend the mixture from tree to more general loopy graph in this way we can learn in a weakly supervised manner using only the face location and orientation a powerful deformable detector that implicitly aligns it part to the detected face in the image by attaching some reference point to the correct part of our detector we can then localize the facial point in term of detection our method clearly outperforms the state of the art even if competing with method that use facial point annotation during training additionally without any facial point annotation at the level of individual training image our method can localize facial point with an accuracy similar to fully supervised approach 
the state of the art in image segmentation build hierarchical segmentation structure based on analyzing local feature cue in spectral setting due to their impressive performance such segmentation approach have become building block in many computer vision application nevertheless the main bottleneck are still the computationally demanding process of local feature processing and spectral analysis in this paper we demonstrate that based on a discrete continuous optimization of oriented gradient signal we are able to provide segmentation performance competitive to state of the art on bsds even without any spectral analysis while reducing computation time by a factor of and memory demand by a factor of 
we address the problem of joint detection and segmentation of multiple object instance in an image a key step towards scene understanding inspired by data driven method we propose an exemplar based approach to the task of instance segmentation in which a set of reference image shape mask is used to find multiple object we design a novel crf framework that jointly model object appearance shape deformation and object occlusion to tackle the challenging map inference problem we derive an alternating procedure that interleaf object segmentation and shape appearance adaptation we evaluate our method on two datasets with instance label and show promising result 
in diffusion based saliency detection an image is partitioned into superpixels and mapped to a graph with superpixels a node and edge strength proportional to superpixel similarity saliency information is then propagated over the graph using a diffusion process whose equilibrium state yield the object saliency map the optimal solution is the product of a propagation matrix and a saliency seed vector that contains a prior saliency assessment this is obtained from either a bottom up saliency detector or some heuristic in this work we propose a method to learn optimal seed for object saliency two type of feature are computed per superpixel the bottom up saliency of the superpixel region and a set of mid level vision feature informative of how likely the superpixel is to belong to an object the combination of feature that best discriminates between object and background saliency is then learned using a large margin formulation of the discriminant saliency principle the propagation of the resulting saliency seed using a diffusion process is finally shown to outperform the state of the art on a number of salient object detection datasets 
this paper proposes a novel photometric stereo solution to jointly estimate surface normal and scattering parameter from a globally planar homogeneous translucent object similar to classic photometric stereo our method only requires a few a three observation of the translucent object under directional lighting naively applying classic photometric stereo result in blurred photometric normal we develop a novel blind deconvolution algorithm based on inverse rendering for recovering the sharp surface normal and the material property we demonstrate our method on a variety of translucent object 
learning a low dimensional representation of image is useful for various application in graphic and computer vision existing solution either require manually specified landmark for corresponding point in the image or are restricted to specific object or shape deformation this paper alleviates these limitation by imposing a specific model for generating image the nested composition of color shape and appearance we show that each component can be approximated by a low dimensional subspace when the others are factored out our formulation allows for efficient learning and experiment show encouraging result 
in this paper we tackle the problem of estimating the depth of a scene from a single image this is a challenging task since a single image on it own doe not provide any depth cue to address this we exploit the availability of a pool of image for which the depth is known more specifically we formulate monocular depth estimation a a discrete continuous optimization problem where the continuous variable encode the depth of the superpixels in the input image and the discrete one represent relationship between neighboring superpixels the solution to this discrete continuous optimization problem is then obtained by performing inference in a graphical model using particle belief propagation the unary potential in this graphical model are computed by making use of the image with known depth we demonstrate the effectiveness of our model in both the indoor and outdoor scenario our experimental evaluation show that our depth estimate are more accurate than existing method on standard datasets 
document image captured by a digital camera often suffer from serious geometric distortion in this paper we propose an active method to correct geometric distortion in a camera captured document image unlike many passive rectification method that rely on text line or feature extracted from image our method us two structured beam illuminating upon the document page to recover two spatial curve a developable surface is then interpolated to the curve by finding the correspondence between them the developable surface is finally flattened onto a plane by solving a system of ordinary differential equation our method is a content independent approach and can restore a corrected document image of high accuracy with undistorted content experimental result on a variety of real captured document image demonstrate the effectiveness and efficiency of the proposed method 
human gesture similar to speech and handwriting are often unique to the individual training a generic classifier applicable to everyone can be very difficult and a such it ha become a standard to use personalized classifier in speech and handwriting recognition in this paper we address the problem of personalization in the context of gesture recognition and propose a novel and extremely efficient way of doing personalization unlike conventional personalization method which learn a single classifier that later get adapted our approach learns a set portfolio of classifier during training one of which is selected for each test subject based on the personalization data we formulate classifier personalization a a selection problem and propose several algorithm to compute the set of candidate classifier our experiment show that such an approach is much more efficient than adapting the classifier parameter but can still achieve comparable or better result 
visual domain adaptation which learns an accurate classifier for a new domain using labeled image from an old domain ha shown promising value in computer vision yet still been a challenging problem most prior work have explored two learning strategy independently for domain adaptation feature matching and instance reweighting in this paper we show that both strategy are important and inevitable when the domain difference is substantially large we therefore put forward a novel transfer joint matching tjm approach to model them in a unified optimization problem specifically tjm aim to reduce the domain difference by jointly matching the feature and reweighting the instance across domain in a principled dimensionality reduction procedure and construct new feature representation that is invariant to both the distribution difference and the irrelevant instance comprehensive experimental result verify that tjm can significantly outperform competitive method for cross domain image recognition problem 
we present an efficient and scalable algorithm for segmenting d rgbd point cloud by combining depth color and temporal information using a multistage hierarchical graph based approach our algorithm process a moving window over several point cloud to group similar region over a graph resulting in an initial over segmentation these region are then merged to yield a dendrogram using agglomerative clustering via a minimum spanning tree algorithm bipartite graph matching at a given level of the hierarchical tree yield the final segmentation of the point cloud by maintaining region identity over arbitrarily long period of time we show that a multistage segmentation with depth then color yield better result than a linear combination of depth and color due to it incremental processing our algorithm can process video of any length and in a streaming pipeline the algorithm s ability to produce robust efficient segmentation is demonstrated with numerous experimental result on challenging sequence from our own a well a public rgbd data set 
we propose a real time robust to outlier and accurate solution to the perspective n point pnp problem the main advantage of our solution are twofold first it integrates the outlier rejection within the pose estimation pipeline with a negligible computational overhead and second it scalability to arbitrarily large number of correspondence given a set of d to d match we formulate pose estimation problem a a low rank homogeneous system where the solution lie on it d null space outlier correspondence are those row of the linear system which perturb the null space and are progressively detected by projecting them on an iteratively estimated solution of the null space since our outlier removal process is based on an algebraic criterion which doe not require computing the full pose and reprojecting back all d point on the image plane at each step we achieve speed gain of more than compared to ransac strategy an extensive experimental evaluation will show that our solution yield accurate result in situation with up to of outlier and can process more than correspondence in le than m 
we present a video co segmentation method that us category independent object proposal a it basic element and can extract multiple foreground object in a video set the use of object element overcomes limitation of low level feature representation in separating complex foreground and background we formulate object based co segmentation a a co selection graph in which region with foreground like characteristic are favored while also accounting for intra video and inter video foreground coherence to handle multiple foreground object we expand the co selection graph model into a proposed multi state selection graph model msg that optimizes the segmentation of different object jointly this extension into the msg can be applied not only to our co selection graph but also can be used to turn any standard graph model into a multi state selection solution that can be optimized directly by the existing energy minimization technique our experiment show that our object based multiple foreground video co segmentation method obmic compare well to related technique on both single and multiple foreground case 
photometric stereo offer the possibility of object shape reconstruction via reasoning about the amount of light reflected from oriented surface however in murky medium such a sea water the illuminating light interacts with the medium and some of it is backscattered towards the camera due to this additive light component the standard photometric stereo equation lead to poor quality shape estimation previous author have attempted to reformulate the approach but have either neglected backscatter entirely or disregarded it non uniformity on the sensor when camera and light are close to each other we show that by compensating effectively for the backscatter component a linear formulation of photometric stereo is allowed which recovers an accurate normal map using only light our backscatter compensation method for point source can be used for estimating the uneven backscatter directly from single image without any prior knowledge about the characteristic of the medium or the scene we compare our method with previous approach through extensive experimental result where a variety of object are imaged in a big water tank whose turbidity is systematically increased and show reconstruction quality which degrades little relative to clean water result even with a very significant scattering level 
while machine learning ha been instrumental to the ongoing progress in most area of computer vision it ha not been applied to the problem of stereo matching with similar frequency or success we present a supervised learning approach for predicting the correctness of stereo match based on a random forest and a set of feature that capture various form of information about each pixel we show highly competitive result in predicting the correctness of match and in confidence estimation which allows u to rank pixel according to the reliability of their assigned disparity moreover we show how these confidence value can be used to improve the accuracy of disparity map by integrating them with an mrf based stereo algorithm this is an important distinction from current literature that ha mainly focused on sparsification by removing potentially erroneous disparity to generate quasi dense disparity map 
there ha been a lot of work on face modeling analysis and landmark detection with active appearance model being one of the most successful technique a major drawback of these model is the large number of detailed annotated training example needed for learning therefore we present a transfer learning method that is able to learn from related training data using an instance weighted transfer technique our method is derived using a generalization of importance sampling and in contrast to previous work we explicitly try to tackle the transfer already during learning instead of adapting the fitting process in our studied application of face landmark detection we efficiently transfer facial expression from other human individual and are thus able to learn a precise face active appearance model only from neutral face of a single individual our approach is evaluated on two common face datasets and outperforms previous transfer method 
markov chain monte carlo mcmc is an elegant tool widely used in variety of area in computer vision it ha been used for the inference on the markov random field model mrf however mcmc le concerned than other deterministic approach although it converges to global optimal solution in theory the major obstacle is it slow convergence to come up with faster sampling method we investigate two idea breaking detailed balance and updating multiple node at a time although detailed balance is considered to be essential element of mcmc it actually is not the necessary condition for the convergence in addition exploiting the structure of mrf we introduce a new kernel which update multiple node in a scanline rather than a single node those two idea are integrated in a novel way to develop an efficient method called scanline sampler without detailed balance in experimental section we apply our method to the opengm benchmark of mrf optimization and show the proposed method achieves faster convergence than the conventional approach 
in this paper we address the problem of synthesizing novel view from a set of input image state of the art method such a the unstructured lumigraph have been using heuristic to combine information from the original view often using an explicit or implicit approximation of the scene geometry while the proposed heuristic have been largely explored and proven to work effectively a bayesian formulation wa recently introduced formalizing some of the previously proposed heuristic pointing out which physical phenomenon could lie behind each however some important heuristic were still not taken into account and lack proper formalization we contribute a new physic based generative model and the corresponding maximum a posteriori estimate providing the desired unification between heuristic based method and a bayesian formulation the key point is to systematically consider the error induced by the uncertainty in the geometric proxy we provide an extensive discussion analyzing how the obtained equation explain the heuristic developed in previous method furthermore we show that our novel bayesian model significantly improves the quality of novel view in particular if the scene geometry estimate is inaccurate 
in this paper we address the problem of object tracking in intensity image and depth data we propose a generic framework that can be used either for tracking d template in intensity image or for tracking d object in depth image to overcome problem like partial occlusion strong illumination change and motion blur that notoriously make energy minimization based tracking method get trapped in a local minimum we propose a learning based method that is robust to all these problem we use random forest to learn the relation between the parameter that defines the object s motion and the change they induce on the image intensity or the point cloud of the template it follows that to track the template when it move we use the change on the image intensity or point cloud to predict the parameter of this motion our algorithm ha an extremely fast tracking performance running at le than m per frame and is robust to partial occlusion moreover it demonstrates robustness to strong illumination change when tracking template using intensity image and robustness in tracking d object from arbitrary viewpoint even in the presence of motion blur that cause missing or erroneous data in depth image extensive experimental evaluation and comparison to the related approach strongly demonstrates the benefit of our method 
this paper present a novel and general method for the detection rectification and segmentation of imaged coplanar repeated pattern the only assumption made of the scene geometry is that repeated scene element are mapped to each other by planar euclidean transformation the class of pattern covered is broad and includes nearly all commonly seen planar man made repeated pattern in addition novel linear constraint are used to reduce geometric ambiguity between the rectified imaged pattern and the scene pattern rectification to within a similarity of the scene plane is achieved from one rotated repeat or to within a similarity with a scale ambiguity along the axis of symmetry from one reflected repeat a stratum of constraint is derived that give the necessary configuration of repeat for each successive level of rectification a generative model for the imaged pattern is inferred and used to segment the pattern with pixel accuracy qualitative result are shown on a broad range of image type on which state of the art method fail 
convolutional neural network cnns have been established a a powerful class of model for image recognition problem encouraged by these result we provide an extensive empirical evaluation of cnns on large scale video classification using a new dataset of million youtube video belonging to class we study multiple approach for extending the connectivity of a cnn in time domain to take advantage of local spatio temporal information and suggest a multiresolution foveated architecture a a promising way of speeding up the training our best spatio temporal network display significant performance improvement compared to strong feature based baseline to but only a surprisingly modest improvement compared to single frame model to we further study the generalization performance of our best model by retraining the top layer on the ucf action recognition dataset and observe significant performance improvement compared to the ucf baseline model up from 
in this work we propose a new framework for recognizing rgb image captured by the conventional camera by leveraging a set of labeled rgb d data in which the depth feature can be additionally extracted from the depth image we formulate this task a a new unsupervised domain adaptation uda problem in which we aim to take advantage of the additional depth feature in the source domain and also cope with the data distribution mismatch between the source and target domain to effectively utilize the additional depth feature we seek two optimal projection matrix to map the sample from both domain into a common space by preserving a much a possible the correlation between the visual feature and depth feature to effectively employ the training sample from the source domain for learning the target classifier we reduce the data distribution mismatch by minimizing the maximum mean discrepancy mmd criterion which compare the data distribution for each type of feature in the common space based on the above two motivation we propose a new svm based objective function to simultaneously learn the two projection matrix and the optimal target classifier in order to well separate the source sample from different class when using each type of feature in the common space an efficient alternating optimization algorithm is developed to solve our new objective function comprehensive experiment for object recognition and gender recognition demonstrate the effectiveness of our proposed approach for recognizing rgb image by learning from rgb d data 
how much data do we need to describe a location we explore this question in the context of d scene reconstruction created from running structure from motion on large internet photo collection where reconstruction can contain many million of d point we consider several method for computing much more compact representation of such reconstruction for the task of location recognition with the goal of maintaining good performance with very small model in particular we introduce a new method for computing compact model that take into account both image point relationship and feature distinctiveness and we show that this method produce small model that yield better recognition performance than previous model reduction technique 
we introduce a new approach for recognizing and reconstructing d object in image our approach is based on an analysis by synthesis strategy a forward synthesis model construct possible geometric interpretation of the world and then selects the interpretation that best agrees with the measured visual evidence the forward model synthesizes visual template defined on invariant hog feature these visual template are discriminatively trained to be accurate for inverse estimation we introduce an efficient brute force approach to inference that search through a large number of candidate reconstruction returning the optimal one one benefit of such an approach is that recognition is inherently re constructive we show state of the art performance for detection and reconstruction on two challenging d object recognition datasets of car and cuboid 
several decade of research in computer and primate vision have resulted in many model some specialized for one problem others more general and invaluable experimental data here to help focus research effort onto the hardest unsolved problem and bridge computer and human vision we define a battery of test that measure the gap between human and machine performance in several dimension generalization across scene category generalization from image to edge map and line drawing invariance to rotation and scaling local global information with jumbled image and object recognition performance we measure model accuracy and the correlation between model and human error pattern experimenting over datasets where human data is available and gauging well established model we find that none fully resembles human in all aspect and we learn from each test which model and feature are more promising in approaching human in the tested dimension across all test we find that model based on local edge histogram consistently resemble human more while several scene statistic or gist model do perform well with both scene and object while computer vision ha long been inspired by human vision we believe systematic effort such a this will help better identify shortcoming of model and find new path forward 
in this paper we focus on the d modeling of flower in particular the petal the complex structure severe occlusion and wide variation make the reconstruction of their d model a challenging task therefore even though the flower is the most distinctive part of a plant there ha been little modeling study devoted to it we overcome these challenge by combining data driven modeling technique with domain knowledge from botany taking a d point cloud of an input flower scanned from a single view our method start with a level set based segmentation of each individual petal using both appearance and d information each segmented petal is then fitted with a scale invariant morphable petal shape model which is constructed from individually scanned exemplar petal novel constraint based on botany study such a the number and spatial layout of petal are incorporated into the fitting process for realistically reconstructing occluded region and maintaining correct d spatial relation finally the reconstructed petal shape is texture mapped using the registered color image with occluded region filled in by content from visible one experiment show that our approach can obtain realistic modeling of flower even with severe occlusion and large shape size variation 
in this work we reconsider labeling problem with virtually continuous state space which are of relevance in low level computer vision in order to cope with such huge state space multi scale method have been proposed to approximately solve such labeling task although performing well in many case these method do usually not come with any guarantee on the returned solution a general and principled approach to solve labeling problem is based on the well known linear programming relaxation which appears to be prohibitive for large state space at the first glance we demonstrate that a coarse to fine exploration strategy in the label space is able to optimize the lp relaxation for non trivial problem instance with reasonable run time and moderate memory requirement 
arguably deformable part model dpms are one of the most prominent approach for face alignment with impressive result being recently reported for both controlled lab and unconstrained setting fitting in most dpm method is typically formulated a a two step process during which discriminatively trained part template are first correlated with the image to yield a filter response for each landmark and then shape optimization is performed over these filter response this process although computationally efficient is based on fixed part template which are assumed to be independent and ha been shown to result in imperfect filter response and detection ambiguity to address this limitation in this paper we propose to jointly optimize a part based trained in the wild flexible appearance model along with a global shape model which result in a joint translational motion model for the model part via gauss newton gn optimization we show how significant computational reduction can be achieved by building a full model during training but then efficiently optimizing the proposed cost function on a sparse grid using weighted least square during fitting we coin the proposed formulation gauss newton deformable part model gn dpm finally we compare it performance against the state of the art and show that the proposed gn dpm outperforms it in some case by a large margin code for our method is available from http ibug doc ic ac uk resource 
we present a method for generating object segmentation proposal from group of superpixels the goal is to propose accurate segmentation for all object of an image the proposed object hypothesis can be used a input to object detection system and thereby improve efficiency by replacing exhaustive search the segmentation are generated in a class independent manner and therefore the computational cost of the approach is independent of the number of object class our approach combine both global and local search in the space of set of superpixels the local search is implemented by greedily merging adjacent pair of superpixels to build a bottom up segmentation hierarchy the region from such a hierarchy directly provide a part of our region proposal the global search provides the other part by performing a set of graph cut segmentation on a superpixel graph obtained from an intermediate level of the hierarchy the parameter of the graph cut problem are learnt in such a manner that they provide complementary set of region experiment with pascal voc image show that we reach state of the art with greatly reduced computational cost 
recent study show that mental disorder change the functional organization of the brain which could be investigated via various imaging technique analyzing such change is becoming critical a it could provide new biomarkers for diagnosing and monitoring the progression of the disease functional connectivity analysis study the covary activity of neuronal population in different brain region the sparse inverse covariance estimation sice also known a graphical lasso is one of the most important tool for functional connectivity analysis which estimate the interregional partial correlation of the brain although being increasingly used for predicting mental disorder sice is basically a generative method that may not necessarily perform well on classifying neuroimaging data in this paper we propose a learning framework to effectively improve the discriminative power of sices by taking advantage of the sample in the opposite class we formulate our objective a convex optimization problem for both one class and two class classification by analyzing these optimization problem we not only solve them efficiently in their dual form but also gain insight into this new learning framework the proposed framework is applied to analyzing the brain metabolic covariant network built upon fdg pet image for the prediction of the alzheimer s disease and show significant improvement of classification performance for both one class and two class scenario moreover a sice is a general method for learning undirected gaussian graphical model this paper ha broader meaning beyond the scope of brain research 
capturing and understanding visual signal is one of the core interest of computer vision much progress ha been made w r t many aspect of imaging but the reconstruction of refractive phenomenon such a turbulence gas and heat flow liquid or transparent solid ha remained a challenging problem in this paper we derive an intuitive formulation of light transport in refractive medium using light field and the transport of intensity equation we show how coded illumination in combination with pair of recorded image allow for robust computational reconstruction of dynamic two and three dimensional refractive phenomenon 
we introduce a method of sparse dictionary learning for edit propagation of high resolution image or video previous approach for edit propagation typically employ a global optimization over the whole set of image pixel incurring a prohibitively high memory and time consumption for high resolution image rather than propagating an edit pixel by pixel we follow the principle of sparse representation to obtain a compact set of representative sample or feature and perform edit propagation on the sample instead the sparse set of sample provides an intrinsic basis for an input image and the coding coefficient capture the linear relationship between all pixel and the sample the representative set of sample is then optimized by a novel scheme which maximizes the kl divergence between each sample pair to remove redundant sample we show several application of sparsity based edit propagation including video recoloring theme editing and seamless cloning operating on both color and texture feature we demonstrate that with a sample to pixel ratio in the order of signifying a significant reduction on memory consumption our method still maintains a high degree of visual fidelity 
this paper describes a patchwork assembly algorithm for depth image super resolution an input low resolution depth image is disassembled into part by matching similar region on a set of high resolution training image and a super resolution image is then assembled using these corresponding matched counterpart we convert the super resolution problem into a markov random field mrf labeling problem and propose a unified formulation embedding the consistency between the resolution enhanced image and the original input the similarity of disassembled part with the corresponding region on training image the depth smoothness in local neighborhood the additional geometric constraint from self similar structure in the scene and the boundary coincidence between the resolution enhanced depth image and an optional aligned high resolution intensity image experimental result on both synthetic and real world data demonstrate that the proposed algorithm is capable of recovering high quality depth image with x resolution enhancement along each coordinate direction and that it outperforms state of the art in both qualitative and quantitative evaluation 
this paper present a framework for simultaneously tracking learning and parsing object with a hierarchical and compositional and or graph aog representation the aog is discriminatively learned online to account for the appearance e g lighting and partial occlusion and structural e g different pose and viewpoint variation of the object itself a well a the distractors e g similar object in the scene background in tracking the state of the object i e bounding box is inferred by parsing with the current aog using a spatial temporal dynamic programming dp algorithm when the aog grows big for handling object with large variation in long term tracking we propose a bottom up top down scheduling scheme for efficient inference which performs focused inference with the most stable and discriminative small sub aog during online learning the aog is re learned iteratively with two step i identifying the false positive and false negative of the current aog in a new frame by exploiting the spatial and temporal constraint observed in the trajectory ii updating the structure of the aog and re estimating the parameter based on the augmented training dataset in experiment the proposed method outperforms state of theart tracking algorithm on a recent public tracking benchmark with testing video and publicly available tracker evaluated 
we describe an approach for simultaneous localization and calibration of a stream of range image our approach jointly optimizes the camera trajectory and a calibration function that corrects the camera s unknown nonlinear distortion experiment with real world benchmark data and synthetic data show that our approach increase the accuracy of camera trajectory and geometric model estimated from range video produced by consumer grade camera 
action analysis in image and video ha been attracting more and more attention in computer vision recognizing specific action in video clip ha been the main focus we move in a new more general direction in this paper and ask the critical fundamental question what is action how is action different from motion and in a given image or video where is the action we study the philosophical and visual characteristic of action which lead u to define actionness intentional bodily movement of biological agent people animal to solve the general problem we propose the lattice conditional ordinal random field model that incorporates local evidence a well a neighboring order agreement we implement the new model in the continuous domain and apply it to scoring actionness in both image and video datasets our experiment demonstrate not only that our new model can outperform the popular ranking svm but also that indeed action is distinct from motion 
we propose an optimization algorithm for mutual information based unsupervised figure ground separation the algorithm jointly estimate the color distribution of the foreground and background and separate them based on their mutual information with geometric regularity to this end we revisit the notion of mutual information and reformulate it in term of the photometric variable and the indicator function and propose a sequential convex optimization strategy for solving the nonconvex optimization problem that arises by minimizing a sequence of convex sub problem for the mutual information based nonconvex energy we efficiently attain high quality solution for challenging unsupervised figure ground segmentation problem we demonstrate the capacity of our approach in numerous experiment that show convincing fully unsupervised figure ground separation in term of both segmentation quality and robustness to initialization 
driven by the wide range of application scene text detection and recognition have become active research topic in computer vision though extensively studied localizing and reading text in uncontrolled environment remain extremely challenging due to various interference factor in this paper we propose a novel multi scale representation for scene text recognition this representation consists of a set of detectable primitive termed a strokelets which capture the essential substructure of character at different granularity strokelets posse four distinctive advantage usability automatically learned from bounding box label robustness insensitive to interference factor generality applicable to variant language and expressivity effective at describing character extensive experiment on standard benchmark verify the advantage of strokelets and demonstrate the effectiveness of the proposed algorithm for text recognition 
detecting pedestrian at a distance from large format wide area imagery is a challenging problem because of low ground sampling distance gsd and low frame rate of the imagery in such a scenario the approach based on appearance cue alone mostly fail because pedestrian are only a few pixel in size frame differencing and optical flow based approach also give poor detection result due to noise camera jitter and parallax in aerial video to overcome these challenge we propose a novel approach to extract multi scale intrinsic motion structure feature from pedestrian s motion pattern for pedestrian detection the mims feature encodes the intrinsic motion property of an object which are location velocity and trajectory shape invariant the extracted mims representation is robust to noisy flow estimate in this paper we give a comparative evaluation of the proposed method and demonstrate that mims outperforms the state of the art approach in identifying pedestrian from low resolution airborne video 
this paper present a novel introduction of online target specific metric learning in track fragment tracklet association by network flow optimization for long term multi person tracking different from other network flow formulation each node in our network represents a tracklet and each edge represents the likelihood of neighboring tracklets belonging to the same trajectory a measured by our proposed affinity score in our method target specific similarity metric are learned which give rise to the appearance based model used in the tracklet affinity estimation trajectory based tracklets are refined by using the learned metric to account for appearance consistency and to identify reliable tracklets the metric are then re learned using reliable tracklets for computing tracklet affinity score long term trajectory are then obtained through network flow optimization occlusion and missed detection are handled by a trajectory completion step our method is effective for long term tracking even when the target are spatially close or completely occluded by others we validate our proposed framework on several public datasets and show that it outperforms several state of art method 
in this paper we present a novel real time algorithm for simultaneous pose and shape estimation for articulated object such a human being and animal the key of our pose estimation component is to embed the articulated deformation model with exponential map based parametrization into a gaussian mixture model benefiting from the probabilistic measurement model our algorithm requires no explicit point correspondence a opposed to most existing method consequently our approach is le sensitive to local minimum and well handle fast and complex motion extensive evaluation on publicly available datasets demonstrate that our method outperforms most state of art pose estimation algorithm with large margin especially in the case of challenging motion moreover our novel shape adaptation algorithm based on the same probabilistic model automatically capture the shape of the subject during the dynamic pose estimation process experiment show that our shape estimation method achieves comparable accuracy with state of the art yet requires neither parametric model nor extra calibration procedure 
in recent year fluorescence analysis of scene ha received attention fluorescence can provide additional information about scene and ha been used in application such a camera spectral sensitivity estimation d reconstruction and color relighting in particular hyperspectral image of reflective fluorescent scene provide a rich amount of data however due to the complex nature of fluorescence hyperspectral imaging method rely on specialized equipment such a hyperspectral camera and specialized illuminant in this paper we propose a more practical approach to hyperspectral imaging of reflective fluorescent scene using only a conventional rgb camera and varied colored illuminant the key idea of our approach is to exploit a unique property of fluorescence the chromaticity of fluorescence emission are invariant under different illuminant this allows u to robustly estimate spectral reflectance and fluorescence emission chromaticity we then show that given the spectral reflectance and fluorescent chromaticity the fluorescence absorption and emission spectrum can also be estimated we demonstrate in result that all scene spectrum can be accurately estimated from rgb image finally we show that our method can be used to accurately relight scene under novel lighting 
the goal of image pre compensation is to process an image such that after being convolved with a known kernel will appear close to the sharp reference image in a practical setting the pre compensated image ha significantly higher dynamic range than the latent image a a result some form of tone mapping is needed in this paper we show how global tone mapping function affect contrast and ringing in image pre compensation in particular we show that linear tone mapping eliminates ringing but incurs severe contrast loss while non linear tone mapping function such a gamma curve slightly enhances contrast but introduces ringing to enable quantitative analysis we design new metric to measure the contrast of an image with ringing specifically we set out to find it equivalent ringing free image that match it intensity histogram and us it contrast a the measure we illustrate our approach on projector defocus compensation and visual acuity enhancement compared with the state of the art our approach significantly improves the contrast we believe our technique is the first to analytically trade off between contrast and ringing 
training a generic objectness measure to produce a small set of candidate object window ha been shown to speed up the classical sliding window object detection paradigm we observe that generic object with well defined closed boundary can be discriminated by looking at the norm of gradient with a suitable resizing of their corresponding image window in to a small fixed size based on this observation and computational reason we propose to resize the window to and use the norm of the gradient a a simple d feature to describe it for explicitly training a generic objectness measure we further show how the binarized version of this feature namely binarized normed gradient bing can be used for efficient objectness estimation which requires only a few atomic operation e g add bitwise shift etc experiment on the challenging pascal voc dataset show that our method efficiently fps on a single laptop cpu generates a small set of category independent high quality object window yielding object detection rate dr with proposal increasing the number of proposal and color space for computing bing feature our performance can be further improved to dr 
sparse coding and dictionary learning have seen their application in many vision task which usually is formulated a a non convex optimization problem many iterative method have been proposed to tackle such an optimization problem however it remains an open problem to have a method that is not only practically fast but also is globally convergent in this paper we proposed a fast proximal method for solving norm based dictionary learning problem and we proved that the whole sequence generated by the proposed method converges to a stationary point with sub linear convergence rate the benefit of having a fast and convergent dictionary learning method is demonstrated in the application of image recovery and face recognition 
the functional difference between a diffuse wall and a mirror is well understood one scatter back into all direction and the other one preserve the directionality of reflected light the temporal structure of the light however is left intact by both assuming simple surface reflection photon that arrive first are reflected first in this paper we exploit this insight to recover object outside the line of sight from second order diffuse reflection effectively turning wall into mirror we formulate the reconstruction task a a linear inverse problem on the transient response of a scene which we acquire using an affordable setup consisting of a modulated light source and a time of flight image sensor by exploiting sparsity in the reconstruction domain we achieve resolution in the order of a few centimeter for object shape depth and laterally and albedo our method is robust to ambient light and work for large room sized scene it is drastically faster and le expensive than previous approach using femtosecond laser and streak camera and doe not require any moving part 
nearest neighbor search method based on hashing have attracted considerable attention for effective and efficient large scale similarity search in computer vision and information retrieval community in this paper we study the problem of learning hash function in the context of multimodal data for cross view similarity search we put forward a novel hashing method which is referred to collective matrix factorization hashing cmfh cmfh learns unified hash code by collective matrix factorization with latent factor model from different modality of one instance which can not only support cross view search but also increase the search accuracy by merging multiple view information source we also prove that cmfh a similarity preserving hashing learning method ha upper and lower boundary extensive experiment verify that cmfh significantly outperforms several state of the art method on three different datasets 
this paper considers human tracking in multi view setup and investigates a robust strategy that learns online key pose to drive a shape tracking method the interest arises in realistic dynamic scene where occlusion or segmentation error occur the corrupted observation present missing data and outlier that deteriorate tracking result we propose to use key pose of the tracked person a multiple reference model in contrast to many existing approach that rely on a single reference model multiple template represent a larger variability of human pose they provide therefore better initial hypothesis when tracking with noisy data our approach identifies these reference model online a distinctive keyframes during tracking the most suitable one is then chosen a the reference at each frame in addition taking advantage of the proximity between successive frame an efficient outlier handling technique is proposed to prevent from associating the model to irrelevant outlier the two strategy are successfully experimented with a surface deformation framework that recovers both the pose and the shape evaluation on existing datasets also demonstrate their benefit with respect to the state of the art 
fisher kernel and deep learning were two development with significant impact on large scale object categorization in the last year both approach were shown to achieve state of the art result on large scale object categorization datasets such a imagenet conceptually however they are perceived a very different and it is not uncommon for heated debate to spring up when advocate of both paradigm meet at conference or workshop in this work we emphasize the similarity between both architecture rather than their difference and we argue that such a unified view allows u to transfer idea from one domain to the other a a concrete example we introduce a method for learning a support vector machine classifier with fisher kernel at the same time a a task specific data representation we reinterpret the setting a a multi layer feed forward network it final layer is the classifier parameterized by a weight vector and the two previous layer compute fisher vector parameterized by the coefficient of a gaussian mixture model we introduce a gradient descent based learning algorithm that in contrast to other feature learning technique is not just derived from intuition or biological analogy but ha a theoretical justification in the framework of statistical learning theory our experiment show that the new training procedure lead to significant improvement in classification accuracy while preserving the modularity and geometric interpretability of a support vector machine setup 
we introduce a new compression scheme for high dimensional vector that approximates the vector using sum of m codewords coming from m different codebooks we show that the proposed scheme permit efficient distance and scalar product computation between compressed and uncompressed vector we further suggest vector encoding and codebook learning algorithm that can minimize the coding error within the proposed scheme in the experiment we demonstrate that the proposed compression can be used instead of or together with product quantization compared to product quantization and it optimized version the proposed compression approach lead to lower coding approximation error higher accuracy of approximate nearest neighbor search in the datasets of visual descriptor and lower image classification error whenever the classifier are learned on or applied to compressed vector 
when do the visual ray associated with triplet of point correspondence converge that is intersect in a common point classical model of trinocular geometry based on the fundamental matrix and trifocal tensor associated with the corresponding camera only provide partial answer to this fundamental question in large part because of underlying but seldom explicit general configuration assumption this paper us elementary tool from projective line geometry to provide necessary and sufficient geometric and analytical condition for convergence in term of transversals to triplet of visual ray without any such assumption in turn this yield a novel and simple minimal parameterization of trinocular geometry for camera with non collinear or collinear pinhole 
we are dealing with the face cluster recognition problem where there are multiple image per subject in both gallery and probe set it is never guaranteed to have a clear spatio temporal relation among the multiple image of each subject considering that the image vector of each subject either in gallery or in probe span a subspace an algorithm dual linear regression classification dlrc for the face cluster recognition problem is developed where the distance between two subspace is defined a the similarity value between a gallery subject and a probe subject dlrc attempt to find a virtual face image located in the intersection of the subspace spanning from both cluster of face image the distance between the virtual face image reconstructed from both subspace is then taken a the distance between these two subspace we further prove that such distance can be formulated under a single linear regression model where we indeed can find the distance without reconstructing the virtual face image extensive experimental evaluation demonstrated the effectiveness of dlrc algorithm compared to other algorithm 
interactive object segmentation ha great practical importance in computer vision many interactive method have been proposed utilizing user input in the form of mouse click and mouse stroke and often requiring a lot of user intervention in this paper we present a system with a far simpler input method the user need only give the name of the desired object with the tag provided by the user we do a text query of an image database to gather exemplar of the object using object proposal and borrowing idea from image retrieval and object detection the object is localized in the target image an appearance model generated from the exemplar and the location prior are used in an energy minimization framework to select the object our method outperforms the state of the art on existing datasets and on a more challenging dataset we collected 
a transient image is the optical impulse response of a scene which visualizes light propagation during an ultra short time interval in this paper we discover that the data captured by a multifrequency time of flight tof camera is the fourier transform of a transient image and identify the source of systematic error based on the discovery we propose a novel framework of frequency domain transient imaging a well a algorithm to remove systematic error the whole process of our approach is of much lower computational cost especially lower memory usage than heide et al s approach using the same device we evaluate our approach on both synthetic and real datasets 
this paper present a framework for object recognition using topological persistence in particular we show that the so called persistence diagram built from function defined on the object can serve a compact and informative descriptor for image and shape complementary to the bag of feature representation which capture the distribution of value of a given function persistence diagram can be used to characterize it structural property reflecting spatial information in an invariant way in practice the choice of function is simple each dimension of the feature vector can be viewed a a function the proposed method is general it can work on various multimedia data including d shape texture and triangle mesh extensive experiment on d shape retrieval hand gesture recognition and texture classification demonstrate the performance of the proposed method in comparison with state of the art method additionally our approach yield higher recognition accuracy when used in conjunction with the bag of feature 
assessing the visual realism of image is increasingly becoming an essential aspect of field ranging from computer graphic cg rendering to photo manipulation in this paper we systematically evaluate factor underlying human perception of visual realism and use that information to create an automated assessment of visual realism we make the following unique contribution first we established a benchmark dataset of image with empirically determined visual realism score second we identified attribute potentially related to image realism and used correlational technique to determine that realism wa most related to image naturalness familiarity aesthetic and semantics third we created an attribute motivated automated computational model that estimated image visual realism quantitatively using human assessment a a benchmark the model wa below human performance but outperformed other state of the art algorithm 
in this paper we revisit the pose determination problem of a partially calibrated camera with unknown focal length hereafter referred to a the p n pf problem by using n n d to d point correspondence our core contribution is to introduce the angle constraint and derive a compact bivariate polynomial equation for each point triplet based on this polynomial equation we propose a truly general method for the p n pf problem which is suited both to the minimal point based ransac application and also to large scale scenario with thousand of point irrespective of the d point configuration in addition by solving bivariate polynomial system via the sylvester resultant our method is very simple and easy to implement it simplicity is especially obvious when one need to develop a fast solver for the point case on the basis of the characteristic polynomial technique experiment result have also demonstrated it superiority in accuracy and efficiency when compared with the existing state of the art solution 
the basic idea of shape from shading is to infer the shape of a surface from it shading information in a single image since this problem is ill posed a number of simplifying assumption have been often used however they rarely hold in practice this paper present a simple shading correction algorithm that transforms the image to a new image that better satisfies the assumption typically needed by existing algorithm thus improving the accuracy of shape recovery the algorithm take advantage of some local shading measure that have been driven under these assumption the method is successfully evaluated on real data of human teeth with ground truth d shape 
we present a robust model to locate facial landmark under different view and possibly severe occlusion to build reliable relationship between face appearance and shape with large view variation we propose to formulate face alignment a an l induced stagewise relational dictionary srd learning problem during each training stage the srd model learns a relational dictionary to capture consistent relationship between face appearance and shape which are respectively modeled by the pose indexed image feature and the shape displacement for current estimated landmark during testing the srd model automatically selects a sparse set of the most related shape displacement for the testing face and us them to refine it shape iteratively to locate facial landmark under occlusion we further propose to learn an occlusion dictionary to model different kind of partial face occlusion by deploying the occlusion dictionary into the srd model the alignment performance for occluded face can be further improved our algorithm is simple effective and easy to implement extensive experiment on two benchmark datasets and two newly built datasets have demonstrated it superior performance over the state of the art method especially for face with large view variation and or occlusion 
in this paper we revisit the phase field approximation of ambrosio and tortorelli for the mumford shah functional we then propose a convex relaxation for it to attempt to compute globally optimal solution rather than solving the nonconvex functional directly which is the main contribution of this paper inspired by mccormick s seminal work on factorable nonconvex problem we split a nonconvex product term that appears in the ambrosio tortorelli elliptic functionals in a way that a typical alternating gradient method guarantee a globally optimal solution without completely removing coupling effect furthermore not only do we provide a fruitful analysis of the proposed relaxation but also demonstrate the capacity of our relaxation in numerous experiment that show convincing result compared to a naive extension of the mccormick relaxation and it quadratic variant indeed we believe the proposed relaxation and the idea behind would open up a possibility for convexifying a new class of function in the context of energy minimization for computer vision 
while most existing multilabel ranking method assume the availability of a single objective label ranking for each instance in the training set this paper deal with a more common case where subjective inconsistent ranking from multiple ranker are associated with each instance the key idea is to learn a latent preference distribution for each instance the proposed method mainly includes two step the first step is to generate a common preference distribution that is most compatible to all the personal ranking the second step is to learn a mapping from the instance to the preference distribution the proposed preference distribution learning pdl method is applied to the problem of multilabel ranking for natural scene image experimental result show that pdl can effectively incorporate the information given by the inconsistent ranker and perform remarkably better than the compared state of the art multilabel ranking algorithm 
recognition is graduating from lab to real world application while it is encouraging to see it potential being tapped it brings forth a fundamental challenge to the vision researcher scalability how can we learn a model for any concept that exhaustively cover all it appearance variation while requiring minimal or no human supervision for compiling the vocabulary of visual variance gathering the training image and annotation and learning the model in this paper we introduce a fully automated approach for learning extensive model for a wide range of variation e g action interaction attribute and beyond within any concept our approach leverage vast resource of online book to discover the vocabulary of variance and intertwines the data collection and modeling step to alleviate the need for explicit human supervision in training the model our approach organizes the visual knowledge about a concept in a convenient and useful way enabling a variety of application across vision and nlp our online system ha been queried by user to learn model for several interesting concept including breakfast gandhi beautiful etc to date our system ha model available for over variation within concept and ha annotated more than million image with bounding box 
over the last few year with the immense popularity of the kinect there ha been renewed interest in developing method for human gesture and action recognition from d skeletal data a number of approach have been proposed to extract representative feature from d skeletal data most commonly hard wired geometric or bio inspired shape context feature we propose a hierarchial dynamic framework that first extract high level skeletal joint feature and then us the learned representation for estimating emission probability to infer action sequence currently gaussian mixture model are the dominant technique for modeling the emission distribution of hidden markov model we show that better action recognition using skeletal feature can be achieved by replacing gaussian mixture model by deep neural network that contain many layer of feature to predict probability distribution over state of hidden markov model the framework can be easily extended to include a ergodic state to segment and recognize action simultaneously 
hashing technique ha become a promising approach for fast similarity search most of existing hashing research pursue the binary code for the same type of entity by preserving their similarity in practice there are many scenario involving nearest neighbor search on the data given in matrix form where two different type of yet naturally associated entity respectively correspond to it two dimension or view to fully explore the duality between the two view we propose a collaborative hashing scheme for the data in matrix form to enable fast search in various application such a image search using bag of word and recommendation using user item rating by simultaneously preserving both the entity similarity in each view and the interrelationship between view our collaborative hashing effectively learns the compact binary code and the explicit hash function for out of sample extension in an alternating optimization way extensive evaluation are conducted on three well known datasets for search inside a single view and search across different view demonstrating that our proposed method outperforms state of the art baseline with significant accuracy gain ranging from to relatively 
new scanning technology are increasing the importance of d mesh data and the need for algorithm that can reliably align it surface registration is important for building full d model from partial scan creating statistical shape model shape retrieval and tracking the problem is particularly challenging for non rigid and articulated object like human body while the challenge of real world data registration are not present in existing synthetic datasets establishing ground truth correspondence for real d scan is difficult we address this with a novel mesh registration technique that combine d shape and appearance information to produce high quality alignment we define a new dataset called faust that contains scan of people in a wide range of pose together with an evaluation methodology to achieve accurate registration we paint the subject with high frequency texture and use an extensive validation process to ensure accurate ground truth we find that current shape registration method have trouble with this real world data the dataset and evaluation website are available for research purpose at http faust is tue mpg de 
this paper introduces a regularization method to explicitly control the rank of a learned symmetric positive semidefinite distance matrix in distance metric learning to this end we propose to incorporate in the objective function a linear regularization term that minimizes the k smallest eigenvalue of the distance matrix it is equivalent to minimizing the trace of the product of the distance matrix with a matrix in the convex hull of rank k projection matrix called a fantope based on this new regularization method we derive an optimization scheme to efficiently learn the distance matrix we demonstrate the effectiveness of the method on synthetic and challenging real datasets of face verification and image classification with relative attribute on which our method outperforms state of the art metric learning algorithm 
the fisher vector fv representation is a high dimensional extension of the popular bag of word representation transformation of the fv by power and l normalization ha shown to significantly improve it performance and led to state of the art result for a range of image and video classification and retrieval task these normalization however render the representation non additive over local descriptor combined with it high dimensionality this make the fv computationally expensive for the purpose of localization task in this paper we present approximation to both these normalization which yield significant improvement in the memory and computational cost of the fv when used for localization second we show how these approximation can be used to define upper bound on the score function that can be efficiently evaluated which enables the use of branch and bound search a an alternative to exhaustive sliding window search we present experimental evaluation result on classification and temporal localization of action in video these show that the our approximation lead to a speedup of at least one order of magnitude while maintaining state of the art action recognition and localization performance 
in this paper we aim for zero shot classification that is visual recognition of an unseen class by using knowledge transfer from known class our main contribution is costa which exploit co occurrence of visual concept in image for knowledge transfer these inter dependency arise naturally between concept and are easy to obtain from existing annotation or web search hit count we estimate a classifier for a new label a a weighted combination of related class using the co occurrence to define the weight we propose various metric to leverage these co occurrence and a regression model for learning a weight for each related class we also show that our zero shot classifier can serve a prior for few shot learning experiment on three multi labeled datasets reveal that our proposed zero shot method are approaching and occasionally outperforming fully supervised svms we conclude that co occurrence statistic suffice for zero shot classification 
we present a distance metric based upon the notion of minimum cost injective mapping between set our function satisfies metric property a long a the cost of the minimum mapping is derived from a semimetric for which the triangle inequality is not necessarily satisfied we show that the jaccard distance alternatively biotope tanimoto or marczewski steinhaus distance may be considered the special case for finite set where cost are derived from the discrete metric extension that allow premetrics not necessarily symmetric multisets generalized to include probability distribution and asymmetric mapping are given that expand the versatility of the metric without sacrificing metric property the function ha potential application in pattern recognition machine learning and information retrieval 
partial differential equation pdes have been successful in solving many low level vision task however it is a challenging task to directly utilize pdes for visual saliency detection due to the difficulty in incorporating human perception and high level prior to a pde system instead of designing pdes with fixed formulation and boundary condition this paper proposes a novel framework for adaptively learning a pde system from an image for visual saliency detection we assume that the saliency of image element can be carried out from the relevance to the saliency seed i e the most representative salient element in this view a general linear elliptic system with dirichlet boundary lesd is introduced to model the diffusion from seed to other relevant point for a given image we first learn a guidance map to fuse human prior knowledge to the diffusion system then by optimizing a discrete submodular function constrained with this lesd and a uniform matroid the saliency seed i e boundary condition can be learnt for this image thus achieving an optimal pde system to model the evolution of visual saliency experimental result on various challenging image set show the superiority of our proposed learning based pdes for visual saliency detection 
it is often desirable to evaluate image quality with a perceptually relevant measure that doe not require a reference image recent approach to this problem use human provided quality score with machine learning to learn a measure the biggest hurdle to these effort are the difficulty of generalizing across diverse type of distortion and collecting the enormity of human scored training data that is needed to learn the measure we present a new blind image quality measure that address these difficulty by learning a robust nonlinear kernel regression function using a rectifier neural network the method is pre trained with unlabeled data and fine tuned with labeled data it generalizes across a large set of image and distortion type without the need for a large amount of labeled data we evaluate our approach on two benchmark datasets and show that it not only outperforms the current state of the art in blind image quality estimation but also outperforms the state of the art in non blind measure furthermore we show that our semi supervised approach is robust to using varying amount of labeled data 
we propose a new fully automated non rigid segmentation approach based on the distance regularized level set method that is initialized and constrained by the result of a structured inference using deep belief network this recently proposed level set formulation achieves reasonably accurate result in several segmentation problem and ha the advantage of eliminating periodic re initialization during the optimization process and a a result it avoids numerical error nevertheless when applied to challenging problem such a the left ventricle segmentation from short axis cine magnetic ressonance mr image the accuracy obtained by this distance regularized level set is lower than the state of the art the main reason behind this lower accuracy are the dependence on good initial guess for the level set optimization and on reliable appearance model we address these two issue with an innovative structured inference using deep belief network that produce reliable initial guess and appearance model the effectiveness of our method is demonstrated on the miccai left ventricle segmentation challenge where we show that our approach achieves one of the most competitive result in term of segmentation accuracy in the field 
most pedestrian detection approach that achieve high accuracy and precision rate and that can be used for real time application are based on histogram of gradient orientation usually multiscale detection is attained by resizing the image several time and by recomputing the image feature or using multiple classifier for different scale in this paper we present a pedestrian detection approach that us the same classifier for all pedestrian scale based on image feature computed for a single scale we go beyond the low level pixel wise gradient orientation bin and use higher level visual word organized into word channel boosting is used to learn classification feature from the integral word channel the proposed approach is evaluated on multiple datasets and achieves outstanding result on the inria and caltech usa benchmark by using a gpu implementation we achieve a classification rate of over million bounding box per second and a fps rate for multiscale detection in a image 
in this paper we would like to evaluate online learning algorithm for large scale visual recognition using state of the art feature which are preselected and held fixed today combination of high dimensional feature and linear classifier are widely used for large scale visual recognition numerous so called mid level feature have been developed and mutually compared on an experimental basis although various learning method for linear classification have also been proposed in the machine learning and natural language processing literature they have rarely been evaluated for visual recognition therefore we give guideline via investigation of state of the art online learning method of linear classifier many method have been evaluated using toy data and natural language processing problem such a document classification consequently we gave those method a unified interpretation from the viewpoint of visual recognition result of controlled comparison indicate three guideline that might change the pipeline for visual recognition 
reconstructing a surface image from corrupted gradient field is a crucial step in many imaging application where a gradient field is subject to both noise and unlocalized outlier resulting typically in a non integrable field we present in this paper a new optimization method for robust surface reconstruction the proposed formulation is based on a triple sparsity prior a sparse prior on the residual gradient field and a double sparse prior on the surface gradient we develop an efficient alternate minimization strategy to solve the proposed optimization problem the method is able to recover a good quality surface from severely corrupted gradient thanks to it ability to handle both noise and outlier we demonstrate the performance of the proposed method on synthetic and real data experiment show that the proposed solution outperforms some existing method in the three possible case noise only outlier only and mixed noise outlier 
topic modeling based on latent dirichlet allocation lda ha been a framework of choice to deal with multimodal data such a in image annotation task recently a new type of topic model called the document neural autoregressive distribution estimator docnade wa proposed and demonstrated state of the art performance for text document modeling in this work we show how to successfully apply and extend this model to multimodal data such a simultaneous image classification and annotation specifically we propose supdocnade a supervised extension of docnade that increase the discriminative power of the hidden topic feature by incorporating label information into the training objective of the model and show how to employ supdocnade to learn a joint representation from image visual word annotation word and class label information we also describe how to leverage information about the spatial position of the visual word for supdocnade to achieve better performance in a simple yet effective manner we test our model on the labelme and uiuc sport datasets and show that it compare favorably to other topic model such a the supervised variant of lda and a spatial matching pyramid spm approach 
scribble in scribble based interactive segmentation such a graph cut are usually assumed to be perfectly accurate i e foreground scribble pixel will never be segmented a background in the final segmentation however it can be hard to draw perfectly accurate scribble especially on fine structure of the image or on mobile touch screen device in this paper we propose a novel ratio energy function that tolerates error in the user input while encouraging maximum use of the user input information more specifically the ratio energy aim to minimize the graph cut energy while maximizing the user input respected in the segmentation the ratio energy function can be exactly optimized using an efficient iterated graph cut algorithm the robustness of the proposed method is validated on the grabcut dataset using both synthetic scribble and manual scribble the experimental result show that the proposed algorithm is robust to the error in the user input and preserve the anchoring capability of the user input 
in this paper we present the first local descriptor designed for dynamic surface a dynamic surface is a surface that can undergo non rigid deformation e g human body surface using state of the art technology detail on dynamic surface such a cloth wrinkle or facial expression can be accurately reconstructed hence various result e g surface rigidity or elasticity could be derived by microscopic categorization of surface element we propose a timing based descriptor to model local spatiotemporal variation of surface intrinsic property the low level descriptor encodes gap between local event dynamic of neighboring keypoints using timing structure of linear dynamical system lds we also introduce the bag of timing bot paradigm for surface dynamic characterization experiment are performed on synthesized and real world datasets we show the proposed descriptor can be used for challenging dynamic surface classification and segmentation with respect to rigidity at surface keypoints 
real world video of human activity exhibit temporal structure at various scale long video are typically composed out of multiple action instance where each instance is itself composed of sub action with variable duration and ordering temporal grammar can presumably model such hierarchical structure but are computationally difficult to apply for long video stream we describe simple grammar that capture hierarchical temporal structure while admitting inference with a finite state machine this make parsing linear time constant storage and naturally online we train grammar parameter using a latent structural svm where latent subactions are learned automatically we illustrate the effectiveness of our approach over common baseline on a new half million frame dataset of continuous youtube video 
in this paper we present a conceptually simple but surprisingly powerful method for visual prediction which combine the effectiveness of mid level visual element with temporal modeling our framework can be learned in a completely unsupervised manner from a large collection of video however more importantly because our approach model the prediction framework on these mid level element we can not only predict the possible motion in the scene but also predict visual appearance how are appearance going to change with time this yield a visual hallucination of probable event on top of the scene we show that our method is able to accurately predict and visualize simple future event we also show that our approach is comparable to supervised method for event prediction 
this paper present a novel method to generate a hypothesis set of class independent object region it ha been shown that such object region can be used to focus computer vision technique on the part of an image that matter most leading to significant improvement in both object localisation and semantic segmentation in recent year of course the higher quality of class independent object region the better subsequent computer vision algorithm can perform in this paper we focus on generating higher quality object hypothesis we start from an oversegmentation for which we propose to extract a wide variety of region feature we group region together in a hierarchical fashion for which we train a random forest which predicts at each stage of the hierarchy the best possible merge hence unlike other approach we use relatively powerful feature and classifier at an early stage of the generation of likely object region finally we identify and combine stable region in order to capture object which consist of dissimilar part we show on the pascal and datasets that our method yield higher quality region than competing approach while it is at the same time more computationally efficient 
several popular and effective object detector separately model intra class variation arising from deformation and appearance change this reduces model complexity while enabling the detection of object across change in viewpoint object pose etc the deformable part model dpm is perhaps the most successful such model to date a common assumption is that the exponential number of template enabled by a dpm is critical to it success in this paper we show the counter intuitive result that it is possible to achieve similar accuracy using a small dictionary of deformation each component in our model is represented by a single hog template and a dictionary of flow field that determine the deformation the template may undergo while the number of candidate deformation is dramatically fewer than that for a dpm the deformed template tend to be plausible and interpretable in addition we discover that the set of deformation base is actually transferable across object category and that learning shared base across similar category can boost accuracy 
a video capture a sequence and interaction of concept that can be static for instance object or scene or dynamic such a action for large datasets containing hundred of thousand of image or video it is impractical to manually annotate all the concept or all the instance of a single concept however a dictionary with visuallydistinct element can be created automatically from unlabeled video which can capture and express the entire dataset the downside to this machine discovered dictionary is meaninglessness i e it element are devoid of semantics and interpretation in this paper we present an approach that leverage the strength of semantic concept and the machine discovered dove by learning a relationship between them since instance of a semantic concept share visual similarity the proposed approach us softconsensus regularization to learn the mapping that enforces instance from each semantic concept to have similar representation the testing is performed by projecting the query onto the dove a well a new representation of semantic concept from training with non negativity and unit summation constraint for probabilistic interpretation we tested our formulation on trecvid med and sin task and obtained encouraging result 
outlier are pervasive in many computer vision and pattern recognition problem automatically eliminating outlier scattering among practical data collection becomes increasingly important especially for internet inspired vision application in this paper we propose a novel one class learning approach which is robust to contamination of input training data and able to discover the outlier that corrupt one class of data source our approach work under a fully unsupervised manner differing from traditional one class learning supervised by known positive label by design our approach optimizes a kernel based max margin objective which jointly learns a large margin one class classifier and a soft label assignment for inliers and outlier an alternating optimization algorithm is then designed to iteratively refine the classifier and the labeling achieving a provably convergent solution in only a few iteration extensive experiment conducted on four image datasets in the presence of artificial and real world outlier demonstrate that the proposed approach is considerably superior to the state of the art in obliterating outlier from contaminated one class of image exhibiting strong robustness at a high outlier proportion up to 
dynamic bayesian network such a hidden markov model hmms are successfully used a probabilistic model for human motion the use of hidden variable make them expressive model but inference is only approximate and requires procedure such a particle filter or markov chain monte carlo method in this work we propose to instead use simple markov model that only model observed quantity we retain a highly expressive dynamic model by using interaction that are nonlinear and non parametric a presentation of our approach in term of latent variable show logarithmic growth for the computation of exact log likelihood in the number of latent state we validate our model on human motion capture data and demonstrate state of the art performance on action recognition and motion completion task 
the main contribution of this work is a framework to register anatomical structure characterized a a point set where each point ha an associated symmetric matrix these matrix can represent problem dependent characteristic of the registered structure for example in airway matrix can represent the orientation and thickness of the structure our framework relies on a dense tensor field representation which we implement sparsely a a kernel mixture of tensor field we equip the space of tensor field with a norm that serf a a similarity measure to calculate the optimal transformation between two structure we minimize this measure using an analytical gradient for the similarity measure and the deformation field which we restrict to be a diffeomorphism we illustrate the value of our tensor field model by comparing our result with scalar and vector field based model finally we evaluate our registration algorithm on synthetic data set and validate our approach on manually annotated airway tree 
in this work we propose a technique to combine bottom up segmentation coming in the form of slic superpixels with sliding window detector such a deformable part model dpms the merit of our approach lie in cleaning up the low level hog feature by exploiting the spatial support of slic superpixels this can be understood a using segmentation to split the feature variation into object specific and background change rather than committing to a single segmentation we use a large pool of slic superpixels and combine them in a scale positionand object dependent manner to build soft segmentation mask the segmentation mask can be computed fast enough to repeat this process over every candidate window during training and detection for both the root and part filter of dpms we use these mask to construct enhanced background invariant feature to train dpms we test our approach on the pascal voc outperforming the standard dpm in out of class yielding an average increase of ap additionally we demonstrate the robustness of this approach extending it to dense sift descriptor for large displacement optical flow 
we address the false response influence problem when learning and applying discriminative part to construct the mid level representation in scene classification it is often caused by the complexity of latent image structure when convolving part filter with input image this problem make mid level representation even after pooling not distinct enough to classify input data correctly to category our solution is to learn important spatial pooling region along with their appearance the experiment show that this new framework suppresses false response and produce improved result on several datasets including mit indoor scene and uiuc sport when combined with global image feature our method achieves state of the art performance on these datasets 
scan line optimization via cost accumulation ha become very popular for stereo estimation in computer vision application and is often combined with a semi global cost integration strategy known a sgm this paper introduces this combination a a general and effective optimization technique it is the first time that this concept is applied to d medical image registration the presented algorithm sgm d employ a coarse to fine strategy and reduces the search space dimension for consecutive pyramid level by a fixed linear rate this allows it to handle large displacement to an extent that is required for clinical application in high dimensional data sgm d is evaluated in context of pulmonary motion analysis on the recently extended dir lab benchmark that provides ten d computed tomography ct image data set a well a ten challenging d ct scan pair from the copdgene study archive result show that both registration error a well a run time performance are very competitive with current state of the art method 
many computer vision algorithm employ subspace model to represent data many of these approach benefit from the ability to create an average or prototype for a set of subspace the most popular method in these situation is the karcher mean also known a the riemannian center of mass the prevalence of the karcher mean may lead some to assume that it provides the best average in all scenario however other subspace average that appear le frequently in the literature may be more appropriate for certain task the extrinsic manifold mean the l median and the flag mean are alternative average that can be substituted directly for the karcher mean in many application this paper evaluates the characteristic and performance of these four average on synthetic and real world data while the karcher mean generalizes the euclidean mean to the grassman manifold we show that the extrinsic manifold mean the l median and the flag mean behave more like median and are therefore more robust to the presence of outlier among the subspace being averaged we also show that while the karcher mean and l median are computed using iterative algorithm the extrinsic manifold mean and flag mean can be found analytically and are thus order of magnitude faster in practice finally we show that the flag mean is a generalization of the extrinsic manifold mean that permit subspace with different number of dimension to be averaged the result is a cookbook that map algorithm constraint and data property to the most appropriate subspace mean for a given application 
this paper proposes a new vectorial total variation prior vtv for color image different from existing vtvs our vtv named the decorrelated vectorial total variation prior d vtv measure the discrete gradient of the luminance component and that of the chrominance one in a separated manner which significantly reduces undesirable uneven color effect moreover a higher order generalization of the d vtv which we call the decorrelated vectorial total generalized variation prior d vtgv is also developed for avoiding the staircasing effect that accompanies the use of vtvs a noteworthy property of the d vt g v is that it enables u to efficiently minimize objective function involving it by a primal dual splitting method experimental result illustrate their utility 
current human in the loop fine grained visual categorization system depend on a predefined vocabulary of attribute and part usually determined by expert in this work we move away from that expert driven and attribute centric paradigm and present a novel interactive classification system that incorporates computer vision and perceptual similarity metric in a unified framework at test time user are asked to judge relative similarity between a query image and various set of image these general query do not require expert defined terminology and are applicable to other domain and basic level category enabling a flexible efficient and scalable system for fine grained categorization with human in the loop our system outperforms existing state of the art system for relevance feedback based image retrieval a well a interactive classification resulting in a reduction of up to in the average number of question needed to correctly classify an image 
given two image we want to predict which exhibit a particular visual attribute more than the other even when the two image are quite similar existing relative attribute method rely on global ranking function yet rarely will the visual cue relevant to a comparison be constant for all data nor will human perception of the attribute necessarily permit a global ordering to address these issue we propose a local learning approach for fine grained visual comparison given a novel pair of image we learn a local ranking model on the fly using only analogous training comparison we show how to identify these analogous pair using learned metric with result on three challenging datasets including a large newly curated dataset for fine grained comparison our method outperforms state of the art method for relative attribute prediction 
low cost rgb d imaging system such a kinect is widely utilized for dense d reconstruction however rgb d system generally suffers from two main problem the spatial resolution of the depth image is low the depth image often contains numerous hole where no depth measurement are available this can be due to bad infra red reflectance property of some object in the scene since the spatial resolution of the color image is generally higher than that of the depth image this paper introduces a new method to enhance the depth image captured by a moving rgb d system using the depth cue from the induced optical flow we not only fill the hole in the raw depth image but also recover fine detail of the imaged scene we address the problem of depth image enhancement by minimizing an energy functional in order to reduce the computational complexity we have treated the textured and homogeneous region in the color image differently experimental result on several rgb d sequence are provided to show the effectiveness of the proposed method 
the objective of this work is to accurately and efficiently detect configuration of one or more people in edited tv material such configuration often appear in standard arrangement due to cinematic style and we take advantage of this to provide scene context we make the following contribution first we introduce a new learnable context aware configuration model for detecting set of people in tv material that predicts the scale and location of each upper body in the configuration second we show that inference of the model can be solved globally and efficiently using dynamic programming and implement a maximum margin learning framework and third we show that the configuration model substantially outperforms a deformable part model dpm for predicting upper body location in video frame even when the dpm is equipped with the context of other upper body experiment are performed over two datasets the tv human interaction dataset and episode from four different tv show we also demonstrate the benefit of the model in recognizing interaction in tv show 
state of the art general purpose blind image quality assessment biqa model rely on example of distorted image and corresponding human opinion score to learn a regression function that map image feature to a quality score these type of model are considered opinion aware oa biqa model a large set of human scored training example is usually required to train a reliable oa biqa model however obtaining human opinion score through subjective testing is often expensive and time consuming it is therefore desirable to develop opinion free of biqa model that do not require human opinion score for training this paper proposes bliss blind learning of image quality using synthetic score bliss is a simple yet effective method for extending oa biqa model to of biqa model instead of training on human opinion score we propose to train biqa model on synthetic score derived from full reference fr iqa measure state of the art fr measure yield high correlation with human opinion score and can serve a approximation to human opinion score unsupervised rank aggregation is applied to combine different fr measure to generate a synthetic score which serf a a better gold standard extensive experiment on standard iqa datasets show that bliss significantly outperforms previous of biqa method and is comparable to state of the art oa biqa method 
in this paper we propose a novel labeling cost for multiview reconstruction existing approach use data term with specific weakness that are vulnerable to common challenge such a low textured region or specularities our new probabilistic method implicitly discard outlier and can be shown to become more exact the closer we get to the true object surface our approach achieves top result among all published method on the middlebury dino sparse dataset and also delivers accurate result on several other datasets with widely varying challenge for which it work in unchanged form 
this paper introduces a new color transfer method which is a process of transferring color of an image to match the color of another image of the same scene the color of a scene may vary from image to image because the photograph are taken at different time with different camera and under different camera setting to solve for a full nonlinear and nonparametric color mapping in the d rgb color space we propose a scattered point interpolation scheme using moving least square and strengthen it with a probabilistic modeling of the color transfer in the d color space to deal with mi alignment and noise experiment show the effectiveness of our method over previous color transfer method both quantitatively and qualitatively in addition our framework can be applied for various instance of color transfer such a transferring color between different camera model camera setting and illumination condition a well a for video color transfer 
we present a video object segmentation approach that extends the particle filter to a region based image representation image partition is considered part of the particle filter measurement which enriches the available information and lead to a re formulation of the particle filter the prediction step us a co clustering between the previous image object partition and a partition of the current one which allows u to tackle the evolution of non rigid structure particle are defined a union of region in the current image partition and their propagation is computed through a single co clustering the proposed technique is assessed on the segtrack dataset leading to satisfactory perceptual result and obtaining very competitive pixel error rate compared with the state of the art method 
we present a novel way to automatically summarize and represent the storyline of a tv episode by visualizing character interaction a a chart we also propose a scene detection method that lends itself well to generate over segmented scene which is used to partition the video the positioning of character line in the chart is formulated a an optimization problem which trade between the aesthetic and functionality of the chart using automatic person identification we present storygraphs for diverse tv series encompassing a total of episode we define quantitative criterion to evaluate storygraphs and also compare them against episode summary to evaluate their ability to provide an overview of the episode 
the presence of occluders significantly impact performance of system for object recognition however occlusion is typically treated a an unstructured source of noise and explicit model for occluders have lagged behind those for object appearance and shape in this paper we describe a hierarchical deformable part model for face detection and keypoint localization that explicitly model occlusion of part the proposed model structure make it possible to augment positive training data with large number of synthetically occluded instance this allows u to easily incorporate the statistic of occlusion pattern in a discriminatively trained model we test the model on several benchmark for keypoint localization including challenging set featuring significant occlusion we find that the addition of an explicit model of occlusion yield a system that outperforms existing approach in keypoint localization accuracy 
a probabilistic model allows u to reason about the world and make statistically optimal decision using bayesian decision theory however in practice the intractability of the decision problem force u to adopt simplistic loss function such a the loss or hamming loss and a result we make poor decision through map estimate or through low order marginal statistic in this work we investigate optimal decision making for more realistic loss function specifically we consider the popular intersection over union iou score used in image segmentation benchmark and show that it result in a hard combinatorial decision problem to make this problem tractable we propose a statistical approximation to the objective function a well a an approximate algorithm based on parametric linear programming we apply the algorithm on three benchmark datasets and obtain improved intersection over union score compared to maximum posterior marginal decision our work point out the difficulty of using realistic loss function with probabilistic computer vision model 
occlusion pose a significant difficulty for object recognition due to the combinatorial diversity of possible occlusion pattern we take a strongly supervised non parametric approach to modeling occlusion by learning deformable model with many local part mixture template using large quantity of synthetically generated training data this allows the model to learn the appearance of different occlusion pattern including figure ground cue such a the shape of occluding contour a well a the co occurrence statistic of occlusion between neighboring part the underlying part mixture structure also allows the model to capture coherence of object support mask between neighboring part and make compelling prediction of figure ground occluder segmentation we test the resulting model on human pose estimation under heavy occlusion and find it produce improved localization accuracy 
dictionary learning dl for sparse coding ha shown promising result in classification task while how to adaptively build the relationship between dictionary atom and class label is still an important open question the existing dictionary learning approach simply fix a dictionary atom to be either class specific or shared by all class beforehand ignoring that the relationship need to be updated during dl to address this issue in this paper we propose a novel latent dictionary learning ldl method to learn a discriminative dictionary and build it relationship to class label adaptively each dictionary atom is jointly learned with a latent vector which associate this atom to the representation of different class more specifically we introduce a latent representation model in which discrimination of the learned dictionary is exploited via minimizing the within class scatter of coding coefficient and the latent value weighted dictionary coherence the optimal solution is efficiently obtained by the proposed solving algorithm correspondingly a latent sparse representation based classifier is also presented experimental result demonstrate that our algorithm outperforms many recently proposed sparse representation and dictionary learning approach for action gender and face recognition 
image matching is one of the most challenging stage in d reconstruction which usually occupies half of computational cost and inaccurate matching may lead to failure of reconstruction therefore fast and accurate image matching is very crucial for d reconstruction in this paper we proposed a cascade hashing strategy to speed up the image matching in order to accelerate the image matching the proposed cascade hashing method is designed to be three layer structure hashing lookup hashing remapping and hashing ranking each layer adopts different measure and filtering strategy which is demonstrated to be le sensitive to noise extensive experiment show that image matching can be accelerated by our approach in hundred time than brute force matching even achieves ten time or more than kd tree based matching while retaining comparable accuracy 
group are the primary entity that make up a crowd understanding group level dynamic and property is thus scientifically important and practically useful in a wide range of application especially for crowd understanding in this study we show that fundamental group level property such a intra group stability and inter group conflict can be systematically quantified by visual descriptor this is made possible through learning a novel collective transition prior which lead to a robust approach for group segregation in public space from the prior we further devise a rich set of group property visual descriptor these descriptor are scene independent and can be effectively applied to public scene with variety of crowd density and distribution extensive experiment on hundred of public scene video clip demonstrate that such property descriptor are not only useful but also necessary for group state analysis and crowd scene understanding 
we present a stereo algorithm designed for speed and efficiency that us local slanted plane sweep to propose disparity hypothesis for a semi global matching algorithm our local plane hypothesis are derived from initial sparse feature correspondence followed by an iterative clustering step local plane sweep are then performed around each slanted plane to produce out of plane parallax and matching cost estimate a final global optimization stage implemented using semi global matching assigns each pixel to one of the local plane hypothesis by only exploring a small fraction of the whole disparity space volume our technique achieves significant speedup over previous algorithm and achieves state of the art accuracy on high resolution stereo pair of up to megapixels 
the capture of multiple image is a simple way to increase the chance of capturing a good photo with a light weight hand held camera for which the camera shake blur is typically a nuisance problem the naive approach of selecting the single best captured photo a output doe not take full advantage of all the observation conventional multi image blind deblurring method can take all observation a input but usually require the multiple image are well aligned however the multiple blurry image captured in the presence of camera shake are rarely free from mi alignment registering multiple blurry image is a challenging task due to the presence of blur while deblurring of multiple blurry image requires accurate alignment leading to an intrinsically coupled problem in this paper we propose a blind multi image restoration method which can achieve joint alignment non uniform deblurring together with resolution enhancement from multiple low quality image experiment on several real world image with comparison to some previous method validate the effectiveness of the proposed method 
long term modeling of background motion in video is an important and challenging problem used in numerous application such a segmentation and event recognition a major challenge in modeling the background from point trajectory lie in dealing with the variable length duration of trajectory which can be due to such factor a trajectory entering and leaving the frame or occlusion from different depth layer this work proposes an online method for background modeling of dynamic point trajectory via tracking of a linear subspace describing the background motion to cope with variability in trajectory duration we cast subspace tracking a an instance of subspace estimation under missing data using a least absolute deviation formulation to robustly estimate the background in the presence of arbitrary foreground motion relative to previous work our approach is very fast and scale to arbitrarily long video a our method process new frame sequentially a they arrive 
parallax handling is a challenging task for image stitching this paper present a local stitching method to handle parallax based on the observation that input image do not need to be perfectly aligned over the whole overlapping region for stitching instead they only need to be aligned in a way that there exists a local region where they can be seamlessly blended together we adopt a hybrid alignment model that combine homography and content preserving warping to provide flexibility for handling parallax and avoiding objectionable local distortion we then develop an efficient randomized algorithm to search for a homography which combined with content preserving warping allows for optimal stitching we predict how well a homography enables plausible stitching by finding a plausible seam and using the seam cost a the quality metric we develop a seam finding method that estimate a plausible seam from only roughly aligned image by considering both geometric alignment and image content we then pre align input image using the optimal homography and further use content preserving warping to locally refine the alignment we finally compose aligned image together using a standard seam cutting algorithm and a multi band blending algorithm our experiment show that our method can effectively stitch image with large parallax that are difficult for existing method 
many binary code embedding technique have been proposed for large scale approximate nearest neighbor search in computer vision recently product quantization that encodes the cluster index in each subspace ha been shown to provide impressive accuracy for nearest neighbor search in this paper we explore a simple question is it best to use all the bit budget for encoding a cluster index in each subspace we have found that a data point are located farther away from the center of their cluster the error of estimated distance among those point becomes larger to address this issue we propose a novel encoding scheme that distributes the available bit budget to encoding both the cluster index and the quantized distance between a point and it cluster center we also propose two different distance metric tailored to our encoding scheme we have tested our method against the state of the art technique on several well known benchmark and found that our method consistently improves the accuracy over other tested method this result is achieved mainly because our method accurately estimate distance between two data point with the new binary code and distance metric 
this paper pose object category detection in image a a type of d to d alignment problem utilizing the large quantity of d cad model that have been made publicly available online using the chair class a a running example we propose an exemplar based d category representation which can explicitly model chair of different style a well a the large variation in viewpoint we develop an approach to establish part based correspondence between d cad model and real photograph this is achieved by i representing each d model using a set of view dependent mid level visual element learned from synthesized view in a discriminative fashion ii carefully calibrating the individual element detector on a common dataset of negative image and iii matching visual element to the test image allowing for small mutual deformation but preserving the viewpoint and style constraint we demonstrate the ability of our system to align d model with d object in the challenging pascal voc image which depict a wide variety of chair in complex scene 
this paper introduces a novel image representation capturing feature dependency through the mining of meaningful combination of visual feature this representation lead to a compact and discriminative encoding of image that can be used for image classification object detection or object recognition the method relies on i multiple random projection of the input space followed by local binarization of projected histogram encoded a set of item and ii the representation of image a histogram of pattern set hop the approach is validated on four publicly available datasets daimler pedestrian oxford flower kth texture and pascal voc allowing comparison with many recent approach the proposed image representation reach state of the art performance on each one of these datasets 
this paper tackle the problem of spotting a set of sign occuring in video with sequence of sign to achieve this we propose to model the spatio temporal signature of a sign using an extension of sequential pattern that contain temporal interval called sequential interval pattern sip we then propose a novel multi class classifier that organises different sequential interval pattern in a hierarchical tree structure called a hierarchical sip tree hsp tree this allows one to exploit any subsequence sharing that exists between different sip of different class multiple tree are then combined together into a forest of hsp tree resulting in a strong classifier that can be used to spot sign we then show how the hsp forest can be used to spot sequence of sign that occur in an input video we have evaluated the method on both concatenated sequence of isolated sign and continuous sign sequence we also show that the proposed method is superior in robustness and accuracy to a state of the art sign recogniser when applied to spotting a sequence of sign 
spectral clustering requires robust and meaningful affinity graph a input in order to form cluster with desired structure that can well support human intuition to construct such affinity graph is non trivial due to the ambiguity and uncertainty inherent in the raw data in contrast to most existing clustering method that typically employ all available feature to construct affinity matrix with the euclidean distance which is often not an accurate representation of the underlying data structure we propose a novel unsupervised approach to generating more robust affinity graph via identifying and exploiting discriminative feature for improving spectral clustering specifically our model is capable of capturing and combining subtle similarity information distributed over discriminative feature subspace for more accurately revealing the latent data distribution and thereby leading to improved data clustering especially with heterogeneous data source we demonstrate the efficacy of the proposed approach on challenging image and video datasets 
in this paper we introduce a bilateral consistency metric on the surface camera scam for light field stereo matching to handle significant occlusion the concept of scam is used to model angular radiance distribution with respect to a d point our bilateral consistency metric is used to indicate the probability of occlusion by analyzing the scam we further show how to distinguish between on surface and free space textured and non textured and lambertian and specular through bilateral scam analysis to speed up the matching process we apply the edge preserving guided filter on the consistency disparity curve experimental result show that our technique outperforms both the state of the art and the recent light field stereo matching method especially near occlusion boundary 
this paper proposes a framework for recognizing complex human activity in video our method describes human activity in a hierarchical discriminative model that operates at three semantic level at the lower level body pose are encoded in a representative but discriminative pose dictionary at the intermediate level encoded pose span a space where simple human action are composed at the highest level our model capture temporal and spatial composition of action into complex human activity our human activity classifier simultaneously model which body part are relevant to the action of interest a well a their appearance and composition using a discriminative approach by formulating model learning in a max margin framework our approach achieves powerful multi class discrimination while providing useful annotation at the intermediate semantic level we show how our hierarchical compositional model provides natural handling of occlusion to evaluate the effectiveness of our proposed framework we introduce a new dataset of composed human activity we provide empirical evidence that our method achieves state of the art activity classification performance on several benchmark datasets 
this paper proposes a novel parametric warp which is a spatial combination of a projective transformation and a similarity transformation given the projective transformation relating two input image based on an analysis of the projective transformation our method smoothly extrapolates the projective transformation of the overlapping region into the non overlapping region and the resultant warp gradually change from projective to similarity across the image the proposed warp ha the strength of both projective and similarity warp it provides good alignment accuracy a projective warp while preserving the perspective of individual image a similarity warp it can also be combined with more advanced local warp based alignment method such a the a projective a possible warp for better alignment accuracy with the proposed warp the field of view can be extended by stitching image with le projective distortion stretched shape and enlarged size 
in this paper we present a method for estimating articulated human pose in video we cast this a an optimization problem defined on body part with spatio temporal link between them the resulting formulation is unfortunately intractable and previous approach only provide approximate solution although such method perform well on certain body part e g head their performance on lower arm i e elbow and wrist remains poor we present a new approximate scheme with two step dedicated to pose estimation first our approach take into account temporal link with subsequent frame for the le certain part namely elbow and wrist second our method decomposes pose into limb generates limb sequence across time and recomposes pose by mixing these body part sequence we introduce a new dataset pose in the wild which is more challenging than the existing one with sequence containing background clutter occlusion and severe camera motion we experimentally compare our method with recent approach on this new dataset a well a on two other benchmark datasets and show significant improvement 
we propose an approach to reconstructing tree structure that evolve over time in d image and d image stack such a neuronal axon or plant branch instead of reconstructing structure in each image independently we do so for all image simultaneously to take advantage of temporal consistency constraint we show that this problem can be formulated a a quadratic mixed integer program and solved efficiently the outcome of our approach is a framework that provides substantial improvement in reconstruction over traditional single time instance formulation furthermore an added benefit of our approach is the ability to automatically detect place where significant change have occurred over time which is challenging when considering large amount of data 
a recent trend of research ha shown how contextual information related to an action such a a scene or object can enhance the accuracy of human action recognition system however using context to improve unsupervised human action clustering ha never been considered before and cannot be achieved using existing clustering method to solve this problem we introduce a novel general purpose algorithm dual assignment k mean dakm which is uniquely capable of performing two co occurring clustering task simultaneously while exploiting the correlation information to enhance both clustering furthermore we describe a spectral extension of dakm sdakm for better performance on realistic data extensive experiment on synthetic data and on three realistic human action datasets with scene context show that dakm sdakm can significantly outperform the state of the art clustering method by taking into account the contextual relationship between action and scene 
we propose to decompose the fine grained human activity analysis problem into two sequential task with increasing granularity firstly we infer the coarse interaction status i e which object is being manipulated and where it is knowing that the major challenge is frequent mutual occlusion during manipulation we propose an interaction tracking framework in which hand object position and interaction status are jointly tracked by explicitly modeling the contextual information between mutual occlusion and interaction status secondly the inferred hand object position and interaction status are utilized to provide more compact feature pooling by effectively pruning large number of motion feature from irrelevant spatio temporal position and discriminative action detection by a granularity fusion strategy comprehensive experiment on two challenging fine grained activity datasets i e cooking action show that the proposed framework achieves high accuracy robustness in tracking multiple mutually occluded hand object during manipulation a well a significant performance improvement on fine grained action detection over state of the art method 
we propose an online solution to non rigid structure from motion that performs camera pose and d shape estimation of highly deformable surface on a frame by frame basis our method model non rigid deformation a a linear combination of some mode shape obtained using modal analysis from continuum mechanic the shape is first discretized into linear elastic triangle modelled by mean of finite element which are used to pose the force balance equation for an undamped free vibration model the shape basis computation come down to solving an eigenvalue problem without the requirement of a learning step the camera pose and time varying weight that define the shape at each frame are then estimated on the fly in an online fashion using bundle adjustment over a sliding window of image frame the result is a low computational cost method that can run sequentially in real time we show experimental result on synthetic sequence with ground truth d data and real video for different scenario ranging from sparse to dense scene our system exhibit a good trade off between accuracy and computational budget it can handle missing data and performs favourably compared to competing method 
when one record a video image sequence through a transparent medium e g glass the image is often a superposition of a transmitted layer scene behind the medium and a reflected layer recovering the two layer from such image seems to be a highly ill posed problem since the number of unknown to recover is twice a many a the given measurement in this paper we propose a robust method to separate these two layer from multiple image which exploit the correlation of the transmitted layer across multiple image and the sparsity and independence of the gradient field of the two layer a novel augmented lagrangian multiplier based algorithm is designed to efficiently and effectively solve the decomposition problem the experimental result on both simulated and real data demonstrate the superior performance of the proposed method over the state of the art in term of accuracy and simplicity 
over the past year multiple instance learning mil ha proven to be an effective framework for learning with weakly labeled data application of mil to object detection however were limited to handling the uncertainty of manual annotation in this paper we propose a new mil method for object detection that is capable of handling the noisier automatically obtained annotation our approach consists in first obtaining confidence estimate over the label space and second incorporating these estimate within a new boosting procedure we demonstrate the efficiency of our procedure on two detection task namely horse detection and pedestrian detection where the training data is primarily annotated by a coarse area of interest detector we show dramatic improvement over existing mil method in both case we demonstrate that an efficient appearance model can be learned using our approach 
sparse coding is a widely involved technique in computer vision however the expensive computational cost can hamper it application typically when the codebook size must be limited due to concern on running time in this paper we study a special case of sparse coding in which the codebook is a cartesian product of two subcodebooks we present algorithm to decompose this sparse coding problem into smaller subproblems which can be separately solved our solution named a product sparse coding psc reduces the time complexity from o k to o root k in the codebook size k in practice this can be x faster than standard sparse coding in experiment we demonstrate the efficiency and quality of this method on the application of image classification and image retrieval 
in this paper we propose a novel approach of learning mid level filter from automatically discovered patch cluster for person re identification it is well motivated by our study on what are good filter for person re identification our mid level filter are discriminatively learned for identifying specific visual pattern and distinguishing person and have good cross view invariance first local patch are qualitatively measured and classified with their discriminative power discriminative and representative patch are collected for filter learning second patch cluster with coherent appearance are obtained by pruning hierarchical clustering tree and a simple but effective cross view training strategy is proposed to learn filter that are view invariant and discriminative third filter response are integrated with patch matching score in ranksvm training the effectiveness of our approach is validated on the viper dataset and the cuhk dataset the learned mid level feature are complementary to existing handcrafted low level feature and improve the best rank matching rate on the viper dataset by 
this paper present a new discriminative deep metric learning ddml method for face verification in the wild different from existing metric learning based face verification method which aim to learn a mahalanobis distance metric to maximize the inter class variation and minimize the intra class variation simultaneously the proposed ddml train a deep neural network which learns a set of hierarchical nonlinear transformation to project face pair into the same feature subspace under which the distance of each positive face pair is le than a smaller threshold and that of each negative pair is higher than a larger threshold respectively so that discriminative information can be exploited in the deep network our method achieves very competitive face verification performance on the widely used lfw and youtube face ytf datasets 
this paper proposes to learn a set of high level feature representation through deep learning referred to a deep hidden identity feature deepid for face verification we argue that deepid can be effectively learned through challenging multi class face identification task whilst they can be generalized to other task such a verification and new identity unseen in the training set moreover the generalization capability of deepid increase a more face class are to be predicted at training deepid feature are taken from the last hidden layer neuron activation of deep convolutional network convnets when learned a classifier to recognize about face identity in the training set and configured to keep reducing the neuron number along the feature extraction hierarchy these deep convnets gradually form compact identity related feature in the top layer with only a small number of hidden neuron the proposed feature are extracted from various face region to form complementary and over complete representation any state of the art classifier can be learned based on these high level representation for face verification verification accuracy on lfw is achieved with only weakly aligned face 
object and structure within man made environment typically exhibit a high degree of organization in the form of orthogonal and parallel plane traditional approach to scene representation exploit this phenomenon via the somewhat restrictive assumption that every plane is perpendicular to one of the ax of a single coordinate system known a the manhattan world model this assumption is widely used in computer vision and robotics the complexity of many real world scene however necessitates a more flexible model we propose a novel probabilistic model that describes the world a a mixture of manhattan frame each frame defines a different orthogonal coordinate system this result in a more expressive model that still exploit the orthogonality constraint we propose an adaptive markov chain monte carlo sampling algorithm with metropolis hastings split merge move that utilizes the geometry of the unit sphere we demonstrate the versatility of our mixture of manhattan frame model by describing complex scene using depth image of indoor scene a well a aerial lidar measurement of an urban center additionally we show that the model lends itself to focal length calibration of depth camera and to plane segmentation 
in this paper we present the latent regression forest lrf a novel framework for real time d hand pose estimation from a single depth image in contrast to prior forest based method which take dense pixel a input classify them independently and then estimate joint position afterwards our method can be considered a a structured coarse to fine search starting from the centre of mass of a point cloud until locating all the skeletal joint the searching process is guided by a learnt latent tree model which reflects the hierarchical topology of the hand our main contribution can be summarised a follows i learning the topology of the hand in an unsupervised data driven manner ii a new forest based discriminative framework for structured search in image a well a an error regression step to avoid error accumulation iii a new multi view hand pose dataset containing k annotated image from different subject our experiment show that the lrf out performs state of the art method in both accuracy and efficiency 
a novel visual tracking algorithm using patch based appearance model is proposed in this paper we first divide the bounding box of a target object into multiple patch and then select only pertinent patch which occur repeatedly near the center of the bounding box to construct the foreground appearance model we also divide the input image into non overlapping block construct a background model at each block location and integrate these background model for tracking using the appearance model we obtain an accurate foreground probability map finally we estimate the optimal object position by maximizing the likelihood which is obtained by convolving the foreground probability map with the pertinence mask experimental result demonstrate that the proposed algorithm outperforms state of the art tracking algorithm significantly in term of center position error and success rate 
identifying subject with variation caused by pose is one of the most challenging task in face recognition since the difference in appearance caused by pose may be even larger than the difference due to identity inspired by the observation that pose variation change non linearly but smoothly we propose to learn pose robust feature by modeling the complex non linear transform from the non frontal face image to frontal one through a deep network in a progressive way termed a stacked progressive auto encoders spae specifically each shallow progressive auto encoder of the stacked network is designed to map the face image at large pose to a virtual view at smaller one and meanwhile keep those image already at smaller pose unchanged then stacking multiple these shallow auto encoders can convert non frontal face image to frontal one progressively which mean the pose variation are narrowed down to zero step by step a a result the output of the topmost hidden layer of the stacked network contain very small pose variation which can be used a the pose robust feature for face recognition an additional attractiveness of the proposed method is that no pose estimation is needed for the test image the proposed method is evaluated on two datasets with pose variation i e multipie and feret datasets and the experimental result demonstrate the superiority of our method to the existing work especially to those d one 
recent study on alzheimer s disease ad or it prodromal stage mild cognitive impairment mci diagnosis presented that the task of identifying brain disease status and predicting clinical score based on neuroimaging feature were highly related to each other however these task were often conducted independently in the previous study regarding the feature selection to our best knowledge most of the previous work considered a loss function defined a an element wise difference between the target value and the predicted one in this paper we consider the problem of joint regression and classification for ad mci diagnosis and propose a novel matrix similarity based loss function that us high level information inherent in the target response matrix and imposes the information to be preserved in the predicted response matrix the newly devised loss function is combined with a group lasso method for joint feature selection across task i e clinical score prediction and disease status identification we conducted experiment on the alzheimer s disease neuroimaging initiative adni dataset and showed that the newly devised loss function wa effective to enhance the performance of both clinical score prediction and disease status identification outperforming the state of the art method 
in classification of object substantial work ha gone into improving the low level representation of an image by considering various aspect such a different feature a number of feature pooling and coding technique and considering different kernel unlike these work in this paper we propose to enhance the semantic representation of an image we aim to learn the most important visual component of an image and how they interact in order to classify the object correctly to achieve our objective we propose a new latent svm model for category level object classification starting from image level annotation we jointly learn the object class and it context in term of spatial location where and appearance what furthermore to regularize the complexity of the model we learn the spatial and co occurrence relation between adjacent region such that unlikely configuration are penalized experimental result demonstrate that the proposed method can consistently enhance result on the challenging pascal voc dataset in term of classification and weakly supervised detection we also show how semantic representation can be exploited for finding similar content 
state of the art multi view stereo mv algorithm deliver dense depth map or complex mesh with very high detail and redundancy over regular surface in turn our interest lie in an approximate but light weight method that is better to consider for large scale application such a urban scene reconstruction from ground based image we present a novel approach for producing dense reconstruction from multiple image and from the underlying sparse structure from motion sfm data in an efficient way to overcome the problem of sfm sparsity and textureless area we assume piecewise planarity of man made scene and exploit both sparse visibility and a fast over segmentation of the image reconstruction is formulated a an energy driven multi view plane assignment problem which we solve jointly over superpixels from all view while avoiding expensive photoconsistency computation the resulting planar primitive defined by detailed superpixel boundary are computed in about second per image 
in this paper we present an attributed grammar for parsing man made outdoor scene into semantic surface and recovering it d model simultaneously the grammar take superpixels a it terminal node and use five production rule to generate the scene into a hierarchical parse graph each graph node actually correlate with a surface or a composite of surface in the d world or the d image they are described by attribute for the global scene model e g focal length vanishing point or the surface property e g surface normal contact line with other surface and relative spatial location etc each production rule is associated with some equation that constraint the attribute of the parent node and those of their child node given an input image our goal is to construct a hierarchical parse graph by recursively applying the five grammar rule while preserving the attribute constraint we develop an effective top down bottom up cluster sampling procedure which can explore this constrained space efficiently we evaluate our method on both public benchmark and newly built datasets and achieve state of the art performance in term of layout estimation and region segmentation we also demonstrate that our method is able to recover detailed d model with relaxed manhattan structure which clearly advance the state of the art of singleview d reconstruction 
we present a novel solution to compute the relative pose of a generalized camera existing solution are either not general have too high computational complexity or require too many correspondence which impedes an efficient or accurate usage within ransac scheme we factorize the problem a a low dimensional iterative optimization over relative rotation only directly derived from well known epipolar constraint common generalized camera often consist of camera cluster and give rise to omni directional landmark observation we prove that our iterative scheme performs well in such practically relevant situation eventually resulting in computational efficiency similar to linear solver and accuracy close to bundle adjustment while using le correspondence experiment on both virtual and real multi camera system prove superior overall performance for robust real time multi camera motion estimation 
in this paper we introduce a novel technique to automatically detect salient region of an image via high dimensional color transform our main idea is to represent a saliency map of an image a a linear combination of high dimensional color space where salient region and background can be distinctively separated this is based on an observation that salient region often have distinctive color compared to the background in human perception but human perception is often complicated and highly nonlinear by mapping a low dimensional rgb color to a feature vector in a high dimensional color space we show that we can linearly separate the salient region from the background by finding an optimal linear combination of color coefficient in the high dimensional color space our high dimensional color space incorporates multiple color representation including rgb cielab hsv and with gamma correction to enrich it representative power our experimental result on three benchmark datasets show that our technique is effective and it is computationally efficient in comparison to previous state of the art technique 
we propose binary range sample feature in depth it is based on test and achieves reasonable invariance with respect to possible change in scale viewpoint and background it is robust to occlusion and data corruption a well the descriptor work in a high speed thanks to it binary property working together with standard learning algorithm the proposed descriptor achieves state of theart result on benchmark datasets in our experiment impressively short running time is also yielded 
this paper address the problem of face alignment for a single image we show how an ensemble of regression tree can be used to estimate the face s landmark position directly from a sparse subset of pixel intensity achieving super realtime performance with high quality prediction we present a general framework based on gradient boosting for learning an ensemble of regression tree that optimizes the sum of square error loss and naturally handle missing or partially labelled data we show how using appropriate prior exploiting the structure of image data help with efficient feature selection different regularization strategy and it importance to combat overfitting are also investigated in addition we analyse the effect of the quantity of training data on the accuracy of the prediction and explore the effect of data augmentation using synthesized data 
we study the problem of understanding object in detail intended a recognizing a wide array of fine grained object attribute to this end we introduce a dataset of airplane annotated in detail with part and their attribute leveraging image donated by airplane spotter and crowdsourcing both the design and collection of the detailed annotation we provide a number of insight that should help researcher interested in designing fine grained datasets for other basic level category we show that the collected data can be used to study the relation between part detection and attribute prediction by diagnosing the performance of classifier that pool information from different part of an object we note that the prediction of certain attribute can benefit substantially from accurate part detection we also show that differently from previous result in object detection employing a large number of part template can improve detection accuracy at the expense of detection speed we finally propose a coarse to fine approach to speed up detection through a hierarchical cascade algorithm 
this paper present a new framework for human activity recognition from video sequence captured by a depth camera we cluster hypersurface normal in a depth sequence to form the polynormal which is used to jointly characterize the local motion and shape information in order to globally capture the spatial and temporal order an adaptive spatio temporal pyramid is introduced to subdivide a depth video into a set of space time grid we then propose a novel scheme of aggregating the low level polynormals into the super normal vector snv which can be seen a a simplified version of the fisher kernel representation in the extensive experiment we achieve classification result superior to all previous published result on the four public benchmark datasets i e msraction d msrdailyactivity d msrgesture d and msractionpairs d 
this paper present a method for acquiring dense nonrigid shape and deformation from a single monocular depth sensor we focus on modeling the human hand and assume that a single rough template model is available we combine and extend existing work on model based tracking subdivision surface fitting and mesh deformation to acquire detailed hand model from a few a frame of depth data we propose an objective that measure the error of fit between each sampled data point and a continuous model surface defined by a rigged control mesh and us a rigid a possible arap regularizers to cleanly separate the model and template geometry a key contribution is our use of a smooth model based on subdivision surface that allows simultaneous optimization over both correspondence and model parameter this avoids the use of iterated closest point icp algorithm which often lead to slow convergence automatic initialization is obtained using a regression forest trained to infer approximate correspondence experiment show that the resulting mesh model the user s hand shape more accurately than just adapting the shape parameter of the skeleton and that the retargeted skeleton accurately model the user s articulation we investigate the effect of various modeling choice and show the benefit of using subdivision surface and arap regularization 
most modern object tracker combine a motion prior with sliding window detection using binary classifier that predict the presence of the target object based on histogram feature although the accuracy of such tracker is generally very good they are often impractical because of their high computational requirement to resolve this problem the paper present a new approach that limit the computational cost of tracker by ignoring feature in image region that after inspecting a few feature are unlikely to contain the target object to this end we derive an upper bound on the probability that a location is most likely to contain the target object and we ignore feature in location for which this upper bound is small we demonstrate the effectiveness of our new approach in experiment with model free and model based tracker that use linear model in combination with hog feature the result of our experiment demonstrate that our approach allows u to reduce the average number of inspected feature by up to without affecting the accuracy of the tracker 
we investigate an inhomogeneous version of the frame filter random field and maximum entropy model and apply it to modeling object pattern the inhomogeneous frame is a non stationary markov random field model that reproduces the observed marginal distribution or statistic of filter response at all the different location scale and orientation our experiment show that the inhomogeneous frame model is capable of generating a wide variety of object pattern in natural image we then propose a sparsified version of the inhomogeneous frame model where the model reproduces observed statistical property of filter response at a small number of selected location scale and orientation we propose to select these location scale and orientation by a shared sparse coding scheme and we explore the connection between the sparse frame model and the linear additive sparse coding model our experiment show that it is possible to learn sparse frame model in unsupervised fashion and the learned model are useful for object classification 
the notion of relative attribute a introduced by parikh and grauman iccv provides an appealing way of comparing two image based on their visual property or attribute such a smiling for face image naturalness for outdoor image etc for learning such attribute a ranking svm based formulation wa proposed that us globally represented pair of annotated image in this paper we extend this idea towards learning relative attribute using local part that are shared across category first instead of using a global representation we introduce a part based representation combining a pair of image that specifically compare corresponding part then with each part we associate a locally adaptive significance coefficient that represents it discriminative ability with respect to a particular attribute for each attribute the significance coefficient are learned simultaneously with a max margin ranking model in an iterative manner compared to the baseline method the new method is shown to achieve significant improvement in relative attribute prediction accuracy additionally it is also shown to improve relative feedback based interactive image search 
this paper proposes a novel mean field based chamfer template matching method in our method each template is represented a a field model and matching a template with an input image is formulated a estimation of a maximum of posteriori in the field model variational approach is then adopted to approximate the estimation the proposed method wa applied for two different variant of chamfer template matching and evaluated through the task of object detection experimental result on benchmark datasets including ethzshapeclass and inriahorse have shown that the proposed method could significantly improve the accuracy of template matching while not sacrificing much of the efficiency comparison with other recent template matching algorithm have also shown the robustness of the proposed method 
labeling large scale datasets with very accurate object segmentation is an elaborate task that requires a high degree of quality control and a budget of ten or hundred of thousand of dollar thus developing solution that can automatically perform the labeling given only weak supervision is key to reduce this cost in this paper we show how to exploit d information to automatically generate very accurate object segmentation given annotated d bounding box we formulate the problem a the one of inference in a binary markov random field which exploit appearance model stereo and or noisy point cloud a repository of d cad model a well a topological constraint we demonstrate the effectiveness of our approach in the context of autonomous driving and show that we can segment car with the accuracy of intersection over union performing a well a highly recommended mturkers 
registering or more range scan is a fundamental problem with application to d modeling while this problem is well addressed by existing technique such a icp when the view overlap significantly at a good initialization no satisfactory solution exists for wide baseline registration we propose here a novel approach which leverage contour coherence and allows u to align two wide baseline range scan with limited overlap from a poor initialization inspired by icp we maximize the contour coherence by building robust corresponding pair on apparent contour and minimizing their distance in an iterative fashion we use the contour coherence under a multi view rigid registration framework and this enables the reconstruction of accurate and complete d model from a few a frame we further extend it to handle articulation and this allows u to model articulated object such a human body experimental result on both synthetic and real data demonstrate the effectiveness and robustness of our contour coherence based registration approach to wide baseline range scan and to d modeling 
we present a nonrigid shape matching technique for establishing correspondence of incomplete d surface that exhibit intrinsic reflectional symmetry the key for solving the symmetry ambiguity problem is to use a point wise local mesh descriptor that ha orientation and is thus sensitive to local reflectional symmetry e g discriminating the left hand and the right hand we devise a way to compute the descriptor orientation by taking the gradient of a scalar field called the average diffusion distance add because add is smoothly defined on a surface invariant under isometry scale and robust to topological error the robustness of the descriptor to non rigid deformation is improved in addition we propose a graph matching algorithm called iterative spectral relaxation which combine spectral embedding and spectral graph matching this formulation allows u to define pairwise constraint in a scale invariant manner from k nearest neighbor local pair such that non isometric deformation can be robustly handled experimental result show that our method can match challenging surface with global intrinsic symmetry data incompleteness and non isometric deformation 
we present a novel method for automatic vanishing point detection based on primal and dual point alignment detection the very same point alignment detection algorithm is used twice first in the image domain to group line segment endpoint into more precise line second it is used in the dual domain where converging line become aligned point the use of the recently introduced pclines dual space and a robust point alignment detector lead to a very accurate algorithm experimental result on two public standard datasets show that our method significantly advance the state of the art in the manhattan world scenario while producing state of the art performance in non manhattan scene 
we tackle stationary crowd analysis in this paper which is similarly important a modeling mobile group in crowd scene and find many application in surveillance our key contribution is to propose a robust algorithm of estimating how long a foreground pixel becomes stationary it is much more challenging than only subtracting background because failure at a single frame due to local movement of object lighting variation and occlusion could lead to large error on stationary time estimation to accomplish decent result sparse constraint along spatial and temporal dimension are jointly added by mixed partial to shape a d stationary time map it is formulated a a l optimization problem besides background subtraction it distinguishes among different foreground object which are close or overlapped in the spatio temporal space by using a locally shared foreground codebook the proposed technology are used to detect four type of stationary group activity and analyze crowd scene structure we provide the first public benchmark dataset for stationary time estimation and stationary group analysis 
we present a novel object recognition framework based on multiple figure ground hypothesis with a large object spatial support generated by bottom up process and mid level cue in an unsupervised manner we exploit the benefit of regression for discriminating segment category and quality where a regressor is trained to each category using the overlapping observation between each figure ground segment hypothesis and the ground truth of the target category in an image object recognition is achieved by maximizing a submodular objective function which maximizes the similarity between the selected segment i e facility location and their group element i e client penalizes the number of selected segment and more importantly encourages the consistency of object category corresponding to maximum regression value from different category specific regressors for the selected segment the proposed framework achieves impressive recognition result on three benchmark datasets including pascal voc caltech and ethz shape 
many learning problem in computer vision can be posed a structured prediction problem where the input and output instance are structured object such a tree graph or string rather than single label or scalar kernel method such a structured support vector machine twin gaussian process tgp structured gaussian process and vector valued reproducing kernel hilbert space rkhs offer powerful way to perform learning and inference over these domain positive definite kernel function allow u to quantitatively capture similarity between a pair of instance over these arbitrary domain a poor choice of the kernel function which decides the rkhs feature space often result in poor performance automatic kernel selection method have been developed but have focused only on kernel on the input domain i e one way in this work we propose a novel and efficient algorithm for learning kernel function simultaneously on both input and output domain we introduce the idea of learning polynomial kernel transformation and call this method simultaneous twin kernel learning stkl stkl can learn arbitrary but continuous kernel function including one way kernel learning a a special case we formulate this problem for learning covariance kernel of twin gaussian process our experimental evaluation using learned kernel on synthetic and several real world datasets demonstrate consistent improvement in performance of tgp s 
it ha long been recognized that one of the fundamental difficulty in theestimation of two view epipolar geometry is the capability of handling outlier in this paper we develop a fast and tractable algorithm that maximizes the number of inliers under the assumption of a purely translating camera compared to classical random sampling method our approach is guaranteed to compute the optimal solution of a cost function based on reprojection error and it ha better time complexity the performance is in fact independent of the inlier outlier ratio of the data this open up for a more reliable approach to robust ego motion estimation our basic translation estimator can be embedded into a system that computes the full camera rotation we demonstrate the applicability in several difficult setting with large amount of outlier it turn out to be particularly well suited for small rotation and rotation around a known axis which is the case for cellular phone where the gravitation axis can be measured experimental result show that compared to standard ransac method based on minimal solver ouralgorithm produce more accurate estimate in the presence of large outlier ratio 
image taken in low light condition with handheld camera are often blurry due to the required long exposure time although significant progress ha been made recently on image deblurring state of the art approach often fail on low light image a these image do not contain a sufficient number of salient feature that deblurring method rely on on the other hand light streak are common phenomenon in low light image that contain rich blur information but have not been extensively explored in previous approach in this work we propose a new method that utilizes light streak to help deblur low light image we introduce a non linear blur model that explicitly model light streak and their underlying light source and pose them a constraint for estimating the blur kernel in an optimization framework our method also automatically detects useful light streak in the input image experimental result show that our approach obtains good result on challenging real world example that no other method could achieve before 
we describe a new approach for generating regular speed low frame rate lfr video from a high frame rate hfr input while preserving the important moment in the original we call this time mapping a time based analogy to high dynamic range to low dynamic range spatial tone mapping our approach make these contribution a robust space time saliency method for evaluating visual importance a re timing technique to temporally resample based on frame importance and temporal filter to enhance the rendering of salient motion result of our space time saliency method on a benchmark dataset show it is state of the art in addition the benefit of our approach to hfr to lfr time mapping over more direct method are demonstrated in a user study 
in this work we use loopy part model to segment ensemble of organ in medical image each organ s shape is represented a a cyclic graph while shape consistency is enforced through inter shape connection our contribution are two fold firstly we use an efficient decomposition coordination algorithm to solve the resulting optimization problem we decompose the model s graph into a set of open chain structured graph each of which is efficiently optimized using dynamic programming with generalized distance transforms we use the alternating direction method of multiplier admm to fix the potential inconsistency of the individual solution and show that admm yield substantially faster convergence than plain dual decomposition based method secondly we employ structured prediction to encompass loss function that better reflect the performance criterion used in medical image segmentation by using the mean contour distance mcd a a structured loss during training we obtain clear test time performance gain we demonstrate the merit of exact and efficient inference with rich structured model in a large x ray image segmentation benchmark where we obtain systematic improvement over the current state of the art 
in this paper we propose a robust method for visual tracking relying on mean shift sparse coding and spatial pyramid firstly we extend the original mean shift approach to handle orientation space and scale space and name this new method a mean transform the mean transform method estimate the motion including the location orientation and scale of the interested object window simultaneously and effectively secondly a pixel wise dense patch sampling technique and a region wise trivial template designing scheme are introduced which enable our approach to run very accurately and efficiently in addition instead of using either holistic representation or local representation only we apply spatial pyramid by combining these two representation into our approach to deal with partial occlusion problem robustly observed from the experimental result our approach outperforms state of the art method in many benchmark sequence 
saliency prediction typically relies on hand crafted multiscale feature that are combined in different way to form a master saliency map which encodes local image conspicuity recent improvement to the state of the art on standard benchmark such a mit have been achieved mostly by incrementally adding more and more hand tuned feature such a car or face detector to existing model in contrast we here follow an entirely automatic data driven approach that performs a large scale search for optimal feature we identify those instance of a richly parameterized bio inspired model family hierarchical neuromorphic network that successfully predict image saliency because of the high dimensionality of this parameter space we use automated hyperparameter optimization to efficiently guide the search the optimal blend of such multilayer feature combined with a simple linear classifier achieves excellent performance on several image saliency benchmark our model outperform the state of the art on mit on which feature and classifier are learned without additional training these model generalize well to two other image saliency data set toronto and nusef despite their different image content finally our algorithm score best of all the model evaluated to date on the mit saliency challenge which us a hidden test set to facilitate an unbiased comparison 
image based classification of histology section play an important role in predicting clinical outcome however this task is very challenging due to the presence of large technical variation e g fixation staining and biological heterogeneity e g cell type cell state in the field of biomedical imaging for the purpose of visualization and or quantification different stain are typically used for different target of interest e g cellular subcellular event which generates multi spectrum data image through various type of microscope and a a result provides the possibility of learning biological component specific feature by exploiting multispectral information we propose a multispectral feature learning model that automatically learns a set of convolution filter bank from separate spectrum to efficiently discover the intrinsic tissue morphometric signature based on convolutional sparse coding csc the learned feature representation are then aggregated through the spatial pyramid matching framework spm and finally classified using a linear svm the proposed system ha been evaluated using two large scale tumor cohort collected from the cancer genome atlas tcga experimental result show that the proposed model outperforms system utilizing sparse coding for unsupervised feature learning e g psd spm is competitive with system built upon feature with biological prior knowledge e g smlspm 
many computer vision problem require optimization of binary non submodular energy we propose a general optimization framework based on local submodular approximation lsa unlike standard lp relaxation method that linearize the whole energy globally our approach iteratively approximates the energy locally on the other hand unlike standard local optimization method e g gradient descent or projection technique we use non linear submodular approximation and optimize them without leaving the domain of integer solution we discus two specific lsa algorithm based on trust region and auxiliary function principle lsa tr and lsa aux these method obtain state of the art result on a wide range of application outperforming many standard technique such a lbp qpbo and trws while our paper is focused on pairwise energy our idea extend to higher order problem the code is available online 
scene recognition is a basic task towards image understanding spatial pyramid matching spm ha been shown to be an efficient solution for spatial context modeling in this paper we introduce an alternative approach orientational pyramid matching opm for orientational context modeling our approach is motivated by the observation that the d orientation of object are a crucial factor to discriminate indoor scene the novelty lie in that opm us the d orientation to form the pyramid and produce the pooling region which is unlike spm that us the spatial position to form the pyramid experimental result on challenging scene classification task show that opm achieves the performance comparable with spm and that opm and spm make complementary contribution so that their combination give the state of the art performance 
interaction between moving target often provide discriminative clue for multiple target tracking mtt though many existing approach ignore such interaction due to difficulty in effectively handling them in this paper we model interaction between neighbor target by pair wise motion context and further encode such context into the global association optimization to solve the resulting global non convex maximization we propose an effective and efficient power iteration framework this solution enjoys two advantage for mtt first it allows u to combine the global energy accumulated from individual trajectory and the between trajectory interaction energy into a united optimization which can be solved by the proposed power iteration algorithm second the framework is flexible to accommodate various type of pairwise context model and we in fact studied two different context model in this paper for evaluation we apply the proposed method to four public datasets involving different challenging scenario such a dense aerial borne traffic tracking dense point set tracking and semi crowded pedestrian tracking in all the experiment our approach demonstrate very promising result in comparison with state of the art tracker 
graph cut method such a expansion and fusion move have been successful at solving many optimization problem in computer vision higher order markov random field mrf s which are important for numerous application have proven to be very difficult especially for multilabel mrf s i e more than label in this paper we propose a new primal dual energy minimization method for arbitrary higher order multilabel mrf s primal dual method provide guaranteed approximation bound and can exploit information in the dual variable to improve their efficiency our algorithm generalizes the pd technique for first order mrfs and relies on a variant of max flow that can exactly optimize certain higher order binary mrf s we provide approximation bound similar to pd and the method is fast in practice it can optimize non submodular mrf s and additionally can incorporate problem specific knowledge in the form of fusion proposal we compare experimentally against the existing approach that can efficiently handle these difficult energy function for higher order denoising and stereo mrf s we produce lower energy while running significantly faster 
in crowded space such a city center or train station human mobility look complex but is often influenced only by a few cause we propose to quantitatively study crowded environment by introducing a dataset of million trajectory collected in train station given this dataset we address the problem of forecasting pedestrian destination a central problem in understanding large scale crowd mobility we need to overcome the challenge posed by a limited number of observation e g sparse camera and change in pedestrian appearance cue across different camera in addition we often have restriction in the way pedestrian can move in a scene encoded a prior over origin and destination od preference we propose a new descriptor coined a social affinity map sam to link broken or unobserved trajectory of individual in the crowd while using the od prior in our framework our experiment show improvement in performance through the use of sam feature and od prior to the best of our knowledge our work is one of the first study that provides encouraging result towards a better understanding of crowd behavior at the scale of million pedestrian 
the desirability of being able to search for specific person in surveillance video captured by different camera ha increasingly motivated interest in the problem of person re identification which is a critical yet under addressed challenge in multi camera tracking system the main difficulty of person re identification arises from the variation in human appearance from different camera view in this paper to bridge the human appearance variation across camera two coupled dictionary that relate to the gallery and probe camera are jointly learned in the training phase from both labeled and unlabeled image the labeled training image carry the relationship between feature from different camera and the abundant unlabeled training image are introduced to exploit the geometry of the marginal distribution for obtaining robust sparse representation in the testing phase the feature of each target image from the probe camera is first encoded by the sparse representation and then recovered in the feature space spanned by the image from the gallery camera the feature of the same person from different camera are similar following the above transformation experimental result on publicly available datasets demonstrate the superiority of our method 
our goal is to obtain a noise free high resolution hr image from an observed noisy low resolution lr image the conventional approach of preprocessing the image with a denoising algorithm followed by applying a super resolution sr algorithm ha an important limitation along with noise some high frequency content of the image particularly textural detail is invariably lost during the denoising step this denoising loss restricts the performance of the subsequent sr step wherein the challenge is to synthesize such textural detail in this paper we show that high frequency content in the noisy image which is ordinarily removed by denoising algorithm can be effectively used to obtain the missing textural detail in the hr domain to do so we first obtain hr version of both the noisy and the denoised image using a patch similarity based sr algorithm we then show that by taking a convex combination of orientation and frequency selective band of the noisy and the denoised hr image we can obtain a desired hr image where i some of the textural signal lost in the denoising step is effectively recovered in the hr domain and ii additional texture can be easily synthesized by appropriately constraining the parameter of the convex combination we show that this part recovery and part synthesis of texture through our algorithm yield hr image that are visually more pleasing than those obtained using the conventional processing pipeline furthermore our result show a consistent improvement in numerical metric further corroborating the ability of our algorithm to recover lost signal 
existing saliency detection approach use image a input and are sensitive to foreground background similarity complex background texture and occlusion we explore the problem of using light field a input for saliency detection our technique is enabled by the availability of commercial plenoptic camera that capture the light field of a scene in a single shot we show that the unique refocusing capability of light field provides useful focusness depth and objectness cue we further develop a new saliency detection algorithm tailored for light field to validate our approach we acquire a light field database of a range of indoor and outdoor scene and generate the ground truth saliency map experiment show that our saliency detection scheme can robustly handle challenging scenario such a similar foreground and background cluttered background complex occlusion etc and achieve high accuracy and robustness 
we present a realtime hand tracking system using a depth sensor it track a fully articulated hand under large viewpoint in realtime fps on a desktop without using a gpu and with high accuracy error below mm to our knowledge it is the first system that achieves such robustness accuracy and speed simultaneously a verified on challenging real data our system is made of several novel technique we model a hand simply using a number of sphere and define a fast cost function those are critical for realtime performance we propose a hybrid method that combine gradient based and stochastic optimization method to achieve fast convergence and good accuracy we present new finger detection and hand initialization method that greatly enhance the robustness of tracking 
we proposed a deformable patch based method for single image super resolution by the concept of deformation a patch is not regarded a a fixed vector but a flexible deformation flow via deformable patch the dictionary can cover more pattern that do not appear thus becoming more expressive we present the energy function with slow smooth and flexible prior for deformation model during example based super resolution we develop the deformation similarity based on the minimized energy function for basic patch matching for robustness we utilize multiple deformed patch combination for the final reconstruction experiment evaluate the deformation effectiveness and super resolution performance showing that the deformable patch help improve the representation accuracy and perform better than the state of art method 
we address the problem of populating object category detection datasets with dense per object d reconstruction bootstrapped from class label ground truth figure ground segmentation and a small set of keypoint annotation our proposed algorithm first estimate camera viewpoint using rigid structure from motion then reconstructs object shape by optimizing over visual hull proposal guided by loose within class shape similarity assumption the visual hull sampling process attempt to intersect an object s projection cone with the cone of minimal subset of other similar object among those pictured from certain vantage point we show that our method is able to produce convincing per object d reconstruction on one of the most challenging existing object category detection datasets pascal voc our result may re stimulate once popular geometry oriented model based recognition approach 
we address the problem of classifying complex video based on their content a typical approach to this problem is performing the classification using semantic attribute commonly termed concept which occur in the video in this paper we propose a contextual approach to video classification based on generalized maximum clique problem gmcp which us the co occurrence of concept a the context model to be more specific we propose to represent a class based on the co occurrence of it concept and classify a video based on matching it semantic co occurrence pattern to each class representation we perform the matching using gmcp which find the strongest clique of co occurring concept in a video we argue that in principal the co occurrence of concept yield a richer representation of a video compared to most of the current approach additionally we propose a novel optimal solution to gmcp based on mixed binary integer programming mbip the evaluation show our approach which open new opportunity for further research in this direction outperforms several well established video classification method 
in large scale image classification feature such a fisher vector or vlad have achieved state of the art result however the combination of large number of example and high dimensional vector necessitates dimensionality reduction in order to reduce it storage and cpu cost to a reasonable range in spite of the popularity of various feature compression method this paper argues that feature selection is a better choice than feature compression we show that strong multicollinearity among feature dimension may not exist which undermines feature compression s effectiveness and render feature selection a natural choice we also show that many dimension are noise and throwing them away is helpful for classification we propose a supervised mutual information mi based importance sorting algorithm to choose feature combining with bit quantization mi feature selection ha achieved both higher accuracy and le computational cost than feature compression method such a product quantization and bpbc 
camera image saved in raw format are being adopted in computer vision task since raw value represent minimally processed sensor response camera manufacturer however have yet to adopt a standard for raw image and current raw rgb value are device specific due to different sensor spectral sensitivity this result in significantly different raw image for the same scene captured with different camera this paper focus on estimating a mapping that can convert a raw image of an arbitrary scene and illumination from one camera s raw space to another to this end we examine various mapping strategy including linear and non linear transformation applied both in a global and illumination specific manner we show that illumination specific mapping give the best result however at the expense of requiring a large number of transformation to address this issue we introduce an illumination independent mapping approach that us white balancing to assist in reducing the number of required transformation we show that this approach achieves state of the art result on a range of consumer camera and image of arbitrary scene and illumination 
active contour especially in conjunction with prior shape model ha become an important tool in image segmentation however most contour method use shape prior based on similarity shape analysis i e analysis that is invariant to rotation translation and scale in practice the training shape used for prior shape model may be collected from viewing angle different from those for the test image and require invariance to a larger class of transformation using an elastic affine invariant shape modeling of planar curve we propose an active contour algorithm in which the training and test shape can be at arbitrary affine transformation and the resulting segmentation is robust to perspective skews we construct a shape space of affine standardized curve and derive a statistical model for capturing class specific shape variability the active contour is then driven by the true gradient of a total energy composed of a data term a smoothing term and an affine invariant shape prior term this framework is demonstrated using a number of example involving the segmentation of occluded or noisy image of target subject to perspective skew 
a novel model based approach is introduced for real time detection and tracking of the pose of general articulated object a variety of dense motion and depth cue are integrated into a novel articulated iterative closest point approach the proposed method can independently track the six degree of freedom pose of over a hundred of rigid part in real time while at the same time imposing articulation constraint on the relative motion of different part we propose a novel rigidization framework for optimally handling unobservable part during tracking this involves rigidly attaching the minimal amount of unseen part to the rest of the structure in order to most effectively use the currently available knowledge we show how this framework can be used also for detection rather than tracking which allows for automatic system initialization and for incorporating pose estimate obtained from independent object part detector improved performance over alternative solution is demonstrated on real world sequence 
we describe a new approach to transfer knowledge across view for action recognition by using example from a large collection of unlabelled mocap data we achieve this by directly matching purely motion based feature from video to mocap our approach recovers d pose sequence without performing any body part tracking we use these match to generate multiple motion projection and thus add view invariance to our action recognition model we also introduce a closed form solution for approximate non linear circulant temporal encoding ncte which allows u to efficiently perform the match in the frequency domain we test our approach on the challenging unsupervised modality of the ixmas dataset and use publicly available motion capture data for matching without any additional annotation effort we are able to significantly outperform the current state of the art 
this paper proposes a robust tracking method that us interval analysis any single posterior model necessarily includes a modeling uncertainty error and thus the posterior should be represented a an interval of probability then the objective of visual tracking becomes to find the best state that maximizes the posterior and minimizes it interval simultaneously by minimizing the interval of the posterior our method can reduce the modeling uncertainty in the posterior in this paper the aforementioned objective is achieved by using the m estimation which combine the maximum a posterior map estimation with minimum mean square error mmse maximum likelihood ml and minimum interval length mil estimation in the m estimation our method maximizes the posterior over the state obtained by the mmse estimation the method also minimizes interval of the posterior by reducing the gap between the lower and upper bound of the posterior the gap is reduced when the likelihood is maximized by the ml estimation and the interval length of the state is minimized by the mil estimation the experimental result demonstrate that m estimation can be easily integrated into conventional tracking method and can greatly enhance their tracking accuracy in several challenging datasets our method outperforms state of the art tracking method 
in this paper we introduce the novel problem of understanding visual persuasion modern mass medium make extensive use of image to persuade people to make commercial and political decision these effect and technique are widely studied in the social science but behavioral study do not scale to massive datasets computer vision ha made great stride in building syntactical representation of image such a detection and identification of object however the pervasive use of image for communicative purpose ha been largely ignored we extend the significant advance in syntactic analysis in computer vision to the higher level challenge of understanding the underlying communicative intent implied in image we begin by identifying nine dimension of persuasive intent latent in image of politician such a socially dominant energetic and trustworthy and propose a hierarchical model that build on the layer of syntactical attribute such a smile and waving hand to predict the intent presented in the image to facilitate progress we introduce a new dataset of image of politician labeled with ground truth intent in the form of ranking this study demonstrates that a systematic focus on visual persuasion open up the field of computer vision to a new class of investigation around mediated image intersecting with medium analysis psychology and political communication 
this paper present a scalable scene parsing algorithm based on image retrieval and superpixel matching we focus on rare object class which play an important role in achieving richer semantic understanding of visual scene compared to common background class towards this end we make two novel contribution rare class expansion and semantic context description first considering the long tailed nature of the label distribution we expand the retrieval set by rare class exemplar and thus achieve more balanced superpixel classification result second we incorporate both global and local semantic context information through a feedback based mechanism to refine image retrieval and superpixel matching result on the siftflow and lmsun datasets show the superior performance of our algorithm especially on the rare class without sacrificing overall labeling accuracy 
d reconstruction of transparent and specular object is a very challenging topic in computer vision for transparent and specular object which have complex interior and exterior structure that can reflect and refract light in a complex fashion it is difficult if not impossible to use either passive stereo or the traditional structured light method to do the reconstruction we propose a frequency based d reconstruction method which incorporates the frequency based matting method similar to the structured light method a set of frequency based pattern are projected onto the object and a camera capture the scene each pixel of the captured image is analyzed along the time axis and the corresponding signal is transformed to the frequency domain using the discrete fourier transform since the frequency is only determined by the source that creates it the frequency of the signal can uniquely identify the location of the pixel in the pattern in this way the correspondence between the pixel in the captured image and the point in the pattern can be acquired using a new labelling procedure the surface of transparent and specular object can be reconstructed with very encouraging result 
we study the theory of projective reconstruction for multiple projection from an arbitrary dimensional projective space into lower dimensional space this problem is important due to it application in the analysis of dynamical scene the current theory due to hartley and schaffalitzky is based on the grassmann tensor generalizing the idea of fundamental matrix trifocal tensor and quadrifocal tensor used in the well studied case of d to d projection we present a theory whose point of departure is the projective equation rather than the grassmann tensor this is a better fit for the analysis of approach such a bundle adjustment and projective factorization which seek to directly solve the projective equation in a first step we prove that there is a unique grassmann tensor corresponding to each set of image point a question that remained open in the work of hartley and schaffalitzky then we prove that projective equivalence follows from the set of projective equation given certain condition on the estimated camera point setup or the estimated projective depth finally we demonstrate how wrong solution to the projective factorization problem can happen and classify such degenerate solution based on the zero pattern in the estimated depth matrix 
the task of estimating complex non rigid d motion through a monocular camera is of increasing interest to the wider scientific community assuming one ha the d point track of the non rigid object in question the vision community refers to this problem a non rigid structure from motion nrsfm in this paper we make two contribution first we demonstrate empirically that the current state of the art approach to nrsfm i e dai et al exhibit poor reconstruction performance on complex motion i e motion involving a sequence of primitive action such a walk sit and stand involving a human object second we propose that this limitation can be circumvented by modeling complex motion a a union of subspace this doe not naturally occur in dai et al s approach which instead make a le compact summation of subspace assumption experiment on both synthetic and real video illustrate the benefit of our approach for the complex nonrigid motion analysis 
based on the concept of lacunarity in fractal geometry we developed a statistical approach to texture description which yield highly discriminative feature with strong robustness to a wide range of transformation including photometric change and geometric change the texture feature is constructed by concatenating the lacunarity related parameter estimated from the multi scale local binary pattern of image benefiting from the ability of lacunarity analysis to distinguish spatial pattern our method is able to characterize the spatial distribution of local image structure from multiple scale the proposed feature wa applied to texture classification and ha demonstrated excellent performance in comparison with several state of theart approach on four benchmark datasets 
algorithm for solving system of polynomial equation are key component for solving geometry problem in computer vision fast and stable polynomial solver are essential for numerous application e g minimal problem or finding for all stationary point of certain algebraic error recently full symmetry in the polynomial system ha been utilized to simplify and speed up state of the art polynomial solver based on gr bner basis method in this paper we further explore partial symmetry i e where the symmetry lie in a subset of the variable in the polynomial system we develop novel numerical scheme to utilize such partial symmetry we then demonstrate the advantage of our scheme in several computer vision problem in both synthetic and real experiment we show that utilizing partial symmetry allow u to obtain faster and more accurate polynomial solver than the general solver 
camera shake during exposure time often result in spatially variant blur effect of the image the non uniform blur effect is not only caused by the camera motion but also the depth variation of the scene the object close to the camera sensor are likely to appear more blurry than those at a distance in such case however recent non uniform deblurring method do not explicitly consider the depth factor or assume fronto parallel scene with constant depth for simplicity while single image non uniform deblurring is a challenging problem the blurry result in fact contain depth information which can be exploited we propose to jointly estimate scene depth and remove non uniform blur caused by camera motion by exploiting their underlying geometric relationship with only single blurry image a input to this end we present a unified layer based model for depth involved deblurring we provide a novel layer based solution using matting to partition the layer and an expectation maximization scheme to solve this problem this approach largely reduces the number of unknown and make the problem tractable experiment on challenging example demonstrate that both depth and camera shake removal can be well addressed within the unified framework 
a key problem often encountered by many learning algorithm in computer vision dealing with high dimensional data is the so called curse of dimensionality which arises when the available training sample are le than the input feature space dimensionality to remedy this problem we propose a joint dimensionality reduction and classification framework by formulating an optimization problem within the maximum margin class separation task the proposed optimization problem is solved using alternative optimization where we jointly compute the low dimensional maximum margin projection and the separating hyperplanes in the projection subspace moreover in order to reduce the computational cost of the developed optimization algorithm we incorporate orthogonality constraint on the derived projection base and show that the resulting combined model is an alternation between identifying the optimal separating hyperplanes and performing a linear discriminant analysis on the support vector experiment on face facial expression and object recognition validate the effectiveness of the proposed method against state of the art dimensionality reduction algorithm 
convolutional neural network cnn have recently shown outstanding image classification performance in the largescale visual recognition challenge ilsvrc the success of cnns is attributed to their ability to learn rich midlevel image representation a opposed to hand designed low level feature used in other image classification method learning cnns however amount to estimating million of parameter and requires a very large number of annotated image sample this property currently prevents application of cnns to problem with limited training data in this work we show how image representation learned with cnns on large scale annotated datasets can be efficiently transferred to other visual recognition task with limited amount of training data we design a method to reuse layer trained on the imagenet dataset to compute mid level image representation for image in the pascal voc dataset we show that despite difference in image statistic and task in the two datasets the transferred representation lead to significantly improved result for object and action classification outperforming the current state of the art on pascal voc and datasets we also show promising result for object and action localization 
we present a practical framework to automatically detect shadow in real world scene from a single photograph previous work on shadow detection put a lot of effort in designing shadow variant and invariant hand crafted feature in contrast our framework automatically learns the most relevant feature in a supervised manner using multiple convolutional deep neural network convnets the layer network architecture of each convnet consists of alternating convolution and sub sampling layer the proposed framework learns feature at the super pixel level and along the object boundary in both case feature are extracted using a context aware window centered at interest point the predicted posterior based on the learned feature are fed to a conditional random field model to generate smooth shadow contour our proposed framework consistently performed better than the state of the art on all major shadow database collected under a variety of condition 
while clustering ha been well studied in the past decade model selection ha drawn le attention this paper address both problem in a joint manner with an indicator matrix formulation in which the clustering cost is penalized by a frobenius inner product term and the group number estimation is achieved by a rank minimization a affinity graph generally contain positive edge value a sparsity term is further added to avoid the trivial solution rather than adopting the conventional convex relaxation approach wholesale we represent the original problem more faithfully by taking full advantage of the particular structure present in the optimization problem and solving it efficiently using the alternating direction method of multiplier the highly constrained nature of the optimization provides our algorithm with the robustness to deal with the varying and often imperfect input affinity matrix arising from different application and different group number evaluation on the synthetic data a well a two real world problem show the superiority of the method across a large variety of setting 
we propose a technique to use the structural information extracted from a set of d model of an object class to improve novel view synthesis for image showing unknown instance of this class these novel view can be used to amplify training image collection that typically contain only a low number of view or lack certain class of view entirely e g top view we extract the correlation of position normal reflectance and appearance from computer generated image of a few exemplar and use this information to infer new appearance for new instance we show that our approach can improve performance of state of the art detector using real world training data additional application include guided version of inpainting d to d conversion superresolution and non local smoothing 
we study the problem of cross population age estimation human aging is determined by the gene and influenced by many factor different population e g male and female caucasian and asian may age differently previous research ha discovered the aging difference among different population and reported large error in age estimation when crossing gender and or ethnicity in this paper we propose novel method for cross population age estimation with a good performance the proposed method are based on projecting the different aging pattern into a common space where the aging pattern can be correlated even though they come from different population the projection are also discriminative between age class due to the integration of the classical discriminant analysis technique further we study the amount of data needed in the target population to learn a cross population age estimator finally we study the feasibility of multi source cross population age estimation experiment are conducted on a large database of more than face image selected from the morph our study are valuable to significantly reduce the burden of training data collection for age estimation on a new population utilizing existing aging pattern even from different population 
the use of wearable camera make it possible to record life logging egocentric video browsing such long unstructured video is time consuming and tedious segmentation into meaningful chapter is an important first step towards adding structure to egocentric video enabling efficient browsing indexing and summarization of the long video two source of information for video segmentation are i the motion of the camera wearer and ii the object and activity recorded in the video in this paper we address the motion cue for video segmentation motion based segmentation is especially difficult in egocentric video when the camera is constantly moving due to natural head movement of the wearer we propose a robust temporal segmentation of egocentric video into a hierarchy of motion class using a new cumulative displacement curve unlike instantaneous motion vector segmentation using integrated motion vector performs well even in dynamic and crowded scene no assumption are made on the underlying scene structure and the method work in indoor a well a outdoor situation we demonstrate the effectiveness of our approach using publicly available video a well a choreographed video we also suggest an approach to detect the fixation of wearer s gaze in the walking portion of the egocentric video 
we use weakly supervised structured learning to track and disambiguate the identity of multiple indistinguishable translucent and deformable object that can overlap for many frame for this challenging problem we propose a novel model which handle occlusion complex motion and non rigid deformation by jointly optimizing the flow of multiple latent intensity across frame these flow are latent variable for which the user cannot directly provide label instead we leverage a structured learning formulation that us weak user annotation to find the best hyperparameters of this model the approach is evaluated on a challenging dataset for the tracking of multiple drosophila larva which we make publicly available our method track multiple larva in spite of their poor distinguishability and minimizes the number of identity switch during prolonged mutual occlusion 
many state of the art image restoration approach do not scale well to larger image such a megapixel image common in the consumer segment computationally expensive optimization is often the culprit while efficient alternative exist they have not reached the same level of image quality the goal of this paper is to develop an effective approach to image restoration that offer both computational efficiency and high restoration quality to that end we propose shrinkage field a random field based architecture that combine the image model and the optimization algorithm in a single unit the underlying shrinkage operation bear connection to wavelet approach but is used here in a random field context computational efficiency is achieved by construction through the use of convolution and dft a the core component high restoration quality is attained through loss based training of all model parameter and the use of a cascade architecture unlike heavily engineered solution our learning approach can be adapted easily to different trade offs between efficiency and image quality we demonstrate state of the art restoration result with high level of computational efficiency and significant speedup potential through inherent parallelism 
when using plenoptic camera for digital refocusing angular undersampling can cause severe angular aliasing artifact previous approach have focused on avoiding aliasing by pre processing the acquired light field via prefiltering demosaicing reparameterization etc in this paper we present a different solution that first detects and then remove aliasing at the light field refocusing stage different from previous frequency domain aliasing analysis we carry out a spatial domain analysis to reveal whether the aliasing would occur and uncover where in the image it would occur the spatial analysis also facilitates easy separation of the aliasing v non aliasing region and aliasing removal experiment on both synthetic scene and real light field camera array data set demonstrate that our approach ha a number of advantage over the classical prefiltering and depth dependent light field rendering technique 
intrinsic characterization of scene is often the best way to overcome the illumination variability artifact that complicate most computer vision problem from d reconstruction to object or material recognition this paper examines the deficiency of existing intrinsic image model to accurately account for the effect of illuminant color and sensor characteristic in the estimation of intrinsic image and present a generic framework which incorporates insight from color constancy research to the intrinsic image decomposition problem the proposed mathematical formulation includes information about the color of the illuminant and the effect of the camera sensor both of which modify the observed color of the reflectance of the object in the scene during the acquisition process by modeling these effect we get a truly intrinsic reflectance image which we call absolute reflectance which is invariant to change of illuminant or camera sensor this model allows u to represent a wide range of intrinsic image decomposition depending on the specific assumption on the geometric property of the scene configuration and the spectral property of the light source and the acquisition system thus unifying previous model in a single general framework we demonstrate that even partial information about sensor improves significantly the estimated reflectance image thus making our method applicable for a wide range of sensor we validate our general intrinsic image framework experimentally with both synthetic data and natural image 
graph matching and graph mining are two typical area in artificial intelligence in this paper we define the soft attributed pattern sap to describe the common subgraph pattern among a set of attributed relational graph args considering both the graphical structure and graph attribute we propose a direct solution to extract the sap with the maximal graph size without node enumeration given an initial graph template and a number of args we modify the graph template into the maximal sap among the args in an unsupervised fashion the maximal sap extraction is equivalent to learning a graphical model i e an object model from large args i e cluttered rgb rgb d image for graph matching which extends the concept of unsupervised learning for graph matching furthermore this study can be also regarded a the first known approach to formulating maximal graph mining in the graph domain of args our method exhibit superior performance on rgb and rgb d image 
with the goal of accelerating the training and testing complexity of nonlinear kernel method several recent paper have proposed explicit embeddings of the input data into low dimensional feature space where fast linear method can instead be used to generate approximate solution analogous to random fourier feature map to approximate shift invariant kernel such a the gaussian kernel on ropf d we develop a new randomized technique called random laplace feature to approximate a family of kernel function adapted to the semigroup structure of ropf d this is the natural algebraic structure on the set of histogram and other non negative data representation we provide theoretical result on the uniform convergence of random laplace feature empirical analysis on image classification and surveillance event detection task demonstrate the attractiveness of using random laplace feature relative to several other feature map proposed in the literature 
we present a machine learned ranking approach for automatically enhancing the color of a photograph unlike previous technique that train on pair of image before and after adjustment by a human user our method take into account the intermediate step taken in the enhancement process which provide detailed information on the person s color preference to make use of this data we formulate the color enhancement task a a learning to rank problem in which ordered pair of image are used for training and then various color enhancement of a novel input image can be evaluated from their corresponding rank value from the parallel between the decision tree structure we use for ranking and the decision made by a human during the editing process we posit that breaking a full enhancement sequence into individual step can facilitate training our experiment show that this approach compare well to existing method for automatic color enhancement 
video motion segmentation technique automatically segment and track object and region from video or image sequence a a primary processing step for many computer vision application we propose a novel motion segmentation approach for both rigid and non rigid object using adaptive manifold denoising we first introduce an adaptive kernel space in which two feature trajectory are mapped into the same point if they belong to the same rigid object after that we employ an embedded manifold denoising approach with the adaptive kernel to segment the motion of rigid and non rigid object the major observation is that the non rigid object often lie on a smooth manifold with deviation which can be removed by manifold denoising we also show that performing manifold denoising on the kernel space is equivalent to doing so on it range space which theoretically justifies the embedded manifold denoising on the adaptive kernel space experimental result indicate that our algorithm named adaptive manifold denoising amd is suitable for both rigid and non rigid motion segmentation our algorithm work well in many case where several state of the art algorithm fail 
the use of multiple feature for tracking ha been proved a an effective approach because limitation of each feature could be compensated since different type of variation such a illumination occlusion and pose may happen in a video sequence especially long sequence video how to dynamically select the appropriate feature is one of the key problem in this approach to address this issue in multicue visual tracking this paper proposes a new joint sparse representation model for robust feature level fusion the proposed method dynamically remove unreliable feature to be fused for tracking by using the advantage of sparse representation a a result robust tracking performance is obtained experimental result on publicly available video show that the proposed method outperforms both existing sparse representation based and fusion based tracker 
we advocate the inference of qualitative information about d human pose called posebits from image posebits represent boolean geometric relationship between body part e g left leg in front of right leg or hand close to each other the advantage of posebits a a mid level representation are for many task of interest such qualitative pose information may be sufficient e g semantic image retrieval it is relatively easy to annotate large image corpus with posebits a it simply requires answer to yes no question and they help resolve challenging pose ambiguity and therefore facilitate the difficult talk of image based d pose estimation we introduce posebits a posebit database a method for selecting useful posebits for pose estimation and a structural svm model for posebit inference experiment show the use of posebits for semantic image retrieval and for improving d pose estimation 
this paper present a new approach to tracking people in crowded scene where people are subject to long term partial occlusion and may assume varying posture and articulation in such video detection based tracker give poor performance since detecting people occurrence is not reliable and common assumption about locally smooth trajectory do not hold rather we use temporal mid level feature e g supervoxels or dense point trajectory a a more coherent spatiotemporal basis for handling occlusion and pose variation thus we formulate tracking a labeling mid level feature by object identifier and specify a new approach called constrained sequential labeling csl for performing this labeling csl us a cost function to sequentially assign label while respecting the implication of hard constraint computed via constraint propagation a key feature of this approach is that it allows for the use of flexible cost function and constraint that capture complex dependency that cannot be represented in standard network flow formulation to exploit this flexibility we describe how to learn constraint and give a provably correct learning algorithm for cost function that achieves finitetime convergence at a rate that improves with the strength of the constraint our experimental result indicate that csl outperforms the state of the art on challenging real world video of volleyball basketball and pedestrian walking 
tractography refers to the process of tracing out the nerve fiber bundle from diffusion magnetic resonance image dmri data acquired either in vivo or ex vivo tractography is a mature research topic within the field of diffusion mri analysis nevertheless several new method are being proposed on a regular basis thereby justifying the need a the problem is not fully solved tractography is usually applied to the model used to represent the diffusion mr signal or a derived quantity reconstructed from the acquired data separating shape and orientation of these model wa previously shown to approximately preserve diffusion anisotropy a useful bio marker in the ubiquitous problem of interpolation however no further intrinsic geometric property of this framework were exploited to date in literature in this paper we propose a new intrinsic recursive filter on the product manifold of shape and orientation the recursive filter dubbed iukfpro is a generalization of the unscented kalman filter ukf to this product manifold the salient contribution of this work are a new intrinsic ukf for the product manifold of shape and orientation derivation of the riemannian geometry of the product manifold iukfpro is tested on synthetic and real data set from various tractography challenge competition from the experimental result it is evident that iukfpro performs better than several competing scheme in literature with regard to some of the error measure used in the competition and is competitive with respect to others 
we present an approach msil crf that incorporates multiple instance learning mil into conditional random field crfs it can generalize crfs to work on training data with uncertain label by the principle of mil in this work it is applied to saving manual effort on annotating training data for semantic segmentation specifically we consider the setting in which the training dataset for semantic segmentation is a mixture of a few object segment and an abundant set of object bounding box our goal is to infer the unknown object segment enclosed by the bounding box so that they can serve a training data for semantic segmentation to this end we generate multiple segment hypothesis for each bounding box with the assumption that at least one hypothesis is close to the ground truth by treating a bounding box a a bag with it segment hypothesis a structured instance msil crf selects the most likely segment hypothesis by leveraging the knowledge derived from both the labeled and uncertain training data the experimental result on the pascal voc segmentation task demonstrate that msil crf can provide effective alternative to manually labeled segment for semantic segmentation 
computer vision system today fail frequently they also fail abruptly without warning or explanation alleviating the former ha been the primary focus of the community in this work we hope to draw the community s attention to the latter which is arguably equally problematic for real application we promote two metric to evaluate failure prediction we show that a surprisingly straightforward and general approach that we call alert can predict the likely accuracy or failure of a variety of computer vision system semantic segmentation vanishing point and camera parameter estimation and image memorability prediction on individual input image we also explore attribute prediction where classifier are typically meant to generalize to new unseen category we show that alert can be useful in predicting failure of this transfer finally we leverage alert to improve the performance of a downstream application of attribute prediction zero shot learning we show that alert can outperform several strong baseline for zero shot learning on four datasets 
the real world image database such a flickr are characterized by continuous addition of new image the recent approach for image annotation i e the problem of assigning tag to image have two major drawback first either model are learned using the entire training data or to handle the issue of dataset imbalance tag specific discriminative model are trained such model become obsolete and require relearning when new image and tag are added to database second the task of feature fusion is typically dealt using ad hoc approach in this paper we present a weighted extension of multi view non negative matrix factorization nmf to address the aforementioned drawback the key idea is to learn query specific generative model on the feature of nearest neighbor and tag using the proposed nmf knn approach which imposes consensus constraint on the coefficient matrix across different feature this result in coefficient vector across feature to be consistent and thus naturally solves the problem of feature fusion while the weight matrix introduced in the proposed formulation alleviate the issue of dataset imbalance furthermore our approach being query specific is unaffected by addition of image and tag in a database we tested our method on two datasets used for evaluation of image annotation and obtained competitive result 
in this paper we cast the problem of point cloud matching a a shape matching problem by transforming each of the given point cloud into a shape representation called the schr dinger distance transform sdt representation this is achieved by solving a static schr dinger equation instead of the corresponding static hamilton jacobi equation in this setting the sdt representation is an analytic expression and following the theoretical physic literature can be normalized to have unit l norm making it a square root density which is identified with a point on a unit hilbert sphere whose intrinsic geometry is fully known the fisher rao metric a natural metric for the space of density lead to analytic expression for the geodesic distance between point on this sphere in this paper we use the well known riemannian framework never before used for point cloud matching and present a novel matching algorithm we pose point set matching under rigid and non rigid transformation in this framework and solve for the transformation using standard nonlinear optimization technique finally to evaluate the performance of our algorithm dubbed sdtm we present several synthetic and real data example along with extensive comparison to state of the art technique the experiment show that our algorithm outperforms state of the art point set registration algorithm on many quantitative metric 
we consider the intersection of two research field transfer learning and statistic on manifold in particular we consider for manifold valued data transfer learning of tangent space model such a gaussians distribution pca regression or classifier though one would hope to simply use ordinary rn transfer learning idea the manifold structure prevents it we overcome this by basing our method on inner product preserving parallel transport a well known tool widely used in other problem of statistic on manifold in computer vision at first this straightforward idea seems to suffer from an obvious shortcoming transporting large datasets is prohibitively expensive hindering scalability fortunately with our approach we never transport data rather we show how the statistical model themselves can be transported and prove that for the tangent space model above the transport commute with learning consequently our compact framework applicable to a large class of manifold is not restricted by the size of either the training or test set we demonstrate the approach by transferring pca and logistic regression model of real world data involving d shape and image descriptor 
in this paper we propose a label propagation framework to handle the multiple object tracking mot problem for a generic object type cf pedestrian tracking given a target object by an initial bounding box all object of the same type are localized together with their identity we treat this a a problem of propagating bi label i e a binary class label for detection and individual object label for tracking to propagate the class label we adopt clustered multiple task learning cmtl while enforcing spatio temporal consistency and show that this improves the performance when given limited training data to track object we propagate label from trajectory to detection based on affinity using appearance motion and context experiment on public and challenging new sequence show that the proposed method improves over the current state of the art on this task 
we propose a novel motion model steadyflow to represent the motion between neighboring video frame for stabilization a steadyflow is a specific optical flow by enforcing strong spatial coherence such that smoothing feature trajectory can be replaced by smoothing pixel profile which are motion vector collected at the same pixel location in the steadyflow over time in this way we can avoid brittle feature tracking in a video stabilization system besides steadyflow is a more general d motion model which can deal with spatially variant motion we initialize the steadyflow by optical flow and then discard discontinuous motion by a spatial temporal analysis and fill in missing region by motion completion our experiment demonstrate the effectiveness of our stabilization on real world challenging video 
this paper address extracting two layer from an image where one layer is smoother than the other this problem arises most notably in intrinsic image decomposition and reflection interference removal layer decomposition from a single image is inherently ill posed and solution require additional constraint to be enforced we introduce a novel strategy that regularizes the gradient of the two layer such that one ha a long tail distribution and the other a short tail distribution while imposing the long tail distribution is a common practice our introduction of the short tail distribution on the second layer is unique we formulate our problem in a probabilistic framework and describe an optimization scheme to solve this regularization with only a few iteration we apply our approach to the intrinsic image and reflection removal problem and demonstrate high quality layer separation on par with other technique but being significantly faster than prevailing method 
visual distracters are detrimental and generally very difficult to handle in target tracking because they generate false positive candidate for target matching the resilience of region based matching to the distracters depends not only on the matching metric but also on the characteristic of the target region to be matched the two task i e learning the best metric and selecting the distracter resilient target region actually correspond to the attribute selection and spatial selection process in the human visual perception this paper present an initial attempt to unify the modeling of these two task for an effective solution based on the introduction of a new quantity called soft visual margin a a function of both matching metric and spatial location it measure the discrimination between the target and it spatial distracters and characterizes the reliability of matching different from other formulation of margin this new quantity is analytical and is insensitive to noisy data this paper present a novel method to jointly determine the best spatial location and the optimal metric based on that a solid distracter resilient region tracker is designed and it effectiveness is validated and demonstrated through extensive experiment 
an action is typically composed of different part of the object moving in particular sequence the presence of different motion represented a a d histogram ha been used in the traditional bag of word bow approach for recognizing action however the interaction among the motion also form a crucial part of an action different object part have varying degree of interaction with the other part during an action cycle it is these interaction we want to quantify in order to bring in additional information about the action in this paper we propose a causality based approach for quantifying the interaction to aid action classification granger causality is used to compute the cause and effect relationship for pair of motion trajectory of a video a d histogram descriptor for the video is constructed using these pairwise measure our proposed method of obtaining pairwise measure for video is also applicable for large datasets we have conducted experiment on challenging action recognition database such a hmdb and ucf and shown that our causality descriptor help in encoding additional information regarding the action and performs on par with the state of the art approach due to the complementary nature a further increase in performance can be observed by combining our approach with state of the art approach 
the paper proposes a diversity enhanced condensation algorithm to address the particle impoverishment problem which stochastic filtering usually suffers from the particle diversity play an important role a it affect the performance of filtering although the condensation algorithm is widely used in computer vision it easily get trapped in local minimum due to the particle degeneracy we introduce a modified evolutionary computing method adaptive differential evolution to resolve the particle impoverishment under a proper size of particle population we apply our proposed method to endoscope tracking for estimating three dimensional motion of the endoscopic camera the experimental result demonstrate that our proposed method offer more robust and accurate tracking than previous method the current tracking smoothness and error were significantly reduced from to mm mm which approximates the clinical requirement of mm 
the number of gps tagged image available on the web is increasing at a rapid rate the majority of such location tag are specified by the user either through manual tagging or localization chip embedded in the camera however a known issue with user shared image is the unreliability of such gps tag in this paper we propose a method for addressing this problem we assume a large dataset of gps tagged image which includes an unknown subset with contaminated tag is available we develop a robust method for identification and refinement of this subset using the rest of the image in the dataset in the proposed method we form a large number of triplet of matching image and use them for estimating the location of the query image utilizing structure from motion some of the generated estimation may be inaccurate due to the noisy gps tag in the dataset therefore we perform random walk on the estimation in order to identify the subset with the maximal agreement finally we estimate the gps tag of the query utilizing the identified consistent subset using a weighted mean we propose a new damping factor for random walk which conforms to the level of noise in the input and consequently robustifies random walk we evaluated the proposed framework on a dataset of over k user shared image the experiment show our method robustly improves the accuracy of gps tag under diverse scenario 
a major challenge in real world feature matching problem is to tolerate the numerous outlier arising in typical visual task variation in object appearance shape and structure within the same object class make it harder to distinguish inliers from outlier due to clutter in this paper we propose a max pooling approach to graph matching which is not only resilient to deformation but also remarkably tolerant to outlier the proposed algorithm evaluates each candidate match using it most promising neighbor and gradually propagates the corresponding score to update the neighbor a final output it assigns a reliable score to each match together with it supporting neighbor thus providing contextual information for further verification we demonstrate the robustness and utility of our method with synthetic and real image experiment 
facial feature detection from facial image ha attracted great attention in the field of computer vision it is a nontrivial task since the appearance and shape of the face tend to change under different condition in this paper we propose a hierarchical probabilistic model that could infer the true location of facial feature given the image measurement even if the face is with significant facial expression and pose the hierarchical model implicitly capture the lower level shape variation of facial component using the mixture model furthermore in the higher level it also learns the joint relationship among facial component the facial expression and the pose information through automatic structure learning and parameter estimation of the probabilistic model experimental result on benchmark database demonstrate the effectiveness of the proposed hierarchical probabilistic model 
image and video are often characterized by multiple type of local descriptor such a sift hog and hof each of which describes certain aspect of object feature recognition system benefit from fusing multiple type of these descriptor two widely applied fusion pipeline are descriptor concatenation and kernel average the first one is effective when different descriptor are strongly correlated while the second one is probably better when descriptor are relatively independent in practice however different descriptor are neither fully independent nor fully correlated and previous fusion method may not be satisfying in this paper we propose a new global representation multi view super vector mvsv which is composed of relatively independent component derived from a pair of descriptor kernel average is then applied on these component to produce recognition result to obtain mvsv we develop a generative mixture model of probabilistic canonical correlation analyzer m pcca and utilize the hidden factor and gradient vector of m pcca to construct mvsv for video representation experiment on video based action recognition task show that mvsv achieves promising result and outperforms fv and vlad with descriptor concatenation or kernel average fusion strategy 
facial expression is temporally dynamic event which can be decomposed into a set of muscle motion occurring in different facial region over various time interval for dynamic expression recognition two key issue temporal alignment and semantics aware dynamic representation must be taken into account in this paper we attempt to solve both problem via manifold modeling of video based on a novel mid level representation i e expressionlet specifically our method contains three key component each expression video clip is modeled a a spatio temporal manifold stm formed by dense low level feature a universal manifold model umm is learned over all low level feature and represented a a set of local st mode to statistically unify all the stm the local mode on each stm can be instantiated by fitting to umm and the corresponding expressionlet is constructed by modeling the variation in each local st mode with above strategy expression video are naturally aligned both spatially and temporally to enhance the discriminative power the expressionlet based stm representation is further processed with discriminant embedding our method is evaluated on four public expression database ck mmi oulu casia and afew in all case our method report result better than the known state of the art 
object category localization is a challenging problem in computer vision standard supervised training requires bounding box annotation of object instance this time consuming annotation process is sidestepped in weakly supervised learning in this case the supervised information is restricted to binary label that indicate the absence presence of object instance in the image without their location we follow a multiple instance learning approach that iteratively train the detector and infers the object location in the positive training image our main contribution is a multi fold multiple instance learning procedure which prevents training from prematurely locking onto erroneous object location this procedure is particularly important when high dimensional representation such a the fisher vector are used we present a detailed experimental evaluation using the pascal voc dataset compared to state of the art weakly supervised detector our approach better localizes object in the training image which translates into improved detection performance 
in this paper we present a novel autonomous pipeline to build a personalized parametric model pose driven avatar using a single depth sensor our method first capture a few high quality scan of the user rotating herself at multiple pose from different view we fit each incomplete scan using template fitting technique with a generic human template and register all scan to every pose using global consistency constraint after registration these watertight model with different pose are used to train a parametric model in a fashion similar to the scape method once the parametric model is built it can be used a an animitable avatar or more interestingly synthesizing dynamic d model from single view depth video experimental result demonstrate the effectiveness of our system to produce dynamic model 
the limitation of current state of the art method for single view depth estimation and semantic segmentation are closely tied to the property of perspective geometry that the perceived size of the object scale inversely with the distance in this paper we show that we can use this property to reduce the learning of a pixel wise depth classifier to a much simpler classifier predicting only the likelihood of a pixel being at an arbitrarily fixed canonical depth the likelihood for any other depth can be obtained by applying the same classifier after appropriate image manipulation such transformation of the problem to the canonical depth remove the training data bias towards certain depth and the effect of perspective the approach can be straight forwardly generalized to multiple semantic class improving both depth estimation and semantic segmentation performance by directly targeting the weakness of independent approach conditioning the semantic label on the depth provides a way to align the data to their physical scale allowing to learn a more discriminative classifier conditioning depth on the semantic class help the classifier to distinguish between ambiguity of the otherwise ill posed problem we tested our algorithm on the kitti road scene dataset and nyu indoor dataset and obtained obtained result that significantly outperform current state of the art in both single view depth and semantic segmentation domain 
this paper describes the development and application of a new approach to total variation tv minimization for reconstruction problem on geometrically complex and unstructured volumetric mesh the driving application of this study is the reconstruction of d ischemic region in the heart from noninvasive body surface potential data where the use of a tv prior can be expected to promote the reconstruction of two piecewise smooth region of healthy and ischemic electrical property with localized gradient in between compared to tv minimization on regular grid of pixel voxels the complex unstructured volumetric mesh of the heart pose unique challenge including the impact of mesh resolution on the tv prior and the difficulty of gradient calculation in this paper we introduce a variational tv prior and when combined with the iteratively re weighted least square concept a new algorithm to tv minimization that is computationally efficient and robust to the discretization resolution in a large set of simulation study a well a two initial real data study we show that the use of the proposed tv prior outperforms l based penalty in reconstruct ischemic region and it show higher robustness and efficiency compared to the commonly used discrete tv prior we also investigate the performance of the proposed tv prior in combination with a l versus l based data fidelity term the proposed method can extend tv minimization to a border range of application that involves physical domain of complex shape and unstructured volumetric mesh 
we propose a unified framework discover to simultaneously discover important segment classify high level event and generate recounting for large amount of unconstrained web video the motivation is our observation that many video event are characterized by certain important segment our goal is to find the important segment and capture their information for event classification and recounting we introduce an evidence localization model where evidence location are modeled a latent variable we impose constraint on global video appearance local evidence appearance and the temporal structure of the evidence the model is learned via a max margin framework and allows efficient inference our method doe not require annotating source of evidence and is jointly optimized for event classification and recounting experimental result are shown on the challenging trecvid medtest dataset 
we propose a joint foreground background mixture model fbm that simultaneously performs background estimation and motion segmentation in complex dynamic scene our fbm consist of a set of location specific dynamic texture dt component for modeling local background motion and set of global dt component for modeling consistent foreground motion we derive an em algorithm for estimating the parameter of the fbm we also apply spatial constraint to the fbm using an markov random field grid and derive a corresponding variational approximation for inference unlike existing approach to background subtraction our fbm doe not require a manually selected threshold or a separate training video unlike existing motion segmentation technique our fbm can segment foreground motion over complex background with mixed motion and detect stopped object since most dynamic scene datasets only contain video with a single foreground object over a simple background we develop a new challenging dataset with multiple foreground object over complex dynamic background in experiment we show that jointly modeling the background and foreground segment with fbm yield significant improvement in accuracy on both background estimation and motion segmentation compared to state of the art method 
deformable object matching which is also called elastic matching or deformation matching is an important and challenging problem in computer vision although numerous deformation model have been proposed in different matching task not many of them investigate the intrinsic physic underlying deformation due to the lack of physical analysis these model cannot describe the structure change of deformable object very well motivated by this we analyze the deformation physically and propose a novel deformation decomposition model to represent various deformation based on the physical model we formulate the matching problem a a two mensional label markov random field the mrf energy function is derived from the deformation decomposition model furthermore we propose a two stage method to optimize the mrf energy function to provide a quantitative benchmark we build a deformation matching database with an evaluation criterion experimental result show that our method outperforms previous approach especially on complex deformation 
in statistical analysis of video sequence for speech recognition and more generally activity recognition it is natural to treat temporal evolution of feature a trajectory on riemannian manifold however different evolution pattern result in arbitrary parameterizations of these trajectory we investigate a recent framework from statistic literature that handle this nuisance variability using a cost function distance for temporal registration and statistical summarization modeling of trajectory it is based on a mathematical representation of trajectory termed transported square root vector field tsrvf and the l norm on the space of tsrvfs we apply this framework to the problem of speech recognition using both audio and visual component in each case we extract feature form trajectory on corresponding manifold and compute parametrization invariant distance using tsrvfs for speech classification on the ouluvs database the classification performance under metric increase significantly by nearly under both modality and for all choice of feature we obtained speaker dependent classification rate of and for visual and audio component respectively 
weighted median in the form of either solver or filter ha been employed in a wide range of computer vision solution for it beneficial property in sparsity representation but it is hard to be accelerated due to the spatially varying weight and the median property we propose a few efficient scheme to reduce computation complexity from o r to o r where r is the kernel size our contribution is on a new joint histogram representation median tracking and a new data structure that enables fast data access the effectiveness of these scheme is demonstrated on optical flow estimation stereo matching structure texture separation image filtering to name a few the running time is largely shortened from several minute to le than second the source code is provided in the project website 
we propose filter forest ff an efficient new discriminative approach for predicting continuous variable given a signal and it context ff can be used for general signal restoration task that can be tackled via convolutional filtering where it attempt to learn the optimal filtering kernel to be applied to each data point the model can learn both the size of the kernel and it value conditioned on the observation and it spatial or temporal context we show that ff compare favorably to both markov random field based and recently proposed regression forest based approach for labeling problem in term of efficiency and accuracy in particular we demonstrate how ff can be used to learn optimal denoising filter for natural image a well a for other task such a depth image refinement and d signal magnitude estimation numerous experiment and quantitative comparison show that ffs achieve accuracy at par or superior to recent state of the art technique while being several order of magnitude faster 
in recent year large image data set such a imagenet tinyimages or ever growing social network like flickr have emerged posing new challenge to image classification that were not apparent in smaller image set in particular the efficient handling of dynamically growing data set where not only the amount of training image but also the number of class increase over time is a relatively unexplored problem to remedy this we introduce nearest class mean forest ncmf a variant of random forest where the decision node are based on nearest class mean ncm classification ncmfs not only outperform conventional random forest but are also well suited for integrating new class to this end we propose and compare several approach to incorporate data from new class so a to seamlessly extend the previously trained forest instead of re training them from scratch in our experiment we show that ncmfs trained on small data set with class can be extended to large data set with class without significant loss of accuracy compared to training from scratch on the full data 
in this paper we propose an unsupervised framework for action spotting in video that doe not depend on any specific feature e g hog hof stip silhouette bag of word etc furthermore our solution requires no human localization segmentation or framewise tracking this is achieved by treating the problem holistically a that of extracting the internal dynamic of video cuboid by modeling them in their natural form a multilinear tensor to extract their internal dynamic we devised a novel two phase decomposition tp decomp of a tensor that generates very compact and discriminative representation that are robust to even heavily perturbed data technically a rank based tensor core pyramid rank tcp descriptor is generated by combining multiple tensor core under multiple rank allowing to represent video cuboid in a hierarchical tensor pyramid the problem then reduces to a template matching problem which is solved efficiently by using two boosting strategy to reduce search space we filter the dense trajectory cloud extracted from the target video to boost the matching speed we perform matching in an iterative coarse to fine manner experiment on benchmark show that our method outperforms current state of the art under various challenging condition we also created a challenging dataset called heavily perturbed video array hpva to validate the robustness of our framework under heavily perturbed situation 
computational and memory cost restrict spectral technique to rather small graph which is a serious limitation especially in video segmentation in this paper we propose the use of a reduced graph based on superpixels in contrast to previous work the reduced graph is reweighted such that the resulting segmentation is equivalent under certain assumption to that of the full graph we consider equivalence in term of the normalized cut and of it spectral clustering relaxation the proposed method reduces runtime and memory consumption and yield on par result in image and video segmentation further it enables an efficient data representation and update for a new streaming video segmentation approach that also achieves state of the art performance 
the objective of this work is object category detection in large scale image datasets in the manner of video google an object category is specified by a hog classifier template and retrieval is immediate at run time we make the following three contribution i a new image representation based on mid level discriminative patch that is designed to be suited to immediate object category detection and inverted file indexing ii a sparse representation of a hog classifier using a set of mid level discriminative classifier patch and iii a fast method for spatial reranking image on their detection we evaluate the detection method on the standard pascal voc dataset together with a k image subset of imagenet and demonstrate near state of the art detection performance at low rank whilst maintaining immediate retrieval speed application are also demonstrated using an exemplar svm for pose matched retrieval 
this paper address the large scale visual font recognition vfr problem which aim at automatic identification of the typeface weight and slope of the text in an image or photo without any knowledge of content although visual font recognition ha many practical application it ha largely been neglected by the vision community to address the vfr problem we construct a large scale dataset containing font class which easily exceeds the scale of most image categorization datasets in computer vision a font recognition is inherently dynamic and open ended i e new class and data for existing category are constantly added to the database over time we propose a scalable solution based on the nearest class mean classifier ncm the core algorithm is built on local feature embedding local feature metric learning and max margin template selection which is naturally amenable to ncm and thus to such open ended classification problem the new algorithm can generalize to new class and new data at little added cost extensive experiment demonstrate that our approach is very effective on our synthetic test image and achieves promising result on real world test image 
a the image enhancement algorithm developed in recent year how to compare the performance of different image enhancement algorithm becomes a novel task in this paper we propose a framework to do quality assessment for comparing image enhancement algorithm not like traditional image quality assessment approach we focus on the relative quality ranking between enhanced image rather than giving an absolute quality score for a single enhanced image we construct a dataset which contains source image in bad visibility and their enhanced image processed by different enhancement algorithm and then do subjective assessment in a pair wise way to get the relative ranking of these enhanced image a rank function is trained to fit the subjective assessment result and can be used to predict rank of new enhanced image which indicate the relative quality of enhancement algorithm the experimental result show that our proposed approach statistically outperforms state of the art general purpose nr iqa algorithm 
recently multi atlas segmentation ma ha achieved a great success in the medical imaging area the key assumption of ma is that multiple atlas encompass richer anatomical variability than a single atlas therefore we can label the target image more accurately by mapping the label information from the appropriate atlas image that have the most similar structure the problem of atlas selection however still remains unexplored current state of the art ma method rely on image similarity to select a set of atlas unfortunately this heuristic criterion is not necessarily related to segmentation performance and thus may undermine segmentation result to solve this simple but critical problem we propose a learning based atlas selection method to pick up the best atlas that would eventually lead to more accurate image segmentation our idea is to learn the relationship between the pairwise appearance of observed instance a pair of atlas and target image and their final labeling performance in term of dice ratio in this way we can select the best atlas according to their expected labeling accuracy it is worth noting that our atlas selection method is general enough to be integrated with existing ma method a is shown in the experiment we achieve significant improvement after we integrate our method with widely used ma method on adni and loni lpba datasets 
the development of facial database with an abundance of annotated facial data captured under unconstrained in the wild condition have made discriminative facial deformable model the de facto choice for generic facial landmark localization even though very good performance for the facial landmark localization ha been shown by many recently proposed discriminative technique when it come to the application that require excellent accuracy such a facial behaviour analysis and facial motion capture the semi automatic person specific or even tedious manual tracking is still the preferred choice one way to construct a person specific model automatically is through incremental updating of the generic model this paper deal with the problem of updating a discriminative facial deformable model a problem that ha not been thoroughly studied in the literature in particular we study for the first time to the best of our knowledge the strategy to update a discriminative model that is trained by a cascade of regressors we propose very efficient strategy to update the model and we show that is possible to automatically construct robust discriminative person and imaging condition specific model in the wild that outperform state of the art generic face alignment strategy 
we pose the following question what happens when test data not only differs from training data but differs from it in a continually evolving way the classic domain adaptation paradigm considers the world to be separated into stationary domain with clear boundary between them however in many real world application example cannot be naturally separated into discrete domain but arise from a continuously evolving underlying process example include video with gradually changing lighting and spam email with evolving spammer tactic we formulate a novel problem of adapting to such continuous domain and present a solution based on smoothly varying embeddings recent work ha shown the utility of considering discrete visual domain a fixed point embedded in a manifold of lower dimensional subspace adaptation can be achieved via transforms or kernel learned between such stationary source and target subspace we propose a method to consider non stationary domain which we refer to a continuous manifold adaptation cma we treat each target sample a potentially being drawn from a different subspace on the domain manifold and present a novel technique for continuous transform based adaptation our approach can learn to distinguish category using training data collected at some point in the past and continue to update it model of the category for some time into the future without receiving any additional label experiment on two visual datasets demonstrate the value of our approach for several popular feature representation 
in this paper we propose a novel two step scheme to filter heavy noise from image with the assistance of retrieved web image there are two key technical contribution in our scheme first for every noisy image block we build two three dimensional d data cube by using similar block in retrieved web image and similar nonlocal block within the noisy image respectively to better use their correlation we propose different denoising strategy the denoising in the d cube built upon the retrieved image is performed a median filtering in the spatial domain whereas the denoising in the other d cube is performed in the frequency domain these two denoising result are then combined in the frequency domain to produce a denoising image second to handle heavy noise we further propose using the denoising image to improve image registration of the retrieved web image d cube building and the estimation of filtering parameter in the frequency domain afterwards the proposed denoising is performed on the noisy image again to generate the final denoising result our experimental result show that when the noise is high the proposed scheme is better than bm d by more than db in psnr and the visual quality improvement is clear to see 
this paper leverage occluding contour aka internal silhouette to improve the performance of multi view stereo method the contribution are a new technique to identify free space region arising from occluding contour and a new approach for incorporating the resulting free space constraint into poisson surface reconstruction the proposed approach outperforms state of the art mv technique for challenging internet datasets yielding dramatic quality improvement both around object contour and in surface detail 
this paper address the problem of assigning object class label to image pixel following recent holistic formulation we cast scene labeling a inference of a conditional random field crf grounded onto superpixels the crf inference is specified a quadratic program qp with mutual exclusion mutex constraint on class label assignment the qp is solved using a beam search b which is well suited for scene labeling because it explicitly account for spatial extent of object conforms to inconsistency constraint from domain knowledge and ha low computational cost b gradually build a search tree whose node correspond to candidate scene labelings successor node are repeatedly generated from a select set of their parent node until convergence we prove that our b efficiently maximizes the qp objective of crf inference effectiveness of our b for scene labeling is evaluated on the benchmark msrc stanford backgroud pascal voc and datasets 
in this work we present neural decision forest a novel approach to jointly tackle data representationand discriminative learning within randomized decision tree recent advance of deep learning architecture demonstrate the power of embedding representation learning within the classifier an idea that is intuitively supported by the hierarchical nature of the decision forest model where the input space is typically left unchanged during training and testing we bridge this gap by introducing randomized multilayer perceptrons rmlp a new split node which are capable of learning non linear data specific representation and taking advantage of them by finding optimal prediction for the emerging child node to prevent overfitting we i randomly select the image data fed to the input layer ii automatically adapt the rmlp topology to meet the complexity of the data arriving at the node and iii introduce an l norm based regularization that additionally sparsifies the network the key finding in our experiment on three different semantic image labelling datasets are consistently improved result and significantly compressed tree compared to conventional classification tree 
a the collection of large datasets becomes increasingly automated the occurrence of outlier will increase big data implies big outlier while principal component analysis pca is often used to reduce the size of data and scalable solution exist it is well known that outlier can arbitrarily corrupt the result unfortunately state of the art approach for robust pca do not scale beyond small to medium sized datasets to address this we introduce the grassmann average ga which express dimensionality reduction a an average of the subspace spanned by the data because average can be efficiently computed we immediately gain scalability ga is inherently more robust than pca but we show that they coincide for gaussian data we exploit that average can be made robust to formulate the robust grassmann average rga a a form of robust pca robustness can be with respect to vector subspace or element of vector we focus on the latter and use a trimmed average the resulting trimmed grassmann average tga is particularly appropriate for computer vision because it is robust to pixel outlier the algorithm ha low computational complexity and minimal memory requirement making it scalable to big noisy data we demonstrate tga for background modeling video restoration and shadow removal we show scalability by performing robust pca on the entire star war iv movie 
current state of the art system for visual content analysis require large training set for each class of interest and performance degrades rapidly with fewer example in this paper we present a general framework for the zeroshot learning problem of performing high level event detection with no training exemplar using only textual description this task go beyond the traditional zero shot framework of adapting a given set of class with training data to unseen class we leverage video and image collection with free form text description from widely available web source to learn a large bank of concept in addition to using several off the shelf concept detector speech and video text for representing video we utilize natural language processing technology to generate event description feature the extracted feature are then projected to a common high dimensional space using text expansion and similarity is computed in this space we present extensive experimental result on the large trecvid med corpus to demonstrate our approach our result show that the proposed concept detection method significantly outperform current attribute classifier such a classemes objectbank and sun attribute further we find that fusion both within a well a between modality is crucial for optimal performance 
in this work we address the problem of d pose estimation of multiple human from multiple view this is a more challenging problem than single human d pose estimation due to the much larger state space partial occlusion a well a across view ambiguity when not knowing the identity of the human in advance to address these problem we first create a reduced state space by triangulation of corresponding body joint obtained from part detector in pair of camera view in order to resolve the ambiguity of wrong and mixed body part of multiple human after triangulation and also those coming from false positive body part detection we introduce a novel d pictorial structure dp model our model infers d human body configuration from our reduced state space the dp model is generic and applicable to both single and multiple human pose estimation in order to compare to the state of the art we first evaluate our method on single human d pose estimation on humaneva i and kth multiview football dataset ii datasets then we introduce and evaluate our method on two datasets for multiple human d pose estimation 
this paper proposes a method for estimating the d body shape of a person with robustness to clothing we formulate the problem a optimization over the manifold of valid depth map of body shape learned from synthetic training data the manifold itself is represented using a novel data structure a multi resolution manifold forest mrmf which contains vertical edge between tree node a well a horizontal edge between node across tree that correspond to overlapping partition we show that this data structure allows both efficient localization and navigation on the manifold for on the fly building of local linear model manifold charting we demonstrate shape estimation of clothed user showing significant improvement in accuracy over global shape model and model using pre computed cluster we further compare the mrmf with alternative manifold charting method on a public dataset for estimating d motion from noisy d marker observation obtaining state of the art result 
this paper present a novel methodology for modelling pedestrian trajectory over a scene based in the hypothesis that when people try to reach a destination they use the path that take le time taking into account environmental information like the type of terrain or what other people did before thus a minimal path approach can be used to model human trajectory behaviour we develop a modified fast marching method that allows u to include both velocity and orientation in the front propagation approach without increasing it computational complexity combining all the information we create a time surface that show the time a target need to reach any given position in the scene we also create different metric in order to compare the time surface against the real behaviour experimental result over a public dataset prove the initial hypothesis correctness 
we introduce an online approach to learn possible elementary group group that contain only two target for inferring high level context that can be used to improve multi target tracking in a data association based framework unlike most existing association based tracking approach that use only low level information e g time appearance and motion to build the affinity model and consider each target a an independent agent we online learn social grouping behavior to provide additional information for producing more robust tracklets affinity social grouping behavior of pairwise target is first learned from confident tracklets and encoded in a disjoint grouping graph the grouping graph is further completed with the help of group tracking the proposed method is efficient handle group merge and split and can be easily integrated into any basic affinity model we evaluate our approach on two public datasets and show significant improvement compared with state of the art method 
this paper describes a framework for modeling human activity a temporally structured process our approach is motivated by the inherently hierarchical nature of human activity and the close correspondence between human action and speech we model action unit using hidden markov model much like word in speech these action unit then form the building block to model complex human activity a sentence using an action grammar to evaluate our approach we collected a large dataset of daily cooking activity the dataset includes a total of participant each performing a total of cooking activity in multiple real life kitchen resulting in over hour of video footage we evaluate the htk toolkit a state of the art speech recognition engine in combination with multiple video feature descriptor for both the recognition of cooking activity e g making pancake a well a the semantic parsing of video into action unit e g cracking egg our result demonstrate the benefit of structured temporal generative approach over existing discriminative approach in coping with the complexity of human daily life activity 
d reconstruction from a single image is a classical problem in computer vision however it still pose great challenge for the reconstruction of daily use object with irregular shape in this paper we propose to learn d reconstruction knowledge from informally captured rgb d image which will probably be ubiquitously used in daily life the learning of d reconstruction is defined a a category modeling problem in which a model for each category is trained to encode category specific knowledge for d reconstruction the category model estimate the pixel level d structure of an object from it d appearance by taking into account considerable variation in rotation d structure and texture learning d reconstruction from ubiquitous rgb d image creates a new set of challenge experimental result have demonstrated the effectiveness of the proposed approach 
image deblurring to remove blur caused by camera shake ha been intensively studied nevertheless most method are brittle and computationally expensive in this paper we analyze multi image approach which capture and combine multiple frame in order to make deblurring more robust and tractable in particular we compare the performance of two approach align and average and multi image deconvolution our deconvolution is non blind using a blur model obtained from real camera motion a measured by a gyroscope we show that in most situation such deconvolution outperforms align and average we also show perhaps surprisingly that deconvolution doe not benefit from increasing exposure time beyond a certain threshold to demonstrate the effectiveness and efficiency of our method we apply it to still resolution imagery of natural scene captured using a mobile camera with flexible camera control and an attached gyroscope 
a scene category imposes tight distribution over the kind of object that might appear in the scene the appearance of those object and their layout in this paper we propose a method to learn scene structure that can encode three main interlacing component of a scene the scene category the context specific appearance of object and their layout our experimental evaluation show that our learned scene structure outperform state of the art method of deformable part model in detecting object in a scene our scene structure provides a level of scene understanding that is amenable to deep visual inference the scene structure can also generate feature that can later be used for scene categorization using these feature we also show promising result on scene categorization 
most state of the art dynamic scene deblurring method based on accurate motion segmentation assume that motion blur is small or that the specific type of motion causing the blur is known in this paper we study a motion segmentation free dynamic scene deblurring method which is unlike other conventional method when the motion can be approximated to linear motion that is locally pixel wise varying we can handle various type of blur caused by camera shake including out of plane motion depth variation radial distortion and so on thus we propose a new energy model simultaneously estimating motion flow and the latent image based on robust total variation tv l model this approach is necessary to handle abrupt change in motion without segmentation furthermore we address the problem of the traditional coarse to fine deblurring framework which give rise to artifact when restoring small structure with distinct motion we thus propose a novel kernel re initialization method which reduces the error of motion flow propagated from a coarser level moreover a highly effective convex optimization based solution mitigating the computational difficulty of the tv l model is established comparative experimental result on challenging real blurry image demonstrate the efficiency of the proposed method 
we present a novel co segmentation method for textured d shape our algorithm take a collection of textured shape belonging to the same category and sparse annotation of foreground segment and produce a joint dense segmentation of the shape in the collection we model the segment by a collectively trained gaussian mixture model the final model segmentation is formulated a an energy minimization across all model jointly where intra model edge control the smoothness and separation of model segment and inter model edge impart global consistency we show promising result on two large real world datasets and also compare with previous shape only d segmentation method using publicly available datasets 
we present a novel approach for event detection in video by temporal sequence modeling exploiting temporal information ha lain at the core of many approach for video analysis i e action activity and event recognition unlike previous work doing temporal modeling at semantic event level we propose to model temporal dependency in the data at sub event level without using event annotation this free our model from ground truth and address several limitation in previous work on temporal modeling based on this idea we represent a video by a sequence of visual word learnt from the video and apply the sequence memoizer to capture long range dependency in a temporal context in the visual sequence this data driven temporal model is further integrated with event classification for jointly performing segmentation and classification of event in a video we demonstrate the efficacy of our approach on two challenging datasets for visual recognition 
accurate ground truth pose is essential to the training of most existing head pose estimation algorithm however in many case the ground truth pose is obtained in rather subjective way such a asking the human subject to stare at different marker on the wall in such case it is better to use soft label rather than explicit hard label therefore this paper proposes to associate a multivariate label distribution mld to each image an mld cover a neighborhood around the original pose labeling the image with mld can not only alleviate the problem of inaccurate pose label but also boost the training example associated to each pose without actually increasing the total amount of training example two algorithm are proposed to learn from the mld by minimizing the weighted jeffrey s divergence between the predicted mld and the ground truth mld experimental result show that the mld based method perform significantly better than the compared state of the art head pose estimation algorithm 
the construction of facial deformable model fdms is a very challenging computer vision problem since the face is a highly deformable object and it appearance drastically change under different pose expression and illumination although several method for generic fdms construction have been proposed for facial landmark localization in still image they are insufficient for task such a facial behaviour analysis and facial motion capture where perfect landmark localization is required in this case person specific fdms psms are mainly employed requiring manual facial landmark annotation for each person and person specific training in this paper a novel method for the automatic construction of psms is proposed to this end an orthonormal subspace which is suitable for facial image reconstruction is learnt next to correct the fitting of a generic model image congealing i e batch image aliment is performed by employing only the learnt orthonormal subspace finally the corrected fitting are used to construct the psm the image congealing problem is solved by formulating a suitable sparsity regularized rank minimization problem the proposed method outperforms the state of the art method that is compared to in term of both landmark localization accuracy and computational time 
expression and pose variation are major challenge for reliable face recognition fr in d in this paper we aim to endow state of the art face recognition sdks with robustness to facial expression variation and pose change by using an extended d morphable model dmm which isolates identity variation from those due to facial expression specifically given a probe with expression a novel view of the face is generated where the pose is rectified and the expression neutralized we present two method of expression neutralization the first one us prior knowledge to infer the neutral expression image from an input image the second method specifically designed for verification is based on the transfer of the gallery face expression to the probe experiment using rectified and neutralized view with a standard commercial fr sdk on two d face database namely multi pie and ar show significant performance improvement of the commercial sdk to deal with expression and pose variation and demonstrates the effectiveness of the proposed approach 
inferring human gaze from low resolution eye image is still a challenging task despite it practical importance in many application scenario this paper present a learning by synthesis approach to accurate image based gaze estimation that is personand head pose independent unlike existing appearance based method that assume person specific training data we use a large amount of cross subject training data to train a d gaze estimator we collect the largest and fully calibrated multi view gaze dataset and perform a d reconstruction in order to generate dense training data of eye image by using the synthesized dataset to learn a random regression forest we show that our method outperforms existing method that use low resolution eye image 
in this paper we deal with the image deblurring problem in a completely new perspective by proposing separable kernel to represent the inherent property of the camera and scene system specifically we decompose a blur kernel into three individual descriptor trajectory intensity and point spread function so that they can be optimized separately to demonstrate the advantage we extract one pixel width trajectory of blur kernel and propose a random perturbation algorithm to optimize them but still keeping their continuity for many case where current deblurring approach fall into local minimum excellent deblurred result and correct blur kernel can be obtained by individually optimizing the kernel trajectory our work strongly suggests that more constraint and prior should be introduced to blur kernel in solving the deblurring problem because blur kernel have lower dimension than image 
in this paper we tackle the problem of co localization in real world image co localization is the problem of simultaneously localizing with bounding box object of the same class across a set of distinct image although similar problem such a co segmentation and weakly supervised localization have been previously studied we focus on being able to perform co localization in real world setting which are typically characterized by large amount of intra class variation inter class diversity and annotation noise to address these issue we present a joint image box formulation for solving the co localization problem and show how it can be relaxed to a convex quadratic program which can be efficiently solved we perform an extensive evaluation of our method compared to previous state of the art approach on the challenging pascal voc and object discovery datasets in addition we also present a large scale study of co localization on imagenet involving ground truth annotation for class and approximately million image 
in this paper we propose a switchable deep network sdn for pedestrian detection the sdn automatically learns hierarchical feature salience map and mixture representation of different body part pedestrian detection face the challenge of background clutter and large variation of pedestrian appearance due to pose and viewpoint change and other factor one of our key contribution is to propose a switchable restricted boltzmann machine srbm to explicitly model the complex mixture of visual variation at multiple level at the feature level it automatically estimate saliency map for each test sample in order to separate background clutter from discriminative region for pedestrian detection at the part and body level it is able to infer the most appropriate template for the mixture model of each part and the whole body we have devised a new generative algorithm to effectively pretrain the sdn and then fine tune it with back propagation our approach is evaluated on the caltech and eth datasets and achieves the state of the art detection performance 
in this paper we consider the approximate weighted graph matching problem and introduce stable and informative first and second order compatibility term suitable for inclusion into the popular integer quadratic program formulation our approach relies on a rigorous analysis of stability of spectral signature based on the graph laplacian in the case of the first order term we derive an objective function that measure both the stability and informativeness of a given spectral signature by optimizing this objective we design new spectral node signature tuned to a specific graph to be matched we also introduce the pairwise heat kernel distance a a stable second order compatibility term we justify it plausibility by showing that in a certain limiting case it converges to the classical adjacency matrix based second order compatibility function we have tested our approach on a set of synthetic graph the widely used cmu house sequence and a set of real image these experiment show the superior performance of our first and second order compatibility term a compared with the commonly used one 
high dimensional representation such a vlad or fv have shown excellent accuracy in action recognition this paper show that a proper encoding built upon vlad can achieve further accuracy boost with only negligible computational cost we empirically evaluated various vlad improvement technology to determine good practice in vlad based video encoding furthermore we propose an interpretation that vlad is a maximum entropy linear feature learning process combining this new perspective with observed vlad data distribution property we propose a simple lightweight but powerful bimodal encoding method evaluated on benchmark action recognition datasets ucf hmdb and youtube the bimodal encoding improves vlad by large margin in action recognition 
in this paper we present a depth guided photometric d reconstruction method that work solely with a depth camera like the kinect existing method that fuse depth with normal estimate use an external rgb camera to obtain photometric information and treat the depth camera a a black box that provides a low quality depth estimate our contribution to such method are two fold firstly instead of using an extra rgb camera we use the infra red ir camera of the depth camera system itself to directly obtain high resolution photometric information we believe that ours is the first method to use an ir depth camera system in this manner secondly photometric method applied to complex object result in numerous hole in the reconstructed surface due to shadow and self occlusion to mitigate this problem we develop a simple and effective multiview reconstruction approach that fuse depth and normal information from multiple viewpoint to build a complete consistent and accurate d surface representation we demonstrate the efficacy of our method to generate high quality d surface reconstruction for some complex d figurine 
a method for online real time learning of individual object detector is presented starting with a pre trained boosted category detector an individual object detector is trained with near zero computational cost the individual detector is obtained by using the same feature cascade a the category detector along with elementary manipulation of the threshold of the weak classifier this is ideal for online operation on a video stream or for interactive learning application addressed by this technique are reidentification and individual tracking experiment on four challenging pedestrian and face datasets indicate that it is indeed possible to learn identity classifier in real time besides being faster trained our classifier ha better detection rate than previous method on two of the datasets 
depth captured by consumer rgb d camera is often noisy and miss value at some pixel especially around object boundary most existing method complete the missing depth value guided by the corresponding color image when the color image is noisy or the correlation between color and depth is weak the depth map cannot be properly enhanced in this paper we present a depth map enhancement algorithm that performs depth map completion and de noising simultaneously our method is based on the observation that similar rgb d patch lie in a very low dimensional subspace we can then assemble the similar patch into a matrix and enforce this low rank subspace constraint this low rank subspace constraint essentially capture the underlying structure in the rgb d patch and enables robust depth enhancement against the noise or weak correlation between color and depth based on this subspace constraint our method formulates depth map enhancement a a low rank matrix completion problem since the rank of a matrix change over matrix we develop a data driven method to automatically determine the rank number for each matrix the experiment on both public benchmark and our own captured rgb d image show that our method can effectively enhance depth map 
in the following paper we present an approach for fine grained recognition based on a new part detection method in particular we propose a nonparametric label transfer technique which transfer part constellation from object with similar global shape the possibility for transferring part annotation to unseen image allows for coping with a high degree of pose and view variation in scenario where traditional detection model such a deformable part model fail our approach is especially valuable for fine grained recognition scenario where intraclass variation are extremely high and precisely localized feature need to be extracted furthermore we show the importance of carefully designed visual extraction strategy such a combination of complementary feature type and iterative image segmentation and the resulting impact on the recognition performance in experiment our simple yet powerful approach achieves and accuracy on the cub and bird datasets which is the current best performance for these benchmark 
recently introduced cost effective depth sensor coupled with the real time skeleton estimation algorithm of shotton et al have generated a renewed interest in skeleton based human action recognition most of the existing skeleton based approach use either the joint location or the joint angle to represent a human skeleton in this paper we propose a new skeletal representation that explicitly model the d geometric relationship between various body part using rotation and translation in d space since d rigid body motion are member of the special euclidean group se the proposed skeletal representation lie in the lie group se se which is a curved manifold using the proposed representation human action can be modeled a curve in this lie group since classification of curve in this lie group is not an easy task we map the action curve from the lie group to it lie algebra which is a vector space we then perform classification using a combination of dynamic time warping fourier temporal pyramid representation and linear svm experimental result on three action datasets show that the proposed representation performs better than many existing skeletal representation the proposed approach also outperforms various state of the art skeleton based human action recognition approach 
gaussian mixture model have become one of the major tool in modern statistical image processing and allowed performance breakthrough in patch based image denoising and restoration problem nevertheless their adoption level wa kept relatively low because of the computational cost associated to learning such model on large image database this work provides a flexible and generic tool for dealing with such model without the computational penalty or parameter tuning difficulty associated to a na ve implementation of gmm based image restoration task it doe so by organising the data manifold in a hirerachical multiscale structure the covariance tree that can be queried at various scale level around any point in feature space we start by explaining how to construct a covariance tree from a subset of the input data how to enrich it statistic from a larger set in a streaming process and how to query it efficiently at any scale we then demonstrate it usefulness on several application including non local image filtering data driven denoising reconstruction from random sample and surface modeling from unorganized d point set 
we introduce a general framework for quickly annotating an image dataset when previous annotation exist the new annotation e g part location may be quite different from the old annotation e g segmentation human annotator may be thought of a helping translate the old annotation into the new one a annotator label image our algorithm incrementally learns a translator from source to target label a well a a computer vision based structured predictor these two component are combined to form an improved prediction system which accelerates the annotator work through a smart gui we show how the method can be applied to translate between a wide variety of annotation type including bounding box segmentation d and d part based system and class and attribute label the proposed system will be a useful tool toward exploring new type of representation beyond simple bounding box object segmentation and class label and toward finding new way to exploit existing large datasets with traditional type of annotation like sun image net and pascal voc experiment on the cub and h d datasets demonstrate our method accelerates collection of part annotation by a factor of compared to manual labeling our system can be used effectively in a scheme where definition of part attribute or action vocabulary are evolved interactively without relabeling the entire dataset and toward collecting pose annotation segmentation are more useful than bounding box and part level annotation are more effective than segmentation 
we address the challenging problem of utilizing related exemplar for complex event detection while multiple feature are available related exemplar share certain positive element of the event but have no uniform pattern due to the huge variance of relevance level among different related exemplar none of the existing multiple feature fusion method can deal with the related exemplar in this paper we propose an algorithm which adaptively utilizes the related exemplar by cross feature learning ordinal label are used to represent the multiple relevance level of the related video label candidate of related exemplar are generated by exploring the possible relevance level of each related exemplar via a cross feature voting strategy maximum margin criterion is then applied in our framework to discriminate the positive and negative exemplar a well a the related exemplar from different relevance level we test our algorithm using the large scale trecvid dataset and it gain promising performance 
this paper aim for generic instance search from a single example where the state of the art relies on global image representation for the search we proceed by including locality at all step of the method a the first novelty we consider many box per database image a candidate target to search locally in the picture using an efficient point indexed representation the same representation allows a the second novelty the application of very large vocabulary in the powerful fisher vector and vlad to search locally in the feature space a the third novelty we propose an exponential similarity function to further emphasize locality in the feature space locality is advantageous in instance search a it will rest on the matching unique detail we demonstrate a substantial increase in generic instance search performance from one example on three standard datasets with building logo and scene from to in map 
recently there ha been a great interest in computeraided alzheimer s disease ad and mild cognitive impairment mci diagnosis previous learning based method defined the diagnosis process a a classification task and directly used the low level feature extracted from neuroimaging data without considering relation among them however from a neuroscience point of view it s well known that a human brain is a complex system that multiple brain region are anatomically connected and functionally interact with each other therefore it is natural to hypothesize that the low level feature extracted from neuroimaging data are related to each other in some way to this end in this paper we first devise a coupled feature representation by utilizing intra coupled and inter coupled interaction relationship regarding multi modal data fusion we propose a novel coupled boosting algorithm that analyzes the pairwise coupled diversity correlation between modality specifically we formulate a new weight updating function which considers both incorrectly and inconsistently classified sample in our experiment on the adni dataset the proposed method presented the best performance with accuracy of and for ad v normal control nc and mci v nc classification respectively outperforming the competing method and the state of the art method 
laser range sensor are often demanded to mount on a moving platform for achieving the good efficiency of d reconstruction however such moving system often suffer from the difficulty of matching the distorted range scan in this paper we propose novel d feature which can be robustly extracted and matched even for the distorted d surface captured by a moving system our feature extraction employ morse theory to construct morse function which capture the critical point approximately invariant to the d surface distortion then for each critical point we extract support region with the maximally stable region defined by extremal region or disconnectivity our feature description is designed a two step we normalize the detected local region to canonical shape for robust matching we encode each key point with multiple vector at different morse function value in experiment we demonstrate that the proposed d feature achieve substantially better performance for distorted surface matching than the state of the art method 
dense d reconstruction of real world object containing textureless reflective and specular part is a challenging task using general smoothness prior such a surface area regularization can lead to defect in the form of disconnected part or unwanted indentation we argue that this problem can be solved by exploiting the object class specific local surface orientation e g a car is always close to horizontal in the roof area therefore we formulate an object class specific shape prior in the form of spatially varying anisotropic smoothness term the parameter of the shape prior are extracted from training data we detail how our shape prior formulation directly fit into recently proposed volumetric multi label reconstruction approach this allows a segmentation between the object and it supporting ground in our experimental evaluation we show reconstruction using our trained shape prior on several challenging datasets 
we show that a non isotropic near point light source rigidly attached to a camera can be calibrated using multiple image of a weakly textured planar scene we prove that if the radiant intensity distribution rid of a light source is radially symmetric with respect to it dominant direction then the shading observed on a lambertian scene plane is bilaterally symmetric with respect to a d line on the plane the symmetry axis detected in an image provides a linear constraint for estimating the dominant light axis the light position and rid parameter can then be estimated using a linear method specular highlight if available can also be used for light position estimation we also extend our method to handle non lambertian reflectance which we model using a biquadratic brdf we have evaluated our method on synthetic data quantitavely our experiment on real scene show that our method work well in practice and enables light calibration without the need of a specialized hardware 
the recent advance in rgb d camera have allowed u to better solve increasingly complex computer vision task however modern rgb d camera are still restricted by the short effective distance the limitation may make rgb d camera not online accessible in practice and degrade their applicability we propose an alternative scenario to address this problem and illustrate it with the application to action recognition we use kinect to offline collect an auxiliary multi modal database in which not only the rgb video but also the depth map and skeleton structure of action of interest are available our approach aim to enhance action recognition in rgb video by leveraging the extra database specifically it optimizes a feature transformation by which the action to be recognized can be concisely reconstructed by entry in the auxiliary database in this way the inter database variation are adapted more importantly each action can be augmented with additional depth and skeleton image retrieved from the auxiliary database the proposed approach ha been evaluated on three benchmark of action recognition the promising result manifest that the augmented depth and skeleton feature can lead to remarkable boost in recognition accuracy 
robust tracking of deformable object like catheter or vascular structure in x ray image is an important technique used in image guided medical intervention for effective motion compensation and dynamic multi modality image fusion tracking of such anatomical structure and device is very challenging due to large degree of appearance change low visibility of x ray image and the deformable nature of the underlying motion field a a result of complex d anatomical movement projected into d image to address these issue we propose a new deformable tracking method using the tensor based algorithm with model propagation specifically the deformable tracking is formulated a a multi dimensional assignment problem which is solved by rank l tensor approximation the model prior is propagated in the course of deformable tracking both the higher order information and the model prior provide powerful discriminative cue for reducing ambiguity arising from the complex background and consequently improve the tracking robustness to validate the proposed approach we applied it to catheter and vascular structure tracking and tested on x ray fluoroscopic sequence obtained from clinical case the result show both quantitatively and qualitatively that our approach achieves a mean tracking error of pixel for vascular structure and pixel for catheter tracking 
in this paper we propose a novel formulation for multi feature clustering using minimax optimization to find a consensus clustering result that is agreeable to all feature modality our objective is to find a universal feature embedding which not only fit each individual feature modality well but also unifies different feature modality by minimizing their pairwise disagreement the loss function consists of both unary embedding cost for each modality and pairwise disagreement cost for each pair of modality with weighting parameter automatically selected to maximize the loss by performing minimax optimization we can minimize the loss for the worst case with maximum disagreement thus can better reconcile different feature modality to solve the minimax optimization an iterative solution is proposed to update the universal embedding individual embedding and fusion weight separately our minimax optimization ha only one global parameter the superior result on various multi feature clustering task validate the effectiveness of our approach when compared with the state of the art method 
this paper present a unified bag of visual word bow framework for dynamic scene recognition the approach build on primitive feature that uniformly capture spatial and temporal orientation structure of the imagery e g video a extracted via application of a bank of spatiotemporally oriented filter various feature encoding technique are investigated to abstract the primitive to an intermediate representation that is best suited to dynamic scene representation further a novel approach to adaptive pooling of the encoded feature is presented that capture spatial layout of the scene even while being robust to situation where camera motion and scene dynamic are confounded the resulting overall approach ha been evaluated on two standard publically available dynamic scene datasets the result show that in comparison to a representative set of alternative the proposed approach outperforms the previous state of the art in classification accuracy by 
persistent surveillance of large geographic area from unmanned aerial vehicle allows u to learn much about the daily activity in the region of interest nearly all of the approach addressing tracking in this imagery are detection based and rely on background subtraction or frame differencing to provide detection this however make it difficult to track target once they slow down or stop which is not acceptable for persistent tracking our goal we present a multiple target tracking approach that doe not exclusively rely on background subtraction and is better able to track target through stop it accomplishes this by effectively running two tracker in parallel one based on detection from background subtraction providing target initialization and reacquisition and one based on a target state regressor providing frame to frame tracking we evaluated the proposed approach on a long sequence from a wide area aerial imagery dataset and the result show improved object detection rate and id switch rate with limited increase in false alarm compared to the competition 
recently unsupervised image segmentation ha become increasingly popular starting from a superpixel segmentation an edge weighted region adjacency graph is constructed amongst all segmentation of the graph the one which best conforms to the given image evidence a measured by the sum of cut edge weight is chosen since this problem is np hard we propose a new approximate solver based on the move making paradigm first the graph is recursively partitioned into small region cut phase then for any two adjacent region we consider alternative cut of these two region defining possible move glue cut phase for planar problem the optimal move can be found whereas for non planar problem efficient approximation exist we evaluate our algorithm on published and new benchmark datasets which we make available here the proposed algorithm find segmentation that a measured by a loss function are a close to the ground truth a the global optimum found by exact solver it doe so significantly faster then existing approximate method which is important for large scale problem 
we present an image set classification algorithm based on unsupervised clustering of labeled training and unlabeled test data where label are only used in the stopping criterion the probability distribution of each class over the set of cluster is used to define a true set based similarity measure to this end we propose an iterative sparse spectral clustering algorithm in each iteration a proximity matrix is efficiently recomputed to better represent the local subspace structure initial cluster capture the global data structure and finer cluster at the later stage capture the subtle class difference not visible at the global scale image set are compactly represented with multiple grassmannian manifold which are subsequently embedded in euclidean space with the proposed spectral clustering algorithm we also propose an efficient eigenvector solver which not only reduces the computational cost of spectral clustering by many fold but also improves the clustering quality and final classification result experiment on five standard datasets and comparison with seven existing technique show the efficacy of our algorithm 
motivated by a bayesian vision of the d multi view reconstruction from image problem we propose a dense d reconstruction technique that jointly refines the shape and the camera parameter of a scene by minimizing the photometric reprojection error between a generated model and the observed image hence considering all pixel in the original image the minimization is performed using a gradient descent scheme coherent with the shape representation here a triangular mesh where we derive evolution equation in order to optimize both the shape and the camera parameter this can be used at a last refinement step in d reconstruction pipeline and help improving the d reconstruction s quality by estimating the d shape and camera calibration more accurately example are shown for multi view stereo where the texture is also jointly optimized and improved but could be used for any generative approach dealing with multi view reconstruction setting ie depth map fusion multi view photometric stereo 
we propose a data structure that capture global geometric property in image histogram of mirror symmetry coefficient we compute such a coefficient for every pair of pixel and group them in a dimensional histogram by marginalizing the hmsc in various way we develop algorithm for a range of application detection of nearly circular cell location of the main axis of reflection symmetry detection of cell division in movie of developing embryo detection of worm tip and indirect cell counting via supervised classification our approach generalizes a series of histogram related method and the proposed algorithm perform with state of the art accuracy 
in this paper we address the problem of recognizing image with weakly annotated text tag most previous work either cannot be applied to the scenario where the tag are loosely related to the image or simply take a pre fusion at the feature level or a post fusion at the decision level to combine the visual and textual content instead we first encode the text tag a the relation among the image and then propose a semi supervised relational topic model s rtm to explicitly model the image content and their relation in such way we can efficiently leverage the loosely related tag and build an intermediate level representation for a collection of weakly annotated image the intermediate level representation can be regarded a a mid level fusion of the visual and textual content which is able to explicitly model their intrinsic relationship moreover image category label are also modeled in the s rtm and recognition can be conducted without training an additional discriminative classifier our extensive experiment on social multimedia datasets image tag demonstrated the advantage of the proposed model 
in this paper we provide the first to the best of our knowledge bayesian formulation of one of the most successful and well studied statistical model of shape and texture i e active appearance model aams to this end we use a simple probabilistic model for texture generation assuming both gaussian noise and a gaussian prior over a latent texture space we retrieve the shape parameter by formulating a novel cost function obtained by marginalizing out the latent texture space this result in a fast implementation when compared to other simultaneous algorithm for fitting aams mainly due to the removal of the calculation of texture parameter we demonstrate that contrary to what is believed regarding the performance of aams in generic fitting scenario optimization of the proposed cost function produce result that outperform discriminatively trained state of the art method in the problem of facial alignment in the wild 
in this paper we propose an efficient method to reconstruct surface from gradient sfg our method is formulated under the framework of discrete geometry processing unlike the existing sfg approach we transfer the continuous reconstruction problem into a discrete space and efficiently solve the problem via a sequence of least square optimization step our discrete formulation brings three advantage the reconstruction preserve sharp feature sparse incomplete set of gradient can be well handled and domain of computation can have irregular boundary our formulation is direct and easy to implement and the comparison with state of the art show the effectiveness of our method 
given a single outdoor image this paper proposes a collaborative learning approach for labeling it a either sunny or cloudy never adequately addressed this twoclass classification problem is by no mean trivial given the great variety of outdoor image our weather feature combine special cue after properly encoding them into feature vector they then work collaboratively in synergy under a unified optimization framework that is aware of the presence or absence of a given weather cue during learning and classification extensive experiment and comparison are performed to verify our method we build a new weather image dataset consisting of k sunny and cloudy image which is available online together with the executable 
we propose a purely geometric correspondence free approach to urban geo localization using d point ray feature extracted from the digital elevation map of an urban environment we derive a novel formulation for estimating the camera pose locus using d to d correspondence of a single point and a single direction alone we show how this allows u to compute putative correspondence between building corner in the dem and the query image by exhaustively combining pair of point ray feature then we employ the two point method to estimate both the camera pose and compute correspondence between building in the dem and the query image finally we show that the computed camera pose can be efficiently ranked by a simple skyline projection step using building edge from the dem our experimental evaluation illustrates the promise of a purely geometric approach to the urban geo localization problem 
the output of many algorithm in computer vision is either non binary map or binary map e g salient object detection and object segmentation several measure have been suggested to evaluate the accuracy of these foreground map in this paper we show that the most commonly used measure for evaluating both non binary map and binary map do not always provide a reliable evaluation this includes the area under the curve measure the average precision measure the f measure and the evaluation measure of the pascal voc segmentation challenge we start by identifying three cause of inaccurate evaluation we then propose a new measure that amends these flaw an appealing property of our measure is being an intuitive generalization of the f measure finally we propose four meta measure to compare the adequacy of evaluation measure we show via experiment that our novel measure is preferable 
we propose a shape matching method that produce dense correspondence tuned to a specific class of shape and deformation in a scenario where this class is represented by a small set of example shape the proposed method learns a shape descriptor capturing the variability of the deformation in the given class the approach enables the wave kernel signature to extend the class of recognized deformation from near isometry to the deformation appearing in the example set by mean of a random forest classifier with the help of the introduced spatial regularization the proposed method achieves significant improvement over the baseline approach and obtains state of the art result while keeping short computation time 
local video feature provide state of the art performance for action recognition while the accuracy of action recognition ha been continuously improved over the recent year the low speed of feature extraction and subsequent recognition prevents current method from scaling up to real size problem we address this issue and first develop highly efficient video feature using motion information in video compression we next explore feature encoding by fisher vector and demonstrate accurate action recognition using fast linear classifier our method improves the speed of video feature extraction feature encoding and action classification by two order of magnitude at the cost of minor reduction in recognition accuracy we validate our approach and compare it to the state of the art on four recent action recognition datasets 
gradient domain method are popular for image processing however these method even the edge preserving one cannot preserve edge well in some case in this paper we present new constraint explicitly to better preserve edge for general gradient domain image filtering and theoretically analyse why these constraint are edge aware our edge aware constraint are easy to implement fast to compute and can be seamlessly integrated into the general gradient domain optimization framework the improved framework can better preserve edge while maintaining similar image filtering effect a the original image filter we also demonstrate the strength of our edge aware constraint on various application such a image smoothing image colorization and poisson image cloning 
we address the problem of estimating the pose of a camera relative to a known d scene from a single rgb d frame we formulate this problem a inversion of the generative rendering procedure i e we want to find the camera pose corresponding to a rendering of the d scene model that is most similar with the observed input this is a non convex optimization problem with many local optimum we propose a hybrid discriminative generative learning architecture that consists of i a set of m predictor which generate m camera pose hypothesis and ii a selector or aggregator that infers the best pose from the multiple pose hypothesis based on a similarity function we are interested in predictor that not only produce good hypothesis but also hypothesis that are different from each other thus we propose and study method for learning marginally relevant predictor and compare their performance when used with different selection procedure we evaluate our method on a recently released d reconstruction dataset with challenging camera pose and scene variability experiment show that our method learns to make multiple prediction that are marginally relevant and can effectively select an accurate prediction furthermore our method outperforms the state of the art discriminative approach for camera relocalization 
we consider the problem of localizing a novel image in a large d model in principle this is just an instance of camera pose estimation but the scale introduces some challenging problem for one it make the correspondence problem very difficult and it is likely that there will be a significant rate of outlier to handle in this paper we use recent theoretical a well a technical advance to tackle these problem many modern camera and phone have gravitational sensor that allow u to reduce the search space further there are new technique to efficiently and reliably deal with extreme rate of outlier we extend these method to camera pose estimation by using accurate approximation and fast polynomial solver experimental result are given demonstrating that it is possible to reliably estimate the camera pose despite more than of outlier correspondence 
in this paper we tackle the problem of unsupervised domain adaptation for classification in the unsupervised scenario where no labeled sample from the target domain are provided a popular approach consists in transforming the data such that the source and target distribution become similar to compare the two distribution existing approach make use of the maximum mean discrepancy mmd however this doe not exploit the fact that probability distribution lie on a riemannian manifold here we propose to make better use of the structure of this manifold and rely on the distance on the manifold to compare the source and target distribution in this framework we introduce a sample selection method and a subspace based method for unsupervised domain adaptation and show that both these manifold based technique outperform the corresponding approach based on the mmd furthermore we show that our subspace based approach yield state of the art result on a standard object recognition benchmark 
in this paper we propose a weighted supervised pooling method for visual recognition system we combine a standard spatial pyramid representation which is commonly adopted to encode spatial information with an appropriate feature space representation favoring semantic information in an appropriate feature space for the latter we propose a weighted pooling strategy exploiting data supervision to weigh each local descriptor coherently with it likelihood to belong to a given object class the two representation are then combined adaptively with multiple kernel learning experiment on common benchmark caltech and pascal voc show that our image representation improves the current visual recognition pipeline and it is competitive with similar state of art pooling method we also evaluate our method on a real human robot interaction setting where the pure spatial pyramid representation doe not provide sufficient discriminative power obtaining a remarkable improvement 
in this paper we propose a technique for video object segmentation using patch seam across frame typically seam which are connected path of low energy are utilised for retargeting where the primary aim is to reduce the image size while preserving the salient image content here we adapt the formulation of seam for temporal label propagation the energy function associated with the proposed video seam provides temporal linking of patch across frame to accurately segment the object the proposed energy function take into account the similarity of patch along the seam temporal consistency of motion and spatial coherency of seam label propagation is achieved with high fidelity in the critical boundary region utilising the proposed patch seam to achieve this without additional overhead we curtail the error propagation by formulating boundary region a rough set the proposed approach out perform state of the art supervised and unsupervised algorithm on benchmark datasets 
we present a new method for tracking the d position global orientation and full articulation of human hand following recent advance in model based hypothesize and test method the high dimensional parameter space of hand configuration is explored with a novel evolutionary optimization technique specifically tailored to the problem the proposed method capitalizes on the fact that sample from quasi random sequence such a the sobol have low discrepancy and exhibit a more uniform coverage of the sampled space compared to random sample obtained from the uniform distribution the method ha been tested for the problem of tracking the articulation of a single hand d parameter space and two hand d space extensive experiment have been carried out with synthetic and real data in comparison with state of the art method the quantitative evaluation show that for case of limited computational resource the new approach achieves a speed up of four single hand tracking and eight two hand tracking without compromising tracking accuracy interestingly the proposed method is preferable compared to the state of the art either in the case of limited computational resource or in the case of more complex i e higher dimensional problem thus improving the applicability of the method in a number of application domain 
this paper present a novel algorithm which us compact hash bit to greatly improve the efficiency of non linear kernel svm in very large scale visual classification problem our key idea is to represent each sample with compact hash bit over which an inner product is defined to serve a the surrogate of the original nonlinear kernel then the problem of solving the nonlinear svm can be transformed into solving a linear svm over the hash bit the proposed hash svm enjoys dramatic storage cost reduction owing to the compact binary representation a well a a sub linear training complexity via linear svm a a critical component of hash svm we propose a novel hashing scheme for arbitrary non linear kernel via random subspace projection in reproducing kernel hilbert space our comprehensive analysis reveals a well behaved theoretic bound of the deviation between the proposed hashing based kernel approximation and the original kernel function we also derive requirement on the hash bit for achieving a satisfactory accuracy level several experiment on large scale visual classification benchmark are conducted including one with over million image the result show that hash svm greatly reduces the computational complexity more than ten time faster in many case while keeping comparable accuracy 
several descriptor have been proposed in the past for d shape analysis yet none of them achieves best performance on all shape class in this paper we propose a novel method for d shape analysis using the covariance matrix of the descriptor rather than the descriptor themselves covariance matrix enable efficient fusion of different type of feature and modality they capture using the same representation not only the geometric and the spatial property of a shape region but also the correlation of these property within the region covariance matrix however lie on the manifold of symmetric positive definite spd tensor a special type of riemannian manifold which make comparison and clustering of such matrix challenging in this paper we study covariance matrix in their native space and make use of geodesic distance on the manifold a a dissimilarity measure we demonstrate the performance of this metric on d face matching and recognition task we then generalize the bag of feature paradigm originally designed in euclidean space to the riemannian manifold of spd matrix we propose a new clustering procedure that take into account the geometry of the riemannian manifold we evaluate the performance of the proposed bag of covariance matrix framework on d shape matching and retrieval application and demonstrate it superiority compared to descriptor based technique 
we tackle the problem of weakly labeled semantic segmentation where the only source of annotation are image tag encoding which class are present in the scene this is an extremely difficult problem a no pixel wise labelings are available not even at training time in this paper we show that this problem can be formalized a an instance of learning in a latent structured prediction framework where the graphical model encodes the presence and absence of a class a well a the assignment of semantic label to superpixels a a consequence we are able to leverage standard algorithm with good theoretical property we demonstrate the effectiveness of our approach using the challenging sift flow dataset and show average per class accuracy improvement of over the state of the art 
this paper solves the speed bottleneck of deformable part model dpm while maintaining the accuracy in detection on challenging datasets three prohibitive step in cascade version of dpm are accelerated including d correlation between root filter and feature map cascade part pruning and hog feature extraction for d correlation the root filter is constrained to be low rank so that d correlation can be calculated by more efficient linear combination of d correlation a proximal gradient algorithm is adopted to progressively learn the low rank filter in a discriminative manner for cascade part pruning neighborhood aware cascade is proposed to capture the dependence in neighborhood region for aggressive pruning instead of explicit computation of part score hypothesis can be pruned by score of neighborhood under the first order approximation for hog feature extraction look up table are constructed to replace expensive calculation of orientation partition and magnitude with simpler matrix index operation extensive experiment show that a the proposed method is time faster than the current fastest dpm method with similar accuracy on pascal voc b the proposed method achieves state of the art accuracy on pedestrian and face detection task with frame rate speed 
we develop a sequential optimal sampling framework for stereo disparity estimation by adapting the sequential probability ratio test sprt model we operate over local image neighborhood by iteratively estimating single pixel disparity value until sufficient evidence ha been gathered to either validate or contradict the current hypothesis regarding local scene structure the output of our sampling is a set of sampled pixel position along with a robust and compact estimate of the set of disparity contained within a given region we further propose an efficient plane propagation mechanism that leverage the pre computed sampling position and the local structure model described by the reduced local disparity set our sampling framework is a general pre processing mechanism aimed at reducing computational complexity of disparity search algorithm by ascertaining a reduced set of disparity hypothesis for each pixel experiment demonstrate the effectiveness of the proposed approach when compared to state of the art method 
we present the discriminative fern ensemble dfe classifier for efficient visual object recognition the classifier architecture is designed to optimize both classification speed and accuracy when a large training set is available speed is obtained using simple binary feature and direct indexing into a set of table and accuracy by using a large capacity model and careful discriminative optimization the proposed framework is applied to the problem of hand pose recognition in depth and infra red image using a very large training set both the accuracy and the classification time obtained are considerably superior to relevant competing method allowing one to reach accuracy target with run time order of magnitude faster than the competition we show empirically that using dfe we can significantly reduce classification time by increasing training sample size for a fixed target accuracy finally a dfe result is shown for the mnist dataset showing the method s merit extends beyond depth image 
the problem of how to arrive at an appropriate d segmentation of a scene remains difficult while current state of the art method continue to gradually improve in benchmark performance they also grow more and more complex for example by incorporating chain of classifier which require training on large manually annotated data set a an alternative to this we present a new efficient learningand model free approach for the segmentation of d point cloud into object part the algorithm begin by decomposing the scene into an adjacency graph of surface patch based on a voxel grid edge in the graph are then classified a either convex or concave using a novel combination of simple criterion which operate on the local geometry of these patch this way the graph is divided into locally convex connected subgraphs which with high accuracy represent object part additionally we propose a novel depth dependent voxel grid to deal with the decreasing point density at far distance in the point cloud this improves segmentation allowing the use of fixed parameter for vastly different scene the algorithm is straightforward to implement and requires no training data while nevertheless producing result that are comparable to state of the art method which incorporate high level concept involving classification learning and model fitting 
we present a novel method for multiple people tracking that leverage a generalized model for capturing interaction among individual at the core of our model lie a learned dictionary of interaction feature string which capture relationship between the motion of target these feature string created from low level image feature lead to a much richer representation of the physical interaction between target compared to hand specified social force model that previous work have introduced for tracking one disadvantage of using social force is that all pedestrian must be detected in order for the force to be applied while our method is able to encode the effect of undetected target making the tracker more robust to partial occlusion the interaction feature string are used in a random forest framework to track target according to the feature surrounding them result on six publicly available sequence show that our method outperforms state of the art approach in multiple people tracking 
in this paper we study the problem of blind deconvolution our analysis is based on the algorithm of chan and wong which popularized the use of sparse gradient prior via total variation we use this algorithm because many method in the literature are essentially adaptation of this framework such algorithm is an iterative alternating energy minimization where at each step either the sharp image or the blur function are reconstructed recent work of levin et al showed that any algorithm that try to minimize that same energy would fail a the desired solution ha a higher energy than the no blur solution where the sharp image is the blurry input and the blur is a dirac delta however experimentally one can observe that chan and wong s algorithm converges to the desired solution even when initialized with the no blur one we provide both analysis and experiment to resolve this paradoxical conundrum we find that both claim are right the key to understanding how this is possible lie in the detail of chan and wong s implementation and in how seemingly harmless choice result in dramatic effect our analysis reveals that the delayed scaling normalization in the iterative step of the blur kernel is fundamental to the convergence of the algorithm this then result in a procedure that eludes the no blur solution despite it being a global minimum of the original energy we introduce an adaptation of this algorithm and show that in spite of it extreme simplicity it is very robust and achieves a performance comparable to the state of the art 
we present a novel feature description algorithm to describe d local spatio temporal feature for human action recognition our descriptor avoids the singularity and limited discrimination power issue of traditional d descriptor by quantizing and describing visual feature in the simplex topological vector space specifically given a feature s support region containing a set of d visual cue we decompose the cue orientation into three angle transform the decomposed angle into the simplex space and describe them in such a space then quadrant decomposition is performed to improve discrimination and a final feature vector is composed from the resulting histogram we develop intuitive visualization tool for analyzing feature characteristic in the simplex topological vector space experimental result demonstrate that our novel simplex based orientation decomposition sod descriptor substantially outperforms traditional d descriptor for the kth ucf sport and hollywood benchmark action datasets in addition the result show that our sod descriptor is a superior individual descriptor for action recognition 
previous effort in hashing intend to preserve data variance or pairwise affinity but neither is adequate in capturing the manifold structure hidden in most visual data in this paper we tackle this problem by reconstructing the locally linear structure of manifold in the binary hamming space which can be learned by locality sensitive sparse coding we cast the problem a a joint minimization of reconstruction error and quantization loss and show that despite it np hardness a local optimum can be obtained efficiently via alternative optimization our method distinguishes itself from existing method in it remarkable ability to extract the nearest neighbor of the query from the same manifold instead of from the ambient space on extensive experiment on various image benchmark our result improve previous state of the art by typically and on the yale face data 
in this paper we tackle the problem of retrieving video using complex natural language query towards this goal we first parse the sentential description into a semantic graph which is then matched to visual concept using a generalized bipartite matching algorithm our approach exploit object appearance motion and spatial relation and learns the importance of each term using structure prediction we demonstrate the effectiveness of our approach on a new dataset designed for semantic search in the context of autonomous driving which exhibit complex and highly dynamic scene with many object we show that our approach is able to locate a major portion of the object described in the query with high accuracy and improve the relevance in video retrieval 
this paper attempt to address the problem of recognizing human action while training and testing on distinct datasets when test video are neither labeled nor available during training in this scenario learning of a joint vocabulary or domain transfer technique are not applicable we first explore reason for poor classifier performance when tested on novel datasets and quantify the effect of scene background on action representation and recognition using only the background feature and partitioning of gist feature space we show that the background scene in recent datasets are quite discriminative and can be used classify an action with reasonable accuracy we then propose a new process to obtain a measure of confidence in each pixel of the video being a foreground region using motion appearance and saliency together in a d mrf based framework we also propose multiple way to exploit the foreground confidence to improve bag of word vocabulary histogram representation of a video and a novel histogram decomposition based representation and kernel we used these foreground confidence to recognize action trained on one data set and test on a different data set we have performed extensive experiment on several datasets that improve cross dataset recognition accuracy a compared to baseline method 
while technique that segment shape into visually meaningful part have generated impressive result these technique also have only focused on relatively simple shape such a those composed of a single object either without hole or with few simple hole in many application shape created from image can contain many overlapping object and hole these hole may come from sensor noise may have important part of the shape or may be arbitrarily complex these complexity that appear in real world d shape can pose grand challenge to the existing part segmentation method in this paper we propose a new decomposition method called dual space decomposition that handle complex d shape by recognizing the importance of hole and classifying hole a either topological noise or structurally important feature our method creates a nearly convex decomposition of a given shape by segmenting both the polygon itself and it complementary we compare our result to segmentation produced by nonexpert human subject based on two evaluation method we show that this new decomposition method creates statistically similar to those produced by human subject 
we present a method for finding correspondence between d model from an initial set of feature correspondence our method us a fast voting scheme to separate the inliers from the outlier the novelty of our method lie in the use of a combination of local and global constraint to determine if a vote should be cast on a local scale we use simple low level geometric invariant on a global scale we apply covariant constraint for finding compatible correspondence we guide the sampling for collecting voter by downward dependency on previous voting stage all of this together result in an accurate matching procedure we evaluate our algorithm by controlled and comparative testing on different datasets giving superior performance compared to state of the art method in a final experiment we apply our method for d object detection showing potential use of our method within higher level vision 
we propose a novel solution to the generalized camera pose problem which includes the internal scale of the generalized camera a an unknown parameter this further generalization of the well known absolute camera pose problem ha application in multi frame loop closure while a well calibrated camera rig ha a fixed and known scale camera trajectory produced by monocular motion estimation necessarily lack a scale estimate thus when performing loop closure in monocular visual odometry or registering separate structure from motion reconstruction we must estimate a seven degree of freedom similarity transform from corresponding observation existing approach solve this problem in specialized configuration by aligning d triangulated point or individual camera pose estimate our approach handle general configuration of ray and point and directly estimate the full similarity transformation from the d d correspondence four correspondence are needed in the minimal case which ha eight possible solution the minimal solver can be used in a hypothesize and test architecture for robust transformation estimation our solver also produce a least square estimate in the overdetermined case the approach is evaluated experimentally on synthetic and real datasets and is shown to produce higher accuracy solution to multi frame loop closure than existing approach 
part based visual tracking is advantageous due to it robustness against partial occlusion however how to effectively exploit the confidence score of individual part to construct a robust tracker is still a challenging problem in this paper we address this problem by simultaneously matching part in each of multiple frame which is realized by a locality constrained low rank sparse learning method that establishes multi frame part correspondence through optimization of partial permutation matrix the proposed part matching tracker pmt ha a number of attractive property it exploit the spatial temporal localityconstrained property for robust part matching it match local part from multiple frame jointly by considering their low rank and sparse structure information which can effectively handle part appearance variation due to occlusion or noise the proposed pmt model ha the inbuilt mechanism of leveraging multi mode target template so that the dilemma of template updating when encountering occlusion in tracking can be better handled this contrast with existing method that only do part matching between a pair of frame we evaluate pmt and compare with popular state of the art method on challenging benchmark experimental result show that pmt consistently outperform these existing tracker 
recently a concave optimization approach ha been proposed to solve the robust point matching rpm problem this method is globally optimal but it requires that each model point ha a counterpart in the data point set unfortunately such a requirement may not be satisfied in certain application when there are outlier in both point set to address this problem we relax this condition and reduce the objective function of rpm to a function with few nonlinear term by eliminating the transformation variable the resulting function however is no longer quadratic we prove that it is still concave over the feasible region of point correspondence the branch and bound bnb algorithm can then be used for optimization to further improve the efficiency of the bnb algorithm whose bottleneck lie in the costly computation of the lower bound we propose a new lower bounding scheme which ha a k cardinality linear assignment formulation and can be efficiently solved experimental result show that the proposed algorithm outperforms state of the art in it robustness to disturbance and point matching accuracy 
the transfer learning and domain adaptation problem originate from a distribution mismatch between the source and target data distribution the cause of such mismatch are traditionally considered different thus transfer learning and domain adaptation algorithm are designed to address different issue and cannot be used in both setting unless substantially modified still one might argue that these problem are just different declination of learning to learn i e the ability to leverage over prior knowledge when attempting to solve a new task we propose a learning to learn framework able to leverage over source data regardless of the origin of the distribution mismatch we consider prior model a expert and use their output confidence value a feature we use them to build the new target model combined with the feature from the target data through a high level cue integration scheme this result in a class of algorithm usable in a plug and play fashion over any learning to learn scenario from binary and multi class transfer learning to single and multiple source domain adaptation setting experiment on several public datasets show that our approach consistently achieves the state of the art 
with the widespread availability of video camera we are facing an ever growing enormous collection of unedited and unstructured video data due to lack of an automatic way to generate summary from this large collection of consumer video they can be tedious and time consuming to index or search in this work we propose online video highlighting a principled way of generating short video summarizing the most important and interesting content of an unedited and unstructured video costly both time wise and financially for manual processing specifically our method learns a dictionary from given video using group sparse coding and update atom in the dictionary on the fly a summary video is then generated by combining segment that cannot be sparsely reconstructed using the learned dictionary the online fashion of our proposed method enables it to process arbitrarily long video and start generating summary before seeing the end of the video moreover the processing time required by our proposed method is close to the original video length achieving quasi real time summarization speed theoretical analysis together with experimental result on more than hour of surveillance and youtube video are provided demonstrating the effectiveness of online video highlighting 
we study the problem of human body configuration analysis more specifically human parsing and human pose estimation these two task ie identifying the semantic region and body joint respectively over the human body image are intrinsically highly correlated however previous work generally solve these two problem separately or iteratively in this work we propose a unified framework for simultaneous human parsing and pose estimation based on semantic part by utilizing parselets and mixture of joint group template a the representation for these semantic part we seamlessly formulate the human parsing and pose estimation problem jointly within a unified framework via a tailored and or graph a novel grid layout feature is then designed to effectively capture the spatial co occurrence occlusion information between within the parselets and mjgts thus the mutually complementary nature of these two task can be harnessed to boost the performance of each other the resultant unified model can be solved using the structure learning framework in a principled way comprehensive evaluation on two benchmark datasets for both task demonstrate the effectiveness of the proposed framework when compared with the state of the art method 
we propose a simple yet effective regularized prior based on intensity and gradient for text image deblurring the proposed image prior is motivated by observing distinct property of text image based on this prior we develop an efficient optimization method to generate reliable intermediate result for kernel estimation the proposed method doe not require any complex filtering strategy to select salient edge which are critical to the state of the art deblurring algorithm we discus the relationship with other deblurring algorithm based on edge selection and provide insight on how to select salient edge in a more principled way in the final latent image restoration step we develop a simple method to remove artifact and render better deblurred image experimental result demonstrate that the proposed algorithm performs favorably against the state of the art text image deblurring method in addition we show that the proposed method can be effectively applied to deblur low illumination image 
this work proposes a method to interpret a scene by assigning a semantic label at every pixel and inferring the spatial extent of individual object instance together with their occlusion relationship starting with an initial pixel labeling and a set of candidate object mask for a given test image we select a subset of object that explain the image well and have valid overlap relationship and occlusion ordering this is done by minimizing an integer quadratic program either using a greedy method or a standard solver then we alternate between using the object prediction to refine the pixel label and vice versa the proposed system obtains promising result on two challenging subset of the labelme and sun datasets the largest of which contains image and class 
a common thread that tie together many prior work in scene understanding is their focus on the aspect directly present in a scene such a it categorical classification or the set of object in this work we propose to look beyond the visible element of a scene we demonstrate that a scene is not just a collection of object and their configuration or the label assigned to it pixel it is so much more from a simple observation of a scene we can tell a lot about the environment surrounding the scene such a the potential establishment near it the potential crime rate in the area or even the economic climate here we explore several of these aspect from both the human perception and computer vision perspective specifically we show that it is possible to predict the distance of surrounding establishment such a mcdonald s or hospital even by using scene located far from them we go a step further to show that both human and computer perform well at navigating the environment based only on visual cue from scene lastly we show that it is possible to predict the crime rate in an area simply by looking at a scene without any real time criminal activity simply put here we illustrate that it is possible to look beyond the visible scene 
a compared to the conventional rgb or gray scale image multispectral image msi can deliver more faithful representation for real scene and enhance the performance of many computer vision task in practice however an msi is always corrupted by various noise in this paper we propose an effective msi denoising approach by combinatorially considering two intrinsic characteristic underlying an msi the nonlocal similarity over space and the global correlation across spectrum in specific by explicitly considering spatial self similarity of an msi we construct a nonlocal tensor dictionary learning model with a group block sparsity constraint which make similar full band patch fbp share the same atom from the spatial and spectral dictionary furthermore through exploiting spectral correlation of an msi and assuming over redundancy of dictionary the constrained nonlocal msi dictionary learning model can be decomposed into a series of unconstrained low rank tensor approximation problem which can be readily solved by off the shelf higher order statistic experimental result show that our method outperforms all state of the art msi denoising method under comprehensive quantitative performance measure 
first order greedy selection algorithm have been widely applied to sparsity constrained optimization the main theme of this type of method is to evaluate the function gradient in the previous iteration to update the non zero entry and their value in the next iteration in contrast relatively le effort ha been made to study the second order greedy selection method additionally utilizing the hessian information inspired by the classic constrained newton method we propose in this paper the newton greedy pursuit ntgp method to approximately minimizes a twice differentiable function over sparsity constraint at each iteration ntgp construct a second order taylor expansion to approximate the cost function and estimate the next iterate a the solution of the constructed quadratic model over sparsity constraint parameter estimation error and convergence property of ntgp are analyzed the superiority of ntgp to several representative first order greedy selection method is demonstrated in synthetic and real sparse logistic regression task 
we propose a kernel based framework for computing component from a set of surface normal this framework allows u to easily demonstrate that component analysis can be performed directly upon normal we link previously proposed mapping function the azimuthal equidistant projection aep and principal geodesic analysis pga to our kernel based framework we also propose a new mapping function based upon the cosine distance between normal we demonstrate the robustness of our proposed kernel when trained with noisy training set we also compare our kernel within an existing shape from shading sfs algorithm our spherical representation of normal when combined with the robust property of cosine kernel produce a very robust subspace analysis technique in particular our result within sfs show a substantial qualitative and quantitative improvement over existing technique 
we present an approach that take a single photograph of a child a input and automatically produce a series of age progressed output between and year of age accounting for pose expression and illumination leveraging thousand of photo of child and adult at many age from the internet we first show how to compute average image subspace that are pixel to pixel aligned and model variable lighting these average depict a prototype man and woman aging from to under any desired illumination and capture the difference in shape and texture between age applying these difference to a new photo yield an age progressed result contribution include relightable age subspace a novel technique for subspace to subspace alignment and the most extensive evaluation of age progression technique in the literature 
visual appearance score appearance mixture type and deformation are three important information source for human pose estimation this paper proposes to build a multi source deep model in order to extract non linear representation from these different aspect of information source with the deep model the global high order human body articulation pattern in these information source are extracted for pose estimation the task for estimating body location and the task for human detection are jointly learned using a unified deep model the proposed approach can be viewed a a post processing of pose estimation result and can flexibly integrate with existing method by taking their information source a input by extracting the non linear representation from multiple information source the deep model outperforms state of the art by up to percent on three public benchmark datasets 
we address the problem of large scale fine grained visual categorization describing new method we have used to produce an online field guide to north american bird specie we focus on the challenge raised when such a system is asked to distinguish between highly similar specie of bird first we introduce one v most classifier by eliminating highly similar specie during training these classifier achieve more accurate and intuitive result than common one v all classifier second we show how to estimate spatio temporal class prior from observation that are sampled at irregular and biased location we show how these prior can be used to significantly improve performance we then show state of the art recognition performance on a new large dataset that we make publicly available these recognition method are integrated into the online field guide which is also publicly available 
due to great challenge such a tremendous intra class variation and low image resolution context information ha been playing a more and more important role for accurate and robust event recognition in surveillance video the context information can generally be divided into the feature level context the semantic level context and the prior level context these three level of context provide crucial bottom up middle level and top down information that can benefit the recognition task itself unlike existing research that generally integrate the context information at one of the three level we propose a hierarchical context model that simultaneously exploit context at all three level and systematically incorporate them into event recognition to tackle the learning and inference challenge brought in by the model hierarchy we develop complete learning and inference algorithm for the proposed hierarchical context model based on variational bayes method experiment on virat and ground datasets demonstrate the effectiveness of the proposed hierarchical context model for improving the event recognition performance even under great challenge like large intra class variation and low image resolution 
in this paper we present a novel method to synthesize dynamic texture sequence from extremely few sample e g merely two possibly disparate frame leveraging both markov random field mrfs and manifold learning decomposing a textural image a a set of patch we achieve dynamic texture synthesis by estimating sequence of temporal patch we select candidate for each temporal patch from spatial patch based on mrfs and regard them a sample from a low dimensional manifold after mapping candidate to a low dimensional latent space we estimate the sequence of temporal patch by finding an optimal trajectory in the latent space guided by some key property of trajectory of realistic temporal patch we derive a curvature based trajectory selection algorithm in contrast to the method based on mrfs or dynamic system that rely on a large amount of sample our method is able to deal with the case of extremely few sample and requires no training phase we compare our method with the state of the art and show that our method not only exhibit superior performance on synthesizing texture but it also produce result with pleasing visual effect 
the probabilistic method based on symmetrical gauss mixture model sgmm have achieved great success in point set registration but are seldom used to find the correspondence between two image due to the complexity of the non rigid transformation and too many outlier in this paper we propose an asymmetrical gmm agmm for point set matching between a pair of image different from the previous sgmm the agmm give each gauss component a different weight which is related to the feature similarity between the data point and model point which lead to two effective algorithm the single gauss model for mismatch rejection sgmr algorithm and the agmm algorithm for point set matching the sgmr algorithm iteratively filter mismatch by estimating a non rigid transformation between two image based on the spatial coherence of point set the agmm algorithm combine the feature information with position information of the sift feature point extracted from the image to achieve point set matching so that much more correct correspondence with high precision can be found a number of comparison and evaluation experiment reveal the excellent performance of the proposed sgmr algorithm and agmm algorithm 
in this paper we propose a novel rigid motion segmentation algorithm called randomized voting rv this algorithm is based on epipolar geometry and computes a score using the distance between the feature point and the corresponding epipolar line this score is accumulated and utilized for final grouping our algorithm basically deal with two frame so it is also applicable to the two view motion segmentation problem for evaluation of our algorithm hopkins dataset which is a representative test set for rigid motion segmentation is adopted it consists of two and three rigid motion our algorithm ha provided the most accurate motion segmentation result among all of the state of the art algorithm the average error rate is in addition when there is measurement noise our algorithm is comparable with other state of the art algorithm 
this paper considers the problem of action localization where the objective is to determine when and where certain action appear we introduce a sampling strategy to produce d t sequence of bounding box called tubelets compared to state of the art alternative this drastically reduces the number of hypothesis that are likely to include the action of interest our method is inspired by a recent technique introduced in the context of image localization beyond considering this technique for the first time for video we revisit this strategy for d t sequence obtained from super voxels our sampling strategy advantageously exploit a criterion that reflects how action related motion deviate from background motion we demonstrate the interest of our approach by extensive experiment on two public datasets ucf sport and msr ii our approach significantly outperforms the state of the art on both datasets while restricting the search of action to a fraction of possible bounding box sequence 
the quantification of similarity between image segmentation is a complex yet important task the ideal similarity measure should be unbiased to segmentation of different volume and complexity and be able to quantify and visualise segmentation bias similarity measure based on overlap e g dice score or surface distance e g hausdorff distance clearly do not satisfy all of these property to address this problem we introduce patch based evaluation of image segmentation pei a general method to ass segmentation quality our method is based on finding patch correspondence and the associated patch displacement which allow the estimation of segmentation bias we quantify both the agreement of the segmentation boundary and the conservation of the segmentation shape we further ass the segmentation complexity within patch to weight the contribution of local segmentation similarity to the global score we evaluate pei on both synthetic data and two medical imaging datasets on synthetic segmentation of different shape we provide evidence that pei in comparison to the dice score produce more comparable score ha increased sensitivity and estimate segmentation bias accurately on cardiac magnetic resonance mr image we demonstrate that pei can evaluate the performance of a segmentation method independent of the size or complexity of the segmentation under consideration on brain mr image we compare five different automatic hippocampus segmentation technique using pei finally we visualise the segmentation bias on a selection of the case 
deformable object are everywhere face car bicycle chair etc recently there ha been a wealth of research on training deformable model for object detection part localization and recognition using annotated data in order to train deformable model with good generalization ability a large amount of carefully annotated data is required which is a highly time consuming and costly task we propose the first to the best of our knowledge method for automatic construction of deformable model using image captured in totally unconstrained condition recently referred to a in the wild the only requirement of the method are a crude bounding box object detector and a priori knowledge of the object s shape e g a point distribution model the object detector can be a simple a the viola jones algorithm e g even the cheapest digital camera feature a robust face detector the d shape model can be created by using only a few shape example with deformation in our experiment on facial deformable model we show that the proposed automatically built model not only performs well but also outperforms discriminative model trained on carefully annotated data to the best of our knowledge this is the first time it is shown that an automatically constructed model can perform a well a method trained directly on annotated data 
we derive an easy to implement and efficient algorithm for solving multi label image partitioning problem in the form of the problem addressed by region competition these problem jointly determine a parameter for each of the region in the partition given an estimate of the parameter a fast approximate solution to the multi label sub problem is derived by a global update that us smoothing and thresholding the method is empirically validated to be robust to fine detail of the image that plague local solution further in comparison to global method for the multi label problem the method is more efficient and it is easy for a non specialist to implement we give sample matlab code for the multi label chan vese problem in this paper experimental comparison to the state of the art in multi label solution to region competition show that our method achieves equal or better accuracy with the main advantage being speed and ease of implementation 
we propose a novel discriminative model for semantic labeling in video by incorporating a prior to model both the shape and temporal dependency of an object in video a typical approach for this task is the conditional random field crf which can model local interaction among adjacent region in a video frame recent work ha shown how to incorporate a shape prior into a crf for improving labeling performance but it may be difficult to model temporal dependency present in video by using this prior the conditional restricted boltzmann machine crbm can model both shape and temporal dependency and ha been used to learn walking style from motioncapture data in this work we incorporate a crbm prior into a crf framework and present a new state of the art model for the task of semantic labeling in video in particular we explore the task of labeling part of complex face scene from video in the youtube face database yfdb our combined model outperforms competitive baseline both qualitatively and quantitatively 
in this paper we present a novel refractive calibration method for an underwater stereo camera system where both camera are looking through multiple parallel flat refractive interface at the heart of our method is an important finding that the thickness of the interface can be estimated from a set of pixel correspondence in the stereo image when the refractive axis is given to our best knowledge such a finding ha not been studied or reported moreover by exploring the search space for the refractive axis and using reprojection error a a measure both the refractive axis and the thickness of the interface can be recovered simultaneously our method doe not require any calibration target such a a checkerboard pattern which may be difficult to manipulate when the camera are deployed deep undersea the implementation of our method is simple in particular it only requires solving a set of linear equation of the form ax b and applies sparse bundle adjustment to refine the initial estimated result extensive experiment have been carried out which include simulation with and without outlier to verify the correctness of our method a well a to test it robustness to noise and outlier the result of real experiment are also provided the accuracy of our result is comparable to that of a state of the art method that requires known d geometry of a scene 
haze is one of the major factor that degrade outdoor image removing haze from a single image is known to be severely ill posed and assumption made in previous method do not hold in many situation in this paper we systematically investigate different haze relevant feature in a learning framework to identify the best feature combination for image dehazing we show that the dark channel feature is the most informative one for this task which confirms the observation of he et al from a learning perspective while other haze relevant feature also contribute significantly in a complementary way we also find that surprisingly the synthetic hazy image patch we use for feature investigation serve well a training data for realworld image which allows u to train specific model for specific application experiment result demonstrate that the proposed algorithm outperforms state of the art method on both synthetic and real world datasets 
the goal of this paper is to question the necessity of feature like sift in categorical visual recognition task a an alternative we develop a generative model for the raw intensity of image patch and show that it can support image classification performance on par with optimized sift based technique in a bag of visual word setting key ingredient of the proposed model is a compact dictionary of mini epitome learned in an unsupervised fashion on a large collection of image the use of epitome allows u to explicitly account for photometric and position variability in image appearance we show that this flexibility considerably increase the capacity of the dictionary to accurately approximate the appearance of image patch and support recognition task for image classification we develop histogram based image encoding method tailored to the epitomic representation a well a an epitomic footprint encoding which is easy to visualize and highlight the generative nature of our model we discus in detail computational aspect and develop efficient algorithm to make the model scalable to large task the proposed technique are evaluated with experiment on the challenging pascal voc image classification benchmark 
human pose estimation ha made significant progress during the last year however current datasets are limited in their coverage of the overall pose estimation challenge still these serve a the common source to evaluate train and compare different model on in this paper we introduce a novel benchmark mpii human pose that make a significant advance in term of diversity and difficulty a contribution that we feel is required for future development in human body model this comprehensive dataset wa collected using an established taxonomy of over human activity the collected image cover a wider variety of human activity than previous datasets including various recreational occupational and householding activity and capture people from a wider range of viewpoint we provide a rich set of label including position of body joint full d torso and head orientation occlusion label for joint and body part and activity label for each image we provide adjacent video frame to facilitate the use of motion information given these rich annotation we perform a detailed analysis of leading human pose estimation approach and gaining insight for the success and failure of these method 
graph are a powerful tool to model structured object but it is nontrivial to measure the similarity between two graph in this paper we construct a two graph model to represent human action by recording the spatial and temporal relationship among local feature we also propose a novel family of context dependent graph kernel cgks to measure similarity between graph first local feature are used a the vertex of the two graph model and the relationship among local feature in the intra frame and inter frame are characterized by the edge then the proposed cgks are applied to measure the similarity between action represented by the two graph model graph can be decomposed into number of primary walk group with different walk length and our cgks are based on the context dependent primary walk group matching taking advantage of the context information make the correctly matched primary walk group dominate in the cgks and improves the performance of similarity measurement between graph finally a generalized multiple kernel learning algorithm with a proposed l norm regularization is applied to combine these cgks optimally together and simultaneously train a set of action classifier we conduct a series of experiment on several public action datasets our approach achieves a comparable performance to the state of the art approach which demonstrates the effectiveness of the two graph model and the cgks in recognizing human action 
reconstructing d object from single line drawing is often desirable in computer vision and graphic application if the line drawing of a complex d object is decomposed into primitive of simple shape the object can be easily reconstructed we propose an effective method to conduct the line drawing separation and turn a complex line drawing into parametric d model this is achieved by recursively separating the line drawing using two type of split face our experiment show that the proposed separation method can generate more basic and simple line drawing and it combination with the example based reconstruction can robustly recover wider range of complex parametric d object than previous method 
we propose a multi view depthmap estimation approach aimed at adaptively ascertaining the pixel level data association between a reference image and all the element of a source image set namely we address the question what aggregation subset of the source image set should we use to estimate the depth of a particular pixel in the reference image we pose the problem within a probabilistic framework that jointly model pixel level view selection and depthmap estimation given the local pairwise image photoconsistency the corresponding graphical model is solved by em based view selection probability inference and patchmatch like depth sampling and propagation experimental result on standard multi view benchmark convey the state of the art estimation accuracy afforded by mitigating spurious pixel level data association additionally experiment on large internet crowd sourced data demonstrate the robustness of our approach against unstructured and heterogeneous image capture characteristic moreover the linear computational and storage requirement of our formulation a well a it inherent parallelism enables an efficient and scalable gpu based implementation 
we focus on the problem of estimating the ground plane orientation and location in monocular video sequence from a moving observer our only assumption are that the d ego motion t and the ground plane normal n are orthogonal and that n and t are smooth over time we formulate the problem a a state continuous hidden markov model hmm where the hidden state contains t and n and may be estimated by sampling and decomposing homographies we show that using blocked gibbs sampling we can infer the hidden state with high robustness towards outlier drifting trajectory rolling shutter and an imprecise intrinsic calibration since our approach doe not need any initial orientation prior it work for arbitrary camera orientation in which the ground is visible 
histogram based feature have significantly contributed to recent development of image classification such a by sift local descriptor in this paper we propose a method to efficiently transform those histogram feature for improving the classification performance the l normalized histogram feature is regarded a a probability mass function which is modeled by dirichlet distribution based on the probabilistic modeling we induce the dirichlet fisher kernel for transforming the histogram feature vector the method work on the individual histogram feature to enhance the discriminative power at a low computational cost on the other hand in the bag of feature bof framework the dirichlet mixture model can be extended to gaussian mixture by transforming histogram based local descriptor e g sift and thereby we propose the method of dirichlet derived gmm fisher kernel in the experiment on diverse image classification task including recognition of subordinate object and material texture the proposed method improve the performance of the histogrambased feature and bof based fisher kernel being favorably competitive with the state of the art 
many traditional challenge in reconstructing d motion such a matching across wide baseline and handling occlusion reduce in significance a the number of unique viewpoint increase however to obtain this benefit a new challenge arises estimating precisely which camera observe which point at each instant in time we present a maximum a posteriori map estimate of the time varying visibility of the target point to reconstruct the d motion of an event from a large number of camera our algorithm take a input camera pose and image sequence and output the time varying set of the camera in which a target patch is visibile and it reconstructed trajectory we model visibility estimation a a map estimate by incorporating various cue including photometric consistency motion consistency and geometric consistency in conjunction with a prior that reward consistent visibility in proximal camera an optimal estimate of visibility is obtained by finding the minimum cut of a capacitated graph over camera we demonstrate that our method estimate visibility with greater accuracy and increase tracking performance producing longer trajectory at more location and at higher accuracy than method that ignore visibility or use photometric consistency alone 
in this paper we present a unified method for joint face image analysis i e simultaneously estimating head pose facial expression and landmark position in real world face image to achieve this goal we propose a novel iterative multi output random forest imorf algorithm which explicitly model the relation among multiple task and iteratively exploit such relation to boost the performance of all task specifically a hierarchical face analysis forest is learned to perform classification of pose and expression at the top level while performing landmark position regression at the bottom level on one hand the estimated pose and expression provide strong shape prior to constrain the variation of landmark position on the other hand more discriminative shape related feature could be extracted from the estimated landmark position to further improve the prediction of pose and expression this relatedness of face analysis task is iteratively exploited through several cascaded hierarchical face analysis forest until convergence experiment conducted on publicly available real world face datasets demonstrate that the performance of all individual task are significantly improved by the proposed imorf algorithm in addition our method outperforms state of the art for all three face analysis task 
rotation in closed contour recognition is a puzzling nuisance in most algorithm in this paper we address three fundamental issue brought by rotation in shape is alignment among shape necessary if the answer is no how to exploit information in different rotation and how to use rotation unaware local feature for rotation aware shape recognition we argue that the origin of these issue is the use of hand crafted rotation unfriendly feature and measurement therefore our goal is to learn a set of hierarchical feature that describe all rotated version of a shape a a class with the capability of distinguishing different such class we propose to rotate shape a many time a possible a training sample and learn the hierarchical feature representation by effectively adopting a convolutional neural network we further show that our method is very efficient because the network response of all possible shifted version of the same shape can be computed effectively by re using information in the overlapping area we tested the algorithm on three real datasets swedish leaf dataset eth shape and a subset of the recently collected leafsnap dataset our approach used the curvature scale space and outperformed the state of the art 
current system for scene understanding typically represent object a d or d bounding box while these representation have proven robust in a variety of application they provide only coarse approximation to the true d and d extent of object a a result object object interaction such a occlusion or ground plane contact can be represented only superficially in this paper we approach the problem of scene understanding from the perspective of d shape modeling and design a d scene representation that reason jointly about the d shape of multiple object this representation allows to express d geometry and occlusion on the fine detail level of individual vertex of d wireframe model and make it possible to treat dependency between object such a occlusion reasoning in a deterministic way in our experiment we demonstrate the benefit of jointly estimating the d shape of multiple object in a scene over working with coarse box on the recently proposed kitti dataset of realistic street scene 
the desire of enabling computer to learn semantic concept from large quantity of internet video ha motivated increasing interest on semantic video understanding while video segmentation is important yet challenging for understanding video the main difficulty of video segmentation arises from the burden of labeling training sample making the problem largely unsolved in this paper we present a novel nearest neighbor based label transfer scheme for weakly supervised video segmentation whereas previous weakly supervised video segmentation method have been limited to the two class case our proposed scheme focus on more challenging multiclass video segmentation which find a semantically meaningful label for every pixel in a video our scheme enjoys several favorable property when compared with conventional method first a weakly supervised hashing procedure is carried out to handle both metric and semantic similarity second the proposed nearest neighbor based label transfer algorithm effectively avoids overfitting caused by weakly supervised data third a multi video graph model is built to encourage smoothness between region that are spatiotemporally adjacent and similar in appearance we demonstrate the effectiveness of the proposed scheme by comparing it with several other state of the art weakly supervised segmentation method on one new wild dataset and two other publicly available datasets 
we propose a novel regularity driven framework for facade detection from aerial image of urban scene gini index is used in our work to form an edge based regularity metric relating regularity and distribution sparsity facade region are chosen so that these local regularity are maximized we apply a greedy adaptive region expansion procedure for facade region detection and growing followed by integer quadratic programming for removing overlapping facade to optimize facade coverage our algorithm can handle image that have wide viewing angle and contain more than facade per image the experimental result on image from three different city nyc rome san francisco demonstrate superior performance on facade detection in both accuracy and speed over state of the art method we also show an application of our facade detection for effective cross view facade matching 
mutual occlusion among target can cause track loss or target position deviation because the observation likelihood of an occluded target may vanish even when we have the estimated location of the target this paper present a novel probability framework for multitarget tracking with mutual occlusion the primary contribution of this work is the introduction of a vectorial occlusion variable a part of the solution the occlusion variable describes occlusion state of the target this form the basis of the proposed probability framework with the following further contribution likelihood a new observation likelihood model is presented in which the likelihood of an occluded target is computed by referring to both of the occluded and oc cluding target priori markov random field mrf is used to model the occlusion priori such that le likely circular or cascading type of occlusion have lower priori probability both the occlusion priori and the motion priori take into consideration the state of occlusion optimization a realtime rjmcmc based algorithm with a newmove type called occlusion state update is presented experimental result show that the proposed framework can handle occlusion well even including long duration full occlusion which may cause tracking failure in the traditional method 
in this paper we propose a method to refine geometry of d mesh from the kinect fusion by exploiting shading cue captured from the infrared ir camera of kinect a major benefit of using the kinect ir camera instead of a rgb camera is that the ir image captured by kinect are narrow band image which filtered out most undesired ambient light that make our system robust to natural indoor illumination we define a near light ir shading model which describes the captured intensity a a function of surface normal albedo lighting direction and distance between a light source and surface point to resolve ambiguity in our model between normal and distance we utilize an initial d mesh from the kinect fusion and multi view information to reliably estimate surface detail that were not reconstructed by the kinect fusion our approach directly operates on a d mesh model for geometry refinement the effectiveness of our approach is demonstrated through several challenging real world example 
our goal is to learn a compact discriminative vector representation of a face track suitable for the face recognition task of verification and classification to this end we propose a novel face track descriptor based on the fisher vector representation and demonstrate that it ha a number of favourable property first the descriptor is suitable for track of both frontal and profile face and is insensitive to their pose second the descriptor is compact due to discriminative dimensionality reduction and it can be further compressed using binarization third the descriptor can be computed quickly using hard quantization and it compact size and fast computation render it very suitable for large scale visual repository finally the descriptor demonstrates good generalization when trained on one dataset and tested on another reflecting it tolerance to the dataset bias in the experiment we show that the descriptor exceeds the state of the art on both face verification task youtube face without outside training data and inria buffy benchmark and face classification task using the oxford buffy dataset 
in this paper we propose a novel algorithm for structured sparsity reconstruction this algorithm is based on the iterative reweighted least square irls framework and accelerated by the preconditioned conjugate gradient method the convergence rate of the proposed algorithm is almost the same a that of the traditional irls algorithm that is exponentially fast moreover with the devised preconditioner the computational cost for each iteration is significantly le than that of traditional irls algorithm which make it feasible for large scale problem besides the fast convergence this algorithm can be flexibly applied to standard sparsity group sparsity and overlapping group sparsity problem experiment are conducted on a practical application compressive sensing magnetic resonance imaging result demonstrate that the proposed algorithm achieves superior performance over state of the art algorithm in term of both accuracy and computational cost 
a main theme in object detection are currently discriminative part based model the powerful model that combine all part is then typically only feasible for few constituent which are in turn iteratively trained to make them a strong a possible we follow the opposite strategy by randomly sampling a large number of instance specific part classifier due to their number we cannot directly train a powerful classifier to combine all part therefore we randomly group them into fewer overlapping composition that are trained using a maximum margin approach in contrast to the common rationale of compositional approach we do not aim for semantically meaningful ensemble rather we seek randomized composition that are discriminative and generalize over all instance of a category our approach not only localizes object in cluttered scene but also explains them by parsing with composition and their constituent part we conducted experiment on pascal voc on the voc evaluation server and on the mitindoor scene dataset to the best of our knowledge our randomized max margin composition rm c are the currently best performing single class object detector using only hog feature moreover the individual contribution of composition and their part are evaluated in separate experiment that demonstrate their potential 
in this paper we focus on the problem of point to set classification where single point are matched against set of correlated point since the point commonly lie in euclidean space while the set are typically modeled a element on riemannian manifold they can be treated a euclidean point and riemannian point respectively to learn a metric between the heterogeneous point we propose a novel euclidean to riemannian metric learning framework specifically by exploiting typical riemannian metric the riemannian manifold is first embedded into a high dimensional hilbert space to reduce the gap between the heterogeneous space and meanwhile respect the riemannian geometry of the manifold the final distance metric is then learned by pursuing multiple transformation from the hilbert space and the original euclidean space or it corresponding hilbert space to a common euclidean subspace where classical euclidean distance of transformed heterogeneous point can be measured extensive experiment clearly demonstrate the superiority of our proposed approach over the state of the art method 
we propose a simple yet effective detector for pedestrian detection the basic idea is to incorporate common sense and everyday knowledge into the design of simple and computationally efficient feature a pedestrian usually appear up right in image or video data the problem of pedestrian detection is considerably simpler than general purpose people detection we therefore employ a statistical model of the up right human body where the head the upper body and the lower body are treated a three distinct component our main contribution is to systematically design a pool of rectangular template that are tailored to this shape model a we incorporate different kind of low level measurement the resulting multi modal multi channel haar like feature represent characteristic difference between part of the human body yet are robust against variation in clothing or environmental setting our approach avoids exhaustive search over all possible configuration of rectangle feature and neither relies on random sampling it thus mark a middle ground among recently published technique and yield efficient low dimensional yet highly discriminative feature experimental result on the inria and caltech pedestrian datasets show that our detector reach state of the art performance at low computational cost and that our feature are robust against occlusion 
we explore whether we can observe time s arrow in a temporal sequence is it possible to tell whether a video is running forward or backwards we investigate this somewhat philosophical question using computer vision and machine learning technique we explore three method by which we might detect time s arrow in video sequence based on distinct way in which motion in video sequence might be asymmetric in time we demonstrate good video forward backwards classification result on a selection of youtube video clip and on natively captured sequence with no temporally dependent video compression and examine what motion the model have learned that help discriminate forward from backwards time 
this paper present a system to reconstruct piecewise planar and compact floorplans from image which are then converted to high quality texture mapped model for freeviewpoint visualization there are two main challenge in image based floorplan reconstruction the first is the lack of d information that can be extracted from image by structure from motion and multi view stereo a indoor scene abound with non diffuse and homogeneous surface plus clutter the second challenge is the need of a sophisticated regularization technique that enforces piecewise planarity to suppress clutter and yield high quality texture mapped model our technical contribution are twofold first we propose a novel structure classification technique to classify each pixel to three region floor ceiling and wall which provide d cue even from a single image second we cast floorplan reconstruction a a shortest path problem on a specially crafted graph which enables u to enforce piecewise planarity besides producing compact piecewise planar model this formulation allows u to directly control the number of vertex i e density of the output mesh we evaluate our system on real indoor scene and show that our texture mapped mesh model provide compelling free viewpoint visualization experience when compared against the state of the art and ground truth 
popular figure ground segmentation algorithm generate a pool of boundary aligned segment proposal that can be used in subsequent object recognition engine these algorithm can recover most image object with high accuracy but are usually computationally intensive since many graph cut are computed with different enumeration of segment seed in this paper we propose an algorithm rigor for efficiently generating a pool of overlapping segment proposal in image by precomputing a graph which can be used for parametric min cut over different seed we speed up the generation of the segment pool in addition we have made design choice that avoid extensive computation without losing performance in particular we demonstrate that the segmentation performance of our algorithm is slightly better than the state of the art on the pascal voc dataset while being an order of magnitude faster 
graph based method are a useful class of method for improving the performance of unsupervised and semi supervised machine learning task such a clustering or information retrieval however the performance of existing graph based method is highly dependent on how well the affinity graph reflects the original data structure we propose that multimedia such a image or video consist of multiple separate component and therefore more than one graph is required to fully capture the relationship between them accordingly we present a new spectral method the feature grouped spectral multigraph fgsm which comprises the following step first mutually independent subset of the original feature space are generated through feature clustering secondly a separate graph is generated from each feature subset finally a spectral embedding is calculated on each graph and the embeddings are scaled aggregated into a single representation using this representation a variety of experiment are performed on three learning task clustering retrieval and recognition on human action datasets demonstrating considerably better performance than the state of the art 
in this paper we propose a novel method for image fusion from a high resolution panchromatic image and a low resolution multispectral image at the same geographical location different from previous method we do not make any assumption about the upsampled multispectral image but only assume that the fused image after downsampling should be close to the original multispectral image this is a severely ill posed problem and a dynamic gradient sparsity penalty is thus proposed for regularization incorporating the intracorrelations of different band this penalty can effectively exploit the prior information e g sharp boundary from the panchromatic image a new convex optimization algorithm is proposed to efficiently solve this problem extensive experiment on four multispectral datasets demonstrate that the proposed method significantly outperforms the state of the art in term of both spatial and spectral quality 
we address the problem of camouflaging a d object from the many viewpoint that one might see it from given photograph of an object s surroundings we produce a surface texture that will make the object difficult for a human to detect to do this we introduce several background matching algorithm that attempt to make the object look like whatever is behind it of course it is impossible to exactly match the background from every possible viewpoint thus our model are forced to make trade offs between different perceptual factor such a the conspicuousness of the occlusion boundary and the amount of texture distortion we use experiment with human subject to evaluate the effectiveness of these model for the task of camouflaging a cube finding that they significantly outperform na ve strategy 
we describe a simple and fast algorithm for optimizing markov random field over image the algorithm performs block coordinate descent by optimally updating a horizontal or vertical line in each step while the algorithm is not a accurate a state of the art mrf solver on traditional benchmark problem it is trivially parallelizable and produce competitive result in a fraction of a second a an application we develop an approach to increasing the accuracy of consumer depth camera the presented algorithm enables high resolution mrf optimization at multiple frame per second and substantially increase the accuracy of the produced range image 
in this paper we propose an efficient and accurate scheme for the integration of multiple stereo based depth measurement for each provided depth map a confidence based weight is assigned to each depth estimate by evaluating local geometry orientation underlying camera setting and photometric evidence subsequently all hypothesis are fused together into a compact and consistent d model thereby visibility conflict are identified and resolved and fitting measurement are averaged with regard to their confidence score the individual stage of the proposed approach are validated by comparing it to two alternative technique which rely on a conceptually different fusion scheme and a different confidence inference respectively pursuing live d reconstruction on mobile device a a primary goal we demonstrate that the developed method can easily be integrated into a system for monocular interactive d modeling by substantially improving it accuracy while adding a negligible overhead to it performance and retaining it interactive potential 
it ha been recently shown that reconstructing an isometric surface from a single d input image matched to a d template wa a well posed problem this however doe not tell u how reconstruction algorithm will behave in practical condition where the amount of perspective is generally small and the projection thus behaves like weak perspective or orthography we here bring answer to what is theoretically recoverable in such imaging condition and explain why existing convex numerical solution and analytical solution to d reconstruction may be unstable we then propose a new algorithm which work under all imaging condition from strong to loose perspective we empirically show that the gain in stability is tremendous bringing our result close to the iterative minimization of a statisticallyoptimal cost our algorithm ha a low complexity is simple and us only one round of linear least square 
this paper proposes an unsupervised method for learning dictionary of hierarchical compositional model for representing natural image each model is in the form of a template that consists of a small group of part template that are allowed to shift their location and orientation relative to each other and each part template is in turn a composition of gabor wavelet that are also allowed to shift their location and orientation relative to each other given a set of unannotated training image a dictionary of such hierarchical template are learned so that each training image can be represented by a small number of template that are spatially translated rotated and scaled version of the template in the learned dictionary the learning algorithm iterates between the following two step image encoding by a template matching pursuit process that involves a bottom up template matching sub process and a top down template localization sub process dictionary re learning by a shared matching pursuit process experimental result show that the proposed approach is capable of learning meaningful template and the learned template are useful for task such a domain adaption and image cosegmentation 
we examine the problem of retrieving high resolution texture of object observed in multiple video under small object deformation in the monocular case the data redundancy necessary to reconstruct a high resolution image stem from temporal accumulation this ha been vastly explored and is known a image super resolution on the other hand a handful of method have considered the texture of a static d object observed from several camera where the data redundancy is obtained through the different viewpoint we introduce a unified framework to leverage both possibility for the estimation of an object s high resolution texture this framework uniformly deal with any related geometric variability introduced by the acquisition chain or by the evolution over time to this goal we use d warp for all viewpoint and all temporal frame and a linear image formation model from texture to image space despite it simplicity the method is able to successfully handle different view over space and time a shown experimentally it demonstrates the interest of temporal information to improve the texture quality additionally we also show that our method outperforms state of the art multi view super resolution method existing for the static case 
preprocessing a d image often produce a noisy cloud of interest point we study the problem of counting hole in noisy cloud in the plane the hole in a given cloud are quantified by the topological persistence of their boundary contour when the cloud is analyzed at all possible scale we design the algorithm to count hole that are most persistent in the filtration of offset neighborhood around given point the input is a cloud of n point in the plane without any user defined parameter the algorithm ha a near linear time and a linear space o n the output is the array number of hole relative persistence in the filtration we prove theoretical guarantee when the algorithm find the correct number of hole component in the complement of an unknown shape approximated by a cloud 
there have been some recent effort to build visual knowledge base from internet image but most of these approach have focused on bounding box representation of object in this paper we propose to enrich these knowledge base by automatically discovering object and their segmentation from noisy internet image specifically our approach combine the power of generative modeling for segmentation with the effectiveness of discriminative model for detection the key idea behind our approach is to learn and exploit top down segmentation prior based on visual subcategories the strong prior learned from these visual subcategories are then combined with discriminatively trained detector and bottom up cue to produce clean object segmentation our experimental result indicate state of the art performance on the difficult dataset introduced by rubinstein et al we have integrated our algorithm in neil for enriching it knowledge base a of th april neil ha automatically generated approximately k segmentation using web data 
this work proposes a novel framework for optimization in the constrained diffeomorphism space for deformable surface registration first the diffeomorphism space is modeled a a special complex functional space on the source surface the beltrami coefficient space the physically plausible constraint in term of feature landmark and deformation type define subspace in the beltrami coefficient space then the harmonic energy of the registration is minimized in the constrained subspace the minimization is achieved by alternating two step optimization diffuse the beltrami coefficient and projection first deform the conformal structure by the current beltrami coefficient and then compose with a harmonic map from the deformed conformal structure to the target the registration result is diffeomorphic satisfies the physical landmark and deformation constraint and minimizes the conformality distortion experiment on human facial surface demonstrate the efficiency and efficacy of the proposed registration framework 
a new method for learning pooling receptive field for recognition is presented the method exploit the statistic of the d tensor of sift response to an image it is argued that the eigentensors of this tensor contain the information necessary for learning class specific pooling receptive field it is shown that this information can be extracted by a simple pca analysis of a specific tensor flattening a novel algorithm is then proposed for fitting box like receptive field to the eigenimages extracted from a collection of image the resulting receptive field can be combined with any of the recently popular coding strategy for image classification this combination is experimentally shown to improve classification accuracy for both vector quantization and fisher vector fv encoding it is then shown that the combination of the fv encoding with the proposed receptive field ha state of the art performance for both object recognition and scene classification finally when compared with previous attempt at learning receptive field for pooling the method is simpler and achieves better result 
a simple approach to learning invariance in image classification consists in augmenting the training set with transformed version of the original image however given a large set of possible transformation selecting a compact subset is challenging indeed all transformation are not equally informative and adding uninformative transformation increase training time with no gain in accuracy we propose a principled algorithm image transformation pursuit itp for the automatic selection of a compact set of transformation itp work in a greedy fashion by selecting at each iteration the one that yield the highest accuracy gain itp also allows to efficiently explore complex transformation that combine basic transformation we report result on two public benchmark the cub dataset of bird image and the imagenet challenge using fisher vector representation we achieve an improvement from to in top accuracy on cub and an improvement from to in top accuracy on imagenet we also show significant improvement for deep convnet feature from to on cub and from to on imagenet 
subjective image quality assessment iqa is the most reliable way to evaluate the visual quality of digital image perceived by the end user it is often used to construct image quality datasets and provide the groundtruth for building and evaluating objective quality measure subjective test based on the mean opinion score mo have been widely used in previous study but have many known problem such a an ambiguous scale definition and dissimilar interpretation of the scale among subject to overcome these limitation paired comparison pc test have been proposed a an alternative and are expected to yield more reliable result however pc test can be expensive and time consuming since for n image they require n comparison we present a hybrid subjective test which combine mo and pc test via a unified probabilistic model and an active sampling method the proposed method actively construct a set of query consisting of mo and pc test based on the expected information gain provided by each test and can effectively reduce the number of test required for achieving a target accuracy our method can be used in conventional laboratory study a well a crowdsourcing experiment experimental result show the proposed method outperforms state of the art subjective iqa test in a crowdsourced setting 
in this study we propose the application of principal component analysis pca to scale space pca is a standard method used in computer vision the translation of an input image into scale space is a continuous operation which requires the extension of conventional finite matrixbased pca to an infinite number of dimension in this study we use spectral decomposition to resolve this infinite eigenproblem by integration and we propose an approximate solution based on polynomial equation to clarify it eigensolutions we apply spectral decomposition to the gaussian scale space and scale normalized laplacian of gaussian log space a an application of this proposed method we introduce a method for generating gaussian blur image and scale normalized log image where we demonstrate that the accuracy of these image can be very high when calculating an arbitrary scale using a simple linear combination we also propose a new scale invariant feature transform sift detector a a more practical example 
in this paper we present our minimal point and linear point algorithm to estimate the relative pose of a multi camera system with known vertical direction i e known absolute roll and pitch angle we solve the minimal point algorithm with the hidden variable resultant method and show that it lead to an degree univariate polynomial that give up to real solution we identify a degenerated case from the linear point algorithm when it is solved with the standard singular value decomposition svd method and adopt a simple alternative solution which is easy to implement we show that our proposed algorithm can be efficiently used within ransac for robust estimation we evaluate the accuracy of our proposed algorithm by comparison with various existing algorithm for the multi camera system on simulation and show the feasibility of our proposed algorithm with result from multiple real world datasets 
the world is full of object with complex reflectance situated in complex illumination environment past work on full d geometry recovery however ha tried to handle this complexity by framing it into simplistic model of reflectance lambetian mirrored or diffuse plus specular or illumination one or more point light source though there ha been some recent progress in directly utilizing such complexity for recovering a single view geometry it is not clear how such single view method can be extended to reconstruct the full geometry to this end we derive a probabilistic geometry estimation method that fully exploit the rich signal embedded in complex appearance though each observation provides partial and unreliable information we show how to estimate the reflectance responsible for the diverse appearance and unite the orientation cue embedded in each observation to reconstruct the underlying geometry we demonstrate the effectiveness of our method on synthetic and real world object the result show that our method performs accurately across a wide range of real world environment and reflectance that lie between the extreme that have been the focus of past work 
in this paper we present a novel object detection approach that is capable of regressing the aspect ratio of object this result in accurately predicted bounding box having high overlap with the ground truth in contrast to most recent work we employ a random forest for learning a template based model but exploit the nature of this learning algorithm to predict arbitrary output space in this way we can simultaneously predict the object probability of a window in a sliding window approach a well a regress it aspect ratio with a single model furthermore we also exploit the additional information of the aspect ratio during the training of the joint classification regression random forest resulting in better detection model our experiment demonstrate several benefit i our approach give competitive result on standard detection benchmark ii the additional aspect ratio regression delivers more accurate bounding box than standard object detection approach in term of overlap with ground truth especially when tightening the evaluation criterion iii the detector itself becomes better by only including the aspect ratio information during training 
the appearance of an attribute can vary considerably from class to class e g a fluffy dog v a fluffy towel making standard class independent attribute model break down yet training object specific model for each attribute can be impractical and defeat the purpose of using attribute to bridge category boundary we propose a novel form of transfer learning that address this dilemma we develop a tensor factorization approach which given a sparse set of class specific attribute classifier can infer new one for object attribute pair unobserved during training for example even though the system ha no labeled image of striped dog it can use it knowledge of other attribute and object to tailor stripedness to the dog category with two large scale datasets we demonstrate both the need for category sensitive attribute a well a our method s successful transfer our inferred attribute classifier perform similarly well to those trained with the luxury of labeled class specific instance and much better than those restricted to traditional mode of transfer 
heart rate is an important indicator of people s physiological state recently several paper reported method to measure heart rate remotely from face video those method work well on stationary subject under well controlled condition but their performance significantly degrades if the video are recorded under more challenging condition specifically when subject motion and illumination variation are involved we propose a framework which utilizes face tracking and normalized least mean square adaptive filtering method to counter their influence we test our framework on a large difficult and public database mahnob hci and demonstrate that our method substantially outperforms all previous method we also use our method for long term heart rate monitoring in a game evaluation scenario and achieve promising result 
we propose a probabilistic method for parsing a temporal sequence such a a complex activity defined a composition of sub activity action the temporal structure of the high level activity is represented by a string length limited stochastic context free grammar given the grammar a bayes network which we term sequential interval network sin is generated where the variable node correspond to the start and end time of component action the network integrates information about the duration of each primitive action visual detection result for each primitive action and the activity s temporal structure at any moment in time during the activity message passing is used to perform exact inference yielding the posterior probability of the start and end time for each different activity action we provide demonstration of this framework being applied to vision task such a action prediction classification of the high level activity or temporal segmentation of a test sequence the method is also applicable in human robot interaction domain where continual prediction of human action is needed 
we extend the classical linear discriminant analysis lda technique to linear ranking analysis lra by considering the ranking order of class centroid on the projected subspace under the constrain on the ranking order of the class two criterion are proposed minimization of the classification error with the assumption that each class is homogenous guassian distributed maximization of the sum average of the k minimum distance of all neighboring class centroid pair both criterion can be efficiently solved by the convex optimization for one dimensional subspace greedy algorithm is applied to extend the result to the multi dimensional subspace experimental result show that lra with both criterion achieve state of the art performance on the task of ranking learning and zero shot learning and the maximum margin criterion provides a discriminative subspace selection method which can significantly remedy the class separation problem in comparing with several representative extension of lda 
recovering a non rigid d structure from a series of d observation is still a difficult problem to solve accurately many constraint have been proposed to facilitate the recovery and one of the most successful constraint is smoothness due to the fact that most real world object change continuously however many existing method require to determine the degree of smoothness beforehand which is not viable in practical situation in this paper we propose a new probabilistic model that incorporates the smoothness constraint without requiring any prior knowledge our approach regard the sequence of d shape a a simple stationary markov process with procrustes alignment whose parameter are learned during the fitting process the markov process is assumed to be stationary because deformation is finite and recurrent in general and the d shape are assumed to be procrustes aligned in order to discriminate deformation from motion the proposed method outperforms the state of the art method even though the computation time is rather moderate compared to the other existing method 
in kernel based learning the kernel trick transforms the original representation of a feature instance into a vector of similarity with the training feature instance known a kernel representation however feature instance are sometimes ambiguous and the kernel representation calculated based on them do not posse any discriminative information which can eventually harm the trained classifier to address this issue we propose to automatically select good feature instance when calculating the kernel representation in multiple kernel learning specifically for the kernel representation calculated for each input feature instance we multiply it element wise with a latent binary vector named a instance selection variable which target at selecting good instance and attenuate the effect of ambiguous one in the resulting new kernel representation beta process is employed for generating the prior distribution for the latent instance selection variable we then propose a bayesian graphical model which integrates both mkl learning and inference for the distribution of the latent instance selection variable variational inference is derived for model learning under a max margin principle our method is called beta process multiple kernel learning extensive experiment demonstrate the effectiveness of our method on instance selection and it high discriminative capability for various classification problem in vision 
in this paper we exploit natural sentential description of rgb d scene in order to improve d semantic parsing importantly in doing so we reason about which particular object each noun pronoun is referring to in the image this allows u to utilize visual information in order to disambiguate the so called coreference resolution problem that arises in text towards this goal we propose a structure prediction model that exploit potential computed from text and rgb d imagery to reason about the class of the d object the scene type a well a to align the noun pronoun with the referred visual object we demonstrate the effectiveness of our approach on the challenging nyu rgbd v dataset which we enrich with natural lingual description we show that our approach significantly improves d detection and scene classification accuracy and is able to reliably estimate the text to image alignment furthermore by using textual and visual information we are also able to successfully deal with coreference in text improving upon the state of the art stanford coreference system 
we present a new feature representation method for scene text recognition problem particularly focusing on improving scene character recognition many existing method rely on histogram of oriented gradient hog or part based model which do not span the feature space well for character in natural scene image especially given large variation in font with cluttered background in this work we propose a discriminative feature pooling method that automatically learns the most informative sub region of each scene character within a multi class classification framework whereas each sub region seamlessly integrates a set of low level image feature through integral image the proposed feature representation is compact computationally efficient and able to effectively model distinctive spatial structure of each individual character class extensive experiment conducted on challenging datasets char k icdar icdar svt show that our method significantly outperforms existing method on scene character classification and scene text recognition task 
multi target tracking is an interesting but challenging task in computer vision field most previous data association based method merely consider the relationship e g appearance and motion pattern similarity between detection in local limited temporal domain leading to their difficulty in handling long term occlusion and distinguishing the spatially close target with similar appearance in crowded scene in this paper a novel data association approach based on undirected hierarchical relation hypergraph is proposed which formulates the tracking task a a hierarchical dense neighborhood searching problem on the dynamically constructed undirected affinity graph the relationship between different detection across the spatiotemporal domain are considered in a high order way which make the tracker robust to the spatially close target with similar appearance meanwhile the hierarchical design of the optimization process fuel our tracker to long term occlusion with more robustness extensive experiment on various challenging datasets i e pet dataset parkinglot including both low and high density sequence demonstrate that the proposed method performs favorably against the state of the art method 
we take a new approach to computing dense scene flow between a pair of consecutive rgb d frame we exploit the availability of depth data by seeking correspondence with respect to patch specified not a the pixel inside square window but a the d point that are the inliers of sphere in world space our primary contribution is to show that by reasoning in term of such patch under dof rigid body motion in d we succeed in obtaining compelling result at displacement large and small without relying on either of two simplifying assumption that pervade much of the earlier literature brightness constancy or local surface planarity a a consequence of our approach our output is a dense field of d rigid body motion in contrast to the d translation that are the norm in scene flow reasoning in our manner additionally allows u to carry out occlusion handling using a dof consistency check for the flow computed in both direction and a patchwise silhouette check to help reason about alignment in occlusion area and to promote smoothness of the flow field using an intuitive local rigidity prior we carry out our optimization in two step obtaining a first correspondence field using an adaptation of patchmatch and subsequently using alpha expansion to jointly handle occlusion and perform regularization we show attractive flow result on challenging synthetic and real world scene that push the practical limit of the aforementioned assumption 
photo sharing website have become very popular in the last few year leading to huge collection of online image in addition to image data these website collect a variety of multimodal metadata about photo including text tag caption gps coordinate camera metadata user profile etc however this metadata is not well constrained and is often noisy sparse or missing altogether in this paper we propose a framework to model these loosely organized multimodal datasets and show how to perform loosely supervised learning using a novel latent conditional random field framework we learn parameter of the lcrf automatically from a small set of validation data using information theoretic metric learning itml to learn distance function and a structural svm formulation to learn the potential function we apply our framework on four datasets of image from flickr evaluating both qualitatively and quantitatively against several baseline 
we present a tool for re ranking the result of a specific query by considering the n n matrix of pairwise similarity among the element of the set of n retrieved result and the query itself the re ranking thus make use of the similarity between the various result and doe not employ additional source of information the tool is based on graphical bayesian model which reinforce retrieved item strongly linked to other retrieval and on repeated clustering to measure the stability of the obtained association the utility of the tool is demonstrated within the context of visual search of document from the cairo genizah and for retrieval of painting by the same artist and in the same style 
we consider the problem of tracking multiple interacting object in d using rgbd input and by considering a hypothesize and test approach due to their interaction object to be tracked are expected to occlude each other in the field of view of the camera observing them a naive approach would be to employ a set of independent tracker sit and to assign one tracker to each object this approach scale well with the number of object but fails a occlusion become stronger due to their disjoint consideration the solution representing the current state of the art employ a single joint tracker jt that account for all object simultaneously this directly resolve ambiguity due to occlusion but ha a computational complexity that grows geometrically with the number of tracked object we propose a middle ground namely an ensemble of collaborative tracker ect that combine best trait from both world to deliver a practical and accurate solution to the multi object d tracking problem we present quantitative and qualitative experiment with several synthetic and real world sequence of diverse complexity experiment demonstrate that ect manages to track far more complex scene than jt at a computational time that is only slightly larger than that of sit 
we propose a deep learning framework for image set classification with application to face recognition an adaptive deep network template adnt is defined whose parameter are initialized by performing unsupervised pre training in a layer wise fashion using gaussian restricted boltzmann machine grbms the pre initialized adnt is then separately trained for image of each class and class specific model are learnt based on the minimum reconstruction error from the learnt class specific model a majority voting strategy is used for classification the proposed framework is extensively evaluated for the task of image set classification based face recognition on honda ucsd cmu mobo youtube celebrity and a kinect dataset our experimental result and comparison with existing state of the art method show that the proposed method consistently achieves the best performance on all these datasets 
in this paper we study the role of context in existing state of the art detection and segmentation approach towards this goal we label every pixel of pascal voc detection challenge with a semantic category we believe this data will provide plenty of challenge to the community a it contains additional class for semantic segmentation and object detection our analysis show that nearest neighbor based approach perform poorly on semantic segmentation of contextual class showing the variability of pascal imagery furthermore improvement of exist ing contextual model for detection is rather modest in order to push forward the performance in this difficult scenario we propose a novel deformable part based model which exploit both local context around each candidate detection a well a global context at the level of the scene we show that this contextual reasoning significantly help in detecting object at all scale 
we propose an integrated probabilistic model for multi modal fusion of aerial imagery lidar data and optional gps measurement the model allows for analysis and dense reconstruction in term of both geometry and appearance of large d scene an advantage of the approach is that it explicitly model uncertainty and allows for missing data a compared with image based method dense reconstruction of complex urban scene are feasible with fewer observation moreover the proposed model allows one to estimate absolute scale and orientation and reason about other aspect of the scene e g detection of moving object a formulated the model lends itself to massively parallel computing we exploit this in an efficient inference scheme that utilizes both general purpose and domain specific hardware component we demonstrate result on large scale reconstruction of urban terrain from lidar and aerial photography data 
the surface bi directional reflectance distribution function brdf can be used to distinguish different material the brdfs of many real material are near isotropic and can be approximated well by a d function when the camera principal axis is coincident with the surface normal of the material sample the captured brdf slice is nearly d which suffers from significant information loss thus improvement in classification performance can be achieved by simply setting the camera at a slanted view to capture a larger portion of the brdf domain we further use a handheld flashlight camera to capture a d brdf slice for material classification this d slice capture important reflectance property such a specular reflection and retro reflectance we apply these result on ink classification which can be used in forensics and analyzing historical manuscript for the first time we show that most of the ink on the market can be well distinguished by their reflectance property 
scale drift is a crucial challenge for monocular autonomous driving to emulate the performance of stereo this paper present a real time monocular sfm system that corrects for scale drift using a novel cue combination framework for ground plane estimation yielding accuracy comparable to stereo over long driving sequence our ground plane estimation us multiple cue like sparse feature dense inter frame stereo and when applicable object detection a data driven mechanism is proposed to learn model from training data that relate observation covariance for each cue to error behavior of it underlying variable during testing this allows per frame adaptation of observation covariance based on relative confidence inferred from visual data our framework significantly boost not only the accuracy of monocular self localization but also that of application like object localization that rely on the ground plane experiment on the kitti dataset demonstrate the accuracy of our ground plane estimation monocular sfm and object localization relative to ground truth with detailed comparison to prior art 
in this paper we study the configuration of motion and structure that lead to inherent ambiguity in radial distortion estimation or d reconstruction with unknown radial distortion by analyzing the motion field of radially distorted image we solve for critical surface pair that can lead to the same motion field under different radial distortion and possibly different camera motion we study the property of the discovered critical configuration and discus the practically important configuration that often occur in real application we demonstrate the impact of the radial distortion ambiguity on multi view reconstruction with synthetic experiment and real experiment 
this paper present an improvement of the j linkage algorithm for fitting multiple instance of a model to noisy data corrupted by outlier the binary preference analysis implemented by j linkage is replaced by a continuous soft or fuzzy generalization that prof to perform better than j linkage on simulated data and compare favorably with state of the art method on public domain real datasets 
in machine learning and statistic probabilistic inference involving multimodal distribution is quite difficult this is especially true in high dimensional problem where most existing algorithm cannot easily move from one mode to another to address this issue we propose a novel bayesian inference approach based on markov chain monte carlo our method can effectively sample from multimodal distribution especially when the dimension is high and the mode are isolated to this end it exploit and modifies the riemannian geometric property of the target distribution to create wormhole connecting mode in order to facilitate moving between them further our proposed method us the regeneration technique in order to adapt the algorithm by identifying new mode and updating the network of wormhole without affecting the stationary distribution to find new mode a opposed to redis covering those previously identified we employ a novel mode searching algorithm that explores a residual energy function obtained by subtracting an approximate gaussian mixture density based on previously discovered mode from the target density function 
linear dynamical system lds is an elegant mathematical framework for modeling and learning multivariate time series mt however in general it is difficult to set the dimension of an lds s hidden state space a small number of hidden state may not be able to model the complexity of a mt while a large number of hidden state can lead to overfitting in this paper we study learning method that impose various regularization penalty on the transition matrix of the lds model and propose a regularized lds learning framework rlds which aim to automatically shut down ldss spurious and unnecessary dimension and consequently address the problem of choosing the optimal number of hidden state prevent the overfitting problem given a small amount of mt data and support accurate mt forecasting to learn the regularized lds from data we incorporate a second order cone program and a generalized gradient descent method into the maximum a posteriori framework and use expectation maximization to obtain a low rank transition matrix of the lds model we propose two prior for modeling the matrix which lead to two instance of our rlds we show that our rlds is able to recover well the intrinsic dimensionality of the time series dynamic and it improves the predictive performance when compared to baseline on both synthetic and real world mt datasets 
in the proposed thesis we study distributed constraint optimization problem dcops which are problem where several agent coordinate with each other to optimize a global cost function the use of dcops ha gained momentum due to their capability of addressing complex and naturally distributed problem however the adoption of dcop on large problem face two main limitation modeling limitation a current resolution method detach the model from the resolution process assuming that each agent control a single variable of the problem and solving capability a the inability of current approach to capitalize on the presence of structural information which may allow incoherent unnecessary data to reticulate among the agent a well a to exploit structure of the agent s local problem the purpose of the proposed dissertation is to address such limitation studying how to adapt and integrate insight gained from centralized solving technique in order to enhance dcop performance and scalability enabling their use for the resolution of real world complex problem to do so we hypothesize that one can exploit the dcop structure in both problem modeling and problem resolution phase 
coverage function are an important class of discrete function that capture the law of diminishing return arising naturally from application in social network analysis machine learning and algorithmic game theory in this paper we propose a new problem of learning time varying coverage function and develop a novel parametrization of these function using random feature based on the connection between time varying coverage function and counting process we also propose an efficient parameter learning algorithm based on likelihood maximization and provide a sample complexity analysis we applied our algorithm to the influence function estimation problem in information diffusion in social network and show that with few assumption about the diffusion process our algorithm is able to estimate influence significantly more accurately than existing approach on both synthetic and real world data 
this paper study the following problem given sample from a high dimensional discrete distribution we want to estimate the leading mode of the underlying distribution a point is defined to be a mode if it is a local optimum of the density within a neighborhood under metric a we increase the amp quot scale amp quot parameter the neighborhood size increase and the total number of mode monotonically decrease the sequence of the mode reveal intrinsic topographical information of the underlying distribution though the mode finding problem is generally intractable in high dimension this paper unveils that if the distribution can be approximated well by a tree graphical model mode characterization is significantly easier an efficient algorithm with provable theoretical guarantee is proposed and is applied to application like data analysis and multiple prediction 
we consider regularized empirical risk minimization problem in particular we minimize the sum of a smooth empirical risk function and a nonsmooth regularization function when the regularization function is block separable we can solve the minimization problem in a randomized block coordinate descent rbcd manner existing rbcd method usually decrease the objective value by exploiting the partial gradient of a randomly selected block of coordinate in each iteration thus they need all data to be accessible so that the partial gradient of the block gradient can be exactly obtained however such a amp quot batch amp quot setting may be computationally expensive in practice in this paper we propose a mini batch randomized block coordinate descent mrbcd method which estimate the partial gradient of the selected block based on a mini batch of randomly sampled data in each iteration we further accelerate the mrbcd method by exploiting the semi stochastic optimization scheme which effectively reduces the variance of the partial gradient estimator theoretically we show that for strongly convex function the mrbcd method attains lower overall iteration complexity than existing rbcd method a an application we further trim the mrbcd method to solve the regularized sparse learning problem our numerical experiment show that the mrbcd method naturally exploit the sparsity structure and achieves better computational performance than existing method 
we provide statistical and computational analysis of sparse principal component analysis pca in high dimension the sparse pca problem is highly nonconvex in nature consequently though it global solution attains the optimal statistical rate of convergence such solution is computationally intractable to obtain meanwhile although it convex relaxation are tractable to compute they yield estimator with suboptimal statistical rate of convergence on the other hand existing nonconvex optimization procedure such a greedy method lack statistical guarantee in this paper we propose a two stage sparse pca procedure that attains the optimal principal subspace estimator in polynomial time the main stage employ a novel algorithm named sparse orthogonal iteration pursuit which iteratively solves the underlying nonconvex problem however our analysis show that this algorithm only ha desired computational and statistical guarantee within a restricted region namely the basin of attraction to obtain the desired initial estimator that fall into this region we solve a convex formulation of sparse pca with early stopping under an integrated analytic framework we simultaneously characterize the computational and statistical performance of this two stage procedure computationally our procedure converges at the rate of formula see text within the initialization stage and at a geometric rate within the main stage statistically the final principal subspace estimator achieves the minimax optimal statistical rate of convergence with respect to the sparsity level s dimension d and sample size n our procedure motivates a general paradigm of tackling nonconvex statistical learning problem with provable statistical guarantee 
in this paper we study the estimation of the k dimensional sparse principal subspace of covariance matrix in the high dimensional setting we aim to recover the oracle principal subspace solution i e the principal subspace estimator obtained assuming the true support is known a priori to this end we propose a family of estimator based on the semidefinite relaxation of sparse pca with novel regularization in particular under a weak assumption on the magnitude of the population projection matrix one estimator within this family exactly recovers the true support with high probability ha exact rank k and attains a formula see text statistical rate of convergence with s being the subspace sparsity level and n the sample size compared to existing support recovery result for sparse pca our approach doe not hinge on the spiked covariance model or the limited correlation condition a a complement to the first estimator that enjoys the oracle property we prove that another estimator within the family achieves a sharper statistical rate of convergence than the standard semidefinite relaxation of sparse pca even when the previous assumption on the magnitude of the projection matrix is violated we validate the theoretical result by numerical experiment on synthetic datasets 
we propose a new method named calibrated multivariate regression cmr for fitting high dimensional multivariate regression model compared to existing method cmr calibrates the regularization for each regression task with respect to it noise level so that it is simultaneously tuning insensitive and achieves an improved finite sample performance computationally we develop an efficient smoothed proximal gradient algorithm which ha a worst case iteration complexity o where is a pre specified numerical accuracy theoretically we prove that cmr achieves the optimal rate of convergence in parameter estimation we illustrate the usefulness of cmr by thorough numerical simulation and show that cmr consistently outperforms other high dimensional multivariate regression method we also apply cmr on a brain activity prediction problem and find that cmr is a competitive a the handcrafted model created by human expert 
the output of many algorithm in computer vision is either non binary map or binary map e g salient object detection and object segmentation several measure have been suggested to evaluate the accuracy of these foreground map in this paper we show that the most commonly used measure for evaluating both non binary map and binary map do not always provide a reliable evaluation this includes the area under the curve measure the average precision measure the f measure and the evaluation measure of the pascal voc segmentation challenge we start by identifying three cause of inaccurate evaluation we then propose a new measure that amends these flaw an appealing property of our measure is being an intuitive generalization of the f measure finally we propose four meta measure to compare the adequacy of evaluation measure we show via experiment that our novel measure is preferable 
this paper proposes a framework for recognizing complex human activity in video our method describes human activity in a hierarchical discriminative model that operates at three semantic level at the lower level body pose are encoded in a representative but discriminative pose dictionary at the intermediate level encoded pose span a space where simple human action are composed at the highest level our model capture temporal and spatial composition of action into complex human activity our human activity classifier simultaneously model which body part are relevant to the action of interest a well a their appearance and composition using a discriminative approach by formulating model learning in a max margin framework our approach achieves powerful multi class discrimination while providing useful annotation at the intermediate semantic level we show how our hierarchical compositional model provides natural handling of occlusion to evaluate the effectiveness of our proposed framework we introduce a new dataset of composed human activity we provide empirical evidence that our method achieves state of the art activity classification performance on several benchmark datasets 
this paper describes a method of gait recognition from image sequence wherein a subject is accelerating or decelerating a a speed change occurs due to a change of pitch the first order derivative of a phase namely a gait stance and or stride we model this speed change using a cylindrical manifold whose azimuth and height corresponds to the phase and the stride respectively a radial basis function rbf interpolation framework is used to learn subject specific mapping matrix for mapping from manifold to image space given an input image sequence of speed transited gait of a test subject we estimate the mapping matrix of the test subject a well a the phase and stride sequence using an energy minimization framework considering the following three point fitness of the synthesized image to the input image sequence a well a to an eigenspace constructed by exemplar of training subject smoothness of the phase and the stride sequence and pitch and stride fitness to the pitch stride preference model using the estimated mapping matrix we synthesize a constant speed gait image sequence and extract a conventional period based gait feature from it for matching we conducted experiment using real speed transited gait image sequence with subject and demonstrated the effectiveness of the proposed method 
a scene category imposes tight distribution over the kind of object that might appear in the scene the appearance of those object and their layout in this paper we propose a method to learn scene structure that can encode three main interlacing component of a scene the scene category the context specific appearance of object and their layout our experimental evaluation show that our learned scene structure outperform state of the art method of deformable part model in detecting object in a scene our scene structure provides a level of scene understanding that is amenable to deep visual inference the scene structure can also generate feature that can later be used for scene categorization using these feature we also show promising result on scene categorization 
we present a novel co segmentation method for textured d shape our algorithm take a collection of textured shape belonging to the same category and sparse annotation of foreground segment and produce a joint dense segmentation of the shape in the collection we model the segment by a collectively trained gaussian mixture model the final model segmentation is formulated a an energy minimization across all model jointly where intra model edge control the smoothness and separation of model segment and inter model edge impart global consistency we show promising result on two large real world datasets and also compare with previous shape only d segmentation method using publicly available datasets 
we propose an image based facial reenactment system that replaces the face of an actor in an existing target video with the face of a user from a source video while preserving the original target performance our system is fully automatic and doe not require a database of source expression instead it is able to produce convincing reenactment result from a short source video captured with an off the shelf camera such a a webcam where the user performs arbitrary facial gesture our reenactment pipeline is conceived a part image retrieval and part face transfer the image retrieval is based on temporal clustering of target frame and a novel image matching metric that combine appearance and motion to select candidate frame from the source video while the face transfer us a d warping strategy that preserve the user s identity our system excels in simplicity a it doe not rely on a d face model it is robust under head motion and doe not require the source and target performance to be similar we show convincing reenactment result for video that we recorded ourselves and for low quality footage taken from the internet 
current human in the loop fine grained visual categorization system depend on a predefined vocabulary of attribute and part usually determined by expert in this work we move away from that expert driven and attribute centric paradigm and present a novel interactive classification system that incorporates computer vision and perceptual similarity metric in a unified framework at test time user are asked to judge relative similarity between a query image and various set of image these general query do not require expert defined terminology and are applicable to other domain and basic level category enabling a flexible efficient and scalable system for fine grained categorization with human in the loop our system outperforms existing state of the art system for relevance feedback based image retrieval a well a interactive classification resulting in a reduction of up to in the average number of question needed to correctly classify an image 
online multi object tracking aim at producing complete track of multiple object using the information accumulated up to the present moment it still remains a difficult problem in complex scene because of frequent occlusion by clutter or other object similar appearance of different object and other factor in this paper we propose a robust online multi object tracking method that can handle these difficulty effectively we first propose the tracklet confidence using the detectability and continuity of a tracklet and formulate a multi object tracking problem based on the tracklet confidence the multi object tracking problem is then solved by associating tracklets in different way according to their confidence value based on this strategy tracklets sequentially grow with online provided detection and fragmented tracklets are linked up with others without any iterative and expensive association here for reliable association between tracklets and detection we also propose a novel online learning method using an incremental linear discriminant analysis for discriminating the appearance of object by exploiting the proposed learning method tracklet association can be successfully achieved even under severe occlusion experiment with challenging public datasets show distinct performance improvement over other batch and online tracking method 
many learning problem in computer vision can be posed a structured prediction problem where the input and output instance are structured object such a tree graph or string rather than single label or scalar kernel method such a structured support vector machine twin gaussian process tgp structured gaussian process and vector valued reproducing kernel hilbert space rkhs offer powerful way to perform learning and inference over these domain positive definite kernel function allow u to quantitatively capture similarity between a pair of instance over these arbitrary domain a poor choice of the kernel function which decides the rkhs feature space often result in poor performance automatic kernel selection method have been developed but have focused only on kernel on the input domain i e one way in this work we propose a novel and efficient algorithm for learning kernel function simultaneously on both input and output domain we introduce the idea of learning polynomial kernel transformation and call this method simultaneous twin kernel learning stkl stkl can learn arbitrary but continuous kernel function including one way kernel learning a a special case we formulate this problem for learning covariance kernel of twin gaussian process our experimental evaluation using learned kernel on synthetic and several real world datasets demonstrate consistent improvement in performance of tgp s 
we present a novel method for automatic vanishing point detection based on primal and dual point alignment detection the very same point alignment detection algorithm is used twice first in the image domain to group line segment endpoint into more precise line second it is used in the dual domain where converging line become aligned point the use of the recently introduced pclines dual space and a robust point alignment detector lead to a very accurate algorithm experimental result on two public standard datasets show that our method significantly advance the state of the art in the manhattan world scenario while producing state of the art performance in non manhattan scene 
we have discovered that d reconstruction can be achieved from asingle still photographic capture due to accidental motion of thephotographer even while attempting to hold the camera still although these motion result in little baseline and therefore high depth uncertainty in theory we can combine many such measurement over the duration of the capture process a few second to achieve usable depth estimate wepresent a novel d reconstruction system tailored for this problemthat produce depth map from short video sequence from standard cameraswithout the need for multi lens optic active sensor or intentionalmotions by the photographer this result lead to the possibilitythat depth map of sufficient quality for rgb d photography application likeperspective change simulated aperture and object segmentation cancome for free for a significant fraction of still photographsunder reasonable condition 
the real world image database such a flickr are characterized by continuous addition of new image the recent approach for image annotation i e the problem of assigning tag to image have two major drawback first either model are learned using the entire training data or to handle the issue of dataset imbalance tag specific discriminative model are trained such model become obsolete and require relearning when new image and tag are added to database second the task of feature fusion is typically dealt using ad hoc approach in this paper we present a weighted extension of multi view non negative matrix factorization nmf to address the aforementioned drawback the key idea is to learn query specific generative model on the feature of nearest neighbor and tag using the proposed nmf knn approach which imposes consensus constraint on the coefficient matrix across different feature this result in coefficient vector across feature to be consistent and thus naturally solves the problem of feature fusion while the weight matrix introduced in the proposed formulation alleviate the issue of dataset imbalance furthermore our approach being query specific is unaffected by addition of image and tag in a database we tested our method on two datasets used for evaluation of image annotation and obtained competitive result 
we consider the intersection of two research field transfer learning and statistic on manifold in particular we consider for manifold valued data transfer learning of tangent space model such a gaussians distribution pca regression or classifier though one would hope to simply use ordinary rn transfer learning idea the manifold structure prevents it we overcome this by basing our method on inner product preserving parallel transport a well known tool widely used in other problem of statistic on manifold in computer vision at first this straight forward idea seems to suffer from an obvious shortcoming transporting large datasets is prohibitively expensive hindering scalability fortunately with our approach we never transport data rather we show how the statistical model themselves can be transported and prove that for the tangent space model above the transport commute with learning consequently our compact framework applicable to a large class of manifold is not restricted by the size of either the training or test set we demonstrate the approach by transferring pca and logistic regression model of real world data involving d shape and image descriptor 
we pose the following question what happens when test data not only differs from training data but differs from it in a continually evolving way the classic domain adaptation paradigm considers the world to be separated into stationary domain with clear boundary between them however in many real world application example cannot be naturally separated into discrete domain but arise from a continuously evolving underlying process example include video with gradually changing lighting and spam email with evolving spammer tactic we formulate a novel problem of adapting to such continuous domain and present a solution based on smoothly varying embeddings recent work ha shown the utility of considering discrete visual domain a fixed point embedded in a manifold of lower dimensional subspace adaptation can be achieved via transforms or kernel learned between such stationary source and target subspace we propose a method to consider non stationary domain which we refer to a continuous manifold adaptation cma we treat each target sample a potentially being drawn from a different subspace on the domain manifold and present a novel technique for continuous transform based adaptation our approach can learn to distinguish category using training data collected at some point in the past and continue to update it model of the category for some time into the future without receiving any additional label experiment on two visual datasets demonstrate the value of our approach for several popular feature representation 
modeling interaction of multiple co occurring object in a complex activity is becoming increasingly popular in the video domain the dynamic bayesian network dbn ha been applied to this problem in the past due to it natural ability to statistically capture complex temporal dependency however standard dbn structure learning algorithm are generatively learned require manual structure definition and or are computationally complex or restrictive we propose a novel structure learning solution that fuse the granger causality statistic a direct measure of temporal dependence with the adaboost feature selection algorithm to automatically constrain the temporal link of a dbn in a discriminative manner this approach enables u to completely define the dbn structure prior to parameter learning which reduces computational complexity in addition to providing a more descriptive structure we refer to this modeling approach a the granger constraint dbn gcdbn our experiment show how the gcdbn outperforms two of the most relevant state of the art graphical model in complex activity classification on handball video data surveillance data and synthetic data 
the objective of this study is to reconstruct image from bag of visual word bovw which is the de facto standard feature for image retrieval and recognition bovw is defined here a a histogram of quantized descriptor extracted densely on a regular grid at a single scale despite it wide use no report describes reconstruction of the original image of a bovw this task is challenging for two reason bovw includes quantization error when local descriptor are assigned to visual word bovw lack spatial information of local descriptor when we count the occurrence of visual word to tackle this difficult task we use a large scale image database to estimate the spatial arrangement of local descriptor then this task creates a jigsaw puzzle problem with adjacency and global location cost of visual word solving this optimization problem is also challenging because it is known a an np hard problem we propose a heuristic but efficient method to optimize it to underscore the effectiveness of our method we apply it to bovws extracted from about different category and demonstrate that it can reconstruct the original image although the image feature lack spatial information and include quantization error 
we address the problem of estimating the pose of a camera relative to a known d scene from a single rgb d frame we formulate this problem a inversion of the generative rendering procedure i e we want to find the camera pose corresponding to a rendering of the d scene model that is most similar with the observed input this is a non convex optimization problem with many local optimum we propose a hybrid discriminative generative learning architecture that consists of i a set of m predictor which generate m camera pose hypothesis and ii a selector or aggregator that infers the best pose from the multiple pose hypothesis based on a similarity function we are interested in predictor that not only produce good hypothesis but also hypothesis that are different from each other thus we propose and study method for learning marginally relevant predictor and compare their performance when used with different selection procedure we evaluate our method on a recently released d reconstruction dataset with challenging camera pose and scene variability experiment show that our method learns to make multiple prediction that are marginally relevant and can effectively select an accurate prediction furthermore our method outperforms the state of the art discriminative approach for camera relocalization 
it ha long been recognized that one of the fundamental difficulty in the estimation of two view epipolar geometry is the capability of handling outlier in this paper we develop a fast and tractable algorithm that maximizes the number of inlier under the assumption of a purely translating camera compared to classical random sampling method our approach is guaranteed to compute the optimal solution of a cost function based on reprojection error and it ha better time complexity the performance is in fact independent of the inlier outlier ratio of the data this open up for a more reliable approach to robust ego motion estimation our basic translation estimator can be embedded into a system that computes the full camera rotation we demonstrate the applicability in several difficult setting with large amount of outlier it turn out to be particularly well suited for small rotation and rotation around a known axis which is the case for cellular phone where the gravitation axis can be measured experimental result show that compared to standard ransac method based on minimal solver our algorithm produce more accurate estimate in the presence of large outlier ratio 
the prevalent approach to image based localization is matching interest point detected in the query image to a sparse d point cloud representing the known world the obtained correspondence are then used to recover a precise camera pose the state of the art in this field often ignores the availability of a set of d descriptor per d point for example by representing each d point by only it centroid in this paper we demonstrate that these set contain useful information that can be exploited by formulating matching a a discriminative classification problem since memory demand and computational complexity are crucial in such a setup we base our algorithm on the efficient and effective random fern principle we propose an extension which project feature to fern specific embedding space which yield improved matching rate in short runtime experiment first show that our novel formulation provides improved matching performance in comparison to the standard nearest neighbor approach and that we outperform related randomization method in our localization scenario 
this paper present a unified bag of visual word bow framework for dynamic scene recognition the approach build on primitive feature that uniformly capture spatial and temporal orientation structure of the imagery e g video a extracted via application of a bank of spatiotemporally oriented filter various feature encoding technique are investigated to abstract the primitive to an intermediate representation that is best suited to dynamic scene representation further a novel approach to adaptive pooling of the encoded feature is presented that capture spatial layout of the scene even while being robust to situation where camera motion and scene dynamic are confounded the resulting overall approach ha been evaluated on two standard publically available dynamic scene datasets the result show that in comparison to a representative set of alternative the proposed approach outperforms the previous state of the art in classification accuracy by 
this paper proposes a new vectorial total variation prior vtv for color image different from existing vtvs our vtv named the decorrelated vectorial total variation prior d vtv measure the discrete gradient of the luminance component and that of the chrominance one in a separated manner which significantly reduces undesirable uneven color effect moreover a higher order generalization of the d vtv which we call the decorrelated vectorial total generalized variation prior d vtgv is also developed for avoiding the staircasing effect that accompanies the use of vtvs a noteworthy property of the d vt g v is that it enables u to efficiently minimize objective function involving it by a primal dual splitting method experimental result illustrate their utility 
we present a video object segmentation approach that extends the particle filter to a region based image representation image partition is considered part of the particle filter measurement which enriches the available information and lead to a re formulation of the particle filter the prediction step us a co clustering between the previous image object partition and a partition of the current one which allows u to tackle the evolution of non rigid structure particle are defined a union of region in the current image partition and their propagation is computed through a single co clustering the proposed technique is assessed on the segtrack dataset leading to satisfactory perceptual result and obtaining very competitive pixel error rate compared with the state of the art method 
we focus on the problem of estimating the ground plane orientation and location in monocular video sequence from a moving observer our only assumption are that the d ego motion t and the ground plane normal n are orthogonal and that n and t are smooth over time we formulate the problem a a state continuous hidden markov model hmm where the hidden state contains t and n and may be estimated by sampling and decomposing homographies we show that using blocked gibbs sampling we can infer the hidden state with high robustness towards outlier drifting trajectory rolling shutter and an imprecise intrinsic calibration since our approach doe not need any initial orientation prior it work for arbitrary camera orientation in which the ground is visible 
we present a practical framework to automatically detect shadow in real world scene from a single photograph previous work on shadow detection put a lot of effort in designing shadow variant and invariant hand crafted feature in contrast our framework automatically learns the most relevant feature in a supervised manner using multiple convolutional deep neural network convnets the layer network architecture of each convnet consists of alternating convolution and sub sampling layer the proposed framework learns feature at the super pixel level and along the object boundary in both case feature are extracted using a context aware window centered at interest point the predicted posterior based on the learned feature are fed to a conditional random field model to generate smooth shadow contour our proposed framework consistently performed better than the state of the art on all major shadow database collected under a variety of condition 
given two image we want to predict which exhibit a particular visual attribute more than the other even when the two image are quite similar existing relative attribute method rely on global ranking function yet rarely will the visual cue relevant to a comparison be constant for all data nor will human perception of the attribute necessarily permit a global ordering to address these issue we propose a local learning approach for fine grained visual comparison given a novel pair of image we learn a local ranking model on the fly using only analogous training comparison we show how to identify these analogous pair using learned metric with result on three challenging datasets including a large newly curated dataset for fine grained comparison our method outperforms stateof the art method for relative attribute prediction 
a novel model based approach is introduced for real time detection and tracking of the pose of general articulated object a variety of dense motion and depth cue are integrated into a novel articulated iterative closest point approach the proposed method can independently track the six degree of freedom pose of over a hundred of rigid part in real time while at the same time imposing articulation constraint on the relative motion of different part we propose a novel rigidization framework for optimally handling unobservable part during tracking this involves rigidly attaching the minimal amount of unseen part to the rest of the structure in order to most effectively use the currently available knowledge we show how this framework can be used also for detection rather than tracking which allows for automatic system initialization and for incorporating pose estimate obtained from independent object part detector improved performance over alternative solution is demonstrated on real world sequence 
many computer vision problem require optimization of binary non submodular energy we propose a general optimization framework based on local submodular approximation lsa unlike standard lp relaxation method that linearize the whole energy globally our approach iteratively approximates the energy locally on the other hand unlike standard local optimization method e g gradient descent or projection technique we use non linear submodular approximation and optimize them without leaving the domain of integer solution we discus two specific lsa algorithm based on trust region and auxiliary function principle lsa tr and lsa aux these method obtain state of the art result on a wide range of application outperforming many standard technique such a lbp qpbo and trws while our paper is focused on pairwise energy our idea extend to higher order problem the code is available online 
the initial step of many computer vision algorithm are interest point extraction and matching in larger image set the pairwise matching of interest point descriptor between image is an important bottleneck for each descriptor in one image the approximate nearest neighbor in the other one ha to be found and checked against the second nearest neighbor to ensure the correspondence is unambiguous here we asked the question how to best decimate the list of interest point without losing match i e we aim to speed up matching by filtering out in advance those point which would not survive the matching stage it turn out that the best filtering criterion is not the response of the interest point detector which in fact is not surprising the goal of detection are repeatable and well localized point whereas the objective of the selection are point whose descriptor can be matched successfully we show that one can in fact learn to predict which descriptor are matchable and thus reduce the number of interest point significantly without losing too many match we show that this strategy a simple a it is greatly improves the matching success with the same number of point per image moreover we embed the prediction in a state of the art structure from motion pipeline and demonstrate that it also outperforms other selection method at system level 
we use weakly supervised structured learning to track and disambiguate the identity of multiple indistinguishable translucent and deformable object that can overlap for many frame for this challenging problem we propose a novel model which handle occlusion complex motion and non rigid deformation by jointly optimizing the flow of multiple latent intensity across frame these flow are latent variable for which the user cannot directly provide label instead we leverage a structured learning formulation that us weak user annotation to find the best hyperparameters of this model the approach is evaluated on a challenging dataset for the tracking of multiple drosophila larva which we make publicly available our method track multiple larva in spite of their poor distinguishability and minimizes the number of identity switch during prolonged mutual occlusion 
we present a novel way to automatically summarize and represent the storyline of a tv episode by visualizing character interaction a a chart we also propose a scene detection method that lends itself well to generate over segmented scene which is used to partition the video the positioning of character line in the chart is formulated a an optimization problem which trade between the aesthetic and functionality of the chart using automatic person identification we present storygraphs for diverse tv series encompassing a total of episode we define quantitative criterion to evaluate storygraphs and also compare them against episode summary to evaluate their ability to provide an overview of the episode 
the fisher vector fv representation is a high dimensional extension of the popular bag of word representation transformation of the fv by power and normalization ha shown to significantly improve it performance and led to state of the art result for a range of image and video classification and retrieval task these normalization however render the representation non additive over local descriptor combined with it high dimensionality this make the fv computationally expensive for the purpose of localization task in this paper we present approximation to both these normalization which yield significant improvement in the memory and computational cost of the fv when used for localization second we show how these approximation can be used to define upper bound on the score function that can be efficiently evaluated which enables the use of branch and bound search a an alternative to exhaustive sliding window search we present experimental evaluation result on classification and temporal localization of action in video these show that the our approximation lead to a speedup of at least one order of magnitude while maintaining state of the art action recognition and localization performance 
this paper describes a framework for modeling human activity a temporally structured process our approach is motivated by the inherently hierarchical nature of human activity and the close correspondence between human action and speech we model action unit using hidden markov model much like word in speech these action unit then form the building block to model complex human activity a sentence using an action grammar to evaluate our approach we collected a large dataset of daily cooking activity the dataset includes a total of participant each performing a total of cooking activity in multiple real life kitchen resulting in over hour of video footage we evaluate the htk toolkit a state of the art speech recognition engine in combination with multiple video feature descriptor for both the recognition of cooking activity e g making pancake a well a the semantic parsing of video into action unit e g cracking egg our result demonstrate the benefit of structured temporal generative approach over existing discriminative approach in coping with the complexity of human daily life activity 
in this paper we aim for zero shot classification that is visual recognition of an unseen class by using knowledge transfer from known class our main contribution is costa which exploit co occurrence of visual concept in image for knowledge transfer these inter dependency arise naturally between concept and are easy to obtain from existing annotation or web search hit count we estimate a classifier for a new label a a weighted combination of related class using the co occurrence to define the weight we propose various metric to leverage these co occurrence and a regression model for learning a weight for each related class we also show that our zero shot classifier can serve a prior for few shot learning experiment on three multi labeled datasets reveal that our proposed zero shot method are approaching and occasionally outperforming fully supervised svms we conclude that co occurrence statistic suffice for zero shot classification 
this paper present a novel and general method for the detection rectification and segmentation of imaged coplanar repeated pattern the only assumption made of the scene geometry is that repeated scene element are mapped to each other by planar euclidean transformation the class of pattern covered is broad and includes nearly all commonly seen planar man made repeated pattern in addition novel linear constraint are used to reduce geometric ambiguity between the rectified imaged pattern and the scene pattern rectification to within a similarity of the scene plane is achieved from one rotated repeat or to within a similarity with a scale ambiguity along the axis of symmetry from one reflected repeat a stratum of constraint is derived that give the necessary configuration of repeat for each successive level of rectification a generative model for the imaged pattern is inferred and used to segment the pattern with pixel accuracy qualitative result are shown on a broad range of image type on which state of the art method fail 
this paper proposes a method for estimating the d body shape of a person with robustness to clothing we formulate the problem a optimization over the manifold of valid depth map of body shape learned from synthetic training data the manifold itself is represented using a novel data structure a multi resolution manifold forest mrmf which contains vertical edge between tree node a well a horizontal edge between node across tree that correspond to overlapping partition we show that this data structure allows both efficient localization and navigation on the manifold for on the fly building of local linear model manifold charting we demonstrate shape estimation of clothed user showing significant improvement in accuracy over global shape model and model using pre computed cluster we further compare the mrmf with alternative manifold charting method on a public dataset for estimating d motion from noisy d marker observation obtaining state of the art result 
object detection performance a measured on the canonical pascal voc dataset ha plateaued in the last few year the best performing method are complex ensemble system that typically combine multiple low level image feature with high level context in this paper we propose a simple and scalable detection algorithm that improves mean average precision map by more than relative to the previous best result on voc achieving a map of our approach combine two key insight one can apply high capacity convolutional neural network cnns to bottom up region proposal in order to localize and segment object and when labeled training data is scarce supervised pre training for an auxiliary task followed by domain specific fine tuning yield a significant performance boost since we combine region proposal with cnns we call our method r cnn region with cnn feature we also present experiment that provide insight into what the network learns revealing a rich hierarchy of image feature source code for the complete system is available at http www c berkeley edu rbg rcnn 
there ha been a lot of work on face modeling analysis and landmark detection with active appearance model being one of the most successful technique a major drawback of these model is the large number of detailed annotated training example needed for learning therefore we present a transfer learning method that is able to learn from related training data using an instance weighted transfer technique our method is derived using a generalization of importance sampling and in contrast to previous work we explicitly try to tackle the transfer already during learning instead of adapting the fitting process in our studied application of face landmark detection we efficiently transfer facial expression from other human individual and are thus able to learn a precise face active appearance model only from neutral face of a single individual our approach is evaluated on two common face datasets and outperforms previous transfer method 
human are capable of perceiving a scene at a glance and obtain deeper understanding with additional time similarly visual recognition deployment should be robust to varying computational budget such situation require anytime recognition ability which is rarely considered in computer vision research we present a method for learning dynamic policy to optimize anytime performance in visual architecture our model sequentially order feature computation and performs subsequent classification crucially decision are made at test time and depend on observed data and intermediate result we show the applicability of this system to standard problem in scene and object recognition on suitable datasets we can incorporate a semantic back off strategy that give maximally specific prediction for a desired level of accuracy this provides a new view on the time course of human visual perception 
photometric stereo offer the possibility of object shape reconstruction via reasoning about the amount of light reflected from oriented surface however in murky medium such a sea water the illuminating light interacts with the medium and some of it is backscattered towards the camera due to this additive light component the standard photometric stereo equation lead to poor quality shape estimation previous author have attempted to reformulate the approach but have either neglected backscatter entirely or disregarded it non uniformity on the sensor when camera and light are close to each other we show that by compensating effectively for the backscatter component a linear formulation of photometric stereo is allowed which recovers an accurate normal map using only light our backscatter compensation method for point source can be used for estimating the uneven backscatter directly from single image without any prior knowledge about the characteristic of the medium or the scene we compare our method with previous approach through extensive experimental result where a variety of object are imaged in a big water tank whose turbidity is systematically increased and show reconstruction quality which degrades little relative to clean water result even with a very significant scattering level 
we explore whether we can observe time s arrow in a temporal sequence is it possible to tell whether a video is running forward or backwards we investigate this somewhat philosophical question using computer vision and machine learning technique we explore three method by which we might detect time s arrow in video sequence based on distinct way in which motion in video sequence might be asymmetric in time we demonstrate good video forward backwards classification result on a selection of youtube video clip and on natively captured sequence with no temporally dependent video compression and examine what motion the model have learned that help discriminate forward from backwards time 
in this paper we address the problem of synthesizing novel view from a set of input image state of the art method such a the unstructured lumigraph have been using heuristic to combine information from the original view often using an explicit or implicit approximation of the scene geometry while the proposed heuristic have been largely explored and proven to work effectively a bayesian formulation wa recently introduced formalizing some of the previously proposed heuristic pointing out which physical phenomenon could lie behind each however some important heuristic were still not taken into account and lack proper formalization we contribute a new physic based generative model and the corresponding maximum a posteriori estimate providing the desired unification between heuristic based method and a bayesian formulation the key point is to systematically consider the error induced by the uncertainty in the geometric proxy we provide an extensive discussion analyzing how the obtained equation explain the heuristic developed in previous method furthermore we show that our novel bayesian model significantly improves the quality of novel view in particular if the scene geometry estimate is inaccurate 
the surface bi directional reflectance distribution function brdf can be used to distinguish different material the brdfs of many real material are near isotropic and can be approximated well by a d function when the camera principal axis is coincident with the surface normal of the material sample the captured brdf slice is nearly d which suffers from significant information loss thus improvement in classification performance can be achieved by simply setting the camera at a slanted view to capture a larger portion of the brdf domain we further use a handheld flashlight camera to capture a d brdf slice for material classification this d slice capture important reflectance property such a specular reflection and retro reflectance we apply these result on ink classification which can be used in forensics and analyzing historical manuscript for the first time we show that most of the ink on the market can be well distinguished by their reflectance property 
convolutional neural network cnn have recently shown outstanding image classification performance in the largescale visual recognition challenge ilsvrc the success of cnns is attributed to their ability to learn rich mid level image representation a opposed to hand designed low level feature used in other image classification method learning cnns however amount to estimating million of parameter and requires a very large number of annotated image sample this property currently prevents application of cnns to problem with limited training data in this work we show how image representation learned with cnns on large scale annotated datasets can be efficiently transferred to other visual recognition task with limited amount of training data we design a method to reuse layer trained on the imagenet dataset to compute mid level image representation for image in the pascal voc dataset we show that despite difference in image statistic and task in the two datasets the transferred representation lead to significantly improved result for object and action classification outperforming the current state of the art on pascal voc and datasets we also show promising result for object and action localization 
in this paper we introduce a fully automated multistage graphical probabilistic framework to segment brain tumour from multimodal magnetic resonance image mri acquired from real patient an initial bayesian tumour classification based on gabor texture feature permit subsequent computation to be focused on area where the probability of tumour is deemed high an iterative multistage markov random field mrf framework is then devised to classify the various tumour subclass e g edema solid tumour enhancing tumour and necrotic core specifically an adapted voxel based mrf provides tumour candidate to a higher level regional mrf which then leverage both contextual texture information and relative spatial consistency of the tumour subclass position to provide updated regional information down to the voxel based mrf for further local refinement the two stage iterate until convergence experiment are performed on publicly available patient brain tumour image from the miccai and brain tumour segmentation challenge the result demonstrate that the proposed method achieves the top performance in the segmentation of tumour core and enhancing tumour and performs comparably to the winner in other tumour category 
robust multi object tracking by detection requires the correct assignment of noisy detection result to object trajectory we address this problem by proposing an online approach based on the observation that object detector primarily fail if object are significantly occluded in contrast to most existing work we only rely on geometric information to efficiently overcome detection failure in particular we exploit the spatio temporal evolution of occlusion region detector reliability and target motion prediction to robustly handle missed detection in combination with a conservative association scheme for visible object this allows for real time tracking of multiple object from a single static camera even in complex scenario our evaluation on publicly available multi object tracking benchmark datasets demonstrate favorable performance compared to the state of the art in online and offline multi object tracking 
motivated by multi distribution divergence which originate in information theory we propose a notion of multi point kernel and study their application we study a class of kernel based on jensen type divergence and show that these can be extended to measure similarity among multiple point we study tensor flattening method and develop a multi point kernel spectral clustering msc method we further emphasize on a special case of the proposed kernel which is a multi point extension of the linear dot product kernel and show the existence of cubic time tensor flattening algorithm in this case finally we illustrate the usefulness of our contribution using standard data set and image segmentation task 
this work proposes a method to interpret a scene by assigning a semantic label at every pixel and inferring the spatial extent of individual object instance together with their occlusion relationship starting with an initial pixel labeling and a set of candidate object mask for a given test image we select a subset of object that explain the image well and have valid overlap relationship and occlusion ordering this is done by minimizing an integer quadratic program either using a greedy method or a standard solver then we alternate between using the object prediction to refine the pixel label and vice versa the proposed system obtains promising result on two challenging subset of the labelme and sun datasets the largest of which contains image and class 
we introduce a new approach for recognizing and reconstructing d object in image our approach is based on an analysis by synthesis strategy a forward synthesis model construct possible geometric interpretation of the world and then selects the interpretation that best agrees with the measured visual evidence the forward model synthesizes visual template defined on invariant hog feature these visual template are discriminatively trained to be accurate for inverse estimation we introduce an efficient brute force approach to inference that search through a large number of candidate reconstruction returning the optimal one one benefit of such an approach is that recognition is inherently re constructive we show state of the art performance for detection and reconstruction on two challenging d object recognition datasets of car and cuboid 
in this study we propose the application of principal component analysis pca to scale space pca is a standard method used in computer vision the translation of an input image into scale space is a continuous operation which requires the extension of conventional finite matrixbased pca to an infinite number of dimension in this study we use spectral decomposition to resolve this infinite eigenproblem by integration and we propose an approximate solution based on polynomial equation to clarify it eigensolutions we apply spectral decomposition to the gaussian scale space and scale normalized laplacian of gaussian log space a an application of this proposed method we introduce a method for generating gaussian blur image and scale normalized log image where we demonstrate that the accuracy of these image can be very high when calculating an arbitrary scale using a simple linear combination we also propose a new scale invariant feature transform sift detector a a more practical example 
human gesture similar to speech and handwriting are often unique to the individual training a generic classifier applicable to everyone can be very difficult and a such it ha become a standard to use personalized classifier in speech and handwriting recognition in this paper we address the problem of personalization in the context of gesture recognition and propose a novel and extremely efficient way of doing personalization unlike conventional personalization method which learn a single classifier that later get adapted our approach learns a set portfolio of classifier during training one of which is selected for each test subject based on the personalization data we formulate classifier personalization a a selection problem and propose several algorithm to compute the set of candidate classifier our experiment show that such an approach is much more efficient than adapting the classifier parameter but can still achieve comparable or better result 
saliency prediction typically relies on hand crafted multiscale feature that are combined in different way to form a master saliency map which encodes local image conspicuity recent improvement to the state of the art on standard benchmark such a mit have been achieved mostly by incrementally adding more and more hand tuned feature such a car or face detector to existing model in contrast we here follow an entirely automatic data driven approach that performs a large scale search for optimal feature we identify those instance of a richly parameterized bio inspired model family hierarchical neuromorphic network that successfully predict image saliency because of the high dimensionality of this parameter space we use automated hyperparameter optimization to efficiently guide the search the optimal blend of such multilayer feature combined with a simple linear classifier achieves excellent performance on several image saliency benchmark our model outperform the state of the art on mit on which feature and classifier are learned without additional training these model generalize well to two other image saliency data set toronto and nusef despite their different image content finally our algorithm score best of all the model evaluated to date on the mit saliency challenge which us a hidden test set to facilitate an unbiased comparison 
a method for online real time learning of individual object detector is presented starting with a pre trained boosted category detector an individual object detector is trained with near zero computational cost the individual detector is obtained by using the same feature cascade a the category detector along with elementary manipulation of the threshold of the weak classifier this is ideal for online operation on a video stream or for interactive learning application addressed by this technique are reidentification and individual tracking experiment on four challenging pedestrian and face datasets indicate that it is indeed possible to learn identity classifier in real time besides being faster trained our classifier ha better detection rate than previous method on two of the datasets 
algorithm for solving system of polynomial equation are key component for solving geometry problem in computer vision fast and stable polynomial solver are essential for numerous application e g minimal problem or finding for all stationary point of certain algebraic error recently full symmetry in the polynomial system ha been utilized to simplify and speed up state of the art polynomial solver based on gro bner basis method in this paper we further explore partial symmetry i e where the symmetry lie in a subset of the variable in the polynomial system we develop novel numerical scheme to utilize such partial symmetry we then demonstrate the advantage of our scheme in several computer vision problem in both synthetic and real experiment we show that utilizing partial symmetry allow u to obtain faster and more accurate polynomial solver than the general solver 
the basic idea of shape from shading is to infer the shape of a surface from it shading information in a single image since this problem is ill posed a number of simplifying assumption have been often used however they rarely hold in practice this paper present a simple shading correction algorithm that transforms the image to a new image that better satisfies the assumption typically needed by existing algorithm thus improving the accuracy of shape recovery the algorithm take advantage of some local shading measure that have been driven under these assumption the method is successfully evaluated on real data of human teeth with ground truth d shape 
this paper present a novel method to generate a hypothesis set of class independent object region it ha been shown that such object region can be used to focus computer vision technique on the part of an image that matter most leading to significant improvement in both object localisation and semantic segmentation in recent year of course the higher quality of class independent object region the better subsequent computer vision algorithm can perform in this paper we focus on generating higher quality object hypothesis we start from an oversegmentation for which we propose to extract a wide variety of region feature we group region together in a hierarchical fashion for which we train a random forest which predicts at each stage of the hierarchy the best possible merge hence unlike other approach we use relatively powerful feature and classifier at an early stage of the generation of likely object region finally we identify and combine stable region in order to capture object which consist of dissimilar part we show on the pascal and datasets that our method yield higher quality region than competing approach while it is at the same time more computationally efficient 
we consider discrete pairwise energy minimization problem weighted constraint satisfaction max sum labeling and method that identify a globally optimal partial assignment of variable when finding a complete optimal assignment is intractable determining optimal value for a part of variable is an interesting possibility existing method are based on different sufficient condition we propose a new sufficient condition for partial optimality which is verifiable in polynomial time invariant to reparametrization of the problem and permutation of label and includes many existing sufficient condition a special case it is derived by using a relaxation technique coherent with the relaxation for energy minimization we pose the problem of finding the maximum optimal partial assignment identifiable by the new sufficient condition a polynomial method is proposed which is guaranteed to assign same or larger part of variable find the same or larger part of optimal assignment than several existing approach the core of the method is a specially constructed linear program that identifies persistent assignment in an arbitrary multi label setting 
we present a distance metric based upon the notion of minimum cost injective mapping between set our function satisfies metric property a long a the cost of the minimum mapping is derived from a semimetric for which the triangle inequality is not necessarily satisfied we show that the jaccard distance alternatively biotope tanimoto or marczewski steinhaus distance may be considered the special case for finite set where cost are derived from the discrete metric extension that allow premetrics not necessarily symmetric multisets generalized to include probability distribution and asymmetric mapping are given that expand the versatility of the metric without sacrificing metric property the function ha potential application in pattern recognition machine learning and information retrieval 
dense d reconstruction of real world object containing textureless reflective and specular part is a challenging task using general smoothness prior such a surface area regularization can lead to defect in the form of disconnected part or unwanted indentation we argue that this problem can be solved by exploiting the object class specific local surface orientation e g a car is always close to horizontal in the roof area therefore we formulate an object class specific shape prior in the form of spatially varying anisotropic smoothness term the parameter of the shape prior are extracted from training data we detail how our shape prior formulation directly fit into recently proposed volumetric multi label reconstruction approach this allows a segmentation between the object and it supporting ground in our experimental evaluation we show reconstruction using our trained shape prior on several challenging datasets 
preprocessing a d image often produce a noisy cloud of interest point we study the problem of counting hole in noisy cloud in the plane the hole in a given cloud are quantified by the topological persistence of their boundary contour when the cloud is analyzed at all possible scale we design the algorithm to count hole that are most persistent in the filtration of offset neighborhood around given point the input is a cloud of n point in the plane without any user defined parameter the algorithm ha a near linear time and a linear space o n the output is the array number of hole relative persistence in the filtration we prove theoretical guarantee when the algorithm find the correct number of hole component in the complement of an unknown shape approximated by a cloud 
we present a new globally optimal algorithm for self calibrating a moving camera with constant parameter our method aim at estimating the dual absolute quadric daq under the rank and optionally camera center chirality constraint we employ the branch and prune paradigm and explore the space of only parameter pruning in our method relies on solving linear matrix inequality lmi feasibility and generalized eigenvalue gev problem that solely depend upon the entry of the daq these lmi and gev problem are used to rule out branch in the search tree in which a quadric not satisfying the rank and chirality condition on camera center is guaranteed not to exist the chirality lmi condition are obtained by relying on the mild assumption that the camera undergoes a rotation of no more than between consecutive view furthermore our method doe not rely on calculating bound on any particular cost function and hence can virtually optimize any objective while achieving global optimality in a very competitive running time 
the functional difference between a diffuse wall and a mirror is well understood one scatter back into all direction and the other one preserve the directionality of reflected light the temporal structure of the light however is left intact by both assuming simple surface reflection photon that arrive first are reflected first in this paper we exploit this insight to recover object outside the line of sight from second order diffuse reflection effectively turning wall into mirror we formulate the reconstruction task a a linear inverse problem on the transient response of a scene which we acquire using an affordable setup consisting of a modulated light source and a time of flight image sensor by exploiting sparsity in the reconstruction domain we achieve resolution in the order of a few centimeter for object shape depth and laterally and albedo our method is robust to ambient light and work for large room sized scene it is drastically faster and le expensive than previous approach using femtosecond laser and streak camera and doe not require any moving part 
we propose ordered subspace clustering osc to segment data drawn from a sequentially ordered union of subspace current subspace clustering technique learn the relationship within a set of data and then use a separate clustering algorithm such a ncut for final segmentation in contrast our technique under certain condition is capable of segmenting cluster intrinsically without providing the number of cluster a a parameter similar to sparse subspace clustering ssc we formulate the problem a one of finding a sparse representation but include a new penalty term to take care of sequential data we test our method on data drawn from infrared hyper spectral data video sequence and face image our experiment show that our method osc outperforms the state of the art method spatial subspace clustering spatsc low rank representation lrr and ssc 
this paper present a method for acquiring dense nonrigid shape and deformation from a single monocular depth sensor we focus on modeling the human hand and assume that a single rough template model is available we combine and extend existing work on model based tracking subdivision surface fitting and mesh deformation to acquire detailed hand model from a few a frame of depth data we propose an objective that measure the error of fit between each sampled data point and a continuous model surface defined by a rigged control mesh and us a rigid a possible arap regularizers to cleanly separate the model and template geometry a key contribution is our use of a smooth model based on subdivision surface that allows simultaneous optimization over both correspondence and model parameter this avoids the use of iterated closest point icp algorithm which often lead to slow convergence automatic initialization is obtained using a regression forest trained to infer approximate correspondence experiment show that the resulting mesh model the user s hand shape more accurately than just adapting the shape parameter of the skeleton and that the retargeted skeleton accurately model the user s articulation we investigate the effect of various modeling choice and show the benefit of using subdivision surface and arap regularization 
learning a low dimensional representation of image is useful for various application in graphic and computer vision existing solution either require manually specified landmark for corresponding point in the image or are restricted to specific object or shape deformation this paper alleviates these limitation by imposing a specific model for generating image the nested composition of color shape and appearance we show that each component can be approximated by a low dimensional subspace when the others are factored out our formulation allows for efficient learning and experiment show encouraging result 
a main theme in object detection are currently discriminative part based model the powerful model that combine all part is then typically only feasible for few constituent which are in turn iteratively trained to make them a strong a possible we follow the opposite strategy by randomly sampling a large number of instance specific part classifier due to their number we cannot directly train a powerful classifier to combine all part therefore we randomly group them into fewer overlapping composition that are trained using a maximum margin approach in contrast to the common rationale of compositional approach we do not aim for semantically meaningful ensemble rather we seek randomized composition that are discriminative and generalize over all instance of a category our approach not only localizes object in cluttered scene but also explains them by parsing with composition and their constituent part we conducted experiment on pascal voc on the voc evaluation server and on the mitindoor scene dataset to the best of our knowledge our randomized max margin composition rm c are the currently best performing single class object detector using only hog feature moreover the individual contribution of composition and their part are evaluated in separate experiment that demonstrate their potential 
in modern face recognition the conventional pipeline consists of four stage detect align represent classify we revisit both the alignment step and the representation step by employing explicit d face modeling in order to apply a piecewise affine transformation and derive a face representation from a nine layer deep neural network this deep network involves more than million parameter using several locally connected layer without weight sharing rather than the standard convolutional layer thus we trained it on the largest facial dataset to date an identity labeled dataset of four million facial image belonging to more than identity the learned representation coupling the accurate model based alignment with the large facial database generalize remarkably well to face in unconstrained environment even with a simple classifier our method reach an accuracy of on the labeled face in the wild lfw dataset reducing the error of the current state of the art by more than closely approaching human level performance 
graph cut method such a alpha expansion and fusion move have been successful at solving many optimization problem in computer vision higher order markov random field mrf s which are important for numerous application have proven to be very difficult especially for multilabel mrf s i e more than label in this paper we propose a new primal dual energy minimization method for arbitrary higher order multilabel mrf s primal dual method provide guaranteed approximation bound and can exploit information in the dual variable to improve their efficiency our algorithm generalizes the pd technique for first order mrfs and relies on a variant of max flow that can exactly optimize certain higher order binary mrf s we provide approximation bound similar to pd and the method is fast in practice it can optimize non submodular mrf s and additionally can incorporate problem specific knowledge in the form of fusion proposal we compare experimentally against the existing approach that can efficiently handle these difficult energy function for higher order denoising and stereo mrf s we produce lower energy while running significantly faster 
this paper address the problem of face alignment for a single image we show how an ensemble of regression tree can be used to estimate the face s landmark position directly from a sparse subset of pixel intensity achieving super realtime performance with high quality prediction we present a general framework based on gradient boosting for learning an ensemble of regression tree that optimizes the sum of square error loss and naturally handle missing or partially labelled data we show how using appropriate prior exploiting the structure of image data help with efficient feature selection different regularization strategy and it importance to combat overfitting are also investigated in addition we analyse the effect of the quantity of training data on the accuracy of the prediction and explore the effect of data augmentation using synthesized data 
the problem of how to arrive at an appropriate d segmentation of a scene remains difficult while current state of the art method continue to gradually improve in benchmark performance they also grow more and more complex for example by incorporating chain of classifier which require training on large manually annotated data set a an alternative to this we present a new efficient learningand model free approach for the segmentation of d point cloud into object part the algorithm begin by decomposing the scene into an adjacency graph of surface patch based on a voxel grid edge in the graph are then classified a either convex or concave using a novel combination of simple criterion which operate on the local geometry of these patch this way the graph is divided into locally convex connected subgraphs which with high accuracy represent object part additionally we propose a novel depth dependent voxel grid to deal with the decreasing point density at far distance in the point cloud this improves segmentation allowing the use of fixed parameter for vastly different scene the algorithm is straightforward to implement and requires no training data while nevertheless producing result that are comparable to state of the art method which incorporate high level concept involving classification learning and model fitting 
we introduce an approach to computing and comparing covariance descriptor covds in infinite dimensional space covds have become increasingly popular to address classification problem in computer vision while covds offer some robustness to measurement variation they also throw away part of the information contained in the original data by only retaining the second order statistic over the measurement here we propose to overcome this limitation by first mapping the original data to a high dimensional hilbert space and only then compute the covds we show that several bregman divergence can be computed between the resulting covds in hilbert space via the use of kernel we then exploit these divergence for classification purpose our experiment demonstrate the benefit of our approach on several task such a material and texture recognition person re identification and action recognition from motion capture data 
while machine learning ha been instrumental to the ongoing progress in most area of computer vision it ha not been applied to the problem of stereo matching with similar frequency or success we present a supervised learning approach for predicting the correctness of stereo match based on a random forest and a set of feature that capture various form of information about each pixel we show highly competitive result in predicting the correctness of match and in confidence estimation which allows u to rank pixel according to the reliability of their assigned disparity moreover we show how these confidence value can be used to improve the accuracy of disparity map by integrating them with an mrf based stereo algorithm this is an important distinction from current literature that ha mainly focused on sparsification by removing potentially erroneous disparity to generate quasi dense disparity map 
we present a new method for tracking the d position global orientation and full articulation of human hand following recent advance in model based hypothesize and test method the high dimensional parameter space of hand configuration is explored with a novel evolutionary optimization technique specifically tailored to the problem the proposed method capitalizes on the fact that sample from quasi random sequence such a the sobol have low discrepancy and exhibit a more uniform coverage of the sampled space compared to random sample obtained from the uniform distribution the method ha been tested for the problem of tracking the articulation of a single hand d parameter space and two hand d space extensive experiment have been carried out with synthetic and real data in comparison with state of the art method the quantitative evaluation show that for case of limited computational resource the new approach achieves a speed up of four single hand tracking and eight two hand tracking without compromising tracking accuracy interestingly the proposed method is preferable compared to the state of the art either in the case of limited computational resource or in the case of more complex i e higher dimensional problem thus improving the applicability of the method in a number of application domain 
we study the theory of projective reconstruction for multiple projection from an arbitrary dimensional projective space into lower dimensional space this problem is important due to it application in the analysis of dynamical scene the current theory due to hartley and schaffalitzky is based on the grassmann tensor generalizing the idea of fundamental matrix trifocal tensor and quadrifocal tensor used in the well studied case of d to d projection we present a theory whose point of departure is the projective equation rather than the grassmann tensor this is a better fit for the analysis of approach such a bundle adjustment and projective factorization which seek to directly solve the projective equation in a first step we prove that there is a unique grassmann tensor corresponding to each set of image point a question that remained open in the work of hartley and schaffalitzky then we prove that projective equivalence follows from the set of projective equation given certain condition on the estimated camera point setup or the estimated projective depth finally we demonstrate how wrong solution to the projective factorization problem can happen and classify such degenerate solution based on the zero pattern in the estimated depth matrix 
we present an image set classification algorithm based on unsupervised clustering of labeled training and unlabeled test data where label are only used in the stopping criterion the probability distribution of each class over the set of cluster is used to define a true set based similarity measure to this end we propose an iterative sparse spectral clustering algorithm in each iteration a proximity matrix is efficiently recomputed to better represent the local subspace structure initial cluster capture the global data structure and finer cluster at the later stage capture the subtle class difference not visible at the global scale image set are compactly represented with multiple grassmannian manifold which are subsequently embedded in euclidean space with the proposed spectral clustering algorithm we also propose an efficient eigenvector solver which not only reduces the computational cost of spectral clustering by many fold but also improves the clustering quality and final classification result experiment on five standard datasets and comparison with seven existing technique show the efficacy of our algorithm 
camera image saved in raw format are being adopted in computer vision task since raw value represent minimally processed sensor response camera manufacturer however have yet to adopt a standard for raw image and current raw rgb value are device specific due to different sensor spectral sensitivity this result in significantly different raw image for the same scene captured with different camera this paper focus on estimating a mapping that can convert a raw image of an arbitrary scene and illumination from one camera s raw space to another to this end we examine various mapping strategy including linear and non linear transformation applied both in a global and illumination specific manner we show that illumination specific mapping give the best result however at the expense of requiring a large number of transformation to address this issue we introduce an illumination independent mapping approach that us white balancing to assist in reducing the number of required transformation we show that this approach achieves state of the art result on a range of consumer camera and image of arbitrary scene and illumination 
this paper present a photometric stereo method that is purely pixelwise and handle general isotropic surface in a stable manner following the recently proposed sum of lobe representation of the isotropic reflectance function we constructed a constrained bivariate regression problem where the regression function is approximated by smooth bivariate bernstein polynomial the unknown normal vector wa separated from the unknown reflectance function by considering the inverse representation of the image formation process and then we could accurately compute the unknown surface normal by solving a simple and efficient quadratic programming problem extensive evaluation that showed the state of the art performance using both synthetic and real world image were performed 
we present a simple vector quantizer that combine low distortion with fast search and apply it to approximate nearest neighbor ann search in high dimensional space leveraging the very same data structure that is used to provide non exhaustive search i e inverted list or a multi index the idea is to locally optimize an individual product quantizer pq per cell and use it to encode residual local optimization is over rotation and space decomposition interestingly we apply a parametric solution that assumes a normal distribution and is extremely fast to train with a reasonable space and time overhead that is constant in the data size we set a new state of the art on several public datasets including a billion scale one 
in this paper we tackle the problem of estimating the depth of a scene from a single image this is a challenging task since a single image on it own doe not provide any depth cue to address this we exploit the availability of a pool of image for which the depth is known more specifically we formulate monocular depth estimation a a discrete continuous optimization problem where the continuous variable encode the depth of the superpixels in the input image and the discrete one represent relationship between neighboring superpixels the solution to this discrete continuous optimization problem is then obtained by performing inference in a graphical model using particle belief propagation the unary potential in this graphical model are computed by making use of the image with known depth we demonstrate the effectiveness of our model in both the indoor and outdoor scenario our experimental evaluation show that our depth estimate are more accurate than existing method on standard datasets 
convolutional neural network cnns have been established a a powerful class of model for image recognition problem encouraged by these result we provide an extensive empirical evaluation of cnns on large scale video classification using a new dataset of million youtube video belonging to class we study multiple approach for extending the connectivity of a cnn in time domain to take advantage of local spatio temporal information and suggest a multiresolution foveated architecture a a promising way of speeding up the training our best spatio temporal network display significant performance improvement compared to strong feature based baseline to but only a surprisingly modest improvement compared to single frame model to we further study the generalization performance of our best model by retraining the top layer on the ucf action recognition dataset and observe significant performance improvement compared to the ucf baseline model up from 
we consider the problem of tracking multiple interacting object in d using rgbd input and by considering a hypothesize and test approach due to their interaction object to be tracked are expected to occlude each other in the field of view of the camera observing them a naive approach would be to employ a set of independent tracker sit and to assign one tracker to each object this approach scale well with the number of object but fails a occlusion become stronger due to their disjoint consideration the solution representing the current state of the art employ a single joint tracker jt that account for all object simultaneously this directly resolve ambiguity due to occlusion but ha a computational complexity that grows geometrically with the number of tracked object we propose a middle ground namely an ensemble of collaborative tracker ect that combine best trait from both world to deliver a practical and accurate solution to the multi object d tracking problem we present quantitative and qualitative experiment with several synthetic and real world sequence of diverse complexity experiment demonstrate that ect manages to track far more complex scene than jt at a computational time that is only slightly larger than that of sit 
given a static scene a human can trivially enumerate the myriad of thing that can happen next and characterize the relative likelihood of each in the process we make use of enormous amount of commonsense knowledge about how the world work in this paper we investigate learning this commonsense knowledge from data to overcome a lack of densely annotated spatiotemporal data we learn from sequence of abstract image gathered using crowd sourcing the abstract scene provide both object location and attribute information we demonstrate qualitatively and quantitatively that our model produce plausible scene prediction on both the abstract image a well a natural image taken from the internet 
local video feature provide state of the art performance for action recognition while the accuracy of action recognition ha been continuously improved over the recent year the low speed of feature extraction and subsequent recognition prevents current method from scaling up to real size problem we address this issue and first develop highly efficient video feature using motion information in video compression we next explore feature encoding by fisher vector and demonstrate accurate action recognition using fast linear classifier our method improves the speed of video feature extraction feature encoding and action classification by two order of magnitude at the cost of minor reduction in recognition accuracy we validate our approach and compare it to the state of the art on four recent action recognition datasets 
most of the state of the art approach to human activity recognition in video need an intensive training stage and assume that all of the training example are labeled and available beforehand but these assumption are unrealistic for many application where we have to deal with streaming video in these video a new activity are seen they can be leveraged upon to improve the current activity recognition model in this work we develop an incremental activity learning framework that is able to continuously update the activity model and learn new one a more video are seen our proposed approach leverage upon state of the art machine learning tool most notably active learning system it doe not require tedious manual labeling of every incoming example of each activity class we perform rigorous experiment on challenging human activity datasets which demonstrate that the incremental activity modeling framework can achieve performance very close to the case when all example are available a priori 
real world video of human activity exhibit temporal structure at various scale long video are typically composed out of multiple action instance where each instance is itself composed of sub action with variable duration and ordering temporal grammar can presumably model such hierarchical structure but are computationally difficult to apply for long video stream we describe simple grammar that capture hierarchical temporal structure while admitting inference with a finite state machine this make parsing linear time constant storage and naturally online we train grammar parameter using a latent structural svm where latent subactions are learned automatically we illustrate the effectiveness of our approach over common baseline on a new half million frame dataset of continuous youtube video 
popular figure ground segmentation algorithm generate a pool of boundary aligned segment proposal that can be used in subsequent object recognition engine these algorithm can recover most image object with high accuracy but are usually computationally intensive since many graph cut are computed with different enumeration of segment seed in this paper we propose an algorithm rigor for efficiently generating a pool of overlapping segment proposal in image by precomputing a graph which can be used for parametric min cut over different seed we speed up the generation of the segment pool in addition we have made design choice that avoid extensive computation without losing performance in particular we demonstrate that the segmentation performance of our algorithm is slightly better than the state of the art on the pascal voc dataset while being an order of magnitude faster 
recently unsupervised image segmentation ha become increasingly popular starting from a superpixel segmentation an edge weighted region adjacency graph is constructed amongst all segmentation of the graph the one which best conforms to the given image evidence a measured by the sum of cut edge weight is chosen since this problem is np hard we propose a new approximate solver based on the move making paradigm first the graph is recursively partitioned into small region cut phase then for any two adjacent region we consider alternative cut of these two region defining possible move glue cut phase for planar problem the optimal move can be found whereas for non planar problem efficient approximation exist we evaluate our algorithm on published and new benchmark datasets which we make available here the proposed algorithm find segmentation that a measured by a loss function are a close to the ground truth a the global optimum found by exact solver it doe so significantly faster then existing approximate method which is important for large scale problem 
a probabilistic model allows u to reason about the world and make statistically optimal decision using bayesian decision theory however in practice the intractability of the decision problem force u to adopt simplistic loss function such a the loss or hamming loss and a result we make poor decision through map estimate or through low order marginal statistic in this work we investigate optimal decision making for more realistic loss function specifically we consider the popular intersection over union iou score used in image segmentation benchmark and show that it result in a hard combinatorial decision problem to make this problem tractable we propose a statistical approximation to the objective function a well a an approximate algorithm based on parametric linear programming we apply the algorithm on three benchmark datasets and obtain improved intersection over union score compared to maximum posterior marginal decision our work point out the difficulty of using realistic loss function with probabilistic computer vision model 
we describe an information driven active selection approach to determine which detector to deploy at which location in which frame of a video to minimize semantic class label uncertainty at every pixel with the smallest computational cost that ensures a given uncertainty bound we show minimal performance reduction compared to a paragon algorithm running all detector at all location in all frame at a small fraction of the computational cost our method can handle uncertainty in the labeling mechanism so it can handle both oracle manual annotation or noisy detector automated annotation 
we propose a probabilistic method for parsing a temporal sequence such a a complex activity defined a composition of sub activity action the temporal structure of the high level activity is represented by a string length limited stochastic context free grammar given the grammar a bayes network which we term sequential interval network sin is generated where the variable node correspond to the start and end time of component action the network integrates information about the duration of each primitive action visual detection result for each primitive action and the activity s temporal structure at any moment in time during the activity message passing is used to perform exact inference yielding the posterior probability of the start and end time for each different activity action we provide demonstration of this framework being applied to vision task such a action prediction classification of the high level activity or temporal segmentation of a test sequence the method is also applicable in human robot interaction domain where continual prediction of human action is needed 
the notion of creativity a opposed to related concept such a beauty or interestingness ha not been studied from the perspective of automatic analysis of multimedia content meanwhile short online video shared on social medium platform or micro video have arisen a a new medium for creative expression in this paper we study creative micro video in an effort to understand the feature that make a video creative and to address the problem of automatic detection of creative content defining creative video a those that are novel and have aesthetic value we conduct a crowdsourcing experiment to create a dataset of over micro video labelled a creative and non creative we propose a set of computational feature that we map to the component of our definition of creativity and conduct an analysis to determine which of these feature correlate most with creative video finally we evaluate a supervised approach to automatically detect creative video with promising result showing that it is necessary to model both aesthetic value and novelty to achieve optimal classification accuracy 
we propose a real time robust to outlier and accurate solution to the perspective n point pnp problem the main advantage of our solution are twofold first it integrates the outlier rejection within the pose estimation pipeline with a negligible computational overhead and second it scalability to arbitrarily large number of correspondence given a set of d to d match we formulate pose estimation problem a a low rank homogeneous system where the solution lie on it d null space outlier correspondence are those row of the linear system which perturb the null space and are progressively detected by projecting them on an iteratively estimated solution of the null space since our outlier removal process is based on an algebraic criterion which doe not require computing the full pose and reprojecting back all d point on the image plane at each step we achieve speed gain of more than time compared to ransac strategy an extensive experimental evaluation will show that our solution yield accurate result in situation with up to of outlier and can process more than correspondence in le than m 
a common thread that tie together many prior work in scene understanding is their focus on the aspect directly present in a scene such a it categorical classification or the set of object in this work we propose to look beyond the visible element of a scene we demonstrate that a scene is not just a collection of object and their configuration or the label assigned to it pixel it is so much more from a simple observation of a scene we can tell a lot about the environment surrounding the scene such a the potential establishment near it the potential crime rate in the area or even the economic climate here we explore several of these aspect from both the human perception and computer vision perspective specifically we show that it is possible to predict the distance of surrounding establishment such a mcdonald s or hospital even by using scene located far from them we go a step further to show that both human and computer perform well at navigating the environment based only on visual cue from scene lastly we show that it is possible to predict the crime rate in an area simply by looking at a scene without any real time criminal activity simply put here we illustrate that it is possible to look beyond the visible scene 
we examine the problem of retrieving high resolution texture of object observed in multiple video under small object deformation in the monocular case the data redundancy necessary to reconstruct a high resolution image stem from temporal accumulation this ha been vastly explored and is known a image super resolution on the other hand a handful of method have considered the texture of a static d object observed from several camera where the data redundancy is obtained through the different viewpoint we introduce a unified framework to leverage both possibility for the estimation of an object s high resolution texture this framework uniformly deal with any related geometric variability introduced by the acquisition chain or by the evolution over time to this goal we use d warp for all viewpoint and all temporal frame and a linear image formation model from texture to image space despite it simplicity the method is able to successfully handle different view over space and time a shown experimentally it demonstrates the interest of temporal information to improve the texture quality additionally we also show that our method outperforms state of the art multi view super resolution method existing for the static case 
when do the visual ray associated with triplet of point correspondence converge that is intersect in a common point classical model of trinocular geometry based on the fundamental matrix and trifocal tensor associated with the corresponding camera only provide partial answer to this fundamental question in large part because of underlying but seldom explicit general configuration assumption this paper us elementary tool from projective line geometry to provide necessary and sufficient geometric and analytical condition for convergence in term of transversals to triplet of visual ray without any such assumption in turn this yield a novel and simple minimal parameterization of trinocular geometry for camera with non collinear or collinear pinhole 
a key problem often encountered by many learning algorithm in computer vision dealing with high dimensional data is the so called curse of dimensionality which arises when the available training sample are le than the input feature space dimensionality to remedy this problem we propose a joint dimensionality reduction and classification framework by formulating an optimization problem within the maximum margin class separation task the proposed optimization problem is solved using alternative optimization where we jointly compute the low dimensional maximum margin projection and the separating hyperplanes in the projection subspace moreover in order to reduce the computational cost of the developed optimization algorithm we incorporate orthogonality constraint on the derived projection base and show that the resulting combined model is an alternation between identifying the optimal separating hyperplanes and performing a linear discriminant analysis on the support vector experiment on face facial expression and object recognition validate the effectiveness of the proposed method against state of the art dimensionality reduction algorithm 
we describe a new approach to transfer knowledge across view for action recognition by using example from a large collection of unlabelled mocap data we achieve this by directly matching purely motion based feature from video to mocap our approach recovers d pose sequence without performing any body part tracking we use these match to generate multiple motion projection and thus add view invariance to our action recognition model we also introduce a closed form solution for approximate non linear circulant temporal encoding ncte which allows u to efficiently perform the match in the frequency domain we test our approach on the challenging unsupervised modality of the ixmas dataset and use publicly available motion capture data for matching without any additional annotation effort we are able to significantly outperform the current state of the art 
recognition is graduating from lab to real world application while it is encouraging to see it potential being tapped it brings forth a fundamental challenge to the vision researcher scalability how can we learn a model for any concept that exhaustively cover all it appearance variation while requiring minimal or no human supervision for compiling the vocabulary of visual variance gathering the training image and annotation and learning the model in this paper we introduce a fully automated approach for learning extensive model for a wide range of variation e g action interaction attribute and beyond within any concept our approach leverage vast resource of online book to discover the vocabulary of variance and intertwines the data collection and modeling step to alleviate the need for explicit human supervision in training the model our approach organizes the visual knowledge about a concept in a convenient and useful way enabling a variety of application across vision and nlp our online system ha been queried by user to learn model for several interesting concept including breakfast gandhi beautiful etc to date our system ha model available for over variation within concept and ha annotated more than million image with bounding box 
we propose a technique to use the structural information extracted from a set of d model of an object class to improve novel view synthesis for image showing unknown instance of this class these novel view can be used to amplify training image collection that typically contain only a low number of view or lack certain class of view entirely e g top view we extract the correlation of position normal reflectance and appearance from computer generated image of a few exemplar and use this information to infer new appearance for new instance we show that our approach can improve performance of state of the art detector using real world training data additional application include guided version of inpainting d to d conversion superresolution and non local smoothing 
this paper introduces a regularization method to explicitly control the rank of a learned symmetric positive semidefinite distance matrix in distance metric learning to this end we propose to incorporate in the objective function a linear regularization term that minimizes the k smallest eigenvalue of the distance matrix it is equivalent to minimizing the trace of the product of the distance matrix with a matrix in the convex hull of rank k projection matrix called a fantope based on this new regularization method we derive an optimization scheme to efficiently learn the distance matrix we demonstrate the effectiveness of the method on synthetic and challenging real datasets of face verification and image classification with relative attribute on which our method outperforms state of the art metric learning algorithm 
deep convolutional neural network have recently achieved state of the art performance on a number of image recognition benchmark including the imagenet large scale visual recognition challenge ilsvrc the winning model on the localization sub task wa a network that predicts a single bounding box and a confidence score for each object category in the image such a model capture the whole image context around the object but cannot handle multiple instance of the same object in the image without naively replicating the number of output for each instance in this work we propose a saliency inspired neural network model for detection which predicts a set of class agnostic bounding box along with a single score for each box corresponding to it likelihood of containing any object of interest the model naturally handle a variable number of instance for each class and allows for cross class generalization at the highest level of the network we are able to obtain competitive recognition performance on voc and ilsvrc while using only the top few predicted location in each image and a small number of neural network evaluation 
scan line optimization via cost accumulation ha become very popular for stereo estimation in computer vision application and is often combined with a semi global cost integration strategy known a sgm this paper introduces this combination a a general and effective optimization technique it is the first time that this concept is applied to d medical image registration the presented algorithm sgm d employ a coarse to fine strategy and reduces the search space dimension for consecutive pyramid level by a fixed linear rate this allows it to handle large displacement to an extent that is required for clinical application in high dimensional data sgm d is evaluated in context of pulmonary motion analysis on the recently extended dir lab benchmark that provides ten d computed tomography ct image data set a well a ten challenging d ct scan pair from the copdgene study archive result show that both registration error a well a run time performance are very competitive with current state of the art method 
the goal of this paper is to question the necessity of feature like sift in categorical visual recognition task a an alternative we develop a generative model for the raw intensity of image patch and show that it can support image classification performance on par with optimized sift based technique in a bag of visual word setting key ingredient of the proposed model is a compact dictionary of mini epitome learned in an unsupervised fashion on a large collection of image the use of epitome allows u to explicitly account for photometric and position variability in image appearance we show that this flexibility considerably increase the capacity of the dictionary to accurately approximate the appearance of image patch and support recognition task for image classification we develop histogram based image encoding method tailored to the epitomic representation a well a an epitomic footprint encoding which is easy to visualize and highlight the generative nature of our model we discus in detail computational aspect and develop efficient algorithm to make the model scalable to large task the proposed technique are evaluated with experiment on the challenging pascal voc image classification benchmark 
in this work we reconsider labeling problem with virtually continuous state space which are of relevance in low level computer vision in order to cope with such huge state space multi scale method have been proposed to approximately solve such labeling task although performing well in many case these method do usually not come with any guarantee on the returned solution a general and principled approach to solve labeling problem is based on the well known linear programming relaxation which appears to be prohibitive for large state space at the first glance we demonstrate that a coarse to fine exploration strategy in the label space is able to optimize the lp relaxation for non trivial problem instance with reasonable run time and moderate memory requirement 
we propose an algorithm called multi label generic cut mlgc for computing optimal solution to mrf map problem with submodular multi label multi clique potential a transformation is introduced to convert a m label k clique problem to an equivalent label mk clique problem we show that if the original multi label problem is submodular then the transformed label multi clique problem is also submodular we exploit sparseness in the feasible configuration of the transformed label problem to suggest an improvement to generic cut to solve the label problem efficiently the algorithm run in time o mk n in the worst case n is the number of pixel generalizing o k n running time of generic cut we show experimentally that mlgc is an order of magnitude faster than the current state of the art while the result of mlgc is optimal for submodular clique potential it is significantly better than the compared method even for problem with non submodular clique potential 
persistent surveillance of large geographic area from unmanned aerial vehicle allows u to learn much about the daily activity in the region of interest nearly all of the approach addressing tracking in this imagery are detection based and rely on background subtraction or frame differencing to provide detection this however make it difficult to track target once they slow down or stop which is not acceptable for persistent tracking our goal we present a multiple target tracking approach that doe not exclusively rely on background subtraction and is better able to track target through stop it accomplishes this by effectively running two tracker in parallel one based on detection from background subtraction providing target initialization and reacquisition and one based on a target state regressor providing frame to frame tracking we evaluated the proposed approach on a long sequence from a wide area aerial imagery dataset and the result show improved object detection rate and id switch rate with limited increase in false alarm compared to the competition 
in this paper we study optimization method for minimizing large scale pseudoconvex l problem in multiview geometry we present a novel algorithm for solving this class of problem based on proximal splitting method we provide a brief derivation of the proposed method along with a general convergence analysis the resulting meta algorithm requires very little effort in term of implementation and instead make use of existing advanced solver for non linear optimization preliminary experiment on a number of real image datasets indicate that the proposed method experimentally match or outperforms current state of the art solver for this class of problem 
the world is full of object with complex reflectance situated in complex illumination environment past work on full d geometry recovery however ha tried to handle this complexity by framing it into simplistic model of reflectance lambetian mirrored or diffuse plus specular or illumination one or more point light source though there ha been some recent progress in directly utilizing such complexity for recovering a single view geometry it is not clear how such single view method can be extended to reconstruct the full geometry to this end we derive a probabilistic geometry estimation method that fully exploit the rich signal embedded in complex appearance though each observation provides partial and unreliable information we show how to estimate the reflectance responsible for the diverse appearance and unite the orientation cue embedded in each observation to reconstruct the underlying geometry we demonstrate the effectiveness of our method on synthetic and real world object the result show that our method performs accurately across a wide range of real world environment and reflectance that lie between the extreme that have been the focus of past work 
we present the discriminative fern ensemble dfe classifier for efficient visual object recognition the classifier architecture is designed to optimize both classification speed and accuracy when a large training set is available speed is obtained using simple binary feature and direct indexing into a set of table and accuracy by using a large capacity model and careful discriminative optimization the proposed framework is applied to the problem of hand pose recognition in depth and infra red image using a very large training set both the accuracy and the classification time obtained are considerably superior to relevant competing method allowing one to reach accuracy target with run time order of magnitude faster than the competition we show empirically that using dfe we can significantly reduce classification time by increasing training sample size for a fixed target accuracy finally a dfe result is shown for the mnist dataset showing the method s merit extends beyond depth image 
we tackle the problem of optimizing over all possible positive definite radial kernel on riemannian manifold for classification kernel method on riemannian manifold have recently become increasingly popular in computer vision however the number of known positive definite kernel on manifold remain very limited furthermore most kernel typically depend on at least one parameter that need to be tuned for the problem at hand a poor choice of kernel or of parameter value may yield significant performance drop off here we show that positive definite radial kernel on the unit n sphere the grassmann manifold and kendall s shape manifold can be expressed in a simple form whose parameter can be automatically optimized within a support vector machine framework we demonstrate the benefit of our kernel learning algorithm on object face action and shape recognition 
this paper considers the problem of action localization where the objective is to determine when and where certain action appear we introduce a sampling strategy to produce d t sequence of bounding box called tubelets compared to state of the art alternative this drastically reduces the number of hypothesis that are likely to include the action of interest our method is inspired by a recent technique introduced in the context of image localization beyond considering this technique for the first time for video we revisit this strategy for d t sequence obtained from super voxels our sampling strategy advantageously exploit a criterion that reflects how action related motion deviate from background motion we demonstrate the interest of our approach by extensive experiment on two public datasets ucf sport and msr ii our approach significantly outperforms the state of the art on both datasets while restricting the search of action to a fraction of possible bounding box sequence 
photo sharing website have become very popular in the last few year leading to huge collection of online image in addition to image data these website collect a variety of multimodal metadata about photo including text tag caption gps coordinate camera metadata user profile etc however this metadata is not well constrained and is often noisy sparse or missing altogether in this paper we propose a framework to model these loosely organized multimodal datasets and show how to perform loosely supervised learning using a novel latent conditional random field framework we learn parameter of the lcrf automatically from a small set of validation data using information theoretic metric learning itml to learn distance function and a structural svm formulation to learn the potential function we apply our framework on four datasets of image from flickr evaluating both qualitatively and quantitatively against several baseline 
recently multi atlas segmentation ma ha achieved a great success in the medical imaging area the key assumption of ma is that multiple atlas encompass richer anatomical variability than a single atlas therefore we can label the target image more accurately by mapping the label information from the appropriate atlas image that have the most similar structure the problem of atlas selection however still remains unexplored current state of the art ma method rely on image similarity to select a set of atlas unfortunately this heuristic criterion is not necessarily related to segmentation performance and thus may undermine segmentation result to solve this simple but critical problem we propose a learning based atlas selection method to pick up the best atlas that would eventually lead to more accurate image segmentation our idea is to learn the relationship between the pairwise appearance of observed instance a pair of atlas and target image and their final labeling performance in term of dice ratio in this way we can select the best atlas according to their expected labeling accuracy it is worth noting that our atlas selection method is general enough to be integrated with existing ma method a is shown in the experiment we achieve significant improvement after we integrate our method with widely used ma method on adni and loni lpba datasets 
arguably deformable part model dpms are one of the most prominent approach for face alignment with impressive result being recently reported for both controlled lab and unconstrained setting fitting in most dpm method is typically formulated a a two step process during which discriminatively trained part template are first correlated with the image to yield a filter response for each landmark and then shape optimization is performed over these filter response this process although computationally efficient is based on fixed part template which are assumed to be independent and ha been shown to result in imperfect filter response and detection ambiguity to address this limitation in this paper we propose to jointly optimize a part based trained in the wild flexible appearance model along with a global shape model which result in a joint translational motion model for the model part via gauss newton gn optimization we show how significant computational reduction can be achieved by building a full model during training but then efficiently optimizing the proposed cost function on a sparse grid using weighted least square during fitting we coin the proposed formulation gauss newton deformable part model gn dpm finally we compare it performance against the state of the art and show that the proposed gn dpm outperforms it in some case by a large margin code for our method is available from http ibug doc ic ac uk resource 
we propose filter forest ff an efficient new discriminative approach for predicting continuous variable given a signal and it context ff can be used for general signal restoration task that can be tackled via convolutional filtering where it attempt to learn the optimal filtering kernel to be applied to each data point the model can learn both the size of the kernel and it value conditioned on the observation and it spatial or temporal context we show that ff compare favorably to both markov random field based and recently proposed regression forest based approach for labeling problem in term of efficiency and accuracy in particular we demonstrate how ff can be used to learn optimal denoising filter for natural image a well a for other task such a depth image refinement and d signal magnitude estimation numerous experiment and quantitative comparison show that ffs achieve accuracy at par or superior to recent state of the art technique while being several order of magnitude faster 
our goal is to obtain a noise free high resolution hr image from an observed noisy low resolution lr image the conventional approach of preprocessing the image with a denoising algorithm followed by applying a super resolution sr algorithm ha an important limitation along with noise some high frequency content of the image particularly textural detail is invariably lost during the denoising step this denoising loss restricts the performance of the subsequent sr step wherein the challenge is to synthesize such textural detail in this paper we show that high frequency content in the noisy image which is ordinarily removed by denoising algorithm can be effectively used to obtain the missing textural detail in the hr domain to do so we first obtain hr version of both the noisy and the denoised image using a patch similarity based sr algorithm we then show that by taking a convex combination of orientation and frequency selective band of the noisy and the denoised hr image we can obtain a desired hr image where i some of the textural signal lost in the denoising step is effectively recovered in the hr domain and ii additional texture can be easily synthesized by appropriately constraining the parameter of the convex combination we show that this part recovery and part synthesis of texture through our algorithm yield hr image that are visually more pleasing than those obtained using the conventional processing pipeline furthermore our result show a consistent improvement in numerical metric further corroborating the ability of our algorithm to recover lost signal 
our goal is to learn a compact discriminative vector representation of a face track suitable for the face recognition task of verification and classification to this end we propose a novel face track descriptor based on the fisher vector representation and demonstrate that it ha a number of favourable property first the descriptor is suitable for track of both frontal and profile face and is insensitive to their pose second the descriptor is compact due to discriminative dimensionality reduction and it can be further compressed using binarization third the descriptor can be computed quickly using hard quantization and it compact size and fast computation render it very suitable for large scale visual repository finally the descriptor demonstrates good generalization when trained on one dataset and tested on another reflecting it tolerance to the dataset bias in the experiment we show that the descriptor exceeds the state of the art on both face verification task youtube face without outside training data and inria buffy benchmark and face classification task using the oxford buffy dataset 
in this paper we introduce a new distance for robustly matching vector of d rotation a special representation of d rotation which we coin full angle quaternion faq allows u to express this distance a euclidean we apply the distance to the problem of d shape recognition from point cloud and d object tracking in color video for the former we introduce a hashing scheme for scale and translation which outperforms the previous state of the art approach on a public dataset for the latter we incorporate online subspace learning with the proposed faq representation to highlight the benefit of the new representation 
we present a method for generating object segmentation proposal from group of superpixels the goal is to propose accurate segmentation for all object of an image the proposed object hypothesis can be used a input to object detection system and thereby improve efficiency by replacing exhaustive search the segmentation are generated in a class independent manner and therefore the computational cost of the approach is independent of the number of object class our approach combine both global and local search in the space of set of superpixels the local search is implemented by greedily merging adjacent pair of superpixels to build a bottom up segmentation hierarchy the region from such a hierarchy directly provide a part of our region proposal the global search provides the other part by performing a set of graph cut segmentation on a superpixel graph obtained from an intermediate level of the hierarchy the parameter of the graph cut problem are learnt in such a manner that they provide complementary set of region experiment with pascal voc image show that we reach state of the art with greatly reduced computational cost 
we propose a novel solution to the generalized camera pose problem which includes the internal scale of the generalized camera a an unknown parameter this further generalization of the well known absolute camera pose problem ha application in multi frame loop closure while a well calibrated camera rig ha a fixed and known scale camera trajectory produced by monocular motion estimation necessarily lack a scale estimate thus when performing loop closure in monocular visual odometry or registering separate structure from motion reconstruction we must estimate a seven degree of freedom similarity transform from corresponding observation existing approach solve this problem in specialized configuration by aligning d triangulated point or individual camera pose estimate our approach handle general configuration of ray and point and directly estimate the full similarity transformation from the d d correspondence four correspondence are needed in the minimal case which ha eight possible solution the minimal solver can be used in a hypothesize and test architecture for robust transformation estimation our solver also produce a least square estimate in the overdetermined case the approach is evaluated experimentally on synthetic and real datasets and is shown to produce higher accuracy solution to multi frame loop closure than existing approach 
face detection and facial point localization are interconnected task recently it ha been shown that solving these two task jointly with a mixture of tree of part mtp lead to state of the art result however mtp a most other method for facial point localization proposed so far requires a complete annotation of the training data at facial point level this is used to predefine the structure of the tree and to place the part correctly in this work we extend the mixture from tree to more general loopy graph in this way we can learn in a weakly supervised manner using only the face location and orientation a powerful deformable detector that implicitly aligns it part to the detected face in the image by attaching some reference point to the correct part of our detector we can then localize the facial point in term of detection our method clearly outperforms the state of the art even if competing with method that use facial point annotation during training additionally without any facial point annotation at the level of individual training image our method can localize facial point with an accuracy similar to fully supervised approach 
in this paper we would like to evaluate online learning algorithm for large scale visual recognition using state of the art feature which are preselected and held fixed today combination of high dimensional feature and linear classifier are widely used for large scale visual recognition numerous so called mid level feature have been developed and mutually compared on an experimental basis although various learning method for linear classification have also been proposed in the machine learning and natural language processing literature they have rarely been evaluated for visual recognition therefore we give guideline via investigation of state of the art online learning method of linear classifier many method have been evaluated using toy data and natural language processing problem such a document classification consequently we gave those method a unified interpretation from the viewpoint of visual recognition result of controlled comparison indicate three guideline that might change the pipeline for visual recognition 
the quantification of similarity between image segmentation is a complex yet important task the ideal similarity measure should be unbiased to segmentation of different volume and complexity and be able to quantify and visualise segmentation bias similarity measure based on overlap e g dice score or surface distance e g hausdorff distance clearly do not satisfy all of these property to address this problem we introduce patch based evaluation of image segmentation pei a general method to ass segmentation quality our method is based on finding patch correspondence and the associated patch displacement which allow the estimation of segmentation bias we quantify both the agreement of the segmentation boundary and the conservation of the segmentation shape we further ass the segmentation complexity within patch to weight the contribution of local segmentation similarity to the global score we evaluate pei on both synthetic data and two medical imaging datasets on synthetic segmentation of different shape we provide evidence that pei in comparison to the dice score produce more comparable score ha increased sensitivity and estimate segmentation bias accurately on cardiac magnetic resonance mr image we demonstrate that pei can evaluate the performance of a segmentation method independent of the size or complexity of the segmentation under consideration on brain mr image we compare five different automatic hippocampus segmentation technique using pei finally we visualise the segmentation bias on a selection of the case 
we propose a method for knowledge transfer between semantically related class in imagenet by transferring knowledge from the image that have bounding box annotation to the others our method is capable of automatically populating imagenet with many more bounding box the underlying assumption that object from semantically related class look alike is formalized in our novel associative embedding ae representation ae recovers the latent low dimensional space of appearance variation among image window the dimension of ae space tend to correspond to aspect of window appearance e g side view close up background we model the overlap of a window with an object using gaussian process gp regression which spread annotation smoothly through ae space the probabilistic nature of gp allows our method to perform self assessment i e assigning a quality estimate to it own output it enables trading off the amount of returned annotation for their quality a large scale experiment on class and million image demonstrates that our method outperforms state of the art method and baseline for object localization using self assessment we can automatically return bounding box annotation for of all image with high localization accuracy i e average overlap with ground truth 
we propose a joint foreground background mixture model fbm that simultaneously performs background estimation and motion segmentation in complex dynamic scene our fbm consist of a set of location specific dynamic texture dt component for modeling local background motion and set of global dt component for modeling consistent foreground motion we derive an em algorithm for estimating the parameter of the fbm we also apply spatial constraint to the fbm using an markov random field grid and derive a corresponding variational approximation for inference unlike existing approach to background subtraction our fbm doe not require a manually selected threshold or a separate training video unlike existing motion segmentation technique our fbm can segment foreground motion over complex background with mixed motion and detect stopped object since most dynamic scene datasets only contain video with a single foreground object over a simple background we develop a new challenging dataset with multiple foreground object over complex dynamic background in experiment we show that jointly modeling the background and foreground segment with fbm yield significant improvement in accuracy on both background estimation and motion segmentation compared to state of the art method 
we propose a simple yet effective detector for pedestrian detection the basic idea is to incorporate common sense and everyday knowledge into the design of simple and computationally efficient feature a pedestrian usually appear up right in image or video data the problem of pedestrian detection is considerably simpler than general purpose people detection we therefore employ a statistical model of the up right human body where the head the upper body and the lower body are treated a three distinct component our main contribution is to systematically design a pool of rectangular template that are tailored to this shape model a we incorporate different kind of low level measurement the resulting multi modal multi channel haar like feature represent characteristic difference between part of the human body yet are robust against variation in clothing or environmental setting our approach avoids exhaustive search over all possible configuration of rectangle feature and neither relies on random sampling it thus mark a middle ground among recently published technique and yield efficient low dimensional yet highly discriminative feature experimental result on the inria and caltech pedestrian datasets show that our detector reach state of the art performance at low computational cost and that our feature are robust against occlusion 
existing method to learn visual attribute are prone to learning the wrong thing namely property that are correlated with the attribute of interest among training sample yet many proposed application of attribute rely on being able to learn the correct semantic concept corresponding to each attribute we propose to resolve such confusion by jointly learning decorrelated discriminative attribute model leveraging side information about semantic relatedness we develop a multi task learning approach that us structured sparsity to encourage feature competition among unrelated attribute and feature sharing among related attribute on three challenging datasets we show that accounting for structure in the visual attribute space is key to learning attribute model that preserve semantics yielding improved generalizability that help in the recognition and discovery of unseen object category 
we propose a kernel based framework for computing component from a set of surface normal this framework allows u to easily demonstrate that component analysis can be performed directly upon normal we link previously proposed mapping function the azimuthal equidistant projection aep and principal geodesic analysis pga to our kernel based framework we also propose a new mapping function based upon the cosine distance between normal we demonstrate the robustness of our proposed kernel when trained with noisy training set we also compare our kernel within an existing shape from shading sfs algorithm our spherical representation of normal when combined with the robust property of cosine kernel produce a very robust subspace analysis technique in particular our result within sfs show a substantial qualitative and quantitative improvement over existing technique 
we propose a data driven approach to facial landmark localization that model the correlation between each landmark and it surrounding appearance feature at runtime each feature cast a weighted vote to predict landmark location where the weight is precomputed to take into account the feature s discriminative power the feature voting based landmark detection is more robust than previous local appearance based detector we combine it with nonparametric shape regularization to build a novel facial landmark localization pipeline that is robust to scale in plane rotation occlusion expression and most importantly extreme head pose we achieve state of the art performance on two especially challenging in the wild datasets populated by face with extreme head pose and expression 
in this paper we propose a new methodology for segmenting non rigid visual object where the search procedure is onducted directly on a sparse low dimensional manifold guided by the classification result computed from a deep belief network our main contribution is the fact that we do not rely on the typical sub division of segmentation task into rigid detection and non rigid delineation instead the non rigid segmentation is performed directly where point in the sparse low dimensional can be mapped to an explicit contour representation in image space our proposal show significantly smaller search and training complexity given that the dimensionality of the manifold is much smaller than the dimensionality of the search space for rigid detection and non rigid delineation aforementioned and that we no longer require a two stage segmentation process we focus on the problem of left ventricle endocardial segmentation from ultrasound image and lip segmentation from frontal facial image using the extended cohn kanade ck database our experiment show that the use of sparse low dimensional manifold reduces the search and training complexity of current segmentation approach without a significant impact on the segmentation accuracy shown by state of the art approach 
we propose an approach for segmenting the individual building in typical skyline image our approach is based on a markov random field mrf formulation that exploit the fact that such image contain overlapping object of similar shape exhibiting a tiered structure our contribution are the following a dataset of high resolution skyline image from twelve different city with over individually labeled building that allows u to quantitatively evaluate the performance of various segmentation method an analysis of low level feature that are useful for segmentation of building and a shape constrained mrf formulation that enforces shape prior over the region for simple shape such a rectangle our formulation is significantly faster to optimize than a standard mrf approach while also being more accurate we experimentally evaluate various mrf formulation and demonstrate the effectiveness of our approach in segmenting skyline image 
this paper introduces a novel image representation capturing feature dependency through the mining of meaningful combination of visual feature this representation lead to a compact and discriminative encoding of image that can be used for image classification object detection or object recognition the method relies on i multiple random projection of the input space followed by local binarization of projected histogram encoded a set of item and ii the representation of image a histogram of pattern set hop the approach is validated on four publicly available datasets daimler pedestrian oxford flower kth texture and pascal voc allowing comparison with many recent approach the proposed image representation reach state of the art performance on each one of these datasets 
many computer vision algorithm employ subspace model to represent data many of these approach benefit from the ability to create an average or prototype for a set of subspace the most popular method in these situation is the karcher mean also known a the riemannian center of mass the prevalence of the karcher mean may lead some to assume that it provides the best average in all scenario however other subspace average that appear le frequently in the literature may be more appropriate for certain task the extrinsic manifold mean the l median and the flag mean are alternative average that can be substituted directly for the karcher mean in many application this paper evaluates the characteristic and performance of these four average on synthetic and real world data while the karcher mean generalizes the euclidean mean to the grassman manifold we show that the extrinsic manifold mean the l median and the flag mean behave more like median and are therefore more robust to the presence of outlier among the subspace being averaged we also show that while the karcher mean and l median are computed using iterative algorithm the extrinsic manifold mean and flag mean can be found analytically and are thus order of magnitude faster in practice finally we show that the flag mean is a generalization of the extrinsic manifold mean that permit subspace with different number of dimension to be averaged the result is a cookbook that map algorithm constraint and data property to the most appropriate subspace mean for a given application 
the paper proposes an advanced driver assistance system that correlate the driver s head pose to road hazard by analyzing both simultaneously in particular we aim at the prevention of rear end crash due to driver fatigue or distraction we contribute by three novel idea asymmetric appearance modeling d to d pose estimation enhanced by the introduced fermat point transform and adaptation of global haar ghaar classifier for vehicle detection under challenging lighting condition the system defines the driver s direction of attention in degree of freedom yawning and head nodding detection a well a vehicle detection and distance estimation having both road and driver s behaviour information and implementing a fuzzy fusion system we develop an integrated framework to cover all of the above subject we provide real time performance analysis for real world driving scenario 
standard geometric model fitting method take a an input a fixed set of feature pair greedily matched based only on their appearance inadvertently many valid match are discarded due to repetitive texture or large baseline between view point to address this problem matching should consider both feature appearance and geometric fitting error we jointly solve feature matching and multi model fitting problem by optimizing one energy the formulation is based on our generalization of the assignment problem and it efficient min cost max flow solver our approach significantly increase the number of correctly matched feature improves the accuracy of fitted model and is robust to larger baseline 
in this paper we study the problem of estimating relative pose between two camera in the presence of radial distortion specifically we consider minimal problem where one of the camera ha no or known radial distortion there are three useful case for this setup with a single unknown distortion i fundamental matrix estimation where the two camera are uncalibrated ii essential matrix estimation for a partially calibrated camera pair iii essential matrix estimation for one calibrated camera and one camera with unknown focal length we study the parameterization of these three problem and derive fast polynomial solver based on gro bner basis method we demonstrate the numerical stability of the solver on synthetic data the minimal solver have also been applied to real imagery with convincing result 
fisher kernel and deep learning were two development with significant impact on large scale object categorization in the last year both approach were shown to achieve state of the art result on large scale object categorization datasets such a imagenet conceptually however they are perceived a very different and it is not uncommon for heated debate to spring up when advocate of both paradigm meet at conference or workshop in this work we emphasize the similarity between both architecture rather than their difference and we argue that such a unified view allows u to transfer idea from one domain to the other a a concrete example we introduce a method for learning a support vector machine classifier with fisher kernel at the same time a a task specific data representation we reinterpret the setting a a multi layer feed forward network it final layer is the classifier parameterized by a weight vector and the two previous layer compute fisher vector parameterized by the coefficient of a gaussian mixture model we introduce a gradient descent based learning algorithm that in contrast to other feature learning technique is not just derived from intuition or biological analogy but ha a theoretical justification in the framework of statistical learning theory our experiment show that the new training procedure lead to significant improvement in classification accuracy while preserving the modularity and geometric interpretability of a support vector machine setup 
we consider the design of a single vector representation for an image that embeds and aggregate a set of local patch descriptor such a sift more specifically we aim to construct a dense representation like the fisher vector or vlad though of small or intermediate size we make two contribution both aimed at regularizing the individual contribution of the local descriptor in the final representation the first is a novel embedding method that avoids the dependency on absolute distance by encoding direction the second contribution is a democratization strategy that further limit the interaction of unrelated descriptor in the aggregation stage these method are complementary and give a substantial performance boost over the state of the art in image search with short or mid size vector a demonstrated by our experiment on standard public image retrieval benchmark 
current system for scene understanding typically represent object a d or d bounding box while these representation have proven robust in a variety of application they provide only coarse approximation to the true d and d extent of object a a result object object interaction such a occlusion or ground plane contact can be represented only superficially in this paper we approach the problem of scene understanding from the perspective of d shape modeling and design a d scene representation that reason jointly about the d shape of multiple object this representation allows to express d geometry and occlusion on the fine detail level of individual vertex of d wireframe model and make it possible to treat dependency between object such a occlusion reasoning in a deterministic way in our experiment we demonstrate the benefit of jointly estimating the d shape of multiple object in a scene over working with coarse box on the recently proposed kitti dataset of realistic street scene 
a method for identifying shape feature of local nature on the shape s boundary in a way that is facilitated by the presence of noise is presented the boundary is seen a a real function a study of a certain distance function reveals almost counter intuitively that vertex can be defined and localized better in the presence of noise thus the concept of noising a opposed to smoothing is conceived and presented the method work on both smooth and noisy shape the presence of noise having an effect of improving on the result of the smoothed version experiment with noise and a comparison to state of the art validate the method 
recently the emergence of kinect system ha demonstrated the benefit of predicting an intermediate body part labeling for d human pose estimation in conjunction with rgb d imagery the availability of depth information play a critical role so an important question is whether a similar representation can be developed with sufficient robustness in order to estimate d pose from rgb image this paper provides evidence for a positive answer by leveraging a d human body part labeling in image b second order label sensitive pooling over dynamically computed region resulting from a hierarchical decomposition of the body and c iterative structured output modeling to contextualize the process based on d pose estimate for robustness and generalization we take advantage of a recent large scale d human motion capture dataset human m that also ha human body part labeling annotation available with image we provide extensive experimental study where alternative intermediate representation are compared and report a substantial error reduction over competitive discriminative baseline that regress d human pose against global hog feature 
many state of the art image restoration approach do not scale well to larger image such a megapixel image common in the consumer segment computationally expensive optimization is often the culprit while efficient alternative exist they have not reached the same level of image quality the goal of this paper is to develop an effective approach to image restoration that offer both computational efficiency and high restoration quality to that end we propose shrinkage field a random field based architecture that combine the image model and the optimization algorithm in a single unit the underlying shrinkage operation bear connection to wavelet approach but is used here in a random field context computational efficiency is achieved by construction through the use of convolution and dft a the core component high restoration quality is attained through loss based training of all model parameter and the use of a cascade architecture unlike heavily engineered solution our learning approach can be adapted easily to different trade offs between efficiency and image quality we demonstrate state of the art restoration result with high level of computational efficiency and significant speedup potential through inherent parallelism 
object and structure within man made environment typically exhibit a high degree of organization in the form of orthogonal and parallel plane traditional approach to scene representation exploit this phenomenon via the somewhat restrictive assumption that every plane is perpendicular to one of the ax of a single coordinate system known a the manhattan world model this assumption is widely used in computer vision and robotics the complexity of many real world scene however necessitates a more flexible model we propose a novel probabilistic model that describes the world a a mixture of manhattan frame each frame defines a different orthogonal coordinate system this result in a more expressive model that still exploit the orthogonality constraint we propose an adaptive markov chain monte carlo sampling algorithm with metropolis hastings split merge move that utilizes the geometry of the unit sphere we demonstrate the versatility of our mixture of manhattan frame model by describing complex scene using depth image of indoor scene a well a aerial lidar measurement of an urban center additionally we show that the model lends itself to focal length calibration of depth camera and to plane segmentation 
intrinsic characterization of scene is often the best way to overcome the illumination variability artifact that complicate most computer vision problem from d reconstruction to object or material recognition this paper examines the deficiency of existing intrinsic image model to accurately account for the effect of illuminant color and sensor characteristic in the estimation of intrinsic image and present a generic framework which incorporates insight from color constancy research to the intrinsic image decomposition problem the proposed mathematical formulation includes information about the color of the illuminant and the effect of the camera sensor both of which modify the observed color of the reflectance of the object in the scene during the acquisition process by modeling these effect we get a truly intrinsic reflectance image which we call absolute reflectance which is invariant to change of illuminant or camera sensor this model allows u to represent a wide range of intrinsic image decomposition depending on the specific assumption on the geometric property of the scene configuration and the spectral property of the light source and the acquisition system thus unifying previous model in a single general framework we demonstrate that even partial information about sensor improves significantly the estimated reflectance image thus making our method applicable for a wide range of sensor we validate our general intrinsic image framework experimentally with both synthetic data and natural image 
the state of the art in image segmentation build hierarchical segmentation structure based on analyzing local feature cue in spectral setting due to their impressive performance such segmentation approach have become building block in many computer vision application nevertheless the main bottleneck are still the computationally demanding process of local feature processing and spectral analysis in this paper we demonstrate that based on a discrete continuous optimization of oriented gradient signal we are able to provide segmentation performance competitive to state of the art on bsds even without any spectral analysis while reducing computation time by a factor of and memory demand by a factor of 
use of higher order clique potential for modeling inference problem ha exploded in last few year the algorithmic scheme proposed so far do not scale well with increasing clique size thus limiting their use to clique of size at most in practice generic cut gc of arora et al show that when potential are submodular inference problem can be solved optimally in polynomial time for fixed size clique in this paper we report an algorithm called approximate cut ac which us a generalization of the gadget of gc and provides an approximate solution to inference in label mrf map problem with clique of size k the algorithm give optimal solution for submodular potential when potential are non submodular we show that important property such a weak persistency hold for solution inferred by ac ac is a polynomial time primal dual approximation algorithm for fixed clique size we show experimentally that ac not only provides significantly better solution in practice it is an order of magnitude faster than message passing scheme like dual decomposition and gtrws or reduction based technique like 
recently introduced cost effective depth sensor coupled with the real time skeleton estimation algorithm of shotton et al have generated a renewed interest in skeleton based human action recognition most of the existing skeleton based approach use either the joint location or the joint angle to represent a human skeleton in this paper we propose a new skeletal representation that explicitly model the d geometric relationship between various body part using rotation and translation in d space since d rigid body motion are member of the special euclidean group se the proposed skeletal representation lie in the lie group se se which is a curved manifold using the proposed representation human action can be modeled a curve in this lie group since classification of curve in this lie group is not an easy task we map the action curve from the lie group to it lie algebra which is a vector space we then perform classification using a combination of dynamic time warping fourier temporal pyramid representation and linear svm experimental result on three action datasets show that the proposed representation performs better than many existing skeletal representation the proposed approach also outperforms various state of the art skeleton based human action recognition approach 
in this paper we address the problem of object tracking in intensity image and depth data we propose a generic framework that can be used either for tracking d template in intensity image or for tracking d object in depth image to overcome problem like partial occlusion strong illumination change and motion blur that notoriously make energy minimization based tracking method get trapped in a local minimum we propose a learning based method that is robust to all these problem we use random forest to learn the relation between the parameter that defines the object s motion and the change they induce on the image intensity or the point cloud of the template it follows that to track the template when it move we use the change on the image intensity or point cloud to predict the parameter of this motion our algorithm ha an extremely fast tracking performance running at le than m per frame and is robust to partial occlusion moreover it demonstrates robustness to strong illumination change when tracking template using intensity image and robustness in tracking d object from arbitrary viewpoint even in the presence of motion blur that cause missing or erroneous data in depth image extensive experimental evaluation and comparison to the related approach strongly demonstrates the benefit of our method 
a the collection of large datasets becomes increasingly automated the occurrence of outlier will increase big data implies big outlier while principal component analysis pca is often used to reduce the size of data and scalable solution exist it is well known that outlier can arbitrarily corrupt the result unfortunately state of the art approach for robust pca do not scale beyond small to medium sized datasets to address this we introduce the grassmann average ga which express dimensionality reduction a an average of the subspace spanned by the data because average can be efficiently computed we immediately gain scalability ga is inherently more robust than pca but we show that they coincide for gaussian data we exploit that average can be made robust to formulate the robust grassmann average rga a a form of robust pca robustness can be with respect to vector subspace or element of vector we focus on the latter and use a trimmed average the resulting trimmed grassmann average tga is particularly appropriate for computer vision because it is robust to pixel outlier the algorithm ha low computational complexity and minimal memory requirement making it scalable to big noisy data we demonstrate tga for background modeling video restoration and shadow removal we show scalability by performing robust pca on the entire star war iv movie 
we propose a robust and accurate method to extract the centerline and scale of tubular structure in d image and d volume existing technique rely either on filter designed to respond to ideal cylindrical structure which lose accuracy when the linear structure become very irregular or on classification which is inaccurate because location on centerline and location immediately next to them are extremely difficult to distinguish we solve this problem by reformulating centerline detection in term of a regression problem we first train regressors to return the distance to the closest centerline in scale space and we apply them to the input image or volume the centerline and the corresponding scale then correspond to the regressors local maximum which can be easily identified we show that our method outperforms state of the art technique for various d and d datasets 
the use of wearable camera make it possible to record life logging egocentric video browsing such long unstructured video is time consuming and tedious segmentation into meaningful chapter is an important first step towards adding structure to egocentric video enabling efficient browsing indexing and summarization of the long video two source of information for video segmentation are i the motion of the camera wearer and ii the object and activity recorded in the video in this paper we address the motion cue for video segmentation motion based segmentation is especially difficult in egocentric video when the camera is constantly moving due to natural head movement of the wearer we propose a robust temporal segmentation of egocentric video into a hierarchy of motion class using a new cumulative displacement curve unlike instantaneous motion vector segmentation using integrated motion vector performs well even in dynamic and crowded scene no assumption are made on the underlying scene structure and the method work in indoor a well a outdoor situation we demonstrate the effectiveness of our approach using publicly available video a well a choreographed video we also suggest an approach to detect the fixation of wearer s gaze in the walking portion of the egocentric video 
we propose a novel discriminative model for semantic labeling in video by incorporating a prior to model both the shape and temporal dependency of an object in video a typical approach for this task is the conditional random field crf which can model local interaction among adjacent region in a video frame recent work ha shown how to incorporate a shape prior into a crf for improving labeling performance but it may be difficult to model temporal dependency present in video by using this prior the conditional restricted boltzmann machine crbm can model both shape and temporal dependency and ha been used to learn walking style from motioncapture data in this work we incorporate a crbm prior into a crf framework and present a new state of the art model for the task of semantic labeling in video in particular we explore the task of labeling part of complex face scene from video in the youtube face database yfdb our combined model outperforms competitive baseline both qualitatively and quantitatively 
we take a new approach to computing dense scene flow between a pair of consecutive rgb d frame we exploit the availability of depth data by seeking correspondence with respect to patch specified not a the pixel inside square window but a the d point that are the inliers of sphere in world space our primary contribution is to show that by reasoning in term of such patch under dof rigid body motion in d we succeed in obtaining compelling result at displacement large and small without relying on either of two simplifying assumption that pervade much of the earlier literature brightness constancy or local surface planarity a a consequence of our approach our output is a dense field of d rigid body motion in contrast to the d translation that are the norm in scene flow reasoning in our manner additionally allows u to carry out occlusion handling using a dof consistency check for the flow computed in both direction and a patchwise silhouette check to help reason about alignment in occlusion area and to promote smoothness of the flow field using an intuitive local rigidity prior we carry out our optimization in two step obtaining a first correspondence field using an adaptation of patchmatch and subsequently using alpha expansion to jointly handle occlusion and perform regularization we show attractive flow result on challenging synthetic and real world scene that push the practical limit of the aforementioned assumption 
this paper attempt to address the problem of recognizing human action while training and testing on distinct datasets when test video are neither labeled nor available during training in this scenario learning of a joint vocabulary or domain transfer technique are not applicable we first explore reason for poor classifier performance when tested on novel datasets and quantify the effect of scene background on action representation and recognition using only the background feature and partitioning of gist feature space we show that the background scene in recent datasets are quite discriminative and can be used classify an action with reasonable accuracy we then propose a new process to obtain a measure of confidence in each pixel of the video being a foreground region using motion appearance and saliency together in a d mrf based framework we also propose multiple way to exploit the foreground confidence to improve bag of word vocabulary histogram representation of a video and a novel histogram decomposition based representation and kernel we used these foreground confidence to recognize action trained on one data set and test on a different data set we have performed extensive experiment on several datasets that improve cross dataset recognition accuracy a compared to baseline method 
in this paper we tackle the problem of co localization in real world image co localization is the problem of simultaneously localizing with bounding box object of the same class across a set of distinct image although similar problem such a co segmentation and weakly supervised localization have been previously studied we focus on being able to perform co localization in real world setting which are typically characterized by large amount of intra class variation inter class diversity and annotation noise to address these issue we present a joint image box formulation for solving the co localization problem and show how it can be relaxed to a convex quadratic program which can be efficiently solved we perform an extensive evaluation of our method compared to previous state of the art approach on the challenging pascal voc and object discovery datasets in addition we also present a large scale study of co localization on imagenet involving ground truth annotation for class and approximately million image 
we consider the problem of deliberately manipulating the direct and indirect light flowing through a time varying fully general scene in order to simplify it visual analysis our approach rest on a crucial link between stereo geometry and light transport while direct light always obeys the epipolar geometry of a projector camera pair indirect light overwhelmingly doe not we show that it is possible to turn this observation into an imaging method that analyzes light transport in real time in the optical domain prior to acquisition this yield three key ability that we demonstrate in an experimental camera prototype producing a live indirect only video stream for any scene regardless of geometric or photometric complexity capturing image that make existing structured light shape recovery algorithm robust to indirect transport and turning them into one shot method for dynamic d shape capture 
we propose an approach to reconstructing tree structure that evolve over time in d image and d image stack such a neuronal axon or plant branch instead of reconstructing structure in each image independently we do so for all image simultaneously to take advantage of temporal consistency constraint we show that this problem can be formulated a a quadratic mixed integer program and solved efficiently the outcome of our approach is a framework that provides substantial improvement in reconstruction over traditional single time instance formulation furthermore an added benefit of our approach is the ability to automatically detect place where significant change have occurred over time which is challenging when considering large amount of data 
inferring human gaze from low resolution eye image is still a challenging task despite it practical importance in many application scenario this paper present a learning by synthesis approach to accurate image based gaze estimation that is personand head pose independent unlike existing appearance based method that assume person specific training data we use a large amount of cross subject training data to train a d gaze estimator we collect the largest and fully calibrated multi view gaze dataset and perform a d reconstruction in order to generate dense training data of eye image by using the synthesized dataset to learn a random regression forest we show that our method outperforms existing method that use low resolution eye image 
in this paper we propose a technique for video object segmentation using patch seam across frame typically seam which are connected path of low energy are utilised for retargeting where the primary aim is to reduce the image size while preserving the salient image content here we adapt the formulation of seam for temporal label propagation the energy function associated with the proposed video seam provides temporal linking of patch across frame to accurately segment the object the proposed energy function take into account the similarity of patch along the seam temporal consistency of motion and spatial coherency of seam label propagation is achieved with high fidelity in the critical boundary region utilising the proposed patch seam to achieve this without additional overhead we curtail the error propagation by formulating boundary region a rough set the proposed approach out perform state of the art supervised and unsupervised algorithm on benchmark datasets 
feature tracking in video is a crucial task in computer vision usually the tracking problem is handled one feature at a time using a single feature tracker like the kanade lucas tomasi algorithm or one of it derivative while this approach work quite well when dealing with high quality video and strong feature it often falter when faced with dark and noisy video containing low quality feature we present a framework for jointly tracking a set of feature which enables sharing information between the different feature in the scene we show that our method can be employed to track feature for both rigid and non rigid motion possibly of few moving body even when some feature are occluded furthermore it can be used to significantly improve tracking result in poorly lit scene where there is a mix of good and bad feature our approach doe not require direct modeling of the structure or the motion of the scene and run in real time on a single cpu core 
curvature ha received increasing attention a an important alternative to length based regularization in computer vision in contrast to length it preserve elongated structure and fine detail existing approach are either inefficient or have low angular resolution and yield result with strong block artifact we derive a new model for computing squared curvature based on integral geometry the model count response of straight line triple clique the corresponding energy decomposes into submodular and supermodular pairwise potential we show that this energy can be efficiently minimized even for high angular resolution using the trust region framework our result confirm that we obtain accurate and visually pleasing solution without strong artifact at reasonable runtimes 
the main contribution of this work is a framework to register anatomical structure characterized a a point set where each point ha an associated symmetric matrix these matrix can represent problem dependent characteristic of the registered structure for example in airway matrix can represent the orientation and thickness of the structure our framework relies on a dense tensor field representation which we implement sparsely a a kernel mixture of tensor field we equip the space of tensor field with a norm that serf a a similarity measure to calculate the optimal transformation between two structure we minimize this measure using an analytical gradient for the similarity measure and the deformation field which we restrict to be a diffeomorphism we illustrate the value of our tensor field model by comparing our result with scalar and vector field based model finally we evaluate our registration algorithm on synthetic data set and validate our approach on manually annotated airway tree 
the seminal multiple view stereo benchmark evaluation from middlebury and by strecha et al have played a major role in propelling the development of multi view stereopsis methodology although seminal these benchmark datasets are limited in scope with few reference scene here we try to take these work a step further by proposing a new multi view stereo dataset which is an order of magnitude larger in number of scene and with a significant increase in diversity specifically we propose a dataset containing scene of large variability each scene consists of or accurate camera position and reference structured light scan all acquired by a axis industrial robot to apply this dataset we propose an extension of the evaluation protocol from the middlebury evaluation reflecting the more complex geometry of some of our scene the proposed dataset is used to evaluate the state of the art multiview stereo algorithm of tola et al campbell et al and furukawa et al hereby we demonstrate the usability of the dataset a well a gain insight into the working and challenge of multi view stereopsis through these experiment we empirically validate some of the central hypothesis of multi view stereopsis a well a determining and reaffirming some of the central challenge 
a recent trend of research ha shown how contextual information related to an action such a a scene or object can enhance the accuracy of human action recognition system however using context to improve unsupervised human action clustering ha never been considered before and cannot be achieved using existing clustering method to solve this problem we introduce a novel general purpose algorithm dual assignment k mean dakm which is uniquely capable of performing two co occurring clustering task simultaneously while exploiting the correlation information to enhance both clustering furthermore we describe a spectral extension of dakm sdakm for better performance on realistic data extensive experiment on synthetic data and on three realistic human action datasets with scene context show that dakm sdakm can significantly outperform the state of the art clustering method by taking into account the contextual relationship between action and scene 
we present an approach that take a single photograph of a child a input and automatically produce a series of age progressed output between and year of age accounting for pose expression and illumination leveraging thousand of photo of child and adult at many age from the internet we first show how to compute average image subspace that are pixel to pixel aligned and model variable lighting these average depict a prototype man and woman aging from to under any desired illumination and capture the difference in shape and texture between age applying these difference to a new photo yield an age progressed result contribution include relightable age subspace a novel technique for subspace to subspace alignment and the most extensive evaluation of age progression technique in the literature 
the construction of facial deformable model fdms is a very challenging computer vision problem since the face is a highly deformable object and it appearance drastically change under different pose expression and illumination although several method for generic fdms construction have been proposed for facial landmark localization in still image they are insufficient for task such a facial behaviour analysis and facial motion capture where perfect landmark localization is required in this case person specific fdms psms are mainly employed requiring manual facial landmark annotation for each person and person specific training in this paper a novel method for the automatic construction of psms is proposed to this end an orthonormal subspace which is suitable for facial image reconstruction is learnt next to correct the fitting of a generic model image congealing i e batch image aliment is performed by employing only the learnt orthonormal subspace finally the corrected fitting are used to construct the psm the image congealing problem is solved by formulating a suitable sparsity regularized rank minimization problem the proposed method outperforms the state of the art method that is compared to in term of both landmark localization accuracy and computational time 
state of the art patch based image representation involve a pooling operation that aggregate statistic computed from local descriptor standard pooling operation include sumand max pooling sum pooling lack discriminability because the resulting representation is strongly influenced by frequent yet often uninformative descriptor but only weakly influenced by rare yet potentially highly informative one max pooling equalizes the influence of frequent and rare descriptor but is only applicable to representation that rely on count statistic such a the bag of visual word bov and it softand sparse coding extension we propose a novel pooling mechanism that achieves the same effect a max pooling but is applicable beyond the bov and especially to the state of the art fisher vector hence the name generalized max pooling gmp it involves equalizing the similarity between each patch and the pooled representation which is shown to be equivalent to re weighting the per patch statistic we show on five public image classification benchmark that the proposed gmp can lead to significant performance gain with respect to heuristic alternative 
we consider the problem of localizing a novel image in a large d model in principle this is just an instance of camera pose estimation but the scale introduces some challenging problem for one it make the correspondence problem very difficult and it is likely that there will be a significant rate of outlier to handle in this paper we use recent theoretical a well a technical advance to tackle these problem many modern camera and phone have gravitational sensor that allow u to reduce the search space further there are new technique to efficiently and reliably deal with extreme rate of outlier we extend these method to camera pose estimation by using accurate approximation and fast polynomial solver experimental result are given demonstrating that it is possible to reliably estimate the camera pose despite more than of outlier correspondence 
the underlying idea of multitask learning is that learning task jointly is better than learning each task individually in particular if only a few training example are available for each task sharing a jointly trained representation improves classification performance in this paper we propose a novel multitask learning method that learns a low dimensional representation jointly with the corresponding classifier which are then able to profit from the latent inter class correlation our method scale with respect to the original feature dimension and can be used with high dimensional image descriptor such a the fisher vector furthermore it consistently outperforms the current state of the art on the sun scene classification benchmark with varying amount of training data 
we study the problem of understanding object in detail intended a recognizing a wide array of fine grained object attribute to this end we introduce a dataset of airplane annotated in detail with part and their attribute leveraging image donated by airplane spotter and crowd sourcing both the design and collection of the detailed annotation we provide a number of insight that should help researcher interested in designing fine grained datasets for other basic level category we show that the collected data can be used to study the relation between part detection and attribute prediction by diagnosing the performance of classifier that pool information from different part of an object we note that the prediction of certain attribute can benefit substantially from accurate part detection we also show that differently from previous result in object detection employing a large number of part template can improve detection accuracy at the expense of detection speed we finally propose a coarse to fine approach to speed up detection through a hierarchical cascade algorithm 
we present a stereo algorithm designed for speed and efficiency that us local slanted plane sweep to propose disparity hypothesis for a semi global matching algorithm our local plane hypothesis are derived from initial sparse feature correspondence followed by an iterative clustering step local plane sweep are then performed around each slanted plane to produce out of plane parallax and matching cost estimate a final global optimization stage implemented using semi global matching assigns each pixel to one of the local plane hypothesis by only exploring a small fraction of the whole disparity space volume our technique achieves significant speedup over previous algorithm and achieves state of the art accuracy on high resolution stereo pair of up to megapixels 
previous effort in hashing intend to preserve data variance or pairwise affinity but neither is adequate in capturing the manifold structure hidden in most visual data in this paper we tackle this problem by reconstructing the locally linear structure of manifold in the binary hamming space which can be learned by locality sensitive sparse coding we cast the problem a a joint minimization of reconstruction error and quantization loss and show that despite it np hardness a local optimum can be obtained efficiently via alternative optimization our method distinguishes itself from existing method in it remarkable ability to extract the nearest neighbor of the query from the same manifold instead of from the ambient space on extensive experiment on various image benchmark our result improve previous state of the art by typically and on the yale face data 
we advocate the inference of qualitative information about d human pose called posebits from image posebits represent boolean geometric relationship between body part e g left leg in front of right leg or hand close to each other the advantage of posebits a a mid level representation are for many task of interest such qualitative pose information may be sufficient e g semantic image retrieval it is relatively easy to annotate large image corpus with posebits a it simply requires answer to yes no question and they help resolve challenging pose ambiguity and therefore facilitate the difficult talk of image based d pose estimation we introduce posebits a posebit database a method for selecting useful posebits for pose estimation and a structural svm model for posebit inference experiment show the use of posebits for semantic image retrieval and for improving d pose estimation 
we present an efficient and scalable algorithm for segmenting d rgbd point cloud by combining depth color and temporal information using a multistage hierarchical graph based approach our algorithm process a moving window over several point cloud to group similar region over a graph resulting in an initial over segmentation these region are then merged to yield a dendrogram using agglomerative clustering via a minimum spanning tree algorithm bipartite graph matching at a given level of the hierarchical tree yield the final segmentation of the point cloud by maintaining region identity over arbitrarily long period of time we show that a multistage segmentation with depth then color yield better result than a linear combination of depth and color due to it incremental processing our algorithm can process video of any length and in a streaming pipeline the algorithm s ability to produce robust efficient segmentation is demonstrated with numerous experimental result on challenging sequence from our own a well a public rgbd data set 
several popular and effective object detector separately model intra class variation arising from deformation and appearance change this reduces model complexity while enabling the detection of object across change in viewpoint object pose etc the deformable part model dpm is perhaps the most successful such model to date a common assumption is that the exponential number of template enabled by a dpm is critical to it success in this paper we show the counter intuitive result that it is possible to achieve similar accuracy using a small dictionary of deformation each component in our model is represented by a single hog template and a dictionary of flow field that determine the deformation the template may undergo while the number of candidate deformation is dramatically fewer than that for a dpm the deformed template tend to be plausible and interpretable in addition we discover that the set of deformation base is actually transferable across object category and that learning shared base across similar category can boost accuracy 
we introduce a method to reduce most higher order term of markov random field with binary label into lower order one without introducing any new variable while keeping the minimizer of the energy unchanged while the method doe not reduce all term it can be used with existing technique that transformsarbitrary term by introducing auxiliary variable and improve the speed the method eliminates a higher order term in the polynomial representation of the energy by finding the value assignment to the variable involved that cannot be part of a global minimizer and increasing the potential value only when that particular combination occurs by the exact amount that make the potential of lower order we also introduce a faster approximation that forego the guarantee of exact equivalence of minimizer in favor of speed with experiment on the same field of expert dataset used in previous work we show that the roof dual algorithm after the reduction label significantly more variable and the energy converges more rapidly 
dynamic bayesian network such a hidden markov model hmms are successfully used a probabilistic model for human motion the use of hidden variable make them expressive model but inference is only approximate and requires procedure such a particle filter or markov chain monte carlo method in this work we propose to instead use simple markov model that only model observed quantity we retain a highly expressive dynamic model by using interaction that are nonlinear and non parametric a presentation of our approach in term of latent variable show logarithmic growth for the computation of exact log likelihood in the number of latent state we validate our model on human motion capture data and demonstrate state of the art performance on action recognition and motion completion task 
histogram based feature have significantly contributed to recent development of image classification such a by sift local descriptor in this paper we propose a method to efficiently transform those histogram feature for improving the classification performance the l normalized histogram feature is regarded a a probability mass function which is modeled by dirichlet distribution based on the probabilistic modeling we induce the dirichlet fisher kernel for transforming the histogram feature vector the method work on the individual histogram feature to enhance the discriminative power at a low computational cost on the other hand in the bag of feature bof framework the dirichlet mixture model can be extended to gaussian mixture by transforming histogram based local descriptor e g sift and thereby we propose the method of dirichlet derived gmm fisher kernel in the experiment on diverse image classification task including recognition of subordinate object and material texture the proposed method improve the performance of the histogrambased feature and bof based fisher kernel being favorably competitive with the state of the art 
this paper address the problem of assigning object class label to image pixel following recent holistic formulation we cast scene labeling a inference of a conditional random field crf grounded onto superpixels the crf inference is specified a quadratic program qp with mutual exclusion mutex constraint on class label assignment the qp is solved using a beam search b which is well suited for scene labeling because it explicitly account for spatial extent of object conforms to inconsistency constraint from domain knowledge and ha low computational cost b gradually build a search tree whose node correspond to candidate scene labelings successor node are repeatedly generated from a select set of their parent node until convergence we prove that our b efficiently maximizes the qp objective of crf inference effectiveness of our b for scene labeling is evaluated on the benchmark msrc stanford backgroud pascal voc and datasets 
in this paper we present a depth guided photometric d reconstruction method that work solely with a depth camera like the kinect existing method that fuse depth with normal estimate use an external rgb camera to obtain photometric information and treat the depth camera a a black box that provides a low quality depth estimate our contribution to such method are two fold firstly instead of using an extra rgb camera we use the infra red ir camera of the depth camera system itself to directly obtain high resolution photometric information we believe that ours is the first method to use an ir depth camera system in this manner secondly photometric method applied to complex object result in numerous hole in the reconstructed surface due to shadow and self occlusion to mitigate this problem we develop a simple and effective multiview reconstruction approach that fuse depth and normal information from multiple viewpoint to build a complete consistent and accurate d surface representation we demonstrate the efficacy of our method to generate high quality d surface reconstruction for some complex d figurine 
the number of gps tagged image available on the web is increasing at a rapid rate the majority of such location tag are specified by the user either through manual tagging or localization chip embedded in the camera however a known issue with user shared image is the unreliability of such gps tag in this paper we propose a method for addressing this problem we assume a large dataset of gps tagged image which includes an unknown subset with contaminated tag is available we develop a robust method for identification and refinement of this subset using the rest of the image in the dataset in the proposed method we form a large number of triplet of matching image and use them for estimating the location of the query image utilizing structure from motion some of the generated estimation may be inaccurate due to the noisy gps tag in the dataset therefore we perform random walk on the estimation in order to identify the subset with the maximal agreement finally we estimate the gps tag of the query utilizing the identified consistent subset using a weighted mean we propose a new damping factor for random walk which conforms to the level of noise in the input and consequently robustifies random walk we evaluated the proposed framework on a dataset of over k user shared image the experiment show our method robustly improves the accuracy of gps tag under diverse scenario 
we address the problem of populating object category detection datasets with dense per object d reconstruction bootstrapped from class label ground truth figure ground segmentation and a small set of keypoint annotation our proposed algorithm first estimate camera viewpoint using rigid structure from motion then reconstructs object shape by optimizing over visual hull proposal guided by loose within class shape similarity assumption the visual hull sampling process attempt to intersect an object s projection cone with the cone of minimal subset of other similar object among those pictured from certain vantage point we show that our method is able to produce convincing per object d reconstruction on one of the most challenging existing object category detection datasets pascal voc our result may re stimulate once popular geometry oriented model based recognition approach 
in this paper we study the problem of blind deconvolution our analysis is based on the algorithm of chan and wong which popularized the use of sparse gradient prior via total variation we use this algorithm because many method in the literature are essentially adaptation of this framework such algorithm is an iterative alternating energy minimization where at each step either the sharp image or the blur function are reconstructed recent work of levin et al showed that any algorithm that try to minimize that same energy would fail a the desired solution ha a higher energy than the no blur solution where the sharp image is the blurry input and the blur is a dirac delta however experimentally one can observe that chan and wong s algorithm converges to the desired solution even when initialized with the no blur one we provide both analysis and experiment to resolve this paradoxical conundrum we find that both claim are right the key to understanding how this is possible lie in the detail of chan and wong s implementation and in how seemingly harmless choice result in dramatic effect our analysis reveals that the delayed scaling normalization in the iterative step of the blur kernel is fundamental to the convergence of the algorithm this then result in a procedure that eludes the no blur solution despite it being a global minimum of the original energy we introduce an adaptation of this algorithm and show that in spite of it extreme simplicity it is very robust and achieves a performance comparable to the state of the art 
several descriptor have been proposed in the past for d shape analysis yet none of them achieves best performance on all shape class in this paper we propose a novel method for d shape analysis using the covariance matrix of the descriptor rather than the descriptor themselves covariance matrix enable efficient fusion of different type of feature and modality they capture using the same representation not only the geometric and the spatial property of a shape region but also the correlation of these property within the region covariance matrix however lie on the manifold of symmetric positive definite spd tensor a special type of riemannian manifold which make comparison and clustering of such matrix challenging in this paper we study covariance matrix in their native space and make use of geodesic distance on the manifold a a dissimilarity measure we demonstrate the performance of this metric on d face matching and recognition task we then generalize the bag of feature paradigm originally designed in euclidean space to the riemannian manifold of spd matrix we propose a new clustering procedure that take into account the geometry of the riemannian manifold we evaluate the performance of the proposed bag of covariance matrix framework on d shape matching and retrieval application and demonstrate it superiority compared to descriptor based technique 
we propose a shape matching method that produce dense correspondence tuned to a specific class of shape and deformation in a scenario where this class is represented by a small set of example shape the proposed method learns a shape descriptor capturing the variability of the deformation in the given class the approach enables the wave kernel signature to extend the class of recognized deformation from near isometry to the deformation appearing in the example set by mean of a random forest classifier with the help of the introduced spatial regularization the proposed method achieves significant improvement over the baseline approach and obtains state of the art result while keeping short computation time 
we address the problem of camouflaging a d object from the many viewpoint that one might see it from given photograph of an object s surroundings we produce a surface texture that will make the object difficult for a human to detect to do this we introduce several background matching algorithm that attempt to make the object look like whatever is behind it of course it is impossible to exactly match the background from every possible viewpoint thus our model are forced to make trade offs between different perceptual factor such a the conspicuousness of the occlusion boundary and the amount of texture distortion we use experiment with human subject to evaluate the effectiveness of these model for the task of camouflaging a cube finding that they significantly outperform nai ve strategy 
computational and memory cost restrict spectral technique to rather small graph which is a serious limitation especially in video segmentation in this paper we propose the use of a reduced graph based on superpixels in contrast to previous work the reduced graph is reweighted such that the resulting segmentation is equivalent under certain assumption to that of the full graph we consider equivalence in term of the normalized cut and of it spectral clustering relaxation the proposed method reduces runtime and memory consumption and yield on par result in image and video segmentation further it enables an efficient data representation and update for a new streaming video segmentation approach that also achieves state of the art performance 
in this work we propose a technique to combine bottom up segmentation coming in the form of slic superpixels with sliding window detector such a deformable part model dpms the merit of our approach lie in cleaning up the low level hog feature by exploiting the spatial support of slic superpixels this can be understood a using segmentation to split the feature variation into object specific and background change rather than committing to a single segmentation we use a large pool of slic superpixels and combine them in a scale positionand object dependent manner to build soft segmentation mask the segmentation mask can be computed fast enough to repeat this process over every candidate window during training and detection for both the root and part filter of dpms we use these mask to construct enhanced background invariant feature to train dpms we test our approach on the pascal voc outperforming the standard dpm in out of class yielding an average increase of ap additionally we demonstrate the robustness of this approach extending it to dense sift descriptor for large displacement optical flow 
we present a novel solution to compute the relative pose of a generalized camera existing solution are either not general have too high computational complexity or require too many correspondence which impedes an efficient or accurate usage within ransac scheme we factorize the problem a a low dimensional iterative optimization over relative rotation only directly derived from well known epipolar constraint common generalized camera often consist of camera cluster and give rise to omni directional landmark observation we prove that our iterative scheme performs well in such practically relevant situation eventually resulting in computational efficiency similar to linear solver and accuracy close to bundle adjustment while using le correspondence experiment on both virtual and real multi camera system prove superior overall performance for robust real time multi camera motion estimation 
gaussian mixture model have become one of the major tool in modern statistical image processing and allowed performance breakthrough in patch based image denoising and restoration problem nevertheless their adoption level wa kept relatively low because of the computational cost associated to learning such model on large image database this work provides a flexible and generic tool for dealing with such model without the computational penalty or parameter tuning difficulty associated to a nai ve implementation of gmm based image restoration task it doe so by organising the data manifold in a hirerachical multiscale structure the covariance tree that can be queried at various scale level around any point in feature space we start by explaining how to construct a covariance tree from a subset of the input data how to enrich it statistic from a larger set in a streaming process and how to query it efficiently at any scale we then demonstrate it usefulness on several application including non local image filtering data driven denoising reconstruction from random sample and surface modeling from unorganized d point set 
graph based method are a useful class of method for improving the performance of unsupervised and semi supervised machine learning task such a clustering or information retrieval however the performance of existing graph based method is highly dependent on how well the affinity graph reflects the original data structure we propose that multimedia such a image or video consist of multiple separate component and therefore more than one graph is required to fully capture the relationship between them accordingly we present a new spectral method the feature grouped spectral multigraph fgsm which comprises the following step first mutually independent subset of the original feature space are generated through feature clustering secondly a separate graph is generated from each feature subset finally a spectral embedding is calculated on each graph and the embeddings are scaled aggregated into a single representation using this representation a variety of experiment are performed on three learning task clustering retrieval and recognition on human action datasets demonstrating considerably better performance than the state of the art 
in this paper we propose a weighted supervised pooling method for visual recognition system we combine a standard spatial pyramid representation which is commonly adopted to encode spatial information with an appropriate feature space representation favoring semantic information in an appropriate feature space for the latter we propose a weighted pooling strategy exploiting data supervision to weigh each local descriptor coherently with it likelihood to belong to a given object class the two representation are then combined adaptively with multiple kernel learning experiment on common benchmark caltech and pascal voc show that our image representation improves the current visual recognition pipeline and it is competitive with similar state of art pooling method we also evaluate our method on a real human robot interaction setting where the pure spatial pyramid representation doe not provide sufficient discriminative power obtaining a remarkable improvement 
when building vision system that predict structured object such a image segmentation or human pose a crucial concern is performance under task specific evaluation measure e g jaccard index or average precision an ongoing research challenge is to optimize prediction so a to maximize performance on such complex measure in this work we present a simple meta algorithm that is surprisingly effective empirical min bayes risk embr take a input a pre trained model that would normally be the final product and learns three additional parameter so a to optimize performance on the complex high order task specific measure we demonstrate embr in several domain taking existing state of the art algorithm and improving performance up to simply with three extra parameter 
in this paper we study the role of context in existing state of the art detection and segmentation approach towards this goal we label every pixel of pascal voc detection challenge with a semantic category we believe this data will provide plenty of challenge to the community a it contains additional class for semantic segmentation and object detection our analysis show that nearest neighbor based approach perform poorly on semantic segmentation of contextual class showing the variability of pascal imagery furthermore improvement of existing contextual model for detection is rather modest in order to push forward the performance in this difficult scenario we propose a novel deformable part based model which exploit both local context around each candidate detection a well a global context at the level of the scene we show that this contextual reasoning significantly help in detecting object at all scale 
in this paper we propose a novel labeling cost for multiview reconstruction existing approach use data term with specific weakness that are vulnerable to common challenge such a low textured region or specularities our new probabilistic method implicitly discard outlier and can be shown to become more exact the closer we get to the true object surface our approach achieves top result among all published method on the middlebury dino sparse dataset and also delivers accurate result on several other datasets with widely varying challenge for which it work in unchanged form 
the essential matrix which encodes the epipolar constraint between point in two projective view is a cornerstone of modern computer vision previous work have proposed different characterization of the space of essential matrix a a riemannian manifold however they either do not consider the symmetric role played by the two view or do not fully take into account the geometric peculiarity of the epipolar constraint we address these limitation with a characterization a a quotient manifold which can be easily interpreted in term of camera pose while our main focus in on theoretical aspect we include experiment in pose averaging and show that the proposed formulation produce a meaningful distance between essential matrix 
the transfer learning and domain adaptation problem originate from a distribution mismatch between the source and target data distribution the cause of such mismatch are traditionally considered different thus transfer learning and domain adaptation algorithm are designed to address different issue and cannot be used in both setting unless substantially modified still one might argue that these problem are just different declination of learning to learn i e the ability to leverage over prior knowledge when attempting to solve a new task we propose a learning to learn framework able to leverage over source data regardless of the origin of the distribution mismatch we consider prior model a expert and use their output confidence value a feature we use them to build the new target model combined with the feature from the target data through a high level cue integration scheme this result in a class of algorithm usable in a plug and play fashion over any learning to learn scenario from binary and multi class transfer learning to single and multiple source domain adaptation setting experiment on several public datasets show that our approach consistently achieves the state of the art 
we propose a method for human pose estimation based on deep neural network dnns the pose estimation is formulated a a dnn based regression problem towards body joint we present a cascade of such dnn regressors which result in high precision pose estimate the approach ha the advantage of reasoning about pose in a holistic fashion and ha a simple but yet powerful formulation which capitalizes on recent advance in deep learning we present a detailed empirical analysis with state ofart or better performance on four academic benchmark of diverse real world image 
in the following paper we present an approach for fine grained recognition based on a new part detection method in particular we propose a nonparametric label transfer technique which transfer part constellation from object with similar global shape the possibility for transferring part annotation to unseen image allows for coping with a high degree of pose and view variation in scenario where traditional detection model such a deformable part model fail our approach is especially valuable for fine grained recognition scenario where intraclass variation are extremely high and precisely localized feature need to be extracted furthermore we show the importance of carefully designed visual extraction strategy such a combination of complementary feature type and iterative image segmentation and the resulting impact on the recognition performance in experiment our simple yet powerful approach achieves and accuracy on the cub and bird datasets which is the current best performance for these benchmark 
an action is typically composed of different part of the object moving in particular sequence the presence of different motion represented a a d histogram ha been used in the traditional bag of word bow approach for recognizing action however the interaction among the motion also form a crucial part of an action different object part have varying degree of interaction with the other part during an action cycle it is these interaction we want to quantify in order to bring in additional information about the action in this paper we propose a causality based approach for quantifying the interaction to aid action classification granger causality is used to compute the cause and effect relationship for pair of motion trajectory of a video a d histogram descriptor for the video is constructed using these pairwise measure our proposed method of obtaining pairwise measure for video is also applicable for large datasets we have conducted experiment on challenging action recognition database such a hmdb and ucf and shown that our causality descriptor help in encoding additional information regarding the action and performs on par with the state of the art approach due to the complementary nature a further increase in performance can be observed by combining our approach with state of the art approach 
we propose a deep learning framework for image set classification with application to face recognition an adaptive deep network template adnt is defined whose parameter are initialized by performing unsupervised pre training in a layer wise fashion using gaussian restricted boltzmann machine grbms the pre initialized adnt is then separately trained for image of each class and class specific model are learnt based on the minimum reconstruction error from the learnt class specific model a majority voting strategy is used for classification the proposed framework is extensively evaluated for the task of image set classification based face recognition on honda ucsd cmu mobo youtube celebrity and a kinect dataset our experimental result and comparison with existing state of the art method show that the proposed method consistently achieves the best performance on all these datasets 
we derive an easy to implement and efficient algorithm for solving multi label image partitioning problem in the form of the problem addressed by region competition these problem jointly determine a parameter for each of the region in the partition given an estimate of the parameter a fast approximate solution to the multi label sub problem is derived by a global update that us smoothing and thresholding the method is empirically validated to be robust to fine detail of the image that plague local solution further in comparison to global method for the multi label problem the method is more efficient and it is easy for a non specialist to implement we give sample matlab code for the multi label chan vese problem in this paper experimental comparison to the state of the art in multi label solution to region competition show that our method achieves equal or better accuracy with the main advantage being speed and ease of implementation 
retinal image contain forest of mutually intersecting and overlapping venous and arterial vascular tree the geometry of these tree show adaptation to vascular disease including diabetes stroke and hypertension segmentation of the retinal vascular network is complicated by inconsistent vessel contrast fuzzy edge variable image quality medium opacity complex intersection and overlap this paper present a bayesian approach to resolving the configuration of vascular junction to correctly construct the vascular tree a probabilistic model of vascular joint terminal bridge and bifurcation and their configuration in junction is built and maximum a posteriori map estimation used to select most likely configuration the model is built using a reference set of joint extracted from the drive public domain vascular segmentation dataset and evaluated on joint from the drive test set demonstrating an accuracy of 
in this paper we present a conceptually simple but surprisingly powerful method for visual prediction which combine the effectiveness of mid level visual element with temporal modeling our framework can be learned in a completely unsupervised manner from a large collection of video however more importantly because our approach model the prediction framework on these mid level element we can not only predict the possible motion in the scene but also predict visual appearance how are appearance going to change with time this yield a visual hallucination of probable event on top of the scene we show that our method is able to accurately predict and visualize simple future event we also show that our approach is comparable to supervised method for event prediction 
a simple approach to learning invariance in image classification consists in augmenting the training set with transformed version of the original image however given a large set of possible transformation selecting a compact subset is challenging indeed all transformation are not equally informative and adding uninformative transformation increase training time with no gain in accuracy we propose a principled algorithm image transformation pursuit itp for the automatic selection of a compact set of transformation itp work in a greedy fashion by selecting at each iteration the one that yield the highest accuracy gain itp also allows to efficiently explore complex transformation that combine basic transformation we report result on two public benchmark the cub dataset of bird image and the imagenet challenge using fisher vector representation we achieve an improvement from to in top accuracy on cub and an improvement from to in top accuracy on imagenet we also show significant improvement for deep convnet feature from to on cub and from to on imagenet 
the notion of relative attribute a introduced by parikh and grauman iccv provides an appealing way of comparing two image based on their visual property or attribute such a smiling for face image naturalness for outdoor image etc for learning such attribute a ranking svm based formulation wa proposed that us globally represented pair of annotated image in this paper we extend this idea towards learning relative attribute using local part that are shared across category first instead of using a global representation we introduce a part based representation combining a pair of image that specifically compare corresponding part then with each part we associate a locally adaptive significance coefficient that represents it discriminative ability with respect to a particular attribute for each attribute the significance coefficient are learned simultaneously with a max margin ranking model in an iterative manner compared to the baseline method the new method is shown to achieve significant improvement in relative attribute prediction accuracy additionally it is also shown to improve relative feedback based interactive image search 
in recent year large image data set such a imagenet tinyimages or ever growing social network like flickr have emerged posing new challenge to image classification that were not apparent in smaller image set in particular the efficient handling of dynamically growing data set where not only the amount of training image but also the number of class increase over time is a relatively unexplored problem to remedy this we introduce nearest class mean forest ncmf a variant of random forest where the decision node are based on nearest class mean ncm classification ncmfs not only outperform conventional random forest but are also well suited for integrating new class to this end we propose and compare several approach to incorporate data from new class so a to seamlessly extend the previously trained forest instead of re training them from scratch in our experiment we show that ncmfs trained on small data set with class can be extended to large data set with class without significant loss of accuracy compared to training from scratch on the full data 
this paper present an improvement of the j linkage algorithm for fitting multiple instance of a model to noisy data corrupted by outlier the binary preference analysis implemented by j linkage is replaced by a continuous soft or fuzzy generalization that prof to perform better than j linkage on simulated data and compare favorably with state of the art method on public domain real datasets 
we propose an approach for semantifying web extracted fact in particular we map subject and object term of these fact to instance and relational phrase to object property defined in a target knowledge base by doing this we resolve the ambiguity inherent in the web extracted fact while simultaneously enriching the target knowledge base with a significant number of new assertion in this paper we focus on the mapping of the relational phrase in the context of the overall work ow furthermore in an open extraction setting identical semantic relationship can be represented by different surface form making it necessary to group these surface form together to solve this problem we propose the use of markov clustering in this work we present a complete ontology independent generalized workflow which we evaluate on fact extracted by nell and reverb our target knowledge base is dbpedia our evaluation show promising result in term of producing highly precise fact moreover the result indicate that the clustering of relational phrase pay of in term of an improved instance and property mapping 
augmented reality ar browser are an emerging category of mobile application that add interactive virtual object to the user s view of the physical world this paper give the first system level evaluation of their security and privacy property we start by analyzing the functional requirement that ar browser must support in order to present ar content we then investigate the security architecture of junaio layar and wikitude browser which are running today on over million mobile device and identify new category of security and privacy vulnerability unique to ar browser finally we provide the first engineering guideline for securely implementing ar functionality 
contest are widely used a a mean for effort elicitation in setting ranging from government r d contest to online crowdsourcing contest on platform such a kaggle innocentive or topcoder such rank order mechanism where agent reward depend only on the relative ranking of their submission quality are natural mechanism for incentivizing effort when it is easier to obtain ordinal rather than cardinal information about agent output or where absolute measure of quality are unverifiable an increasing number of online contest however rank entry according to some numerical evaluation of their absolute quality for instance the performance of an algorithm on a test dataset or the performance of an intervention in a randomized trial can the contest designer incentivize higher effort by making the reward in an ordinal rank order mechanism contingent on such cardinal information we model and analyze cardinal contest where a principal running a rank order tournament ha access to an absolute measure of the quality of agent submission in addition to their relative ranking and ask how modifying the rank order tournament to incorporate cardinal information can improve incentive for effort our main result is that a simple threshold mechanism a mechanism that award the prize for a rank if and only if the absolute quality of the agent at that rank exceeds a certain threshold is optimal amongst all mixed cardinal ordinal mechanism where the fraction of the jth prize awarded to the jth ranked agent is any arbitrary non decreasing function of her submission s quality further the optimal threshold mechanism us exactly the same threshold for each rank we study what contest parameter determine the extent of the benefit from incorporating such cardinal information into an ordinal rank order contest and investigate the extent of improvement in equilibrium effort via numerical simulation 
we investigate the practice of website selling counterfeit good we inspect web search result for query across brand we devise a binary classifier that predicts whether a given website is selling counterfeit by examining automatically extracted feature such a whois information pricing and website content we then apply the classifier to result collected between january and august we find that overall of search result point to website selling fake for complicit search term such a replica rolex of the search result point to fake compared to for innocent term such a hermes buy online using a linear regression we find that brand with a higher street price for fake have higher incidence of counterfeit in search result but that brand who take active countermeasure such a filing dmca request experience lower incidence of counterfeit in search result finally we study how the incidence of counterfeit evolves over time finding that the fraction of search result pointing to fake remains remarkably stable 
voice activated intelligent assistant such a siri google now and cortana are prevalent on mobile device however it is challenging to evaluate them due to the varied and evolving number of task supported e g voice command web search and chat since each task may have it own procedure and a unique form of correct answer it is expensive to evaluate each task individually this paper is the first attempt to solve this challenge we develop consistent and automatic approach that can evaluate different task in voice activated intelligent assistant we use implicit feedback from user to predict whether user are satisfied with the intelligent assistant a well a it component i e speech recognition and intent classification using this approach we can potentially evaluate and compare different task within and across intelligent assistant ac cording to the predicted user satisfaction rate our approach is characterized by an automatic scheme of categorizing user system interaction into task independent dialog action e g the user is commanding selecting or confirming an action we use the action sequence in a session to predict user satisfaction and the quality of speech recognition and intent classification we also incorporate other feature to further improve our approach including feature derived from previous work on web search satisfaction prediction and those utilizing acoustic characteristic of voice request we evaluate our approach using data collected from a user study result show our approach can accurately identify satisfactory and unsatisfactory session 
we examine the first large real world data set on personal knowledge question s security and memorability from their deployment at google our analysis confirms that secret question generally offer a security level that is far lower than user chosen password it turn out to be even lower than proxy such a the real distribution of surname in the population would indicate surprisingly we found that a significant cause of this insecurity is that user often don t answer truthfully a user survey we conducted revealed that a significant fraction of user who admitted to providing fake answer did so in an attempt to make them harder to guess although on aggregate this behavior had the opposite effect a people harden their answer in the same and predictable way on the usability side we show that secret answer have surprisingly poor memorability despite the assumption that their reliability motivates their continued deployment from million of account recovery attempt we observed a significant fraction of user e g of our english speaking u user were unable to recall their answer when needed this is lower than the success rate of alternative recovery mechanism such a sm reset code over comparing question strength and memorability reveals that the question that are potentially the most secure e g what is your first phone number are also the one with the worst memorability we conclude that it appears next to impossible to find secret question that are both secure and memorable secret question continue have some use when combined with other signal but they should not be used alone and best practice should favor more reliable alternative 
the successful development and deployment of large scale internet service depends critically on performance even small regression in processing time can translate directly into significant energy and user experience cost despite the widespread use of distributed server infrastructure e g in cloud computing and web service there is little research on how to benchmark such system to obtain valid and precise inference with minimal data collection cost correctly a b testing distributed internet service can be surprisingly difficult because interdependency between user request e g for search result social medium stream photo and host server violate assumption required by standard statistical test we develop statistical model of distributed internet service performance based on data from perflab a production system used at facebook which vet thousand of change to the company s codebase each day we show how these model can be used to understand the tradeoff between different benchmarking routine and what factor must be taken into account when performing statistical test using simulation and empirical data from perflab we validate our theoretical result and provide easy to implement guideline for designing and analyzing such benchmark 
consider a user who submits a search query shakira having a specific search goal in mind such a her age but at the same time willing to explore information for other entity related to her such a comparable singer in previous work a system called spark wa developed to provide such search experience given a query submitted to the yahoo search engine spark provides related entity suggestion for the query exploiting among else public knowledge base from the semantic web we refer to this search scenario a explorative entity search the effectiveness and efficiency of the approach ha been demonstrated in previous work the way user interact with these related entity suggestion and whether this interaction can be predicted have however not been studied in this paper we perform a large scale analysis into how user interact with the entity result returned by spark we characterize the user query and session that appear to promote an explorative behavior based on this analysis we develop a set of query and user based feature that reflect the click behavior of user and explore their effectiveness in the context of a prediction task 
it is well known that the performance of web browsing a well a mobile application or apps suffers on today s cellular network in this work we perform a systematic measurement study of more than popular apps and cellular network and discover that while cellular network have predictable latency it is the path between exit point of cellular network e g ggsn and cloud server that degrades apps performance high latency and unpredictability over this path affect browsing and activity completion time of apps worsening the performance by several magnitude furthermore we find that a the number of apps on mobile device increase cellular network in turn suffer due to large number of active connection primarily used for push notification experiencing heavy signaling overhead in the network towards accelerating the performance of apps and improving their operational efficiency we envision an easy to deploy operator managed platform and study two architectural optimization that sit at vantage point inside cellular network virtual app server vapp and network assisted virtual push notification server vpns vapps improve apps browsing experience while vpnss take the burden of carrying periodic message off cellular network using trace driven simulation we find that vapps can improve activity completion time by more than fold whereas vpns can reduce the signaling load by a factor of in cellular network and reduce energy consumption by a factor of on mobile device 
the heterogeneous information network hin is a graph data model in which node and edge are annotated with class and relationship label large and complex datasets such a yago or dblp can be modeled a hin recent work ha studied how to make use of these rich information source in particular meta path which represent sequence of node class and edge type between two node in a hin have been proposed for such task a information retrieval decision making and product recommendation current method assume meta path are found by domain expert however in a large and complex hin retrieving meta path manually can be tedious and difficult we thus study how to discover meta path automatically specifically user are asked to provide example pair of node that exhibit high proximity we then investigate how to generate meta path that can best explain the relationship between these node pair since this problem is computationally intractable we propose a greedy algorithm to select the most relevant meta path we also present a data structure to enable efficient execution of this algorithm we further incorporate hierarchical relationship among node class in our solution extensive experiment on real world hin show that our approach capture important meta path in an efficient and scalable manner 
we present a co clustering framework that can be used to discover multiple semantic and visual sens of a given noun phrase np unlike traditional clustering approach which assume a one to one mapping between the cluster in the text based feature space and the visual space we adopt a one to many mapping between the two space this is primarily because each semantic sense concept can correspond to different visual sens due to viewpoint and appearance variation our structure em style optimization not only extract the multiple sens in both semantic and visual feature space but also discovers the mapping between the sens we introduce a challenging dataset cmu polysemy for this problem consisting of np labeled instance out of k total instance we have also conducted a large scale experiment that performs sense disambiguation for np 
this paper develops a novel framework for semantic image retrieval based on the notion of a scene graph our scene graph represent object man boat attribute of object boat is white and relationship between object man standing on boat we use these scene graph a query to retrieve semantically related image to this end we design a conditional random field model that reason about possible grounding of scene graph to test image the likelihood of these grounding are used a ranking score for retrieval we introduce a novel dataset of human generated scene graph grounded to image and use this dataset to evaluate our method for image retrieval in particular we evaluate retrieval using full scene graph and small scene subgraphs and show that our method outperforms retrieval method that use only object or low level image feature in addition we show that our full model can be used to improve object localization compared to baseline method 
in this paper we introduce a spherical embedding technique to position a given set of silhouette of an object a observed from a set of camera arbitrarily positioned around the object our technique estimate dissimilarity among the silhouette and embeds them directly in the rotation space so the embedding is obtained by an optimization scheme applied over the rotation represented with exponential map since the measure for inter silhouette dissimilarity contains many outlier our key idea is to perform the embedding by only using a subset of the estimated dissimilarity we present a technique that carefully screen for inlier distance and the pairwise scaled dissimilarity are embedded in a spherical space diffeomorphic to so we show that our method outperforms spherical md embedding demonstrate it performance on various multi view set and highlight it robustness to outlier 
in this paper we address the problem of model free online object tracking based on color representation according to the finding of recent benchmark evaluation such tracker often tend to drift towards region which exhibit a similar appearance compared to the object of interest to overcome this limitation we propose an efficient discriminative object model which allows u to identify potentially distracting region in advance furthermore we exploit this knowledge to adapt the object representation beforehand so that distractors are suppressed and the risk of drifting is significantly reduced we evaluate our approach on recent online tracking benchmark datasets demonstrating state of the art result in particular our approach performs favorably both in term of accuracy and robustness compared to recent tracking algorithm moreover the proposed approach allows for an efficient implementation to enable online object tracking in real time 
depth camera have helped commoditize d digitization of the real world it is now feasible to use a single kinect like camera to scan in an entire building or other large scale scene at large scale however there is an inherent challenge of dealing with distortion and drift due to accumulated pose estimation error existing technique suffer from one or more of the following a requiring an expensive offline global optimization step taking hour to compute b needing a full second pas over the input depth frame to correct for accumulated error c relying on rgb data alongside depth data to optimize pose or d requiring the user to create explicit loop closure to allow gross alignment error to be resolved in this paper we present a method that address all of these issue our method support online model correction without needing to reprocess or store any input depth data even while performing global correction of a large d model our method take only minute rather than hour to compute our model doe not require any explicit loop closure to be detected and finally relies on depth data alone allowing operation in low lighting condition we show qualitative result on many large scale scene highlighting the lack of error and drift in our reconstruction we compare to state of the art technique and demonstrate large scale dense surface reconstruction in the dark a capability not offered by rgb d technique 
light field camera are now used in consumer and industrial application recent paper and product have demonstrated practical depth recovery algorithm from a passive single shot capture however current light field capture device have narrow baseline and constrained spatial resolution therefore the accuracy of depth recovery is limited requiring heavy regularization and producing planar depth that do not resemble the actual geometry using shading information is essential to improve the shape estimation we develop an improved technique for local shape estimation from defocus and correspondence cue and show how shading can be used to further refine the depth light field camera are able to capture both spatial and angular data suitable for refocusing by locally refocusing each spatial pixel to it respective estimated depth we produce an all in focus image where all viewpoint converge onto a point in the scene therefore the angular pixel have angular coherence which exhibit three property photo consistency depth consistency and shading consistency we propose a new framework that us angular coherence to optimize depth and shading the optimization framework estimate both general lighting in natural scene and shading to improve depth regularization our method outperforms current state of the art light field depth estimation algorithm in multiple scenario including real image 
in this paper we show that multiple object tracking mot can be formulated in a framework where the detection and data association are performed simultaneously our method allows u to overcome the confinement of data association based mot approach where the performance is dependent on the object detection result provided at input level at the core of our method lie structured learning which learns a model for each target and infers the best location of all target simultaneously in a video clip the inference of our structured learning is done through a new target identity aware network flow tinf where each node in the network encodes the probability of each target identity belonging to that node the proposed lagrangian relaxation optimization find the high quality solution to the network during optimization a soft spatial constraint is enforced between the node of the graph which help reducing the ambiguity caused by nearby target with similar appearance in crowded scenario we show that automatically detecting and tracking target in a single framework can help resolve the ambiguity due to frequent occlusion and heavy articulation of target our experiment involve challenging yet distinct datasets and show that our method can achieve result better than the state of art 
we tackle the problem of large scale visual place recognition where the task is to quickly and accurately recognize the location of a given query photograph we present the following four principal contribution first we develop a convolutional neural network cnn architecture that is trainable in an end to end manner directly for the place recognition task the main component of this architecture netvlad is a new generalized vlad layer inspired by the vector of locally aggregated descriptor image representation commonly used in image retrieval the layer is readily pluggable into any cnn architecture and amenable to training via backpropagation second we create a new weakly supervised ranking loss which enables end to end learning of the architecture s parameter from image depicting the same place over time downloaded from google street view time machine third we develop an efficient training procedure which can be applied on very large scale weakly labelled task finally we show that the proposed architecture and training procedure significantly outperform non learnt image representation and off the shelf cnn descriptor on challenging place recognition and image retrieval benchmark 
there are various parametric model for analyzing pairwise comparison data including the bradley terry luce btl and thurstone model but their reliance on strong parametric assumption is limiting in this paper we study a flexible model for pairwise comparison under which the probability of outcome are required only to satisfy a natural form of stochastic transitivity this class includes parametric model including the btl and thurstone model a special case but is considerably more general we provide various example of model in this broader stochastically transitive class for which classical parametric model provide poor fit despite this greater flexibility we show that the matrix of probability can be estimated at the same rate a in standard parametric model up to logarithmic term on the other hand unlike in the btl and thurstone model computing the minimax optimal estimator in the stochastically transitive model is non trivial and we explore various computationally tractable alternative we show that a simple singular value thresholding algorithm is statistically consistent but doe not achieve the minimax rate we then propose and study algorithm that achieve the minimax rate over interesting sub class of the full stochastically transitive class we complement our theoretical result with thorough numerical simulation 
in the graph inference problem one seek to recover the edge of an unknown graph from the observation of cascade propagating over this graph we approach this problem from the sparse recovery perspective we introduce a general model of cascade including the voter model and the independent cascade model for which we provide the first algorithm which recovers the graph s edge with high probability and o s log m measurement where s is the maximum degree of the graph and m is the number of node furthermore we show that our algorithm also recovers the edge weight the parameter of the diffusion process and is robust in the context of approximate sparsity finally we validate our approach empirically on synthetic graph 
the state of the art salient object detection model are able to perform well for relatively simple scene yet for more complex one they still have difficulty in highlighting salient object completely from background largely due to the lack of sufficiently robust feature for saliency prediction to address such an issue this paper proposes a novel hierarchy associated feature construction framework for salient object detection which is based on integrating elementary feature from multi level region in a hierarchy furthermore multi layered deep learning feature are introduced and incorporated a elementary feature into this framework through a compact integration scheme this lead to a rich feature representation which is able to represent the context of the whole object background and is much more discriminative a well a robust for salient object detection extensive experiment on the most widely used and challenging benchmark datasets demonstrate that the proposed approach substantially outperforms the state of the art on salient object detection 
deep learning technique have been successfully applied in many area of computer vision including low level image restoration problem for image super resolution several model based on deep neural network have been recently proposed and attained superior performance that overshadows all previous handcrafted model the question then arises whether large capacity and data driven model have become the dominant solution to the ill posed super resolution problem in this paper we argue that domain expertise represented by the conventional sparse coding model is still valuable and it can be combined with the key ingredient of deep learning to achieve further improved result we show that a sparse coding model particularly designed for super resolution can be incarnated a a neural network and trained in a cascaded structure from end to end the interpretation of the network based on sparse coding lead to much more efficient and effective training a well a a reduced model size our model is evaluated on a wide range of image and show clear advantage over existing state of the art method in term of both restoration accuracy and human subjective quality 
in this work we address the human parsing task with a novel contextualized convolutional neural network co cnn architecture which well integrates the cross layer context global image level context semantic edge context within super pixel context and cross super pixel neighborhood context into a unified network given an input human image co cnn produce the pixel wise categorization in an end to end way first the cross layer context is captured by our basic local to global to local structure which hierarchically combine the global semantic information and the local fine detail across different convolutional layer second the global image level label prediction is used a an auxiliary objective in the intermediate layer of the co cnn and it output are further used for guiding the feature learning in subsequent convolutional layer to leverage the global image level context third semantic edge context is further incorporated into co cnn where the high level semantic boundary are leveraged to guide pixel wise labeling finally to further utilize the local super pixel context the within super pixel smoothing and cross super pixel neighbourhood voting are formulated a natural sub component of the co cnn to achieve the local label consistency in both training and testing process comprehensive evaluation on two public datasets well demonstrate the significant superiority of our co cnn over other state of the art for human parsing in particular the f score on the large dataset reach text percent by co cnn significantly higher than text percent and text percent by the state of the art algorithm m cnn and atr respectively by utilizing our newly collected large dataset for training our co cnn can achieve text percent in f score 
we present saul a new probabilistic programming language designed to address some of the shortcoming of programming language that aim at advancing and simplifying the development of ai system such language need to interact with messy naturally occurring data to allow a programmer to specify what need to be done at an appropriate level of abstraction rather than at the data level to be developed on a solid theory that support moving to and reasoning at this level of abstraction and finally to support flexible integration of these learning and inference model within an application program saul is an object functional programming language written in scala that facilitates these by allowing a programmer to learn name and manipulate named abstraction over relational data supporting seamless incorporation of trainable probabilistic or discriminative component into the program and providing a level of inference over trainable model to support composition and make decision that respect domain and application constraint saul is developed over a declaratively defined relational data model can use piecewise learned factor graph with declaratively specified learning and inference objective and it support inference over probabilistic model augmented with declarative knowledge based constraint we describe the key construct of saul and exemplify it use in developing application that require relational feature engineering and structured output prediction 
we propose an automatic music generation demo based on artificial neural network which integrates the ability of long short term memory lstm in memorizing and retrieving useful history information together with the advantage of restricted boltzmann machine rbm in high dimensional data modelling our model can generalize to different musical style and generate polyphonic music better than previous model 
given a repeatedly issued query and a document with a not yet confirmed potential to satisfy the user need a search system should place this document on a high position in order to gather user feedback and obtain a more confident estimate of the document utility on the other hand the main objective of the search system is to maximize expected user satisfaction over a rather long period what requires showing more relevant document on average the state of the art approach to solving this exploration exploitation dilemma rely on strongly simplified setting making these approach infeasible in practice we improve the most flexible and pragmatic of them to handle some actual practical issue the first one is utilizing prior information about query and document the second is combining bandit based learning approach with a default production ranking algorithm we show experimentally that our framework enables to significantly improve the ranking of a leading commercial search engine 
many recommenders aim to provide relevant recommendation to user by building personal topic interest profile and then using these profile to find interesting content for the user in social medium recommender system build user profile by directly combining user topic interest signal from a wide variety of consumption and publishing behavior such a social medium post they authored commented on d or liked here we propose to separately model user topical interest that come from these various behavioral signal in order to construct better user profile intuitively since publishing a post requires more effort the topic interest coming from publishing signal should be more accurate of a user s central interest than say a simple gesture such a a by separating a single user s interest profile into several behavioral profile we obtain better and cleaner topic interest signal a well a enabling topic prediction for different type of behavior such a topic that the user might or comment on but might never write a post on that topic to do this at large scale in google we employed matrix factorization technique to model each user s behavior a a separate example entry in the input user by topic matrix using this technique which we call behavioral factorization we implemented and built a topic recommender predicting user s topical interest using their action within google we experimentally showed that we obtained better and cleaner signal than baseline method and are able to more accurately predict topic interest a well a achieve better coverage 
micro task crowdsourcing is rapidly gaining popularity among research community and business a a mean to leverage human computation in their daily operation unlike any other service a crowdsourcing platform is in fact a marketplace subject to human factor that affect it performance both in term of speed and quality indeed such factor shape the dynamic of the crowdsourcing market for example a known behavior of such market is that increasing the reward of a set of task would lead to faster result however it is still unclear how different dimension interact with each other reward task type market competition requester reputation etc in this paper we adopt a data driven approach to a perform a long term analysis of a popular micro task crowdsourcing platform and understand the evolution of it main actor worker requester and platform b we leverage the main finding of our five year log analysis to propose feature used in a predictive model aiming at determining the expected performance of any batch at a specific point in time we show that the number of task left in a batch and how recent the batch is are two key feature of the prediction c finally we conduct an analysis of the demand new task posted by the requester and supply number of task completed by the workforce and show how they affect task price on the marketplace 
researcher have shown that in recent year unwanted web tracking is on the rise with browser based fingerprinting being adopted by more and more website a a viable alternative to third party cooky in this paper we propose privaricator a solution to the problem of browser based fingerprinting a key insight is that when it come to web tracking the real problem with fingerprinting is not uniqueness of a fingerprint it is linkability i e the ability to connect the same fingerprint across multiple visit thus making fingerprint non deterministic also make them hard to link across browsing session in privaricator we use the power of randomization to break linkability by exploring a space of parameterized randomization policy we evaluate our technique in term of being able to prevent fingerprinting and not breaking existing benign site the best of our randomization policy render all the fingerprinters we tested ineffective while causing minimal damage on a set of alexa site on which we tested with no noticeable performance overhead 
complex network phenomenon such a information cascade in online social network are hard to fully observe model and forecast in forecasting a recent trend ha been to forgo the use of parsimonious model in favor of model with increasingly large degree of freedom that are trained to learn the behavior of a process from historical data extrapolating this trend into the future eventually we would renounce model all together but is it possible to forecast the evolution of a complex stochastic process directly from the data without a model in this work we show that model free forecasting is possible we present sed an algorithm that forecast process statistic based on relationship of statistical equivalence using two general axiom and historical data to the best of our knowledge sed is the first method that can perform axiomatic model free forecast of complex stochastic process our simulation using simple and complex evolving process and test performed on a large real world dataset show promising result 
densest subgraph computation ha emerged a an important primitive in a wide range of data analysis task such a community and event detection social medium such a facebook and twitter are highly dynamic with new friendship link and tweet being generated incessantly calling for efficient algorithm that can handle very large and highly dynamic input data while either scalable or dynamic algorithm for finding densest subgraphs have been proposed a viable and satisfactory solution for addressing both the dynamic aspect of the input data and it large size is still missing we study the densest subgraph problem in the the dynamic graph model for which we present the first scalable algorithm with provable guarantee in our model edge are added adversarially while they are removed uniformly at random from the current graph we show that at any point in time we are able to maintain a approximation of a current densest subgraph while requiring o polylog n r amortized cost per update with high probability where r is the total number of update operation executed and n is the maximum number of node in the graph in contrast a naive algorithm that recomputes a dense subgraph every time the graph change requires omega m work per update where m is the number of edge in the current graph our theoretical analysis is complemented with an extensive experimental evaluation on large real world graph showing that approximate densest subgraphs can be maintained efficiently within hundred of microsecond per update 
the proliferation of heterogeneous linked data on the web pose new challenge to database system in particular because of this heterogeneity the capacity to store track and query provenance data is becoming a pivotal feature of modern triple store in this paper we tackle the problem of efficiently executing provenance enabled query over rdf data we propose implement and empirically evaluate five different query execution strategy for rdf query that incorporate knowledge of provenance the evaluation is conducted on web data obtained from two different web crawl the billion triple challenge and the web data common our evaluation show that using an adaptive query materialization execution strategy performs best in our context interestingly we find that because provenance is prevalent within web data and is highly selective it can be used to improve query processing performance this is a counterintuitive result a provenance is often associated with additional overhead 
trending search topic cause unpredictable query load spike that hurt the end user search experience particularly the mobile one by introducing longer delay to understand how trending search topic are formed and evolve over time we analyze million query submitted during period where popular event caused search query volume spike based on our finding we design and evaluate pockettrend a system that automatically detects trending topic in real time identifies the search content associated to the topic and then intelligently push this content to user in a timely manner in that way pockettrend enables a client side search engine that can instantly answer user query related to trending event while at the same time reducing the impact of these trend on the datacenter workload our result using real mobile search log show that in the presence of a trending event up to of the overall search traffic can be eliminated from the datacenter with a many a of all user benefiting from pockettrend 
the frequently changing user preference and or item profile have put essential importance on the dynamic modeling of user and item in personalized recommender system however due to the insufficiency of per user item record when splitting the already sparse data across time dimension previous method have to restrict the drifting purchasing pattern to pre assumed distribution and were hardly able to model them rather directly with for example time series analysis integrating content information help to alleviate the problem in practical system but the domain dependent content knowledge is expensive to obtain due to the large amount of manual effort in this paper we make use of the large volume of textual review for the automatic extraction of domain knowledge namely the explicit feature aspect in a specific product domain we thus degrade the product level modeling of user preference which suffers from the lack of data to the feature level modeling which not only grant u the ability to predict user preference through direct time series analysis but also allows u to know the essence under the surface of product level change in purchasing pattern besides the expanded feature space also help to make cold start recommendation for user with few purchasing record technically we develop the fourier assisted auto regressive integrated moving average farima process to tackle with the year long seasonal period of purchasing data to achieve daily aware preference prediction and we leverage the conditional opportunity model for daily aware personalized recommendation extensive experimental result on real world cosmetic purchasing data from a major e commerce website jd com in china verified both the effectiveness and efficiency of our approach 
downside management is an important topic in the field of recommender system user satisfaction increase when good item are recommended but satisfaction drop significantly when bad recommendation are pushed to them for example a parent would be disappointed if violent movie are recommended to their kid and may stop using the recommendation system entirely a vegetarian would feel steak house recommendation useless a ceo in a mid sized company would feel offended by receiving intern level job recommendation under circumstance where there is penalty for a bad recommendation a bad recommendation is worse than no recommendation at all while most existing work focus on upside management recommending the best item to user this paper emphasizes on achieving better downside management reducing the recommendation of irrelevant or offensive item to user the approach we propose is general and can be applied to any scenario or domain where downside management is key to the system to tackle the problem we design a user latent preference model to predict the user preference in a specific dimension say the dietary restriction of the user the acceptable level of adult content in a movie or the geographical preference of a job seeker we propose to use multinomial regression a the core model and extend it with a hierarchical bayesian framework to address the problem of data sparsity after the user latent preference is predicted we leverage it to filter out downside item we validate the soundness of our approach by evaluating it with an anonymous job application dataset on linkedin the effectiveness of the latent preference model wa demonstrated in both offline experiment and online a b testing the user latent preference model help to improve the vpi view per impression and api application per impression significantly which in turn achieves a higher user satisfaction 
we present a novel method for open domain named entity extraction by exploiting the collective hidden structure in webpage title our method uncovers the hidden textual structure shared by set of webpage title based on generalized url pattern and a multiple sequence alignment technique the highlight of our method include the boundary of entity can be identified automatically in a collective way without any manually designed pattern seed or class name the connection between entity are also discovered naturally based on the hidden structure which make it easy to incorporate distant or weak supervision the experiment show that our method can harvest large scale of open domain entity with high precision a large ratio of the extracted entity are long tailed and complex and cover diverse topic given the extracted entity and their connection we further show the effectiveness of our method in a weakly supervised setting our method can produce better domain specific entity in both precision and recall compared with the state of the art approach 
better access to on line information graphic is a pressing need for people who are blind or have severe vision impairment we present a new model for accessible presentation of on line information graphic and demonstrate it use for presenting floor plan while floor plan are increasingly provided on line people who are blind are at best provided with only a high level textual description this make it difficult for them to understand the spatial arrangement of the object on the floor plan our new approach provides user with significantly better access to such plan the user can automatically generate an accessible version of a floor plan from an on line floor plan image quickly and independently by using a web service this generates a simplified graphic showing the room wall door and window in the original floor plan a well a a textual overview the accessible floor plan is presented on an ipad using audio feedback a the user touch graphic element on the screen the element they are touching is described by speech and non speech audio in order to help them navigate the graphic 
many previous technique identify trending topic in social medium even topic that are not pre defined we present a technique to identify trending rumor which we define a topic that include disputed factual claim putting aside any attempt to ass whether the rumor are true or false it is valuable to identify trending rumor a early a possible it is extremely difficult to accurately classify whether every individual post is or is not making a disputed factual claim we are able to identify trending rumor by recasting the problem a finding entire cluster of post whose topic is a disputed factual claim the key insight is that when there is a rumor even though most post do not raise question about it there may be a few that do if we can find signature text phrase that are used by a few people to express skepticism about factual claim and are rarely used to express anything else we can use those a detector for rumor cluster indeed we have found a few phrase that seem to be used exactly that way including is this true really and what relatively few post related to any particular rumor use any of these enquiry phrase but lot of rumor diffusion process have some post that do and have them quite early in the diffusion we have developed a technique based on searching for the enquiry phrase clustering similar post together and then collecting related post that do not contain these simple phrase we then rank the cluster by their likelihood of really containing a disputed factual claim the detector which search for the very rare but very informative phrase combined with clustering and a classifier on the cluster yield surprisingly good performance on a typical day of twitter about a third of the top cluster were judged to be rumor a high enough precision that human analyst might be willing to sift through them 
we consider the problem of learning distributed representation for document in data stream the document are represented a low dimensional vector and are jointly learned with distributed vector representation of word token using a hierarchical framework with two embedded neural language model in particular we exploit the context of document in stream and use one of the language model to model the document sequence and the other to model word sequence within them the model learn continuous vector representation for both word token and document such that semantically similar document and word are close in a common vector space we discus extension to our model which can be applied to personalized recommendation and social relationship mining by adding further user layer to the hierarchy thus learning user specific vector to represent individual preference we validated the learned representation on a public movie rating data set from movielens a well a on a large scale yahoo news data comprising three month of user activity log collected on yahoo server the result indicate that the proposed model can learn useful representation of both document and word token outperforming the current state of the art by a large margin 
knowledge on the web relies heavily on multi relational representation such a rdf and schema org automatically extracting knowledge from document and linking existing database are common approach to construct multi relational data complementary to such approach there is still a strong demand for manually encoding human expert knowledge for example human annotation is necessary for constructing a common sense knowledge base which store fact implicitly shared in a community because such knowledge rarely appears in document a human annotation is both tedious and costly an important research challenge is how to best use limited human resource while maximizing the quality of the resulting dataset in this paper we formalize the problem of dataset construction a active learning problem and present the active multi relational data construction amdc method amdc repeatedly interleaf multi relational learning and expert input acquisition allowing u to acquire helpful label for data construction experiment on real datasets demonstrate that our solution increase the number of positive triple by a factor of to and that the predictive performance of the multi relational model in amdc achieves the highest or comparable to the best performance throughout the data construction process 
in second price auction with symmetric bidder we find that improved targeting via enhanced information disclosure decrease revenue when there are two bidder and increase revenue if there are at least four bidder with asymmetry improved targeting increase revenue if the most frequent winner win le than of the time but can decrease revenue otherwise we derive analogous result for position auction finally we show that revenue can vary non monotonically with the number of bidder who are able to take advantage of improved targeting 
most semantic web application rely on querying graph typically by using sparql with a triple store increasingly application also analyze property of the graph structure to compute statistical inference the current semantic web infrastructure however doe not efficiently support such operation this force developer to extract the relevant data for external statistical post processing in this paper we propose to rethink query execution in a triple store a a highly parallelized asynchronous graph exploration on an active index data structure this approach also allows to integrate sparql querying with the sampling of graph property to evaluate this architecture we implemented random walk triplerush which is built on a distributed graph processing system our evaluation show that this architecture enables both competitive graph querying a well a the ability to execute various type of random walk with restarts that sample interesting graph property thanks to the asynchronous architecture first result are sometimes returned in a fraction of the full execution time we also evaluate the scalability and show that the architecture support fast query time on a dataset with more than a billion triple 
recognizing entity instance in document according to a knowledge base is a fundamental problem in many data mining application the problem is extremely challenging for short document in complex domain such a social medium and biomedical domain large concept space and instance ambiguity are key issue that need to be addressed most of the document are created in a social context by common author via social interaction such a reply and citation such social context are largely ignored in the instance recognition literature how can user interaction help entity instance recognition how can the social context be modeled so a to resolve the ambiguity of different instance in this paper we propose the socinst model to formalize the problem into a probabilistic model given a set of short document e g tweet or paper abstract posted by user who may connect with each other socinst can automatically construct a context of subtopics for each instance with each subtopic representing one possible meaning of the instance the model is also able to incorporate social relationship between user to help build social context we further incorporate domain knowledge into the model using a dirichlet tree distribution we evaluate the proposed model on three different genre of datasets icdm contest weibo and i b in icdm contest the proposed model clearly outperforms p l e with t test all the top contestant in weibo and i b our result also show that the recognition accuracy of socinst is up to better than those of several alternative method 
the linked data principle provide a decentral approach for publishing structured data in the rdf format on the web in contrast to structured data published in relational database where a key is often provided explicitly finding a set of property that allows identifying a resource uniquely is a non trivial task still finding key is of central importance for manifold application such a resource deduplication link discovery logical data compression and data integration in this paper we address this research gap by specifying a refinement operator dubbed rocker which we prove to be finite proper and non redundant we combine the theoretical characteristic of this operator with two monotonicities of key to obtain a time efficient approach for detecting key i e set of property that describe resource uniquely we then utilize a hash index to compute the discriminability score efficiently therewith we ensure that our approach can scale to very large knowledge base result show that rocker yield more accurate result ha a comparable runtime and consumes le memory w r t existing state of the art technique 
given the abundance of online information available to mobile user particularly tourist and weekend traveler recommender system that effectively filter this information and suggest interesting participatory opportunity will become increasingly important previous work ha explored recommending interesting location however user would also benefit from recommendation for activity in which to participate at those location along with suitable time and day thus system that provide collaborative recommendation involving multiple dimension such a location activity and time would enhance the overall experience of user the relationship among these dimension can be modeled by higher order matrix called tensor which are then solved by tensor factorization however these tensor can be extremely sparse in this paper we present a system and an approach for performing multi dimensional collaborative recommendation for who user what activity when time and where location using tensor factorization on sparse user generated data we formulate an objective function which simultaneously factorizes coupled tensor and matrix constructed from heterogeneous data source we evaluate our system and approach on large scale real world data set consisting of flickr photo collected from three major metro region in usa we compare our approach with several state of the art baseline and demonstrate that it outperforms all of them 
child s online privacy ha garnered much attention in medium legislation and industry adult are concerned that child may not adequately protect themselves online however relatively little discussion ha focused on the privacy breach that may occur to child at the hand of others namely their parent and relative when adult post information online they may reveal personal information about their child to other people online service data broker or surveillant authority this information can be gathered in an automated fashion and then linked with other online and offline source creating detailed profile which can be continually enhanced throughout the child s life in this paper we conduct a study to see how widespread these behavior are among adult on facebook and instagram we use a number of method firstly we automate a process to examine adult user on facebook for evidence of child in their public photo album using the associated comment in combination with publicly available voter registration record we are able to infer child s name face birth date and address secondly in order to understand what additional information is available to facebook and the user friend we survey adult facebook user about their behavior and attitude with regard to posting their child s information online thirdly we analyze user on instagram to infer fact about their child finally we make recommendation for privacy conscious parent and suggest an interface change through which facebook can nudge parent towards better stewardship of their child s privacy 
microblogging platform such a twitter have recently received much attention a great source for live web sensing real time event detection and opinion analysis previous work usually assumed that tweet mainly describe what s happening now however a large portion of tweet contains time expression that refer to time frame within the past or the future such message often reflect expectation or memory of social medium user in this work we investigate how microblogging user collectively refer to time in particular we analyze half a year long portion of japanese and four month long collection of u tweet and we quantify collective temporal attention of user a well a other related temporal characteristic this kind of knowledge is helpful in the context of growing interest for detection and prediction of important event within social medium the exploratory analysis we perform is possible thanks to the development of visual analytics framework for robust overview and easy detection of various regularity in the past and future oriented thinking of twitter user we believe that the visualization we provide and the finding we outline can be also valuable for sociologist and computer scientist to test and refine their model about time in natural language 
recent online service rely heavily on automatic personalization to recommend relevant content to a large number of user this requires system to scale promptly to accommodate the stream of new user visiting the online service for the first time in this work we propose a content based recommendation system to address both the recommendation quality and the system scalability we propose to use a rich feature set to represent user according to their web browsing history and search query we use a deep learning approach to map user and item to a latent space where the similarity between user and their preferred item is maximized we extend the model to jointly learn from feature of item from different domain and user feature by introducing a multi view deep learning model we show how to make this rich feature based user representation scalable by reducing the dimension of the input and the amount of training data the rich user feature representation allows the model to learn relevant user behavior pattern and give useful recommendation for user who do not have any interaction with the service given that they have adequate search and browsing history the combination of different domain into a single model for learning help improve the recommendation quality across all the domain a well a having a more compact and a semantically richer user latent feature vector we experiment with our approach on three real world recommendation system acquired from different source of microsoft product window apps recommendation news recommendation and movie tv recommendation result indicate that our approach is significantly better than the state of the art algorithm up to enhancement on existing user and enhancement on new user in addition experiment on a publicly open data set also indicate the superiority of our method in comparison with transitional generative topic model for modeling cross domain recommender system scalability analysis show that our multi view dnn model can easily scale to encompass million of user and billion of item entry experimental result also confirm that combining feature from all domain produce much better performance than building separate model for each domain 
the proliferation of the web present an unsolved problem of automatically analyzing billion of page of natural language we introduce a scalable algorithm that cluster hundred of million of web page into hundred of thousand of cluster it doe this on a single mid range machine using efficient algorithm and compressed document representation it is applied to two web scale crawl covering ten of terabyte clueweb and clueweb contain and million web page and were clustered into to cluster to the best of our knowledge such fine grained clustering ha not been previously demonstrated previous approach clustered a sample that limit the maximum number of discoverable cluster the proposed em tree algorithm us the entire collection in clustering and produce several order of magnitude more cluster than the existing algorithm fine grained clustering is necessary for meaningful clustering in massive collection where the number of distinct topic grows linearly with collection size these fine grained cluster show an improved cluster quality when assessed with two novel evaluation using ad hoc search relevance judgment and spam classification for external validation these evaluation solve the problem of assessing the quality of cluster where categorical labeling is unavailable and unfeasible 
with broadband penetration rate of le than per caput tribal area in the u s represent some of the most underserved community in term of internet access although numerous source have identified this digital divide there have been no empirical measurement of the performance and usage of service that do exist in these area in this paper we present the characterization of the tribal digital village tdv network a multi hop wireless network currently connecting reservation in san diego county this work represents the first traffic analysis of broadband usage in tribal land after identifying some of the unique purpose of broadband connectivity in indigenous community such a language revitalization and cultural development we focus on the performance of popular application that enable such activity including youtube and instagram though only a fraction of the bandwidth capacity is actually used of youtube uploads and of instagram uploads fail due to packet loss on the relay and access link that connect the reservation to the tdv backbone although failure rate are prohibitive to the contribution of locally generated medium particularly video our analysis of instagram medium interaction and engagement in the tdv network reveals a high locality of interest resident engage with locally created medium time more than medium created by outside source furthermore locally created medium circulates through the network two day longer than non local medium the result of our analysis point to new direction for increasing content availability on reservation 
web scale information network containing billion of entity are common nowadays querying these network can be modeled a a subgraph matching problem since information network are incomplete and noisy in nature it is important to discover answer that match exactly a well a answer that are similar to query existing graph matching algorithm usually use graph index to improve the efficiency of query processing for web scale information network it may not be feasible to build the graph index due to the amount of work and the memory storage required in this paper we propose an efficient algorithm for finding the best k answer for a given query without precomputing graph index the quality of an answer is measured by a matching score that is computed online to speed up query processing we propose a novel technique for bounding the matching score during the computation by using bound we can efficiently prune the answer that have low quality without having to evaluate all possible answer the bounding technique can be implemented in a distributed environment allowing our approach to efficiently answer the query on web scale information network we demonstrate the effectiveness and the efficiency of our approach through a series of experiment on real world information network the result show that our bounding technique can reduce the running time up to two order of magnitude comparing to an approach that doe not use bound 
in this paper we detail our effort at creating and running a controlled study designed to examine how student in a mooc might be motivated to do a better job during peer grading this study involves more than one thousand student of a popular mooc we ask two specific question when a student know that his or her own peer grading effort are being examined by peer doe this knowledge alone tend to motivate the student to do a better job when grading assignment and when a student not only know that his or her own peer grading effort are being examined by peer but he or she is also given a number of other peer grading effort to evaluate so the peer grader see how other peer grader evaluate assignment do both of these together tend to motivate the student to do a better job when grading assignment we find strong statistical evidence that grading the grader doe in fact tend to increase the quality of peer grading 
a common complaint about online auction for consumer good is the presence of sniper who place bid in the final second of sequential ascending auction with predetermined ending time the literature conjecture that sniper are best responding to the existence of incremental bidder that bid up to their valuation only a they are outbid sniper aim to catch these incremental bidder at a price below their reserve with no time to respond a a consequence these incremental bidder may experience regret when they are outbid at the last moment at a price below their reservation value we measure the effect of this experience on a new buyer s propensity to participate in future auction we show the effect to be causal using a carefully selected subset of auction from ebay com and instrumental variable estimation strategy bidder respond to sniping quite strongly and are between and percent le likely to return to the platform 
entity linking connects the web of document with knowledge base it is the task of linking an entity mention in text to it corresponding entity in a knowledge base whereas a large body of work ha been devoted to automatically generating candidate entity or ranking and choosing from them manual effort are still needed e g for defining gold standard link for evaluating automatic approach and for improving the quality of link in crowdsourcing approach however structured description of entity in knowledge base are sometimes very long to avoid overloading human user with too much information and help them more efficiently choose an entity from candidate we aim to substitute entire entity description with compact equally effective structured summary that are automatically generated to achieve it our approach analyzes entity description in the knowledge base and the context of entity mention from multiple perspective including characterizing and differentiating power information overlap and relevance to context extrinsic evaluation where human user carry out entity linking task and intrinsic evaluation where human user rate summary demonstrate that summary generated by our approach help human user carry out entity linking task more efficiently faster without significantly affecting the quality of link obtained and our approach outperforms existing approach to summarizing entity description 
many data processing task such a semantic annotation of image translation of text in foreign language and labeling of training data for machine learning model require human input and on a large scale can only be accurately solved using crowd based online work recent work show that framework where crowd worker compete against each other can drastically reduce crowdsourcing cost and outperform conventional reward scheme where the payment of online worker is proportional to the number of accomplished task pay per task in this paper we investigate how team mechanism can be leveraged to further improve the cost efficiency of crowdsourcing competition to this end we introduce strategy for team based crowdsourcing ranging from team formation process where worker are randomly assigned to competing team over strategy involving self organization where worker actively participate in team building to combination of team and individual competition our large scale experimental evaluation with more than participant and overall hour of work spent by crowd worker demonstrates that our team based crowdsourcing mechanism are well accepted by online worker and lead to substantial performance boost 
many modern desktop and mobile platform including ubuntu google chrome window and firefox o support so called web based system application that run outside the web browser and enjoy direct access to native object such a file camera and geolocation we show that the access control model of these platform are a incompatible and b prone to unintended delegation of native access right when application request native access for their own code they unintentionally enable it for untrusted third party code too this enables malicious ad and other third party content to steal user oauth authentication credential access camera on their device etc we then design implement and evaluate powergate a new access control mechanism for web based system application it solves two key problem plaguing all existing platform security and consistency first unlike the existing platform powergate correctly protects native object from unauthorized access second powergate provides uniform access control semantics across all platform and is backward compatible powergate enables application developer to write well defined native object access policy with explicit principal such a application s own local code and third party web code is easy to configure and incurs negligible performance overhead 
ad hoc keyword search engine built using modern information retrieval method do a good job of handling fine grained query however they perform poorly at facilitating spatial and spatially embedded thematic exploration of the result despite the fact that many query e g civil war refer to different document and topic in different place this is not for lack of data geographic information such a place name event and coordinate are common in unstructured document collection on the web the association between geographic and thematic content in these document can provide a rich groundwork to organize information for exploratory research in this paper we describe the architecture of an interactive thematic map search engine frankenplace designed to facilitate document exploration at the intersection of theme and place the map interface enables a user to zoom the geographic context of their query in and out and quickly explore through thousand of search result in a meaningful way and by combining topic model with geographically contextualized search result user can discover related topic based on geographic context frankenplace utilizes a novel indexing method called geoboost for boosting term associated with cell on a discrete global grid the resulting index factor in the geographic scale of the place or feature mentioned in related text the relative textual scope of the place reference and the overall importance of the containing document in the document network the system is currently indexed with over million document from the web including the english wikipedia and online travel blog entry we demonstrate that frankenplace can support four distinct type of exploratory search task while being adaptive to scale and location of interest 
many of the world s most popular website catalyze their growth through invitation from existing member new member can then in turn issue invitation and so on creating cascade of member signups that can spread on a global scale although these diffu sive invitation process are critical to the popularity and growth of many website they have rarely been studied and their property remain elusive for instance it is not known how viral these cascade structure are how cascade grow over time or how diffusive growth affect the resulting distribution of member characteristic present on the site in this paper we study the diffusion of linkedin an online professional network comprising over million member a large fraction of whom joined the site a part of a signup cascade first we analyze the structural pattern of these signup cascade and find them to be qualitatively different from previously studied information diffusion cascade we also examine how signup cascade grow over time and observe that diffusion via invitation on linkedin occurs over much longer timescales than are typically associated with other type of online diffusion finally we connect the cascade structure with rich individual level attribute data to investigate the interplay between the two using novel technique to study the role of homophily in diffusion we find striking difference between the local edge wise homophily and the global cascade level homophily we observe in our data suggesting that signup cascade form surprisingly coherent group of member 
advertising is a significant source of revenue for most online social network conventional online advertising method need to be customized for online social network in order to address their distinct characteristic recent experimental study have shown that providing social cue along with ad e g information about friend liking the ad or clicking on an ad lead to higher click rate in other word the probability of a user clicking an ad is a function of the set of friend that have clicked the ad in this work we propose formal probabilistic model to capture this phenomenon and study the algorithmic problem that then arises our work is in the context of display advertising where a contract is signed to show an ad to a pre determined number of user the problem we study is the following given a certain number of impression what is the optimal display strategy i e the optimal order and the subset of user to show the ad to so a to maximize the expected number of click unlike previous model of influence maximization we show that this optimization problem is hard to approximate in general and that it is related to finding dense subgraphs of a given size in light of the hardness result we propose several heuristic algorithm including a two stage algorithm inspired by influence and exploit strategy in viral marketing we evaluate the performance of these heuristic on real data set and observe that our two stage heuristic significantly outperforms the natural baseline 
the advent of mobile device and medium cloud service ha led to the unprecedented growing of personal photo collection one of the fundamental problem in managing the increasing number of photo is automatic image tagging existing research ha predominantly focused on tagging general web image with a well labelled image database e g imagenet however they can only achieve limited success on personal photo due to the domain gap between personal photo and web image these gap originate from the difference in semantic distribution and visual appearance to deal with these challenge in this paper we present a novel transfer deep learning approach to tag personal photo specifically to solve the semantic distribution gap we have designed an ontology consisting of a hierarchical vocabulary tailored for personal photo this ontology is mined from active user in flickr with million photo and million unique tag to deal with the visual appearance gap we discover the intermediate image representation and ontology prior by deep learning with bottom up and top down transfer across two domain where web image are the source domain and personal photo are the target moreover we present two mode single and batch mode in tagging and find that the batch mode is highly effective to tag photo collection we conducted personal photo tagging on real personal photo and personal photo search on the mit adobe fivek photo dataset the proposed tagging approach is able to achieve a performance gain of and in term of ndcg against the state of the art hand crafted feature based and deep learning based method respectively 
the web browser is a killer app on mobile device such a smartphones however the user experience of mobile web browsing is undesirable because of the slow resource loading to improve the performance of web resource loading caching ha been adopted a a key mechanism however the existing passive measurement study cannot comprehensively characterize the performance of mobile web caching for example most of these study mainly focus on client side implementation but not server side configuration suffer from biased user behavior and fail to study miscached resource to address these issue in this paper we present a proactive approach for a comprehensive measurement study on mobile web cache performance the key idea of our approach is to proactively crawl resource from hundred of website periodically with a fine grained time interval thus we are able to uncover the resource update history and cache configuration at the server side and analyze the cache performance in various time granularity based on our collected data we build a new cache analysis model and study the upper bound of how high percentage of resource could potentially be cached and how effective the caching work in practice we report detailed analysis result of different website and various type of web resource and identify the problem caused by unsatisfactory cache performance in particular we identify two major problem redundant transfer and miscached resource which lead to unsatisfactory cache performance we investigate three main root cause same content heuristic expiration and conservative expiration time and discus what mobile web developer can do to mitigate those problem 
opinion spamming refers to the illegal marketing practice which involves delivering commercially advantageous opinion a regular user in this paper we conduct a real case study based on a set of internal record of opinion spam leaked from a shady marketing campaign we explore the characteristic of opinion spam and spammer in a web forum to obtain some insight including subtlety property of opinion spam spam post ratio spammer account first post and reply submission time of post activeness of thread and collusion among spammer then we present feature that could be potentially helpful in detecting spam opinion in thread the result of spam detection on first post show spam first post put more focus on certain topic such a the user experience on the promoted item spam first post generally use more word and picture to showcase the promoted item in an attempt to impress people spam first post tend to be submitted during work time and the thread that spam first post initiate are more active to be placed at striking position the spam detection on reply is more challenging besides lower spam ratio and le content reply even do not mention the promoted item their major intention is to keep the discussion in a thread alive to attract more attention on it submission time of reply thread activeness position of reply and spamicity of first post are more useful than content based feature in spam detection on reply 
implicit feedback from user of a web search engine is an essential source providing consistent personal relevance label from the actual population of user however previous study on personalized search employ this source in a rather straightforward manner basically document that were clicked on get maximal gain and the rest of the document are assigned the zero gain a we demonstrate in our paper a ranking algorithm trained using these gain directly a the ground truth relevance label lead to a suboptimal personalized ranking in this paper we develop a framework for automatic reweighting of these label our approach is based on more subtle aspect of user interaction with the result page we propose an efficient methodology for deriving confidence level for relevance label that relies directly on the objective ranking measure all our algorithm are evaluated on a large scale query log provided by a major commercial search engine the result of the experiment prove that the current state of the art personalization approach could be significantly improved by enriching relevance grade with weight extracted from post impression user behavior 
we study a natural generalization of the correlation clustering problem to graph in which the pairwise relation between object are categorical instead of binary this problem wa recently introduced by bonchi et al under the name of chromatic correlation clustering and is motivated by many real world application in data mining and social network including community detection link classification and entity de duplication our main contribution is a fast and easy to implement constant approximation framework for the problem which build on a novel reduction of the problem to that of correlation clustering this result significantly progress the current state of knowledge for the problem improving on a previous result that only guaranteed linear approximation in the input size we complement the above result by developing a linear programming based algorithm that achieves an improved approximation ratio of although this algorithm cannot be considered to be practical it further extends our theoretical understanding of chromatic correlation clustering we also present a fast heuristic algorithm that is motivated by real life scenario in which there is a ground truth clustering that is obscured by noisy observation we test our algorithm on both synthetic and real datasets like social network data our experiment reinforce the theoretical finding by demonstrating that our algorithm generally outperform previous approach both in term of solution cost and reconstruction of an underlying ground truth clustering 
numerous graph mining application rely on detecting subgraphs which are large near clique since formulation that are geared towards finding large near clique are hard and frequently inapproximable due to connection with the maximum clique problem the poly time solvable densest subgraph problem which maximizes the average degree over all possible subgraphs lie at the core of large scale data mining however frequently the densest subgraph problem fails in detecting large near clique in network in this work we introduce the k clique densest subgraph problem k this generalizes the well studied densest subgraph problem which is obtained a a special case for k for k we obtain a novel formulation which we refer to a the triangle densest subgraph problem given a graph g v e find a subset of vertex s such that s max limit v t s s where t s is the number of triangle induced by the set s on the theory side we prove that for any k constant there exist an exact polynomial time algorithm for the k clique densest subgraph problem furthermore we propose an efficient k approximation algorithm which generalizes the greedy peeling algorithm of asahiro and charikar for k finally we show how to implement efficiently this peeling framework on mapreduce for any k generalizing the work of bahmani kumar and vassilvitskii for the case k on the empirical side our two main finding are that i the triangle densest subgraph is consistently closer to being a large near clique compared to the densest subgraph and ii the peeling approximation algorithm for both k and k achieve on real world network approximation ratio closer to rather than the pessimistic k guarantee an interesting consequence of our work is that triangle counting a well studied computational problem in the context of social network analysis can be used to detect large near clique finally we evaluate our proposed method on a popular graph mining application 
we motivate and describe technique that allow to detect an emergent relational schema from rdf data we show that on a wide variety of datasets the found structure explains well over of the rdf triple further we also describe technical solution to the semantic challenge to give short name that human find logical to these emergent table column and relationship between table our technique can be exploited in many way e g to improve the efficiency of sparql system or to use existing sql based application on top of any rdf dataset using a rdbms 
storing and searching large labeled graph is indeed becoming a key issue in the design of space time efficient online platform indexing modern social network or knowledge graph but a far a we know all these result are limited to design compressed graph index which support basic access operation onto the link structure of the input graph such a given a node u return the adjacency list of u this paper take inspiration from the facebook unicorn s platform and proposes some compressed indexing scheme for large graph whose node are labeled with string of variable length i e node s attribute such a user s nick name that support sophisticated search operation which involve both the linked structure of the graph and the string content of it node an extensive experimental evaluation over real social network will show the time and space efficiency of the proposed indexing scheme and their query processing algorithm 
we study the ability of a passive eavesdropper to leverage third party http tracking cooky for mass surveillance if two web page embed the same tracker which tag the browser with a unique cookie then the adversary can link visit to those page from the same user i e browser instance even if the user s ip address varies further many popular website leak a logged in user s identity to an eavesdropper in unencrypted traffic to evaluate the effectiveness of our attack we introduce a methodology that combine web measurement and network measurement using openwpm our web privacy measurement platform we simulate user browsing the web and find that the adversary can reconstruct of a typical user s browsing history we then analyze the effect of the physical location of the wiretap a well a legal restriction such a the nsa s one end foreign rule using measurement unit in various location asia europe and the united state we show that foreign user are highly vulnerable to the nsa s dragnet surveillance due to the concentration of third party tracker in the u s finally we find that some browser based privacy tool mitigate the attack while others are largely ineffective 
we investigate current deployment practice for virtual hosting a widely used method for serving multiple http and http origin from the same server in popular content delivery network cloud hosting infrastructure and web server our study uncovers a new class of http origin confusion attack when two virtual host use the same tl certificate or share a tl session cache or ticket encryption key a network attacker may cause a page from one of them to be loaded under the other s origin in a client browser these attack appear when http server are configured to allow virtual host fallback from a client requested secure origin to some other unexpected le secure origin we present evidence that such vulnerable virtual host configuration are widespread even on the most popular and security scrutinized website thus allowing a network adversary to hijack page or steal secure cooky and single sign on token to prevent our virtual host confusion attack and recover the isolation guarantee that are commonly assumed in shared hosting environment we propose fix to web server software and advocate conservative configuration guideline for the composition of http with tl 
wifi based indoor positioning ha recently gained more attention due to the advent of the ieee v standard requirement by the fcc for e call and increased interest in location based service while there exist several indoor localization technique we find that these technique tradeoff either accuracy scalability pervasiveness or cost all of which are important requirement for a truly deployable positioning solution wireless signal strength based approach suffer from location error whereas time of flight tof based solution provide good accuracy but are not scalable recent solution address these issue by augmenting wifi with either smartphone sensing or mobile crowdsourcing however they require tight coupling between wifi infrastructure and a client device or they can determine the client s location only if it is mobile in this paper we present cupid which improved our previously proposed cupid indoor positioning system to overcome these limitation we achieve this by addressing the fundamental limitation in time of flight based localization and combining tof with signal strength to address scalability experiment from city using different mobile device comprising of more than million location fix demonstrate feasibility cupid is currently under production and we expect cupid to ignite the wide adoption of wlan based positioning system and their service 
most recent question answering qa system query large scale knowledge base kb to answer a question after parsing and transforming natural language question to kb executable form e g logical form a a well known fact kb are far from complete so that information required to answer question may not always exist in kb in this paper we develop a new qa system that mine answer directly from the web and meanwhile employ kb a a significant auxiliary to further boost the qa performance specifically to the best of our knowledge we make the first attempt to link answer candidate to entity in freebase during answer candidate generation several remarkable advantage follow redundancy among answer candidate is automatically reduced the type of an answer candidate can be effortlessly determined by those of it corresponding entity in freebase capitalizing on the rich information about entity in freebase we can develop semantic feature for each answer candidate after linking them to freebase particularly we construct answer type related feature with two novel probabilistic model which directly evaluate the appropriateness of an answer candidate s type under a given question overall such semantic feature turn out to play significant role in determining the true answer from the large answer candidate pool the experimental result show that across two testing datasets our qa system achieves an improvement under f metric compared with various existing qa system 
modern internet company improve their service by mean of data driven decision that are based on online controlled experiment also known a a b test to run more online controlled experiment and to get statistically significant result faster are the emerging need for these company the main way to achieve these goal is to improve the sensitivity of a b experiment we propose a novel approach to improve the sensitivity of user engagement metric that are widely used in a b test by utilizing prediction of the future behavior of an individual user this problem of prediction of the exact value of a user engagement metric is also novel and is studied in our work we demonstrate the effectiveness of our sensitivity improvement approach on several real online experiment run at yandex especially we show how it can be used to detect the treatment effect of an a b test faster with the same level of statistical significance 
query issued to a search engine are often under specified or ambiguous the user s search context or background may provide information that disambiguates their information need in order to automatically predict and issue a more effective query the disambiguation can take place at different stage of the retrieval process for instance contextual query suggestion may be computed and recommended to user on the result page when appropriate an approach that doe not require modifying the original query s result alternatively the search engine can attempt to provide efficient access to new relevant document by injecting these document directly into search result based on the user s context in this paper we explore these complementary approach and how they might be combined we first develop a general framework for mining context sensitive query reformulations for query suggestion we evaluate our context sensitive suggestion against a state of the art baseline using a click based metric the resulting query suggestion generated by our approach outperform the baseline by overall and by on an ambiguous query subset while the query suggestion generated by our approach have higher quality than the existing baseline we demonstrate that using them naively for injecting new document into search result can lead to inferior ranking to remedy this issue we develop a classifier that decides when to inject new search result using feature based on suggestion quality and user context we show that our context sensitive result fusion approach corfu improves retrieval quality for ambiguous query by up to our approach can efficiently scale to massive search log enabling a data driven strategy that benefit from observing how user issue and reformulate query in different context 
the spanning centrality of an edge e in an undirected graph g is the fraction of the spanning tree of g that contain e despite it appealing definition and apparent value in certain application in computational biology spanning centrality hasn t so far received a wider attention a a measure of edge centrality we may partially attribute this to the perceived complexity of computing it which appears to be prohibitive for very large network contrary to this intuition spanning centrality can in fact be approximated arbitrary well by very efficient near linear time algorithm due to spielman and srivastava combined with progress in linear system solver in this article we bring theory into practice with careful and optimized implementation that allow the fast computation of spanning centrality in very large graph with million of node with this computational tool in our disposition we demonstrate experimentally that spanning centrality is in fact a useful tool for the analysis of large network specifically we show that relative to common centrality measure spanning centrality is more effective in identifying edge whose removal cause a higher disruption in an information propagation procedure while being very resilient to noise in term of both the edge score and the resulting edge ranking 
given a large collection of co evolving online activity such a search for the keywords xbox playstation and wii how can we find pattern and rule are these keywords related if so are they competing against each other can we forecast the volume of user activity for the coming month we conjecture that online activity compete for user attention in the same way that specie in an ecosystem compete for food we present ecoweb i e ecosystem on the web which is an intuitive model designed a a non linear dynamical system for mining large scale co evolving online activity our second contribution is a novel parameter free and scalable fitting algorithm ecoweb fit that estimate the parameter of ecoweb extensive experiment on real data show that ecoweb is effective in that it can capture long range dynamic and meaningful pattern such a seasonalities and practical in that it can provide accurate long range forecast ecoweb consistently outperforms existing method in term of both accuracy and execution speed 
semantic tagging of mathematical expression stme give semantic meaning to token in mathematical expression in this work we propose a novel stme approach that relies on neither text along with expression nor labelled training data instead our method only requires a mathematical grammar set we point out that besides the grammar of mathematics the special property of variable and user habit of writing expression help u understand the implicit intent of the user we build a system that considers both restriction from the grammar and variable property and then apply an unsupervised method to our probabilistic model to learn the user habit to evaluate our system we build large scale training and test datasets automatically from a public math forum the result demonstrate the significant improvement of our method compared to the maximum frequency baseline we also create statistic to reveal the property of mathematics language 
the goal of collaborative filtering is to get accurate recommendation at the top of the list for a set of user from such a perspective collaborative ranking based formulation with suitable ranking loss function are natural while recent literature ha explored the idea based on objective function such a ndcg or average precision such objective are difficult to optimize directly in this paper building on recent advance from the learning to rank literature we introduce a novel family of collaborative ranking algorithm which focus on accuracy at the top of the list for each user while learning the ranking function collaboratively we consider three specific formulation based on collaborative p norm push infinite push and reverse height push and propose efficient optimization method for learning these model experimental result illustrate the value of collaborative ranking and show that the proposed method are competitive usually better than existing popular approach to personalized recommendation 
twitter contains a wealth of timely information however staying on top of breaking event requires that an information analyst constantly scan many source leading to information overload for example a user might wish to be made aware whenever an infectious disease outbreak take place when a new smartphone is announced or when a distributed denial of service do attack might affect an organization s network connectivity there are many possible event category an analyst may wish to track making it impossible to anticipate all those of interest in advance we therefore propose a weakly supervised approach in which extractor for new category of event are easy to define and train by specifying a small number of seed example we cast seed based event extraction a a learning problem where only positive and unlabeled data is available rather than assuming unlabeled instance are negative a is common in previous work we propose a learning objective which regularizes the label distribution towards a user provided expectation our approach greatly outperforms heuristic negative used in most previous work in experiment on real world data significant performance gain are also demonstrated over two novel and competitive baseline semi supervised em and one class support vector machine we investigate three security related event breaking on twitter do attack data breach and account hijacking a demonstration of security event extracted by our system is available at http kb cse ohio state edu event hacked 
minwise hashing minhash is a widely popular indexing scheme in practice minhash is designed for estimating set resemblance and is known to be suboptimal in many application where the desired measure is set overlap i e inner product between binary vector or set containment minhash ha inherent bias towards smaller set which adversely affect it performance in application where such a penalization is not desirable in this paper we propose asymmetric minwise hashing em mh alsh to provide a solution to this well known problem the new scheme utilizes asymmetric transformation to cancel the bias of traditional minhash towards smaller set making the final collision probability monotonic in the inner product our theoretical comparison show that for the task of retrieving with binary inner product asymmetric minhash is provably better than traditional minhash and other recently proposed hashing algorithm for general inner product thus we obtain an algorithmic improvement over existing approach in the literature experimental evaluation on four publicly available high dimensional datasets validate our claim the proposed scheme outperforms often significantly other hashing algorithm on the task of near neighbor retrieval with set containment our proposal is simple and easy to implement in practice 
online review on product and service can be very useful for customer but they need to be protected from manipulation so far most study have focused on analyzing online review from a single hosting site how could one leverage information from multiple review hosting site this is the key question in our work in response we develop a systematic methodology to merge compare and evaluate review from multiple hosting site we focus on hotel review and use more than million review from more than million user spanning three prominent travel site our work consists of three thrust a we develop novel feature capable of identifying cross site discrepancy effectively b we conduct arguably the first extensive study of cross site variation using real data and develop a hotel identity matching method with accuracy c we introduce the trueview score a a proof of concept that cross site analysis can better inform the end user our result show that we detect time more suspicious hotel by using multiple site compared to using the three site in isolation and we find that of all hotel appearing in all three site seem to have low trustworthiness score our work is an early effort that explores the advantage and the challenge in using multiple reviewing site towards more informed decision making 
the rise of crowdsourcing brings new type of malpractice in internet advertising one can easily hire web worker through malicious crowdsourcing platform to attack other advertiser such human generated crowd fraud are hard to detect by conventional fraud detection method in this paper we carefully examine the characteristic of the group behavior of crowd fraud and identify three persistent pattern which are moderateness synchronicity and dispersivity then we propose an effective crowd fraud detection method for search engine advertising based on these pattern which consists of a constructing stage a clustering stage and a filtering stage at the constructing stage we remove irrelevant data and reorganize the click log into a surfer advertiser inverted list at the clustering stage we define the sync similarity between surfer click history and transform the coalition detection to a clustering problem solved by a nonparametric algorithm and finally we build a dispersity filter to remove false alarm cluster the nonparametric nature of our method ensures that we can find an unbounded number of coalition with nearly no human interaction we also provide a parallel solution to make the method scalable to web data and conduct extensive experiment the empirical result demonstrate that our method is accurate and scalable 
most of today s mobile device come equipped with both cellular lte and wifi wireless radio making radio bundling simultaneous data transfer over multiple interface both appealing and practical despite recent study documenting the benefit of radio bundling with mptcp many fundamental question remain about potential gain from radio bundling or the relationship between performance and energy consumption in these scenario in this study we seek to answer these question using extensive measurement to empirically characterize both energy and performance for radio bundling approach in doing so we quantify potential gain of bundling using mptcp versus an ideal protocol we study the link between traffic partitioning and bundling performance and use a novel componentized energy model to quantify the energy consumed by cpu and radio during traffic management our result show that mptcp achieves only a fraction of the total performance gain possible and that it energy agnostic design lead to considerable power consumption by the cpu we conclude that not only there is room for improved bundling performance but an energy aware bundling protocol is likely to achieve a much better tradeoff between performance and power consumption 
crowdsourcing via paid microtasks ha been successfully applied in a plethora of domain and task previous effort for making such crowdsourcing more effective have considered aspect a diverse a task and workflow design spam detection quality control and pricing model our work expands upon such effort by examining the potential of adding gamification to microtask interface a a mean of improving both worker engagement and effectiveness we run a series of experiment in image labeling one of the most common use case for microtask crowdsourcing and analyse worker behavior in term of number of image completed quality of annotation compared against a gold standard and response to financial and game specific reward each experiment study these parameter in two setting one based on a state of the art non gamified task on crowdflower and another one using an alternative interface incorporating several game element our finding show that gamification lead to better accuracy and lower cost than conventional approach that use only monetary incentive in addition it seems to make paid microtask work more rewarding and engaging especially when sociality feature are introduced following these initial insight we define a predictive model for estimating the most appropriate incentive for individual worker based on their previous contribution this allows u to build a personalised game experience with gain seen on the volume and quality of work completed 
a large number of extension exist in browser vendor online store for million of user to download and use many of those extension process sensitive information from user input and webpage however it remains a big question whether those extension may accidentally leak such sensitive information out of the browser without protection in this paper we present a framework lvdetector that combine static and dynamic program analysis technique for automatic detection of information leakage vulnerability in legitimate browser extension extension developer can use lvdetector to locate and fix the vulnerability in their code browser vendor can use lvdetector to decide whether the corresponding extension can be hosted in their online store advanced user can also use lvdetector to determine if certain extension are safe to use the design of lvdetector is not bound to specific browser or javascript engine and can adopt other program analysis technique we implemented lvdetector and evaluated it on popular firefox and google chrome extension lvdetector identified previously unknown information leakage vulnerability in extension with a accuracy rate the evaluation result and the feedback to our responsible disclosure demonstrate that lvdetector is useful and effective 
increased popularity of smartphones ha attracted a large number of developer to various smartphone platform a a result app market are also populated with spam apps which reduce the user quality of experience and increase the workload of app market operator apps can be spammy in multiple way including not having a specific functionality unrelated app description or unrelated keywords and publishing similar apps several time and across diverse category market operator maintain anti spam policy and apps are removed through continuous human intervention through a systematic crawl of a popular app market and by identifying a set of removed apps we propose a method to detect spam apps solely using app metadata available at the time of publication we first propose a methodology to manually label a sample of removed apps according to a set of checkpoint heuristic that reveal the reason behind removal this analysis suggests that approximately of the apps being removed are very likely to be spam apps we then map the identified heuristic to several quantifiable feature and show how distinguishing these feature are for spam apps finally we build an adaptive boost classifier for early identification of spam apps using only the metadata of the apps our classifier achieves an accuracy over with precision varying between and recall varying between by applying the classifier on a set of apps present at the app market during our crawl we estimate that at least of them are spam apps 
a well known phenomenon in social network is homophily the tendency of agent to connect with similar agent a derivative of this phenomenon is the emergence of community another phenomenon observed in numerous network is the existence of certain agent that belong simultaneously to multiple community an understanding of these phenomenon constitutes a central research topic of network science in this work we focus on a fundamental theoretical question related to the above phenomenon with various application given an undirected graph g can we infer efficiently the latent vertex feature which explain the observed network structure under the assumption of a generative model that exhibit homophily we propose a probabilistic generative model with the property that the probability of an edge among two vertex is a non decreasing function of the common feature they posse this property is true for many real world network and surprisingly is ignored by many popular overlapping community detection method a it wa shown recently by the empirical work of yang and leskovec our main theoretical contribution is the first provably rapidly mixing markov chain for inferring latent feature on the experimental side we verify the efficiency of our method in term of run time where we observe that it significantly outperforms state of the art method our method is more than time faster than a state of the art machine learning method and typically provides non trivial speedup compared to bigclam furthermore we verify on real data with ground truth available that our method learns efficiently high quality labelings we use our method to learn social circle from twitter ego network and perform multilabel classification 
we are living in a world of big sensor data due to the widespread prevalence of visual sensor e g surveillance camera and social sensor e g twitter feed many event are implicitly captured in real time by such heterogeneous sensor combining these two complementary sensor stream can significantly improve the task of event detection and aid in comprehending evolving situation however the different characteristic of these social and sensor data make such information fusion for event detection a challenging problem to tackle this problem we propose an innovative multi layer tweeting camera framework integrating both physical sensor and social sensor to detect various concept of real world event in this framework visual concept detector are applied on camera video frame and these concept can be construed a camera tweet posted regularly these tweet are represented by a unified probabilistic spatio temporal pst data structure which is then aggregated to a concept based image cmage a the common representation for visualization to facilitate event analysis we define a set of operator and analytic function that can be applied on the pst data by the user to discover occurrence of event and to analyse evolving situation we further leverage on geo located social medium data by mining current topic discussed on twitter to obtain the high level semantic meaning of detected event in image we quantitatively evaluate our framework with a large scale dataset containing image from new york real time traffic cctv camera university foodcourt camera feed and twitter data which demonstrates the feasibility and effectiveness of the proposed framework result of combining camera tweet and social tweet are shown to be promising for detecting real world event 
in this paper we address the problem of estimating the index size needed by web search engine to answer a many query a possible by exploiting the marked difference between query and click frequency we provide a possible formal definition for the notion of essential web page a those that cover a large fraction of distinct query i e we look at the problem a a version of maxcover although in general maxcover is approximable to within a factor of e from the optimum we provide a condition under which the greedy algorithm doe find the actual best cover or remains at a known bounded factor from it the extra check for optimality or for bounding the ratio from the optimum come at a negligible algorithmic cost moreover in most practical instance of this problem the algorithm is able to provide solution that are provably optimal or close to optimal we relate this observed phenomenon to some property of the query click graph our experimental result confirm that a small number of web page can respond to a large fraction of the query e g of the page answer of the query our approach can be used in several related search application and ha in fact an even more general appeal a a first example our preliminary experimental study confirms that our algorithm ha extremely good performance on other social network based maxcover instance 
understanding the correlation between two different score for the same set of item is a common problem in graph analysis and information retrieval the most commonly used statistic that quantifies this correlation is kendall s tau however the standard definition fails to capture that discordance between item with high rank are more important than those between item with low rank recently a new measure of correlation based on average precision ha been proposed to solve this problem but like many alternative proposal in the literature it assumes that there are no tie in the score this is a major deficiency in a number of context and in particular when comparing centrality score on large graph a the obvious baseline indegree ha a very large number of tie in social network and web graph we propose to extend kendall s definition in a natural way to take into account weight in the presence of tie we prove a number of interesting mathematical property of our generalization and describe an o n log n algorithm for it computation we also validate the usefulness of our weighted measure of correlation using experimental data on social network and web graph 
we study the following problem given the name of an ad hoc concept a well a a few seed entity belonging to the concept output all entity belonging to it since producing the exact set of entity is hard we focus on returning a ranked list of entity previous approach either use seed entity a the only input or inherently require negative example they suffer from input ambiguity and semantic drift or are not viable option for ad hoc tail concept in this paper we propose to leverage the million of table on the web for this problem the core technical challenge is to identify the exclusive table for a concept to prevent semantic drift existing holistic ranking technique like personalized pagerank are inadequate for this purpose we develop novel probabilistic ranking method that can model a new type of table entity relationship experiment with real life concept show that our proposed solution is significantly more effective than applying state of the art set expansion or holistic ranking technique 
malvertising is a malicious activity that leverage advertising to distribute various form of malware because advertising is the key revenue generator for numerous internet company large ad network such a google yahoo and microsoft invest a lot of effort to mitigate malicious ad from their ad network this drive adversary to look for alternative method to deploy malvertising in this paper we show that browser extension that use ad a their monetization strategy often facilitate the deployment of malvertising moreover while some extension simply serve ad from ad network that support malvertising other extension maliciously alter the content of visited webpage to force user into installing malware to measure the extent of these behavior we developed expector a system that automatically inspects and identifies browser extension that inject ad and then classifies these ad a malicious or benign based on their landing page using expector we automatically inspected over chrome browser extension we found extension that inject ad and detected extension that participate in malvertising using different ad network and with a total user base of 
in this paper we propose and evaluate a scheme to produce canonical label for blank node in rdf graph these label can be used a the basis for a skolemisation scheme that get rid of the blank node in an rdf graph by mapping them to globally canonical iris assuming no hash collision the scheme guarantee that two skolemised graph will be equal if and only if the two input graph are isomorphic although the proposed scheme is exponential in the worst case we claim that such case are unlikely to be encountered in practice to support these claim we present the result of applying our skolemisation scheme over a diverse collection of million real world rdf graph btc we also provide result for some nasty synthetic case 
latent dirichlet allocation lda is a widely used probabilistic topic modeling tool for content analysis such a web mining to handle web scale content analysis on just a single pc we propose multi core parallel expectation maximization pem algorithm to infer and estimate lda parameter in shared memory system by avoiding memory access conflict reducing the locking time among multiple thread and residual based dynamic scheduling we show that pem algorithm are more scalable and accurate than the current state of the art parallel lda algorithm on a commodity pc this parallel lda toolbox is made publicly available a open source software at mloss org 
web page consist of not only actual content but also other element such a branding banner navigational element advertisement copyright etc this noisy content is typically not related to the main subject of the webpage identifying the part of actual content or clipping web page ha many application such a high quality web printing e reading on mobile device and data mining although there are many existing method attempting to address this task most of them can either work only on certain type of web page e g article page or ha to develop different model for different website we formulate the actual content identifying problem a a dom tree node selection problem we develop multiple feature by utilizing the dom tree node property to train a machine learning model then candidate node are selected based on the learning model based on the observation that the actual content is usually located in a spatially continuous block we develop a grouping technology to further filter out noisy data and pick missing data for the candidate node we conduct extensive experiment on a real dataset and demonstrate our solution ha high quality output and outperforms several baseline method 
a b testing also known a bucket testing split testing or controlled experiment is a standard way to evaluate user engagement or satisfaction from a new service feature or product it is widely used in online website including social network site such a facebook linkedin and twitter to make data driven decision the goal of a b testing is to estimate the treatment effect of a new change which becomes intricate when user are interacting i e the treatment effect of a user may spill over to other user via underlying social connection when conducting these online controlled experiment it is a common practice to make the stable unit treatment value assumption sutva that each individual s response is affected by their own treatment only though this assumption simplifies the estimation of treatment effect it doe not hold when network interference is present and may even lead to wrong conclusion in this paper we study the problem of network a b testing in real network which have substantially different characteristic from the simulated random network studied in previous work we first examine the existence of network effect in a recent online experiment conducted at linkedin secondly we propose an efficient and effective estimator for average treatment effect ate considering the interference between user in real online experiment finally we apply our method in both simulation and a real world online experiment the simulation result show that our estimator achieves better performance with respect to both bias and variance reduction the real world online experiment not only demonstrates that large scale network a b test is feasible but also further validates many of our observation in the simulation study 
in online market a store s reputation is closely tied to it profitability seller desire to quickly achieve high reputation ha fueled a profitable underground business which operates a a specialized crowdsourcing marketplace and accumulates wealth by allowing online seller to harness human laborer to conduct fake transaction for improving their store reputation we term such an underground market a seller reputation escalation sre market in this paper we investigate the impact of the sre service on reputation escalation by performing in depth measurement of the prevalence of the sre service the business model and market size of sre market and the characteristic of seller and offered laborer to this end we have infiltrated five sre market and studied their operation using daily data collection over a continuous period of two month we identified more than online seller posting at least fake purchase task on the five sre market these transaction earned at least in revenue for the five sre market and the total value of merchandise involved exceeded our study demonstrates that online seller using sre service can increase their store reputation at least time faster than legitimate one while only of them were detected and penalized even worse we found a newly launched service that can within a single day boost a seller s reputation by such a degree that would require a legitimate seller at least a year to accomplish finally armed with our analysis of the operational characteristic of the underground economy we offer some insight into potential mitigation strategy 
when user interact with the web today they leave sequential digital trail on a massive scale example of such human trail include web navigation sequence of online restaurant review or online music play list understanding the factor that drive the production of these trail can be useful for e g improving underlying network structure predicting user click or enhancing recommendation in this work we present a general approach called hyptrails for comparing a set of hypothesis about human trail on the web where hypothesis represent belief about transition between state our approach utilizes markov chain model with bayesian inference the main idea is to incorporate hypothesis a informative dirichlet prior and to leverage the sensitivity of bayes factor on the prior for comparing hypothesis with each other for eliciting dirichlet prior from hypothesis we present an adaption of the so called trial roulette method we demonstrate the general mechanic and applicability of hyptrails by performing experiment with i synthetic trail for which we control the mechanism that have produced them and ii empirical trail stemming from different domain including website navigation business review and online music played our work expands the repertoire of method available for studying human trail on the web 
the availability of an increasing amount of user generated data is transformative to our society we enjoy the benefit of analyzing big data for public interest such a disease outbreak detection and traffic control a well a for commercial interest such a smart grid and product recommendation however the large collection of user generated data contains unique pattern and can be used to re identify individual which ha been exemplified by the aol search log release incident in this paper we propose a practical framework for data analytics while providing differential privacy guarantee to individual data contributor our framework generates differentially private aggregate which can be used to perform data mining and recommendation task to alleviate the high perturbation error introduced by the differential privacy mechanism we present two method with different sampling technique to draw a subset of individual data for analysis empirical study with real world data set show that our solution enable accurate data analytics on a small fraction of the input data reducing user privacy risk and data storage requirement without compromising the analysis result 
password continue to dominate the authentication landscape in spite of numerous proposal to replace them even though usability is a key factor in replacing password very few alternative have been subjected to formal usability study and even fewer have been analyzed using a standard metric we report the result of four within subject usability study for seven web authentication system these system span federated smartphone paper token and email based approach our result indicate that participant prefer single sign on system we report several insightful finding based on participant qualitative response transparency increase usability but also lead to confusion and a lack of trust participant prefer single sign on but wish to augment it with site specific low entropy password and participant are intrigued by biometrics and phone based authentication we utilize the system usability scale sus a a standard metric for empirical analysis and find that it produce reliable replicable result sus prof to be an accurate measure of baseline usability we recommend that new authentication system be formally evaluated for usability using sus and should meet a minimum acceptable sus score before receiving serious consideration 
sparql entailment regime are strongly influenced by the big body of work on ontology based query answering notably in the area of description logic dl however the semantics of query answering under sparql entailment regime is defined in a more naive and much le expressive way than the certain answer semantics usually adopted in dl the goal of this work is to introduce an intuitive certain answer semantics also for sparql and to show the feasibility of this approach for owl ql entailment we present algorithm for the evaluation of an interesting fragment of sparql the so called well designed sparql moreover we show that the complexity of the most fundamental query analysis task such a query containment and equivalence testing is not negatively affected by the presence of owl ql entailment under the proposed semantics 
many individual on social networking site provide trait about themselves such a interest or demographic social networking site can use this information to provide better content to match their user interest such a recommending scheduled event or various relevant product these task require accurate probability estimate to determine the correct answer to return relational machine learning rml is an excellent framework for these problem a it jointly model the user label given their attribute and the relational structure further semi supervised learning method could enable rml method to exploit the large amount of unlabeled data in network however existing rml approach have limitation that prevent their application in large scale domain first semi supervised method for rml do not fully utilize all the unlabeled instance in the network second the collective inference procedure necessary to jointly infer the missing label are generally viewed a too expensive to apply in large scale domain in this work we address each of these limitation we analyze the effect of full semi supervised rml and find that collective inference method can introduce considerable bias into prediction we correct this by implementing a maximum entropy constraint on the inference step forcing the prediction to have the same distribution a the observed label next we outline a massively scalable variational inference algorithm for large scale relational network domain we extend this inference algorithm to incorporate the maximum entropy constraint proving that it only requires a constant amount of overhead while remaining massively parallel we demonstrate our method s improvement over a variety of baseline on seven real world datasets including large scale network with over five million edge 
social medium ha led to the democratisation of opinion sharing a wealth of information about public opinion current event and author insight into specific topic can be gained by understanding the text written by user however there is a wide variation in the language used by different author in different context on the web this diversity in language make interpretation an extremely challenging task crowdsourcing present an opportunity to interpret the sentiment or topic of free text however the subjectivity and bias of human interpreter raise challenge in inferring the semantics expressed by the text to overcome this problem we present a novel bayesian approach to language understanding that relies on aggregated crowdsourced judgement our model encodes the relationship between label and text feature in document such a tweet web article and blog post accounting for the varying reliability of human labellers it allows inference of annotation that scale to arbitrarily large pool of document our evaluation using two challenging crowdsourcing datasets show that by efficiently exploiting language model learnt from aggregated crowdsourced label we can provide up to improved classification when only a small portion le than of document ha been labelled compared to the six state of the art method we reduce by up to the number of crowd response required to achieve comparable accuracy our method wa a joint winner of the crowdflower crowdscale shared task challenge at the conference on human computation and crowdsourcing hcomp 
large graph arise in a number of context and understanding their structure and extracting information from them is an important research area early algorithm on mining community have focused on the global structure and often run in time functional to the size of the entire graph nowadays a we often explore network with billion of vertex and find community of size hundred it is crucial to shift our attention from macroscopic structure to microscopic structure when dealing with large network a growing body of work ha been adopting local expansion method in order to identify the community from a few exemplary seed member very few approach can systematically demonstrate both high efficiency and effectiveness that significantly stand out amongst the divergent approach in finding community in this paper we propose a novel approach for finding overlapping community called lemon local expansion via minimum one norm different from pagerank like diffusion method lemon find the community by seeking a sparse vector in the span of the local spectrum such that the seed are in it support we show that lemon can achieve the highest detection accuracy among state of the art proposal the running time depends on the size of the community rather than that of the entire graph the algorithm is easy to implement and is highly parallelizable moreover given that network are not all similar in nature a comprehensive analysis on how the local expansion approach is suited for uncovering community in different network is still lacking we thoroughly evaluate our approach using both synthetic and real world datasets across different domain and analyze the empirical variation when applying our method to inherently different network in practice in addition the heuristic on how the quality and quantity of the seed set would affect the performance are provided 
sociolinguistic study investigate the relation between language and extra linguistic variable this requires both representative text data and the associated socio economic meta data of the subject traditionally sociolinguistic study use small sample of hand curated data and meta data this can lead to exaggerated or false conclusion using social medium data offer a large scale source of language data but usually lack reliable socio economic meta data our research aim to remedy both problem by exploring a large new data source international review website with user profile they provide more text data than manually collected study and more meta data than most available social medium text we describe the data and present various pilot study illustrating the usefulness of this resource for sociolinguistic study our approach can help generate new research hypothesis based on data driven finding across several country and language 
entity extraction is a process of identifying meaningful entity from text document in enterprise extracting entity improves enterprise efficiency by facilitating numerous application including search recommendation etc however the problem is particularly challenging on enterprise domain due to several reason first the lack of redundancy of enterprise entity make previous web based system like nell and openie not effective since using only high precision low recall pattern like those system would miss the majority of sparse enterprise entity while using more low precision pattern in sparse setting also introduces noise drastically second semantic drift is common in enterprise blue refers to window blue such that public signal from the web cannot be directly applied on entity moreover many internal entity never appear on the web sparse internal signal are the only source for discovering them to address these challenge we propose an end to end framework for extracting entity in enterprise taking the input of enterprise corpus and limited seed to generate a high quality entity collection a output we introduce the novel concept of semantic pattern graph to leverage public signal to understand the underlying semantics of lexical pattern reinforce pattern evaluation using mined semantics and yield more accurate and complete entity experiment on microsoft enterprise data show the effectiveness of our approach 
people diagnosed with a serious illness often turn to the web for their rising information need especially when decision are required we analyze the search and browsing behavior of searcher who show a surge of interest in prostate cancer prostate cancer is the most common serious cancer in men and is a leading cause of cancer related death diagnosis of prostate cancer typically involve reflection and decision making about treatment based on assessment of preference and outcome we annotated timeline of treatment related query from nearly searcher with tag indicating different phase of treatment including decision making preparation and recovery using this corpus we present a variety of analysis toward the goal of understanding search and decision making about treatment we characterize search query and the content of accessed page for different treatment phase model search behavior during the decision making phase and create an aggregate alignment of treatment timeline illustrated with a variety of visualization the experiment provide insight about how people who are engaged in intensive search about prostate cancer over an extended period of time pursue and access information from the web 
decomposing a graph into a hierarchical structure via k core analysis is a standard operation in any modern graph mining toolkit k core decomposition is a simple and efficient method that allows to analyze a graph beyond it mere degree distribution more specifically it is used to identify area in the graph of increasing centrality and connectedness and it allows to reveal the structural organization of the graph despite the fact that k core analysis relies on vertex degree k core do not satisfy a certain rather natural density property simply put the most central k core is not necessarily the densest subgraph this inconsistency between k core and graph density provides the basis of our study we start by defining what it mean for a subgraph to be locally dense and we show that our definition entail a nested chain decomposition of the graph similar to the one given by k core but in this case the component are arranged in order of increasing density we show that such a locally dense decomposition for a graph g v e can be computed in polynomial time the running time of the exact decomposition algorithm is o v e but is significantly faster in practice in addition we develop a linear time algorithm that provides a factor approximation to the optimal locally dense decomposition furthermore we show that the k core decomposition is also a factor approximation however a demonstrated by our experimental evaluation in practice k core have different structure than locally dense subgraphs and a predicted by the theory k core are not always well aligned with graph density 
personal mobile device offer a growing variety of personalized service that enrich considerably the user experience this is made possible by increased access to personal information which to a large extent is extracted from user email message and archive there are however two main issue first currently these service can be offered only by large web service company that can also deploy email service second keeping a large amount of structured personal information on the cloud raise privacy concern to address these problem we propose ln annote a new method to extract personal information from the email that is locally available on mobile device without remote access to the cloud ln annote enables third party service provider to build a question answering system on top of the local personal information without having to own the user data in addition ln annote mitigates the privacy concern by keeping the structured personal information directly on the personal device our method is based on a named entity recognizer trained in two separate step first using a common dataset on the cloud and then using a personal dataset in the mobile device at hand our contribution include also the optimization of the implementation of ln annote in particular we implemented an opencl version of the custom training algorithm to leverage the graphic processing unit gpu available on the mobile device we present an extensive set of experiment result beside proving the feasibility of our approach they demonstrate it efficiency in term of the named entity extraction performance a well a the execution speed and the energy consumption spent in mobile device 
research ethic is an important and timely topic in academia federally regulated institutional review board irbs protect participant of human subject research and offer researcher a mechanism to ass the ethical implication of their work industry research lab are not subject to the same requirement and may lack process for research ethic review we describe the creation of a new ethic framework and a research ethic submission system res within microsoft research msr this res is customized to the need of web researcher we describe our iterative development process including our assessment of the current state of web research developing a framework of method based on a survey of research paper build and evaluate our system with user to identify the benefit and pitfall of full deployment evaluate how our system match with existing federal regulation and suggest next step for supporting ethical web research 
online social network nowadays enjoy their worldwide prosperity a they have revolutionized the way for people to discover to share and to distribute information with million of registered user and the proliferation of user generated content the social network become giant likely eligible to carry on any research task however the giant do have their achilles heel extreme data sparsity compared with the massive data over the whole collection individual posting document e g a microblog le than character seem to be too sparse to make a difference under various research scenario while actually they are different in this paper we propose to tackle the achilles heel of social network by smoothing the language model via influence propagation we formulate a socialized factor graph model which utilizes both the textual correlation between document pair and the socialized augmentation network behind the document such a user relationship and social interaction these factor are modeled a attribute and dependency among document and their corresponding user an efficient algorithm is designed to learn the proposed factor graph model finally we propagate term count to smooth document based on the estimated influence experimental result on twitter and weibo datasets validate the effectiveness of the proposed model by leveraging the smoothed language model with social factor our approach obtains significant improvement over several alternative method on both intrinsic and extrinsic evaluation measured in term of perplexity ndcg and map result 
this paper first reveals the relationship between inverse document frequency idf a global term weighting scheme and information distance a universal metric defined by kolmogorov complexity we concretely give a theoretical explanation that the idf of a term is equal to the distance between the term and the empty string in the space of information distance in which the kolmogorov complexity is approximated using web document and the shannon fano coding based on our finding we propose n gram idf a theoretical extension of idf for handling word and phrase of any length by comparing weight among n gram of any n n gram idf enables u to determine dominant n gram among overlapping one and extract key term of any length from text without using any nlp technique to efficiently compute the weight for all possible n gram we adopt two string processing technique i e maximal substring extraction using enhanced suffix array and document listing using wavelet tree we conducted experiment on key term extraction and web search query segmentation and found that n gram idf wa competitive with state of the art method that were designed for each application using additional resource and effort the result exemplified the potential of n gram idf 
community based question answering platform can be rich source of information on a variety of specialized topic from finance to cooking the usefulness of such platform depends heavily on user contribution question and answer but also on respecting the community rule a a crowd sourced service such platform rely on their user for monitoring and flagging content that violates community rule common wisdom is to eliminate the user who receive many flag our analysis of a year of trace from a mature q a site show that the number of flag doe not tell the full story on one hand user with many flag may still contribute positively to the community on the other hand user who never get flagged are found to violate community rule and get their account suspended this analysis however also show that abusive user are betrayed by their network property we find strong evidence of homophilous behavior and use this finding to detect abusive user who go under the community radar based on our empirical observation we build a classifier that is able to detect abusive user with an accuracy a high a 
the rapid growth of location based service provide the potential to understand people s mobility pattern at an unprecedented level which can also enable food service industry to accurately predict consumer s dining behavior in this paper by leveraging user historical dining pattern socio demographic characteristic and restaurant attribute we aim at generating the top k restaurant for a user s next dining compared to previous study in location prediction which mainly focus on regular mobility pattern we present a novelty seeking based dining recommender system termed ndrs in consideration of both exploration and exploitation first we apply a conditional random field crf with additional constraint to infer user novelty seeking status by considering both spatial temporal historical feature and user socio demographic characteristic on the one hand when a user is predicted to be novelty seeking by incorporating the influence of restaurant contextual factor such a price and service quality we propose a context aware collaborative filtering method to recommend restaurant she ha never visited before on the other hand when a user is predicted to be not novelty seeking we then present a hidden markov model hmm considering the temporal regularity to recommend the previously visited restaurant to evaluate the performance of each component a well a the whole system we conduct extensive experiment with a large dataset we have collected covering the concerned dining related check in user demographic and restaurant attribute the result reveal that our system is effective for dining recommendation 
we present gerbil an evaluation framework for semantic entity annotation the rationale behind our framework is to provide developer end user and researcher with easy to use interface that allow for the agile fine grained and uniform evaluation of annotation tool on multiple datasets by these mean we aim to ensure that both tool developer and end user can derive meaningful insight pertaining to the extension integration and use of annotation application in particular gerbil provides comparable result to tool developer so a to allow them to easily discover the strength and weakness of their implementation with respect to the state of the art with the permanent experiment uris provided by our framework we ensure the reproducibility and archiving of evaluation result moreover the framework generates data in machine processable format allowing for the efficient querying and post processing of evaluation result finally the tool diagnostics provided by gerbil allows deriving insight pertaining to the area in which tool should be further refined thus allowing developer to create an informed agenda for extension and end user to detect the right tool for their purpose gerbil aim to become a focal point for the state of the art driving the research agenda of the community by presenting comparable objective evaluation result 
in crowdsourcing system the interest of contributing participant and system stakeholder are often not fully aligned participant seek to learn be entertained and perform easy task which offer them instant gratification system stakeholder want user to complete more difficult task which bring higher value to the crowdsourced application we directly address this problem by presenting technique that optimize the crowdsourcing process by jointly maximizing the user longevity in the system and the true value that the system derives from user participation we first present model that predict the survival probability of a user at any given moment that is the probability that a user will proceed to the next task offered by the system we then leverage this survival model to dynamically decide what task to assign and what motivating goal to present to the user this allows u to jointly optimize for the short term getting difficult task done and for the long term keeping user engaged for longer period of time we show that dynamically assigning task significantly increase the value of a crowdsourcing system in an extensive empirical evaluation we observed that our task allocation strategy increase the amount of information collected by up to we also explore the utility of motivating user with goal we demonstrate that setting specific static goal can be highly detrimental to the long term user participation a the completion of a goal e g earning a badge is also a common drop off point for many user we show that setting the goal dynamically in conjunction with judicious allocation of task increase the amount of information collected by the crowdsourcing system by up to compared to the existing baseline that use fixed objective 
everyday million of user save content item for future use on site like pinterest by pinning them onto carefully categorised personal pinboards thereby creating personal taxonomy of the web this paper seek to understand pinterest a a distributed human computation that categorises image from around the web we show that despite being categorised onto personal pinboards by individual action there is a generally a global agreement in implicitly assigning image into a coarse grained global taxonomy of category and furthermore user tend to specialise in a handful of category by exploiting these characteristic and augmenting with image related feature drawn from a state of the art deep convolutional neural network we develop a cascade of predictor that together automate a large fraction of pinterest action our end to end model is able to both predict whether a user will repin an image onto her own pinboard and also which pinboard she might choose with an accuracy of accuracy of 
a word s sentiment depends on the domain in which it is used computational social science research thus requires sentiment lexicon that are specific to the domain being studied we combine domain specific word embeddings with a label propagation framework to induce accurate domain specific sentiment lexicon using small set of seed word we show that our approach achieves state of the art performance on inducing sentiment lexicon from domain specific corpus and that our purely corpus based approach outperforms method that rely on hand curated resource e g wordnet using our framework we induce and release historical sentiment lexicon for year of english and community specific sentiment lexicon for online community from the social medium forum reddit the historical lexicon we induce show that more than of sentiment bearing non neutral english word completely switched polarity during the last year and the community specific lexicon highlight how sentiment varies drastically between different community 
we construct a human in the loop supervised learning framework that integrates crowdsourcing feedback and local knowledge to detect job related tweet from individual and business account using data driven ethnography we examine discourse about work by fusing language based analysis with temporal geospational and labor statistic information 
random forest are one type of the most effective ensemble learning method in spite of their sound empirical performance the study on their theoretical property ha been left far behind recently several random forest variant with nice theoretical basis have been proposed but they all suffer from poor empirical performance in this paper we propose a bernoulli random forest model brf which intends to close the gap between the theoretical consistency and the empirical soundness of random forest classification compared to breiman s original random forest brf make two simplification in tree construction by using two independent bernoulli distribution the first bernoulli distribution is used to control the selection of candidate attribute for each node of the tree and the second one control the splitting point used by each node a a result brf enjoys proved theoretical consistency so it accuracy will converge to optimum i e the bayes risk a the training data grow infinitely large empirically brf demonstrates the best performance among all theoretical random forest and is very comparable to breiman s original random forest which do not have the proved consistency yet the theoretical and experimental study advance the research one step further towards closing the gap between the theory and the practical performance of random forest classification 
the depth image based rendering dibr play a key role in d video synthesis by which other virtual view can be generated from a d video and it depth map however in the synthesis process the background occluded by the foreground object might be exposed in the new view resulting in some hole in the synthetized video in this paper a hole filling approach based on background reconstruction is proposed in which the temporal correlation information in both the d video and it corresponding depth map are exploited to construct a background video to construct a clean background video the foreground object are detected and removed also motion compensation is applied to make the background reconstruction model suitable for moving camera scenario each frame is projected to the current plane where a modified gaussian mixture model is performed the constructed background video is used to eliminate the hole in the synthetized video our experimental result have indicated that the proposed approach ha better quality of the synthetized d video compared with the other method 
statistical model such a linear regression drive numerous application in computer vision and machine learning the landscape of practical deployment of these formulation is dominated by forward regression model that estimate the parameter of a function mapping a set of p covariates x to a response variable y the le known alternative inverse regression offer various benefit that are much le explored in vision problem the goal of this paper is to show how inverse regression in the abundant feature setting i e many subset of feature are associated with the target label or response a is the case for image together with a statistical construction called sufficient reduction yield highly flexible model that are a natural fit for model estimation task in vision specifically we obtain formulation that provide relevance of individual covariates used in prediction at the level of specific example sample in a sense explaining why a particular prediction wa made with no compromise in performance relative to other method an ability to interpret why a learning algorithm is behaving in a specific way for each prediction add significant value in numerous application we illustrate these property and the benefit of abundant inverse regression air on three distinct application 
convolutional neural network cnn are state of the art model for many image classification task however to recognize cancer subtypes automatically training a cnn on gigapixel resolution whole slide tissue image wsi is currently computationally impossible the differentiation of cancer subtypes is based on cellular level visual feature observed on image patch scale therefore we argue that in this situation training a patch level classifier on image patch will perform better than or similar to an image level classifier the challenge becomes how to intelligently combine patch level classification result and model the fact that not all patch will be discriminative we propose to train a decision fusion model to aggregate patch level prediction given by patch level cnns which to the best of our knowledge ha not been shown before furthermore we formulate a novel expectation maximization em based method that automatically locates discriminative patch robustly by utilizing the spatial relationship of patch we apply our method to the classification of glioma and non small cell lung carcinoma case into subtypes the classification accuracy of our method is similar to the inter observer agreement between pathologist although it is impossible to train cnns on wsis we experimentally demonstrate using a comparable non cancer dataset of smaller image that a patch based cnn can outperform an image based cnn 
cascade ha been widely used in face detection where classifier with low computation cost can be firstly used to shrink most of the background while keeping the recall the cascade in detection is popularized by seminal viola jones framework and then widely used in other pipeline such a dpm and cnn however to our best knowledge most of the previous detection method use cascade in a greedy manner where previous stage in cascade are fixed when training a new stage so optimization of different cnns are isolated in this paper we propose joint training to achieve end to end optimization for cnn cascade we show that the back propagation algorithm used in training cnn can be naturally used in training cnn cascade we present how jointly training can be conducted on naive cnn cascade and more sophisticated region proposal network rpn and fast r cnn experiment on face detection benchmark verify the advantage of the joint training 
