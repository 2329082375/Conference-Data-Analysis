unsupervisedlearningalgorithmshavebeenderivedforseveralstatistical model of english grammar but their computational complexity make applying them to large data set intractable this paper present a probabilistic model of english grammar that is much simpler than conventional model but which admits an e cient em training algorithm the model is based upon grammatical bigram i e syntactic relationship between pair of word we present the result of experiment that quantify the representational adequacy of the grammatical bigram model it ability to generalize from labelled data and it ability to induce syntactic structure from large amount of raw text 
in a previous work we presented systemltree a multivariate tree that combine adecision tree with a linear discriminant bymeans of constructive induction we haveshown that it performs quite well in term ofaccuracy and learning time in comparisonwith other multivariate system like lmdt oc and cart in this work we extend theprevious work by using two new discriminantfunctions a quadratic discriminant and a logistic discriminant using the same architectureas ltree we 
substantial data support a temporal difference td model of dopamine da neuron activity in which the cell provide a global error signal for reinforcement learning however in certain circumstance da activity seems anomalous under the td model responding to non rewarding stimulus we address these anomaly by suggesting that da cell multiplex information about reward bonus including sutton s exploration bonus and ng et al s non distorting shaping bonus we interpret this 
this paper is about new statistic and new efficientalgorithms for a form of mixture modelthat learns filamentary structure suchmodels are important in several area of scientificdata analysis but in this paper ourmain example is identification of large scalestructure among galaxy we describe softwarewhich can extract the position of sphericaland line shaped cluster from data aboutthe location of object such a galaxy wedo so by fitting a particular type of 
incremental search technique find optimal solution to ser y of similar search task much faster than is possible by solving each search task from scratch while researcher have developed incremental version of uninformed search method we develop an incremental version of a the first search of lifelong planning a is the same a that of a but all subsequent search are much faster because it reuses th ose part of the previous search tree that are identical to the new search tree we then present experimental result that demonstrate the advanta ge of lifelong planning a for simple route planning task overview artificial intelligence ha investigated knowledge based search technique that allow one to solve search task in large domain most of the research o n these method ha studied how to solve one shot search problem however search is often a repetitive process where one need to solve a series of similar search task for example because the actual situation turn out to be slightly different from the one ini tially assumed or because the situation change over time an example for route planning task are changing traffic condition thus one need to replan for the new situation for example if one always want to display the least time consuming route from the airport to the conference center on a web page in these situation most search method replan from scratch that is solve the search problem independently incremental search technique share with case based planning plan adaptation repair based planning and lea rning search control knowledge the property that they find solution to series of similar sea rch task much faster than is possible by solving each search task from scratch incremental search technique however differ from the other technique in that the quality of their solution is guaranteed to be a good a the quality of the solution obtained by replanning from scratch although incremental search method are not widely known in artificial intelligence and control different researcher have developed incrementa l search version of uninformed search method in the algorithm literature an overview ca n be found in fmsn we on the other hand develop an incremental version of a thus combining idea from the algorithm literature and the artificial intelligence l iterature we call the algorithm lifelong planning a lpa in analogy to lifelong learning thr because it reuses 
it ha been shown that the receptive field of simple cell in v can be explained by assuming optimal encoding provided that an extra constraint of sparseness is added this finding suggests that there is a r eason independent of optimal representation for sparseness however this work used an ad hoc model for the noise here i show that if a biologically more plausible noise model describing neuron a poisson process is used sparseness doe not have to be added a a constraint thus i conclude that sparseness is not a feature that evolution ha str iven for but is simply the result of the evolutionary pressure towards an op timal representation 
recommender system have been revolutionizing the way shopper and information seeker find what they want we will study some of the tremendous success and spectacular failure of recommenders in e commerce to understand the cause of the success or failure we will leverage that understanding into a set of principle for successfully applying recommenders to business problem finally we will study the economic and social force that are shaping the evolution of recommenders and peer into the crystal ball to glimpse the direction the technology will be going in the future 
we introduce a new type of self organizing map som to navigate in the semantic space of large text collection we propose a hyperbolic som hsom based on a regular tesselation of the hyperbolic plane which is a non euclidean space characterized by constant negative gaussian curvature the exponentially increasing size of a neighborhood around a point in hyperbolic space provides more freedom to map the complex information space arising from language into spatial relation we describe experiment showing that the hsom can successfully be applied to text categorization task and yield result comparable to other state of the art method 
we introduce a novel distributional clustering algorithm t hat explicitly maximizes the mutual information per cluster between the data and given category this algorithm can be considered a a bottom up hard version of the recently introduced information bottleneck method we relate the mutual information between cluster and category to the bayesian classification error which provides another motivation for using the obtained cluster s a feature the algorithm is compared with the top down soft version of the information bottleneck method and a relationship between the hard and soft result is established we demonstrate the alg orithm on the newsgroupsdata set for a subset of two news group we achieve compression by order of magnitude loosing only of the original mutual information 
quantitative data on the speed with which animal acquire behavioral response during classical conditioning experiment should provide strong constraint on model of learning however most model have simply ignored these data the few that have attempted to address them have failed by at least an order of magnitude we discus key data on the speed of acquisition and show how to account for them using a statistically sound model of learning in which differential reliability of stimulus play a crucial role 
in this article we propose and analyze a class of actor critic algorithm these are two time scale algorithm in which the critic us temporal difference learning with a linearly parameterized approximation architecture and the actor is updated in an approximate gradient direction based on information provided by the critic we show that the feature for the critic should ideally span a subspace prescribed by the choice of parameterization of the actor we study actor critic algorithm for markov decision process with polish state and action space we state and prove two result regarding their convergence 
we describe data mining technique designed to address the problem of selecting prospective customer from a large pool of candidate these technique cover a number of different scenario namely whether the marketing researcher have demographic information on the current customer or the general market population or people with propensity to become customer we also present a novel approach to the problem by exploiting the availability of a data sample from the general market population finally we describe an on line lead management and delivery system that us the mining approach described in this paper for insurance agent to obtain qualified customer lead 
this article present a phase computational learning model and application a a demonstration a system ha been built called chime for computer human interacting musical entity in phase of training recurrent back propagation train the machine to reproduce jazz melody the recurrent network is expanded and is further trained in phase with a reinforcement learning algorithm and a critique produced by a set of basic rule for jazz improvisation after each phase chime can interactively improvise with a human in real time 
having access to massive amount of data doe not necessarily imply that inductionalgorithms must use them all sample often provide the same accuracy with far lesscomputational cost however the correct sample size is rarely obvious we analyzemethods for progressive sampling starting with small sample and progressively increasingthem a long a model accuracy improves we show that a simple geometricsampling schedule is efficient in an asymptotic sense we then explore the notion 
traditional theory of child language acquisition center around theexistence of a language acquisition device which is specifically tunedfor learning a particular class of language more recent proposalssuggest that language acquisition is assisted by the evolution oflanguages towards form that are easily learnable in this paper we evolve combinatorial language which can be learned by a simplerecurrent network quickly and from relatively few example additionally we evolve 
i present a simple variation of importance sampling that explicitly search for important region in the target distribution i prove that the technique yield unbiased estimate and show empirically it can reduce the variance of standard monte carlo estimator this is achieved by concentrating sample in more significant region of the sample space 
belief propagation bp wa only supposed to work for tree like network but work surprisingly well in many application involving network with loop including turbo code however there ha been little understanding of the algorithm or the nature of the solution it find for general graph we show that bp can only converge to a stationary point of an approximate free energy known a the bethe free energy in statistical physic this result characterizes bp fix ed point and make connection with variational approach to approximate inference more importantly our analysis let u build on the progress made in statistical physic since bethe s approximation wa introduced in kikuchi and others have shown how to construct more accurate free energy approximation of which bethe s approximation is the simplest exploiting the insight from our analysis we derive generalized belief propagation gbp version of these kikuchi approximation these new message passing algorithm can be significantly more accurate than ordinary bp at an adjustable increase in complexity we illustrate such a new gbp algorithm on a grid markov network and show that it give much more accurate marginal probability than those found using ordinary bp 
we consider a group of bayesian learner whose interaction with the environment and other agent allow them to improve their model of the dependency among various factor that have influence on their interaction with the environment effective collaboration can improve the performance of isolated individual learner we present a mechanism to pool together the knowledge of many modeler in the domain each of whom may have only partial access to the environment the application domain used in 
determining and setting maximal revenue expectation or other business performance target whether it is for regional company division or individual customer can have profound financial implication operational technique are changed staffing level are altered and management attention is re focused all in the name of expectation in practice these expectation are often derived in an ad hoc manner to address this unsupervised task we combine nearest neighbor method and classical statistical method and derive a new solution to the classical econometric task of frontier analysis we apply our methodology to two real world business problem in verizon a major telecommunication provider in the united state more specifically in the print yellow page division verizon information service identifying under marketed customer for targeted upselling campaign and focused sale attention and benchmarking regional directory division to incent performance improvement our analysis uncovers some commercially useful aspect of these domain and by conservative estimate can increase revenue by several million dollar in each domain 
many interesting problem such a powergrids network switch and traffic flow thatare candidate for solving with reinforcementlearning rl also have property that makedistributed solution desirable we proposean algorithm for distributed reinforcementlearning based on distributing the representationof the value function across node eachnode in the system only ha the ability tosense state locally choose action locally andreceive reward locally the goal of 
in theory the winnow multiplicative update ha certain advantage over the perceptron additive update when there are many irrelevant attribute recently there ha been much effort on enhancing the perceptron algorithm by using regularization leading to a class of linear c lassification method called support vector machine similarly it is al so possible to apply the regularization idea to the winnow algorithm which give method we call regularized winnow we show that the resulting method compare with the basic winnow in a similar way that a support vector machine compare with the perceptron we investigate algorithmic issue and learning property of the derived method some experimental result will also be provided to illustrate different metho d 
this paper present an introduction to reinforcement learning and relational reinforcement learning at a level to be understood by student and researcher with different background it give an overview of the fundamental principle and technique of reinforcement learning without involving a rigorous deduction of the mathematics involved through the use of an example application then relational reinforcement learning is presented a a combination of reinforcement learning with relational learning it advantage such a the possibility of using structural representation making abstraction from specific goal pursued and exploiting the result of previous learning phase are discussed 
we rst show how to represent sharp posterior probability distribu tions using real valued coe cients on broadly tuned basis function then we show how the precise time of spike can be used to con vey the real valued coe cients on the basis function quickly and accurately finally we describe a simple simulation in which spik ing neuron learn to model an image sequence by tting a dynamic generative model 
the structured policy iteration spi algorithm boutilier et al construct structured solution to markov decision problem mdps it find the optimal value function by treating all state which have the same value under a particular policy a a single aggregate state here we show that this approach can also be applied locally in a structured version of prioritized sweeping a model based reinforcement learning algorithm that attempt to focus the learning agent s limited computational resource on state where the model is most likely to be incorrect in structured prioritized sweeping we maintain a structured value function and update the value of aggregate state this increase the scope of local value function change and the rate at which they propagate across the state space which make learning more efficient particularly in larger problem 
many domain are naturally organized in an abstraction hierarchy or taxonomy where the instance in nearby class in the taxonomy are similar in this paper we provide a general probabilistic framework for clust ering data into a set of class organized a a taxonomy where each class is associated with a probabilistic model from which the data wa generated the clustering algorithm simultaneously optimizes three thing the assignment of data instance to cluster the model associated with the cluster and the struc ture of the abstraction hierarchy a unique feature of our approach is that it utiliz e global optimization algorithm for both of the last two step reducing the sensi tivity to noise and the propensity to local maximum that are characteristic of algor ithms such a hierarchical agglomerative clustering that only take local step we provide a theoretical analysis for our algorithm showing that it converges to a lo cal maximum of the joint likelihood of model and data we present experimental result on synthetic data and on real data in the domain of gene expression and text 
classification and regression are fundamental data mining technique the goal of such technique is to build predictor based on a training dataset and use them to predict the property of new data for a wide range of technique combining predictor built on sample from the training dataset provides lower error rate faster construction or both than a predictor built from the entire training dataset this provides a natural parallelization strategy in which predictor based on sample are built independently and hence concurrently we discus the performance implication for two subclass those in which predictor are independent and those in which knowing a set of predictor reduces the difficulty of finding a new one 
we suggest a nonparametric framework for unsupervised learning of projection model in term of density estimation on quantized sample space the objective is not to optimally reconstruct the data but instead the quantizer is chosen to optimally reconstruct the density of the data for the resulting quantizing density estimator qde we present a general method for parameter estimation and model selection we show how projection set which correspond to traditional unsupervised method like vector quantization or pca appear in the new framework for a principal component quantizer we present result on synthetic and realworld data which show that the qde can improve the generalization of the kernel density estimator although it estimate is based on significantly lower dimensional projection index of the data 
ockham s razor is a powerful rule that ha impregnated western logical method of challenging problem and ha fostered the development of science in this essay i discus briefly the historical origin of this epistemological rule during the middle age i suggest that ockham s razor may represent a cognitive ability which ha been applied by several thinker since before medieval time furthermore i suggest that the application of ockham s razor ha had an important role both in the development and rejection of new revolutionary theory 
these are node type univariate v multivariate branching factor two or more grouping of class into two if the tree is binary error impurity measure and the method for minimization to find the best split vector and threshold we then propose a new decision tree induction method that we name linear discriminant tree ldt which us the best combination of these criterion in term of accuracy simplicity and learning time this tree induction method can be univariate or multivariate the method ha a supervised outer optimization layer for converting a k class problem into a sequence of two class problem and each two class problem is solved analytically using fisher s linear discriminant analysis lda on twenty datasets from the uci repository we compare the linear discriminant tree with the univariate decision tree method c and c multivariate decision tree method cart oc quest neural tree and lmdt our proposed linear discriminant tree learn fast are accurate and the tree generated are small 
kernel method like support vector machine have successfully been used for text categorization a standard choice of kernel function ha been the inner product between the vector space representation of two document in analogy with classical information retrieval ir approach latent semantic indexing lsi ha been successfully used for ir purpose a a technique for capturing semantic relation between term and inserting them into the similarity measure between two document one of it main drawback in ir is it computational cost in this paper we describe how the lsi approach can be implemented in a kernel defined feature space we provide experimental result demonstrating that the approach can significantly improve performance and that it doe not impair it 
a lightweight rule induction method is describedthat generates compact disjunctivenormal form dnf rule each class hasan equal numberofunweighted rule a newexample is classified by applying all rule andassigning the example to the class with themost satisfied rule the induction methodattempts to minimize the training error withno pruning an overall design is specifiedby setting limit on the size and numberof rule during training case are adaptivelyweighted 
we present a monte carlo algorithm for learning to act in partially observable markov decision process pomdps with real valued state and action space our approach us importance sampling for representing belief and monte carlo approximation for belief propagation a reinforcement learning algorithm value iteration is employed to learn value function over belief state finally a samplebased version of nearest neighbor is used to generalize across state initial empirical result suggest that our approach work well in practical application 
gaussian mixture or so called radial basis function network fordensity estimation provide a natural counterpart to sigmoidal neuralnetworks for function tting and approximation in both case it is possible to give simple expression for the iterative improvementof performance a component of the network are introducedone at a time in particular for mixture density estimation we showthat a k component mixture estimated by maximum likelihood orby an iterative likelihood 
we present a general framework for discriminative estimation based on the maximum entropy principle and it extension all calculation involve distribution over structure and or parameter rather than specific setting and reduce to relative entropy projection this hold even when the data is not separable within the chosen parametric class in the context of anomaly detection rather than classification or when the label in the training set are uncertain or incomplete support vector machine are naturally subsumed under this class and we provide several extension we are also able to estimate exactly and efficiently discriminative distribution over tree structure of class conditional model within this framework preliminary experimental result are indicative of the potential in these technique 
we describe an iterative algorithm for building vector mach ines used in classification task the algorithm build on idea from sup port vector machine boosting and generalized additive model the algorithm can be used with various continuously differential function t hat bound the discrete classification loss and is very simple to impl ement we test the proposed algorithm with two different loss function on synthetic and natural data we also describe a norm penalized version of the algorithm for the exponential loss function used in adaboost the performance of the algorithm on natural data is comparable to support vecto r machine while typically it running time is shorter than of svm 
when constructing a classier the probability of correct classi cation of future data point should be maximized in the currentpaper this desideratum is translated in a very direct way into anoptimization problem which is solved using method from convexoptimization we also show how to exploit mercer kernel inthis setting to obtain nonlinear decision boundary a worst casebound on the probability of misclassication of future data is obtainedexplicitly 
the standard reinforcement learning view of the involvement of neuromodulatory system in instrumental conditioning includes a rather straightforward conception of motivation a prediction of sum future reward competition between action is based on the motivating characteristic of their consequent state in this sense substantial careful experiment reviewed in dickinson amp balleine into the neurobiology and psychology of motivation show that this view is incomplete in many case 
this paper introduces multiple instance regression a variant of multiple regression in which each data point may be described by more than one vector of value for the independent variable the goal of this work are to understand the computational complexity of the multiple instance regression task and develop an ecien t algorithm that is superior to ordinary multiple regression when applied to multiple instance data set 
we propose a novel method for the analysis of sequential data that exhibit an inherent mode switching in particular the data might be a non stationary time series from a dynamical system that switch between multiple operating mode unlike other approach our method process the data incrementally and without any training of internal parameter we use an hmm with a dynamically changing number of state and an on line variant of the viterbi algorithm that performs an unsupervised segmentation and classication of the data on the y i e the method is able to process incoming data in real time the main idea of the approach is to track and segment change of the probability density of the data in a sliding window on the incoming data stream the usefulness of the algorithm is demonstrated by an application to a switching dynamical system 
the vicinal risk minimization principle establishes a bridge between generative model and method derived from the structural risk minimization principle such a support vector machine or statistical regularization we explain how vrm provides a framework which integrates a number of existing algorithm such a parzen window support vector machine ridge regression constrained logistic classifier and tangent prop we then show how the approach implies new algorithm for solving problem usually associated with generative model new algorithm are described for dealing with pattern recognit ion problem with very different pattern distribution and dealing with unlabeled data preliminary empirical result are presented 
we present an algorithm that induces a class of model with thin junction tree model that are characterized by an upper bound on the size of the maximal clique of their triangulated graph by ensurin g that the junction tree is thin inference in our model remains tract able throughout the learning process this allows both an efficient implemen tation of an iterative scaling parameter estimation algorithm and al so ensures that inference can be performed efficiently with the final model w e illustrate the approach with application in handwritten digit recogn ition and dna splice site detection 
a few year ago pearl s probability propagationalgorithmin graph with cycle wa shown to produce excellent result for error correcting turbodecoding ever since we have wondered whether iterative probability propagation could be used successfully for machine learning a a first step in this direction we study iterative inference and learning in the simple factor analyzer network a two layer densely connected network that model bottom layer sensory input a a linear combination of top layer factor plus independent gaussian sensor noise the number of bottom up top down iteration needed to exactly infer the factor given a network and an input scale with the number of factor in the top layer in online learning this iterative procedure must be reinitialized upon each pattern presentation and so learning becomes prohibitively slow in big network such a those used for face recognition and for large scale model of the cortex we show that probabilitypropagation in a factor analyzer usually take just a few iteration to achieve a low inference error even in network with sensor and factor we derive an expression for the algorithm s fixed point and provide an eigenvalue condition for globalconvergence we also show how iterativeinference can be used to do online learning and give result on using this method to do online dimensionalityreduction for the purpose of recognizing pixelimages of face using a dimensional subspace this work suggests that iterative probability propagation in densely connected network may lead to a broad class of useful algorithm for machine learning 
using statistical mechanic result i calculate learning curve average generalization error for gaussian process gps and bayesian neural network nns used for regression applying the result to learning a teacher defined by a two layer network i can directly compare gp and bayesian nn learning i find that a gp in general requires training example to learn input feature of order is the input dimension whereas a nn can learn the task with order the number of adjustable weight training example since a gp can be considered a an infinite nn the result show that even in the bayesian approach it is important to limit the complexity of the learning machine the theoretical finding are confirmed in simulation with analytical gp learning and a nn mean field algorithm 
support vector machine have met with significant success in numerous real worldlearning task however like most machine learning algorithm they are generally appliedusing a randomly selected training set classified in advance in many setting we also havethe option of using pool based active learning instead of using a randomly selected trainingset the learner ha access to a pool of unlabeled instance and can request the label for somenumber of them we introduce a new 
abstract classifier system are now viewed disappointing because of their problem such a the rule strength v rule set performance problem and the credit assignment problem in order to solve the problem we have developed a hybrid classifier system gls generalization learning system in designing gls we view cs a model free learning in pomdps and take a hybrid approach to finding the best generalization given the total number of rule gls us the policy improvement procedure by jaakkola et al for an locally optimal stochastic policy when a set of rule condition is given gls us ga to search for the best set of rule condition 
reinforcement learning agent attempt to learn and construct a decision policy which maximises some reward signal in turn this policy is directly derived from long term value estimate of state action pair in environment with real valued state space however it is impossible to enumerate the value of every state action pair necessitating the use of a function approximator in order to infer state action value from similar state typically function approximators require many parameter 
data mining research ha long concentrated on the five main area clustering association discovery classification forecasting and sequential pattern web data mining project are concerned mainly with text mining user segmentation forecasting web usage and analyzing user clickstream pattern we present a new type of web usage mining called funnel analysis or funnel report mining a funnel report is a study of the retention behavior among a series of page or site for example of all hit on the home page of www msn com what percentage of those are followed by hit to moneycentral msn com what percentage of www msn com hit are followed by moneycentral msn com and then www msnbc com what are the most interesting funnel starting with www msn com where doe the greatest drop off rate occur after a user ha hit msnbc funnel report are extremely useful in e business because they give product planner an idea of how usable and well structured their site is from our experience performing web usage mining for the msn network of site funnel report are requested even more than user segmentation analysis site affiliation study and classification exercise in this paper we define a framework for funnel analysis and provide a tree based solution we have been using successfully to extract all relevant funnel using only one scan of the data file 
preliminary work by the author made use of the so called manhattan world assumption about the scene statistic of city and indoor scene this assumption stated that such scene were built on a cartesian grid which led to regularity in the image edge gradient statistic in this paper we explore the general applicability of this assumption and show that surprisingly it hold in a large variety of le structured environment including rural scene this enables u from a single image to determine the orientation of the viewer relative to the scene structure and also to detect target object which are not aligned with the grid these inference are performed using a bayesian model with probability distribution e g on the image gradient statistic learnt from real data 
stimulus array are inevitably presented at different position on the retina in visual task even those that nominally require fixation in par ticular this applies to many perceptual learning task we show that per ceptual inference or discrimination in the face of positional variance ha a structurally different quality from inference about fixed position stimulus involving a particular quadratic non linearity rather than a purely lin ear discrimination we show the advantage taking this non linearity into account ha for discrimination and suggest it a a role for recurrent con nections in area v by demonstrating the superior discrimination perfor mance of a recurrent network we propose that learning the feedforward and recurrent neural connection for these task corresponds to the fast and slow component of learning observed in perceptual learning task 
recurrent neural network of analog unit are computer for realvaluedfunctions we study the time complexity of real computationin general recurrent neural network these have sigmoidal linear and product unit of unlimited order a node and no restrictionson the weight for network operating in discrete time we exhibit a family of function with arbitrarily high complexity and we derive almost tight bound on the time required to computethese function thus evidence is 
independent component analysis of natural image lead to emergence of simple cell property i e linear filter that resemble wavelet or gabor function in this paper we extend ica to explain further property of v cell first we decompose natural image into independent subspace instead of scalar component this model lead to emergence of phase and shift invariant feature similar to those in v complex cell second we define a topography between the linear component obtained 
in this paper we explore two quantitative approach to the modelling of counterfactual reasoning a linear and a noisy or model based on information contained in conceptual dependency network empirical data is acquired in a study and the fit of the model compared to it we conclude by considering the appropriateness of non parametric approach to counterfactual reasoning and examining the prospect for other parametric approach in the future 
in this paper we describe inca an adaptive advisable assistant for crisis response the system let user guide the search towardparticular schedule by giving highlevel operational advice about the solutionsdesired because trace of user interactionsprovide information regarding theuser s preference among schedule incacan draw on machine learning techniquesto construct user model that reflect thesepreferences we characterize the modelingtask a that of learning a 
theory of object recognition often assume that only one representation scheme is used within one visual processing pathway versatility of the visual system come from having multiple visual processing pathway each specialized in a different category of object we propose a theoretically simpler alternative capable of explaining the same set of data and more a single primary visual processing pathway loosely modular is assumed memory module are attached to site along this pathway object identity decision is made independently at each site a site s response time is a monotonic decreasing function of it confidence regarding it decision an observer s response is the first arriving response from any site the effective representation s of such a system determined empirically can appear to be specialized for different task and stimulus consistent with recent clinical and functional imaging finding this however merely reflects a decision being made at it appropriate level of abstraction the system itself is intrinsically flexible and adaptive 
ensemble method have recently garnered a great deal of attention in the machine learning community technique such a boosting and bagging have proven to be highly effective but require repeated resampling of the training data making them inappropriate in a data mining context the method presented in this paper take advantage of plentiful data building separate classifier on sequential chunk of training point these classifier are combined into a fixed size ensemble using a heuristic replacement strategy the result is a fast algorithm for large scale or streaming data that classifies a well a a single decision tree built on all the data requires approximately constant memory and adjusts quickly to concept drift 
many cachine learning application require classifier that minimize an asymmetric cost function rather than the misclassification rate and several recent paper have addressed this problem however these paper have either applied no statistical testing or have applied statistical method that are not appropriate for the cost sensititve setting without good statistical method it is difficult to tell whether these new cost sensastive method are better than existing method that ignore cost and it is also difficult to tell whether one cost sensitive method is better than another to rectify this problem this paper present two statistical method for the cost sensitive setting the first construct a confidence interval for the expected cost of a single classifier the second construct a confidence interval for the expected difference in cost of two classifier in both case the basic idea is to separate the problem of estimating the probability of each cell in the confusion matrix which is independent of the cost matrix from the problem of computing the expected cost we show experimentally that these bootstrap test work better than applying standard z test based on the normal distribution 
we study a population decoding paradigm in which the maximum likelihood inference is based on an unfaithful decoding model umli this is usually the case for neural population decoding because the encoding process of the brain is not exactly known or because a simplified decoding model is preferred for saving computational cost we calculate the decoding error of umli and show an example of an unfaithful model which neglect the neuronal correlation the perf ormance of umli is compared with that of the maximum likelihood inference based on a faithful model and that of the center of mass decoding method it turn out that umli ha advantage of decreasing the computational complexity remarkablely and maintaining a high level decoding accuracy at the same time 
we study using reinforcement learning in dynamic environment such environment may contain many dynamic object which make optimal planning hard one way of using information about all dynamic object is to expand the state description but this result in a high dimensional policy space our approach is to instantiate information about dynamic object in the model of the environment and to replan using model based reinforcement learning whenever this information change furthermore our 
one of the most powerful and widely acceptedanalytical formalism for modeling biologicaland physical system is that of thepartial dierential equation pde establishingan acceptable pde model for a dynamicsystem occupies a major portion of thework of the mathematical modeler there aretwo main aspect to this activity first anappropriate structure ha to be determinedfor the equation involved the model identication problem second acceptably accuratevalues for 
identification of outlier can lead to the discovery of unexpected interesting and useful knowledge existing method are designed for detecting spatial outlier in multidimensional geometric data set where a distance metric is available in this paper we focus on detecting spatial outlier in graph structured data set we define statistical test analyze the statistical foundation underlying our approach design several fast algorithm to detect spatial outlier and provide a cost model for outlier detection procedure in addition we provide experimental result from the application of our algorithm on a minneapolis st paul twin city traffic dataset to show their effectiveness and usefulness 
abstract we present a variational bayesian method for model selection over family of kernel classiers like support vector machine or gaussian process the algorithm need no user interaction and is able to adapt a large number of kernel parameter to given data without having to sacrice training case for validation this open the possibility to use sophisticated family of kernel in situation where the small standard kernel class are clearly inappropriate we relate the method to other work done on gaussian process and clarify the relation between support vector machine and certain gaussian process model 
we investigate the problem of classifying individual based on estimated density functionsfor each individual given labelled histogram characterizing red blood cell rbc fordifferent individual the learning problem is to build a classifier which can classify newunlabelled histogram into normal and iron deficient class thus the problem is similarto conventional classification in that there is labelled training data but different in thatthe underlying measurement are not 
learning curve for gaussian process regression are well understood when the student model happens to match the teacher true data generation process i derive approximation to the learning curve for the more generic case of mismatched model and find very rich behaviour for large input space dimensionality where the result become exact there are universal studentindependent plateau in the learning curve with transition in between that can exhibit arbitrarily many over fitting maximum in lower dimension plateau also appear and the asymptotic decay of the learning curve becomes strongly student dependent all prediction are confirmed by simulation 
unsupervisedlearningalgorithmshavebeenderivedforseveralstatistical model of english grammar but their computational complexity make applying them to large data set intractable this paper present a probabilistic model of english grammar that is much simpler than conventional model but which admits an e cient em training algorithm the model is based upon grammatical bigram i e syntactic relationship between pair of word we present the result of experiment that quantify the representational adequacy of the grammatical bigram model it ability to generalize from labelled data and it ability to induce syntactic structure from large amount of raw text 
in a previous work we presented systemltree a multivariate tree that combine adecision tree with a linear discriminant bymeans of constructive induction we haveshown that it performs quite well in term ofaccuracy and learning time in comparisonwith other multivariate system like lmdt oc and cart in this work we extend theprevious work by using two new discriminantfunctions a quadratic discriminant and a logistic discriminant using the same architectureas ltree we 
substantial data support a temporal difference td model of dopamine da neuron activity in which the cell provide a global error signal for reinforcement learning however in certain circumstance da activity seems anomalous under the td model responding to non rewarding stimulus we address these anomaly by suggesting that da cell multiplex information about reward bonus including sutton s exploration bonus and ng et al s non distorting shaping bonus we interpret this 
this paper is about new statistic and new efficientalgorithms for a form of mixture modelthat learns filamentary structure suchmodels are important in several area of scientificdata analysis but in this paper ourmain example is identification of large scalestructure among galaxy we describe softwarewhich can extract the position of sphericaland line shaped cluster from data aboutthe location of object such a galaxy wedo so by fitting a particular type of 
incremental search technique find optimal solution to ser y of similar search task much faster than is possible by solving each search task from scratch while researcher have developed incremental version of uninformed search method we develop an incremental version of a the first search of lifelong planning a is the same a that of a but all subsequent search are much faster because it reuses th ose part of the previous search tree that are identical to the new search tree we then present experimental result that demonstrate the advanta ge of lifelong planning a for simple route planning task overview artificial intelligence ha investigated knowledge based search technique that allow one to solve search task in large domain most of the research o n these method ha studied how to solve one shot search problem however search is often a repetitive process where one need to solve a series of similar search task for example because the actual situation turn out to be slightly different from the one ini tially assumed or because the situation change over time an example for route planning task are changing traffic condition thus one need to replan for the new situation for example if one always want to display the least time consuming route from the airport to the conference center on a web page in these situation most search method replan from scratch that is solve the search problem independently incremental search technique share with case based planning plan adaptation repair based planning and lea rning search control knowledge the property that they find solution to series of similar sea rch task much faster than is possible by solving each search task from scratch incremental search technique however differ from the other technique in that the quality of their solution is guaranteed to be a good a the quality of the solution obtained by replanning from scratch although incremental search method are not widely known in artificial intelligence and control different researcher have developed incrementa l search version of uninformed search method in the algorithm literature an overview ca n be found in fmsn we on the other hand develop an incremental version of a thus combining idea from the algorithm literature and the artificial intelligence l iterature we call the algorithm lifelong planning a lpa in analogy to lifelong learning thr because it reuses 
it ha been shown that the receptive field of simple cell in v can be explained by assuming optimal encoding provided that an extra constraint of sparseness is added this finding suggests that there is a r eason independent of optimal representation for sparseness however this work used an ad hoc model for the noise here i show that if a biologically more plausible noise model describing neuron a poisson process is used sparseness doe not have to be added a a constraint thus i conclude that sparseness is not a feature that evolution ha str iven for but is simply the result of the evolutionary pressure towards an op timal representation 
recommender system have been revolutionizing the way shopper and information seeker find what they want we will study some of the tremendous success and spectacular failure of recommenders in e commerce to understand the cause of the success or failure we will leverage that understanding into a set of principle for successfully applying recommenders to business problem finally we will study the economic and social force that are shaping the evolution of recommenders and peer into the crystal ball to glimpse the direction the technology will be going in the future 
we introduce a new type of self organizing map som to navigate in the semantic space of large text collection we propose a hyperbolic som hsom based on a regular tesselation of the hyperbolic plane which is a non euclidean space characterized by constant negative gaussian curvature the exponentially increasing size of a neighborhood around a point in hyperbolic space provides more freedom to map the complex information space arising from language into spatial relation we describe experiment showing that the hsom can successfully be applied to text categorization task and yield result comparable to other state of the art method 
we introduce a novel distributional clustering algorithm t hat explicitly maximizes the mutual information per cluster between the data and given category this algorithm can be considered a a bottom up hard version of the recently introduced information bottleneck method we relate the mutual information between cluster and category to the bayesian classification error which provides another motivation for using the obtained cluster s a feature the algorithm is compared with the top down soft version of the information bottleneck method and a relationship between the hard and soft result is established we demonstrate the alg orithm on the newsgroupsdata set for a subset of two news group we achieve compression by order of magnitude loosing only of the original mutual information 
quantitative data on the speed with which animal acquire behavioral response during classical conditioning experiment should provide strong constraint on model of learning however most model have simply ignored these data the few that have attempted to address them have failed by at least an order of magnitude we discus key data on the speed of acquisition and show how to account for them using a statistically sound model of learning in which differential reliability of stimulus play a crucial role 
in this article we propose and analyze a class of actor critic algorithm these are two time scale algorithm in which the critic us temporal difference learning with a linearly parameterized approximation architecture and the actor is updated in an approximate gradient direction based on information provided by the critic we show that the feature for the critic should ideally span a subspace prescribed by the choice of parameterization of the actor we study actor critic algorithm for markov decision process with polish state and action space we state and prove two result regarding their convergence 
we describe data mining technique designed to address the problem of selecting prospective customer from a large pool of candidate these technique cover a number of different scenario namely whether the marketing researcher have demographic information on the current customer or the general market population or people with propensity to become customer we also present a novel approach to the problem by exploiting the availability of a data sample from the general market population finally we describe an on line lead management and delivery system that us the mining approach described in this paper for insurance agent to obtain qualified customer lead 
this article present a phase computational learning model and application a a demonstration a system ha been built called chime for computer human interacting musical entity in phase of training recurrent back propagation train the machine to reproduce jazz melody the recurrent network is expanded and is further trained in phase with a reinforcement learning algorithm and a critique produced by a set of basic rule for jazz improvisation after each phase chime can interactively improvise with a human in real time 
having access to massive amount of data doe not necessarily imply that inductionalgorithms must use them all sample often provide the same accuracy with far lesscomputational cost however the correct sample size is rarely obvious we analyzemethods for progressive sampling starting with small sample and progressively increasingthem a long a model accuracy improves we show that a simple geometricsampling schedule is efficient in an asymptotic sense we then explore the notion 
traditional theory of child language acquisition center around theexistence of a language acquisition device which is specifically tunedfor learning a particular class of language more recent proposalssuggest that language acquisition is assisted by the evolution oflanguages towards form that are easily learnable in this paper we evolve combinatorial language which can be learned by a simplerecurrent network quickly and from relatively few example additionally we evolve 
i present a simple variation of importance sampling that explicitly search for important region in the target distribution i prove that the technique yield unbiased estimate and show empirically it can reduce the variance of standard monte carlo estimator this is achieved by concentrating sample in more significant region of the sample space 
belief propagation bp wa only supposed to work for tree like network but work surprisingly well in many application involving network with loop including turbo code however there ha been little understanding of the algorithm or the nature of the solution it find for general graph we show that bp can only converge to a stationary point of an approximate free energy known a the bethe free energy in statistical physic this result characterizes bp fix ed point and make connection with variational approach to approximate inference more importantly our analysis let u build on the progress made in statistical physic since bethe s approximation wa introduced in kikuchi and others have shown how to construct more accurate free energy approximation of which bethe s approximation is the simplest exploiting the insight from our analysis we derive generalized belief propagation gbp version of these kikuchi approximation these new message passing algorithm can be significantly more accurate than ordinary bp at an adjustable increase in complexity we illustrate such a new gbp algorithm on a grid markov network and show that it give much more accurate marginal probability than those found using ordinary bp 
we consider a group of bayesian learner whose interaction with the environment and other agent allow them to improve their model of the dependency among various factor that have influence on their interaction with the environment effective collaboration can improve the performance of isolated individual learner we present a mechanism to pool together the knowledge of many modeler in the domain each of whom may have only partial access to the environment the application domain used in 
determining and setting maximal revenue expectation or other business performance target whether it is for regional company division or individual customer can have profound financial implication operational technique are changed staffing level are altered and management attention is re focused all in the name of expectation in practice these expectation are often derived in an ad hoc manner to address this unsupervised task we combine nearest neighbor method and classical statistical method and derive a new solution to the classical econometric task of frontier analysis we apply our methodology to two real world business problem in verizon a major telecommunication provider in the united state more specifically in the print yellow page division verizon information service identifying under marketed customer for targeted upselling campaign and focused sale attention and benchmarking regional directory division to incent performance improvement our analysis uncovers some commercially useful aspect of these domain and by conservative estimate can increase revenue by several million dollar in each domain 
many interesting problem such a powergrids network switch and traffic flow thatare candidate for solving with reinforcementlearning rl also have property that makedistributed solution desirable we proposean algorithm for distributed reinforcementlearning based on distributing the representationof the value function across node eachnode in the system only ha the ability tosense state locally choose action locally andreceive reward locally the goal of 
in theory the winnow multiplicative update ha certain advantage over the perceptron additive update when there are many irrelevant attribute recently there ha been much effort on enhancing the perceptron algorithm by using regularization leading to a class of linear c lassification method called support vector machine similarly it is al so possible to apply the regularization idea to the winnow algorithm which give method we call regularized winnow we show that the resulting method compare with the basic winnow in a similar way that a support vector machine compare with the perceptron we investigate algorithmic issue and learning property of the derived method some experimental result will also be provided to illustrate different metho d 
this paper present an introduction to reinforcement learning and relational reinforcement learning at a level to be understood by student and researcher with different background it give an overview of the fundamental principle and technique of reinforcement learning without involving a rigorous deduction of the mathematics involved through the use of an example application then relational reinforcement learning is presented a a combination of reinforcement learning with relational learning it advantage such a the possibility of using structural representation making abstraction from specific goal pursued and exploiting the result of previous learning phase are discussed 
we rst show how to represent sharp posterior probability distribu tions using real valued coe cients on broadly tuned basis function then we show how the precise time of spike can be used to con vey the real valued coe cients on the basis function quickly and accurately finally we describe a simple simulation in which spik ing neuron learn to model an image sequence by tting a dynamic generative model 
the structured policy iteration spi algorithm boutilier et al construct structured solution to markov decision problem mdps it find the optimal value function by treating all state which have the same value under a particular policy a a single aggregate state here we show that this approach can also be applied locally in a structured version of prioritized sweeping a model based reinforcement learning algorithm that attempt to focus the learning agent s limited computational resource on state where the model is most likely to be incorrect in structured prioritized sweeping we maintain a structured value function and update the value of aggregate state this increase the scope of local value function change and the rate at which they propagate across the state space which make learning more efficient particularly in larger problem 
many domain are naturally organized in an abstraction hierarchy or taxonomy where the instance in nearby class in the taxonomy are similar in this paper we provide a general probabilistic framework for clust ering data into a set of class organized a a taxonomy where each class is associated with a probabilistic model from which the data wa generated the clustering algorithm simultaneously optimizes three thing the assignment of data instance to cluster the model associated with the cluster and the struc ture of the abstraction hierarchy a unique feature of our approach is that it utiliz e global optimization algorithm for both of the last two step reducing the sensi tivity to noise and the propensity to local maximum that are characteristic of algor ithms such a hierarchical agglomerative clustering that only take local step we provide a theoretical analysis for our algorithm showing that it converges to a lo cal maximum of the joint likelihood of model and data we present experimental result on synthetic data and on real data in the domain of gene expression and text 
classification and regression are fundamental data mining technique the goal of such technique is to build predictor based on a training dataset and use them to predict the property of new data for a wide range of technique combining predictor built on sample from the training dataset provides lower error rate faster construction or both than a predictor built from the entire training dataset this provides a natural parallelization strategy in which predictor based on sample are built independently and hence concurrently we discus the performance implication for two subclass those in which predictor are independent and those in which knowing a set of predictor reduces the difficulty of finding a new one 
we suggest a nonparametric framework for unsupervised learning of projection model in term of density estimation on quantized sample space the objective is not to optimally reconstruct the data but instead the quantizer is chosen to optimally reconstruct the density of the data for the resulting quantizing density estimator qde we present a general method for parameter estimation and model selection we show how projection set which correspond to traditional unsupervised method like vector quantization or pca appear in the new framework for a principal component quantizer we present result on synthetic and realworld data which show that the qde can improve the generalization of the kernel density estimator although it estimate is based on significantly lower dimensional projection index of the data 
ockham s razor is a powerful rule that ha impregnated western logical method of challenging problem and ha fostered the development of science in this essay i discus briefly the historical origin of this epistemological rule during the middle age i suggest that ockham s razor may represent a cognitive ability which ha been applied by several thinker since before medieval time furthermore i suggest that the application of ockham s razor ha had an important role both in the development and rejection of new revolutionary theory 
these are node type univariate v multivariate branching factor two or more grouping of class into two if the tree is binary error impurity measure and the method for minimization to find the best split vector and threshold we then propose a new decision tree induction method that we name linear discriminant tree ldt which us the best combination of these criterion in term of accuracy simplicity and learning time this tree induction method can be univariate or multivariate the method ha a supervised outer optimization layer for converting a k class problem into a sequence of two class problem and each two class problem is solved analytically using fisher s linear discriminant analysis lda on twenty datasets from the uci repository we compare the linear discriminant tree with the univariate decision tree method c and c multivariate decision tree method cart oc quest neural tree and lmdt our proposed linear discriminant tree learn fast are accurate and the tree generated are small 
kernel method like support vector machine have successfully been used for text categorization a standard choice of kernel function ha been the inner product between the vector space representation of two document in analogy with classical information retrieval ir approach latent semantic indexing lsi ha been successfully used for ir purpose a a technique for capturing semantic relation between term and inserting them into the similarity measure between two document one of it main drawback in ir is it computational cost in this paper we describe how the lsi approach can be implemented in a kernel defined feature space we provide experimental result demonstrating that the approach can significantly improve performance and that it doe not impair it 
a lightweight rule induction method is describedthat generates compact disjunctivenormal form dnf rule each class hasan equal numberofunweighted rule a newexample is classified by applying all rule andassigning the example to the class with themost satisfied rule the induction methodattempts to minimize the training error withno pruning an overall design is specifiedby setting limit on the size and numberof rule during training case are adaptivelyweighted 
we present a monte carlo algorithm for learning to act in partially observable markov decision process pomdps with real valued state and action space our approach us importance sampling for representing belief and monte carlo approximation for belief propagation a reinforcement learning algorithm value iteration is employed to learn value function over belief state finally a samplebased version of nearest neighbor is used to generalize across state initial empirical result suggest that our approach work well in practical application 
gaussian mixture or so called radial basis function network fordensity estimation provide a natural counterpart to sigmoidal neuralnetworks for function tting and approximation in both case it is possible to give simple expression for the iterative improvementof performance a component of the network are introducedone at a time in particular for mixture density estimation we showthat a k component mixture estimated by maximum likelihood orby an iterative likelihood 
we present a general framework for discriminative estimation based on the maximum entropy principle and it extension all calculation involve distribution over structure and or parameter rather than specific setting and reduce to relative entropy projection this hold even when the data is not separable within the chosen parametric class in the context of anomaly detection rather than classification or when the label in the training set are uncertain or incomplete support vector machine are naturally subsumed under this class and we provide several extension we are also able to estimate exactly and efficiently discriminative distribution over tree structure of class conditional model within this framework preliminary experimental result are indicative of the potential in these technique 
we describe an iterative algorithm for building vector mach ines used in classification task the algorithm build on idea from sup port vector machine boosting and generalized additive model the algorithm can be used with various continuously differential function t hat bound the discrete classification loss and is very simple to impl ement we test the proposed algorithm with two different loss function on synthetic and natural data we also describe a norm penalized version of the algorithm for the exponential loss function used in adaboost the performance of the algorithm on natural data is comparable to support vecto r machine while typically it running time is shorter than of svm 
when constructing a classier the probability of correct classi cation of future data point should be maximized in the currentpaper this desideratum is translated in a very direct way into anoptimization problem which is solved using method from convexoptimization we also show how to exploit mercer kernel inthis setting to obtain nonlinear decision boundary a worst casebound on the probability of misclassication of future data is obtainedexplicitly 
the standard reinforcement learning view of the involvement of neuromodulatory system in instrumental conditioning includes a rather straightforward conception of motivation a prediction of sum future reward competition between action is based on the motivating characteristic of their consequent state in this sense substantial careful experiment reviewed in dickinson amp balleine into the neurobiology and psychology of motivation show that this view is incomplete in many case 
this paper introduces multiple instance regression a variant of multiple regression in which each data point may be described by more than one vector of value for the independent variable the goal of this work are to understand the computational complexity of the multiple instance regression task and develop an ecien t algorithm that is superior to ordinary multiple regression when applied to multiple instance data set 
we propose a novel method for the analysis of sequential data that exhibit an inherent mode switching in particular the data might be a non stationary time series from a dynamical system that switch between multiple operating mode unlike other approach our method process the data incrementally and without any training of internal parameter we use an hmm with a dynamically changing number of state and an on line variant of the viterbi algorithm that performs an unsupervised segmentation and classication of the data on the y i e the method is able to process incoming data in real time the main idea of the approach is to track and segment change of the probability density of the data in a sliding window on the incoming data stream the usefulness of the algorithm is demonstrated by an application to a switching dynamical system 
the vicinal risk minimization principle establishes a bridge between generative model and method derived from the structural risk minimization principle such a support vector machine or statistical regularization we explain how vrm provides a framework which integrates a number of existing algorithm such a parzen window support vector machine ridge regression constrained logistic classifier and tangent prop we then show how the approach implies new algorithm for solving problem usually associated with generative model new algorithm are described for dealing with pattern recognit ion problem with very different pattern distribution and dealing with unlabeled data preliminary empirical result are presented 
we present an algorithm that induces a class of model with thin junction tree model that are characterized by an upper bound on the size of the maximal clique of their triangulated graph by ensurin g that the junction tree is thin inference in our model remains tract able throughout the learning process this allows both an efficient implemen tation of an iterative scaling parameter estimation algorithm and al so ensures that inference can be performed efficiently with the final model w e illustrate the approach with application in handwritten digit recogn ition and dna splice site detection 
a few year ago pearl s probability propagationalgorithmin graph with cycle wa shown to produce excellent result for error correcting turbodecoding ever since we have wondered whether iterative probability propagation could be used successfully for machine learning a a first step in this direction we study iterative inference and learning in the simple factor analyzer network a two layer densely connected network that model bottom layer sensory input a a linear combination of top layer factor plus independent gaussian sensor noise the number of bottom up top down iteration needed to exactly infer the factor given a network and an input scale with the number of factor in the top layer in online learning this iterative procedure must be reinitialized upon each pattern presentation and so learning becomes prohibitively slow in big network such a those used for face recognition and for large scale model of the cortex we show that probabilitypropagation in a factor analyzer usually take just a few iteration to achieve a low inference error even in network with sensor and factor we derive an expression for the algorithm s fixed point and provide an eigenvalue condition for globalconvergence we also show how iterativeinference can be used to do online learning and give result on using this method to do online dimensionalityreduction for the purpose of recognizing pixelimages of face using a dimensional subspace this work suggests that iterative probability propagation in densely connected network may lead to a broad class of useful algorithm for machine learning 
using statistical mechanic result i calculate learning curve average generalization error for gaussian process gps and bayesian neural network nns used for regression applying the result to learning a teacher defined by a two layer network i can directly compare gp and bayesian nn learning i find that a gp in general requires training example to learn input feature of order is the input dimension whereas a nn can learn the task with order the number of adjustable weight training example since a gp can be considered a an infinite nn the result show that even in the bayesian approach it is important to limit the complexity of the learning machine the theoretical finding are confirmed in simulation with analytical gp learning and a nn mean field algorithm 
support vector machine have met with significant success in numerous real worldlearning task however like most machine learning algorithm they are generally appliedusing a randomly selected training set classified in advance in many setting we also havethe option of using pool based active learning instead of using a randomly selected trainingset the learner ha access to a pool of unlabeled instance and can request the label for somenumber of them we introduce a new 
abstract classifier system are now viewed disappointing because of their problem such a the rule strength v rule set performance problem and the credit assignment problem in order to solve the problem we have developed a hybrid classifier system gls generalization learning system in designing gls we view cs a model free learning in pomdps and take a hybrid approach to finding the best generalization given the total number of rule gls us the policy improvement procedure by jaakkola et al for an locally optimal stochastic policy when a set of rule condition is given gls us ga to search for the best set of rule condition 
reinforcement learning agent attempt to learn and construct a decision policy which maximises some reward signal in turn this policy is directly derived from long term value estimate of state action pair in environment with real valued state space however it is impossible to enumerate the value of every state action pair necessitating the use of a function approximator in order to infer state action value from similar state typically function approximators require many parameter 
data mining research ha long concentrated on the five main area clustering association discovery classification forecasting and sequential pattern web data mining project are concerned mainly with text mining user segmentation forecasting web usage and analyzing user clickstream pattern we present a new type of web usage mining called funnel analysis or funnel report mining a funnel report is a study of the retention behavior among a series of page or site for example of all hit on the home page of www msn com what percentage of those are followed by hit to moneycentral msn com what percentage of www msn com hit are followed by moneycentral msn com and then www msnbc com what are the most interesting funnel starting with www msn com where doe the greatest drop off rate occur after a user ha hit msnbc funnel report are extremely useful in e business because they give product planner an idea of how usable and well structured their site is from our experience performing web usage mining for the msn network of site funnel report are requested even more than user segmentation analysis site affiliation study and classification exercise in this paper we define a framework for funnel analysis and provide a tree based solution we have been using successfully to extract all relevant funnel using only one scan of the data file 
preliminary work by the author made use of the so called manhattan world assumption about the scene statistic of city and indoor scene this assumption stated that such scene were built on a cartesian grid which led to regularity in the image edge gradient statistic in this paper we explore the general applicability of this assumption and show that surprisingly it hold in a large variety of le structured environment including rural scene this enables u from a single image to determine the orientation of the viewer relative to the scene structure and also to detect target object which are not aligned with the grid these inference are performed using a bayesian model with probability distribution e g on the image gradient statistic learnt from real data 
stimulus array are inevitably presented at different position on the retina in visual task even those that nominally require fixation in par ticular this applies to many perceptual learning task we show that per ceptual inference or discrimination in the face of positional variance ha a structurally different quality from inference about fixed position stimulus involving a particular quadratic non linearity rather than a purely lin ear discrimination we show the advantage taking this non linearity into account ha for discrimination and suggest it a a role for recurrent con nections in area v by demonstrating the superior discrimination perfor mance of a recurrent network we propose that learning the feedforward and recurrent neural connection for these task corresponds to the fast and slow component of learning observed in perceptual learning task 
recurrent neural network of analog unit are computer for realvaluedfunctions we study the time complexity of real computationin general recurrent neural network these have sigmoidal linear and product unit of unlimited order a node and no restrictionson the weight for network operating in discrete time we exhibit a family of function with arbitrarily high complexity and we derive almost tight bound on the time required to computethese function thus evidence is 
independent component analysis of natural image lead to emergence of simple cell property i e linear filter that resemble wavelet or gabor function in this paper we extend ica to explain further property of v cell first we decompose natural image into independent subspace instead of scalar component this model lead to emergence of phase and shift invariant feature similar to those in v complex cell second we define a topography between the linear component obtained 
in this paper we explore two quantitative approach to the modelling of counterfactual reasoning a linear and a noisy or model based on information contained in conceptual dependency network empirical data is acquired in a study and the fit of the model compared to it we conclude by considering the appropriateness of non parametric approach to counterfactual reasoning and examining the prospect for other parametric approach in the future 
in this paper we describe inca an adaptive advisable assistant for crisis response the system let user guide the search towardparticular schedule by giving highlevel operational advice about the solutionsdesired because trace of user interactionsprovide information regarding theuser s preference among schedule incacan draw on machine learning techniquesto construct user model that reflect thesepreferences we characterize the modelingtask a that of learning a 
theory of object recognition often assume that only one representation scheme is used within one visual processing pathway versatility of the visual system come from having multiple visual processing pathway each specialized in a different category of object we propose a theoretically simpler alternative capable of explaining the same set of data and more a single primary visual processing pathway loosely modular is assumed memory module are attached to site along this pathway object identity decision is made independently at each site a site s response time is a monotonic decreasing function of it confidence regarding it decision an observer s response is the first arriving response from any site the effective representation s of such a system determined empirically can appear to be specialized for different task and stimulus consistent with recent clinical and functional imaging finding this however merely reflects a decision being made at it appropriate level of abstraction the system itself is intrinsically flexible and adaptive 
ensemble method have recently garnered a great deal of attention in the machine learning community technique such a boosting and bagging have proven to be highly effective but require repeated resampling of the training data making them inappropriate in a data mining context the method presented in this paper take advantage of plentiful data building separate classifier on sequential chunk of training point these classifier are combined into a fixed size ensemble using a heuristic replacement strategy the result is a fast algorithm for large scale or streaming data that classifies a well a a single decision tree built on all the data requires approximately constant memory and adjusts quickly to concept drift 
many cachine learning application require classifier that minimize an asymmetric cost function rather than the misclassification rate and several recent paper have addressed this problem however these paper have either applied no statistical testing or have applied statistical method that are not appropriate for the cost sensititve setting without good statistical method it is difficult to tell whether these new cost sensastive method are better than existing method that ignore cost and it is also difficult to tell whether one cost sensitive method is better than another to rectify this problem this paper present two statistical method for the cost sensitive setting the first construct a confidence interval for the expected cost of a single classifier the second construct a confidence interval for the expected difference in cost of two classifier in both case the basic idea is to separate the problem of estimating the probability of each cell in the confusion matrix which is independent of the cost matrix from the problem of computing the expected cost we show experimentally that these bootstrap test work better than applying standard z test based on the normal distribution 
we study a population decoding paradigm in which the maximum likelihood inference is based on an unfaithful decoding model umli this is usually the case for neural population decoding because the encoding process of the brain is not exactly known or because a simplified decoding model is preferred for saving computational cost we calculate the decoding error of umli and show an example of an unfaithful model which neglect the neuronal correlation the perf ormance of umli is compared with that of the maximum likelihood inference based on a faithful model and that of the center of mass decoding method it turn out that umli ha advantage of decreasing the computational complexity remarkablely and maintaining a high level decoding accuracy at the same time 
we study using reinforcement learning in dynamic environment such environment may contain many dynamic object which make optimal planning hard one way of using information about all dynamic object is to expand the state description but this result in a high dimensional policy space our approach is to instantiate information about dynamic object in the model of the environment and to replan using model based reinforcement learning whenever this information change furthermore our 
one of the most powerful and widely acceptedanalytical formalism for modeling biologicaland physical system is that of thepartial dierential equation pde establishingan acceptable pde model for a dynamicsystem occupies a major portion of thework of the mathematical modeler there aretwo main aspect to this activity first anappropriate structure ha to be determinedfor the equation involved the model identication problem second acceptably accuratevalues for 
identification of outlier can lead to the discovery of unexpected interesting and useful knowledge existing method are designed for detecting spatial outlier in multidimensional geometric data set where a distance metric is available in this paper we focus on detecting spatial outlier in graph structured data set we define statistical test analyze the statistical foundation underlying our approach design several fast algorithm to detect spatial outlier and provide a cost model for outlier detection procedure in addition we provide experimental result from the application of our algorithm on a minneapolis st paul twin city traffic dataset to show their effectiveness and usefulness 
abstract we present a variational bayesian method for model selection over family of kernel classiers like support vector machine or gaussian process the algorithm need no user interaction and is able to adapt a large number of kernel parameter to given data without having to sacrice training case for validation this open the possibility to use sophisticated family of kernel in situation where the small standard kernel class are clearly inappropriate we relate the method to other work done on gaussian process and clarify the relation between support vector machine and certain gaussian process model 
we investigate the problem of classifying individual based on estimated density functionsfor each individual given labelled histogram characterizing red blood cell rbc fordifferent individual the learning problem is to build a classifier which can classify newunlabelled histogram into normal and iron deficient class thus the problem is similarto conventional classification in that there is labelled training data but different in thatthe underlying measurement are not 
learning curve for gaussian process regression are well understood when the student model happens to match the teacher true data generation process i derive approximation to the learning curve for the more generic case of mismatched model and find very rich behaviour for large input space dimensionality where the result become exact there are universal studentindependent plateau in the learning curve with transition in between that can exhibit arbitrarily many over fitting maximum in lower dimension plateau also appear and the asymptotic decay of the learning curve becomes strongly student dependent all prediction are confirmed by simulation 
the paper present a novel technique of constrained independent component analysis cica to introduce constraint into the classical ica and solve the constrained optimization problem by using lagrange multiplier method this paper show that cica can be used to order the resulted independent component in a specific manner and normalize the demixing matrix in the signal separation procedure it can systematically eliminate the ica s indeterminacy on permutation and dilation the experiment 
by thinking of each state in a hidden markov model a corresponding to some spatial region of a fictitious topology space it is possible to naturally define neighbouring state a those which are connected in that space the transition matrix can then be constrained to allow transition only between neighbour this mean that all valid state sequence correspond to connected path in the topology space i show how such constrained hmms can learn to discover underlying structure in complex sequence of high dimensional data and apply them to the problem of recovering mouth movement from acoustic in continuous speech latent variable model for structured sequence data structured time series are generated by system whose underlying state variable change in a continuous way but whose state to output mapping are highly nonlinear many to one and not smooth probabilistic unsupervised learning for such sequence requires model with two essential feature latent hidden variable and topology in those variable hidden markov model hmms can be thought of a dynamic generalization of discrete state static data model such a gaussian mixture or a discrete state version of linear dynamical system ldss which are themselves dynamic generalization of continuous latent variable model such a factor analysis while both hmms and ldss provide probabilistic latent variable model for time series both have important limitation traditional hmms have a very powerful model of the relationship between the underlying state and the associated observation because each state store a private distribution over the output variable this mean that any change in the hidden state can cause arbitrarily complex change in the output distribution however it is extremely difficult to capture reasonable dynamic on the discrete latent variable because in principle any state is reachable from any other state at any time step and the next state depends only on the current state ldss on the other hand have an extremely impoverished representation of the output a a function of the latent variable since this transformation is restricted to be global and linear but it is somewhat easier to capture state dynamic since the state is a multidimensional vector of continuous variable on which a matrix flo w is acting this enforces some continuity of the latent variable across time constrained hidden markov model address the modeling of state dynamic by building some topology into the hidden state representation the essential idea is to constrain the transition parameter of a conventional hmm so that the discretevalued hidden state evolves in a structured way in particular below i consider parameter restriction which constrain the state to evolve a a discretized version of a continuous multivariate variable i e so that it inscribes only connected path in some space this lends a physical interpretation to the discrete state trajectory in an hmm a standard trick in traditional speech application of hmms is to use left to right transition matrix which are a special case of the type of constraint investigated in this paper however leftto right bakis hmms force state trajectory that are inherently one dimensional and uni directional whereas here i also consider higher dimensional topology and free omni directional motion 
the proliferation of hypertext and the popularity of kleinberg s hit algorithm have brought about an increased inter est in link analysis while hit and it older relative from the bibliometrics literature provide a method for finding au thoritative source on a particular topic they do not allow individual user to inject their own opinion about what sourc e are authoritative this paper present a technique that inc orporates user feedback by adjusting the measure of authority to match an individual s internal notion of what source are important by lifting the authority of a few user specifie d source the eigenvectors of the entire link matrix are realigned resulting in a computationally cheap method that is much more rich than simple spreading activation we present experimental result based on a database of about one million reference collected a part of the cora on line index of the computer science literature 
prior knowledge about video structure can be used both a a mean to improve the performance of content analysis and to extract feature tha t allow semantic classification we introduce statistical model fo r two important component of this structure shot duration and activi ty and demonstrate the usefulness of these model by introducing a bayesian formulation for the shot segmentation problem the new formul ations is shown to extend standard thresholding method in an adaptive an d intuitive way leading to improved segmentation accuracy 
this paper proposes an approach to classification of adjacent segment of a time series a being either of k class we use a hierarchical model that consists of a feature extraction stage and a generative classifier which is built on top of these feature such two stage approach are often used in signal and image processing the novel part of our work is that we link these stage probabilistically by using a latent feature space to use one joint model is a bayesian requirement which ha the advantage to fuse information according to it certainty the classifier is implemented a hidden markov model with gaussian and multinomial observation distribution defined on a suitably chosen representation of autoregressive model the markov dependency is motivated by the assumption that successive classification will be correlated inference is done with markov chain monte carlo mcmc technique we apply the proposed approach to synthetic data and to classification of eeg that wa recorded while the subject performed different cognitive task all experiment show that using a latent feature space result in a significant improvement in generalization accuracy hence we expect that this idea generalizes well to other hierarchical model 
abstract we introduce the problem of discovering functional determinacies that result from rolling up data to a higher abstraction leveldependency rud an example rud is the probability that two file in the same directory have the same file extension is greater than a specific number we show the applicability of ruds for olap and data mining we consider the problem of mining ruds that satisfy specified support and confidence threshold this problem is np hard in the number of attribute we give an algorithm for this problem experimental result show that the algorithm us linear time in the number of tuples of the input database 
the mutual information of two random variable i and j with joint probability t ij is commonly used in learning bayesian net a well a in many other field the chance t ij are usually estimated by the empirical sampling frequency n ij n leading to a point estimate i n ij n for the mutual information to answer question like is i n ij n consistent with zero or what is the probability that the true mutual information is much larger than the point estimate one ha to go beyond the point estimate in the bayesian framework one can answer these question by utilizing a second order prior distribution p t comprising prior information about t from the prior p t one can compute the posterior p t n from which the distribution p i n of the mutual information can be calculated we derive reliable and quickly computable approximation for p i n we concentrate on the mean variance skewness and kurtosis and non informative prior for the mean we also give an exact expression numerical issue and the range of validity are discussed 
the problem of multiple global comparison infamilies of biological sequence ha been wellstudied fewer algorithm have been developedfor identifying local consensus patternsor motif in biological sequence these twoimportant problem have different biologicalconstraints and consequently different computationalapproaches the difficulty of findingthe biologically meaningful motif resultsfrom the variation among motif base the alignment of motif position site 
we study property of popular near uniform dirichlet prior for learning undersampled probability distribution on discrete nonmetric space and show that they lead to disastrous result however an occam style phase space argument expands the prior into their infinite mixture and resolve most of the observed problem this lead to a surprisingly good estimator of entropy of discrete distribution 
this paper discus theoretical and experimental aspect of gradient based approach to the direct optimization of policy performance in controlled s we introduce a like algorithm for estimating an approximation to the gradient of the average reward a a function of the parameter of a stochastic policy the algorithm s chief advantage are that it requires only a single sample path of the underlying markov chain it us only one free parameter which ha a natural interpretation in term of bias variance trade off and it requires no knowledge of the underlying state we prove convergence of and show how the gradient estimate produced by can be used in a conjugate gradient procedure to find local optimum of the average reward 
the information bottleneck methodis an unsupervised non parametric data organization technique given a joint distribution this method construct a new variable that extract partition or cluster over the value of that are informative about in a recent paper we introduced a general principled framework for multivariate extension of the information b ottleneck method that allows u to consider multiple system of data partition th at are inter related in this paper we present a new family of simple agglomerative algorithm to construct such system of inter related cluster we analyze t he behavior of these algorithm and apply them to several real life datasets 
we provide a natural gradient method that represents the steepestdescent direction based on the underlying structure of the parameterspace although gradient method cannot make large changesin the value of the parameter we show that the natural gradientis moving toward choosing a greedy optimal action rather thanjust a better action these greedy optimal action are those thatwould be chosen under one improvement step of policy iterationwith approximate compatible value 
numerical function approximation over a boolean domain is a classical problem with wide application to data modeling task and various form of learning a great many function approximation algorithm have been devised over the year because the goal is to produce an approximating function that ha low expected error algorithm are typically guided by error reduction this guiding force to reduce error can bias the algorithm in a detrimental manner we illustrate this bias and then propose an alternative approach based on a notion of value unification 
the success of the world wide web measured in term of the number of it user and of the resulting traffic increase is only commensurate to the patience required when sitting in front of one s computer waiting for a document to be down loaded if one could identify the typical access pattern for a set of document on a web server the server could use or extend the existing protocol to accordingly pre fetch or push document to the browser and proxy server in this paper we present and evaluate a strategy for making web server pushier which document is to be pushed is determined by a set of association rule mined from a sample of the access log of the web server once a rule of the form document a document b ha been identified and selected the web server decides to push document if document is requested the strategy is individual user oriented while not ignoring the aggregate perspective we evaluate the effectiveness and the cost of such a strategy for two architecture a two tier web server web browser architecture and a three tier web server proxy server web browser architecture we consider different setting in the architecture a well a refinement of the strategy taking into account the size of the document 
the partition function for a boltzmann machine can be boundedfrom above and below we can use this to bound the mean andthe correlation for network with small weight the value ofthese statistic can be restricted to non trivial region i e a subsetof gamma experimental result show that reasonable boundingoccurs for weight size where mean field expansion generally givegood result 
we introduce the mixture of gaussian process mgp model which is useful for application in which the optimal bandwidth of a map is input dependent the mgp is derived from the mixture of expert model and can also be used for modeling general conditional probability density we discus how gaussian process in particular in form of gaussian process classi cation the support vector machine and the mgp model can be used for quantifying the dependency in graphical model 
in this paper we shall focus here on mixturesof factor analyzer from the perspectiveof a method for model based density estimationfrom high dimensional data and hencefor the clustering of such data this modelenables a normal mixture model to be ttedto high dimensional data the number of freeparameters is controlled through the dimensionof the latent factor space by workingin this reduced space it allows an interpolationin model complexity from isotropic tofull 
this paper is concerned with extendingneural network to multi instance learning in multi instance learning each example corresponds to a set of tuples in asingle relation furthermore example are classied a positive if at least onetuple i e at least one attribute value pair satises certain condition if none ofthe tuples satisfy the requirement the example is classied a negative we willstudy how to extend standard neural network and backpropagation to multiinstance 
we describe a class of probabilistic model that we call credibility network using parse tree a internal representation of image credibility network are able to perform segmentation and recog nition simultaneously removing the need for ad hoc segmentation heuristic promising result in the problem of segmenting hand written digit were obtained 
swiftfile is an intelligent assistant that help user organize their e mail into folder swiftfile us a text classifier to predict where each new message is likely to be filed by the user and provides shortcut button to quickly file message into one of it predicted folder one of the challenge faced by swiftfile is that the user s mail filing habit are constantly changing user are frequently creating deleting and rearranging folder to meet their current filing need in this paper we discus the importance of incremental learning in swiftfile we present several criterion for judging how well incremental learning algorithm adapt to quickly changing data and evaluate swiftfile s classifier using these criterion we find that swiftfile s classifier is surprisingly responsive and doe not require the extensive training that is often assumed in most learning system 
in this work we introduce an information theoretic based correction term to the likelihood ratio classification method for multiple class under certain condition the term is sufficient for optimally correcting the difference between the true and estimated likelihood ratio and we analyze this in the gaussian case we find that the new correction term significantly improves the classification result when tested on medium vocabulary speech recognition task moreover the addition of this term make the class comparison analogous to an intransitive game and we therefore use several tournament like strategy to deal with this issue we find that further small improvement are obtained by using an appropriate tournament lastly we find that intransitivity appears to be a good measure of classification confidence 
we present an improvement of noviko s perceptron convergencetheorem reinterpreting this mistake bound a a margin dependentsparsity guarantee allows u to give a pacstyle generalisation errorbound for the classier learned by the perceptron learning algorithm the bound value crucially depends on the margin a supportvector machine would achieve on the same data set using the samekernel ironically the bound yield better guarantee than are currentlyavailable for the support 
abstract we introduce a method of feature selection for support vector machine the method is based upon finding those feature which minimize bound on the leave one out error this search can be efficiently performed via gradient descent the resulting algorithm are shown to be superior to some standard feature selection algorithm on both toy data and real life problem of face recognition pedestrian detection and analyzing dna microarray data 
abstract a robust integrative algorithm is presented for computing the position of the focus of expansion or axis of rotation the singular point in optical flow field such a those generated by self motion measurement are shown of a fully parallel cmos analog vlsi motion sensor array which computes the direction of local motion sign of optical flow at each pixel and can directly implement this algorithm the flow field singular point is computed in real time with a power consumption of le than computation of the singular point for more general flow field requires measure of field expansion and rotation which it is shown can also be computed in real time hardware again using only the sign of the optical flow field these measure along with the location of the singular point provide robust real time self motion information for the visual guidance of a moving platform such a a robot 
realistic application of nearest neighborclassiers suer from capacity related problem the size of today s data warehousesrenders loading the entire data into the mainmemory impossible moreover comparingeach object with million of stored examplescan be prohibitively expensive some researchershave therefore developed methodsthat replace large set of example with theirrepresentative subset in this paper we suggestan approach that selects three very smallgroups of 
most real world data is stored in relational form in contrast most statistical learning method work with flat data representation forcing u to convert our data into a form that loses much of the relational structure the recently introduced framework of probabilistic relational model prms allows u to represent probabilistic model over multiple entity that utilize the relation between them in this paper we propose the use of probabilistic model not only for the attribute in a relational model but for the relational structure itself we propose two mechanism for modeling structural uncertainty reference uncertainty and existence uncertainty we describe the appropriate condition for using each model and present learning algorithm for each we present experimental result showing that the learned model can be used to predict relational structure and moreover the observed relational structure can be used to provide better prediction for the attribute in the model 
an important issue in applying svms to speech recognition is the ability to classify variable length sequence this paper present extension to a standard scheme for handling this variable length data the fisher score a more discriminatory mapping is introduced based on the likelihood ratio the score space de ned by this mapping avoids some of the problem with the fisher score it also allows the discriminative power of the generative model to be directly incorporated into the 
in this paper we propose a general framework for distributed boosting intended for efficient integrating specialized classifier learned over very large and distributed homogeneous database that cannot be merged at a single location our distributed boosting algorithm can also be used a a parallel classification technique where a massive database that cannot fit into main computer memory is partitioned into disjoint subset for a more efficient analysis in the proposed method at each boosting round the classifier are first learned from disjoint datasets and then exchanged amongst the site finally the classifier are combined into a weighted voting ensemble on each disjoint data set the ensemble that is applied to an unseen test set represents an ensemble of ensemble built on all distributed site in experiment performed on four large data set the proposed distributed boosting method achieved classification accuracy comparable or even slightly better than the standard boosting algorithm while requiring le memory and le computational time in addition the communication overhead of the distributed boosting algorithm is very small making it a viable alternative to the standard boosting for large scale database 
we propose a general approach for estimating the parameter of latentvariable probability model to maximize conditional likelihoodand discriminant criterion unlike joint likelihood these objectivesare better suited for classification and regression the approachutilizes and extends the previously introduced cem framework conditional expectation maximization which reformulates emto handle the conditional likelihood case we generalize the cemalgorithm to estimate any mixture of 
we present an algorithm that sample the hypothesis space of kernelclassifiers given a uniform prior over normalised weight vectorsand a likelihood based on a model of label noise lead to a piecewiseconstant posterior that can be sampled by the kernel gibbssampler kg the kg is a markov chain monte carlo methodthat chooses a random direction in parameter space and samplesfrom the resulting piecewise constant density along the line chosen the kg can be used a an analytical 
the nonnegative boltzmann machine nnbm is a recurrent neural network model that can describe multimodal nonnegative data application of maximum likelihood estimation to this model give a learning rule that is analogous to the binary boltzmann machine we examine the utility of the mean field approximation for the nnbm and describe how monte carlo sampling technique can be used to learn the parameter of the nnbm reflective slice sampling is particularly well suited for this distribution and can efficiently be implemented to sample the distribution we illustrate learning of the nnbm on a translationally invariant distribution a well a on a generative model for image of human face 
the support vector machine svm is a state of the art techniquefor regression and classication combining excellent generalisationproperties with a sparse kernel representation however it doessuer from a number of disadvantage notably the absence of probabilisticoutputs the requirement to estimate a trade o parameterand the need to utilise mercer kernel function in this paper weintroduce the relevance vector machine rvm a bayesian treatmentof a generalised linear 
in this paper we propose a novel method for nonlinear non gaussian on line estimation the algorithm consists of a particle filter that us an unscented kalman filter ukf to generate the importance proposal distribution the ukf allows the particle filter to incorporate the latest observation into a prior updating routine in addition the ukf generates proposal distribution that match the true posterior more closely and also ha the capability of generating heavier tailed distribution 
a feature of data mining that distinguishes it from quot classical quot machine learning ml and statistical modeling sm is scale the community seems to agree on this yet progress to this point ha been limited we present a methodology that address scale in a novel fashion that ha the potential for revolutionizing the field while the methodology applies most directly to flat row by column data set we believe that it can be adapted to other representation our approach to the problem is not 
gradient based policy search is an alternative to value fu nction based method for reinforcement learning in non markovian domain one apparent drawback of policy search is it requirement that all action be on policy that is that there be no explicit exploration in this paper we provide a method for using importance sampling to allow any wellbehaved directed exploration policy during learning we show both theoretically and experimentally that using this method can achieve dramatic performance improvement 
source separation or computational auditory scene analysis attempt to extract individual acoustic object from input which contains a mixture of sound from different source altered by the acoustic environment unmixing algorithm such a ica and it extension recover source by reweighting multiple observation sequence and thus cannot operate when only a single observation signal is available i present a technique called refiltering which recovers source by a nonstationary reweighting masking of frequency sub band from a single recording and argue for the application of statistical algorithm to learning this masking function i present result of a simple factorial hmm system which learns on recording of single speaker and can then separate mixture using only one observation signal by computing the masking function and then refiltering 
we introduce the notion of kernel alignment a measure of similarity between two kernel function or between a kernel and a target function this quantity capture the degree of agreement between a kernel and a given learning task and ha very natural interpretation in machine learning leading also to simple algorithm for model selection and learning we analyse it theoretical property proving that it is sharply concentrated around it expected value and we discus it relation with 
recent interpretation of the adaboost algorithm view it a performinga gradient descent on a potential function simply changingthe potential function allows one to create new algorithm relatedto adaboost however these new algorithm are generallynot known to have the formal boosting property this paper examinesthe question of which potential function lead to new algorithmsthat are booster the two main result are general setsof condition on the potential one set implies 
this dissertation examines the use of partial programming a a mean of designing agent for large markov decision problem in this approach a programmer specifies only that which they know to be correct and the system then learns the rest from experience using reinforcement learning in contrast to previous low level language for partial programming this dissertation present alisp a lisp based high level partial programming language alisp allows the programmer to constrain the policy considered by a learning process and to express his or her prior knowledge in a concise manner optimally completing a partial alisp program is shown to be equivalent to solving a semi markov decision problem smdp under a finite memory use condition online learning algorithm for alisp are proved to converge to an optimal solution of the smdp and thus to an optimal completion of the partial program this dissertation then present method for exploiting the modularity allows an agent to ignore aspect of it current state that are irrelevant to it current decision and therefore speed up reinforcement learning by decomposing representation of the value of action along subroutine boundary optimality i e optimality among all policy consistent with the partial program these method are demonstrated on two simulated taxi task function approximation a method for representing the value of action allows reinforcement learning to be applied to problem where exact method are intractable soft shaping is a method for guiding an agent toward a solution without constraining the search space both can be integrated with alisp alisp with function approximation and reward shaping is successfully applied on a difficult continuous variant of the simulated taxi task together the method presented in this work comprise a system for agent design that allows the programmer to specify what they know hint at what they suspect using soft shaping and leave unspecified that which they don t know the system then optimally completes the program through experience and take advantage of the hierarchical structure of the specified program to speed learning 
we propose a novel probabilistic framework for semantic video indexing we define probabilistic multimedia object multijects to map low level medium feature to high level semantic label a graphical network of such multijects multinet capture scene context by discovering intra frame a well a inter frame dependency relation between the concept the main contribution is a novel application of a factor graph framework to model this network we model relation between semantic concept in term of their co occurrence a well a the temporal dependency between these concept within video shot using the sum product algorithm for approximate or exact inference in these factor graph multinets we attempt to correct error made during isolated concept detection by forcing high level constraint this result in a significant improvement in the overall detection performance 
we present a probabilistic method for fusion of image producedby multiple sensor the approach is based on an image formationmodel in which the sensor image are noisy locally linear functionsof an underlying true scene a bayesian framework then providesfor maximum likelihood or maximum a posteriori estimate of thetrue scene from the sensor image maximum likelihood estimatesof the parameter of the image formation model involve local second order image statistic and 
both document clustering and word clustering are well studied problem most existing algorithm cluster document and word separately but not simultaneously in this paper we present the novel idea of modeling the document collection a a bipartite graph between document and word using which the simultaneous clustering problem can be posed a a bipartite graph partitioning problem to solve the partitioning problem we use a new spectral co clustering algorithm that us the second left and right singular vector of an appropriately scaled word document matrix to yield good bipartitionings the spectral algorithm enjoys some optimality property it can be shown that the singular vector solve a real relaxation to the np complete graph bipartitioning problem we present experimental result to verify that the resulting co clustering algorithm work well in practice 
neocortical circuit are dominated by massive excitatory feedback more than eighty percent of the synapsis made by excitatory cortical neuron are onto other excitatory cortical neuron why is there such massive recurrent excitation in the neocortex and what is it role in cortical computation recent neurophysiological experiment have shown that the plasticity of recurrent neocortical synapsis is governed by a temporally asymmetric hebbian learning rule we describe how such a rule may allow the cortex to modify recurrent synapsis for prediction of input sequence the goal is to predict the next cortical input from the recent past based on previous experience of similar input sequence we show that a temporal difference learning rule for prediction used in conjunction with dendritic back propagating action potential reproduces the temporally asymmetric hebbian plasticity observed physiologically biophysical simulation demonstrate that a network of cortical neuron can learn to predict moving stimulus and develop direction selective response a a consequence of learning the space time response property of model neuron are shown to be similar to those of direction selective cell in alert monkey v 
hidden markov model hmms are a powerful probabilistic tool for modeling sequential data and have been applied with success to many text relatedtasks suchas part of speechtagging text segmentation and information extraction in these case the observation are usually modeled a multinomial distribution over a discrete vocabulary and the hmm parameter are set to maximize the likelihood of the observation this paper present a new markovian sequence model closely related to hmms that allows observation to be represented a arbitrary overlapping feature such a word capitalization formatting part of speech and defines the conditional probability of state sequence given observation sequence it doe this by using the maximumentropyframeworkto fit a set of exponential model that represent the probability of a state given an observation and the previous state we present positive experimental result on the segmentation of faq s 
the hierarchical hidden markov model hhmm is a generalization of the hidden markov model hmm that model sequence with structure at many length time scale fst unfortunately the original inference algorithm is rather complicated and take time where is the length of the sequence making it impractical for many domain in this paper we show how hhmms are a special kind of dynamic bayesian network dbn and thereby derive a much simpler inference algorithm which only take time furthermore by drawing the connection between hhmms and dbns we enable the application of many standard approximation technique to further speed up inferenc e 
the proliferation of hypertext and the popularityof kleinberg s hit algorithm havebrought about an increased interest in linkanalysis while hit and it older relativesfrom the bibliometrics provide a method forfinding authoritative source on a particulartopic they do not allow individual user toinject their own opinion on what source areauthoritative this paper present a techniquefor learning a user s internal model ofauthority we present experimental result 
dynamic control task are good candidate for the application of reinforcement learning technique however many of these task inherently have continuous state or action variable this can cause problem for traditional reinforcement learning algorithm which assume discrete state and action in this paper we introduce an algorithm that safely approximates the value function for continuous state control task and that learns quickly from a small amount of data we give experimental result using this algorithm to learn policy for both a simulated task and also for a real robot operating in an unaltered environment the algorithm work well in a traditional learning setting and demonstrates extremely good learning when bootstrapped with a small amount of human provided data 
algorithm for feature selection fall into two broad category wrapper use thelearning algorithm itself to evaluate the usefulness of feature while filter evaluatefeatures according to heuristic based on general characteristic of the data for applicationto large database filter have proven to be more practical than wrappersbecause they are much faster however most existing filter algorithm only work withdiscrete classification problem this paper describes a fast 
chow and liu introduced an algorithm for fitting a multivariate distribution with a tree i e a density model that assumes that there are only pairwise dependency between variable and that the graph of these dependency is a spanning tree the original algorithm is quadratic in the dimesion of the domain and linear in the number of data point that define the target distribution p this paper show that for sparse discrete data fitting a tree distribution can be done in time and memory that is jointly subquadratic in the number of variable and the size of the data set the new algorithm called the accl algorithm take advantage of the sparsity of the data to accelerate the computation of pairwise marginals and the sorting of the resulting mutual information achieving speed ups of up to order of magnitude in the experiment 
a latent variable generative model with finite noise is used to describe several different algorithm for independent component analysis ica in particular the fixed point ica algorithm is shown to be equivalent to the expectationmaximization algorithm for maximum likelihood under certain constraint allowing the condition for global convergence to be elucidated the algorithm can also be explained by their generic behavior near a singular point where the size of the optimal generative base vanishes an expansion of the likelihood about this singular point indicates the role of higher order correlation in determining the feature discovered by ica the application and convergence of these algorithm are demonstrated on the learning of edge feature a the independent component of natural image 
in many optimization and decision problem the objective function can be expressed a a linear combination of competing criterion the weight of which specify the relative importance of the criterion for the user we consider the problem of learning such a subjective function from preference judgment collected from trace of user interaction we propose a new algorithm for that task based on the theory of support vector machine one advantage of the algorithm is that prior knowledge about the domain can easily be included to constrain the solution we demonstrate the algorithm in a route recommendation system that adapts to the driver s route preference we present experimental result on real user that show that the algorithm performs well in practice 
principal component analysis pca is a commonly applied technique for dimensionalityreduction pca implicitly minimizes a squared loss function which maybe inappropriate for data that is not real valued such a binary valued data thispaper draw on idea from the exponential family generalized linear model andbregman distance to give a generalization of pca to loss function which weargue are better suited to other data type we describe algorithm for minimizingthe loss 
this paper present a unified probabilistic framework for denoising and dereverberation of speech signal the framework transforms the denoising and dereverberation problem into bayes optimal signal estimation the key idea is to use a strong speech model that is pre trained on a large data set of clean speech computational efficiency is achieved by using variational em working in the frequency domain and employing conjugate prior the framework cover both single and multiple microphone we apply this approach to noisy reverberant speech signal and get result substantially better than standard method 
we present a probabilistic generative model for timing deviationsin expressive music performance the structure of the proposedmodel is equivalent to a switching state space model we formulatetwo well known music recognition problem namely tempotracking and automatic transcription rhythm quantization a lteringand maximum a posteriori map state estimation task 
previous biophysical modeling work showed that nonlinear interactionsamong nearby synapsis located on active dendritic tree canprovide a large boost in the memory capacity of a cell mel a b the aim of our present work is to quantify this boost byestimating the capacity of a neuron model with passive dendriticintegration where input are combined linearly across theentire cell followed by a single global threshold and an activedendrite model in which a 
almost two decade ago hopeld showed that network ofhighly reduced model neuron can exhibit multiple attractingxedpoints thus providing a substrate for associative memory it is stillnot clear however whether realistic neuronal network can supportmultiple attractor the main diculty is that neuronal networksin vivo exhibit a stable background state at lowring rate typicallya few hz embedding attractor is easy doing so withoutdestabilizing the background is 
a collective intelligence coin is a set of interacting reinforcement learning rl algorithm designed in an automated fashion so that their collective behavior optimizes a global utility function we summarize the theory of coin then present experiment using that theory to design coin to control internet traffic routing these experiment indicate that coin outperform all previously investigated rl based shortest path routing algorithm 
abstract in many rule induction algorithm it is relatively common that a large number of rule is generated this complexity may harm the comprehensibility of the model without improving it predictive performance for aiding automatic knowledge acquisition and knowledge discovery in classification domain we propose a framework to post process boolean rule obtained from rule induction algorithm this process generates probabilistic rule set with fewer rule and premise while maintaining comparable classification accuracy 
we investigate the application of bayesian network markov random field and mixture model to the problem of query answering for transaction data set we formulate two version of the querying problem the query selectivity estimation i e finding exact count for tuples in a data set and the query generalization problem i e computing the probability that a tuple will occur in new data we show that frequent itemsets are useful for reducing the original data to a compressed representation and introduce a method to store them using an adtree data structure in an extension of our earlier work on this topic we propose several new scheme for query answering based on the compressed representation that avoid direct scan of the data at query time experimental result on real world transaction data set provide insight into various tradeoff involving the offline time for model building the online time for query answering the memory footprint of the compressed data and the accuracy of the estimate provided to the query 
in order to select a good hypothesis language or model from a collection of possible model one ha to ass the generalization performance of the hypothesis which is returned by a learner that is bound to use that model the paper deal with a new and very efficient way of assessing this generalization performance we present an analysis which characterizes the expected generalization error of the hypothesis with least training error in term of the distribution of error rate of the hypothesis in the model this distribution can be estimated very efficiently from the data which immediately lead to an efficient model selection algorithm the analysis predicts learning curve with a very high precision and thus contributes to a better understanding of why and when over fitting occurs we present empirical study controlled experiment on boolean decision tree and a large scale text categorization problem which show that the model selection algorithm lead to error rate which are often a low a those obtained by fold cross validation sometimes even lower however the algorithm is much more efficient because the learner doe not have to be invoiced at all and thus solves model selection problem with a many a a thousand relevant attribute and example 
we look at distributed representation of structure with variable binding that is natural for neural net and allows traditional symbolic representation and processing the representation support learning from example this is demonstrated by taking several instance of the mother of relation implying the parent of relation by encoding them into a mapping vector and by showing that the mapping vector map new instance of mother of into parent of 
we describe a neurally inspired unsupervised learning algorithm that build a non linear generative model for pair of face image from the same individual individual are then recognized by finding the highest relative probability pair among all pair that consist of a test image and an image whose identity is known our method compare favorably with other method in the literature the generative model consists of a single layer of rate coded non linear feature detector and it ha the property that given a data vector the true posterior probability distribution over the feature detector activity can be inferred rapidly without iteration or approximation the weight of the feature detector are learned by comparing the correlation of pixel intensity and feature activation in two phase when the network is observing real data and when it is observing reconstruction of real data generated from the feature activation 
the committee approach ha been proposed for reducing modeluncertainty and improving generalization performance the advantageof committee depends on the performance of individualmembers and the correlational structure of error betweenmembers this paper present an input grouping technique for designing a heterogeneous committee with this technique all inputvariables are first grouped based on their mutual information statisticallysimilar variable are assigned to 
many collection of data do not come packagedin a form amenable to the ready applicationof machine learning technique nevertheless there ha been only limited researchon the problem of preparing raw data forlearning perhaps because widespread di erencesbetween domain make generalizationdi cult this paper focus on one commonclass of raw data in which the entitiesof interest actually comprise collectionsof smaller piece of homologous data wepresent a technique 
a cortical model for motion in depth selectivity of complex cell in the visual cortex is proposed the model is based on a time extension of the phase based technique for disparity estimation we consider the computation of the total temporal derivative of the time varying disparity through the combination of the response of disparity energy unit to take into account the physiological plausibility the model is based on the combination of binocular cell characterized by difierent ocular dominance index the resulting cortical unit of the model show a sharp selectivity for motion indepth that ha been compared with that reported in the literature for real cortical cell 
this paper present a unied bias variance decomposition that is applicable to squared loss zero one loss variable misclassication cost and other loss function the unied decomposition shed light on a number of signican t issue the relation between some of the previously proposed decomposition for zero one loss and the original one for squared loss the relation between bias variance and schapire et al s notion of margin and the nature of the trade o between bias and variance in classication while the biasvariance behavior of zero one loss and variable misclassication cost is quite dieren t from that of squared loss this dierence derives directly from the dieren t denitions of loss we have applied the proposed decomposition to decision tree learning instancebased learning and boosting on a large suite of benchmark data set and made several signican t observation 
we describe a method for improving the classification of short text string using a combination of labeled training data plus a secondary corpus of unlabeled but related longer document we show that such unlabeled background knowledge can greatly decrease error rate particularly if the number of example or the size of the string in the training set is small this is particularly useful when labeling text is a labor intensive job and when there is a large amount of information available about a particular problem on the world wide web our approach view the task a one of information integration using whirl a tool that combine database functionality with technique from the information retrieval literature 
in this paper we will treat input selection for a radial basis function rbf like classifier within a bayesian framework we approximate the a posteriori distribution over both model coecients and input subset by sample drawn with gibbs update and reversible jump move using some public datasets we compare the classification accuracy of the method with a conventional ard scheme these datasets are also used to infer the a posteriori probability of different input subset 
in this paper we analyze the performance of clusteringmethods on the task of constructing communitymodels for the user of large web site 
this long abstract ha been accepted for a full oral presentation at nip ful l paper to appear research supported in part by nih grant ey and nsf grant dm g wahba x lin f gao and d xiang and nih grant ey and ey r klein and b klein the bias variance tradeoff and the randomized gacv grace wahba xiwu lin and fangyu gao 
abstract wepresentapowerfulmeta clusteringtechniquecallediterativedouble clustering idc the idc method is a natural extension of the recentdoubleclustering dc methodofslonimandtishbythatexhibited impressive performance on text categorization task usingsyntheticallygenerateddataweempiricallyflndthatwheneverthe dcprocedureissuccessfulinrecoveringsomeofthestructurehidden in the data the extended idc procedure can incrementally compute a signiflcantly more accurate classiflcation idc is especially advantageous when the data exhibit high attribute noise our simulation resultsalso show theefiectiveness ofidcin text categorization problem surprisingly this unsupervised procedure can be competitive with a supervised svm trained with a small training set finally weproposeasimpleandnaturalextensionofidcforsemi supervised and transductive learning where we are given both labeled and unlabeled example 
for many kdd operation such a nearest neighbor search distance based clustering and outlier detection there is an underlying kgr d data space in which each tuple object is represented a a point in the space in the presence of differing scale variability correlation and or outlier we may get unintuitive result if an inappropriate space is used the fundamental question that this paper address is what then is an appropriate space we propose using a robust space transformation called the donoho stahel estimator in the first half of the paper we show the key property of the estimator of particular importance to kdd application involving database is the stability property which say that in spite of frequent update the estimator doe not a change much b lose it usefulness or c require re computation in the second half we focus on the computation of the estimator for high dimensional database we develop randomized algorithm and evaluate how well they perform empirically the novel algorithm we develop called the hybrid random algorithm is in most case at least an order of magnitude faster than the fixed angle and subsampling algorithm 
this paper explains why well known discretizationmethods such a entropy basedand ten bin work well for naive bayesianclassifiers with continuous variable regardlessof their complexity these methodsusually assume that discretized variableshave dirichlet prior since perfect aggregation hold for dirichlets we can show that generally a wide variety of discretizationmethods can perform well with insignificantdifference we identify situation where discretization 
maximum margin classifier such a support vector machine svms critically depends upon the convex hull of the trainingsamples of each class a they implicitly search for the minimumdistance between the convex hull we propose extrapolated vectormachines xvms which rely on extrapolation outside theseconvex hull xvms improve svm generalization very significantlyon the mnist ocr data they share similarity with thefisher discriminant maximize the inter class margin 
today it is quite common for web page content to include an advertisement since advertiser often want totarget their message to people with certain demographic attribute the anonymity of internet user pose a specialproblem for them the purpose of the present research is to find an effective way to infer demographic information e g gender age or income about people who use the internet but for whom demographic information is nototherwise available our hope is to build a 
in this article we propose a new reinforcement learning rl method based on an actor critic architecture the actor andthe critic are approximated by normalized gaussian network ngnet which are network of local linear regression unit thengnet is trained by the on line em algorithm proposed in our previouspaper we apply our rl method to the task of swinging upand stabilizing a single pendulum and the task of balancing a doublependulum near the upright position the experimental 
we describe a model of document citationthat learns to identify hub and authoritiesin a set of linked document such a pagesretrieved from the world wide web or papersretrieved from a research paper archive unlikethe popular hit algorithm which relieson dubious statistical assumption ourmodel provides probabilistic estimate thathave clear semantics we also find thatin general the identified authoritative documentscorrespond better to human intuition 
the goal of many unsupervised learning procedure is to bring twoprobability distribution into alignment generative model suchas gaussian mixture and boltzmann machine can be cast in thislight a can recoding model such a ica and projection pursuit 
in adaptive boosting several weak learner trained sequentiallyare combined to boost the overall algorithm performance recentlyadaptive boosting method for classification problem havebeen derived a gradient descent algorithm this formulation justifieskey element and parameter in the method all chosen tooptimize a single common objective function we propose an analogousformulation for adaptive boosting of regression problem utilizing a novel objective function that lead to a 
the work of the russian mathematician vladimir vapnik at t lab enables u to go back to the root of theoretical statistic leaving behind fisher s parameter in favor of the general approach started in the s by glivenko cantelli kolmogorov nowadays it ha become possible to model million of event described by thousand of variable within a reasonable time for a specific application the srm approach work with a family of model and calibrates the family of model to a point which is the best compromise between accuracy and robustness it also measure the complexity of the model using vc dimension which is not plagued by number of parameter hence model for large event described by several parameter can be generalized this open up great prospect in numerous field like customer relationship management network optimization risk management manufacturing yield management and a number of other data rich problem 
the goal of path analysis is to understand visitor navigation of a web site the fundamental analysis component is a path a path is a finite sequence of element typically representing url or group of url a full path is an abstraction of a visit or a session which can contain attribute described below subpaths represent interesting subsequence of the full path path analysis provides user configurable extraction filtering preprocessing noise reduction descriptive statistic and detailed analysis of three basic specific object element sub path and couple of element in each case list of frequent object subject to particular filtering and sorting are available we call the corresponding interactive tool element path and couple analyzer we also allow in depth exploration of individual element path and couple element explorer investigates composition and convergence of traffic through an element and allows conditioning based on the number of preceding succeeding step path explorer visualizes in and out flow of a path and attrition rate along the path couple explorer present distinct path connecting couple element along with measure of their association and some additional statistic 
we investigate the following data mining problem from computational chemistry from a large data set of compound find those that bind to a target molecule in a few iteration of biological testing a possible in each iteration a comparatively small batch of compound is screened for binding to the target we apply active learning technique for selecting the successive batch one selection strategy pick unlabeled example closest to the maximum margin hyperplane another produce many weight vector by running perceptrons over multiple permutation of the data each weight vector vote with it prediction and we pick the unlabeled example for which the prediction is most evenly split between and for a third selection strategy note that each unlabeled example bisects the version space of consistent weight vector we estimate the volume on both side of the split by bouncing a billiard through the version space and select unlabeled example that cause the most even split of the version space we demonstrate that on two data set provided by dupont pharmaceutical that all three selection strategy perform comparably well and are much better than selecting random batch for testing 
lbr is a lazy semi naive bayesian classifier learning technique designed to alleviate the attribute interdependence problem of naive bayesian classification to classify a test example it creates a conjunctive rule that selects a most appropriate subset of training example and induces a local naive bayesian classifier using this subset lbr can significantly improve the performance of the naive bayesian classifier a bias and variance analysis of lbr reveals that it significantly reduces the 
the human visual system encodes the chromatic signal conveyedby the three type of retinal cone photoreceptors in an opponentfashion this color opponency ha been shown to constitute anefficient encoding by spectral decorrelation of the receptor signal we analyze the spatial and chromatic structure of natural scene bydecomposing the spectral image into a set of linear basis functionssuch that they constitute a representation with minimal redundancy independent component 
this paper present an unsupervised learning algorithm that can derive the probabilistic dependence structure of part of an object a moving human body in our example automatically from unlabeled data the distinguished part of this work is that it is based on unlabeled data i e the training feature include both useful foreground part and background clutter and the correspondence between the part and detected feature are unknown we use decomposable triangulated graph to depict the probabilistic independence of part but the unsupervised technique is not limited to this type of graph in the new approach labeling of the data part assignment is taken a hidden variable and the em algorithm is applied a greedy algorithm is developed to select part and to search for the optimal structure based on the differential entropy of these variable the success of our algorithm is demonstrated by applying it to generate model of human motion automatically from unlabeled real image sequence 
stochastic fluctuation of voltage gated ion channel generate current and voltage noise in neuronal membrane this noise may be a critical determinant of the efficacy of information processing within neural system using monte carlo simulation we carry out a systematic investigation of the relationship between channel kinetics and the resulting membrane voltage noise using a stochastic markov version of the mainen sejnowski model of dendritic excitability in cortical neuron our simulation show that kinetic parameter which lead to an increase in membrane excitability increasing channel density decreasing temperature also lead to an increase in the magnitude of the sub threshold voltage noise noise also increase a the membrane is depolarized from rest towards threshold this suggests that channel fluctuation may interfere with a neuron s ability to function a an integrator of it syn aptic input and may limit the reliability and precision of neural informatio n processing 
the problem of controlling the capacity of decision tree is considered for the case where the decision node implement linear threshold function in addition to the standard early stopping and pruning procedure we implement a strategy based on the margin of the decision boundary at the node the approach is motivated by bound on generalization error obtained in term of the margin of the individual classifier experimental result are given which demonstrate that considerable advantage can be derived from using the margin information the same strategy is applied to the problem of transduction where the position of the testing point are revealed to the training algorithm this information is used to generate an alternative training criterion motivated by transductive theory in the transductive case the result are not a encouraging suggesting that little if any consistent advantage is culled from using the unlabelled data in the proposed fashion this conclusion doe not contradict theoretical result but leaf open the theoretical and practical question of whether more effective use can be made of the additional information 
the similarity join is an important operation for mining high dimensional feature space given two data set the similarity join computes all tuples x y that are within a distance egr one of the most efficient algorithm for processing similarity join is the multidimensional spatial join msj by koudas and sevcik in our previous work pursued for the two dimensional case we found however that msj ha several performance shortcoming in term of cpu and i o cost a well a memory requirement therefore msj is not generally applicable to high dimensional data in this paper we propose a new algorithm named generic external space sweep ge ge introduces a modest rate of data replication to reduce the number of expensive distance computation we present a new cost model for replication an i o model and an inexpensive method for duplicate removal the principal component of our algorithm is a highly flexible replication engine our analytical model predicts a tremendous reduction of the number of expensive distance computation by several order of magnitude in comparison to msj factor in addition the memory requirement of ge are shown to be lower by several order of magnitude furthermore the i o cost of our algorithm is by factor better independent from the fact whether replication occurs or not our analytical result are confirmed by a large series of simulation and experiment with synthetic and real high dimensional data set 
we report the result of a study on topic spotting in conversational speech using a machine learning approach we build classifier that accept an audio file of conversational human speech a input and output an estimate of the topic being discussed our methodology make use of a well known corpus of transcribed and topic labeled speech the switchboard corpus and involves an interesting double use of the boostexter learning algorithm our work is distinguished from previous effort in topic spotting by our explicit study of the effect of dialogue length on classifier performance and by our use of off the shelf speech recognition technology one of our main result is the identification of a single classifier with good performance relative to our classifier space across all subdialogue length 
the problem of neural coding is to understand how sequence ofaction potential spike are related to sensory stimulus motor output or ultimately thought and intention one clear questionis whether the same coding rule are used by dierent neuron orby corresponding neuron in dierent individual we present aquantitative formulation of this problem using idea from informationtheory and apply this approach to the analysis of experimentsin the y visual system we nd 
this paper report on our work and resultsframing signal processing algorithm optimizationas a machine learning task a singlesignal processing algorithm can be representedby many different but mathematicallyequivalent formula when these formulasare implemented in actual code they havevery different running time signal processingoptimization is concerned with finding aformula that implement the algorithm a efficientlyas possible unfortunately a correctmapping 
clickstream data collected at any web site site centric data is inherently incomplete since it doe not capture user browsing behavior across site user centric data hence model learned from such data may be subject to limitation the nature of which ha not been well studied understanding the limitation is particularly important since most current personalization technique are based on site centric data only in this paper we empirically examine the implication of learning from incomplete data in the context of two specific problem a predicting if the remainder of any given session will result in a purchase and b predicting if a given user will make a purchase at any future session for each of these problem we present new algorithm for fast and accurate data preprocessing of clickstream data based on a comprehensive experiment on user level clickstream data gathered from user browsing behavior we demonstrate that model built on user centric data outperform model built on site centric data for both prediction task 
predictive model developed by applying data mining technique are used to improve forecasting accuracy in the airline business in order to maximize the revenue on a flight the number of seat available for sale is typically higher than the physical seat capacity overbooking to optimize the overbooking rate an accurate estimation of the number of no show passenger passenger who hold a valid booking but do not appear at the gate to board for the flight is essential currently no show on future flight are estimated from the number of no show on historical flight averaged on booking class level in this work classification tree and logistic regression model are applied to estimate the probability that an individual passenger turn out to be a no show passenger information stored in the reservation system of the airline is either directly used a explanatory variable or used to create attribute that have an impact on the probability of a passenger to be a no show the total number of no show in each booking class or on the total flight is then obtained by accumulating the individual no show probability over the entity of interest we show that this forecasting approach is more accurate than the currently used method in addition the selected model lead to a deepened insight into passenger behavior 
we describe a lightweight learning method that induces an ensemble of decision rule solution for regression problem instead of direct prediction of a continuous output variable the method discretizes the variable by k mean clustering and solves the resultant classification problem prediction on new example are made by averaging the mean value of class with vote that are close in number to the most likely class we provide experimental evidence that this indirect approach can often yield strong result for many application generally outperforming direct approach such a regression tree and rivaling bagged regression tree 
in this paper we show how we can learn to select good word for a document title we view the problem of selecting good title word for a document a a variant of an information retrieval problem each title word is treated a a document and selection of appropriate title word a finding relevant document based on our training collection consisting of document and title pair we learn the document representation for all the title word and apply these learned representation to select appropriate title word over test document compared to other learning approach namely k nearest neighbor approach a na ve bayesian approach and a variant of a machine translation model we find that our approach is significantly better a indicated by the f metric 
this paper describes an algorithm for generating compact d model of indoor environment with mobile robot our algorithm employ the expectation maximization algorithm to fit a lowcomplexity planar model to d data collected by range finder and a panoramic camera the complexity of the model is determined during model fitting by incrementally adding and removing surface in a final post processing step measurement are converted into polygon and projected onto the surface model where possible empirical result obtained with a mobile robot illustrate that high resolution model can be acquired in reasonable time 
in the missing data approach to improving the robustness of automatic speech recognition to added noise an initial process identifies spectraltemporal region which are dominated by the speech source the remaining region are considered to be missing in this paper we develop a connectionist approach to the problem of adapting speech recognition to the missing data case using recurrent neural network in contrast to method based on hidden markov model rnns allow u to make use of long term time constraint and to make the problem of classification with incomplete data and imputing missing value interact we report encouraging result on an isolated digit recognition task 
abstract spike triggered averaging technique are effective for linear characteri zation of neural response but neuron exhibit important nonlinear be haviors such a gain control that are not captured by such analysis we describe a spike triggered covariance method for retrieving suppres sive component of the gain control signal in a neuron the method in simulation and on salamander retinal ganglion cell data analysis of physiological data reveals meaningful suppressive ax and explains interesting nonlinearities we expect this method to be applica ble to other sensory area and modality white noise analysis ha emerged a a powerful technique for characterizing response prop erties of spiking neuron a sequence of stimulus are drawn randomly from an ensemble and presented in rapid succession and one examines the subset that elicit action potential spike triggered stimulus ensemble can provide information about the neuron s response characteristic in the most widely used form of this analysis one estimate an excitatory linear kernel by computing the spike triggered average sta that is the mean stimulus that elicited a spike e g under the assumption that spike are generated by a poisson process with instantaneous rate determined by linear projection onto a kernel fol lowed by a static nonlinearity the sta provides an unbiased estimate of this kernel recently a number of author have developed interesting extension of white noise anal ysis some have examined spike triggered average in a reduced linear subspace of input stimulus e g others have recovered excitatory subspace by computing the spike triggered covariance stc followed by an eigenvector analysis to determine the subspace ax e g sensory neuron exhibit striking nonlinear behavior that are not explained by fundamen tally linear mechanism for example the response of a neuron typically saturates for large amplitude stimulus the response to the optimal stimulus is often suppressed by the presence of a non optimal mask e g and the kernel recovered from sta analysis may change shape a a function of stimulus amplitude e g iors can be attributed to gain control e g in which neural response are suppressively modulated by a gain signal derived from the stimulus derlying mechanism and time scale associated with such gain control are current topic of research the basic functional property appear to be ubiquitous occurring throughout the nervous system howard hughes medical inst center for neural science edu new york university eero 
we present hemp a novel learning algorithm designed to operate in the domain of markov process and markov decision process hemp learns to perform a 
this paper introduces a foundation for inductive learning based on the use of higher order logic for knowledge representation in particular the paper i provides a systematic individual a term approach to knowledge representation for inductive learning and demonstrates the utility of type and higher order construct for this purpose ii introduces a systematic way to construct predicate for use in induced definition and iii widens the applicability of decision tree algorithm beyond the usual attribute value setting to the classification of individual with complex internal structure the paper contains several illustrative application the effectiveness of the approach is demonstrated by applying the decision tree learning system to two benchmark problem 
i describe a framework for interpreting support vector machine svms a maximum a posteriori map solution to inferenceproblems with gaussian process prior this can provide intuitiveguidelines for choosing a good svm kernel it can also assign by evidence maximization optimal value to parameter such asthe noise level c which cannot be determined unambiguously fromproperties of the map solution alone such a cross validation error i illustrate this using a simple 
hierarchical learning machine are non regular and non identifiable statistical model whose true parameter set are analytic set with singularity using algebraic analysis we rigorously prove that the stochastic complexity of a non identifiable learning machine is asymptotically equal to log n m log log n const where n is the number of training sample moreover we show that the rational number and the integer m can be algorithmically calculated using resolution of singularity in algebraic geometry also we obtain inequality d and m d where d is the number of parameter 
we describe a computer system that provides a real time musicalaccompaniment for a live soloist in a piece of non improvisedmusic for soloist and accompaniment a bayesian network is developedthat represents the joint distribution on the time at whichthe solo and accompaniment note are played relating the twoparts through a layer of hidden variable the network is first constructedusing the rhythmic information contained in the musicalscore the network is then trained to 
abstract this paper discus the use of unlabeled example for the problem of named entity classification a large number of rule is needed for coverage of the domain suggesting that a fairly large number of labeled example should be required to train a classifier however we show that the use of unlabeled data can reduce the requirement for supervision to just simple seed rule the approach gain leverage from natural redundancy in the data for many named entity instance both the spelling of the name and the context inwhich it appears are sufficient to determine it type we present two algorithm the first method us a similar algorithm to that of yarowsky with modification motivated by blum and mitchell the second algorithm extends idea from boosting algorithm designed for supervised learning task to the framework suggested by blum and mitchell 
visual inspection of neuron suggests that dendritic orientation may be determined both by internal constraint e g membrane tension and by external vector field e g neurotrophic gradient for example basal dendrite of pyramidal cell appear nicely fan out this regular orientation is hard to justify completely with a general t endency to grow straight given the z igzags observed experimentally instead dendrite could a favor a fixed external direction or b repel from t heir own soma to investigate these possibility quantitatively reconstructed h ippocampal cell were subjected to bayesian analysis the statistical model combined linearly factor a and b a well a the tendency to grow straight for all morphological class b wa found to be significantly positive and consistently greater than a in addition when dendrite were artificially re oriented according to this model the resulting structure closely resembled real morphology these result suggest that somatodendritic repulsion may play a role in determining dendritic orientation since hippocampal cell are very densely packed and their dendritic tree highly overlap the repulsion must be ce llspecific we discus possible mechanism underlying such specificity 
in many practical learning scenario there isa small amount of labeled data along witha large pool of unlabeled data many supervisedlearning algorithm have been developedand extensively studied we presenta new quot co training quot strategy for using unlabeleddata to improve the performanceof standard supervised learning algorithm unlike much of the prior work such a theco training procedure of blum and mitchell we do not assume there are two redundantviews both of 
the paper introduces logan h a system for learning first order function free horn expression from interpretation the system is based on an algorithm that learns by asking question and that wa proved correct in previous work the current paper show how the algorithm can be implemented in a practical system and introduces a new algorithm based on it that avoids interaction and learns from example only the logan h system implement these algorithm and add several facility and optimization that allow efficient application in a wide range of problem a one of the important ingredient the system includes several fast procedure for solving the subsumption problem an np complete problem that need to be solved many time during the learning process we describe qualitative and quantitative experiment in several domain the experiment demonstrate that the system can deal with varied problem large amount of data and that it achieves good classification accuracy 
we present a new technique for time series analysis based on dynamicprobabilistic network in this approach the observed dataare modeled in term of unobserved mutually independent factor a in the recently introduced technique of independent factor analysis ifa however unlike in ifa the factor are not i i d eachfactor ha it own temporal statistical characteristic we derive afamily of em algorithm that learn the structure of the underlyingfactors and their relation to the 
we propose a new classification for multi agent learning algorithm with each league of player characterized by both their possible strategy and possible belief using this classification we review the optimality of existing algorithm including the case of interleague play we propose an incremental improvement to the existing algorithm that seems to achieve average payoff that are at least the nash equilibrium payoff in the longrun against fair opponent 
we describe a unifying method for proving relative loss bound for online linear threshold classification algorithm such a the perceptron and the winnow algorithm for classification problem the discrete loss is used i e the total number of prediction mistake we introduce a continuous loss function called the linear hinge loss that can be employed to derive the update of the algorithm we first prove bound w r t the linear hinge loss and then convert them to the discrete loss we introduce a notion of average margin of a set of example we show how relative loss bound based on the linear hinge loss can be converted to relative loss bound i t o the discrete loss using the average margin 
coil challenge wa a supervised learning contest that attracted entry the author of entry later wrote explanation of their work this paper discus these report and reach three main conclusion first naive bayesian classifier remain competitive in practice they were used by both the winning entry and the next best entry second identifying feature interaction correctly is important for maximizing predictive accuracy this wa the difference between the winning classifier and all others third and most important too many researcher and practitioner in data mining do not appreciate properly the issue of statistical significance and the danger of overfitting given a dataset such a the one for the coil contest it is pointless to apply a very complicated learning algorithm or to perform a very time consuming model search in either ease one is likely to overfit the training data and to fool oneself in estimating predictive accuracy and in discovering useful correlation 
we describe an extension of the markov decision process model in which a continuous time dimension is included in the state space this allows for the representation and exact solution of a wide range of problem in which transition or reward vary over time we examine problem based on route planning with public transportation and telescope observation scheduling 
abstract this paper address the issue of reducing the storage requirement on instance based learning algorithm algorithm proposed by other research use heuristic to prune instance of the training set or modify the instance themselves to achieve a reduced set of instance our work present an alternative way we propose to induce a reduced set of partially dened instance with evolutionary algorithm experiment were performed with gale our ne grained parallel evolutionary algorithm and other well known reduction technique on several datasets result suggest that evolutionary algorithm are competitive and robust for inducing set of partially dened instance achieving better reduction rate in storage requirement without loss in generalization accuracy 
many neural system extend their dynamic range by adaptation we examine the timescales of adaptation in the context of dynamically modulated rapidly varying stimulus and demonstrate in the fly v isual system that adaptation to the statistical ensemble of the stimulus dynamically maximizes information transmission about the time dependent stimulus further while the rate response ha long transient the ad aptation take place on timescales consistent with optimal variance estimation 
it is well known that naive bayes can only represent linearly separable function in binary domain but the learnability of general augmented naive bayes is open little work is done on the learnability of bayesian network in nominal domain a general case of binary domain this paper explores the learnability of augmented naive bayes in nominal domain we introduce a complexity measure for nominal function and prove upper bound of the learnability of augmented naive bayes in term of 
a the web and e business have proliferated the practice of using customer facing knowledge base to augment customer service and support operation ha increased this can be a very efficient scalable and cost effective way to share knowledge the effectiveness and cost saving are proportional to the utility of the information within the knowledge base and inversely proportional to the amount of labor required in maintaining the knowledge to address this issue we have developed an algorithm and methodology to increase the utility of the information within a knowledge base while greatly reducing the labor required in this paper we describe an implementation of an algorithm and methodology for comparing a knowledge base to a set of problem ticket to determine which category and subcategories are not well addressed within the knowledge base we utilize text clustering on problem ticket text to determine a set of problem category we then compare each knowledge base solution document to each problem category centroid using a cosine distance metric the distance between the closest solution document and the corresponding centroid becomes the basis of that problem category s knowledge gap our claim is that this gap metric serf a a useful method for quickly and automatically determining which problem category have no relevant solution in a knowledge base we have implemented our approach and we present the result of performing a knowledge gap analysis on a set of support center problem ticket 
the nearest neighbor technique is a simple and appealing method to address classificationproblems it relies on the assumption of locally constant class conditionalprobabilities this assumption becomes invalid in high dimension with a finitenumber of example due to the curse of dimensionality severe bias can be introducedunder these condition when using the nearest neighbor rule the employmentof a local adaptive metric becomes crucial in order to keep class conditional 
tom m mitchell is author of the textbook machine learning mcgraw hill president of the american association for artificial intelligence and a member of the national research council s computer science and telecommunication board he is vice president and chief scientist at whizbang lab and is currently on a two year leave of absence from carnegie mellon university where he is the fredkin professor of learning and ai in the school of computer science and founding director of cmu s center for automated learning and discovery mitchell s research interest span many area of machine learning theory and practice his current work at whizbang lab involves developing machine learning method for extracting information from text for example whizbang ha developed the world s largest database of job opening by training it software to automatically locate and extract detailed information from job posting on corporate web site see www flipdog com 
we propose a novel approach for building finite memory predic tive model similar in spirit to variable memory length markov model vlmms the model are constructed by first transforming the block structure of the training sequence into a spatial structure of point in a unit hypercube such that the longer is the common suffix shared by any two block the closer lie their point representation such a transfor mation embodies a markov assumption block with long common suffix are likely to produce similar continuation finding a set of predicti on context is formulated a a resource allocation problem solved by vector quantizing the spatial block representation we compare our model with both the classical and variable memory length markov model on three data set with different memory and stochastic component our model have a superior performance yet their construction is fully aut omatic which is shown to be problematic in the case of vlmms 
developing automated agent that intelligently perform complex real world task is time consuming and expensive the most expensive part of developing these intelligent task performance agent involves extracting knowledge from human expert and encoding it into a form useable by automated agent machine learning from a sufficiently rich and focused knowledge source can significantly reduce the cost of developing intelligent performance agent by automating the knowledge acquisition and encoding process potential knowledge source include instruction from human expert experiment performed in the task environment and observation of an expert performing the task observation is particularly well suited to learning hierarchical performance knowledge for task that require realistic human like behavior our learning by observation system called knomic knowledge mimic extract knowledge from observation of an expert performing a task and generalizes this knowledge into rule that an agent can use to perform the same task learning performance knowledge by observation is more efficient than hand coding the knowledge in a number of way knowledge can be encoded directly from the expert without the need for a knowledge engineer to act a an intermediary also the expert only need to demonstrate the task rather than organize and communicate all the relevant information this paper will describe the knowledge required for task performance describe how this knowledge is learned by knomic and report on our effort to learn performance knowledge in the tactical air combat domain and the computer game quake ii 
we propose to scale learning algorithm to arbitrarily large database by the following method first derive an upper bound for the learner s loss a a function of the number of example used in each step of the algorithm then use this to minimize each step s number of example while guaranteeing that the model produced doe not differ significantly from the one that would be obtained with infinite data we apply the method to k mean clustering and empirically observe it speedup relative 
it is well known that under noisy condition we can hear speech much more clearly when we read the speaker s lip this suggests the utility of audio visual information for the task of speech enhancement we propose a method to exploit audio visual cue to enable speech separation under non stationary noise and with a single microphone we revise and extend hmm based speech enhancement technique in which signal and noise model are factorially combined to incorporate visual lip information 
to control the walking gait of a four legged robot we present a novel neuromorphic vlsi chip that coordinate the relative phasing of the robot s leg similar to how spinal central pattern generato r are believed to control vertebrate locomotion the chip control the leg movement by driving motor with time varying voltage which are the output of a small network of coupled oscillator the characte ristics of the chip s output voltage depend on a set of input parameter t he relationship between input parameter and output voltage can be computed analytically for an idealized system in practice however this ideal relationship is only approximately true due to transistor mi match and offset fine tuning of the chip s input parameter is done auto matically by the robotic system using an unsupervised support vector sv learning algorithm introduced recently the learning requires o nly that the description of the desired output is given the machine lear n from unlabeled example how to set the parameter to the chip in order to obtain a desired motor behavior 
this paper revisits the classical neuroscience paradigm of hebbian learning we find that a necessary requirement for effective associative memory learning is that the efficacy of the incoming synapsis should be uncorrelated this requirement is difficult to achieve in a robust manner by hebbian synaptic learning since it depends on network level information effective learning can yet be obtained by a neuronal process that maintains a zero sum of the incoming synaptic efficacy this 
in this paper we present committee a new multi class learning algorithm related to the winnow family of algorithm committee is an algorithm for combining the prediction of a set of sub expert s in the online mistake bounded model of learning a sub expert is a special type of attribute that predicts with a distribution over a finite number of class committee learns a linear function of sub expert and us t his function to make class prediction we provide bound for committee that show it performs well when the target can be represented by a few relevant sub expert we also show how committee can be used to solve more traditional problem composed of attribute this lead to a natural extension that learns on multi class problem that contain bo th traditional attribute and sub expert in this paper we present a new multi class learning algorit hm called committee committee learns a k class target function by combining information from a large set of sub expert a sub expert is a special type of attribute that predicts wit h a distribution over the target class the target space of function are linear max funct ion we define these a function that take a linear combination of sub expert prediction an d return the class with maximum value it may be useful to think of the sub expert a individ ual classifying function that are attempting to predict the target function even though t he individual sub expert may not be perfect committee attempt to learn a linear max function that represents the target function in truth this picture is not quite accurate the r eason we call them sub expert and not expert is because even though a individual sub expert might be poor at prediction it may be useful when used in a linear max function for example some sub expert might be used to add constant weight to the linear max function the algorithm is analyzed for the on line mistake bounded model of learning lit this is a useful model for a type of incremental learning where an algorithm can use feedback about it current hypothesis to improve it performance in this model the algorithm go through a series of learning trial a trial is composed of th ree step first the algorithm 
this paper present a computational model for perceptual organization a figure ground segregation network is proposed based on a novel boundary pair representation the system solves the figure ground segregation problem through temporal evolution gestalt like grouping rule are incorporated by modulating connection which determines the temporal behavior and thus the perception of the system the result are then fed to a surface completion module based on local diffusion different perceptual phenomenon such a modal and a modal completion virtual contour grouping and shape decomposition are explained by the model with a fixed set of parameter computationally the system eliminates combinatorial optimization which is common to many existing computational approach it also account for more example that are consistent with psychological experiment in addition the boundary pair representation is consistent with well known onand off center cell response and thus biologically more plausible 
research in machine learning statistic and related field ha produced a wide variety of algorithm for classification however most of these algorithm assume that all error have the same cost which is seldom the case in kdd problem individually making each classification learner costsensitive is laborious and often non trivial in this paper we propose a principled method for making an arbitrary classifier cost sensitive by wrapping a cost minimizing procedure around it this 
rightnow web is an integrated software package for web based customer service that ha at it core a database of answer to frequently asked question faq one major design goal is to facilitate end user interaction with this dynamic document collection i e make it a easy and efficient a possible for user to browse the collection and locate desired information to this end we perform several type of analysis on the session tracking database that record user navigation history first using both explicit and implicit measure of user satisfaction we infer a solved count representing the average utility of an faq second using the user navigation pattern we construct a link matrix representing connection between faq the technique of building up the link matrix and using it to advise user on related information amount to a form of the swarm intelligence method of finding optimal path both solved count and the link matrix are continuously updated a user interact with the site furthermore they are periodically aged to emphasize recent activity the synergistic combination of these technique allows user to learn from the database in a more effective manner a evidenced by usage statistic 
we build upon our work on iterated phan 
in kernel based learning the data is mapped to a kernel feature space ofa dimension that corresponds to the number of training data point inpractice however the data form a smaller submanifold in feature space a fact that ha been used e g by reduced set technique for svms wepropose a new mathematical construction that permit to adapt to the intrinsicdimension and to find an orthonormal basis of this submanifold 
abstract non negative matrix factorization nmf ha previously been shown to be a useful decomposition for multivariate data two different multiplicative algorithm for nmf are analyzed they differ only slightly in the multiplicative factor used in the update rule one algorithm can be shown to minimize the conventional least square error while the other minimizes the generalized kullback leibler divergence the monotonic convergence of both algorithm can be proven using an auxiliary function analogous to that used for proving convergence of the expectationmaximization algorithm the algorithm can also be interpreted a diagonally rescaled gradient descent where the rescaling factor is optimally chosen to ensure convergence 
we derive an equivalence between adaboost and the dual of a convex optimization problem showing that the only difference between minimizing the exponential loss used by adaboost and maximum likelihood for exponential model is that the latter requires the model to be normalized to form a conditional probability distribution over label in addition to establishing a simple and easily understood connection between the two method this framework enables u to derive new regularization procedure for boosting that directly correspond to penalized maximum likelihood experiment on uci datasets support our theoretical analysis and give additional insight into the relationship between boosting and logistic regression 
although connectionist model have provided insight into the nature of perception and motor control connectionist account of higher cognition seldom go beyond an implementation of traditional symbol processing theory we describe a connectionist constraint satisfaction model of how people solve anagram problem the model exploit statistic of english orthography but also address the interplay of subsymbol ic and symbolic computation by a mechanism that extract approximate symbolic representation partial ordering of letter from subsym bolic structure and injects the extracted representation back into the model to assist in the solution of the anagram we show the computational benefit of this extraction injection process and discus it relationshi p to conscious mental process and working memory we also account for experimental data concerning the difficulty of anagram solution based on the orthographic structure of the anagram string and the target word historically the mind ha been viewed from two opposing computational perspective the symbolic perspective view the mind a a symbolic information processing engine according to this perspective cognition operates on representation that en code logical relationship among discrete symbolic element such a stack and structu red tree and cognition involves basic operation such a mean end analysis and best fi rst search in contrast the subsymbolic perspective view the mind a performing statistical inference and involves basic operation such a constraint satisfaction search the d ata structure on which these operation take place are numerical vector in some domain of cognition significant progress ha been made throug h analysis from one computational perspective or the other the thesis of our work is t hat many of these domain might be understood more completely by focusing on the interplay of subsymbolic and symbolic information processing consider the higher cognitive domain of problem solving at an abstract level of description problem solving task can r eadily be formalized in term of symbolic representation and operation however the n eurobiological hardware that underlies human cognition appears to be subsymbolic representation are noisy and graded and the brain operates and adapts in a continuous fashion that is difficult to characterize in discrete symbolic term at some level between the computational level of the task description and the implementation level of human neurobiology the symbolic and subsymbolic account must come into contact with one another we focus on this point of contact by proposing mechanism by which symbolic representation can modulate subsymbolic processing and mechanism by which subsymbolic representation 
in this paper we propose an unsupervised method for discovering inference rule from text such a x is author of y ap x wrote y x solved y ap x found a solution to y and x caused y ap y is triggered by x inference rule are extremely important in many field such a natural language processing information retrieval and artificial intelligence in general our algorithm is based on an extended version of harris distributional hypothesis which state that word that occurred in the same context tend to be similar instead of using this hypothesis on word we apply it to path in the dependency tree of a parsed corpus 
this paper present a unifying probabilistic framework for clustering individual or systemsinto group when the available data measurement are not multivariate vector of fixeddimensionality for example one might have data from a set of medical patient wherefor each patient one ha different number of time series observation each time series ofdifferent length we propose a general model based framework for clustering heterogeneousdata type of this form we discus a general 
in this paper we examine the problem of estimating the parameter of a multinomial distribution over a large number of discrete outcome most of which do not appear in the training data we analyze this problem from a bayesian perspective and develop a hierarchical prior that incorporates the assumption that the observed outcome constitute only a small subset of the possible outcome we show how to efficiently perform exact inference with this form of hierarchical prior and compare our method to standard approach and demonstrate it merit category algorithm and architecture presentation preference none this paper wa not submitted elsewhere nor will be submitted during nip review period 
a central issue in principal component analysis pca is choosing the number of principal component to be retained by interpreting pca a density estimation we show how to use bayesian model selection to estimate the true dimensionality of the data the resulting e timate is simple to compute yet guaranteed to pick the correct dimensionality given enough data the estimate involves an integral over the steifel manifold of frame which is difficult to compute exactly but after cho osing an appropriate parameterization and applying laplace s meth od an accurate and practical estimator is obtained in simulation i t is convincingly better than cross validation and other proposed algorithm s plus it run much faster 
many process in biology from the regulation of gene expression in bacteria to memory in the brain involve switch constructed from network of biochemical reaction crucial molecule are present in small number raising question about noise and stability analysis of noise in simple reaction scheme indicates that switch stable for year and switchable in millisecond can be built from fewer than one hundred molecule prospect for direct test of this prediction a well a implication are discussed 
we present a class of approximate inference algorithm for graphical model of the qmr dt type we give convergence rate for these algorithm and for the jaakkola and jordan algorithm and verify these theoretical prediction empirically we also present empirical result on the difficult qmr dt network problem obtaining performance of the new algorithm roughly comparable to the jaakkola and jordan algorithm 
in this paper we consider the problem of active learning in trigonometric polynomial network and give a necessary and sufficient condition of sample point to provide the optimal generalization capability by analyzing the condition from the functional analytic point of view we clarify the mechanism of achieving the optimal generalization capability we also show that a set of training example satisfying the condition doe not only provide the optimal generalization but also reduces the computational complexity and memory required for the calculation of learning result finally example of sample point satisfying the condition are given and computer simulation are performed to demonstrate the effectiveness of the proposed active learning method 
competition in the wireless telecommunication industry is rampant to maintain profitability wireless carrier must control churn the loss of subscriber who switch from one carrier to another we explore statistical technique for churn prediction and based on these prediction an optimal policy for identifying customer to whom incentive should be offered to increase retention our experiment are based on a data base of nearly u s domestic subscriber and includes information about their usage billing credit application and complaint history we show that under a wide variety of assumption concerning the cost of intervention and the retention rate resulting from intervention churn prediction and remediation can yield significant saving to a carrier we also show the importance of a data representation crafted by domain expert competition in the wireless telecommunication industry is rampant a many a seven competing carrier operate in each market the industry is extremely dynamic with new service technology and carrier constantly altering the landscape carrier announce new rate and incentive weekly hoping to entice new subscriber and to lure subscriber away from the competition the extent of rivalry is reflected in the deluge of advertisement for wireless service in the daily newspaper and other mass medium the united state had million wireless subscriber in roughly of the population some market are further developed for example the subscription rate in finland is industry forecast are for a u s penetration rate of by although there is significant room for growth in most market the industry growth rate is declining and competition is rising consequently it ha become crucial for wireless carrier to control churn the loss of customer who switch from one carrier to another at present domestic monthly churn rate are of the customer base at an average cost of to acquire a subscriber churn cost the industry nearly billion in the total annual loss rose to nearly billion when lost monthly revenue from subscriber cancellation is considered luna it cost roughly five time a much to sign on a new subscriber a to retain an existing one consequently for a carrier with million subscriber reducing the monthly churn rate from to would yield an increase in annual earnings of at least million and an increase in shareholder value of approximately million estimate are even higher when lost monthly revenue is considered see fowlkes madan andrew jensen luna the goal of our research is to evaluate the benefit of predicting churn using technique from statistical machine learning we designed model that predict the probability mozer m c w olniewicz r grime d b johnson e kaushansky h churn reduction in the wireless industry in s a solla t k leen k r mueller ed advance in neural information processing system pp cambridge ma mit press 
we examine linear program lp approach to boosting and demonstrate their efficient solution using lpboost a column generation simplex method we prove that minimizing the soft margin error function equivalent to solving an lp directly optimizes a generalization error bound lpboost can be used to solve any boosting lp by iteratively optimizing the dual classification cost in a restricted lp and dynamically generating weak learner to make new lp column unlike gradient boosting algorithm lpboost converges finitely to a global solution using well defined stopping criterion computationally lpboost find very sparse solution a good a or better than those found by adaboost using comparable computation 
we investigate a probabilistic framework for automatic speechrecognition based on the intrinsic geometric property of curve in particular we analyze the setting in which two variable onecontinuous x one discrete s evolve jointly in time we supposethat the vector x trace out a smooth multidimensional curveand that the variable s evolves stochastically a a function of thearc length traversed along this curve since arc length doe notdepend on the rate at which a curve is 
after the pioneering work of the bacon system the study in the field of scientific discoveryhas been directed to the discovery ofmore plausible law equation to represent thefirst principle underlying objective system the state of the art ha only succeeded ina weak sense that the soundness the reproducibilityand the mathematical admissibilityof the candidate hold within the experimentalmeasurements the plausibility shouldbe checked for various object and or 
we call data weakly labeled if it ha no exact label but rather a numerical indication of correctness of the label guessed by the learning algorithm a situation commonly encountered in problem of reinforcement learning the term emphasizes similarity of our approach to the known technique of solving unsupervised and transductive problem in this paper we present an on line algorithm that cast the problem a a multi arm bandit with hidden state and solves it iteratively within the expectation maximization framework the hidden state is represented by a parameterized probability distribution over state tied to the reward the parameterization is formally justified allowing for smooth blending between likelihoodand reward based cost 
an on line recursive algorithm for training support vector machine one vector at a time is presented adiabatic increment retain the kuhntucker condition on all previously seen training data in a number of step each computed analytically the incremental procedure is reversible and decremental unlearning offer an efficient method to exactly evaluate leave one out generalization performance interpretation of decremental unlearning in feature space shed light on the relationship between generalization and geometry of the data 
discovering significant pattern that exist implicitly in huge spatial database is an important computational task a common approach to this problem is to use cluster analysis we propose a novel approach to clustering based on the deterministic analysis of random walk on a weighted graph generated from the data our approach can decompose the data into arbitrarily shaped cluster of different size and density overcoming noise and outlier that may blur the natural decomposition of the data the method requires only o n log n time and one of it variant need only constant space 
singularity are ubiquitous in the parameter space of hierarchicalmodels such a multilayer perceptrons at singularity the fisherinformation matrix degenerate and the cram er rao paradigmdoes no more hold implying that the classical model selection theorysuch a aic and mdl cannot be applied it is important tostudy the relation between the generalization error and the trainingerror at singularity the present paper demonstrates a methodof analyzing these error both for 
logistic unit in therst hidden layer of a feedforward neural networkcompute the relative probability of a data point under twogaussians this lead u to consider substituting other densitymodels we present an architecture for performing discriminativelearning of hidden markov model using a network of many smallhmm s experiment on speech data show it to be superior to thestandard method of discriminatively training hmm s 
the multiple instance learning model ha received much attention recently with a primary application area being that of drug activity prediction most prior work on multiple instance learning ha been for concept learning yet for drug activity prediction the label is a real valued affinity measurement giving the binding strength we present extension of k nearest neighbor k nn citation knn and the diverse density algorithm for the real valued setting and study their performance on boolean and real valued data we also provide a method for generating chemically realistic artificial data 
the curse of dimensionality give rise to prohibitive computational requirement that render infeasible the exact solution of large scale stochastic control problem we study an efficient method based on linear programming for approximating solution to such problem 
the problem of reinforcement learning in a non markov environment is explored using a dynamic bayesian network where conditional independence assumption between random variable are compactly represented by network parameter the parameter are learned on line and approximation are used to perform inference and to compute the optimal value function the relative effect of inference and value function approximation on the quality of the final policy are investigated by learning to solve a moderately difficult driving task the two value function approximation linear and quadratic were found to perform similarly but the quadratic model wa more sensitive to initialization both performed below the level of human performance on the task the dynamic bayesian network performed comparably to a model using a localist hidden state representation while requiring exponentially fewer parameter 
animal data on delayed reward conditioning experiment show astriking property the data for different time interval collapsesinto a single curve when the data is scaled by the time interval 
hierarchical reinforcement learning promise to be the key to scaling reinforcement learning method to large complex real world problem many theoretical model have been proposed but so far there ha been little in the way of empirical work published to demonstrate these claim in this paper we begin to fill this void by by demonstrating the application of the rl top hierarchical reinforcement learning system to the problem of learning to control an aircraft in a ight simulator we 
bagging and boosting are well known ensemble learning method they combine multiple learned base model with the aim of improving generalization performance to date they have been used primarily in batch mode i e they require multiple pass through the training data in previous work we presented online bagging and boosting algorithm that only require one pas through the training data and presented experimental result on some relatively small datasets through additional experiment on a variety of larger synthetic and real datasets this paper demonstrates that our online version perform comparably to their batch counterpart in term of classification accuracy we also demonstrate the substantial reduction in running time we obtain with our online algorithm because they require fewer pass through the training data 
we describe a simple active learning heuristic which greatly enhances the generalization behavior of support vector machine svms on several practical document classification task we observe a number of benefit the most surprising of which is that a svm trained on a wellchosen subset of the available corpus frequently performs better than one trained on all available data the heuristic for choosing this subset is simple to compute and make no use of information about the test set given that the training time of svms depends heavily on the training set size our heuristic not only offer better performance with fewer data it frequently doe so in le time than the naive approach of training on all available data 
many researcher have explored method for hierarchical reinforcement learning rl with temporal abstraction in which abstract action are defined that can perform many primitive action before terminating however little is known about learning with state abstraction in which aspect of the state space are ignored in previous work we developed the maxq method for hierarchical rl in this paper we define five condition under which state abstraction can be combined with the maxq value function decomposition we prove that the maxq q learning algorithm converges under these condition and show experimentally that state abstraction is important for the successful application of maxq q learning 
probabilistic dfa inference is the problem of inducing a stochastic regular grammar from a positive sample of an unknown language the alergia algorithm is one of the most successful approach to this problem in the present work we review this algorithm and explain why it generalization criterion a state merging operation is purely local this characteristic lead to the conclusion that there is no explicit way to bound the divergence between the distribution de ned by the solution and the 
we propose two novel method for reducing dimension in training polynomial network we consider the class of polynomial network whose output is the weighted sum of a basis of monomials our first method for dimension reduction eliminates redundancy in the training process using an implicit matrix structure we derive iterative method that converge quickly a second method for dimension reduction involves a novel application of random dimension reduction to feature space the combination of these algorithm produce a method for training polynomial network on large data set with decreased computation over traditional method and model complexity reduction and control 
we discus an information theoretic approach for categorizing and modeling dynamic process the approach can learn a compact and informative statistic which summarizes past state to predict future observation furthermore the uncertainty of the prediction is characterized nonparametrically by a joint density over the learned statistic and present observation we discus the application of the technique to both noise driven dynamical system and random process sampled from a density which is conditioned on the past in the first case we show result in which both the dynamic of random walk and the statistic of the driving noise are captured in the second case we present result in which a summarizing statistic is learned on noisy random telegraph wave with differing dependency on past state in both case the algorithm yield a principled approach for discriminating process with differing dynamic and or dependency the method is grounded in idea from information theory and nonparametric statistic 
data visualization and feature selection method are proposedbased on the joint mutual information and ica the visualizationmethods can find many good d projection for high dimensionaldata interpretation which cannot be easily found by the other existingmethods the new variable selection method is found to bebetter in eliminating redundancy in the input than other methodsbased on simple mutual information the efficacy of the methodsis illustrated on a radar signal analysis 
data independent sample bound are known to grossly overestimate the amount of data needed for most individual problem instance this ha led to significant recent interest in sequential algorithm which also give precise guarantee about the quality of result but determine the amount of data needed based on characteristic of the actual problem instance at hand and thus need significantly fewer example in this paper we present a practical sequential sampling algorithm which a is 
the snow sparse network of winnow architecture hasrecently been successful applied to a number of natural languageprocessing nlp problem in this paper we proposelarge margin version of the winnow algorithm which weargue can potentially enhance the performance of basic winnow and hence the snow architecture we demonstratethat the resulting method achieve performance comparablewith support vector machine for text categorization application we also explain why both large 
novelty detection involves modeling the normal behaviour of a systemhence enabling detection of any divergence from normality ithas potential application in many area such a detection of machinedamage or highlighting abnormal feature in medical data one approach is to build a hypothesis estimating the support ofthe normal data i e constructing a function which is positive in theregion where the data is located and negative elsewhere recentlykernel method have been proposed 
we examine a psychophysical law that describes the inuence ofstimulus and context on perception according to this law choiceprobability ratio factorize into component independently controlledby stimulus and context it ha been argued that this patternof result is incompatible with feedback model of perception in this paper we examine this claim using neural network modelsdened via stochastic dierential equation we show that the lawis related to a condition named channel 
the curse of dimensionality is severe when modeling high dimensional discrete data the number of possible combination of the variable explodes exponentially in this paper we propose a new architecture for modeling high dimensional data that requires resource parameter and computation that grow only at most a the square of the number of variable using a multi layer neural network to represent the joint distribution of the variable a the product of conditional distribution the neural network can be interpreted a a graphical model without hidden random variable but in which the conditional distribution are tied through thehiddenunits theconnectivityoftheneuralnetworkcanbe prunedby using dependency test between the variable experiment on modeling the distribution of several discrete data set show statistically significant improvement over other method such a naive bayes and comparable bayesian network and show that significant improvement can be obtained by pruning the network 
we consider noisy euclidean traveling salesman problem in the plane which are random combinatorial problem with underlying structure gibbs sampling is used to compute average trajectory which estimate the underlying structure common to all instance this procedure requires identifying the exact relationship between permutation and tour in a learning setting the average trajectory is used a a model to construct solution to new instance sampled from the same source experimental result show that the average trajectory can in fact estimate the underlying structure and that overtting eects occur if the trajectory adapts too closely to a single instance 
we present a generalization of frequent itemsets allowing for the notion of error in the itemset definition we motivate the problem and present an efficient algorithm that identifies error tolerant frequent cluster of item in transactional data customer purchase data web browsing data text etc the algorithm exploit sparseness of the underlying data to find large group of item that are correlated over database record row the notion of transaction coverage allows u to extend the algorithm and view it a a fast clustering algorithm for discovering segment of similar transaction in binary sparse data we evaluate the new algorithm on three real world application clustering high dimensional data query selectivity estimation and collaborative filtering result show that the algorithm consistently uncovers structure in large sparse database that other traditional clustering algorithm fail to find 
this paper describes a clustering algorithm for vector quantizers using a stochastic association model it offer a new simple and powerful softmax adaptation rule the adaptation process is the same a the on line k mean clustering method except for adding random fluctuation in the distortion error evaluation process simulation result demonstrate that the new algorithm can achieve efficient adaptation a high a the neural gas algorithm which is reported a one of the most efficient clustering method it is a key to add uncorrelated random fluctuation in the similarity evaluation process for each reference vector for hardware implementation of this process we propose a nanostructure whose operation is described by a single electron circuit it positively us fluctuation in quantum mechanical tunneling process 
we generalize a recent formalism to describe the dynamic of supervised learning in layered neural network in the regime where data recycling is inevitable to the case of noisy teacher our theory generates reliable prediction for the evolution in time of trainingand generalization error and extends the class of mathematically solvable learning process in large neural network to those situation where overfitting can occur 
neuron receive excitatory input via both fast ampa and slownmda type receptor wend that neuron receiving input vianmda receptor can have two stable membrane state which areinput dependent action potential can only be initiated from thehigher voltage state similar observation have been made in severalbrain area which might be explained by our model the interactionsbetween the two kind of input lead u to suggest thatsome neuron may operate in state disabled 
we present a new learning architecture the decision directed acyclic graph ddag which is used to combine many two class classifier into a multiclass classifier for an n class problem the ddag contains n n classifier one for each pair of class we present a vc analysis of the case when the node classifier are hyperplanes the resulting bound on the test error depends on n and on the margin achieved at the node but not on the dimension of the space this motivates an algorithm 
we analyze the asymptotic behavior of autoregressive neural network ar nn process using technique from markov chain and non linear time series analysis it is shown that standard ar nns without shortcut connection are asymptotically stationary if linear shortcut connection are allowed only the shortcut weight determine whether the overall system is stationary hence standard condition for linear ar process can be used 
we model hippocampal place cell and head direction cell by combining allothetic visual and idiothetic proprioceptive s timuli visual input provided by a video camera on a miniature robot is preprocessed by a set of gabor filter on node of a log polar retinotopic graph unsupervised hebbian learning is employed to incrementally build a population of localized overlapping place field place cell serv e a basis function for reinforcement learning experimental result fo r goal oriented navigation of a mobile robot are presented 
we present an approach to reward maximizationin a non stationary mobile robot environment the approach work within therealistic constraint of limited local sensingand limited a priori knowledge of the environment it is based on the use of augmentedmarkov model amms a general modelingtool we have developed amms are essentiallymarkov chain having additional statisticsassociated with state and state transition we have developed an algorithm thatconstructs amms 
research in feature selection ha paid littleattention to unsupervised learning in thispaper we follow the guideline suggested inprevious work by gennari and present someempirical result in incremental learning ofprobabilistic concept hierarchy we identifydierent type of feature selection and justifythe use of method that run in parallel withlearning and individually select a dierent setof feature for each node in the hierarchy weuse a very simple and inexpensive 
in this paper we use mutual information to characterize the distributionsof phonetic and speaker channel information in a timefrequencyspace the mutual information mi between the phoneticlabel and one feature and the joint mutual information jmi between the phonetic label and two or three feature are estimated the miller s bias formula for entropy and mutual information estimatesare extended to include higher order term the mi andthe jmi for speaker channel recognition 
this paper present autodj a system for automatically generating music play list based on one or more seed song selected by a user autodj us gaus sian process regression to learn a user preference function over song this function take music metadata a input this paper further introduces kernel meta training which is a method of learning a gaussian process kernel from a distribution of function that generates the learned function for playlist gen eration autodj learns a kernel from a large set of album this learned kernel is shown to be more effective at predicting user playlist than a reasonable hand designed kernel 
we study novel aspect of multi agent qlearningin a model market in which twoidentical competing quot pricebots quot strategicallyprice a commodity two fundamentallydifferent solution are observed an exact stationary solution with zero bellman errorconsisting of symmetric policy anda non stationary broken symmetry pseudosolution with small but non zero bellmanerror this quot pseudo convergent quot asymmetricsolution ha no analog in ordinary qlearning we calculate analytically 
many application dealing with textual information require classification of word into semantic class or concept however manually constructing semantic class is a tedious task in this paper we present an algorithm unicon for unsupervised induction of concept some advantage of unicon over previous approach include the ability to classify word with low frequency count the ability to cluster a large number of element in a high dimensional space and the ability to classify previously unknown word into existing cluster furthermore since the algorithm is unsupervised a set of concept may be constructed for any corpus 
a serious problem in learning probabilistic model is the presence of hidden variable these variable are not observed yet interact with several of the observed variable a such they induce seemingly complex dependency among the latter in recent year much attention ha been devoted to the development of algorithm for learning parameter and in some case structure in the presence of hidden variable in this paper we address the related problem of detecting hidden variable that interact with the observed variable this problem is of interest both for improving our understanding of the domain and a a preliminary step that guide the learning procedure towards promising model a very natural approach is to search for structural signature of hidden variable substructure in the learned network that tend to suggest the presence of a hidden variable we make this basic idea concrete and show how to integrate it with structure search algorithm we evaluate this method on several synthetic and real life datasets and show that it performs surprisingly well 
a new class of support vector machine svm that is applicable to sequential pattern recognition such a speech recognition is developed by incorporating an idea of non linear time alignment into the kernel function since the time alignment operation of sequential pattern is embedded in the new kernel function standard svm training and classication algorithm can be employed without further modications the proposed svm dtak svm is evaluated in speaker dependent speech recognition experiment of hand segmented phoneme recognition preliminary experimental result show comparable recognition performance with hidden markov model hmms 
in this paper we develop the method of bounding the generalization error of a classifier in term of it margin distribution which wa i ntroduced in the recent paper of bartlett and schapire freund bartlet t and lee the theory of gaussian and empirical process allow u to prove the margin type inequality for the most general functional class the complexity of the class being measured via the so called gaussian complexity function a a simple application of our result we obtain the b ounds of schapire freund bartlett and lee for the generalization e rror of boosting we also substantially improve the result of bartlett o n bounding the generalization error of neural network in term of norm of the weight of neuron furthermore under additional assumption on the complexity of the class of hypothesis we provide some tighter bound which in the case of boosting improve the result of schapire freund bartlett and lee 
we consider the problem of reconstructing a temporal discrete sequenceof multidimensional real vector when part of the data is missing underthe assumption that the sequence wa generated by a continuous process a particular case of this problem is multivariate regression whichis very difficult when the underlying mapping is one to many we proposean algorithm based on a joint probability model of the variablesof interest implemented using a nonlinear latent variable model eachpoint 
traditional anomaly detection technique focus on detecting anomaly in new data after training on normal or clean data in this paper we present a technique for detecting anomaly without training on normal data we present a method for detecting anomaly within a data set that contains a large number of normal element and relatively few anomaly we present a mixture model for explaining the presence of anomaly in the data motivated by the model the approach us machine learning 
the reinforcement learning problem can be decomposed into two parallel type of inference i estimating the parameter of a model for the underlying process ii determining behavior which maximizes return under the estimated model following dearden friedman and andre it is proposed that the learning process estimate online the full posterior distribution over model to determine behavior a hypothesis is sampled from this distribution and the greedy policy with respect to the hypothesis is obtained by dynamic programming by using a different hypothesis for each trial appropriate exploratory and exploitative behavior is obtained this bayesian method always converges to the optimal policy for a stationary process with discrete state 
a very simple model of two reciprocally connected attractor neural network is studied analytically in situation similar to those encountered in delay match to sample task with intervening stimulus and in task of memory guided attention the model qualitatively reproduces many of the experimental data on these type of task and provides a framework for the understanding of the experimental observation in the context of the attractor neural network scenario 
massive transaction data set are recorded in a routine manner in telecommunication retail commerce and web site management in this paper we address the problem of inferring predictive individual proflles from such historical transaction data we describe a generative mixture model for count data and use an an approximate bayesian estimation framework that efiectively combine an individual s speciflc history with more general population pattern we use a large real world retail transaction data set to illustrate how these proflles consistently outperform non mixture and non bayesian technique in predicting customer behavior in out of sample data 
the first step prior to data mining is often to merge database from different source entry in these database or description retrieved using information extraction may use significantly different vocabulary so one often need to determine whether similar description refer to the same item or to different item e g people or good string edit distance is an elegant way of defining the degree of similarity between entry and can be efficiently computed using dynamic programming ristad and yianilos however in order to achieve reasonable accuracy most real problem require the use of extended set of edit rule with associated cost that are tuned specifically to each data set we present a flexible approach to string edit distance which can be automatically tuned to different data set and can use synonym dictionary dynamic programming is used to calculate the edit distance between a pair of string based on a set of string edit rule including a new edit rule that allows word and phrase to be deleted or substituted a genetic algorithm is used to learn cost corresponding to each edit rule based on a small set of labeled training data deleting content le word like method and substituting synonym such a ibuprofen for motrin significantly increase the algorithm s accuracy from to on a difficult sample medical data set when cost are correctly tuned this string edit based matching tool is easily adapted for a variety of different case when one need to recognize which text string from different information source refer to the same item such a a person address medical procedure or product 
fisher linear discriminant analysis lda is a classical multivariatetechnique both for dimension reduction and classication the data vectorsare transformed into a low dimensional subspace such that the classcentroids are spread out a much a possible in this subspace lda worksas a simple prototype classier the resulting decision boundary arelinear however in many application the linear boundary do not adequatelyseparate the class and the possibility of modeling more 
nearest neighbor is a well known algorithmextensively studied by the pattern recognitionand machine learning community andwidely exploited in case based reasoningapplications the notion of metric is centralto nearest neighbor s working and differentfeature weighting metric have been proposedin order to increase it performance in this work we present an original probabilitybased metric i e a metric for classificationtasks that relies on estimate ofthe posterior 
nearest neighbor classification assumes locally constant class conditional probability this assumption becomes invalid in high dimension with finite sample due to the curse of dimensionality severe bias can be introduced under these condition when using the nearest neighbor rule we propose a locally adaptive nearest neighbor classification method to try to minimize bias we use a chi squared distance analysis to compute a flexible metric for producing neighborhood that are elongated 
in this work we propose a generalisation of the notion of association rule in the context of flat transaction to that of a composite association rule in the context of a structured directed graph such a the world wide web the technique proposed aim at finding pattern in che user behaviour when traversing such a hypertext system we redefine the concept of confidence and support for composite association rule and two algorithm to mine such rule are proposed extensive experiment with random data were conducted and the result show that in spite of the worst case complexity analysis which indicates exponential behaviour in practice the algorithm complexity measured in the number of iteration performed is linear in the number of node traversed 
experimental data show that biological synapsis behave quite differently from the symbolic synapsis in common artificial neural network model biological synapsis are dynamic i e their weight change on a short time scale by several hundred percent in dependence of the past input to the synapse in this article we explore the consequence that these synaptic dynamic entail for the computational power of feedforward neural network we show that gradient descent suffices to approximate a given quadratic filter by a rather small neural system with dynamic synapsis we also compare our network model to artificial neural network designed for time series processing our numerical result are complemented by theoretical analysis which show that even with just a single hidden layer such network can approximate a surprisingly large large class of nonlinear filter all filter that can be characterized by volterra series this result is robust with regard to various change in the model for synaptic dynamic 
new functionals for parameter model selection of support vector machine are introduced based on the concept of the span of support vector and rescaling of the feature space it is shown that usin g these functionals one can both predict the best choice of parameter of the model and the relative quality of performance for any value of parameter 
we present an algorithm which compensates for the mismatchesbetween characteristic of real world problem and assumption ofindependent component analysis algorithm to provide additionalinformation to the ica network we incorporate top down selectiveattention an mlp classier is added to the separated signalchannel and the error of the classier is backpropagated to theica network this backpropagation process result in estimationof expected ica output signal for the top down 
machine learning research ha been very successful at producing powerful broadlyapplicable classification learner however many practical learning problem do not fit the classification framework well and a a result the initial phase of suitably formulating the problem and incorporating the relevant domain knowledge can be very difficult and time consuming here we propose a framework to systematize and speed this process based on the notion of version space algebra we extend the notion of version space beyond concept learning and propose that carefully tailored version space for complex application can be built by composing simpler restricted version space we illustrate our approach with smartedit a programming by demonstration application for repetitive text editing that us version space algebra to guide a search over text editing action sequence we demonstrate the system on a suite of repetitive text editing problem and present experimental result showing it effectiveness in learning from a small number of example 
bioinformatics is the study of information flow in biology interest in the field ha exploded in the last year with the emergence of technique for large scale experimental data collection including genome sequencing gene expression analysis protein interaction detection high throughput structure determination and others these technique in the context of a large online published literature have created relatively large data set at least by biological standard that are not possible to analyze manually there is therefore a critical need for method to analyze these data and reduce them to new knowledge the principle challenge to the field include the great diversity of data type and question that are asked of the data and the communication difficulty that can exist between expert in biology and expert in machine learning in this talk i will provide an introduction to the major biological question that are being addressed why they are important and how the field is trying to address them with technical approach 
this paper argues that two apparently distinct mode of generalizing con cepts abstracting rule and computing similarity to exemplar should both be seen a special case of a more general bayesian learning frame work bayes explains the specific working of these two mode which rule are abstracted how similarity is measured a well a why gener alization should appear rule or similarity based in different situation this analysis also suggests why the rule similarity distinction even if not computationally fundamental may still be useful at the algorithmic level a part of a principled approximation to fully bayesian learning 
tangential hand velocity profile of rapid human arm movement often appear a sequence of several bell shaped acceleration deceleration phase called submovements or movement unit this suggests how the nervous system might efficiently control a motor plant in the presence of noise and feedback delay another critical observation is that stochasticity in a motor control problem make the optimal control policy essentially different from the optimal control policy for the deterministic case we use a simplified dynamic model of an arm and address rapid aimed arm movement we use reinforcement learning a a tool to approximate the optimal policy in the presence of noise and feedback delay using a simplified model we show that multiple submovements emerge a an optimal policy in the presence of noise and feedback delay the optimal policy in this situation is to drive the arm s end point close to the target by one fast submovement and then apply a few slow submovements to accurately drive the arm s end point into the target region in our simulation the controller sometimes generates corrective submovements before the initial fast submovement is completed much like the predictive correction observed in a number of psychophysical experiment 
unsupervised learning algorithm are designed to extract structurefrom data sample reliable and robust inference requires a guaranteethat extracted structure are typical for the data source i e similar structure have to be infered from a second sample set of thesame data source the overtting phenomenon in maximum entropybased annealing algorithm is exemplarily studied for a classof histogram clustering model bernstein s inequality for largedeviations is used to determine the 
we propose a method of approximate dynamic programming for markov decision process mdps using algebraic decision diagram add we produce near optimal value function and policy with much lower time and space requirement than exact dynamic programming our method reduces the size of the intermediate value function generated during value iteration by replacing the value at the terminal of the add with range of value our method is demonstrated on a class of large mdps with up to billion state and we compare the result with the optimal value function 
the support vector machine svm is known for it good performance in binary classification but it extension to multi class classification is still an on going research issue in this paper we propose a new approach for classification called the import vector machine ivm which is built on kernel logistic regression klr we show that the ivm not only performs a well a the svm in binary classification but also can naturally be generalized to the multi class case furthermore the ivm provides an estimate of the underlying probability similar to the support point of the svm the ivm model us only a fraction of the training data to index kernel basis function typically a much smaller fraction than the svm this give the ivm a computational advantage over the svm especially when the size of the training data set is large is the conditional probability of a point being in class given in this paper we propose a new approach called the import vector machine ivm to address the classification problem we show that the ivm not only performs a well a the svm in binary classification but also can naturally be generalized to the multi class case furthermore the ivm provides an estimate of the probability similar to the support point of the svm the ivm model us only a fraction of the training data to index the kernel basis function we call these training data import point the computational cost of the svm is while the computational cost of the ivm is 
before applying learning algorithm to datasets practitioner often globally discretize any numeric attribute if the algorithm cannot handle numeric attribute directly prior discretization is essential even if it can prior discretization often accelerates induction and may produce simpler and more accurate classifier a it is generally done global discretization denies the learning algorithm any chance of taking advantage of the ordering information implicit in numeric attribute however a simple transformation of discretized data preserve this information in a form that learner can use we show that compared to using the discretized data directly this transformation significantly increase the accuracy of decision tree built by c decision list built by part and decision table built using the wrapper method on several benchmark datasets moreover it can significantly reduce the size of the resulting classifier this simple technique make global discretization an even more useful tool for data preprocessing 
we present a computational model of the neural mechanism in the parietal and temporal lobe that support spatial navigation recall of scene and imagery of the product of recall long term representation are stored in the hippocampus and are associated with local spatial and object related feature in the parahippocampal region viewer centered representation are dynamically generated from long term memory in the parietal part of the model the model thereby simulates recall and imagery of location and object in complex environment after parietal damage the model exhibit hemispatial neglect in mental imagery that rotates with the imagined perspective of the observer a in the famous milan square experiment our model make novel prediction for the neural representation in the parahippocampal and parietal region and for behavior in healthy volunteer and neuropsychological patient we perform spatial computation everday task such a reaching and navigating around visible obstacle are predominantly sensory driven rather than memory based and presumably rely upon egocentric or viewer centered representation of space these representation and the ability to translate between them have been accounted for in several computational model of the parietal cortex e g in other situation such a route planning recall and imagery for scene or event one must also reply upon representation of spatial layout from long term memory neuropsychological and neuroimaging study implicate both the parietal and hippocampal region in such task with the long term memory component associated with the hippocampus the discovery of place cell in the hippocampus provides evidence that hippocampal representation are allocentric in that absolute location in open space are encoded irrespective of viewing direction this paper address the nature and source of the spatial representation in the hippocampal and parietal region and how they interact during recall and navigation we assume that in the hippocampus proper long term spatial memory are stored allocentrically whereas in the parietal cortex view based image are created on the fly during perception or recall intuitively it make sense to use an allocentric representation for long term storage a the 
one of the major application of data mining is in helping company determine which potential customer to market to if the expected profit from a customer is greater than the cost of marketing to her the marketing action for that customer is executed so far work in this area ha considered only the intrinsic value of the customer i e the expected profit from sale to her we propose to model also the customer s network value the expected profit from sale to other customer she may influence to buy the customer those may influence and so on recursively instead of viewing a market a a set of independent entity we view it a a social network and model it a a markov random field we show the advantage of this approach using a social network mined from a collaborative filtering database marketing that exploit the network value of customer also known a viral marketing can be extremely effective but is still a black art our work can be viewed a a step towards providing a more solid foundation for it taking advantage of the availability of large relevant database 
we present new algorithm for the k mean clustering problem they use the kd tree data structure to reduce the large number ofnearest neighbor query issued by the traditional algorithm sufficientstatistics are stored in the node of the kd tree then an analysis ofthe geometry of the current cluster center result in great reductionof the work needed to update the center our algorithm behaveexactly a the traditional k mean algorithm proof of correctnessare included the 
a single signal processing algorithm can be represented by many mathematically equivalent formula however when these formula are implemented in code and run on real machine they have very different running time unfortunately it is extremely difficult to model this broad performance range further the space of formula for real signal transforms is so large that it is impossible to search it exhaustively for fast implementation we approach this search question a a control learning problem we present a new method for learning to generate fast formula allowing u to intelligently search through only the most promising formula our approach incorporates signal processing knowledge hardware feature and formula performance data to learn to construct fast formula our method learns from performance data for a few formula of one size and then can construct formula that will have the fastest run time possible across many size 
low dimensional representation are key to solving problem in highlevel vision such a face compression and recognition factorial coding strategy for reducing the redundancy present in natural i mages on the basis of their second order statistic have been successfu l in accounting for both psychophysical and neurophysiological property of early vision class specific representation are presumably for med later at the higher level stage of cortical processing here we sho w that when retinotopic factorial code are derived for ensemble of na tural object such a human face not only redundancy but also dimensionality is reduced we also show that object are built from part in a non gaussian fashion which allows these local feature code to have dimensionality that are substantially lower than the respective nyquist sa mpling rate 
we analyze the condition under which synaptic learning rule based on action potential timing can be approximated by learning rule based on ring rate in particular we consider a form of plasticity in which synapsis depress when a presynaptic spike is followed by a postsynaptic spike and potentiate with the opposite temporal ordering such differential anti hebbian plasticitycan be approximated under certain condition by a learning rule that depends on the time derivative of the postsynaptic ring rate such a learning rule act to stabilize persistent neural activity pattern in recurrent neural network 
if the promise of computational modeling is to be fully realized in higherlevel cognitive domain such a language processing principled method must be developed to construct the semantic representation used in such model in this paper we propose the use of an established formalism from mathematical psychology additive clustering a a mean of automatically constructing binary representation for object using only pairwise similarity data however existing method for the unsupervised learning of additive clustering model do not scale well to large problem we present a new algorithm for additive clustering based on a novel heuristic technique for combinatorial optimization the algorithm is simpler than previous formulation and make fewer independence assumption extensive empirical test on both human and synthetic data suggest that it is more effective than previous method and that it also scale better to larger problem by making additive clustering practical we take a significant step toward scaling connectionist model beyond hand coded example 
for many problem the correct behavior of a model depends not only on it input output mapping but also on property of it jacobian matrix the matrix of partial derivative of the model s output with respect to it input we introduce the j prop algorithm an efcient general method for computing the exact partial derivative of a variety of simple function of the jacobian of a model with respect to it free parameter the algorithm applies to any parametrized feedforward model including nonlinear regression multilayer perceptrons and radial basis function network 
this article present a support vector machine svm like learning system to handle multi label problem such problem are usually decomposed into many two class problem but the expressive power of such a system can be weak we explore a new direct approach it is based on a large margin ranking system that share a lot of common property with svms we tested it on a yeast gene functional classification problem with positive result 
we introduce a new regularization criterionthat exploit unlabeled data to adaptivelycontrol hypothesis complexity in generalsupervised learning task the technique isbased on an abstract metric space view ofsupervised learning that ha been successfullyapplied to model selection in previous research the new regularization criterion weintroduce involves no free parameter and yetperforms well on a variety of regression andconditional density estimation task the only 
the principle of maximizing mutual information is applied to learning overcomplete and recurrent representation the underlyi ng model consists of a network of input unit driving a larger number of ou tput unit with recurrent interaction in the limit of zero noise the network is deterministic and the mutual information can be related to the entropy of the output unit maximizing this entropy with respect to both the feedforward connection a well a the recurrent interaction r esults in simple learning rule for both set of parameter the conventiona l independent component ica learning algorithm can be recovered a a special case where there is an equal number of output unit and no recurrent connection the application of these new learning rule is ill ustrated on a simple two dimensional input example 
estimating insurance premia from data is a difficult regression problem for several reason the large number of variable many of which are discrete and the very peculiar shape of the noise distribution asymmetric with fat tail with a large majority zero and a few unreliable and very large value we introduce a methodology for estimating insurance premia that ha been applied in the car insurance industry it is based on mixture of specialized neural network in order to reduce the effect of outlier on the estimation statistical comparison with several different alternative including decision tree and generalized linear model show that the proposed method is significantly more precise allowing to identify the least and most risky contract and reducing the median premium by charging more to the most risky customer 
topic detection and tracking tdt is a variant of classicationin which the set of class grows over time this paper describesa new approach to tdt based on probabilistic generative model strong statistical technique address the many challenge hierarchicalshrinkage for sparse data statistical garbage collection quot fornew event detection clustering in time to separate the dierentevents of a common topic and deterministic annealing to createthe hierarchy preliminary 
abstract we discus the problem of ranking instance where an instance is associated with an integer from to in other word the specialization of the general multi class learning problem when there exists an ordering among the instance a problem known a ordinal regression or ranking learning this problem arises in various setti ng both in visual recognition and other information retrieval task in the context of applying a large margin principle to this learning proble m we introduce two main approach for implementing the large margin optimization criterion for margin the first is the fixed margin policy in which the margin of the closest neighboring class is being maximized which turn out to be a direct generalization of svm to ranking learning the second approach allows for different margin where the sum of margin is maximized thus effectively having the solution biased towards the pair of neighboring class which are the farthest apart from each other this approach is shown to reduce to when the number of class both approach are optimal in size of the dual functional of where is the total number of training example experiment performed on visual classification and collab orative filtering show that both approach outperform existing ordinal regression algorithm applied for ranking and multi class svm applied to general multi class classification 
this paper reveals a previously ignored connection between two important field regularization and independent component analysis ica we show that at least one representative of a broad class of algorithm regularizers that reduce network complexity extract independent feature a a by product this algorithm is flat minimum search fm a recent general method for finding low complexity network with high generalization capability fm work by minimizing both training error and 
we present a statistical approach to adapting the sample set size of particle filter on thefly the key idea of the kld sampling method is to bound the error introduced by the samplebased representation of the particle filter thereby our approach chooses a small number of sample if the density is focused on a small subspace of the state space and it chooses a large number of sample if the state uncertainty is high both the implementation and computation overhead of this approach are 
clustering is a widely used technique in data mining application to discover pattern in the underlying data most traditional clustering algorithm are limited to handling datasets that contain either continuous or categorical attribute however datasets with mixed type of attribute are common in real life data mining problem in this paper we propose a distance measure that enables clustering data with both continuous and categorical attribute this distance measure is derived from a probabilistic model that the distance between two cluster is equivalent to the decrease in log likelihood function a a result of merging calculation of this measure is memory efficient a it depends only on the merging cluster pair and not on all the other cluster zhang et al proposed a clustering method named birch that is especially suitable for very large datasets we develop a clustering algorithm using our distance measure based on the framework of birch similar to birch our algorithm first performs a pre clustering step by scanning the entire dataset and storing the dense region of data record in term of summary statistic a hierarchical clustering algorithm is then applied to cluster the dense region apart from the ability of handling mixed type of attribute our algorithm differs from birch in that we add a procedure that enables the algorithm to automatically determine the appropriate number of cluster and a new strategy of assigning cluster membership to noisy data for data with mixed type of attribute our experimental result confirm that the algorithm not only generates better quality cluster than the traditional k mean algorithm but also exhibit good scalability property and is able to identify the underlying number of cluster in the data correctly the algorithm is implemented in the commercial data mining tool clementine which support the pmml standard of data mining model deployment 
most machine learning algorithm are lazy they extract from the training set the minimum information needed to predict it label unfortunately this often lead to model that are not robust when feature are removed or obscured in future test data for example a backprop net trained to steer a car typically learns to recognize the edge of the road but doe not learn to recognize other feature such a the stripe painted on the road which could be useful when road edge disappear in tunnel or are obscured by passing truck the net learns the minimum necessary to steer on the training set in contrast human driving is remarkably robust a feature become obscured motivated by this we propose a framework for robust learning that bias induction to learn many different model from the same input we present a meta algorithm for robust learning called featureboost and demonstrate it on several problem using backprop net k nearest neighbor and decision tree 
separation of connected component from a graph with disconnected graph component mostly use breadth first search bfs or depth first search dfs graph algorithm here we propose a new algebraic method to separate disconnected and nearly disconnected component this method is based on spectral graph partitioning following a key observation that disconnected component will show up after properly sorted a step function like curve in the lowest eigenvectors of the laplacian matrix of the graph following an perturbative analysis framework we systematically analyzed the graph structure first on the disconnected subgraph case and second on the effect of adding edge sparsely connecting different subgraphs a a perturbation several new result are derived providing insight to spectral method and related clustering objective function example are given illustrating the concept and result our method comparing to the standard graph algorithm this method ha the same o verbar e verbar verbar v verbar log verbar v verbar complexity but is easier to implement using readily available eigensolvers further more the method can easily identify articulation point and bridge on nearly disconnected graph segmentation of a real example of web graph for query amazon is given we found that each disconnected or nearly disconnected component form a cluster on a clear topic 
we address two open theoretical question in policy gradient reinforcement learning the first concern the efficacy of using function approximation to represent the state action value function q theory is presented showing that linear function approximation representation of q can degrade the rate of convergence of performance gradient estimate by a factor of o ml relative to when no function approximation of q is used where m is the number of possible action and l is the number of basis function in the function approximation representation the second concern the use of a bias term in estimating the state action value function theory is presented showing that a non zero bias term can improve the rate of convergence of performance gradient estimate by o m where m is the number of possible action experimental evidence is presented showing that these theoretical result lead to significant improvement in the convergence property of policy gradient reinforcement learning algorithm 
how should we decide among competing explanation of a cognitive process given limited observation the problem of model selection is at the heart of progress in cognitive science in this paper minimum description length mdl is introduced a a method for selecting among computational model of cognition we also show that differential geometry provides an intuitive understanding of what drive model selection in mdl finally adequacy of mdl is demonstrated in two area of cognitive modeling 
the rule based bootstrapping introduced by yarowsky and it cotraining variant by blum and mitchell have met with considerable empirical success earlier work on the theory of co training ha been only loosely related to empirically useful co training algorithm here we give a new pac style bound on generalization error which justifies both the use of confidence partial rule and partial labeling of the unlabeled data and the use of an agreement based objective function a suggested by collins and singer our bound apply to the multiclass case i e where instance are to be assigned one of label for one well known form of bootstrapping is the em algorithm dempster laird and rubin this algorithm iteratively update model parameter by using the current model to infer a probability distribution on label for the unlabeled data and then adjusting the model parameter to fit the distribution on filled in label when the model defines a joint probability distribution over observable data and unobservable label each iteration of the em algorithm can be shown to increase the probability of the observable data given the model parameter however em is often subject to local minimum situation in which the filled in data and the model parameter fit each other well but the model parameter are far from their maximum likelihood value furthermore even if em doe find the globally optimal maximum likelihood parameter a model with a large number of parameter will over fit the data no pac style guarantee ha yet been given for the generalization accuracy of the maximum likelihood model 
when trying to recover d structure from a set of image themost difficult problem is establishing the correspondence betweenthe measurement most existing approach assume that featurescan be tracked across frame whereas method that exploit rigidityconstraints to facilitate matching do so only under restricted cameramotion in this paper we propose a bayesian approach thatavoids the brittleness associated with singling out one quot best quot correspondence and instead consider the 
we report a result of perturbation analysis on decoding error of the belief propagation decoder for gallager code the analysis is based on information geometry and it show that the principal term of decoding error at equilibrium come from the m embedding curvature of the log linear submanifold spanned by the estimated pseudoposteriors one for the full marginal and k for partial posterior each of which take a single check into account where k is the number of check in the gallager code it is then shown that the principal error term vanishes when the parity check matrix of the code is so sparse that there are no two column with overlap greater than 
team partitioned opaque transition reinforcement learning tpot rl is a distributed reinforcement learning technique that allows a team of independent agent to learn a collaborative task tpot rl wa first successfully applied to simulated robotic soccer stone veloso this paper demonstrates that tpot rl is general enough to apply to a completely different domain namely network packet routing empirical result in an abstract network routing simulator indicate that agent situated at individual node can learn to efficiently route packet through a network that exhibit changing traffic pattern based on locally observable sensation 
we investigate the short term dynamic of the recurrent competition and neural activity in the primary visual cortex in term of info rmation processing and in the context of orientation selectivity we pr opose that after stimulus onset the strength of the recurrent excitatio n decrease due to fast synaptic depression a a consequence the network shift from an initially highly nonlinear to a more linear operating reg ime sharp orientation tuning is established in the first highly compet itive phase in the second and le competitive phase precise signaling of multiple orientation and long range modulation e g by intraand inter areal connection becomes possible surround effect thus the network first extract the salient feature from the stimulus and then st art to process the detail we show that this signal processing strategy is optimal if the neuron have limited bandwidth and their objective is to transmit the maximum amount of information in any time interval beginning with the stimulus onset 
abstract learning real time a lrta is a popular control method that interleaf planning and plan execution and ha been shown to solve search problem in known environment efficiently in this paper we apply lrta to the problem of getting to a given goal location in an initially unknown environment uninformed lrta with maximal lookahead always move on a shortest path to the closest unvisited state that is to the closest potential goal state this wa believ ed to be a good exploration heuristic but we show that it doe not minimize the worst case plan execution time compared to other uninformed exploration method this result is also of interest to reinforcement learning researcher since many reinforce ment learning method use asynchronous dynamic programming interleave planning and plan execution and exhibit optimism in the face of uncertainty just like lrta 
in this paper we discus some new research direction in automaticspeech recognition asr and which somewhat deviate from theusual approach more specically we will motivate and brieydescribe new approach based on multi stream and multi bandasr these approach extend the standard hidden markov model hmm based approach by assuming that the dierent frequency channel representing the speech signal are processed by dierent independent expert quot each expert focusing 
experimental data ha shown that synaptic strength modificationin some type of biological neuron depends upon precise spike timingdifferences between presynaptic and postsynaptic spike severaltemporally asymmetric hebbian learning rule motivated bythis data have been proposed we argue that such learning rulesare suitable to analog vlsi implementation we describe an easilytunable circuit to modify the weight of a silicon spiking neuronaccording to those learning rule test 
algorithm for feature selection fall into twobroad category wrapper that use thelearning algorithm itself to evaluate the usefulnessof feature and lters that evaluatefeatures according to heuristic based on generalcharacteristics of the data for applicationto large database lters have provento be more practical than wrapper becausethey are much faster however most existinglter algorithm only work with discreteclassication problem this paper describesa 
one general strategy for approximatelysolving large markov decision process is quot divide and conquer quot the original problemis decomposed into sub problem which interactwith each other but yet can besolved independently by taking into accountthe nature of the interaction in this paperwe focus on a class of quot policy coupled quot semi markov decision process smdps which arise in many nonstationary real worldmulti agent task such a manufacturing androbotics the nature of the 
the technology of data warehousing data mining hypertext analysis information visualization and web information resource are rapidly converging the challenge is to architect these technology into a system for systematic business intelligence for a corporation we need to move from an information refining process that is often haphazard and narrow to one that is reliable and continuous web farming is a new area that suggests a methodology and architecture for accomplishing this 
we show that support vector machine svm s are much better than conventional algorithm in a relevancy feedback rf environment in information retrieval ir of text document we track performance a a function of feedback iteration and show that while the conventional algorithm do very well in the initial feedback iteration if the topic searched for ha high visibility in the data base they do very poorly if the relevant document are a small percentage of the data base svm s however do very well when the number of document returned in the preliminary search is low and the number of relevant document is small the competitive algorithm examined are rocchio ide regular and ide dec hi 
in this paper we discus the semiparametric statistical model for blind deconvolution first we introduce a lie group to the manifold of noncausal fir filter then blind deconvolution problem is formulated in the framework of a semiparametric model and a family of estimating function is derived for blind deconvolution a natural gradient learning algorithm is developed for training noncausal filter stability of the natural gradient algorithm is also analyzed in this framework 
the article focus on distributed reinforcementlearning in cooperative multiagent decision process where an ensembleof simultaneously and independently actingagents try to maximize a discounted sumof reward we assume that each agenthas no information about it teammate behaviour thus in contrast to single agentreinforcement learning each agent ha to considerits teammate behaviour and to nd acooperative policy we propose a model freedistributed q learning 
abstract a new algorithm for support vector regression is described for a priori chosen it automatically adjusts a flexible tube of minimal radius to the data such that at most a fraction of the data point lie outside moreover it is shown how to use parametric tube shape with non constant radius the algorithm is analysed theoretically and experimentally 
abstract locally weighted projection regression is a new algorithm that achieves nonlinear function approximation in high dimensional space with redundant and irrelevant input dimension at it core it us locally linear model spanned by a small number of univariate regression in selected direction in input space this paper evaluates different method of projection regression and derives a nonlinear function approximator based on them this nonparametric local learning system i learns rapidly with second order learning method based on incremental training ii us statistically sound stochastic cross validation to learn iii adjusts it weighting kernel based on local information only iv ha a computational complexity that is linear in the number of input and v can deal with a large number of possibly redundant input a shown in evaluation with up to dimensional data set to our knowledge this is the first truly incremental spatially localized learning method to combine all these property 
mesoscopical mathematical description of dynamic of populationsof spiking neuron are getting increasingly important for theunderstanding of large scale process in the brain using simulation 
we introduce the first algorithm for off policy temporal difference learning that is stable with linear function approximation off policy learning is of interest because it form the basis for popular reinforcement learning method such a q learning which ha been known to diverge with linear function approximation and because it is critical to the practical utility of multi scale multi goal learning framework such a option ham and maxq our new algorithm combine td over state action pair with importance sampling idea from our previous work we prove that given training under any soft policy the algorithm converges w p to a close approximation a in tsitsiklis and van roy tadic to the action value function for an arbitrary target policy variation of the algorithm designed to reduce variance introduce additional bias but are also guaranteed convergent we also illustrate our method empirically on a small policy evaluation problem our current result are limited to episodic task with episode of bounded length 
this paper considers the framework of the so called market basket problem in which a database of transaction is mined for the occurrence of unusually frequent item set in our case unusually frequent involves estimate of the frequency of each item set divided by a baseline frequency computed a if item occurred independently the focus is on obtaining reliable estimate of this measure of interestingness for all item set even item set with relatively low frequency for example in a medical database of patient history unusual item set including the item patient death or other serious adverse event might hopefully be flagged with a few a or occurrence of the item set it being unacceptable to require that item set occur in a many a of million of patient report before the data mining algorithm detects a signal similar consideration apply in fraud detection application thus we abandon the requirement that interesting item set must contain a relatively large fixed minimal support and adopt a criterion based on the result of fitting an empirical bayes model to the item set count the model allows u to define a bayesian lower confidence limit for the interestingness measure of every item set whereupon the item set can be ranked according to their empirical bayes confidence limit for item set of size j we also distinguish between multi item association that can be explained by the observed j j pairwise association and item set that are significantly more frequent than their pairwise association would suggest such item set can uncover complex or synergistic mechanism generating multi item association this methodology ha been applied within the u s food and drug administration fda to database of adverse drug reaction report and within at t to customer international calling history we also present graphical technique for exploring and understanding the modeling result 
in the present study we introduce a simple iterative procedure that allows to correct the output of a classifler with respect to the new a priori probability of a new data set to be scored even when these new a priori probability are unknown in advance we also show that a signiflcant increase in classiflcation accuracy can be observed when using this procedure properly more speciflcally by applying the correcting procedure to the output of a simple logistic regression model we observe an increase of of classiflcation rate on a di cult real world multi class problem the automatic labeling of geographical map based on remote sensing information moreover the resulting classifler the logistic regression model whose output have been adjusted according to our procedure outperformed by more that all of our previous model in term of classiflcation accuracy including bagfs a multiple classifler system based on c decision tree the best obtained model up to now 
this report present a svm like learning system to handle multi label problem such problem arise naturally in bio informatics consider for instance the mips yeast genome database in it is formed by around gene associated to their functional class one gene can have many class and different gene do not belong to the same number of functional category such a problem can not be solved directly with classical approach and it is ge nerally decomposed into many two class problem the binary decomposition ha been done partially by different researcher on the yeast dataset but it doe not provide a satisfactory answer we exp lore in this report a new direct approach it is based on a large margin ranking system that share a lot of c ommon property with support vector machine we tested it on a toy problem and on real datasets with positive result we also present a new method to do feature selection with multi labelled datasets our method is based on the multiplicative update rule defined in 
we propose a new and efficient technique for incorporating co ntextual information into object classification most of the current techniq ues face the problem of exponential computation cost in this paper we propose a new general framework that incorporates partial context at a linear cost this technique is applied to microscopic urinalysis image recognition resulting in a significant improvement of recognition rate over the context free approach thi s gain would have been impossible using conventional context incorporation technique 
abstract to classify a large number of unlabeled example we combine a limited number of labeled example with a markov random walk representation over the unlabeled example the random walk representation exploit any low dimensional structure in the data in a robust probabilistic manner we develop and compare several estimation criterion algorithm suited to this representation this includes in particular multi way classification with an average margin criterion which permit a closed form solution the time scale of the random walk regularizes the representation and can be set through a margin based criterion favoring unambiguous classification we also extend this basic regularization by adapting time scale for individual example we demonstrate the approach on synthetic example and on text classification problem 
we propose a locally adaptive technique toaddress the problem of setting the bandwidthparameters optimally for kernel density estimation our technique is efficient and canbe performed in only two dataset pass wealso show how to apply our technique toefficiently solve range query approximation classification and clustering problem for verylarge datasets we validate the efficiency andaccuracy of our technique by presenting experimentalresults on a variety of both 
the speech waveform can be modelled a a piecewise stationary linear stochastic state space system and it parameter can be est imated using an expectation maximisation em algorithm one problem is the initialisation of the em algorithm standard initialisation s chemes can lead to poor formant trajectory but these trajectory howev er are important for vowel intelligibility the aim of this paper is to in vestigate the suitability of subspace identification method to initiali se em the paper compare the subspace state space system identific ation sid method with the em algorithm the sid and em method are similar in that they both estimate a state sequence but usin g kalman filter and kalman smoother respectively and then estimate parameter but using least square and maximum likelihood respectively the similarity of sid and em motivates the use of sid to initialise em also sid is non iterative and requires no initialisation wher ea em is iterative and requires initialisation however sid is sub opti mal compared to em in a probabilistic sense during experiment on real speech sid method compare favourably with conventional initialisation technique they produce smoother formant trajectory have greater f requency resolution and produce higher likelihood 
abstract a neural model is described which us oscillatory correlation to segregate speech from interfering sound source the core of the model is a two layer neural oscillator network a sound stream is represented by a synchronized population of oscillator and different stream are represented by desynchronized oscillator population the model ha been evaluated using a corpus of speech mixed with interfering sound and produce an improvement in signal to noise ratio for every mixture 
we propose an algorithm eciently implementing td using the innite tree of haar basis function the algorithm canmaintain and update the information of theinnite tree of coecients in it nitely compressedform by taking advantage of the factthat the information obtained from nitetraining data is nite our algorithm computesthe whole updating at each time stepin time linear in the precision measured bythe number of bit of each observation thesystem of haar 
this paper describes a method of dogleg trust region step or restrictedlevenberg marquardt step based on a projection processonto the krylov subspace for neural network nonlinear leastsquares problem in particular the linear conjugate gradient cg method work a the inner iterative algorithm for solving the linearizedgauss newton normal equation whereas the outer nonlinearalgorithm repeatedly take so called quot krylov dogleg quot step relyingonly on matrix vector 
greedy approximation algorithm have been frequently used to obtain sparse solution to learning problem in this paper we present a general greedy algorithm for solving a class of convex optimization problem we derive a bound on the rate of approximation for this algorithm and show that our algorithm includes a number of earlier study a special case 
abstract this paper examines the application of reinforcement learning to a telecommunication networking problem the problem requires that revenue be maximized while simultaneously meeting a quality of service constraint that forbids entry into certain state we present a general solution to this multi criterion problem that is able to earn significan tly higher revenue than alternative 
we present the application of feature mining technique to the developmental therapeutic program s aid antiviral screen database the database consists of compound which were measured for their capability to protect human cell from hiv infection according to these measurement the compound were classified a either active moderately active or inactive the distribution of class is extremely skewed only of the molecule is known to be active and is known to be moderately active given this database we were interested in molecular substructure i e feature that are frequent in the active molecule and infrequent in the inactives in data mining term we focused on feature with a minimum support in active compound and a maximum support in inactive compound we analyzed the database using the levelwise version space algorithm that form the basis of the inductive query and database system molfea molecular feature miner within this framework it is possible to declaratively specify the feature of interest such a the frequency of feature on possibly different datasets a well a on the generality and syntax of them assuming that the detected substructure are causally related to biochemical mechanism it should be possible to facilitate the development of new pharmaceutical with improved activity 
we propose a new approach to reinforcement learning which combine least square function approximation with policy iteration our method is model free and completely off policy we are motivated by the least square temporal difference learning algorithm lstd which is known for it efficient use of sample experience compared to pure temporal difference algorithm lstd is ideal for prediction problem however it heretofore ha not had a straightforward application to control problem moreover approximation learned by lstd are strongly influenced by the visitation distribution over state our new algorithm least square policy iteration lspi address these issue the result is an off policy method which can use or reuse data collected from any source we have tested lspi on several problem including a bicycle simulator in which it learns to guide the bicycle to a goal efficiently by merely observing a relatively small number of completely random trial 
belief propagation bp wa only supposed to work for tree likenetworks but work surprisingly well in many application involvingnetworks with loop including turbo code however there hasbeen little understanding of the algorithm or the nature of thesolutions it nd for general graph we show that bp can only converge to a stationary point of anapproximate free energy known a the bethe free energy in statisticalphysics this result characterizes bp xed point and make 
motivated by our recent work on rooted tree matching in this paper we provide a solution to the problem of matching two free i e unrooted tree by constructing an association graph whose maximal clique are in one to one correspondence with maximal common subtrees we then solve the problem using simple replicator dynamic from evolutionary game theory experiment on hundred of uniformly random tree are presented the result are impressive despite the inherent inability of these simple dynamic to escape from local optimum they always returned a globally optimal solution 
ever since the beginning of the web finding useful information from the web ha been an important problem existing approach include keyword based search wrapper based information extraction web query and user preference these approach essentially find information that match the user s explicit specification this paper argues that this is insufficient there is another type of information that is also of great interest i e unexpected information which is unanticipated by the user finding unexpected information is useful in many application for example it is useful for a company to find unexpected information bout it competitor e g unexpected service and product that it competitor offer with this information the company can learn from it competitor and or design counter measure to improve it competitiveness since the number of page of a typical commercial site is very large and there are also many relevant site competitor it is very difficult for a human user to view each page to discover the unexpected information automated assistance is needed in this paper we propose a number of method to help the user find various type of unexpected information from his her competitor web site experiment result show that these technique are very useful in practice and also efficient 
we present a new approach to partial parsingof natural language text that relies onmachine learning method the approachcombines corpus based grammar inductionwith a very simple pattern matching algorithmand an optional constituent verificationstep the grammar induction algorithm acquiresa set of rule for each level of linguisticanalysis using a new technique for errordrivenpruning of treebank grammar theconstituent verification step employ standardinductive 
statistical learning and probabilistic inference technique are used to infer the hand position of a subject from multi electrode recording of neural activity in motor cortex first an array of electrode provides training data of neural firing conditioned on hand kinematics we learn a nonparametric representation of this firing activity using a bayesian model and rigorously compare it with previous model using cross validation second we infer a posterior probability distribution over hand motion conditioned on a sequence of neural test data using bayesian inference the learned firing model of multiple cell are used to define a nongaussian likelihood term which is combined with a prior probability for the kinematics a particle filtering method is used to represent update and propagate the posterior distribution over time the approach is compared with traditional linear filtering method the result suggest that it may be appropriate for neural prosthetic application 
the smart vision chip ha a large potential for application in general purpose high speed image processing system in order to fabricate smart vision chip including photo detector compactly we have proposed the application of three dimensional lsi technology for smart vision chip three dimensional technology ha great potential to realize new neuromorphic system inspired by not only the biological function but also the biological structure in this paper we describe our three dimensional lsi technology for neuromorphic circuit and the design of smart vision chip 
this paper present an electronic system that extract the periodicity of a sound it us three analogue vlsi building block a silicon cochlea two inner hair cell circuit and two spiking neuron chip the silicon cochlea consists of a cascade of filter because of the delay between two output from the silicon cochlea spike train created at these output are synchronous only for a narrow range of periodicity in contrast to traditional band pas filter where an increase in selectivity ha to be traded off against decrease in response time the proposed system responds quickly independent of selectivity 
we address the problem of non convergence of online reinforcement learning algorithm e g q learning and sarsa by adopting an incremental batch approach that separate the exploration process from the function fitting process our bfbp batch fit to best path algorithm alternate between an exploration phase during which trajectory are generated to try to find fragment of the optimal policy and a function fitting phase during which a function approximator is fit to the best known 
to find out how the representation of structured visual object depend on the co occurrence statistic of their part we exposed subject to a set of composite image with controlled conditional probability of the constituent fragment we then compared the part verification response time for various probe target combination before and after the exposure with composite probe the drop in verification rt following exposure wa much larger for target that contained pair of fragment perfectly predictive of each other compared to those that did not for lonefragment probe this difference wa reversed this pattern of result is consistent with the principle according to which object should be treated a whole unless their part are observed sufficiently frequently in more than one configuration 
variational approximation are becoming a widespread tool forbayesian learning of graphical model we provide some theoreticalresults for the variational update in a very general family ofconjugate exponential graphical model we show how the beliefpropagation and the junction tree algorithm can be used in theinference step of variational bayesian learning applying these resultsto the bayesian analysis of linear gaussian state space modelswe obtain a learning procedure that 
accurate well calibrated estimate of class membership probability are needed in many supervised learning application in particular when a cost sensitive decision must be made about example with example dependent cost this paper present simple but successful method for obtaining calibrated probability estimate from decision tree and naive bayesian classifier using the large and challenging kdd contest dataset a a testbed we report the result of a detailed experimental comparison of ten method according to four evaluation measure we conclude that binning succeeds in significantly improving naive bayesian probability estimate while for improving decision tree probability estimate we recommend smoothing by estimation and a new variant of pruning that we call curtailment 
this paper address the issue of multipleinstanceinduction of rule in the presence ofnoise it first proposes a multiple instanceextensions of rule based learning algorithm then it show what kind of noise can appear inmultiple instance data and how to handle ittheoretically finally it describes theimplementation of such a noise tolerant multipleinstance learner and show it performance onseveral problem including the well knownmutagenesis prediction task 
eligibility trace have been shown to speed reinforcement learning to make it more robust to hidden state and to provide a link between monte carlo and temporal difference method here we generalize eligibility trace to off policy learning in which one learns about a policy different from the policy that generates the data off policy method can greatly multiply learning a many policy can be learned about from the same data stream and have been identified a particularly useful for learning about subgoals and temporally extended macro action in this paper we consider the off policy version of the policy evaluation problem for which only one eligibility trace algorithm is known a monte carlo method we analyze and compare this and four new eligibility trace algorithm emphasizing their relationship to the classical statistical technique known a importance sampling o ur main result are to establish the consistency and bias property of the new method and to empirically rank the new method showing improvement over one step and monte carlo method our result are restricted to model free table lookup method and to offline updating at the end of each episode although several of the algorithm could be applied more generally 
this paper ha no novel learning or statistic it is concerned with making a wide class of preexisting statistic and learning algorithm computationally tractable when faced with data set with massive number of record or attribute it briefly review the static ad tree structure of moore and lee and offer a new structure with more attractive property the new structure scale better with the number of attribute in the data set it ha zero initial build time it adaptively cache only statistic relevant to the current task and it can be used incrementally in case where new data is frequently being appended to the data set we provide a careful explanation of the data structure and then empirically evaluate the performance under varying access pattern induced by different learning algorithm such a association rule decision tree and bayes net structure we conclude by discussing the longer term benefit of the new structure the eventual ability to apply ad tree to data set with real valued attribute 
stochastic game are a generalization of mdps to multiple agent and can be used a a framework for investigating multiagent learning hu and wellman recently proposed a multiagent q learning method for general sum stochastic game in addition to describing the algorithm they provide a proof that the method will converge to a nash equilibrium for the game under specified condition the convergence depends on a lemma stating that the iteration used by this method is a contraction mapping unfortunately the proof is incomplete in this paper we present a counterexample and flaw to the lemma s proof we also introduce strengthened assumption under which the lemma hold and examine how this affect the class of game to which the theoretical result can be applied 
we present feature transformation useful for exploratory data analysis or for pattern recognition transformation are learned from example data set by maximizing the mutual information between transformed data and their class label we make use of renyi s quadratic entropy and we extend the work of principe et al to mutual information between continuous multidimensional variable and discrete valued class label 
inferior temporal cortex it neuron have large receptive field when a single effective object stimulus is shown against a blank background but have much smaller receptive field when the object is placed in a natural scene thus translation invariant object recognition is reduced in natural scene and this may help object selection we describe a model which account for this by competition within an attractor in which the neuron are tuned to different object in the scene and the fovea ha a higher cortical magnification factor than the peripheral visual field furthermore we show that top down object bias can increase the receptive field size facilitating object search in complex visual scene and providing a model of object based attention the model lead to the prediction that introduction of a second object into a scene with blank background will reduce the receptive field size to value that depend on the closeness of the second object to the target stimulus we suggest that mechanism of this type enable the output of it to be primarily about one object so that the area that receive from it can select the object a a potential target for action 
we present a framework for learning feature for visual discrimination the learning system is exposed to a sequence of training image whenever it fails to recognize a visual context adequately new feature are sought that discriminate further between the true and false class feature consist of hierarchical combination of primitive feature local edge and texture characteristic that are sampled from example image the system continues to learn better feature even after all recognition error have been eliminated similarly to mechanism underlying human visual expertise whenever the probabilistic recognition algorithm return any posterior class probability greater than zero and le than one the system attempt to find new feature that improve discrimination between the class in question our experiment indicate that this procedure tends to improve classification accuracy on independent test image while reducing the number of feature used for recognition 
the problem of similarity search query by content ha attracted much research interest it is a difficult problem because of the inherently high dimensionality of the data the most promising solution involve performing dimensionality reduction on the data then indexing the reduced data with a multidimensional index structure many dimensionality reduction technique have been proposed including singular value decomposition svd the discrete fourier transform dft the discrete wavelet transform dwt and piecewise polynomial approximation in this work we introduce a novel framework for using ensemble of two or more representation for more efficient indexing the basic idea is that instead of committing to a single representation for an entire dataset different representation are chosen for indexing different part of the database the representation are chosen based upon a local view of the database for example section of the data that can achieve a high fidelity representation with wavelet are indexed a wavelet but highly spectral section of the data are indexed using the fourier transform at query time it is necessary to search several small heterogeneous index rather than one large homogeneous index a we will theoretically and empirically demonstrate this result in much faster query response time 
we show that the recently proposed variant of the support vectormachine svm algorithm known a svm can be interpretedas a maximal separation between subset of the convex hull of thedata which we call soft convex hull the soft convex hull arecontrolled by choice of the parameter if the intersection of theconvex hull is empty the hyperplane is positioned halfway betweenthem such that the distance between convex hull measured alongthe normal is maximized and if it is not 
a major problem for kernel based predictor such a support vector machine and gaussian process is that the amount of computation required to find the solution scale a o n where n is the number of training example we show that an approximation to the eigendecomposition of the gram matrix can be computed by the nystr m method which is used for the numerical solution of eigenproblems this is achieved by carrying out an eigendecomposition on a smaller system of size m lt n and then 
neural activity appears to be a crucial component for shaping the receptive field of cortical simple cell into adjacent oriented subregions alternately receiving onand off center excitatory geniculate input it is known that the orientation selective response of v neuron are refined by visual experience after eye opening the spatiotemporal structure of neural activity in the early stage of the visual pathway depends both on the visual environment and on how the environment is scanned we have used computational modeling to investigate how eye movement might affect the refinement of the orientation tuning of simple cell in the presence of a hebbian scheme of synaptic plasticity level of correlation between the activity of simulated cell were examined while natural scene were scanned so a to model sequence of saccade and fixational eye movement such a microsaccades tremor and ocular drift the specific pattern of activity required for a quantitatively accurate development of simple cell receptive field with segregated on and off subregions were observed during fixational eye movement but not in the presence of saccade or with static presentation of natural visual input these result suggest an important role for the eye movement occurring during visual fixation in the refinement of orientation selectivity 
we propose the following general method for scaling learning algorithm to arbitrarily large data set consider the model m n learned by the algorithm using n i example in step i n n nm and the model m that would be learned using infinite example upper bound the loss l m n m between them a a function of n and then minimize the algorithm s time complexity f n subject to the constraint that l m m n be at most with probability at most we apply this 
abstract cached statistic are a mean of extendingthe reach of traditional statistical machinelearning algorithm into application areaswhere computational complexity is a limitingfactor recent work ha shown that cachedstatistics greatly reduce the computationalrequirements of building a mixture model of adistribution using expectation maximizationfor a small trade off in model error this paperdescribes a method whereby a mixturemodel built using cached statistic is usedas a 
the way group of auditory neuron interact to code acoustic information is investigated using an information theoretic approach identifying the case of stimulus conditioned independent neuron we develop redundancy measure that allow enhanced information estimation for group of neuron these measure are then applied to study the collaborative coding eciency in two processing station in the auditory pathway the inferior colliculus ic and the primary auditory cortex a under two dierent coding paradigm we show dierences in both information content and group redundancy between ic and cortical auditory neuron these result provide for the rst time a direct evidence for redundancy reduction along the ascending auditory pathway a ha been hypothesized by barlow the redundancy eects under the single spike coding paradigm are signicant only for group larger than ten cell and cannot be revealed with the standard redundancy measure that use only pair of cell our result suggest that redundancy reduction transformation are not limited to low level sensory processing aimed to reduce redundancy in input statistic but are applied even at cortical sensory station 
in this paper we present a new method of estimating the novelty of rule discovered by data mining method using wordnet a lexical knowledge base of english word we ass the novelty of a rule by the average semantic distance in a knowledge hierarchy between the word in the antecedent and the consequent of the rule the more the average distance more is the novelty of the rule the novelty of rule extracted by the discotex text mining system on amazon com book description were evaluated by both human subject and by our algorithm by computing correlation coefficient between pair of human rating and between human and automatic rating we found that the automatic scoring of rule based on our novelty measure correlate with human judgment about a well a human judgment correlate with one another text mining 
we model hippocampal place cell and head direction cell by combining allothetic visual and idiothetic proprioceptive stimulus visual input provided by a video camera on a miniature robot is preprocessed by a set of gabor filter on node of a log polar retinotopic graph unsupervised hebbian learning is employed to incrementally build a population of localized overlapping place field place cell serve a basis function for reinforcement learning experimental result for goal oriented navigation of a mobile robot are presented 
suppose you are given some dataset drawn from an underlying probability distribution and you want to estimate a subset of input space such that the probability that a test point drawn from lie outside of is bounded by some a priori specified we propose an algorithm which approach this problem by trying to estimate a function which is positive on and negative on the complement the functional form of is given by a kernel expansion in term of a potentially small subset of the training data it is regu larized by controlling the length of the weight vector in an associated fea ture space the algorithm is a natural extension of the support vector al gorithm to the case of unlabelled data 
principal component analysis and fisher linear discriminant method have demonstrated their success in face detection recognition and tracking the representation in these subspace method is based on second order statistic of the image set and doe not address higher order statistical dependency such a the relationship among three or more pixel recently higher order statistic and independent component analysis ica have been used a informative low dimensional representation for 
a new form of covariance modelling for gaussian mixture model and hidden markov model is presented this is an extension to an efficient form of covariance modelling used in speech recognition semi tied covariance matrix in the standard form of semi tied covari ance matrix the covariance matrix is decomposed into a highly shared decorrelating transform and a component specific diagonal covariance mat rix the use of a factored decorrelating transform is presented in this p aper this factoring effectively increase the number of possible transf orms without increasing the number of free parameter maximum likelihood estimation scheme for all the model parameter are presented includin g the component transform assignment transform and component parameter this new model form is evaluated on a large vocabulary speech recognition task it is shown that using this factored form of covariance modelling reduces the word error rate 
transaction data can arrive at a ferocious rate in the order that transaction are completed the data contain an enormous amount of information about customer not just transaction but extracting up to date customer information from an ever changing stream of data and mining it in real time is a challenge this paper describes a statistically principled approach to designing short accurate summary or signature of high dimensional customer behavior that can be kept current with a stream of transaction a signature database can then be used for data mining and to provide approximate answer to many kind of query about current customer quickly and accurately a an empirical study of the calling pattern of wireless customer who made about million wireless call over a three month period show 
we introduce a new non parametric and principled distance basedclustering method this method combine a pairwise based approachwith a vector quantization method which provide a meaningfulinterpretation to the resulting cluster the idea is basedon turning the distance matrix into a markov process and thenexamine the decay of mutual information during the relaxation ofthis process the cluster emerge a quasi stable structure duringthis relaxation and then are extracted 
we focus on the problem of finding pattern across two large multidimensional datasets for example given feature vector of healthy and of non healthy patient we want to answer the following question are the two cloud of point separable what is the smallest largest pair wise distance across the two datasets which of the two cloud doe a new point feature vector come from we propose a new tool the tri plot and it generalization the pq plot which help u answer the above question we provide a set of rule on how to interpret a tri plot and we apply these rule on synthetic and real datasets we also show how to use our tool for classification when traditional method nearest neighbor classification tree may fail 
in this paper it is shown how to extract a hypothesis with small risk from the ensemble of hypothesis generated by an arbitrary on line learning algorithm run on an independent and identically distributed i i d sample of data using a simple large deviation argument we prove tight data dependent bound for the risk of this hypothesis in term of an easily computable statistic mn associated with the on line performance of the ensemble via sharp pointwise bound on mn we then obtain risk tail bound for kernel perceptron algorithm in term of the spectrum of the empirical kernel matrix these bound reveal that the linear hypothesis found via our approach achieve optimal tradeoff between hinge loss and margin size over the class of all linear function an issue that wa left open by previous result a distinctive feature of our approach is that the key tool for our analysis come from the model of prediction of individual sequence i e a model making no probabilistic assumption on the source generating the data in fact these tool turn out to be so powerful that we only need very elementary statistical fact to obtain our final risk bound 
we generalize the classical algorithm of valiant and haussler for learning conjunction and disjunction of boolean attribute to the problem of learning these function over arbitrary set of feature including feature that are constructed from the data the result is a general purpose learning machine suitable for practical learning task that we call the set covering machine we present a version of the set covering machine that us generalized ball for it set of data dependent feature and compare it performance to the famous support vector machine by extending a technique pioneered by littlestone and warmuth we bound it generalization error a function of the amount of data compression it achieves during training 
we consider the problem of reliably choosing a near best strategy from a restricted class of strategy in a partially observable markov decision process pomdp we assume we are given the ability to simulate the pomdp and study what might be called the sample complexity that is the amount of data one must generate in the pomdp in order to choose a good strategy we prove upper bound on the sample complexity showing that even for infinitely large and arbitrarily complex pomdps the amount of data needed can be finite and depends only linearly on the complexity of the restricted strategy class and exponentially on the horizon time this latter dependence can be eased in a variety of way including the application of gradient and local search algorithm our measure of complexity generalizes the classical supervised learning notion of vc dimension to the setting of reinforcement learning and planning 
survey are an important part of marketing and customer relationship management and open answer i e answer to open question in particular may contain valuable information and provide an important basis for making business decision we have developed a text mining system that provides a new way for analyzing open answer in questionnaire data the product is able to perform the following two function a accurate extraction of characteristic for individual analysis target b accurate extraction of the relationship among characteristic of analysis target in this paper we describe the working of our text mining system it employ two statistical learning technique rule analysis and correspondence analysis for performing the two function our text mining system ha already been put into use by a number of large corporation in japan in the performance of text mining on various type of survey data including open answer about brand image open answer about company image complaint about product comment written on home page business report and help desk record in this it ha been found to be useful in forming a basis for effective business decision 
there ha been considerable interest recently in various approach to scaling up machine learning system to large and distributed data set we have been studying approach based upon the parallel application of multiple learning program at distributed site followed by a meta learning stage to combine the multiple model in a principled fashion in this paper we empirically determine the best data partitioning scheme for a selected data set to compose appropriatelysized subset and we evaluate and compare three different strategy voting stacking and stacking with correspondence analysis scann for combining classification model trained over these subset we seek to find way to efficiently scale up to large data set while maintaining or improving predictive performance measured by the error rate a cost model and the tp fp spread 
algebraic geometry is essential to learning theory in hierarchical learning machine such a layered neural network and gaussian mixture the asymptotic normality doe not hold since fisher information matrix are singular in this paper the rigorous asymptotic form of the stochastic complexity is clarified based on resolution of singularity and two different problem are studied if the prior is positive then the stochastic complexity is far smaller than bic resulting in the smaller generalization error than regular statistical model even when the true distribution is not contained in the parametric model if jeffreys prior which is coordinate free and equal to zero at singularity is employed then the stochastic complexity ha the same form a bic it is useful for model selection but not for generalization 
hypertext pose new text classification research challenge a hyperlink content of linked document and meta data about related web site all provide richer source of information for hypertext classification that are not available in traditional text classification we investigate the use of such information for representing web site and the effectiveness of different classifier naive bayes nearest neighbor and sc foil in exploiting those representation we find that using word in web page alone often yield suboptimal performance of classifier compared to exploiting additional source of information beyond document content on the other hand we also observe that linked page can be more harmful than helpful when the linked neighborhood are highly noisy and that link have to be used in a careful manner more importantly our investigation suggests that meta data which is often available or can be acquired using information extraction technique can be extremely useful for improving classification accuracy finally the relative performance of the different classifier being tested give u insight into the strength and limitation of our algorithm for hypertext classification 
most machine learning algorithm share thefollowing drawback they only output barepredictions but not the confidence in thosepredictions in the s algorithmic informationtheory supplied universal measuresof confidence but these are unfortunately non computable in this paper we combinethe idea of algorithmic information theorywith the theory of support vector machinesto obtain practicable approximation to universalmeasures of confidence we show thatin some 
lyapunov analysis is a standard approach to studying the stability of dynamical system and to designing controller we propose to design the action of a reinforcement learning rl agent to be descending on a lyapunov function for minimum cost to target problem this ha the theoretical benefit of guaranteeing that the agent will reach a goal state on every trial regardless of the rl algorithm it us in practice lyapunov descent constraint can significantly shorten learning trial improve initial and worst case performance and accelerate learning although this method of constraining action may limit the extent to which an rl agent can minimize cost it allows one to construct robust rl system for problem in which lyapunov domain knowledge is available this includes many important individual problem a well a general class of problem such a the control of feedback linearizable system e g industrial robot and continuous state path planning problem we demonstrate the general approach on two simulated control problem pendulum swing up and robot arm control 
child learn to ride a bicycle by using training wheel they are actually trying to learn one task riding without training wheel by training another one in general solving a difficult problem can be facilitated by training other problem this is the basic idea of shaping it is essential to ensure that spending time on the modified task will help solving the original one in this paper we prove that given a finite mdp with a limited reward signal and we are guaranteed that if a series of task converge to the original one then the optimal value function converges to the original one a well 
we consider the existence of efficient algorithm for learning theclass of half space in nin the agnostic learning model i e makingno prior assumption on the example generating distribution the resulting combinatorial problem finding the best agreementhalf space over an input sample is np hard to approximate towithin some constant factor we suggest a way to circumvent thistheoretical bound by introducing a new measure of success for suchalgorithms an algorithm is 
most research in text classification to date ha used a bag of word representation in which each feature corresponds to a single word the paper examines some alternative way to represent text based on syntactic and semantic relationship between word phrase synonym and hypernym we describe the new representation and try to justify our hypothesis that they could improve the performance of a rule based learner the representation are evaluated using the ripper learning algorithm on the reuters and digitrad test corpus on their own the new representation are not found to produce significant performance improvement we also try combining classifier based on different representation using a majority voting technique and this improves performance on both test collection in our opinion more sophisticated natural language processing technique need to be developed before better text representation can be produced for classification 
a system ha been developed to extract diagnostic information from jet engine carcass vibration data support vector machine applied to novelty detection provide a measure of how unusual the shape of a vibration signature is by learning a representation of normality we descri be a novel method for support vector machine of including information from a second class for novelty detection and give result from the application to jet engine vibration analysis 
this study investigates a population decoding paradigm in which the estimation of stimulus in the previous step is used a prior knowledge for consecutive decoding we analyze the decoding accuracy of such a bayesian decoder maximum a posteriori estimate and show that it can be implemented by a biologically plausible recurrent network where the prior knowledge of stimulus is conveyed by the change in recurrent interaction a a result of hebbian learning 
revi miner is a kdd environment which support the detection and analysis of deviation in warranty and goodwill cost statement the system wa developed within the framework of a cooperation between daimlerchrysler research technology and global service and part gsp and is based upon the crisp dm methodology a a widely accepted process model for the solution of data mining problem also we have implemented different approach based on machine learning and statistic which can be utilized for data cleaning in the preprocessing phase the data mining model applied have been developed by using a statistical deviation detection approach the tool support controller in their task of auditing the authorized repair shop in this paper we describe the development phase which have led to revi miner 
this paper provides a characterization of biasfor evaluation metric in classification e g information gain gini etc our characterizationprovides a uniform representationfor all traditional evaluation metric such representation lead naturally to a measurefor the distance between the bias oftwo evaluation metric we give a practicalvalue to our measure by observing ifthe distance between the bias of two evaluationmetrics correlate with difference in 
neuron in area v have relatively large receptive field rf so multiple visual feature are simultaneously seen by these cell recording from single v neuron suggest that simultaneously presented stimulus compete to set the output firing rate and that attention act to isolate individual feature by biasing the competition in favor of the attended object we propose that both stimulus competition and attentional biasing arise from the spatial segregation of afferent synapsis onto different region of the excitable dendritic tree of v neuron the pattern of feedforward stimulus driven input follows from a hebbian rule excitatory afferent with similar rf tend to group together on the dendritic tree avoiding randomly located inhibitory input with similar rf the same principle guide the formation of input that mediate attentional modulation using both biophysically detailed compartmental model and simplified model of computation in single neuron we demonstrate that such an architecture could account for the response property and attentional modulation of v neuron our result suggest an important role for nonlinear dendritic conductance in extrastriate cortical processing 
this paper explores a framework for recognition of image sequence using partially observable stochastic differential equation sde model monte carlo importance sampling technique are used for efficient estimation of sequence likelihood and sequence likelihood gradient once the network dynamic are learned we apply the sde model to sequence recognition task in a manner similar to the way hidden markov model hmms are commonly applied 
human computer intelligent interaction hcii is an emerging field of science aimed at providing natural way for human to use computer a aid it is argued that for the computer to be able to interact with human it need to have the communication skill of human one of these skill is the ability to understand the emotional state of the person the most expressive way human display emotion is through facial expression this work focus on automatic facial expression recognition from live video input using temporal cue method for using temporal information have been extensively explored for speech recognition application among these method are template matching using dynamic programming method and hidden markov model hmm this work exploit existing method and proposes a new architecture of hmms for automatically segmenting and recognizing human facial expression from video sequence the novelty of this architecture is that both segmentation and recognition of the facial expression are done automatically using a multilevel hmm architecture while increasing the discrimination power between the different class in the work we explore person dependent and person independentrecognition of expression 
function approximation is essential to reinforcement learning butthe standard approach of approximating a value function and determininga policy from it ha so far proven theoretically intractable in this paper we explore an alternative approach in which the policyis explicitly represented by it own function approximator independentof the value function and is updated according to the gradientof expected reward with respect to the policy parameter williams sreinforce method and 
we present linear relational embedding lre a new method of learning a distributed representation of concept from data consisting of instance of relation between given concept it final goal is to be able to generalize i e infer new instance of these relation among the concept on a task involving family relationship we show that lre can generalize better than any previously published method we then show how lre can be used effectively to find compact distributed representation for variable sized recursive data structure such a tree and list 
this paper is concerned with the problem of detecting outlier from unlabeled data in prior work we have developed smartsifter which is an on line outlier detection algorithm based on unsupervised learning from data on the basis of smartsifter this paper yield a new framework for outlier filtering using both supervised and unsupervised learning technique iteratively in order to make the detection process more effective and more understandable the outline of the framework is a follows in the first round for an initial dataset we run smartsifter to give each data a score with a high score indicating a high possibility of being an outlier next giving positive label to a number of higher scored data and negative label to a number of lower scored data we create labeled example then we construct an outlier filtering rule by supervised learning from them here the rule is generated based on the principle of minimizing extended stochastic complexity in the second round for a new dataset we filter the data using the constructed rule then among the filtered data we run smartsifter again to evaluate the data in order to update the filtering rule applying of our framework to the network intrusion detection we demonstrate that it can significantly improve the accuracy of smartsifter and outlier filtering rule can help the user to discover a general pattern of an outlier group 
this paper present an active learning method that directly optimizes expected future error this is in contrast to many other popular technique that instead aim to reduce version space size these method are popular because for many learning model closed form calculation of the expected future error is intractable our approach is made feasible by taking a monte carlo approach to estimating the expected reduction in error due to the labeling of a query in experimental result on three real world data set we reach high accuracy with four time fewer labelled example than competing method 
the problem of developing good policy for partially observable markov decision problem pomdps remains one of the most challenging area of research in stochastic planning one line of research in this area involves the use of reinforcement learning with belief state probability distribution over the underlying model state this is a promising method for small problem but it application is limited by the intractability of computing or representing a full belief state for large problem recent work show that in many setting we can maintain an approximate belief state which is fairly close to the true belief state in particular great success ha been shown with approximate belief state that marginalize out correlation between state variable in this paper we investigate two method of full belief state reinforcement learning and one novel method for reinforcement learning using factored approximate belief state we compare the performance of these algorithm on several well known problem from the literature our result demonstrate the importance of approximate belief state representation for large problem 
we show how a wavelet basis may be adapted to best representnatural image in term of sparse coefficient the wavelet basis which may be either complete or overcomplete is specified by asmall number of spatial function which are repeated across spaceand combined in a recursive fashion so a to be self similar acrossscale these function are adapted to minimize the estimated codelength under a model that assumes image are composed of a linearsuperposition of sparse 
for many learning task where data is collected over an extended period of time it underlying distribution is likely to change a typical example is information filtering i e the adaptive classification of document with respect to a particular user interest both the interest of the user and the document content change over time a filtering system should be able to adapt to such concept change this paper proposes a new method to recognize and handle concept change with support vector machine the method maintains a window on the training data the key idea is to automatically adjust the window size so that the estimated generalization error is minimized the new approach is both theoretically well founded a well a effective and efficient in practice since it doe not require complicated parameterization it is simpler to use and more robust than comparable heuristic experiment with simulated concept drift scenario based on real world text data compare the new method with other window management approach we show that it can effectively select an appropriate window size in a robust way 
decision tree have been successfully used for the task of classification however state of the art algorithm do not incorporate the user in the tree construction process this paper present a new user centered approach to this process where the user and the computer can both contribute their strength the user provides domain knowledge and evaluates intermediate result of the algorithm the computer automatically creates pattern satisfying user constraint and generates appropriate visualization of these pattern in this cooperative approach domain knowledge of the user can direct the search of the algorithm additionally by providing adequate data and knowledge visualization the pattern recognition capability of the human can be used to increase the effectivity of decision tree construction furthermore the user get a deeper understanding of the decision tree than just obtaining it a a result of an algorithm to achieve the intended level of cooperation we introduce a new visualization of data with categorical and numerical attribute a novel technique for visualizing decision tree is presented which provides deep insight into the process of decision tree construction a a key contribution we integrate a state of the art algorithm for decision tree construction such that many different style of cooperation ranging from completely manual over combined to completely automatic classification are supported an experimental performance evaluation demonstrates that our cooperative approach yield an efficient construction of decision tree that have a small size but a high accuracy 
we present technique for rendering and animation of realistic scene by analyzing and training on short video sequence this work extends the new paradigm for computer animation video texture which us recorded video to generate novel animation by replaying the video sample in a new order here we concentrate on video sprite which are a special type of video texture in video sprite instead of storing whole image the object of interest is separated from the background and the video sample are stored a a sequence of alpha matted sprite with associated velocity information they can be rendered anywhere on the screen to create a novel animation of the object we present method to create such animation by finding a sequence of sprite sample that is both visually smooth and follows a desired path to estimate visual smoothness we train a linear classifier to estimate visual similarity between video sample if the motion path is known in advance we use beam search to find a good sample sequence we can specify the motion interactively by precomputing the sequence cost function using q learning 
for many problem which would be natural for reinforcement learning the reward signal is not a single scalar value but ha multiple scalar component example of such problem include agent with multiple goal and agent with multiple user creating a single reward value by combining the multiple component can throw away vital information and can lead to incorrect solution we describe the multiple reward source problem and discus the problem with applying traditional reinforcement learning we then present an new algorithm for finding a solution and result on simulated environment 
wideband source recorded using closely spaced receiver can feasibly be separated based only on second order statistic when using a physical model of the mixing process in this case we show that the parameter estimation problem can be essentially reduced to considering direction of arrival and attenuation of each signal the paper present two demixing method operating in the time and frequency domain and experimentally show that it is always possible to demix signal arriving at different angle moreover one can use spatial cue to solve the channel selection problem and a post processing wiener filter to ameliorate the artifact caused by demixing 
virtual reality vr provides immersive and controllable experimentalenvironments it expands the bound of possible evoked potential ep experiment by providing complex dynamic environment in orderto study cognition without sacrificing environmental control vralso serf a a safe dynamic testbed for brain computer interface bci research however there ha been some concern about detecting ep signalsin a complex vr environment this paper show that eps exist atred green 
in this paper we focus on mining surprising periodic pattern in a sequence of event in many application e g computational biology an infrequent pattern is still considered very significant if it actual occurrence frequency exceeds the prior expectation by a large margin the traditional metric such a support is not necessarily the ideal model to measure this kind of surprising pattern because it treat all pattern equally in the sense that every occurrence carry the same weight towards the assessment of the significance of a pattern regardless of the probability of occurrence a more suitable measurement information is introduced to naturally value the degree of surprise of each occurrence of a pattern a a continuous and monotonically decreasing function of it probability of occurrence this would allow pattern with vastly different occurrence probability to be handled seamlessly a the accumulated degree of surprise of all repetition of a pattern the concept of information gain is proposed to measure the overall degree of surprise of the pattern within a data sequence the bounded information gain property is identified to tackle the predicament caused by the violation of the downward closure property by the information gain measure and in turn provides an efficient solution to this problem empirical test demonstrate the efficiency and the usefulness of the proposed model 
building predictive model and finding useful rule are two important task of data mining while building predictive model ha been well studied finding useful rule for action still present a major problem a main obstacle is that many data mining algorithm often produce too many rule existing research ha shown that most of the discovered rule are actually redundant or insignificant pruning technique have been developed to remove those spurious and or insignificant rule in this paper we argue that being a significant rule or a non redundant rule however doe not mean that it is a potentially useful rule for action many significant rule unpruned rule are in fact not actionable this paper study this issue and present an efficient algorithm to identify these non actionable rule experiment result on many real life datasets show that the number of non actionable rule is typically quite large the proposed technique thus enables the user to focus on fewer rule and to be assured that the remaining rule are non redundant and potentially useful for action 
this paper proposes and analyzes an efficient and effective approach for estimating the generalization performance of a support vector machine svm for text classification without any computation intensive resampling the new estimator are computationally much more efficient than cross validation or bootstrapping they can be computed at essentially no extra cost immediately after training a single svm moreover the estimator developed here address the special performance measure needed for evaluating text classifier they can be used not only to estimate the error rate but also to estimate recall precision and f a theoretical analysis and experiment show that the new method can effectively estimate the performance of svm text classifier in an efficient way 
this paper introduces the open loop intermittent feedback optimal predictive olifo controller a an alternative to human movement control modelsbased on system inverse control olifo ha the advantage of being applicableto any system not requiring a desired system trajectory and handling naturallysystems with time delay and constraint moreover it share important functionalcharacteristics with the human movement control system it behaviour isillustrated through the control of a 
the paper present a series of noise detectionexperiments in a medical problem of coronaryartery disease diagnosis the following algorithmsfor noise detection and elimination aretested a saturation filter a classification filter a combined classification saturation filter and a consensus saturation filter thedistinguishing feature of the novel consensussaturation filter is it high reliability which isdue to the multiple detection of potentiallynoisy example reliable 
direct policy search is a practical way to solve reinforcement learning problem involving continuous state and action space the goal becomes finding policy parameter that maximize a noisy objective function the pegasus method convert this stochastic optimization problem into a deterministic one by using fixed start state and fixed random number sequence for comparing policy ng jordan we evaluate pegasus and other paired comparison method using the mountain car problem and a difficult pursuer evader problem we conclude that i paired test can improve performance of deterministic and stochastic optimization procedure ii our proposed alternative to pegasus can generalize better by using a different test statistic or changing the scenario during learning iii adapting the number of trial used for each policy comparison yield fast and robust learning 
a key challenge for reinforcement learning is scaling up to largepartially observable domain in this paper we show how a hierarchyof behavior can be used to create and select among variablelength short term memory appropriate for a task at higher levelsin the hierarchy the agent abstract over lower level detailsand look back over a variable number of high level decision intime we formalize this idea in a framework called hierarchicalsux memory hsm hsm us a 
we present a novel information theoretic algorithm for unsupervised segmentation of sequence into alternating variable memory markov source the algorithm is based on competitive learning between markov model when implemented a prediction suffix tree ron et al using the mdl principle by applying a model clustering procedure based on rate distortion theory combined with deterministic annealing we obtain a hierarchical segmentation of sequence between alternating markov 
we examine the statistic of natural monochromatic image decomposed using a multi scale wavelet basis although the coefficient of this rep resentation are nearly decorrelated they exhibit important higher order statistical dependency that cannot be eliminated with purely linear processing in particular rectified coefficient corresponding to basis function at neighboring spatial position orientation and scale are hig hly correlated a method of removing these dependency is to divide each coefficient by a weighted combination of it rectified neighbor several successful model of the steady state behavior of neuron in primary visual cortex are based on such divisive normalization computation and thus our analysis provides a theoretical justification for these model s perhaps more importantly the statistical measurement explicitly specif y the weight that should be used in computing the normalization signal we demonstrate that this weighting is qualitatively consistent with r ecent physiological experiment that characterize the suppressive effect of stimulus presented outside of the classical receptive field our observation thus provide evidence for the hypothesis that early visual neu ral processing is well matched to these statistical property of image an appealing hypothesis for neural processing state that sensory system develop in response to the statistical property of the signal to which they are e xposed e g this ha led many researcher to look for a mean of deriving a model of cortical processing purely from a statistical characterization of sensory signal in parti cular many such attempt are based on the notion that neural response should be statisti cally independent the pixel of digitized natural image are highly redundant but one can always find a linear decomposition i e principal component analysis that eliminates second order corresearch supported by an alfred p sloan fellowship to eps and by the sloan center for theoretical neurobiology at nyu 
recently a number of author have proposed treating dialogue system a markov decision process mdps however the practical application of mdp algorithm to dialogue system face a number of severe technical challenge we have built a general software tool rlds for reinforcement learning for dialogue system based on the mdp framework and have applied it to dialogue corpus gathered from two dialogue system built at at t lab our experiment demonstrate that rlds hold promise a a tool for browsing and understanding correlation in complex temporally dependent dialogue corpus 
it ha been known that people after being exposed to sentencesgenerated by an articial grammar acquire implicit grammaticalknowledge and are able to transfer the knowledge to input that aregenerated by a modied grammar we show that a second orderrecurrent neural network is able to transfer grammatical knowledgefrom one language generated by a finite state machine to anotherlanguage which dier both in vocabulary and syntax representationof the grammatical knowledge in 
much recent attention both experimental and theoretical ha been focussed on classificationalgorithms which produce voted combination of classifier recent theoreticalwork ha shown that the impressive generalization performance of algorithm like adaboostcan be attributed to the classifier having large margin on the training data we present an abstract algorithm for finding linear combination of function that minimizearbitrary cost functionals i e functionals that do not 
bayesian prediction are stochastic just like prediction of any otherinference scheme that generalize from a finite sample while a simplevariational argument show that bayes averaging is generalizationoptimal given that the prior match the teacher parameterdistribution the situation is le clear if the teacher distribution isunknown i define a class of averaging procedure the temperatedlikelihoods including both bayes averaging with a uniform priorand maximum likelihood 
in this work we explore homeostasis in a silicon integrate and fire neuron the neuron adapts it firing rate over long time period on the order of second or minute so that it return to it spontaneous firing rate after a lasting perturbation homeostasis is implemented via two scheme one scheme look at the presynaptic activity and adapts the synaptic weight depending on the presynaptic spiking rate the second scheme adapts the synaptic threshold depending on the neuron s activity the threshold is lowered if the neuron s activity decrease over a long time and is increased for prolonged increase in postsynaptic activity both these mechanism for adaptation use floating gate technology the result shown here are measured from a chip fabricated in a m cmos process 
a mixed signal paradigm is presented for high resolution parallel innerproduct computation in very high dimension suitable for efficient implementation of kernel in image processing at the core of the externally digital architecture is a high density low power analog array performing binary binary partial matrix vector multiplication full digital resolution is maintained even with low resolution analog to digital conversion owing to random statistic in the analog summation of binary product a random modulation scheme produce near bernoulli statistic even for highly correlated input the approach is validated with real image data and with experimental result from a cid dram analog array prototype in m cmos 
web caching and prefetching are well known strategy for improving the performance of internet system when combined with web log mining these strategy can decide to cache and prefetch web document with higher accuracy in this paper we present an application of web log mining to obtain web document access pattern and use these pattern to extend the well known gdsf caching policy and prefetching policy using real web log we show that this application of data mining can achieve dramatic improvement to web access performance 
learning in many visual perceptual task ha been shown to be specific to practiced stimulus while new stimulus have to be learned from scratch here we demonstrate generalization using a novel paradigm in motion discrimination where learning ha been previously shown to be specific we trained subject to discriminate direction of moving dot and verified the previous result that learning doe not transfer from a trained direction to a new one however by tracking the subject performance across time in the new direction we found that their speed of learning doubled therefore we found generalization in a task previously considered too difficult to generalize we also replicated in a second experiment transfer following training with easy stimulus when the difference between motion direction is enlarged in a third experiment we found a new mode of generalization after mastering the task with an easy stimulus subject who have practiced briefly to discriminate the easy stimulus in a new direction generalize to a difficult stimulus in that direction this generalization depends on both the mastering and the brief practice the specificity of perceptual learning and the dichotomy between learning of easy versus difficult task have been assumed to involve different learning process at different cortical area here we show how to interpret these result in term of signal detection theory with the assumption of limited computational capacity we obtain the observed phenomenon direct transfer and acceleration of learning for increasing level of task difficulty human perceptual learning and generalization therefore concur with a generic discrimination system 
hebbian and competitive hebbian algorithm are almost ubiquitous in modeling pattern formation in cortical development we analyse in theoretical detail a particular model adapted from piepenbro ck obermayer for the development of d stripe like pattern which place competitive and interactive cortical influence and free a nd restricted initial arborisation onto a common footing 
we present a learning algorithm for non parametric hidden markov model with continuous state and observation space all necessary probability density are approximated using sample along with density tree generated from such sample a monte carlo version of baum welch em is employed to learn model from data regularization during learning is achieved using an exponential shrinking technique the shrinkage factor which determines the effective capacity of the learning algorithm is annealed down over multiple iteration of baum welch and early stopping is applied to select the right model once trained monte carlo hmms can be run in an any time fashion we prove that under mild assumption monte carlo hidden markov model converge to a local maximum in likelihood space just like conventional hmms in addition we provide empirical result obtained in a gesture recognition domain 
high dimensional data ha always been a challenge for clustering algorithm because of the inherent sparsity of the point therefore technique have recently been proposed to find cluster in hidden subspace of the data however since the behavior of the data may vary considerably in different subspace it is often difficult to define the notion of a cluster with the use of simple mathematical formalization in fact the meaningfulness and definition of a cluster is best characterized with the use of human intuition in this paper we propose a system which performs high dimensional clustering by effective cooperation between the human and the computer the complex task of cluster creation is accomplished by a combination of human intuition and the computational support provided by the computer the result is a system which leverage the best ability of both the human and the computer in order to create very meaningful set of cluster in high dimensionality 
this paper considers the task of predicting the future decision of an agent a based on his past decision we assume that a is rational he us the principle of maximum expected utility we also assume that the probability distribution p he assigns to random event is known so that we need only infer his utility function u to model his decision process we consider the task of using a s previous decision to learn about u in particular a s past decision can be viewed a constraint on u 
supervised classification is one of the most active area of machine learning research most work ha focused on classification in static domain where an instantaneous snapshot of attribute is meaningful in many domain attribute are not static in fact it is the way they vary temporally that can make classification possible example of such domain include speech recognition gesture recognition and electrocardiograph classification while it is possible to use ad hoc domain specific 
it is desirable that a complex decision making problem in an uncertain world be adequately modeled by a markov decision process mdp whose structural representation is adaptively designed by a parsimonious resource allocation process resource include time and cost of exploration amount of memory and computational time allowed for the policy or value function representation concerned about making the best use of the available resource we address the problem of e ciently estimating where 
we report on the use of reinforcement learning with cobot a software agent residing in the wellknown online community lambdamoo our initial work on cobot isbell et al provided him with the ability to collect social statistic and report them to user here we describe our application of rl to allow cobot to proactively take action in this complex social environment and adapt his behavior from multiple source of human reward after month of training and reward and punishment event from different lambdamoo user cobot learned nontrivial preference for a number of user modifing his behavior based on his current state here we describe lambdamoo and the state and action space of cobot and report the statistical result of the learning experiment 
we consider the problem of learning to attain multiple goal in a dynamic environment which is initially unknown in addition the environment may contain arbitrarily varying element related to action of other agent or to non stationary movesofnature thisproblemismodelledasastochastic markov gamebetween the learning agent and an arbitrary player with a vector valued reward function the objective of the learning agent is to have it long term average reward vector belongtoagiventargetset wedeviseanalgorithmforachievingthistask which isbasedonthetheoryofapproachabilityforstochasticgames thisalgorithmcombines in an appropriate way a flnite set of standard scalar reward learning algorithm su cientconditionsaregivenfortheconvergenceofthelearningalgorithm to a general target set the specialization of these result to the single controller markov decision problem are discussedas well 
this paper address the problem of inversereinforcement learning irl in markov decisionprocesses that is the problem of extractinga reward function given observed optimal behavior irl may be useful forapprenticeship learning to acquire skilled behavior and for ascertaining the reward functionbeing optimized by a natural system werst characterize the set of all reward functionsfor which a given policy is optimal wethen derive three algorithm for irl therst two 
this paper present reinforcement learning with a long shorttermmemory recurrent neural network rl lstm model freerl lstm using advantage learning and directed explorationcan solve non markovian task with long term dependency betweenrelevantevents this is demonstrated in a t maze task aswell a in a di cult variation of the pole balancing task 
the recent introduction of the relevance vector machine ha eectivelydemonstrated how sparsity may be obtained in generalisedlinear model within a bayesian framework using a particularform of gaussian parameter prior learning is the maximisation with respect to hyperparameters of the marginal likelihood of thedata this paper study the property of that objective function and demonstrates that conditioned on an individual hyperparameter the marginal likelihood ha a 
adacost a variant of adaboost is a misclassication cost sensitive boosting method it us the cost of misclassications to update the training distribution on successive boosting round the purpose is to reduce the cumulative misclassication cost more than adaboost we formally show that adacost reduces the upper bound of cumulative misclassication cost of the training set empirical evaluation have shown signican t reduction in the cumulative misclassication cost over adaboost without consuming additional computing power 
despite the fact that mental arithmetic is based on only a few hundredbasic fact and some simple algorithm human have a difficulttime mastering the subject and even experienced individualsmake mistake associative multiplication the process of doingmultiplication by memory without the use of rule or algorithm is especially problematic human exhibit certain characteristicphenomena in performing associative multiplication both in thetype of error and in the error frequency we 
automatically labeling document cluster with word which indicate their topic is difficult to do well the most commonly used method labeling with the most frequent word in the cluster end up using many word that are virtually void of descriptive power even after traditional stop word are removed another method labeling with the most predictive word often includes rather obscure word we present two method of labeling document cluster motivated by the model that word are generated by a hierarchy of mixture component of varying generality the first method assumes existence of a document hierarchy manually constructed or resulting from a hierarchical clustering algorithm and us a test of significance to detect different word usage across category in the hierarchy the second method selects word which both occur frequently in a cluster and effectively discriminate the given cluster from the other cluster we compare these method on abstract of document selected from a subset of the hierarchy of the cora search engine for computer science research paper label produced by our method showed superior result to the commonly employed method 
we study the dynamic of a hebbian ica algorithm extracting a single non gaussian component from a high dimensional gaussian background for both on line and batch learning we find that a surprisingly large number of example are required to avoid trapping in a sub optimal state close to the initial condition to extract a skewed signal at least example are required for dimensional data and example are required to extract a symmetrical signal with non zero kurtosis 
the hr program by colton et al performs theory formation in mathematics byexploring a space of mathematical concept by enabling hr to determine when it hasfound a particular concept and by adding aforward looking mechanism we have appliedhr to the problem of identifying mathematicalconcepts we illustrate this by using hrto identify and extrapolate integer sequencesand by performing a qualitative comparisonwith the machine learning program progol introduction 
people are active experimenter constantly seeking new information relevant to their goal a reasonable approach to active information gathering is to ask question and conduct experiment that minimize the expected state of uncertainty or maximize the expected information gain given current belief fedorov mackay oaksford chater in this paper we present result on an exploratory experiment designed to study people s active information gathering behavior on a concept learning task the result of the experiment suggest subject behavior may be explained well from the point of view of bayesian information maximization introduction 
acetylcholine ach ha been implicated in a wide variety of task involving attentional process and plasticity following extensive animal study it ha previously been suggested that ach report on uncertainty and control hippocampal cortical and cortico amygdalar plasticity we extend this view and consider it effect on cortical representational inference arguing that ach control the balance between bottom up inference inuenced by input stimulus and top down inference inuenced by contextual information we illustrate our proposal using a hierarchical hidden markov model 
a novel learning approach for human face detection using a network of linear unit is presented the snow learning architecture is a sparse network of linear function over a pre defined or incrementally learned feature space and is specifically tailored for learning in the presence of a very large number of feature a wide range of face image in different pose with different expression and under different lighting condition are used a a training set to capture the variation of human 
we present a novel way of obtaining pac style bound on the generalizationerror of learning algorithm explicitly using their stabilityproperties a stable learner being one for which the learnedsolution doe not change much for small change in the trainingset the bound we obtain do not depend on any measure of thecomplexity of the hypothesis space e g vc dimension but ratherdepend on how the learning algorithm search this space andcan thus be applied even when the vc 
in this paper we introduce a new sparseness inducing prior which doe not involve any hyper parameter that need to be adjusted or estimated although other application are possible we focus here on supervised learning problem regression and classification experiment with several publicly available benchmark data set show that the proposed approach yield state of the art performance in particular our method outperforms support vector machine and performs competitively with the best alternative technique both in term of error rate and sparseness although it involves no tuning or adjusting of sparsenesscontrolling hyper parameter 
many computational problem can be solved by multiple algorithm with different algorithm fastest for different problem size input distribution and hardware characteristic we consider the problem of algorithm selection dynamically choose an algorithm to attack an instance of a problem with the goal of minimizing the overall execution time we formulate the problem a a kind of markov decision process mdp and use idea from reinforcement learning to solve it this paper introduces a kind of mdp that model the algorithm selection problem by allowing multiple state transition the well known q learning algorithm is adapted for this case in a way that combine both monte carlo and temporal difference method also this work us and extends in a way to control problem the least square temporal difference algorithm lstd of boyan the experimental study focus on the classic problem of order statistic selection and sorting the encouraging result reveal the potential of applying learning method to traditional computational problem 
an active set strategy is applied to the dual of a simple reformulation of the standard quadratic program of a linear support vector machine this application generates a fast new dual algorithm that consists of solving a finite number of linear equation with a typically large dimensionality equal to the number of point to be classified however by making novel use of the sherman morrisonwoodbury formula a much smaller matrix of the order of the original input space is inverted at each step thus a problem with a dimensional input space and million point required inverting positive definite symmetric matrix of size with a total running time of minute on a mhz pentium ii the algorithm requires no specialized quadratic or linear programming code but merely a linear equation solver which is publicly available 
in many scientific and engineering application detecting and understanding difference between two group of example can be reduced to a classical problem of training a classifier for labeling new example while making a few mistake a possible in the traditional classification setting the resulting classifier is rarely analyzed in term of the property of the input data captured by the discriminative model however such analysis is crucial if we want to understand and visualize the detected difference we propose an approach to interpretation of the statistical model in the original feature space that allows u to argue about the model in term of the relevant change to the input vector for each point in the input space we define a discriminative direction to be the direction that move the point towards the other class while introducing a little irrelevant change a possible with respect to the classifier function we derive the discriminative direction for kernel based classifier demonstrate the technique on several example and briefly discus it use in the statistical shape analysis an application that originally motivated this work 
abstract the contrast response function crf of many neuron in the primary vi sual cortex saturates and shift towards higher contrast value following prolonged presentation of high contrast visual stimulus using a recurrent neural network of excitatory spiking neuron with adapting synapsis we show that both effect could be explained by a fast and a slow compo nent in the synaptic adaptation i fast synaptic depression lead to sat uration of the crf and phase advance in the cortical response to high contrast stimulus ii slow adaptation of the synaptic transmitter release probability is derived such that the mutual information between the input and the output of a cortical neuron is maximal this component given by infomax learning rule explains contrast adaptation of the averaged membrane potential dc component a well a the surprising experi mental result that the stimulus modulated component f component of a cortical cell s membrane potential adapts only weakly based on our result we propose a new experiment to estimate the strength of the ef fective excitatory feedback to a cortical neuron and we also suggest a relatively simple experimental test to justify our hypothesized synaptic mechanism for contrast adaptation 
vapnik s result that the expectation of the generalisation error of the o ptimal hyperplane is bounded by the expectation of the ratio of the number of support vector to the number of training example is extended to a broad class of kernel machine the class includes support vector machine for soft margin classification and regression and regularization network with a variety of kernel and cost function we show that key inequality in vapnik s result become equality once the classification error is replaced by the margin error with the latter defined a an instance with positive cost in particular we show that expectation of th e true margin error and the empirical margin error are equal and that the sparse solution for kernel machine are possible only if the cost fun ction is partially insensitive 
the world around u change constantly knowing what ha changed is an important part of our life for business recognizing change is also crucial it allows business to adapt themselves to the changing market need in this paper we study change of association rule from one time period to another one approach is to compare the support and or confidence of each rule in the two time period and report the difference this technique however is too simplistic a it tends to report a huge number of rule change and many of them are in fact simply the snowball effect of a small subset of fundamental change here we present a technique to highlight the small subset of fundamental change a change is fundamental if it cannot be explained by some other change the proposed technique ha been applied to a number of real life datasets experiment result show that the number of rule whose change are unexplainable is quite small about of the total number of change discovered and many of these unexplainable change reflect some fundamental shift in the application domain 
partial information can trigger a complete memory at the same time human memory is not perfect a cue can contain enough information to specify an item in memory but fail to trigger that item in the context of word memory we present experiment that demonstrate some basic pattern in human memory error we use cue that consist of word fragment we show that short and long cue are completed more accurately than medium length one and study some of the factor that lead to this behavior we then present a novel computational model that show some of the flexibility and pattern of error that occur in human memory this model iterates between bottom up and top down computation these are tied together using a markov model of word that allows memory to be accessed with a simple feature set and enables a bottom up process to compute a probability distribution of possible completion of word fragment in a manner similar to model of visual perceptual completion 
machine learning typically involves discovering regularity in a training set then applying these learned regularity to classify object in a test set in this paper we present an approach to discovering additional regularity in the test set and show that in relational domain such test set regularity can be used to improve classification accuracy beyond that achieved using the training set alone for example we have previously shown how foil a relational learner can learn to classify web page by discovering training set regularity in the word occurring on target page and on other page related by hyperlink here we show how the classification accuracy of foil on this task can be improved by discovering additional regularity on the test set page that must be classified our approach can be seen a an extension to kleinberg s hub and authority algorithm that analyzes hyperlink relation among web page we present evidence that this new algorithm lead to better test set precision and recall on three binary web classification task where the test set web page are taken from different web site than the training set 
based on a statistical mechanic approach we develop a methodfor approximately computing average case learning curve for gaussianprocess regression model the approximation work well inthe large sample size limit and for arbitrary dimensionality of theinput space we explain how the approximation can be systematicallyimproved and argue that similar technique can be applied togeneral likelihood model introductiongaussian process gp model have gained considerable interest in 
the statistic of photographic image when represented usingmultiscale wavelet base exhibit two striking type of nongaussianbehavior first the marginal density of the coefficientshave extended heavy tail second the joint density exhibit variancedependencies not captured by second order model we examineproperties of the class of gaussian scale mixture and showthat these density can accurately characterize both the marginaland joint distribution of natural 
ensemble learning algorithm combine the result of several classifier to yield an aggregate classification we present a normative evaluation of combination method applying and extending existing axiomatizations from social choice theory and statistic for the case of multiple class we show that several seemingly innocuous and desirable property are mutually satisfied only by a dictatorship a weaker set of property admit only the weighted average combination rule for the case of binary classification we give axiomatic justification for majority vote and for weighted majority we also show that even when all component algorithm report that an attribute is probabilistically independent of the classification common ensemble algorithm often destroy this independence information we exemplify these theoretical result with experiment on stock market data demonstrating how ensemble of classifier can exhibit canonical voting paradox 
in the analysis of data recorded by optical imaging from intr insic signal measurement of change of light reflectance from cortical t issue the removal of noise and artifact such a blood vessel pattern is a serious problem often bandpass filtering is used but the underlyin g assumption that a spatial frequency exists which separate the mappin g component from other component especially the global signal is qu estionable here we propose alternative way of processing optical imaging data using blind source separation technique based on the spatial decorrelation of the data we first perform benchmark on artificial data in o rder to select the way of processing which is most robust with respect to sensor noise we then apply it to recording of optical imaging experiment from macaque primary visual cortex we show that our bs technique is able to extract ocular dominance and orientation preferenc e map from single condition stack for data where standard post pro cessing procedure fail artifact especially blood vessel pattern can often be completely removed from the map in summary our method for blind source separation using extended spatial decorrelation is a superior technique for the analysis of optical recording data 
in this paper we present a novel approach to multichannel blindseparation generalized deconvolution assuming that both mixingand demixing model are described by stable linear state space system we decompose the blind separation problem into two process separation and state estimation based on the minimizationof kullback leibler divergence we develop a novel learning algorithmto train the matrix in the output equation to estimate thestate of the demixing model we introduce a new 
explaining away ha mostly been considered in term of inference of state in belief network we show how it can also arise in a bayesian context in inference about the weight governing relationship such a those between stimulus and reinforcer in conditioning experiment such a backward blocking we show how explaining away in weight space can be accounted for using an extension of a kalman filter model provide a new approximate way of looking at the kalman gain matrix a a whitener for the correlation matrix of the observation process suggest a network implementation of this whitener using an architecture due to goodall and show that the resulting model exhibit backward blocking 
population code often rely on the tuning of the mean response to the stimulus parameter however this information can be greatly suppressed by long range correlation here we study the efficiency of coding information in the second order statistic of the population response we show that the fisher information of this system grows linearly with the size of the system we propose a bilinear readout model for extracting information from correlation code and evaluate it performance in discrimination and estimation task it is shown that the main source of information in this system is the stimulus dependence of the variance of the single neuron response neuronal population response can represent information in the higher order statistic of the response not only in their mean in this work we study the accuracy of coding information in the second order statistic we call such scheme correlation code specifically we assume that the neuronal response obey multivariate gaussian statistic governed by a stimulus dependent correlation matrix we ask whether the fisher information of such a system is extensive even in the presence of strong correlation in the neuronal 
a theory of categorization is presented in which knowledge ofcausal relationship between category feature is represented a abayesian network referred to a causal model theory this theorypredicts that object are classified a category member to theextent they are likely to have been produced by a category s causalmodel on this view people have model of the world that leadthem to expect a certain distribution of feature in categorymembers e g correlation between 
humanoid robot are high dimensional movement system for which analytical system identification and control method are insufficient due to unknown nonlinearities in the system structure a a way out supervised learning method can be employed to create model based nonlinear controller which use function in the control loop that are estimated by learning algorithm however internal model for humanoid system are rather high dimensional such that conventional learning algorithm would suffer from slow learning speed catastrophic interference and the curse of dimensionality in this paper we explore a new statistical learning algorithm locally weighted projection regression lwpr for learning internal model in real time lwpr is a nonparametric spatially localized learning system that employ the le familiar technique of partial least square regression to represent functional relationship in a piecewise linear fashion the algorithm can work successfully in very high dimensional space and detect irrelevant and redundant input while only requiring a computational complexity that is linear in the number of input dimension we demonstrate the application of the algorithm in learning two classical internal model of robot control the inverse kinematics and the inverse dynamic of an actual seven degree of freedom anthropomorphic robot arm for both example lwpr can achieve excellent real time learning result from le than one hour of actual training data 
traditionally computer application to game domain have taken a brute force approach relying on sheer computational power to overcome the complexity of the domain although many of these program have been quite successful it is interesting to note that human can still perform extremely well against them thus we are compelled to ask if no human could match the computational power of most of these program are there method for learning and performance in game domain that more closely reflect human cognition in response to this question this paper attempt to model how human learn and play game by developing a backgammon playing algorithm based on cognition analysis of this algorithm show that it is efficient and commensurate with human ability suggesting that it provides a cognitively plausible theory of learning in backgammon 
we adopt stochastic game a a general framework for dynamicnoncooperative system this framework provides a way of describingthe dynamic interaction of agent in term of individual markovdecision process by studying this framework we go beyond thecommon practice in the study of learning in game which primarilyfocus on repeated game or extensive form game for stochasticgames with incomplete information we design a multiagent reinforcementlearning method which allows agent 
kernel are problem specific function that act a an interface between the learning system and the data while it is well known when the combination of two kernel is again a valid kernel it is an open question if the resulting kernel will perform well in particular in which situation can a combination of kernel be expected to perform better than it component considered separately intuitively one would like each of the two kernel to contribute information that is not available to the other this characterization hence must consider the data at hand both the kernel and also the task that is the information given by the label we investigate this problem by looking at the task of designing kernel for hypertext classification where both word and link information can be exploited firstly we introduce a novel kernel whose gram matrix is the well known co citation matrix from bibliometrics and demonstrate on real data that it ha a good performance then we study the problem of combining it with a standard bag of word kernel we provide sufficient condition that indicate when an improvement can be expected highlighting and formalising the notion of independent kernel experimental result confirm the prediction of the theory in the hypertext domain 
recommender system use rating from user on item such a movie and music for the purpose of predicting the user preference on item that have not been rated prediction are normally done by using the rating of other user of the system by learning the user preference a a function of the feature of the item or by a combination of both these method in this paper we pose the problem a one of collaboratively learning of preference function by multiple user of the recommender system we study several mixture model for this task we show via theoretical analysis and experiment on a movie rating database how the model can be designed to overcome common problem in recommender system including the new user problem the recurring startup problem the sparse rating problem and the scaling problem 
this paper discus visual method that can be used to understand and interpret the result of classification using support vector machine svm on data with continuous real valued variable svm induction algorithm build pattern classifier by identifying a maximal margin separating hyperplane from training example in high dimensional pattern space or space induced by suitable nonlinear kernel transformation over pattern space svm have been demonstrated to be quite effective in a number of practical pattern classification task since the separating hyperplane is defined in term of more than two variable it is necessary to use visual technique that can navigate the viewer through high dimensional space we demonstrate the use of projection based tour method to gain useful insight into svm classifier with linear kernel on dimensional data 
an eigenvalue method is developed for analyzing periodic structure in speech signal are analyzed by a matrix diagonalization reminiscent of method for principal component analysis pca and independent component analysis ica our method called periodic component analysis ca us constructive interference to enhance periodic component of the frequency spectrum and destructive interference to cancel noise the front end emulates important aspect of auditory processing such a cochlear filtering nonlinear compression and insensitivity to phase with the aim of approaching the robustness of human listener the method avoids the inefficiency of autocorrelation at the pitch period it doe not require long delay line and it correlate signal at a clock rate on the order of the actual pitch a opposed to the original sampling rate we derive it cost function and present some experimental result 
we investigate the problem of learning a classification task on data represented in term of their pairwise proximity this representation doe not refer to an expli cit feature representation of the data item and is thus more general than the standard approach of using euclidean feature vector from which pairwise proximity can always be calculated our first approach is based on a combined linea r embedding and classification procedure resulting in an extension of the optimal hyperplane algorithm to pseudo euclidean data a an alternative we present another approach based on a linear threshold model in the proximity value themselves which is optimized using structural risk minimization we show that prior knowledge about the problem can be incorporated by the choice of distance measure and examine different metric w r t their genus lization finally the algorithm are successfully applie d to protein structure data and to data from the cat s cerebral cortex they show better performance than k nearestneighbor classification 
driven by the progress in the field of single trial analysis of eeg there is a growing interest in brain computer interface bcis i e system that enable human subject to control a computer only by mean of their brain signal in a pseudo online simulation our bci detects upcoming finger movement in a natural keyboard typing condition and predicts their laterality this can be done on average m before the respective key is actually pressed i e long before the onset of emg our approach is appealing for it short response time and high classification accuracy in a binary decision where no human training is involved we compare discriminative classifier like support vector machine svms and different variant of fisher discriminant that posse favorable regularization property for dealing with high noise case inter trial variablity 
this essay give advice to author of paperson machine learning although much of it carriesover to other computational discipline the issue covered include the material thatshould appear in a well balanced paper factorsthat arise in different approach to evaluation and way to improve a submission sability to communicate idea to it reader introductionalthough machine learning ha become a scientific discipline the effective communication of it idea remainsan 
we introduce and evaluate treedt a novel gene mapping method which is based on discovering and assessing tree like pattern in genetic marker data gene mapping aim at discovering a statistical connection from a particular disease or trait to a narrow region in the genome in a typical case control setting data consists of genetic marker typed for a set of disease associated chromosome and a set of control chromosome a computer scientist would view this data a a set of string treedt extract essentially in the form of substring and prefix tree information about the historical recombination in the population this information is used to locate fragment potentially inherited from a common diseased founder and to map the disease gene into the most likely such fragment the method measure for each chromosomal location the disequilibrium of the prefix tree of marker string starting from the location to ass the distribution of disease associated chromosome we evaluate experimentally the performance of treedt on realistic simulated data set and comparison to state of the art method tdt hpm show that treedt is very competitive 
the most popular algorithm for object detection require the use of exhaustive spatial and scale search procedure in such approach an object is defined by mean of local feature in this paper we show that including contextual information in object detection procedure provides an efficient way of cutting down the need for exhaustive search we present result with real image showing that the proposed scheme is able to accurately predict likely object class location and size 
abstract a critical component of applying machine learning algorithm is evaluating the per formance of the model induced and using the evaluation to guide further development traditionally the most common evaluation metric is error or loss however this provides very little information for the designer to use when constructing a system we argue that an evaluation method should provide detailed feedback on the performance of an algorithm and that this feedback should be in the lan guage of the problem our goal is to char acterize model error or the di erences be tween model in the feature space we pro vide a framework for this that allows di er ent algorithm to be used a the discovery engine and we consider two approach a classi cation strategy where we use a stan dard rule learner such a c a descriptive paradigm where we use a new discovery algo rithm a contrast set miner we show that c su er from several problem that make it unsuitable for this task 
the problem that we address in this paper is how a mobile robot can plan in order to arrive at it goal with minimum uncertainty traditional motion planning algorithm often assume that a mobile robot can track it position reliably however in real world situation reliable localization may not always be feasible partially observable markov decision process pomdps provide one way to maximize the certainty of reaching the goal state but at the cost of computational intractability for large state space the method we propose explicitly model the uncertainty of the robot s position a a state variable and generates trajectory through the augmented pose uncertainty space by minimizing the positional uncertainty at the goal the robot reduces the likelihood it becomes lost we demonstrate experimentally that coastal navigation reduces the uncertainty at the goal especially with degraded localization 
hierarchical reinforcement learning rl is a general framework which study how to exploit the structure of action and task to accelerate policy learning in large domain prior work in hierarchical rl such a the maxq method ha been limited to the discrete time discounted reward semimarkov decision process smdp model this paper generalizes the maxq method to continuous time discounted and average reward smdp model we describe two hierarchical reinforcement learning algorithm continuous time discounted reward maxq and continuous time average reward maxq we apply these algorithm to a complex multiagent agv scheduling problem and compare their performance and speed with each other a well a several well known agv scheduling heuristic 
olshausen field demonstrated that a learning algorithm that attempt to generate a sparse code for natural scene develops a complete family of localised oriented bandpass receptive field similar to those of simple cell in v this paper describes an algorithm which find a sparse code for sequence of image that preserve information about the input this algorithm when trained on natural video sequence develops base representing the movement in particular direction with particular speed similar to the receptive field of the movement sensitive cell observed in cortical visual area furthermore in contrast to previous approach to learning direction selectivity the timing of neuronal activity encodes the phase of the movement so the precise timing of spike is crucially important to the information encoding 
we consider prediction model evaluation in the context of marketing campaign planning in order to evaluate and compare model with specific campaign objective in mind we need to concentrate our attention on the appropriate evaluation criterion these should portray the model s ability to score accurately and to identify the relevant target population in this paper we discus some applicable model evaluation and selection criterion their relevance for campaign planning their robustness under changing population distribution and their employment when constructing confidence interval we illustrate our result with a case study based on our experience from several project 
clustering algorithm conduct a search through the space of possible organization of a data set in this paper we propose two type of instance level clustering constraint must link and cannot link constraint and show how they can be incorporated into a clustering algorithm to aid that search for three of the four data set tested our result indicate that the incorporation of surprisingly few such constraint can increase clustering accuracy while decreasing runtime we also investigate the relative eects of each type of constraint and nd that the type that contributes most to accuracy improvement depends on the behavior of the clustering algorithm without constraint 
the choice of an svm kernel corresponds to the choice of a representation of the data in a feature space and to improve performance it should therefore incorporate prior knowledge such a known transformation invariance we propose a technique which extends earlier work and aim at incorporating invariance in nonlinear kernel we show on a digit recognition task that the proposed approach is superior to the virtual support vector method which previously had been the method of choice 
this paper further develops aumann and lindell s proposal for a variant of association rule for which the consequent is a numeric variable it is argued that these rule can discover useful interaction with numeric data that cannot be discovered directly using traditional association rule with discretization alternative measure for identifying interesting rule are proposed efficient algorithm are presented that enable these rule to be discovered for dense data set for which application of auman and lindell s algorithm is infeasible 
random projection have recently emerged a a powerful method for dimensionality reduction theoretical result indicate that the method preserve distance quite nicely however empirical result are sparse we present experimental result on using random projection a a dimensionality reduction tool in a number of case where the high dimensionality of the data would otherwise lead to burden some computation our application area are the processing of both noisy and noiseless image and information retrieval in text document we show that projecting the data onto a random lower dimensional subspace yield result comparable to conventional dimensionality reduction method such a principal component analysis the similarity of data vector is preserved well under random projection however using random projection is computationally significantly le expensive than using e g principal component analysis we also show experimentally that using a sparse random matrix give additional computational saving in random projection 
despite it popularity for general clustering k mean suffers three major shortcoming it scale poorly computationally the numberof cluster k ha to be supplied by theuser and the search is prone to local minimum we propose solution for the first twoproblems and a partial remedy for the third building on prior work for algorithmic accelerationthat is not based on approximation we introduce a new algorithm that efficiently search the space of cluster location and 
we present new simulation result in which a computational model of interacting visual neuron simultaneously predicts the modulation of spatial vision threshold by focal visual attention for five dual task human psychophysics experiment this new study complement our previous finding that attention activates a winner take all competition among early visual neuron within one cortical hypercolumn this quot intensified competition quot hypothesis assumed that attention equally affect all 
output coding is a general method for solving multiclass problem by reducing them to multiple binary classification problem previous research on output coding ha employed almost solely predefined discrete code we describe an algorithm that improves the performance of output code by relaxing them to continuous code the relaxation procedure is cast a an optimization problem and is reminiscent of the quadratic program for support vector machine we describe experiment with the proposed algorithm comparing it to standard discrete output code the experimental result indicate that continuous relaxation of output code often improve the generalization performance especially for short code 
instead of a standard support vector machine svm that classifies point by assigning them to one of two disjoint half space point are classified by assigning them to the closest of two parallel plane in input or feature space that are pushed apart a far a possible this formulation which can also be interpreted a regularized least square and considered in the much more general context of regularized network lead to an extremely fast and simple algorithm for generating a linear or nonlinear classifier that merely requires the solution of a single system of linear equation in contrast standard svms solve a quadratic or a linear program that require considerably longer computational time computational result on publicly available datasets indicate that the proposed proximal svm classifier ha comparable test set correctness to that of standard svm classifier but with considerably faster computational time that can be an order of magnitude faster the linear proximal svm can easily handle large datasets a indicated by the classification of a million point attribute set in second all computational result are based on line of matlab code 
we propose a new approach to the problem of searching a space of stochastic controller for a markov decision process mdp or a partially observable markov decision process pomdp following several other author our approach is based on searching in parameterized family of policy for example via gradient descent to optimize solution quality however rather than trying to estimate the value and derivative of a policy directly we do so indirectly using estimate for the probability density that the policy induces on state at the different point in time this enables our algorithm to exploit the many technique for efficient and robust approximate density propagation in stochastic system we show how our technique can be applied both to deterministic propagation scheme where the mdp s dynamic are given explicitly in compact form and to stochastic propagation scheme where we have access only to a generative model or simulator of the mdp we present empirical result for both of these variant on complex problem 
dual estimation refers to the problem of simultaneously estimating thestate of a dynamic system and the model which give rise to the dynamic algorithm include expectation maximization em dual kalmanfiltering and joint kalman method these method have recently beenexplored in the context of nonlinear modeling where a neural networkis used a the functional form of the unknown model typically an extendedkalman filter ekf or smoother is used for the part of the algorithm 
dual estimation refers to the problem of simultaneously estimating the state of a dynamic system and the model which give rise to the dynamic algorithm include expectation maximization em dual kalman filtering and joint kalman method these method have recently been explored in the context of nonlinear modeling where a neural network is used a the functional form of the unknown model typically an extended kalman filter ekf or smoother is used for the part of the algorithm that estimate the clean state given the current estimated model an ekf may also be used to estimate the weight of the network this paper point out the flaw in using the ekf and proposes an improvement based on a new approach called the unscented transformation ut a substantial performance gain is achieved with the same order of computational complexity a that of the standard ekf the approach is illustrated on several dual estimation method 
the human figure exhibit complex and rich dynamic behavior that is both nonlinear and time varying effective model of human dynamic can be learned from motion capture data using switching linear dynamic system slds model we present result for human motion synthesis classification and visual tracking using learned slds model since exact inference in slds is intractable we present three approximate inference algorithm and compare their performance in particular a new variational inference algorithm is obtained by casting the slds model a a dynamic bayesian network classification experiment show the superiority of slds over conventional hmm s for our problem domain in this paper we present a framework for slds learning and apply it to figure motion modeling we derive three different approximate inference scheme viterbi variational and gpb we apply learned motion model to three task classification motion synthesis and visual tracking our result include an empirical comparison between slds and hmm model on classification and one step ahead prediction task the slds model class consistently outperforms standard hmms even on fairly simple motion sequence our result suggest that slds model are a promising tool for figure motion analysis and could play a key role in application such a gesture recognition visual surveillance and computer animation in addition this paper provides a summary of approximate inference technique which is lacking in the previous literature on slds furthermore our variational inference algorithm is novel and it provides another example of the benefit of interpreting classical statistical model a mixed state graphical model 
in this work we introduce an interactive part ip model a an alternative to hidden markov model hmms we tested both model on a database of on line cursive script we show that implementation of hmms and the ip model in which all letter are assumed to have the same average width give comparable result however in contrast to hmms the ip model can handle duration modeling without an increase in computational complexity 
we formulate the problem of retrieving image from visual database a a problem of bayesian inference this lead to natural and effective solution for two of the most challenging issue in the design of a retrieval system providing support for region based query without requiring prior image segmentation and accounting for user feedback during a retrieval session we present a new learning algorithm that relies on belief propagation to account for both positive and negative example of the user s interest 
we develop an intuitive geometric framework for support vector regression svr by examining when tube exist we show that svr can be regarded a a classication problem in the dual space hard and soft tube are constructed by separating the convex or reduced convex hull respectively of the training data with the response variable shifted up and down by a novel svr model is proposed based on choosing the max margin plane between the two shifted datasets maximizing the margin corresponds to shrinking the eectiv e tube in the proposed approach the eects of the choice of all parameter become clear geometrically 
we replace the commonly used gaussian noise model in nonlinear regression by a moreexible noise model based on the student tdistribution the degree of freedom of the t distribution can be chosen such that a special case either the gaussian distribution or the cauchy distribution are realized the latter is commonly used in robust regression since the t distribution can be interpreted a being an in nite mixture of gaussians parameter and hyperparameters such a the degree of freedom of the t distribution can be learned from the data based on an em learning algorithm we show that modeling using the t distribution lead to improved predictor on real world data set in particular if outlier are present the t distribution is superior to the gaussian noise model in effect by adapting the degree of freedom the system can learn to distinguish between outlier and non outlier especially for online learning task one is interested in avoiding inappropriate weight change due to measurement outlier to maintain stable online learning capability we show experimentally that using the t distribution a a noise model lead to stable online learning algorithm and outperforms state of the art online learning method like the extended kalman lter algorithm 
a fundamental problem with the modeling of chaotic time series data is that minimizing short term prediction error doe not guarantee a match between the reconstructed attractor of model and experiment we introduce a modeling paradigm that simultaneously learns to short term predict and to locate the outline of the attractor by a new way of nonlinear principal component analysis closed loop prediction are constrained to stay within these outline to prevent divergence from the attractor learning is exceptionally fast parameter estimation for the sample laser data from the santa fe time series competition took le than a minute on a mhz pentium pc 
reinforcement learning is often done using parameterized function approximators to storevalue function algorithm are typically developed for lookup table and then appliedto function approximators by using backpropagation this can lead to algorithmsdiverging on very small simple mdps and markov chain even with linear functionapproximators and epoch wise training these algorithm are also very difficult toanalyze and difficult to combine with other algorithm a series of new 
in this paper we consider nite mdps withfatal state we dene the risk under a policyas the probability of entering a fatal state which is dierent to the notion of risk normallyused in dp and rl most often regardingthe variance of the return we considerthe problem of nding optimal policieswith bounded risk i e where the risk issmaller than some user specied threshold and formalize it a a constrained mdp withtwo innite horizon criterion a discountedone for 
we investigate the generalization performance of some learning problem in hilbert functional space we introduce a notion of convergence of the estimated functional predictor to the best underlying predictor and obtain an estimate on the rate of the convergence this estimate allows u to derive generalization bound on some learning formulation 
in this paper we give necessary and sucient condition under which kernel of dot product type k x y k x y satisfy mercer s condition and thus may be used in support vector machine svm regularization network rn or gaussian process gp in particular we show that if the kernel is analytic i e can be expanded in a taylor series all expansion coecients have to be nonnegative we give an explicit functional form for the feature map by calculating it eigenfunctions and eigenvalue 
in previous work on transformed mixture of gaussians and transformed hidden markov model we showed how the em algorithm in a discrete latent variable model can be used to jointly normalize data e g center image pitch normalize spectrogram and learn a mixture model of the normalized data the only input to the algorithm is the data a list of possible transformation and the number of cluster to nd the main criticism of this work wa that the exhaustive computation of the posterior probability over transformation would make scaling up to large feature vector and large set of transformation intractable here we describe how a tremendous speed up is acheived through the use of a variational technique for decoupling transformation and a fast fourier transform method for computing posterior probability for n n image learning c cluster under n rotation n scale n x translation and n y translation take only c logn n scalar operation per iteration in contrast the original algorithm take cn operation to account for these transformation we give result on learning a component mixture model from a video sequence with frame of size the model account for rotation and translation each iteration of em take only second per frame in matlab which is over million time faster than the original algorithm 
a multiagent environment become more prevalent we need to understand how this change the agent based paradigm one aspect that is heavily affected by the presence of multiple agent is learning traditional learning algorithm have core assumption such a markovian transition which are violated in these environment yet understanding the behavior of learning algorithm in these domain is critical singh kearns and mansour examine gradient ascent learning specifically within a restricted class of repeated matrix game they prove that when using this technique the average of expected payoff over time converges on the other hand they also show that neither the player strategy nor their expected payoff themselves are guaranteed to converge in this paper we introduce a variable learning rate for gradient ascent along with the wolf win or learn fast principle for regulating the learning rate we then prove that this modification to gradient ascent ha the stronger notion of convergence that is strategy and payoff converge to a nash equilibrium 
many algorithm for approximate reinforcement learning are notknown to converge in fact there are counterexample showingthat the adjustable weight in some algorithm may oscillate withina region rather than converging to a point this paper show that for two popular algorithm such oscillation is the worst that canhappen the weight cannot diverge but instead must convergeto a bounded region the algorithm are sarsa and v thelatter algorithm wa used in the 
if all feature causing heterogeneity were observed a mixture of expert approach jacob et al is likely to be superior to using a single model when unobserved or very noisy spatial feature are the cause for the heterogeneity the observed feature space of homogeneous subset can highly overlap leading to a biased global model or biased mixture of expert our goal is to allow more accurate prediction in such situation here a supervised machine learning algorithm for the analysis of heterogeneous spatial data is proposed it is based on partitioning the data set into more homogeneous region by competition of regression model linear or nonlinear the algorithm start from learning a global model and add new model into the competition until each model becomes specialized for one of the region the competition convergence is proven theoretically also the influence of filtering the competing model residual for improving convergence speed and accuracy is discussed a number of experiment on artificial and real life spatial data are performed to validate some aspect of the algorithm and to illustrate it potential application the obtained result provide strong evidence that homogeneous region can be identified with high accuracy by using the proposed approach even when their observed feature space highly overlap an assumption of data independence valid for most of standard machine learning data set is often unrealistic for spatial variable whose dependence is strongly tied to a location where observation spatially close to each other are more likely to be similar than observation widely separated in space a a consequence error of spatial prediction model are also spatially correlated cressie the method proposed here incorporates knowledge of spatial correlation for more accurate partitioning of heterogeneous spatial data set the new partitioning algorithm is based on three important mechanism a competition among learning model for spatial data point b averaging error of each competing model over neighboring data point and c an incremental introduction of additional model into the competition when needed 
the paper describes a case study in combiningdifferent method for acquiring medicalknowledge given a huge amount of noisy high dimensional numerical time series datadescribing patient in intensive care the supportvector machine is used to learn whenand how to change the dose of which drug given medical knowledge about and expertisein clinical decision making a first orderlogic knowledge base about effect of therapeuticalinterventions ha been built a apreprocessing 
organization conducting electronic commerce e commerce can greatly benefit from the insight that data mining of transactional and clickstream data provides such insight help not only to improve the electronic channel e g a web site but it is also a learning vehicle for the bigger organization conducting business at brick and mortar store the e commerce site serf a an early alert system for emerging pattern and a laboratory for experimentation for successful data mining several ingredient are needed and e commerce provides all the right one the good web server log which are commonly used a the source of data for mining e commerce data were designed to debug web server and the data they provide is insufficient requiring the use of heuristic to reconstruct event moreover many event are never logged in web server log limiting the source of data for mining the bad many of the problem of dealing with web server log data can be resolved by properly architecting the e commerce site to generate data needed for mining even with a good architecture however there are challenging problem that remain hard to solve the ugly lesson and metric based on mining real e commerce data are presented 
this paper present disciple coa the most recent learning agent shell developed in the disciple framework that aim at changing the way an intelligent agent is built from being programmed by a knowledge engineer to being taught by a domain expert disciple coa can collaborate with the expert to develop it knowledge base consisting of a frame based ontology that defines the term from the application domain and a set of plausible version space rule expressed with these term it central component is a plausible reasoner that can distinguish between four type of problem solving situation routine innovative inventive and creative this ability guide the interaction with the expert during which the agent learns general rule from specific example by integrating a wide range of knowledge acquisition and machine learning strategy including apprenticeship learning empirical inductive learning from example and explanation and analogical learning disciplecoa wa developed in the darpa s high performance knowledge base program to solve the challenge problem of critiquing military course of action that were developed a hasty candidate plan for ground combat operation we present the course of action challenge problem the process of teaching disciple coa to solve it and the result of darpa s evaluation in which disciple coa demonstrated the best knowledge acquisition rate and problem solving performance we also present a separate knowledge acquisition experiment conducted at the battle command battle lab where expert with no prior knowledge engineering experience succeeded to rapidly teach disciple coa to correctly critique course of action 
we compare discriminative and generative learning a typified by logistic regression and naive bayes we show contrary to a widely held belief that discriminative classifier are almost always to be preferred that there can often be two distinct regime of performance a the training set size is increased one in which each algorithm doe better this stem from the observation which is borne out in repeated experiment that while discriminative learning ha lower asymptotic error a 
bayesian belief propagation in graphical model ha been recently shown to have very close tie to inference method based in statistical physic after yedidia et al demonstrated that belief propagation xed point correspond to extremum of the so called bethe free energy yuille derived a double loop algorithm that is guaranteed to converge to a local minimum of the bethe free energy yuille s algorithm is based on a certain decomposition of the bethe free energy and he mention that other decomposition are possible and may even be fruitful in the present work we begin with the bethe free energy and show that it ha a principled interpretation a pairwise mutual information minimization and marginal entropy maximization mime next we construct a family of free energy function from a spectrum of decomposition of the original bethe free energy for each free energy in this family we develop a new algorithm that is guaranteed to converge to a local minimum preliminary computer simulation are in agreement with this theoretical development 
this paper introduces an algorithm that combine na ve bayes classification with feature weighting most of the related approach to feature transformation for na ve bayes suggest various heuristic and non exhaustive search strategy for selecting a subset of feature with which na ve bayes performs better than with the complete set of feature in contrast the algorithm introduced in this paper employ feature weighting performed by a support vector machine the weight are optimised such that the danger of overfitting is reduced to the best of our knowledge this is the first time that na ve bayes classification ha been combined with feature weighting experimental result on uci domain demonstrate that wbcsvm compare favourably to state of theart machine learning approach 
we are interested in the mechanism by which individual monitor and adjust their performance of simple cognitive task we model a speeded discrimination task in which individual are asked to classify a sequence of stimulus jones braver response conflict arises when one stimulus class is infrequent relative to another resulting in more error and slower reaction time for the infrequent class how do control process modulate behavior based on the relative class frequency we explain performance from a rational perspective that cast the goal of individual a minimizing a cost that depends both on error rate and reaction time with two additional assumption of rationality that class prior probability are accurately estimated and that inference is optimal subject to limitation on rate of information transmission we obtain a good fit to overall rt and error data a well a trial by trial variation in performance consider the following scenario while driving you approach an intersection at which the traffic light ha already turned yellow signaling that it is about to turn red you also notice that a car is approaching you rapidly from behind with no indication of slowing should you stop or speed through the intersection the decision is difficult due to the presence of two conflicting signal such response conflict can be produced in a psychological laboratory a well for example stroop asked individual to name the color of ink on which a word is printed when the word are color name incongruous with the ink color e g blue printed in red reaction time are slower and error rate are higher we are interested in the control mechanism underlying performance of high conflict task conflict requires individual to monitor and adjust their behavior possibly responding more slowly if error are too frequent in this paper we model a speeded discrimination paradigm in which individual are asked to classify a sequence of stimulus jones braver the stimulus are letter of the alphabet a z presented in rapid succession in a choice task individual are asked to press one response key if the letter is an x or another response key for any letter other than x a a shorthand we will refer to non x stimulus a y in a go no go task individual 
integration in the head direction system is a computation by which horizontal angular head velocity signal from the vestibular nucleus are integrated to yield a neural representation of head direction in the thalamus the postsubiculum and the mammillary nucleus the head direction representation ha the form of a place code neuron have a preferred head direction in which their firing is maximal blair and sharp blair et al integration is a difficult computation given that head velocity can vary over a large range previous model of the head direction system relied on the assumption that the integration is achieved in a firing rate based attractor network with a ring structure in order to correctly integrate head velocity signal during high speed head rotation very fast synaptic dynamic had to be assumed here we address the question whether integration in the head direction system is possible with slow synapsis for example excitatory nmda and inhibitory gaba b type synapsis for neural network with such slow synapsis rate based dynamic are a good approximation of spiking neuron ermentrout we find that correct integration during high speed head rotation imposes strong constraint on possible network architecture 
theory of cue combination suggest the possibility of constructing visual stimulus that evoke different pattern of neural activity in sensory area of the brain but that cannot be distinguished by any behavioral measure of perception such stimulus if they exist would be interesting for two reason first one could know that none of the difference between the stimulus survive past the computation used to build the percept second it can be difficult to distinguish stimulus driven component of measured neural activity from top down component such a those due to the interestingness of the stimulus changing the stimulus without changing the percept could be exploited to measure the stimulusdriven activity here we describe stimulus in which vertical and horizontal disparity trade during the construction of percept of slanted surface yielding stimulus equivalence class equivalence class membership changed after a change of vergence eye posture alone without change to the retinal image a formal correspondence can be drawn between these perceptual metamers and more familiar sensory metamers such a color metamers 
this paper describes using multi agent reinforcement learning rl algorithm for learning traffic light controller to minimize the overall waiting time of car in a city the rl system learn value function estimating expected waiting time for car given different setting of traffic light selected setting of traffic light result from combining the predicted waiting time of all car involved we investigate rl system using different kind of global communicated information between 
this paper present a novel practical framework for bayesian modelaveraging and model selection in probabilistic graphical model our approach approximates full posterior distribution over modelparameters and structure a well a latent variable in an analyticalmanner these posterior fall out of a free form optimizationprocedure which naturally incorporates conjugate prior unlikein large sample approximation the posterior are generally nongaussianand no hessian need 
observer translation relative to the world creates image ow that expands from the observer s direction of translation heading from which the observer can recover heading direction yet the image flow is often more complex depending on rotation of the eye scene layout and translation velocity a number of model have been proposed on how the human visual system extract heading from flow in a neurophysiologically plausible way these model represent heading by a set of neuron that 
we describe a programmable multi chip vlsi neuronal system that can be used for exploring spike based information processing model the system consists of a silicon retina a pic microcontroller and a transceiver chip whose integrate and fire neuron are connected in a soft winner take all architecture the circuit on this multi neuron chip approximates a cortical microcircuit the neuron can be configured for different computational property by the virtual connection of a selected set of pixel on the silicon retina the virtual wiring between the different chip is effected by an event driven communication protocol that us asynchronous digital pulse similar to spike in a neuronal system we used the multi chip spike based system to synthesize orientation tuned neuron using both a feedforward model and a feedback model the performance of our analog hardware spiking model matched the experimental observation and digital simulation of continuous valued neuron the multi chip vlsi system ha advantage over computer neuronal model in that it is real time and the computational time doe not scale with the size of the neuronal network 
jensen s inequality is a powerful mathematical tool and one of the workhorse in statistical learning it application therein include the em algorithm bayesian estimation and bayesian inference jensen computes simple lower bound on otherwise intractable quantity such a product of sum and latent log likelihood this simplification then permit operation like integration and maximization quite often i e in discriminative learning upper bound are needed a well we derive and prove an efficient analytic inequality that provides such variational upper bound this inequality hold for latent variable mixture of exponential family distribution and thus span a wide range of contemporary statistical model we also discus application of the upper bound including maximum conditional likelihood large margin discriminative model and conditional bayesian inference convergence efficiency and prediction result are shown 
web usage mining is the application of data mining techniquesto large web data repository in order to extract usage pattern a with many data mining application domain the identification of patternsthat are considered interesting is a problem that must be solved inaddition to simply generating them a necessary step in identifying interestingresults is quantifying what is considered uninteresting in order toform a basis for comparison several research effort have relied on 
this paper explores the problem of featuresubset selection for unsupervised learningwithin the wrapper framework in particular we examine feature subset selection wrappedaround expectation maximization em clusteringwith order identification identifyingthe number of cluster in the data weinvestigate two different performance criteriafor evaluating candidate feature subset scatter separability and maximum likelihood when the quot true quot number of cluster k is unknown our 
the project pursued in this paper is to develop from rstinformation geometric principle a general method for learningthe similarity between text document each individual documentis modeled a a memoryless information source based ona latent class decomposition of the term document matrix a lowdimensional curved multinomial subfamily is learned from thismodel a canonical similarity function known a the fisher kernel is derived our approach can be applied for 
we incorporate prior knowledge to construct nonlinear algorithm for invariant feature extraction and discrimination employing a unified framework in term of a nonlinear variant of the rayleigh coefficient we propose non linear generalization of fisher s discriminant and oriented pca using support vector kernel function extensive simulation show the utility of our approach 
in kernel based method such a regularizationnetworks large datasets pose signi cant problem since the number of basis functionsrequired for an optimal solution equalsthe number of sample we present a sparsegreedy approximation technique to constructa compressed representation of the designmatrix experimental result are given andconnections to kernel pca sparse kernelfeature analysis and matching pursuit arepointed out introductionmany recent advance in 
stochastic meta descent smd is a new technique for online adaptation of local learning rate in arbitrary twice differentiable system like matrix momentum it us full second order information while retaining o n computational complexity by exploiting the efficient computation of hessian vector product here we apply smd to independent component analysis and employ the resulting algorithm for the blind separation of time varying mixture by matching individual learning rate to the rate of change in each source signal s mixture coefficient our technique is capable of simultaneously tracking source that move at very different a priori unknown speed 
with the increasing number of user of mobile computing device e g personal digital assistant and the advent of third generation mobile phone wireless communication are becoming increasingly important many application rely on the device maintaining a replica of a data structure which is stored on a server for example news database calendar and e mail in this paper we explore the question of the optimal strategy for synchronising such replica we utilise probabilistic model to represent how the data structure evolve and to model user behaviour we then formulate objective function which can be minimised with respect to the synchronisation timing we demonstrate using two real world data set that a user can obtain more up to date information using our approach 
mass collaboration is a new p p style approach to large scale knowledge sharing with application in customer support focused community development and capturing knowledge distributed within large organization effectively supporting this paradigm raise many technical challenge and offer intriguing opportunity for mining massive amount of data captured continually from user interaction data mining offer the promise of increased business intelligence and also improved user experience leading to increased participation and greater quality in the knowledge that is captured both of which are central objective in mass collaboration in this talk i will introduce mass collaboration and discus some important data mining related issue 
we develop an intuitive geometric interpretation of the standard support vector machine svm for classification of both linearly separable and inseparable data and provide a rigorous derivation of the concept behind the geometry for the separable case finding the maximum margin between the two set is equivalent to finding the closest point in the smallest convex set that contain each class the convex hull we now extend this argument to the inseparable case by using a reduced convex hull reduced away from outlier we prove that solving the reduced convex hull formulation is exactly equivalent to solving the standard inseparable svm for appropriate choice of parameter some additional advantage of the new formulation are that the effect of the choice of parameter becomes geometrically clear and that the formulation may be solved by fast nearest point algorithm by changing norm these argument hold for both the standard norm and norm svm a successful application there are three key idea needed to understand svm maximizing margin the dual formulation and kernel most people intuitively grasp the idea that maximizing margin should help improve generalization but changing from the primal to dual formulation is typically black magic for those uninitiated in duality theory duality is really the key concept frequently missing in the understanding of svm in this paper we provide an intuitive geometric explanation of svm for classification from the dual perspective along with a mathematically rigorous derivation of the idea behind the geometry we begin with an explanation of the geometry of svm based on the idea of convex hull for the separable case this geometric explanation ha existed in various form vapnik mangasarian keerthi et al bennett bredensteiner in press the new contribution is the adaptation of the convex hull argument for the inseparable case to the most commmonly used norm and norm soft margin svm the primal form resulting from this argument can be regarded a an especially elegant minor variant of the svm formulation scholkopf et al or a soft margin form of the msm method mangasarian related geometric idea for the svm formulation were developed independently by crisp and burges the primary contribution of this paper are 
this paper investigates condition underwhich modification to the reward functionof a markov decision process preserve the optimalpolicy it is shown that besides thepositive linear transformation familiar fromutility theory one can add a reward for transitionsbetween state that is expressible asthe difference in value of an arbitrary potentialfunction applied to those state furthermore this is shown to be a necessary conditionfor invariance in the sense that anyother 
we describe our research in applying data mining technique to construct intrusion detection model the key idea are to mine system audit data for consistent and useful pattern of program and user behavior and use the set of relevant system feature presented in the pattern to compute inductively learned classifier that can recognize anomaly and known intrusion our past experiment showed that classification rule can be used to detect intrusion provided that sufficient audit data is available for training and the right set of system feature are selected we use the association rule and frequent episode computed from audit data a the basis for guiding the audit data gathering and feature selection process in order to compute only the relevant useful pattern we consider the order of importance and reference relation among the attribute of data and modify these two basic algorithm accordingly to use axis attribute s and reference attribute s a form of item constraint in the data mining process we also use an iterative level wise approximate mining procedure for uncovering the low frequency but important pattern we report our experiment in using these algorithm on real world audit data 
the project pursued in this paper is to develop from rstinformation geometric principle a general method for learningthe similarity between text document each individual documentis modeled a a memoryless information source based ona latent class decomposition of the term document matrix a lowdimensional curved multinomial subfamily is learned from thismodel a canonical similarity function known a the fisher kernel is derived our approach can be applied for 
poker is an interesting test bed for artificialintelligence research it is a game of imperfectknowledge where multiple competing agentsmust deal with risk management opponentmodeling unreliable information and deception much like decision making application in thereal world opponent modeling is one of themost difficult problem in decision makingapplications and in poker it is essential toachieving high performance this paperdescribes and evaluates the implicit and 
the question of how software agent can learn strategic behavior in complex continually changing multi agent environment is not only a challenging forefront of theoretical research but potentially of immense practical importance a well in such system it would be difficult at best to hand code fixed strategy that would always perform well with high confidence especially if the other agent in the environment change their behavior over time using adaptive learning algorithm hence the need for learning a a component of overall agent programming methodology is readily apparent we expect this to be particularly true in the domain of agent economy in which large population of agent engage in various form of economic activity with each other and possibly with human a well 
td is a popular family of algorithm for approximate policy evaluation in large mdps td work by incrementally updating the value function after each observed transition it ha two major drawback it make inefficient use of data and it requires the user to manually tune a stepsize schedule for good performance for the case of linear value function approximation and the least square td lstd algorithm of bradtke and barto eliminates all stepsize parameter and improves data efficiency this paper extends bradtke and barto s work in three significant way first it present a simpler derivation of the lstd algorithm second it generalizes from to arbitrary value of at the extreme of the resulting algorithm is shown to be a practical formulation of supervised linear regression third it present a novel intuitive interpretation of lstd a a model based reinforcement learning technique 
we present a new view of image segmentation by pairwise similarity we interpret the similarity a edge ows in a markovrandom walk and study the eigenvalue and eigenvectors of thewalk s transition matrix this interpretation show that spectralmethods for clustering and segmentation have a probabilistic foundation in particular we prove that the normalized cut methodarises naturally from our framework finally the framework providesa principled method for learning the 
the concept of averaging over classiers is fundamental to thebayesian analysis of learning based on this viewpoint it ha recentlybeen demonstrated for linear classiers that the centre ofmass of version space the set of all classiers consistent with thetraining set also known a the bayes point exhibit excellentgeneralisation ability however the billiard algorithm a presentedin is restricted to small sample size because it requireso m of memory and o n 
kernel based learning algorithm allow the mapping of data set into an infinite dimensional feature space in which a classification may be perform ed a such kernel method represent a powerful approach to the solution of many non linear problem however kernel method do suffer from one unfortunate drawback the gram matrix contains row and column where is the number of data point many operation are precluded e g matrix inverse when data set containing more than about point are encountered one approach to resolving these issue is to look for s parse representation of the data set a sparse representation contains a reduc ed number of example loosely speaking we are interested in extracting the maximum amount of information from the minimum number of data point to achieve this in a principled manner we are interested in estimating the amount of information each data point contains in the framework presented here we make use of the bayesian methodology to determine how much information is gained from each data point 
we investigate a new kernel based classifier the kernel fisher discriminant kfd a mathematical programming formulation based on the observation that kfd maximizes the average marginpermits an interesting modification of the original kfd algorithm yielding the sparse kfd we find that both kfd and the proposed sparse kfd can be understood in an unifying probabilistic context furthermore we show connection to support vector machine and relevance vector machine from this understanding we are able to outline an interesting kernel regression technique based upon the kfd algorithm simulation support the usefulness of our approach 
we consider the problem of measuring the eigenvalue of a randomlydrawn sample of point we show that these value can bereliably estimated a can the sum of the tail of eigenvalue furthermore the residual when data is projected into a subspace isshown to be reliably estimated on a random sample experimentsare presented that conrm the theoretical result 
the blind source separation problem is concerned with extractionof the underlying source signal from a set of theirlinear mixture where the mixing matrix is unknown it wasdiscovered recently that exploiting the sparsity of sourcesin an appropriate representation according to some signaldictionary dramatically improves the quality of separation in this work we use the property of multiscale transforms such a wavelet or wavelet packet to decompose signalsinto set of local 
reinforcement learning rl problem withhidden state present significant obstaclesto prevailing rl method in this paper we present experiment conducted usinga straightforward approach to solving suchproblems that train artificial neural networkswith recurrent connection to representaction policy using evolutionary search 
we introduce a problem class which we term activity monitoring such problem involve monitoring the behavior of alarge population of entity for interesting event requiringaction we present a framework within which each of theindividual problem ha a natural expression a well a amethodology for evaluating performance of activity monitoringtechniques we show that two superficially differenttasks news story monitoring and intrusion detection canbe expressed naturally within the 
we analyze the bit error probability of multiuser demodulator for directsequence binary phase shift keying d bpsk cdma channel with additive gaussian noise the problem of multiuser demodulation is cast into the finite temperature decoding problem and replica analysis is applied to evaluate the performance of the resulting mpm marginal posterior mode demodulator which include the optimal demodulator and the map demodulator a special case an approximate implementation of demodulator is proposed using analog valued hopfield model a a naive mean field approximation to the mpm demodulator and it performance is also evaluated by the replica analysis result of the performance evaluation show effectiveness of the optimal demodulator and the mean field demodulator compared with the conventional one especially in the case of small information bit rate and low noise level 
we present a new approach to bounding the true error rate of a continuous valued classifier based upon pac bayes bound the method first construct a distribution over classifier by determining how sensitive each parameter in the model is to noise the true error rate of the stochastic classifier found with the sensitivity analysis can then be tightly bounded using a pac bayes bound in this paper we demonstrate the method on artificial neural network with result of a order of magnitude improvement v the best deterministic neural net bound 
we introduce an expandable bayesian network ebn to handle the combination of diverse multiple homogeneous evidence set an ebn is an augmented bayesian network which instantiates it structure at runtime according to the structure of input we show an application of an ebn for a multi view d object description problem in computer vision the experiment show that the proposed method give reasonable performance even for an unlearned structure of input data 
the application of boosting procedure to decisiontree algorithm ha been shown to producevery accurate classifier these classifiersare in the form of a majority vote overa number of decision tree unfortunately these classifier are often large complex anddifficult to interpret this paper describes anew type of classification rule the alternatingdecision tree which is a generalization ofdecision tree voted decision tree and voteddecision stump at the same time 
most statistical and machine learning algorithm assume that the data is a random sample drawn from a stationary distribution unfortunately most of the large database available for mining today violate this assumption they were gathered over month or year and the underlying process generating them changed during this time sometimes radically although a number of algorithm have been proposed for learning time changing concept they generally do not scale well to very large database in this paper we propose an efficient algorithm for mining decision tree from continuously changing data stream based on the ultra fast vfdt decision tree learner this algorithm called cvfdt stay current while making the most of old data by growing an alternative subtree whenever an old one becomes questionable and replacing the old with the new when the new becomes more accurate cvfdt learns a model which is similar in accuracy to the one that would be learned by reapplying vfdt to a moving window of example every time a new example arrives but with o complexity per example a opposed to o w where w is the size of the window experiment on a set of large time changing data stream demonstrate the utility of this approach 
we develop a hierarchical generative model to study cue combination the model map a global shape parameter to local cuespecific parameter which in turn generate an intensity image inferring shape from image is achieved by inverting this model inference produce a probability distribution at each level using distribution rather than a single value of underlying variable at each stage preserve information about the validity of each local cue for the given image this allows the model unlike standard combination model to adaptively weight each cue based on general cue reliability and specific image context we describe the result of a cue combination psychophysics experiment we conducted that allows a direct comparison with the model the model provides a good fit to our data and a natural account for some interesting aspect of cue combination understanding cue combination is a fundamental step in developing computational model of visual perception because many aspect of perception naturally involve multiple cue such a binocular stereo motion texture and shading it is often formulated a a problem of inferring or estimating some relevant parameter e g depth shape position by combining estimate from individual cue an important finding of psychophysical study of cue combination is that cue vary in the degree to which they are used in different visual environment weight assigned to estimate derived from a particular cue seem to reflect it estimated reliability in the current scene and viewing condition for example motion and stereo are weighted approximately equally at near distance but motion is weighted more at far distance presumably due to distance limit on binocular disparity experiment have also found these weighting sensitive to image manipulation if a cue is weakened such a by adding noise then the uncontaminated cue is utilized more in making depth judgment a recent study ha shown that observer can adjust the weighting they assign to a cue based on it relative utility for a particular task from these and other experiment we can identify two type of information that determine relative cue weighting cue reliability it relative utility in the context of the task and general viewing condition and region informativeness cue information available locally in a given image 
we analyze gallager code by employing a simple mean field ap proximation that distorts the model geometry and preserve important interaction between site the method naturally recovers the prob ability propagation decoding algorithm a an extremization of a proper free energy we find a thermodynamic phase transition that coincides with information theoretical upper bound and explain the practical co de performance in term of the free energy landscape 
subproblem generation solution and recombination is a standard approach to combinatorial optimization problem in many setting identifying suitable subproblems is itself a significant component of the technique such subproblems are often identified using a heuristic rule here we show how to use machine learning to make this identification in particular we use a learned objective function to direct search in an appropriate space of subproblem decomposition we demonstrate the efficacy of our technique for problem decomposition on a particular wellknown combinatorial optimization problem graph coloring for geometric graph 
psychophysical and physiological evidence show that sound localizationof acoustic signal is strongly inuenced by their synchronywith visual signal this eect known a ventriloquism is at workwhen sound coming from the side of a tv set feel a if it werecoming from the mouth of the actor the ventriloquism eectsuggests that there is important information about sound locationencoded in the synchrony between the audio and video signal inspite of this evidence audiovisual 
this paper present a method by which a reinforcement learning agent can automatically discover certain type of subgoals online by creating useful new subgoals while learning the agent is able to accelerate learning on the current task and to transfer it expertise to other related task through the reuse of it ability to attain subgoals the agent discovers subgoals based on commonality across multiple path to a solution we cast the task of finding these commonality a a multiple instance learning problem and use the concept of diverse density to find solution we illustrate this approach using several gridworld task 
previous work in mixture model clusteringhas focused primarily on the issue of modelselection model scoring function includingpenalized likelihood and bayesian approximation can guide a search of the model parameterand structure space relatively littleresearch ha addressed the issue of howto move through this space local optimizationtechniques such a expectation maximization solve only part of the problem westill need to move between different local optimum the 
clustering algorithm have become increasingly important in handling and analyzing data considerable work ha been done in devising effective but increasingly specific clustering algorithm in contrast we have developed a generalized framework that accommodates diverse clustering algorithm in a systematic way this framework view clustering a a general process of iterative optimization that includes module for supervised learning and instance assignment the framework ha also suggested several novel clustering method in this paper we investigate experimentally the efficacy of these algorithm and test some hypothesis about the relation between such unsupervised technique and the supervised method embedded in them 
probabilistic graphical model in particular bayesian ne tworks are useful model for representing statistical pattern in propositional domain recent work develops effective technique for learning these model directly from data however these technique apply only to attribute value i e flat representation of the data probabilistic relational model prms allow u to represent much richer dependency structure involving multiple entity and the rela tions between them they allow the property of an entity to depend probabilistically on property of related entity prms represent a generic dependence which is then instantiated for specific circumstance i e for a particular se t of entity and relation between them friedman et al showed how to learn prms from relational data and presented technique for learning both parameter and probabilistic depe ndency structure for the attribute in a relational model he re we examine the benefit that class hierarchy can provide prms we show how the introduction of subclass allows u to use inheritance and specialization to refine our model we show how to learn prms with class hierarchy prmch in two setting in the first the class hierarchy is provided a part of the input in the relational schema for the domain in the second setting in addition to learning the prm we must learn the class hierarchy finally we discus how prm chs allow u to build model that can represent model for both particular instance in our domain and class of object in our domain bridging the gap between a class based model and an attribute value based model 
vector based information retrieval method such a the vector space model vsm latent semantic indexing lsi and the generalized vector space model gvsm represent both query and document by high dimensional vector learned from analyzing a training corpus of text vsm scale well to large collection but cannot represent term term correlation which prevents it from being used in translingual retrieval gvsm and lsi can represent term term correlation but do not scale well to very large retrieval collection we present a novel method we call approximate dimension equalization ade that combine idea from vsm lsi and gvsm to produce a method that performs well on large collection scale well computationally and can represent term term correlation we compare the performance of ade to the other method on both large and small collection of both monolingual and bilingual text ade outperforms all other method on large bilingual collection and performs close to the best in all other case 
although bayesian model averaging is theoretically the optimal method for combining learned model it ha seen very little use in machine learning in this paper we study it application to combining rule set and compare it with bagging and partitioning two popular but more ad hoc alternative our experiment show that surprisingly bayesian model averaging s error rate are consistently higher than the other method further investigation show this to be due to a marked tendency to overt on the part of bayesian model averaging contradicting previous belief that it solves or avoids the overtting problem 
we describe a joint probabilistic model for modeling the content and inter connectivity of document collection such a set of web page or research paper archive the model is based on a probabilistic factor decomposition and allows identifying principal topic of the collection a well a authoritative document within those topic furthermore the relationship between topic is mapped out in order to build a predictive model of link content among the many application of this approach are information retrieval and search topic identification query disambiguation focused web crawling web authoring and bibliometric analysis 
skewed distribution appear very often in practice unfortunately the traditional zipf distribution often fails to model them well in this paper we propose a new probability distribution the discrete gaussian exponential dgx to achieve excellent fit in a wide variety of setting our new distribution includes the zipf distribution a a special case we present a statistically sound method for estimating the dgx parameter based on maximum likelihood estimation mle we applied dgx to a wide variety of real world data set such a sale data from a large retailer chain u age data from at t and internet clickstream data in all case dgx fit these distribution very well with almost a correlation coefficient in quantile quantile plot our algorithm also scale very well because it requires only a single pas over the data finally we illustrate the power of dgx a a new tool for data mining task such a outlier detection 
the three dimensional motion of human is underdetermined when the observation is limited to a single camera due to the inherent d ambiguity of d video we present a system that reconstructs the d motion of human subject from single camera video relying on prior knowledge about human motion learned from training data to resolve those ambiguity after initialization in d the tracking and d reconstruction is automatic we show result for several video sequence the result show the power of treating d body tracking a an inference problem 
over the last year particle filter have been applied with great success to a variety of state estimation problem we present a statistical approach to increasing the efficiency of particle filter by adapting the size of sample set on the fly the key idea of the kld sampling method is to bound the approximation error introduced by the sample based representation of the particle filter the name kld sampling is due to the fact that we measure the approximation error by the kullback leibler 
cognitive modeling with neural network unrealistically ignores the role of knowledge in learning by starting from random weight it is likely that effective use of knowledge by neural network could significantly speed learning a new algorithm knowledge based cascadecorrelation kbcc find and adapts it relevant knowledge in new learning comparison to multi task learning mtl reveals that kbcc us it knowledge more effectively to learn faster 
a major challenge in producing large scalesimulations of the type used in ecosystemmodeling is the problem of model calibration this paper present a method for solvinga particularly dicult model calibrationtask that arose a part of a global climatechange research project an obvious approachto solving calibration problem is toformulate them a global optimization problemsin which the goal is to nd value forthe free parameter that minimize the errorof the model on 
we give result about the learnability and required complexity of logical formula to solve classification problem these result are obtained by linking propositional logic with kernel machine in particular we show that decision tree and disjunctive normal form dnf can be represented by the help of a special kernel linking regularized risk to separation margin subsequently we derive a number of lower bound on the required complexity of logic formula using property of algorithm for generation of linear estimator such a perceptron and maximal perceptron learning 
it is known that decision tree learning can be viewed a a formof boosting however existing boosting theorem for decision treelearning allow only binary branching tree and the generalization tomulti branching tree is not immediate practical decision tree algorithm such a cart and c implement a trade off betweenthe number of branch and the improvement in tree quality asmeasured by an index function here we give a boosting justificationfor a particular quantitative 
it is known that decision tree learning can be viewed a a formof boosting however existing boosting theorem for decision treelearning allow only binary branching tree and the generalization tomulti branching tree is not immediate practical decision tree algorithm such a cart and c implement a trade off betweenthe number of branch and the improvement in tree quality asmeasured by an index function here we give a boosting justificationfor a particular quantitative 
invariance to topographic transformation such a translation andshearing in an image ha been successfully incorporated into feedforwardmechanisms e g convolutional neural network quot tangentpropagation quot we describe a way to add transformation invarianceto a generative density model by approximating the nonlineartransformation manifold by a discrete set of transformation anem algorithm for the original model can be extended to the newmodel by computing expectation over 
modern classification application necessitate supplementing the fewavailable labeled example with unlabeled example to improve classificationperformance we present a new tractable algorithm for exploitingunlabeled example in discriminative classification this is achievedessentially by expanding the input vector into longer feature vector viaboth labeled and unlabeled example the resulting classification methodcan be interpreted a a discriminative kernel density estimate 
we present a sequential monte carlo method applied to additive noise compensation for robust speech recognition in time varying noise the method generates a set of sample according to the prior distribution given by clean speech model and noise prior evolved from previous estimation an explicit model representing noise effect on speech feature is used so that an extended kalman fllter is constructed for each sample generating the updated continuous state estimate a the estimation of the noise parameter and prediction likelihood for weighting each sample minimum mean square error mmse inference of the time varying noise parameter is carried out over these sample by fusion the estimation of sample according to their weight a residual resampling selection step and a metropolis hastings smoothing step are used to improve calculation e ciency experiment were conducted on speech recognition in simulated non stationary noise where noise power changed artiflcially and highly non stationary machinegun noise in all the experiment carried out we observed that the method can have signiflcant recognition performance improvement over that achieved by noise compensation with stationary noise assumption 
the paradigm of hebbian learning ha recently received a novel interpretation with the discovery of synaptic plasticity that depends on the relative timing of pre and post synaptic spike this paper derives a temporally dependent learning rule from the basic principle of mutual information maximization and study it relation to the experimentally observed plasticity we nd that a supervised spike dependent learning rule sharing similar structure with the experimentally observed plasticity increase mutual information to a stable near optimal level moreover the analysis reveals how the temporal structure of time dependent learning rule is determined by the temporal lter applied by neuron over their input these result suggest experimental prediction a to the dependency of the learning rule on neuronal biophysical parameter 
clustering is a widely used knowledge discovery technique it help uncovering structure in data that were not previouslyknown the clustering of large data set ha receiveda lot of attention in recent year however clustering is astill a challenging task since many published algorithm failto do well in scaling with the size of the data set and thenumber of dimension that describe the point or in ndingarbitrary shape of cluster or dealing e ectively with thepresence of noise 
an important issue in visualizing categorical data is how to order categorical value the focus of this paper is on constructing such ordering efficiently without compromising their visual quality 
the problem constructing weak classifier for boosting algorithm is studied we presentan algorithm that produce a linear classifier that is guaranteed to achieve an error betterthan random guessing for any distribution on the data while this weak learneris not useful for learning in general we show that under reasonable condition on thedistribution it yield an effective weak learner for one dimensional problem preliminarysimulations suggest that the same behavior can be 
in this paper we propose a unified non quadratic loss function for regression known a soft insensitive loss function silf silf is a flexible model and posse most of the desirable characteristic of popular non quadratic loss function such a laplacian huber s and vapnik s insensitive loss function we describe the property of silf and illustrate our assumption on the underlying noise model in detail moreover the introduction of silf in regression make it possible to apply bayesian technique on support vector method experimental result on simulated and real world datasets indicate the feasibility of the approach 
the cluster variation method is a class of approximation method containing the bethe and kikuchi approximation a special case we derive two novel iteration scheme for the cluster variation method one is a fixed point iteration scheme which give a significant improvement over loopy bp mean field and tap method on directed graphical model the other is a gradient based method that is guaranteed to converge and is shown to give useful result on random graph with mild frustration we 
a bayes network based classifier for distinguishing terrestrial rock from meteorite is implemented onboard the nomad robot equipped with a camera spectrometer and eddy current sensor this robot searched the ice sheet of antarctica and autonomously made the first robotic identification of a meteorite in january at the elephant moraine this paper discus rock classification from a robotic platform and describes the system onboard nomad 
experimental data have shown that synapsis are heterogeneous different synapsis respond with different sequence of amplitude of postsynaptic response to the same spike train neither the role of synapt ic dynamic itself nor the role of the heterogeneity of synaptic dynamic s for computation in neural circuit is well understood we present in this article method that make it feasible to compute for a given synapse with known synaptic parameter the spike train that is optimally fitted to the synapse for example in the sense that it produce the largest sum of postsynaptic response to our surprise we find that most of these optim ally fitted spike train match common firing pattern of specific type of neuron that are discussed in the literature 
the adaptive tap gibbs free energy for a general densely connected probabilistic model with quadratic interaction and arbritary single site constraint is derived we show how a specific sequential minimization of the free energy lead to a generalization of minka s expectation propagation lastly we derive a sparse representation version of the sequential algorithm the usefulness of the approach is demonstrated on classification and density estimation with gaussian process and on an independent component analysis problem 
the gibbs classifier is a simple approximationto the bayesian optimal classifier inwhich one sample from the posterior for theparameter and then classifies using thesingle classifier indexed by that parametervector in this paper we study the votinggibbs classifier which is the extension of thisscheme to the full monte carlo setting inwhich n sample are drawn from the posteriorand new input are classified by votingthe n resulting classifier we show that theerror 
for building implementable and industryvaluable classification solution machine learning method must focus not only on accuracy but also o n computational and space complexity we discus a multistage method namely cascading where there is a sequence of classifier ordered in term of increasing complexity and specificity such that early classifier are simple and g eneral whereas later one are more c omplex and specific being localized on pattern rejected by the previous classifier we present t he technique and it rationale and validate it use by comparing it with the individual classifier a well a the widely accepted ensemble method bagging and adaboost on eight data set from t he uci repository we do see that cascading increase accuracy without t he c oncomitant i ncrease in complexity and cost 
we study resource limited online learning motivated by the problem of conditional branch outcome prediction in computer architecture in particular we consider parallel time and space efficient ensemble learner for online setting empirically demonstrating benefit similar to those shown previously for offline ensemble our learning algorithm are inspired by the previously published boosting by filtering framework a well a the offline arc x boosting style algorithm we train ensemble of online decision tree using a novel variant of the id online decision tree algorithm a the base learner and show empirical result for both boosting and bagging style online ensemble method our result evaluate these method on both our branch prediction domain and online variant of three familiar machine learning benchmark our data justifies three key claim first we show empirically that our extension to id significantly improve performance for single tree and additionally are critical to achieving performance gain in tree ensemble second our result indicate significant improvement in predictive accuracy with ensemble size for the boosting style algorithm the bagging algorithm we tried showed poor performance relative to the boosting style algorithm but still improve upon individual base learner third we show that ensemble of small tree are often able to outperform large single tree with the same number of node and similarly outperform smaller ensemble of larger tree that use the same total number of node this make online boosting particularly useful in domain such a branch prediction with tight space restriction i e the available realestate on a microprocessor chip 
estimating the parameter of sparse multinomial distribution isan important component of many statistical learning task recentapproaches have used uncertainty over the vocabulary of symbolsin a multinomial distribution a a mean of accounting for sparsity 
locally linear embedding lle is an elegant nonlineardimensionality reduction technique recently introduced by roweisand saul it fails when the data is divided into separate group 
interactive visualization are effective tool in mining scientific engineering and business data to support decision making activity star coordinate is proposed a a new multi dimensional visualization technique which support various interaction to stimulate visual thinking in early stage of knowledge discovery process in star coordinate coordinate ax are arranged on a two dimensional surface where each axis share the same origin point each multi dimensional data element is represented by a point where each attribute of the data contributes to it location through uniform encoding interaction feature of star coordinate provide user the ability to apply various transformation dynamically integrate and separate dimension analyze correlation of multiple dimension view cluster trend and outlier in the distribution of data and query point based on data range our experience with star coordinate show that it is particularly useful for the discovery of hierarchical cluster and analysis of multiple factor providing insight in various real datasets including telecommunication churn 
the strong correlation between the frequency of word and their naming latency ha been well documented however a early a the age of acquisition aoa of a word wa alleged to be the actual variable of interest but these study seem to have been ignored in most of the literature recently there ha been a resurgence of interest in aoa while some study have shown that frequency ha no effect when aoa is controlled for more recent study have found independent contribution of frequency and aoa connectionist model have repeatedly shown strong effect of frequency but little attention ha been paid to whether they can also show aoa effect indeed several researcher have explicitly claimed that they cannot show aoa effect in this work we explore these claim using a simple feed forward neural network we find a significant contribution of aoa to naming latency a well a condition under which frequency provides an independent contribution 
the voting technique which combine the prediction of several classifier can improve the generalization performance significantly by increasing the fraction of training example with large margin romma the relaxed online maximum margin algorithm is a perceptron like online learning algorithm to approximate the optimal margin classifier and aggressive romma update it prediction vector whenever the output produced by the current prediction vector doe not exceed the wanted threshold 
in this paper we present an average case analysisof the naive bayesian classifier a simpleinduction algorithm that performs wellin many domain our analysis assumes amonotone m of n target concept and trainingdata that consists of independent booleanattributes the analysis supposes a knowntarget concept and distribution of instance but includes parameter for the number oftraining case the number of irrelevant relevant and necessary attribute the probabilityof 
visual search is the task of finding a target in an image against abackground of distractors unique feature of target enable themto pop out against the background while target defined by lack offeatures or conjunction of feature are more difficult to spot it isknown that the ease of target detection can change when the rolesof figure and ground are switched the mechanism underlyingthe ease of pop out and asymmetry in visual search have beenelusive this paper show that a 
this paper describes an approach to feature subset selection that take into account problem specific and learning algorithm characteristic it is developed for the naive bayesian classifier applied on text data since it combine well with the addressed learning problem we focus on domain with many feature that also have a highly unbalanced class distribution and asymmetric misclassification cost given only implicitly in the problem by asymmetric misclassification cost we mean that one of the class value is the target class value for which we want to get prediction and we prefer false positive over false negative our example problem is automatic document categorization using machine learning where we want to identify document relevant for the selected category usually only about of example belong to the selected category our experimental comparison of eleven feature scoring measure show that considering domain and algorithm characteristic significantly improves the result of classification 
abstract we present a class of approximate inference algorithm for graphical model of the qmr dt type we give convergence rate for these algorithm and for the jaakkola and jordan algorithm and verify these theoretical prediction empirically we also presen t empirical result on the difficult qmr dt network problem obtaining per formance of the new algorithm roughly comparable to the jaakkola and jordan algorithm 
incomplete data set have become almost ubiquitous in a wide variety of application domain common example can be found in climate and image data set sensor data set and medical data set the incompleteness in these data set may arise from a number of factor in some case it may simply be a reflection of certain measurement not being available at the time in others the information may be lost due to partial system failure or it may simply be a result of user being unwilling to specify attribute due to privacy concern when a significant fraction of the entry are missing in all of the attribute it becomes very difficult to perform any kind of reasonable extrapolation on the original data for such case we introduce the novel idea of conceptual reconstruction in which we create effective conceptual representation on which the data mining algorithm can be directly applied the attraction behind the idea of conceptual reconstruction is to use the correlation structure of the data in order to express it in term of concept rather the original dimension a a result the reconstruction procedure estimate only those conceptual aspect of the data which can be mined from the incomplete data set rather than force error created by extrapolation we demonstrate the effectiveness of the approach on a variety of real data set 
consider the task of exploring the web in order to find page of a particular kind or on a particular topic this task arises in the construction of search engine and web knowledge base the paper argues that the creation of efficient web spider is best framed and solved by reinforcement learning a branch of machine learning that concern itself with optimal sequential decision making one strength of reinforcement learning is that it provides a formalism for measuring the utility of action that give benefit only in the future we present an algorithm for learning a value function that map hyperlink to future discounted reward using a naive bayes text classifier experiment on two real world spidering task show a three fold improvement in spidering efficiency over traditional breadth first search and up to a two fold improvement over reinforcement learning with immediate reward only 
we present efficient algorithm for all point pair problem or nbody like problem which are ubiquitous in statistical learning wefocus on six example including nearest neighbor classification kerneldensity estimation outlier detection and the two point correlation 
efficient learning of dfa is a challengingresearch problem in grammatical inference both exact and approximate in the pacsense identifiability of dfa from examplesis known to be hard pitt in his seminalpaper posed the following open researchproblem quot are dfa pac identifiable if examplesare drawn from the uniform distribution or some other known simple distribution quot pitt we demonstrate that theclass of simple dfa i e dfa whose canonicalrepresentations have 
this study compare five well known association rule algorithm using three real world datasets and an artificial dataset the experimental result confirm the performance improvement previously claimed by the author on the artificial data but some of these gain do not carry over to the real datasets indicating overfitting of the algorithm to the ibm artificial dataset more importantly we found that the choice of algorithm only matter at support level that generate more rule than would be useful in practice for support level that generate le than rule which is much more than human can handle and is sufficient for prediction purpose where data is loaded into ram apriori finish processing in le than minute on our datasets we observed super exponential growth in the number of rule on one of our datasets a change in the support increased the number of rule from le than a million to over a billion implying that outside a very narrow range of support value the choice of algorithm is irrelevant 
clustering is traditionally viewed a an unsupervisedmethod for data analysis however in some case information about theproblem domain is available in addition tothe data instance themselves in this paper we demonstrate how the popular k meansclustering algorithm can be protably modi ed to make use of this information in experimentswith articial constraint on sixdata set we observe improvement in clusteringaccuracy we also apply this methodto the real world 
value function approach for markov decision process have been used successfully to find optimal policy for a large number of problem recent findi ng have demonstrated that policy search can be used effectively in reinforcement learning wh en standard value function technique become overwhelmed by the size and dimensionality of the state space we demonstrate that substantial benefit can be achieved by combining the tw o approach we use an approximate value function solution in a low dimensional space to seed policy search in a continuous high dimensional space we demonstrate our approach a a motion planner on a mobile robot we show that this combination can in practice find good policy more efficiently t han policy search alone and is capable of solving more complex problem than value function alone 
data mining started it move out of the statistic and machine learning ghetto and into the mainstream almost year ago with great fanfare and a large influx of venture capital data mining wa going to change the very nature of business yet data mining product have had relatively modest success in the marketplace the reason include limitation and misplaced emphasis in the product feature and function unrealistic expectation set by message from the data mining community and a lack of readiness by many prospective user this session will look at where vendor have succeeded and failed with their product what expectation user should have and suggestion for achieving the potential of this exciting and valuable technology 
a challenging unsolved problem in the speech recognition community is recognizing speech signal that are corrupted by loud highly nonstationary noise one approach to noisy speech recognition is to automatically remove the noise from the cepstrum sequence before feeding it in to a clean speech recognizer in previous work published in eurospeech we showed how a probability model trained on clean speech and a separate probability model trained on noise could be combined for the purpose of 
we propose a data mining model that capture the user navigation behaviour pattern the user navigation session are modelled a a hypertext probabilistic grammar whose higher probability string correspond to the user s preferred trail an algorithm to efficiently mine such trail is given we make use of the n gram model which assumes that the last n page browsed affect the probability of the next page to be visited the model is based on the theory of probabilistic grammar providing it with a sound theoretical foundation for future enhancement moreover we propose the use of entropy a an estimator of the grammar s statistical property extensive experiment were conducted and the result show that the algorithm run in linear time the grammar s entropy is a good estimator of the number of mined trail and the real data rule confirm the effectiveness of the model 
abstract 
the problem of reinforcement learning in large factored markov decision process is explored the q value of a state action pair is approximated by the free energy of a product of expert network network parameter are learned on line using a modified sarsa algorithm which minimizes the inconsistency of the q value of consecutive state action pair action are chosen based on the current value estimate by fixing the current state and sampling action from the network using gibbs sampling the algorithm is tested on a co operative multi agent task the product of expert model is found to perform comparably to table based q learning for small instance of the task and continues to perform well when the problem becomes too large for a table based representation 
we present a probabilistic latent variable framework for data visualisation a key feature of which is it applicability to binary andcategorical data type for which few established method exist avariational approximation to the likelihood is exploited to derive afast algorithm for determining the model parameter illustrationsof application to real and synthetic binary data set are given introductionvisualisation is a powerful tool in the exploratory analysis of multivariate 
we give necessary and sufficient condition for uniqueness of thesupport vector solution for the problem of pattern recognition andregression estimation for a general class of cost function we showthat if the solution is not unique all support vector are necessarilyat bound and we give some simple example of non unique solution we note that uniqueness of the primal dual solution doesnot necessarily imply uniqueness of the dual primal solution weshow how to compute 
in this paper we propose that information maximization can providea unified framework for understanding saccadic eyemovements in this framework the mutual information among the corticalrepresentations of the retinal image the prior constructedfrom our long term visual experience and a dynamic short terminternal representation constructed from recent saccade providesa map for guiding eyenavigation by directing the eye to locationsof maximum complexity in neuronal ensemble 
a model of auditory grouping is described in which auditory attention play a key role the model is based upon an oscillatory correlation framework in which neural oscillator representing a single perceptual stream are synchronised and are desynchronised from oscillator representing other stream the model suggests a mechanism by which attention can be directed to the high or low tone in a repeating sequence of tone with alternating frequency in addition it simulates the perceptual segregation of a mistuned harmonic from a complex tone 
in nature animal encounter high dimensional sensory stimulus that have complex statistical and dynamical structure attempt to study the neural coding of these natural signal face challenge both in the selection of the signal ensemble and in the analysis of the resulting neural response for zebra finch naturalistic stimulus can be defined a sound that they encounter in a colony of conspecific bird we assembled an ensemble of these sound by recording group of zebra finch and then analyzed the response of single neuron in the songbird central auditory area field l to continuous playback of long segment from this ensemble following method developed in the fly visual system we measured the information that spike train provide about the acoustic stimulus without any assumption about which feature of the stimulus are relevant preliminary result indicate that large amount of information are carried by spike timing with roughly half of the information accessible only at time resolution better than m additional information is still being revealed a time resolution is improved to m information can be decomposed into that carried by the locking of individual spike to the stimulus or modulation of spike rate v that carried by timing in spike pattern initial result show that in field l temporal pattern give at least extra information thus single central auditory neuron can provide an informative representation of naturalistic sound in which spike timing may play a significant role 
abstract 
abstract a new approach to inference in belief network ha been recently proposed which is based on an algebraic representation of belief network using multi linear function according to this approach the key computational question is that of representing multi linear function compactly since inference reduces to a simple process of evaluating and dierentiating such function we show here that mainstream inference algorithm based on jointrees are a special case of this approach in a very precise sense we use this result to prove new property of jointree algorithm and then discus some of it practical and theoretical implication 
this paper develops a new approach for extremely fast detection in domain where the distribution of positive and negative example is highly skewed e g face detection or database retrieval in such domain a cascade of simple classifier each trained to achieve high detection rate and modest false positive rate can yield a final detector with many desirable feature including high detection rate very low false positive rate and fast performance achieving extremely high detection rate rather than low error is not a task typically addressed by machine learning algorithm we propose a new variant of adaboost a a mechanism for training the simple classifier used in the cascade experimental result in the domain of face detection show the training algorithm yield significant improvement in performance over conventional adaboost the final face detection system can process frame per second achieves over detection and a false positive rate of in a 
we explore combining reinforcement learning with a hand crafted local controller in a manner suggested by the chaotic control algorithm of vincent schmitt and vincent a closedloop controller is designed using conventional mean that creates a domain of attraction about a target state chaotic behavior is used or induced to bring the system into this region at which time the local controller is turned on to bring the system to the target state and stabilize it there we describe experiment in which we use reinforcement learning instead of and in addition to chaotic behavior to learn an efficient policy for driving the system into the local controller s domain of attraction using a simulated double pendulum we illustrate how this method allows reinforcement learning to be effective in a problem that cannot be easily solved by reinforcement learning alone and we show how reinforcement learning can improve upon the chaotic control algorithm when the domain of attraction can only be approximately determined similar result are shown using the h non map this is a simple and effective way of extending reinforcement learning to more difficult problem 
the reliability and accuracy of spike train have been shown to depend on the nature of the stimulus that the neuron encodes adding ion channel stochasticity to neuronal model result with a macroscopic behavior that replicates the input dependent reliability and precision of real neuron we calculate the amount of information that an ion channel based stochastic hodgkin huxley hh neuron model can encode about a wide set of stimulus we show that both the information rate and the information per spike of the stochastic model is similar to the value reported experimentally moreover the amount of information that the neuron encodes is correlated with the amplitude of fluctuation in the input and le so with the average firing r ate of the neuron we also show that for the hh ion channel density the information capacity is robust to change in the density of ion channel in the membrane whereas changing the ratio between the and ion channel ha a considerable effect on the information that the neuron can encode this suggests that neuron may maximize their information capacity by appropriately balancing the density of the different ion channel that underlies neuronal excitability 
we propose a general bayesian framework for performing independent component analysis ica which relies on ensemble learning and linear response theory known from statistical physic we apply it to both discrete and continuous source for the continuous source the underdetermined overcomplete case is studied the naive mean fie ld approach fails in this case whereas linear response theory which giv e an improved estimate of covariance is very efficient the example giv en are for source without temporal correlation however this deri vation can easily be extended to treat temporal correlation finally th e framework offer a simple way of generating new ica algorithm without needing to define the prior distribution of the source explicitly 
now that the complete genome of numerousorganisms have been determined a key problemin computational molecular biology is uncoveringthe relationship that exist amongthe gene in each organism and the regulatorymechanisms that control their operation 
we describe the g factor which relates probability distribution on image feature to distribution on the image themselves the g factor depends only on our choice of feature and lattice quantization and is independent of the training image data we illustrate the importance of the g factor by analyzing minimax entropy learning mel which learns image distribution in term of clique potential corresponding to feature statistic we rst use our analysis of the g factor to determine when the mel clique potential decouple for dieren t feature secondly we show that mel clique potential can be computed analytically by approximating the g factor we support our analysis by computer simulation 
genetic programming gp can learn complexconcepts by searching for the target conceptthrough evolution of population of candidatehypothesis program however unlikesome learning technique such a artificialneural network anns gp doe not havea principled procedure for changing part ofa learned structure based on that structure sperformance on the training data gp ismissing a clear locally optimal update procedure an equivalent of gradient descent backpropagation 
statistical approach to text mining can be enhanced and improved through the qualitative representation of free text ideally a representation which accommodates ambiguity and imprecision we introduce a specialized lexicon that assigns semantic category to word together with numeric value for centrality and intensity within each category from this lexicon we automatically generate an additional set of resource to implement some of the common operation of text mining profiling querying and query profile expansion and compression in qualitative domain we exploit the hierarchical structure of free text i e sentence paragraph document and develop a set of operator whose argument are fuzzy representation profile of text at any hierarchical level various operator compute the centrality and intensity of category within a profile a profile s overall intensity and the cardinality and fuzziness of a profile others are used in profile merging profile expansion or compression and discovery of related category from a profile we address the meaning and mode of deployment of these operator using practical example finally we discus the utility of fuzzy typing for various task such a qualitative browsing and similarity estimate we discus how the existing approach can be enhanced using automatic lexicon expansion and information extraction technique we offer a practical software demonstration with several visualization example illustrating the power of the proposed operator in affect analysis of news report and movie review 
we present monte carlo generalized em equation for learning in non linear state space model the dif culties lie in the monte carlo e step which consists of sampling from the posterior distribution of the hidden variable given the observation the new idea presented in this paper is to generate sample from a gaussian approximation to the true posterior from which it is easy to obtain independent sample the parameter of the gaussian approximation are either derived from the extended kalman lter or the fisher scoring algorithm in case the posterior density is mul timodal we propose to approximate the posterior by a sum of gaussians mixture of mode approach we show that sampling from the approxi mate posterior density obtained by the above algorithm lead to better model than using point estimate for the hidden state in our exper iment the fisher scoring algorithm obtained a better approximation of the posterior mode than the ekf for a multimodal distribution the mix ture of mode approach gave superior result 
in this paper we describe our research in computer aided image analysis we have incorporated machine learning methodology with traditional image processing to perform unsupervised image segmentation first we apply image processing technique to extract from an image a set of training case which are histogram peak described by their intensity range and spatial and textural attribute second we use learning by discovery methodology to cluster these case the first methodology we use is based on cobweb a conceptual clustering approach whose objective is to cluster the case incrementally a the concept hierarchy is refined the second methodology is based on an aggregated population equalization ape strategy this approach attempt to maintain similar strength for all population in it environment the clustering result of either approach tell u the number of visually significant class in the image and what these class are and thus enables u to perform unsupervised segmentation i e the labeling of all image pixel based on the result of the visual evaluation of the segmented image we have built an unsupervised segmentation software tool called asis and have applied it to a range of remotely sensed image such a sea ice and vegetation index in this paper we present our machine learning approach to unsupervised image segmentation and discus our experiment and their result 
the pagerank algorithm used in the google search engine greatly improves the result of web search by taking into account the link structure of the web pagerank assigns to a page a score propor tional to the number of time a random surfer would visit that page if it surfed indefinitely from page to page following all outlinks from a page with equal probability we propose to improve page rank by using a more intelligent surfer one that is guided by a probabilistic model of the relevance of a page to a query efficient execution of our algorithm at query time is made possible by pre computing at crawl time and thus once for all query the neces sary term experiment on two large subset of the web indicate that our algorithm significantly outperforms pagerank in the hu man rated quality of the page returned while remaining efficient enough to be used in today s large search engine 
we describe the task of user oriented anomaly detection for computer security in this domain the goal is to develop a model of a computer user s normal behavioral pattern and to detect anomalous condition a deviation from expected behavior we present an instance based learning ibl system for profiling user and examine some domain constraint with respect to our approach in particular we explore the data reduction problem this domain is subject to unbounded data and concept drift but is constrained by limited resource so we must limit the size of the learned model we empirically examine the data reduction performance of two clustering method an em procedure k center and a greedy clustering method developed to address domain characteristic we evaluate the relative strength of the two method along three performance ax accuracy mean time to generation of an alarm tta and data compression 
we explore the use of a connectionistlearningsystem designed to allow the applicationof reinforcement learning to robot control in particular we compare direct and indexedpartitioning method and find indexedpartitioning ha advantage in time and spacecomplexity learning speed measured in trial and success rate we make these comparisonsbased on extensive simulation andruns on a real robot learning on line 
we present a simple sparse greedy technique to approximate the maximum a posteriori estimate of gaussian process with much improved scaling behaviour in the sample size m in particular computational requirement are o n m storage is o nm the cost for prediction is o n and the cost to compute con dence bound is o nm where n m we show how to compute a stopping criterion give bound on the approximation error and show application to large scale problem introduction gaussian 
network routing is a distributed decision problem which naturally admits numerical performance measure such a the average time for a packet to travel from source to destination olpomdp a policy gradient reinforcement learning algorithm wa successfully applied to simulated network routing under a number of network model multiple distributed agent router learned cooperative behavior without explicit interagent communication and they avoided behavior which wa individually desirable 
we explore the statistical property of natural sound stimulus preprocessed with a bank of linear filter the response of such filter exhibit a striking form of statistical dependency in which the response variance of each filter grows with the response amplitude of filter tuned for nearby frequency these dependency may be substantially reduced using an operation known a divisive normalization in which the response of each filter is divided by a weighted sum of the rectified response of other filter the weight may be chosen to maximize the independence of the normalized response for an ensemble of natural sound we demonstrate that the resulting model account for non linearity in the response characteristic of the auditory nerve by comparing model simulation to electrophysiological recording in previous work nip we demonstrated that an analogous model derived from the statistic of natural image account for non linear property of neuron in primary visual cortex thus divisive normalization appears to be a generic mechanism for eliminating a type of statistical dependency that is prevalent in natural signal of different modality 
kernel principal component analysis pca is an elegant nonlineargeneralisation of the popular linear data analysis method where a kernel function implicitly denes a nonlinear transformationinto a feature space wherein standard pca is performed unfortunately the technique is not sparse since the componentsthus obtained are expressed in term of kernel associated with everytraining vector this paper show that by approximating thecovariance matrix in feature space by a reduced 
a key question in neuroscience is how to encode sensory stimulus such a image and sound motivated by study of response property of neuron in the early cortical area we propose an encoding scheme that dispenses with absolute measure of signal intensity or contrast and us instead only local ordinal measure in this scheme the structure of a signal is represented by a set of equality and inequality across adjacent region in this paper we focus on characterizing the fidelity of this representation strategy we develop a regularization approach for image reconstruction from ordinal measure and thereby demonstrate that the ordinal representation scheme can faithfully encode signal structure we also present a neurally plausible implementation of this computation that us only local update rule the result highlight the robustness and generalization ability of local ordinal encoding for the task of pattern classification in this paper we introduce and characterize a biologically plausible representation scheme for encoding signal structure the scheme employ a simple vocabulary of local ordinal relation of the kind that early sensory neuron are capable of extracting our result so far suggest that this scheme posse several desirable characteristic including tolerance to object appearance variation computational simplicity and low memory requirement we develop and demonstrate our idea in the visual domain but they are intended to be applicable to other sensory modality a well the starting point for our proposal lie in study of the response property of neuron in the early sensory cortical area these response property constrain 
this paper describes pairwise bisection a nonparametric approach to optimizing anoisy function with few function evaluation the algorithm us nonparametric reasoningabout simple geometric relationship to findminima efficiently two factor often frustrateoptimization noise and cost outputcan contain significant quantity of noise orerror while time or money allows for onlya handful of experiment pairwise bisectionis used here to attempt to automate the processof 
this paper investigates how the splitting criterion and pruning method of decision tree learning algorithm are influenced by misclassification cost or change to the class distribution splitting criterion that are relatively insensitive to cost class distribution are found to perform a well a or better than in term of expected misclassification cost splitting criterion that are cost sensitive consequently there are two opposite way of dealing with imbalance one is to combine a costinsensitive splitting criterion with a cost insensitive pruning method to produce a decision tree algorithm little affected by cost or prior class distribution the other is to grow a cost independent tree which is then pruned in a cost sensitive manner 
an important issue in neural computing concern the description oflearning dynamic with macroscopic dynamical variable recentprogress on on line learning only address the often unrealisticcase of an innite training set we introduce a new framework tomodel batch learning of restricted set of example widely applicableto any learning cost function and fully taking into account thetemporal correlation introduced by the recycling of the example for illustration we analyze 
the bayesian framework is ideally suited for induction problem the probability of observing x t at time t given past observation x x t can be computed with bayes rule if the true distribution mu of the sequence x x x is known the problem however is that in many case one doe not even have a reasonable estimate of the true distribution in order to overcome this problem a universal distribution xi is defined a a weighted sum of distribution mu i in m where m is any countable set of distribution including mu this is a generalization of solomonoff induction in which m is the set of all enumerable semi measure system which predict y t given x x t and which receive loss l x t y t if x t is the true next symbol of the sequence are considered it is proven that using the universal xi a a prior is nearly a good a using the unknown true distribution mu furthermore game of chance defined a a sequence of bet observation and reward are studied the time needed to reach the winning zone is bounded in term of the relative entropy of mu and xi extension to arbitrary alphabet partial and delayed prediction and more active system are discussed 
i consider the problem of learning concept from small number of po itive example a feat which human perform routinely but which com puters are rarely capable of bridging machine learning and cognitive science perspective i present both theoretical analysis and an empirical study with human subject for the simple task of learning concept corre sponding to axis aligned rectangle in a multidimensional feature space existing learning model when applied to this task cannot explain how subject generalize from only a few example of the concept i propose a principled bayesian model based on the assumption that the example are a random sample from the concept to be learned the model give precise fit to human behavior on this simple task and provides qualitative insight into more complex realistic case of concept learning 
we present a hidden markov model hmm for inferring the hiddenpsychological state or neural activity during single trail fmri activationexperiments with blocked task paradigm inference is based onbayesian methodology using a combination of analytical and a varietyof markov chain monte carlo mcmc sampling technique the advantageof this method is that detection of short time learning effect betweenrepeated trail is possible since inference is based only on singletrail 
we present a hidden markov model hmm for inferring the hidden psychological state or neural activity during single trial fmri activation experiment with blocked task paradigm inference is based on bayesian methodology using a combination of analytical and a variety of markov chain monte carlo mcmc sampling technique the advantage of this method is that detection of short time learning effect between repeated trial is possible since inference is based only on single trial experiment 
we present a novel method for clustering using the support vector machine approach data point are mapped to a high dimensional feature space where support vector are used to define a sphere enclosing them the boundary of the sphere form in data space a set of closed contour containing the data a the kernel parameter is varied these contour fit the data more tightly and splitting of contour occurs the contour are interpretedas cluster boundariesand the point within each disconnected contour are defined a a cluster cluster boundary can take on arbitrary geometricalshapesandclustersareseparatedbyvalleysintheunderlying probability distribution a in other sv algorithm outlier can be dealt with by introducing a soft margin constant leading to smoother cluster boundary the hierarchical structure of the data is explored by varying the two parameter we investigate the dependence of our method on these parameter and apply it to several data set 
using method of statistical physic we investigate the r amp ocirc le of model complexity in learning with support vector machine svms we show the advantage of using svms with kernel of infinite complexity on noisy target rule which in contrast to common theoretical belief are found to achieve optimal generalization error although the training error doe not converge to the generalization error moreover we find a universal asymptotics of the learning curve which only depend on the 
nonlinear support vector machine svms are investigated forvisual sex classification with low resolution quot thumbnail quot face by pixel processed from image from the feret facedatabase the performance of svms is shown to be superior totraditional pattern classifier linear quadratic fisher linear discriminant nearest neighbor a well a more modern techniquessuch a radial basis function rbf classifier and large ensemblerbfnetworks furthermore the svm 
guided by an initial idea of building a complex non linear decision surface with maximal local margin in input space we give a possible geometrical intuition a to why k nearest neighbor knn algorithm often perform more poorly than svms on classification task we then propose modified k nearest neighbor algorithm to overcome the perceived problem the approach is similar in spirit to tangent distance but with invariance inferred from the local neighborhood rath er than prior knowledge experimental result on real world classificati on task suggest that the modified knn algorithm often give a dramatic im provement over standard knn and perform a well or better than svms 
abstract we show how map learning can be formulated a inference in a graphical model which allows u to handle changing environment in a natural manner we describe several different approximation scheme for the problem and illustrate some result on a simulated grid wo rld with door that can open and close we close by briefly discussing how to l earn more general model of partially observed environment which can contain a variable number of object with changing internal state 
data noise is present in many machine learning problem domain some of these are well studied but others have received le attention in this paper we propose an algorithm for constructing a kernel fisher discriminant kfd from training example with noisy label the approach allows to associate with each example a probability of the label being flipped we utilise an expectation maximization em algorithm for updating the probability the e step us class conditional probability estimated a a by product of the kfd algorithm the m step update the flip probability and determines the parameter of the discriminant we demonstrate the feasibility of the approach on two real world data set 
abstract we investigate a learning algorithm for the classification of nonnegative data by mixture model multiplicative update rule are derived that directly optimize the performance of these model a classifier the update rule have a simple closed form and an intuitive appeal our algorithm retains the main virtue of the expectation maximization em algorithm it guarantee of monotonic improvement and it absence of tuning parameter with the added advantage of optimizing a discriminative objective function the algorithm reduces a a special case to the method of generalized iterative scaling for log linear model the learning rate of the algorithm is controlled by the sparseness of the training data we use the method of nonnegative matrix factorization nmf to discover sparse distributed representation of the data this form of feature selection greatly accelerates learning and make the algorithm practical on large problem experiment show that discriminatively trained mixture model lead to much better classification than comparably sized model trained by em 
abstract this paper describes an approach to reinforcementlearning in multiagent general sumgames in which a learner is told to treat eachother agent a either a friend quot or foe quot thisq learning style algorithm provides strongconvergence guarantee compared to an existingnash equilibrium based learning rule 
we propose randomized technique for speeding up kernel principal component analysis on three level sampling and quantization of the gram matrix in training randomized rounding in evaluating the kernel expansion and random projection in evaluating the kernel itself in all three case we give sharp bound on the accuracy of the obtained approximation rather intriguingly all three technique can be viewed a instantiation of the following idea replace the kernel function by a randomized kernel which behaves like in expectation 
abstract this paper present a novel and fast k nn classifier that is based on a binary cmm correlation matrix memory neural network a robust encoding method is developed to meet cmm input requirement a hardware implementation of the cmm is described which give over time the speed of a current mid range workstation and is scaleable to very large problem when tested on several benchma rks and compared with a simple k nn method the cmm classifier gave le than lower accuracy and over and time speed up in softwa re and hardware respectively 
we introduce a novel algorithm termed ppa performance prediction algorithm that quantitatively measure the contributio n of element of a neural system to the task it performs the algorithm ide ntifies the neuron or area which participate in a cognitive or behavio ral task given data about performance decrease in a small set of lesion it also allows the accurate prediction of performance due to multi element lesion the effectiveness of the new algorithm is demonstrated in two model of recurrent neural network with complex interactionsamong the element the algorithm is scalable and applicable to the analysis of large neural network given the recent advance in reversible inactivation technique it ha the potential to significantly contribute to the unde rstanding of the organization of biological nervous system and to shed light on the long lasting debate about local versus distrib uted computation in the brain 
this paper address the question whatis the outcome of multi agent learningvia no regret algorithm in repeatedgames specically can the outcomeof no regret learning be characterizedby traditional game theoreticsolution concept such a nash equilibrium the conclusion of this studyis that no regret learning is reminiscentof ctitious play play convergesto nash equilibrium in dominancesolvable constant sum and generalsum game but cycle exponentiallyin the 
we propose a novel clustering method that is an extension of idea inherent to scale space clustering and support vector clusteri ng like the latter it associate every data point with a vector in hilbert s pace and like the former it put emphasis on their total sum that is equal t o the scalespace probability function the novelty of our approach is t he study of an operator in hilbert space represented by the schroding er equation of which the probability function is a solution this schrodi nger equation contains a potential function that can be derived analytica lly from the probability function we associate minimum of the potential with cluster center the method ha one variable parameter the scale of it gaussian kernel we demonstrate it applicability on known data set by limiting the evaluation of the schrodinger potential to the locatio n of data point we can apply this method to problem in high dimension 
we present a neural network model that show how the prefrontalcortex interacting with the basal ganglion can maintain a sequenceof phonological information in activation based working memory i e the phonological loop the primary function of this phonologicalloop may be to transiently encode arbitrary binding ofinformation necessary for task the combinatorial expressivepower of language enables very exible binding of essentially arbitrarypieces of information our model 
the concave convex procedure cccp is a way to construct discrete time iterative dynamical system which are guaranteed to monotonically decrease global optimization energy function this procedure can be applied to almost any optimization problem and many existing algorithm can be interpreted in term of 
effective method of capacity control via uniform convergence bound for function expansion have been largely limited to support vector machine where good bound are obtainable by the entropy number approach we extend these method to system with expansion in term of arbitrary parametrized basis function and a wide range of regularization method covering the whole range of general linear additive model this is achieved by a data dependent analysis of the eigenvalue of the corresponding design matrix 
abstract we show that it is possible to extend hidden markov model to havea countably infinite number of hidden state by using the theory ofdirichlet process we can implicitly integrate out the infinitely manytransition parameter leaving only three hyperparameters which can belearned from data these three hyperparameters define a hierarchicaldirichlet process capable of capturing a rich set of transition dynamic 
in a bayesian mixture model it is not necessary a priori to limit the number of component to be finite in this paper an infinite gaussian mixture model is presented which neatly sidestep the difficult problem of finding the right number of mixture component inference in the model is done using an efficient parameter free markov chain that relies entirely on gibbs sampling 
the performance of regular and irregular gallager type errorcorrectingcode is investigated via method of statistical physic the transmitted codeword comprises product of the original messagebits selected by two randomly constructed sparse matrix the number of non zero row column element in these matricesconstitutes a family of code we show that shannon s channelcapacity may be saturated in equilibrium for many of the regularcodes while slightly lower performance is 
an important class of problem can be cast a inference in noisyorbayesian network where the binary state of each variable isa logical or of noisy version of the state of the variable s parent for example in medical diagnosis the presence of a symptomcan be expressed a a noisy or of the disease that may cause thesymptom on some occasion a disease may fail to activate thesymptom inference in richly connected noisy or network is intractable but approximate method 
the ability to identify the mineral composition of rock and soil is an important tool for the exploration of geological site for instance nasa intends to design robot that are sufficiently autonomous to perform this task on planetary mission spectrometer reading provide one important source of data for identifying site with mineral of interest reflectance spectrometer measure intensity of light reflected from surface over a range of wavelength spectral intensity pattern may in some case be sufficiently distinctive for proper identification of mineral or class of mineral for some mineral class carbonate for example specific short spectral interval are known to carry a distinctive signature finding similar distinctive spectral range for other mineral class is not an easy problem we propose and evaluate data driven technique that automatically search for spectral range optimized for specific mineral in one set of study we partition the whole interval of wavelength available in our data into sub interval or bin and use a genetic algorithm to evaluate a candidate selection of subintervals a alternative to this computationally expensive search technique we present an entropy based heuristic that give higher score for wavelength more likely to distinguish between class a well a other greedy search procedure result are presented for four different class showing reasonable improvement in identifying some but not all of the mineral class tested 
in recent year bayesian network have become highly successful tool for diagnosis analysis and decision making in real world domain we present an efficient algorithm for learning bayes network from data our approach construct bayesian network by first identifying each node s markov blanket then connecting node in a maximally consistent way in contrast to the majority of work which typically us hill climbing approach that may produce dense and causally incorrect net our approach yield much more compact causal network by heeding independency in the data compact causal network facilitate fast inference and are also easier to understand we prove that under mild assumption our approach requires time polynomial in the size of the data and the number of node a randomized variant also presented here yield comparable result at much higher speed 
a novel approach for comparing sequence of observation using an explicit expansion kernel is demonstrated the kernel is derived using the assumption of the independence of the sequence of observation and a mean squared error training criterion the use of an explicit expansion kernel reduces classifier model size and computation dramatically resulting in model size and computation one hundred time smaller in our application the explicit expansion also preserve the computational advantage of an earlier architecture based on mean squared error training training using standard support vector machine methodology give accuracy that significantly exceeds the performance of state of the art mean squared error training for a speaker recognition task 
bayesian network are graphical representation of probability distribution in virtually all of the work on learning these network the assumption is that we are presented with a data set consisting of randomly generated instance from the underlying distribution in many situation however we also have the option of active learning where we have the possibility of guiding the sampling process by querying for certain type of sample this paper address the problem of estimating the parameter of bayesian network in an active learning setting we provide a theoretical framework for this problem and an algorithm that chooses which active learning query to generate based on the model learned so far we present experimental result showing that our active learning algorithm can significantly reduce the need for training data in many situation 
parameter tuning through cross validation becomes very difficult when the validation set contains no or only a few example of the class in the evaluation set we address this open challenge by using a combination of classifier with different performance characteristic to effectively reduce the performance variance on average of the overall system across all class including those not seen before this approach allows u to tune the combination system on available but le representative validation data and obtain smaller performance degradation of this system on the evaluation data than using a single method classifier alone we tested this approach by applying k nearest neighbor rocchio and language modeling classifier and their combination to the event tracking problem in the topic detection and tracking tdt domain where new class event are created constantly over time and representative validation set for new class are often difficult to obtain on time when parameter tuned on an early benchmark tdt corpus were evaluated on a later tdt benchmark corpus with no overlapping event we observed a reduction in tracking cost a weighted combination of error by the combined system over the individual method evaluated under the same condition strongly suggesting the robustness of this approach a a solution for improving cross class performance consistency of statistical classifier when standard cross validation fails due to the lack of representative validation set 
a system emulating the functionality of a moving eye hence the name oculo motor system ha been built and successfully tested it is made of an optical device for shifting the field of view of an image sensor by up to in any direction four neuromorphic analog vlsi circuit implementing an oculo motor control loop and some off the shelf electronics the custom integrated circuit communicate with each other primarily by non arbitrated address event bus the system implement the behavior of saliency based saccadic exploration a ndsmooth pursuit of light spot the duration of saccade range from m to m which is comparable to human eye performance smooth pursuit operates on light source moving at up to s in the visual field 
facesync is an optimal linear algorithm that find the degree of synchronization between the audio and image recording of a human speaker using canonical correlation it find the best direction to combine all the audio and image data projecting them onto a single axis facesync us pearson s correlation to measure the degree of synchronization between the audio and image data we derive the optimal linear transform to combine the audio and visual information and describe an implementation that avoids the numerical problem caused by computing the correlation matrix motivation in many application we want to know about the synchronization between an audio signal and the corresponding image data in a teleconferencing system we might want to know which of the several people imaged by a camera is heard by the microphone then we can direct the camera to the speaker in post production for a film clean audio dialog is often dubbed over the video we want to adjust the audio signal so that the lip sync is perfect when analyzing a film we want to know when the person talking is in the shot instead of off camera when evaluating the quality of dubbed film we can measure of how well the translated word and audio fit the actor s face this paper describes an algorithm facesync that measure the degree of synchronization between the video image of a face and the associated audio signal we can do this task by synthesizing the talking face using technique such a video rewrite and then comparing the synthesized video with the test video that process however is expensive our solution find a linear operator that when applied to the audio and video signal generates an audio video synchronization error signal the linear operator gather information from throughout the image and thus allows u to do the computation inexpensively hershey and movellan describe an approach based on measuring the mutual information between the audio signal and individual pixel in the video the correlation between the audio signal x and one pixel in the image y is given by pearson s correlation r the mutual information between these two variable is given by i x y log r they create movie that show the region of the video that have high correlation with the audio 
recent work ha shown impressive transform invariant modeling and clustering for set of image of object with similar appearance we seek to expand these capability to set of image of an object class that show considerable variation across individual instance e g pedestrian image using a representation based on pixel wise similarity similarity template because of it invariance to the color of particular component of an object this representation enables detection of instance of an object class and enables alignment of those instance further this model implicitly represents the region of color regularity in the class specic image set enabling a decomposition of that object class into component region 
this paper describes an algorithm for generating compact d model of indoor environment with mobile robot our algorithm employ the expectation maximization algorithm to fit a lowcomplexity planar model to d data collected by range finder and a panoramic camera the complexity of the model is determined during model fitting by incrementally adding and removing surface in a final post processing step measurement are converted into polygon and projected onto the surface model where possible empirical result obtained with a mobile robot illustrate that high resolution model can be acquired in reasonable time 
the product of expert learning procedure can discover a set of stochastic binary feature that constitute a non linear generative model of handwritten image of digit the quality of generative model learned in this way can be assessed by learning a separate model for each class of digit and then comparing the unnormalized probability of test image under the different class specific model to improve discriminative performance each of the digit model can be given more layer of feature detector the layer are trained sequentially and each layer learns a generative model of the pattern of feature activity in the preceding layer after training each layer of feature dectectors produce a separate unnormalized log probabilty score with three layer of feature detector in each of the digit model a test image produce score which can be used a input to a supervised logistic classification network that is trained on separate data on the mnist database our system is comparable with current state of the art discriminative method demonstrating that the product of expert learning procedure can produce effective generative model of high dimensional data 
we report on the successful application of feature selection method to a classification problem in molecular biology involving only data point in a dimensional space our approach is a hybrid of filter and wrapper approach to feature selection we make use of a sequence of simple filter culminating in koller and sahami s markov blanket filter to decide on particular feature subset for each subset cardinality we compare between the resulting subset cardinality using cross validation the paper also investigates regularization method a an alternative to feature selection showing that feature selection method are preferable in this problem 
domain knowledge ha been shown to be an important component of machine learning however the cost of obtaining domain knowledge to improve classifier generation can exceed the cost of manually creating classifier an alternative approach is to use existing knowledge source to collect relevant domain knowledge and improve machine learning we investigated the use of two existing knowledge source a natural language processor and controlled vocabulary metathesaurus to improve machine learning algorithm performance in building classifier for medical text report both knowledge source were found to significantly improve classifier performance this demonstrates that existing knowledge source can easily be used to improve machine learning performance 
the active selection of instance can significantlyimprove the generalisation performanceof a learning machine large marginclassifiers such a support vector machinesclassify data using the most informative instance the support vector this makesthem natural candidate for instance selectionstrategies in this paper we propose analgorithm for the training of support vectormachines using instance selection wegive a theoretical justification for the strategyand 
we consider the problem of finding neighboring class set object of each instance of a neighboring class set are grouped using their euclidean distance from each other recently location based service are growing along with mobile computing infrastructure such a cellular phone and pda therefore we expect to see the development of spatial database that contains very large number of access record including location information the most typical type would be a database of point object record of the object may consist of requested service name number of packet transmitted in addition to x and y coordinate value indicating where the request came from the algorithm presented here efficiently find set of service name that were frequently close to each other in the spatial database for example it may find a frequent neighboring class set where ticket and timetable are frequently requested close to each other by recognizing this location based service provider can promote a ticket service for customer who access the timetable 
we present a system timemines that automatically generatestimelines from date tagged free text corpus timeminesdetects rank and group semantic feature based ontheir statistical property we use these feature to discoversets of related story that deal with a single topic 
adaboost and other ensemble method have successfully been appliedto a number of classification task seemingly defying problemsof overfitting adaboost performs gradient descent in an errorfunction with respect to the margin asymptotically concentratingon the pattern which are hardest to learn for very noisy problem however this can be disadvantageous indeed theoreticalanalysis ha shown that the margin distribution a opposed to justthe minimal margin play a crucial 
the concept of jumping emerging pattern jeps ha been proposed to describe those discriminating feature which only occur in the positive training instance but do not occur in the negative class at all jeps have been used to construct classifier which generally provide better accuracy than the state of the art classifier such a c the algorithm for maintaining the space of jumping emerging pattern jep space are presented in this paper we prove that jep space satisfy the property of convexity therefore jep space can be concisely represented by two bound consisting respectively of the most general element and the most specific element in response to insertion of new training instance a jep space is modified by operating on it boundary element and the boundary element of the jep space associated with the new instance this strategy completely avoids the need to go back to the most initial step to build the new jep space in addition our maintenance algorithm can well handle such other case a deletion of instance insertion of new attribute and deletion of attribute 
recent work ha exploited boundedness of data in the unsupervised learning of new type of generative model for nonnegative data it wa recently shown that the maximum entropy generative model is a nonnegative boltzmann distributionnot a gaussian distribution when the model is constrained to match the first and second order stati stics of the data learning for practical sized problem is made difficul t by the need to compute expectation under the model distribution the computational cost of markov chain monte carlo method and low fideli ty of naive mean field technique ha led to increasing interest in advanced mean field theory and variational method here i present a secondorder mean field approximation for the nonnegative boltzma nn machine model obtained using a high temperature expansion the theory is tested on learning a bimodal dimensional model a high dimensional translationally invariant distribution and a generative model for handwritten digit 
this paper look at the use of a self organizing map som to link of record of crime of serious sexual attack once linked a profile can be derived of the offender s responsible the data wa drawn from the major crime database at the national crime faculty of the national police staff college bramshill uk the data wa encoded from text by a small team of specialist working to a well defined protocol the encoded data wa analyzed using som two exercise were conducted these resulted in the linking of several offence in to cluster each of which were sufficiently similar to have possibly been committed by the same offender s a number of cluster were used to form profile of offender some of these profile were confirmed by independent analyst a either belonging to known offender or appeared sufficiently interesting to warrant further investigation the prototype wa developed over week this contrast with an in house study using a conventional approach which took year to reach similar result a a consequence of this study the ncf intends to pursue an in depth follow up study 
training a support vector machine svm requires the solution of a very large quadratic programming qp problem this paper proposes an al gorithm for training svms sequential minimal optimization or smo smo break the large qp problem into a series of smallest possible qp problem which are analytically solvable thus smo doe not require a numerical qp library smo s computation time is dominated by eval uation of the kernel hence kernel optimization substantially quicken smo for the mnist database smo is time a fast a pcg chunk ing while for the uci adult database and linear svms smo can be time faster than the pcg chunking algorithm 
fraud cause substantial loss to telecommunication carrier detec tion system which automatically detect illegal use of the network can be used to alleviate the problem previous approach worked on feature derived from the call pattern of individual user in this paper we present a call based detection system based on a hierarchical regime switching model the detection problem is formulated a an inference problem on the regime probability inference is implemented by applying the junc tion tree algorithm to the underlying graphical model the dynamic are learned from data using the em algorithm and subsequent discriminative training the method are assessed using fraud data from a real mobile communication network 
although feature selection is a central problemin inductive learning a suggested by thegrowing amount of research in this area mostof the work ha been carried out under the supervisedlearning paradigm paying little attentionto unsupervised learning task and particularly clustering task in this paper we analyze the particular benefit thatfeature selection may provide in hierarchicalclustering task and explore the power of featureselection method applied a a preprocessing 
we present an algorithm that infers the model structure of a mixture of factor analyser using an efficient and deterministic variational approximation to full bayesian integration over model parameter this procedure can automatically determine the optimal number of component and the local dimensionality of each component i e the number of factor in each factor analyser alternatively it can be used to infer posterior distribution over number of component and dimensionality since 
we conduct an average case analysis of the generalization error rate of holdout testing and n fold cross validation quot wrapper quot for model selection unlike previous approach we do not rely on worst case bound that hold for all possible learning problem instead we study the behavior of a learning algorithm with a cross validation wrapper for a given problem taking property of the problem that can be estimated using the sample into account we have to pay for this and the efficiency 
this paper is concerned with the construction of regression and classification tree that are more adapted to data mining application than conventional tree to this end we propose new splitting criterion for growing tree conventional splitting criterion attempt to perform well on both side of a split by attempting a compromise in the quality of fit between the left and the right side by contrast we adopt a data mining point of view by proposing criterion that search for interesting subset of the data a opposed to modeling all of the data equally well the new criterion do not split based on a compromise between the left and the right bucket they effectively pick the more interesting bucket and ignore the other a expected the result is often a simpler characterization of interesting subset of the data le expected is that the new criterion often yield whole tree that provide more interpretable data description surprisingly it is a flaw that work to their advantage the new criterion have an increased tendency to accept split near the boundary of the predictor range this so called end cut problem lead to the repeated peeling of small layer of data and result in very unbalanced but highly expressive and interpretable tree 
knowledge representation is a key issue for any machine learning task there have already been many comparative study about knowledge representation with respect to machine learning in classification task however apart from some work done on reinforcement learning technique in relation to state representation very few study have concentrated on the effect of knowledge representation for machine learning applied to problem solving and more specifically to planning in this paper we 
this paper is concerned with the estimation of a classier s accuracy we present a number of novel bootstrap estimator based on kernel smoothing that consistently show superior performance on both synthetic and real data with respect to other established method we call the process of re sampling the data via kernel based smoothed bootstrap data cloning the new cloning method outperform cross validation and the bootstrap which according to efron and tibshirani is the estimator of choice finally we extend our estimator to complex real life data set in which a data point might include real bounded integer and nominal attribute thus allowing for better classier evaluation over limited real data repository such a the uci repository 
boosting is a method by which an ensembleof classiers can be assembled with generallyspeaking much improved result over individualclassiers and other ensemble method it work by re weighting the trainingset after each classier is induced so thatmisclassied training instance are given increasedweight in the subsequently inducedclassier the focus on misclassied instanceshas meant that boosting cannot continueif the learning system creates a classi er that t 
we introduce an algorithm for estimating the value of a function at a set of test point x xm given a set of training point x y x y without estimating a an intermediate step the regression function we demonstrate that this direct transductive way for estimating value of the regression or classification in pattern recognition is more accurate than the traditional one based on two step first estimating the function and then calculating the value of this function at the point of interest 
learning the dependency structure of a bayesian belief net involves a trade o betweensimplicity and goodness of t to thetraining data we describe the result ofan empirical comparison of three standardmodel selection criterion viz a minimumdescription length criterion mdl akaike s information criterion aic and across validation criterion xv applied tothis problem our result suggest that aicand xv are both good criterion for avoidingovertting but mdl doe 
lazy learning is a memory based technique that once a query is received extract a prediction interpolating locally the neighboring example of the query which are considered relevant according to a distance measure in this paper we propose a data driven method to select on a query by query basis the optimal number of neighbor to be considered for each prediction a an efficient way to identify and validate local model the recursive least square algorithm is introduced in the context of local approximation and lazy learning furthermore beside the winner take all strategy for model selection a local combination of the most promising model is explored the method proposed is tested on six different datasets and compared with a state of the art approach 
transaction data is ubiquitous in data mining application example include market basket data in retail commerce telephone call record in telecommunication and web log of individual page request at web site profiling consists of using historical transaction data on individual to construct a model of each individual s behavior simple profiling technique such a histogram do not generalize well from sparse transaction data in this paper we investigate the application of probabilistic mixture model to automatically generate profile from large volume of transaction data in effect the mixture model represents each individual s behavior a a linear combination of basis transaction we evaluate several variation of the model on a large retail transaction data set and show that the proposed model provides improved predictive power over simpler histogram based technique a well a being relatively scalable interpretable and flexible in addition we point to application in outlier detection customer ranking interactive visualization and so forth the paper concludes by comparing and relating the proposed framework to other transaction data modeling technique such a association rule 
a basic aspect of understanding the neural code of a neuron or aneural system is the ability to form a dictionary from the stimulipresented to the neuron or the system and the pattern of spikesthat the neuron responds with a neuron may respond unreliablyto their stimulus such a dictionary will be stochastic by nature ifthe neuron responds to many dierent stimulus in a similar way i e the number of stimulus feature that the neuron care about issmall then the 
fingerhut business intelligence bi ha a long and successful history of building statistical model to predict consumer behavior the model constructed are typically segmentation based model in which the target audience is split into subpopulation i e customer segment and individually tailored statistical model are then developed for each segment such model are commonly employed in the direct mail industry however segmentation is often performed on an ad hoc basis without directly considering how segmentation affect the accuracy of the resulting segment model fingerhut bi approached ibm research with the problem of how to build segmentation based model more effectively so a to maximize predictive accuracy the ibm advanced targeted marketing single eventstm ibm atm setm solution is the result of ibm research and fingerhut bi directing their effort jointly towards solving this problem this paper present an evaluation of atm se s modeling capability using data from fingerhut s catalog mailing 
we have developed a silicon neuron that is inspired by a mathematical model of the leech heartbeat hn interneuron the temporal and ionic current behavior of this silicon neuron are close to that of the living cell because of this similarity we were able to interface this silicon neuron to a living hn cell using a dynamic clamp technique we present data showing dynamic behavior of the hybrid half center oscillator 
the facial action coding system facs is an objectivemethod for quantifying facial movement in term of componentactions this system is widely used in behavioral investigationsof emotion cognitive process and social interaction the codingis presently performed by highly trained human expert thispaper explores and compare technique for automatically recognizingfacial action in sequence of image these method includeunsupervised learning technique for nding 
the eigenfunction expansion of a kernel function k x y a used in support vector machine or gaussian process predictor is studied when the input data is drawn from a distribution p x in this case it is shown that the eigenfunctions f i g obey the equation k x y p x i x dx i i y this ha a number of consequence including i the eigenvalue vector of the n amp time n gram matrix k obtained by evaluating the kernel at all pair of training point k x i x j converge to the 
a opposed to traditional supervised learning multiple instance learning concern the problem of classifying a bag of instance given bag that are labeled by a teacher a being overall positive or negative current research mainly concentrate on adapting traditional concept learning to solve this problem in this paper we investigate the use of lazy learning and hausdorff distance to approach the multipleinstance problem we present two variant of the k nearest neighbor algorithm called bayesianknn and citation knn solving the multipleinstance problem experiment on the drug discovery benchmark data show that both algorithm are c ompetitive with the best one conceived in the c oncept l earning framework further work includes exploring of a combination of lazy and eager multiple instance problem classifier 
gaussian process are powerful regression model specified by parameterized mean and covariance function standard approach to choose these parameter known by the name hyperparameters are maximum likelihood ml and maximum aposterior map approach in this paper we propose and investigate predictive approach based on geisser s predictive sample reuse psr methodology and the related stone s cross validation cv methodology more specifically we derive result for geisser s 
a new decomposition algorithm for training regression supportvector machine svm is presented the algorithm build onthe basic principle of decomposition proposed by osuna et al and address the issue of optimal working set selection the newcriteria for testing optimality of a working set are derived basedon these criterion the principle of quot maximal inconsistency quot is proposedto form approximately optimal working set experimentalresults show superior performance of the 
recently there ha been significant interest in supervised learningalgorithms that combine labeled and unlabeled data fortext learning task the co training setting blum amp mitchell applies to datasets that have a natural separation oftheir feature into two disjoint set we demonstrate thatwhen learning from labeled and unlabeled data algorithmsexplicitly leveraging a natural independent split of the featuresoutperform algorithm that do not when a naturalsplit doe not 
since the discovery that the best error correcting decoding algorithm can be viewed a belief propagation in a cycle bound graph researcher have been trying to determine under what circumstance quot loopy belief propagation quot is effective for probabilistic inference 
factor analysis and principal component analysis can be used tomodel linear relationship between observed variable and linearlymap high dimensional data to a lower dimensional hidden space in factor analysis the observation are modeled a a linear combinationof normally distributed hidden variable we describe anonlinear generalization of factor analysis called product analysis quot that model the observed variable a a linear combinationof product of normally distributed hidden 
the attribute interdependency have strong effect on understandability of tree based model if strong dependency between the attribute are not recognized and these attribute are not used a split near the root of the tree this cause node replication in lower level of the tree blur the description of dependency and also might cause drop of accuracy if relief family of algorithm which is capable of estimating the attribute dependency is used for split selector we can partly overcome the problem however typically we still want to optimize accuracy of the tree and therefore use accuracy a the split selector measure near the fringe of the tree we present a technique which help u select a split criterion during tree growing based on some theoretical property of relief s estimate we support our claim with empirical result 
the choice of an svm kernel corresponds to the choice of a representationof the data in a feature space and to improve performance it should therefore incorporate prior knowledge such a known transformationinvariances we propose a technique which extends earlierwork and aim at incorporating invariance in nonlinear kernel weshow on a digit recognition task that the proposed approach is superiorto the virtual support vector method which previously had been themethod of choice 
we demonstrate that statistical analysis of ill posed data set issubject to a bias which can be observed when projecting independenttest set example onto a basis dened by the training example 
a nonlinear supervised learning model the specialized mappingsarchitecture sma is described and applied to the estimation ofhuman body pose from monocular image the sma consists ofseveral specialized forward mapping function and an inverse mappingfunction each specialized function map certain domainsof the input space image feature onto the output space bodypose parameter the key algorithmic problem faced are those oflearning the specialized domain and mapping 
despite many empirical success of spectral clustering method algorithm that cluster point using eigenvectors of matrix derivedfrom the distance between the point there are several unresolvedissues first there is a wide variety of algorithm thatuse the eigenvectors in slightly di erent way second many ofthese algorithm have no proof that they will actually compute areasonable clustering in this paper we present a simple spectralclustering algorithm that can be 
probabilistic mixture model are used for a broad range of data analysis task such a clustering classification predictive modeling etc due to their inherent probabilistic nature mixture model can easily be combined with other probabilistic or non probabilistic technique thus forming more complex data analysis system in the case of online data where there is a stream of data available model can be constantly updated to reflect the most current distribution of the incoming data however in many business application the model themselves represent a parsimonious summary of the data and therefore it is not desirable to change model frequently much le with every new data point in such a framework it becomes crucial to track the applicability of the mixture model and detect the point in time when the model fails to adequately represent the data in this paper we formulate the problem of change detection and propose a principled solution empirical result over both synthetic and real life data set are presented 
imagine that you wish to classify data consisting of ten of thousand of example residing in a twenty thousand dimensional space how can one apply standard machine learning algorithm we describe the parallel problem server ppserver and matlab p in tandem they allow user of networked computer to work transparently on large data set from within matlab this work is motivated by the desire to bring the many benefit of scientific computing algorithm and computational power to machine learning researcher we demonstrate the usefulness of the system on a number of task for example we perform independent component analysis on very large text corpus consisting of ten of thousand of document making minimal change to the original bell and sejnowski matlab source bell and sejnowski applying ml technique to data previously beyond their reach lead to interesting analysis of both data and algorithm this paper describes the parallel problem server ppserver and matlab p the ppserver is a linear algebra server that executes distributed memory algorithm on large data set together with matlab p user can manipulate large data set within matlab transparently this system brings the efficiency and power of highly optimized parallel computation to researcher using networked machine but maintain the many benefit of interactive environment we demonstrate the usefulness of the ppserver on a number of task for example we perform independent component analysis on very large text corpus consisting of ten of thousand of document with minimal change to the original bell and sejnowski matlab source bell and sejnowski applying ml technique to datasets previously beyond 
in most neural network model synapsis are treated a static weight that change only with the slow time scale of learning it is well known however that synapsis are highly dynamic and show use dependent plasticity over a wide range of time scale moreover synaptic transmission is an inherently stochastic process a spike arriving at a presynaptic terminal trigger the release of a vesicle of neurotransmitter from a release site with a probability that can be much le than one we consider a simple model for dynamic stochastic synapsis that can easily be integrated into common model for network of integrate and fire neuron spiking neuron the parameter of this model have direct interpretation in term of synaptic physiology we investigate the consequence of the model for computing with individual spike and demonstrate through rigorous theoretical result that the computational power of the network is increased through the use of dynamic synapsis 
function approximation fa representation of the state action value function q have been proposed in order to reduce variance in performance gradient estimate and thereby improve performance of policy gradient pg reinforcement learning in large continuous domain e g the pifa algorithm of sutton et al in press we show empirically that although pifa converges significantly faster than traditional pg algorithm such a reinforce which directly sample q without using fa fa representation of q are not necessary to reduce variance in performance gradient estimate and pg algorithm which use selective direct sample of q can converge order of magnitude faster than pifa we present a new pg algorithm called action transition policy gradient atpg which us direct sample of q and restricts estimate of the gradient to coincide with action transition thus obtaining relative value estimate of executing action without using fa representation of q we prove that atpg give an unbiased estimate of the performance gradient and converges to an optimal policy under piece wise continuity condition on the policy and the state action value function further in an experimental comparison with pifa and reinforce atpg always outperforms both algorithm taking order of magnitude fewer iteration to converge on all but very simple problem 
learning of a smooth but nonparametric probability density can be regularized using method of quantum field theory we implement a field theoretic prior numerically test it efficacy and show tha t the free parameter of the theory smoothness scale can be determine d self consistently by the data this form an infinite dimensional gen eralization of the mdl principle finally we study the implication of one s choice of the prior and the parameterization and conclude that the smoothness scale determination make density estimation very weakly sensitive to the choice of the prior and that even wrong choice can be advantageous for small data set one of the central problem in learning is to balance goodness of fit criterion against the complexity of model an important development in the bayesian approach wa thus the realization that there doe not need to be any extra penalty f or model complexity if we compute the total probability that data are generated by a model there is a factor from the volume in parameter space the occam factor that discriminates against model with more parameter this work remarkably well for system with a finite number of parameter and creates a complexity razor after occam s razor that is almost equivalent to the celebrated minimal description length mdl principle in addition if the a priori distribution involved are strictly gaussian the idea have also been proven to apply to some infinite dimensional nonparametric proble m it is not clear however what happens if we leave the finite dimensional setting to con sider nonparametric problem which are not gaussian such a the estimation of a smooth probability density a possible route to progress on the nonparametric problem wa opened by noticing that a bayesian prior for density estimation is equivalent to a qu antum field theory qft in particular there are field theoretic method for computing the infinite dimensional analog of the occam factor at least asymptotically for large number of example these observation have led to a number of paper exploring alternative formulation and their implication for the speed of learning here we return to the original formulation of ref and use numerical method to address some of the question left open by the analytic work what is the result of balancing the infini te dimensional occam factor against the goodness of fit is the qft inference optimal in u ing all of the information relevant for learning what happens if our learning problem is strongly atypical of the prior distribution following ref if i i d sample are observed then the probability 
landmarking is a novel approach to describing task in meta learning previous approach to meta learning mostly considered only statistic inspired measure of the data a a source for the definition of meta attribute contrary to such approach landmarking try to determine the location of a specific learning problem in the space of all learning problem by directly measuring the performance of some simple and efficient learning algorithm themselves in the experiment reported we show how such a use of landmark value can help to distinguish between area of the learning space favouring different learner experiment both with artificial and real world database show that landmarking selects with moderate but reasonable level of success the best performing of a set of learning algorithm 
a challenging unsolved problem in the speech recognition communityis recognizing speech signal that are corrupted by loud highly nonstationary noise one approach to noisy speech recognitionis to automatically remove the noise from the cepstrum sequencebefore feeding it in to a clean speech recognizer in previouswork published in eurospeech we showed how a probability modeltrained on clean speech and a separate probability model trainedon noise could be combined for the purpose of 
we present a model of the firing of place and head direction cell in rat hippocampus the model can predict the response of individual cell and population to parametric manipulation of both geometric e g o keefe burgess and orientational fenton et al a cue extending a previous geometric model hartley et al it provides a functional description of how these cell spatial response are derived from the rat s environment and make easily testable quantitative prediction consideration of the phenomenon of remapping muller kubie bostock et al indicates that the model may also be consistent with nonparametric change in firing and provides constraint for it future development 
we present a model of binding of relationship information in a spatial domain e g square above triangle that us low order coarse coded conjunctive representation instead of more popular temporal synchrony mechanism supporter of temporal synchrony argue that conjunctive representation lack both efficiency i e combinatorial number of unit are required and systematicity i e the resulting representation are overly specific and thus do not support generalization to novel exemplar to counter these claim we show that our model a us far fewer hidden unit than the number of conjunction represented by using coarse coded distributed representation where each unit ha a broad tuning curve through high dimensional conjunction space and b is capable of considerable generalization to novel input 
symmetrically connected recurrent network have recently been used a model of a host of neural computation however biological neural network have asymmetrical connection at the very least because of the separation between excitatory and inhibitory neuron in the brain we study characteristic difference between asymmetrical network and their symmetrical counterpart in case for which they act a selective amplifier for particular class of input pattern we show that the dramatically different dynamical behaviour to which they have access often make the asymmetrical network computationally superior we illustrate our result in network that selectively amplify oriented bar and smooth contour in visual input 
abstract high dimensional data that lie on or near a low dimensional manifold can be described by a collection of local linear model such a description however doe not provide a global parameterization of the manifold arguably an important goal of unsupervised learning in this paper we show how to learn a collection of local linear model that solves this more difficult proble m our local linear model are represented by a mixture of factor analyzer and the global coordination of these model is achieved by adding a regularizing term to the standard maximum likelihood objective function the regularizer break a degeneracy in the mixture model s parameter space favoring model who se internal coordinate system are aligned in a consistent way a a result t he internal coordinate change smoothly and continuously a one traverse a connected path on the manifold even when the path cross the domain of many different local model the regularizer take the form of a kullback leibler divergence and illustrates an unexpected application of variational meth od not to perform approximate inference in intractable probabilistic model but to learn more useful internal representation in tractable one manifold learning 
we use graphical model to explore the question of how people learn simple causal relationship from data the two leading psychological theory can both be seen a estimating the parameter of a fixed graph we argue that a complete account of causal induction should also consider how people learn the underlying causal graph structure and we propose to model this inductive process a a bayesian inference our argument is supported through the discussion of three data set 
we present the embedded tree algorithm an iterative technique for estimation of gaussian process dened on arbitrary graph by exactly solving a series of modied problem on embedded spanning tree it computes the conditional mean with an eciency comparable to or better than other technique unlike other method the embedded tree algorithm also computes exact error covariance the error covariance computation is most ecien t for graph in which removing a small number of edge reveals an embedded tree in this context we demonstrate that sparse loopy graph can provide a signican t increase in modeling power relative to tree with only a minor increase in estimation complexity 
reaching movement require the brain to generate motor command that rely on an internal model of the task s dynamic here we consider the error that subject make early in their reaching trajectory to various target a they learn an internal model using a framework from function approximation we argue that the sequence of error should reect the process of gradient descent if so then the sequence of error should obey hidden state transition of a simple dynamical system fitting the system to human data we nd a surprisingly good t accounting for of the variance this allows u to draw tentative conclusion about the basis element used by the brain in transforming sensory space to motor command to test the robustness of the result we estimate the shape of the basis element under two condition in a traditional learning paradigm with a consistent force eld and in a random sequence of force eld where learning is not possible remarkably we nd that the basis remains invariant 
this paper present a novel approach to the unsupervised learning of syntactic analysis of natural language text most previous work ha focused on maximizing likelihood according to generative pcfg model in contrast we employ a simpler probabilistic model over tree based directly on constituent identity and linear context and use an em like iterative procedure to induce structure this method produce much higher quality analysis giving the best published result on the atis dataset 
large margin linear classification method have been successfully applied to many application for a linearly separable problem it is known that under appropriate assumption the expected misclassification error of the computed optimal hyperplane approach zero at a rate proportional to the inverse training sample size this rate is usually characterized by the margin and the maximum norm of the input data in this paper we argue that another quantity namely the robustness of the input data distribution also play an important role in characterizing the convergence behavior of expected misclassification error based on this concept of robustness we show that for a large margin separable linear classification problem the expected misclassification error may converge exponentially in the number of training sample size 
this paper considers approach which rerank the output of an existing probabilistic parser the base parser produce a set of candidate par for each input sentence with associated probability that define an initial ranking of these par a second model then attempt to improve upon this initial ranking using additional feature of the tree a evidence we describe and compare two approach to the problem one based on markov random field the other based on boosting approach to reranking problem the method were applied to reranking output of the parser of collins on the wall street journal corpus with a relative decrease in error rate 
this paper investigates the use of evoluti onary algorithm for the search of hypothesis space in machine learning task a opposed to the common scalar evaluation function imposing a complete order onto the hypothesis space we propose genetic search incorporating pairwise comparison of hypothesis particularly we allow incomparability of hypothesis what implies a partial order in the hypothesis space we claim that such an extension protects the interesting hypothesis from being discarded in the search process and thus increase the diversity of the population allowing better exploration of the solution space a a result it is more probable to reach hypothesis with good predictive accuracy this supposition ha been positively verified in an extensive comparative experiment of evolutionary visual learning concerning the recognition of handwritten character 
this paper describes a study of different adaptation of boosting algorithm for cost sensitive classification the purpose of the study is to improve our understanding of the behavior of various cost sensitive boosting algorithm and how variation in the boosting procedure affect misclassification cost and high cost error we find that boosting can be simplified for cost sensitive classification a new variant which excludes a factor used in ordinary boosting performs best at minimizing 
electromyographic signal may provide an important new class of user interface for consumer electronics in order to make such interface effective it will be crucial to map emg signal to user gesture in real time the mapping from signal to gesture will vary from user to user so it must be acquired adaptively in this paper we describe and compare three method for static classification of emg signal we then go on to explore method for adapting the classifier over time and for sequential analysis of the gesture stream by combining the static classification algorithm with a hidden markov model we conclude with an evaluation of the combined model on an unsegmented stream of gesture 
abstract we present a tree based reparameterization framework for the approximate estimation ofstochastic process on graph with cycle this framework provides a new conceptual view ofa large class of iterative algorithm for computing approximate marginals in graph with cycle 
this paper investigates how behavioralcloning can be used to decrease training timefor student learning to y on simulator the challenge presented to each studentmust be tailored to their unique learning experience this requires an intelligent trainingregime that exploit a model of each studentthat predicts where the student s performancewill be de cient here we show thatcloning the behavior of student pilot with amodular neural network result in the automatic 
reinforcement learning in nonstationary environment is generallyregarded a an important and yet difficult problem this paperpartially address the problem by formalizing a subclass of nonstationaryenvironments the environment model called hidden modemarkov decision process hm mdp assumes that environmentalchanges are always confined to a small number of hidden mode a mode basically index a markov decision process mdp andevolves with time according to a markov chain 
recently several author developed a new approach to bounding the generalization error of complex classifier of large or even infinite vc dimension obtained by combining simpler classifier the new bound are in term of the distribution of the margin of combined classifier and they provide some theoretical explanation of generalization performance of large neural network see bartlett ieee transaction on information theory a well a such technique of combining 
we investigate a general characteristic of the trade o in learningproblems between goodness of t and model complexity speci cally we characterize a general class of learning problem where thegoodness of t function can be shown to be convex within rstorderas a function of model complexity this general propertyof diminishing return quot is illustrated on a number of real datasets and learning problem including nite mixture modeling andmultivariate linear regression 
this paper deal with a neural network architecture which establishes a portfolio management system similar to the black littermanapproach this allocation scheme distributes fund across various security or financial market while simultaneously complying with specific allocation constraint which meet the requirement of an investor the portfolio optimization algorithm is modeled by a feedforward neural network the underlying expected return forecast are based on error correction neural network ecnn which utilize the last model error a an auxiliary input to evaluate their own misspecification the portfolio optimization is implemented such that i the allocation comply with investor s constraint and that ii the risk of the portfolio can be controlled we demonstrate the profitability of our approach by constructing internationally diversified portfolio across different financial market of the g contries it turn out that our approach is superior to a preset benchmark portfolio 
a novel noise suppression scheme for speech signal is proposedwhich is based on a neurophysiologically motivated estimation ofthe local signal to noise ratio snr in dierent frequency channel 
we explore the use of genetic algorithm to directly evolve classification decision tree we argue on the suitability of such a concept learner due to it ability to efficiently search complex hypothesis space and discover conditionally dependent a well a irrelevant attribute the performance of the system is measured on a set of artificial and standard discretized concept learning problem and compared with the performance of two known algorithm c oner we demonstrate that the derived hypothesis of standard algorithm can substantially deviate from the optimum this deviation is partly because of their non universal procedural bias and it can be reduced using global metric of tree quality like the one proposed 
abstract signal processing and pattern recognition algorithm make exten sive use of convolution in many case computational accuracy is not a important a computational speed in feature extraction for instance the feature of interest in a signal are usually quite distorted this form of noise justi e some level of quantization in order to achieve faster feature extraction our approach consists of approximating region of the signal with low degree polynomi al and then di erentiating the resulting signal in order to obtain impulse function or derivative of impulse function with this representation convolution becomes extremely simple and can be implemented quite e ectively the true convolution can be recov ered by integrating the result of the convolution this method yield substantial speed up in feature extraction and is applicable to convolutional neural network 
we investigate the behavior of a hebbian cell assembly of spiking neuron formed via a temporal synaptic learning curve this learning function is based on recent experimental finding it includes potentiation for short time delay between preand post synaptic neuronal spiking and depression for spiking event occuring in the reverse order the coupling between the dynamic of the synaptic learning and of the neuronal activation lead to interesting result we find that the cell assembly 
we examine mathematical model for semi supervised support vector machine s vm given a training set of labeled data and a working set of unlabeled data s vm construct a support vector machine using both the training and working set we use s vm to solve the transductive inference problem posed by vapnik in transduction the task is to estimate the value of a classificationfunction at the given point in the working set this contrast with inductive inference which estimate the classificationfunction at all possible value we propose a general s vm model that minimizes both the misclassification error and the function capacity based on all the available data depending on how poorly estimated unlabeled data are penalized different mathematical model result we examine several practical algorithm for solving these model the firstapproach utilizes the s vm model for norm linear support vector machine converted to a mixedinteger program mip a global solution of the mip is found using a commerical integer programming solver the second approach us a noncovex quadratic program variation of block coordinate descent algorithm are used to find local solution of this problem using this mip within a local learning algorithm produced the best result our experimental study on these statistical learning method indicates that incorporating working data can improve generalization 
this paper explores in detail the use of error correcting output coding ecoc for learning text classifier we show that the accuracy of a naive bayes classifier over text classification task can be significantly improved by taking advantage of the error correcting property of the code we also explore the use of different kind of code namely error correcting code random code and domain and data specific code and give experimental result for each of them the ecoc method scale well to large data set with a large number of class experiment on a real world data set show a reduction in classification error by up to over the traditional naive bayes classifier we also compare our empirical result to semi theoretical result and find that the two closely agree 
we present method for learning and tracking human motion in video we estimate a statistical model of typical activity from a large set of d periodic human motion data by segmenting these data automatically into quot cycle quot then the mean and the principal component of the cycle are computed using a new algorithm that account for missing information and enforces smooth transition between cycle the learned temporal model provides a prior probability distribution over human motion that 
the deep layer of the superior colliculus dsc integrate multisensory input and initiate an orienting response toward the source of stimulation multisensory response enhancement mre is the augmentation of a neural response of a dsc neuron to sensory input of one modality by input of another modality the maximum likelihood model presented here extends the bayesian model for mre by anastasio et al by incorporating a decision strategy to maximize the number of correct decision it account for the inverse eectiveness observed in neurophysiological recording data and it predicts a functional relation between uniand bimodal level of discriminability that is testable both in neurophysiological and behavioral experiment 
we consider the problem of maximizing thetotal number of success while learningabout a probability function determining thelikelihood of a success in particular weconsider the case in which the probabilityfunction is represented by a linear functionof the attribute vector associated with eachaction choice in the scenario we consider learning proceeds in trial and in each trial the algorithm is given a number of alternativesto choose from each having an attributevector 
we investigate applying theory renemen t to the task of extracting information from text in theory renemen t partial domain knowledge which may be incorrect is given to a supervised learner the provided knowledge guide the learner in it task but the learner can rene or even discard this knowledge during training our supervised learner is a knowledge based neural network that initially contains compiled prior knowledge about a particular information extraction ie task the prior knowledge need to specify the extraction slot for the specic ie task our approach us generate and test to address the ie task in the generation step we produce candidate extraction by intelligently searching the space of possible extraction in the test step we use the trained network to judge each candidate and output those that exceed a system selected threshold experiment on the cmu seminarannouncements and the yeast subcellularlocalization domain demonstrate our approach s value this paper we demonstrate how the theory renemen t approach e g towell shavlik can be used to build an ie system by using theory renemen t we are able to strike a balance between needing a large number of labeled example and having a complete and correct set of domain knowledge our system take advantage of the intuition that information retrieval ir and ie are nearly inverse problem of each other an ir system is given a set of keywords and is asked to rate the relevance of document an ie system is given a set of document and is asked to ll in the slot in a given template we explore how what is essentially an ir system can be used to address the ie task 
in this paper we present pva an adaptive personal view information agent system to track learn and manage user s interest in internet document when user s interest change pva in not only the content but also in the structure of user profile is modified to adapt to the change experimental result show that modulating the structure of user profile doe increase the accuracy of personalization system 
narayanan and jurafsky proposed that human language comprehension can be modeled by treating human comprehenders a bayesian reasoner and modeling the comprehension process with bayesian decision tree in this paper we extend the narayanan and jurafsky model to make further prediction about reading time given the probability of difference par or interpretation and test the model against reading time data from a psycholinguistic experiment 
linear relational embedding is a method of learning a distributed representation of concept from data consisting of binary relation between concept concept are represented a vector binary relation a matrix and the operation of applying a relation to a concept a a matrix vector multiplication that produce an approximation to the related concept a representation for concept and relation is learned by maximizing an appropriate discriminative goodness function using gradient 
this paper present an active learning method that directly optimizes expected future error this is in contrast to many other popular technique that instead aim to reduce version space size these other method are popular because for many learning model closed form calculation of the expected future error is intractable our approach is made feasible by taking a sampling approach to estimating the expected reduction in error due to the labeling of a query in experimental result on two real world data set we reach high accuracy very quickly sometimes with four time fewer labeled example than competing method 
we develop an approach for a sparse representation for gaussian process gp model in order to overcome the limitation of gps caused by large data set the method is based on a combination of a bayesian online algorithm together with a sequential construction of a releva nt subsample of the data which fully specifies the prediction of the model experimental result on toy example and large real world dataset s indicate the efficiency of the approach 
the popular k mean clustering partition a data set by minimizing a sum of square cost function a coordinate descend method is then used to find local minimum in this paper we show that the minimization can be reformulated a a trace maximization problem associated with the gram matrix of the data vector furthermore we show that a relaxed version of the trace maximization problem posse global optimal solution which can be obtained by computing a partial eigendecomposition of the gram 
we present a novel method for clustering using the support vector machine approach data point are mapped to a high dimensional feature space where support vector are used to define a sphere enclosing them the boundary of the sphere form in data space a set of closed contour containing the data data point enclosed by each contour are defined a a cluster a the width parameter of the gaussian kernel is decreased these contour fit the data more tightly and splitting of contour occurs the algorithm work by separating cluster according to valley in the underlying probability distribution and thus cluster can take on arbitrary geometrical shape a in other sv algorithm outlier can be dealt with by introducing a soft margin constant leading to smoother cluster boundary the structure of the data is explored by varying the two parameter we investigate the dependence of our method on these parameter and apply it to several data set 
we investigate the formation of a hebbian cell assembly of spiking neuron usinga temporal synaptic learning curve that is based on recent experimental finding itincludes potentiation for short time delay between preand post synaptic neuronalspiking and depression for spiking event occurring in the reverse order the couplingbetween the dynamic of synaptic learning and that of neuronal activation lead tointeresting result one possible mode of activity is distributed synchrony 
we study online learning in boolean domain using kernel which capture feature expansion equivalent to using conjunction over basic feature we demonstrate a tradeoff between the computational efficiency with which these kernel can be computed and the generalization ability of the resulting classifier we first describe several kernel function which capture either limited form of conjunction or all conjunction we show that these kernel can be used to efficiently run the perceptron algorithm over an exponential number of conjunction however we also prove that using such kernel the perceptron algorithm can make an exponential number of mistake even when learning simple function we also consider an analogous use of kernel function to run the multiplicative update winnow algorithm over an expanded feature space of exponentially many conjunction while known upper bound imply that winnow can learn dnf formula with a polynomial mistake bound in this setting we prove that it is computationally hard to simulate winnow s behavior for learning dnf over such a feature set and thus that such kernel function for winnow are not efficiently computable 
robocup simulated soccer present many challenge to reinforcement learning method including a large state space hidden and uncertain state multiple agent and long and variable delay in the effect of action we describe our application of episodic smdp sarsa with linear tile coding function approximation and variable to learning higher level decision in a keepaway subtask of robocup soccer in keepaway one team the keeper try to keep control of the ball for a long a possible despite the effort of the taker the keeper learn individually when to hold the ball and when to pas to a teammate while the taker learn when to charge the ball holder and when to cover possible passing lane our agent learned policy that significantly out performed a range of benchmark policy we demonstrate the generality of our approach by applying it to a number of task variation including different field size and different number of player on each team 
the encoding accuracy of a population of stochastically spiking neuron is studied for different distribution of their tuning widt h the situation of identical radially symmetric receptive field for all neu ron which is usually considered in the literature turn out to be disa dvantageous from an information theoretic point of view both a variabi lity of tuning width and a fragmentation of the neural population into specialized subpopulation improve the encoding accuracy 
we describe a unified framework for the understanding of structure representation in primate vision a model derived from this framework is shown to be effectively systematic in that it ha the ability to interpret and associate together object that are related through a rearrangement of common middle scale part represented a image fragment the model address the same concern a previous work on compositional representation through the use of what where receptive field and attentional gain modulation it doe not require prior exposure to the individual part and avoids the need for abstract symbolic binding 
the marriage of renyi entropy with parzen density estimation ha been shown to be a viable tool in learning discriminative feature transforms however it suffers from computational complexity proportional to the square of the number of sample in the training data this set a practical limit to using large database we suggest immediate divorce of the two method and remarriage of renyi entropy with a semi parametric density estimation method such a a gaussian mixture model gmm this allows all of the computation to take place in the low dimensional target space and it reduces computational complexity proportional to square of the number of component in the mixture furthermore a convenient extension to hidden markov model a commonly used in speech recognition becomes possible 
the computer and telecommunication industry rely heavily on knowledge based expert system to manage the performance of their network these expert system are developed by knowledge engineer who must first interview domain expert to extract the pertinent knowledge this knowledge acquisition process is laborious and costly and typically is better at capturing qualitative knowledge than quantitative knowledge this is a liability especially for domain like the telecommunication domain where enormous amount of data are readily available for analysis data mining hold tremendous promise for the development of expert system for monitoring network performance since it provides a way of automatically identifying subtle yet important pattern in data this case study describes a project in which a temporal data mining system called timeweaver is used to identify faulty telecommunication equipment from log of network alarm message 
the mystery of belief propagation bp decoder especially of the turbo decoding is studied from information geometrical viewpoint the loopy belief network bn of turbo code make it difficult to obtain the true belief by bp and the characteristic of the algorithm and it equilibrium are not clearly understood our study give an intuitive understanding of the mechanism and a new framework for the analysis based on the framework we reveal basic property of the turbo decoding 
we have designed and fabricated a vlsi synapse that can learn a conditional probability or correlation between spike based input and feedback signal the synapse is low power compact provides nonvolatile weight storage and can perform simultaneous multiplication and adaptation we can calibrate array of synapsis to ensure uniform adaptation characteristic finally adaptation in our synapse doe not necessarily depend on the signal used for computation consequently our synapse can implement learning rule that correlate past and present synaptic activity we provide analysis and experimental chip result demonstrating the operation in learning and calibration mode and show how to use our synapse to implement various learning rule in silicon 
we present an extension to the mixture of expert me model where the individual expert are gaussian process gp regression model using an input dependent adaptation of the dirichlet process we implement a gating network for an infinite number of expert inference in this model may be done efficiently using a markov chain relying on gibbs sampling the model allows the effective covariance function to vary with the input and may handle large datasets thus potentially overcoming two of the biggest hurdle with gp model simulation show the viability of this approach 
gt y sign ha x i where ha x i is the posterior mean ha x i ea x q ti p y i ja x i eq ti p y i ja x i e is the expectation over the gp prior and t is the number of example exact parametrisationmean and covariance of the posterior can be expressed a ha x i txi k x x lt f 
condensation a form of likelihood weighted particle filte ring ha been successfully used to infer the shape of highly constrained active contour in video sequence however when the contour are highly flexible e g for tracking finger of a hand a computationally burdensom e number of particle is needed to successfully approximate the c ontour distribution we show how the metropolis algorithm can be used to update a particle set representing a distribution over contour at e ach frame in a video sequence we compare this method to condensation using a video sequence that requires highly flexible contour and show th at the new algorithm performs dramatically better that the condensat ion algorithm we discus the incorporation of this method into the active contour framework where a shape subspace is used constrain shape variation 
this paper considers the hypothesis that system learning aspect of visual perception may benefit from the use of suitably designed developmental progression during training we report the result of simulation in which three different artificial neural network model were trained to detect binocular disparity in pair of visual image two of the model were developmental model in the sense that the nature of their training input changed during the course of training either a 
the conventional wisdom is that backprop net with excess hidden unit generalize poorly we show that net with excess capacity generalize well when trained with backprop and early stopping experiment suggest two reason for this overfitting can vary significant ly in different region of the model excess capacity allows better fit to reg ion of high non linearity and backprop often avoids overfitting the re gions of low non linearity regardless of size net learn task subco mponents in similar sequence big net pas through stage similar to th ose learned by smaller net early stopping can stop training the large n et when it generalizes comparably to a smaller net we also show that conjugate gradient can yield worse generalization because it overfits region of low non linearity when learning to fit region of high non linea rity 
it ha long been known that lateral inhibition in neural network can lead to a winner take all competition so that only a single neuron is active at a steady state here we show how to organize lateral inhibition so that group of neuron compete to be active given a collection of potentially overlapping group the inhibitory connectivity is set by a formula that can be interpreted a arising from a simple learning rule our analysis demonstrates that such inhibition generally result in 
drawing on the correspondence between the graph laplacian thelaplace beltrami operator on a manifold and the connection tothe heat equation we propose a geometrically motivated algorithmfor constructing a representation for data sampled from a low dimensionalmanifold embedded in a higher dimensional space thealgorithm provides a computationally efficient approach to nonlineardimensionality reduction that ha locality preserving propertiesand a natural connection to 
we present a new model for studying multitasklearning linking theoretical result topractical simulation in our model all tasksare combined in a single feedforward neuralnetwork learning is implemented in abayesian fashion in this bayesian frameworkthe hidden to output weight beingspecific to each task play the role of modelparameters the input to hidden weight which are shared between all task aretreated a hyperparameters other hyperparametersdescribe error 
we introduce a novel method of constructing language model which avoids some of the problem associated with recurrent neuralnetworks the method of creating a prediction fractal machine pfm is briefly described and some experiment are presentedwhich demonstrate the suitability of pfms for language modeling pfms distinguish reliably between minimal pair and their behavioris consistent with the hypothesis that wellformedness is graded not absolute a discussion of 
this is a survey of some theoretical result onboosting obtained from an analogous treatmentof some regression and classificationboosting algorithm some related paper include j and j a b c d which is a setof mutually overlapping paper concerningthe assumption of weak hypothesis behaviorof generalization error in the large time limitand during the process of boosting comparisonto the optimal bayes error in noisy situation overfitting and regularization 
competitive learning is a technique for training classification and clustering network we have designed and fabricated an transistor primitive that we term an automaximizing bump circuit that implement competitive learning dynamic the circuit performs a similarity computation affords nonvolatile storage and implement simultaneous local adaptation and computation we show that our primitive is suitable for implementing competitive learning in vlsi and demonstrate it effectiveness in a standard clustering task 
we describe a method for learning an overcomplete set of basisfunctions for the purpose of modeling sparse structure in image the sparsity of the basis function coefficient is modeled with amixture of gaussians distribution one gaussian capture nonactivecoefficients with a small variance distribution centered atzero while one or more other gaussians capture active coefficientswith a large variance distribution we show that when the prior isin such a form there exist 
function approximation is essential to reinforcement learning but the standard approach of approximating a value function and determining a policy from it ha so far proven theoretically intractable in this paper we explore an alternative approach in which the policy is explicitly represented by it own function approximator independent of the value function and is updated according to the gradient of expected reward with respect to the policy parameter williams s reinforce method and actor critic method are example of this approach our main new result is to show that the gradient can be written in a form suitable for estimation from experience aided by an approximate action value or advantage function using this result we prove for the first time that a version of policy iteration with arbitrary differentiable function approximation is convergent to a locally optimal policy large application of reinforcement learning rl require the use of generalizing function approximators such neural network decision tree or instance based method the dominant approach for the last decade ha been the value function approach in which all function approximation effort go into estimating a value function with the action selection policy represented implicitly a the greedy policy with respect to the estimated value e g a the policy that selects in each state the action with highest estimated value the value function approach ha worked well in many application but ha several limitation first it is oriented toward finding deterministic policy whereas the optimal policy is often stochastic selecting different action with specific probability e g see singh jaakkola and jordan second an arbitrarily small change in the estimated value of an action can cause it to be or not be selected such discontinuous change have been identified a a key obstacle to establishing convergence assurance for algorithm following the value function approach bertsekas and tsitsiklis for example q learning sarsa and dynamic programming method have all been shown unable to converge to any policy for simple mdps and simple function approximators gordon baird tsitsiklis and van roy bertsekas and tsitsiklis this can occur even if the best approximation is found at each step before changing the policy and whether the notion of best is in the mean squared error sense or the slightly different sens of residual gradient temporal difference and dynamic programming method in this paper we explore an alternative approach to function approximation in rl 
in this paper we describe an approach to model selection in unsupervised learning thisapproach determines both the feature set and the number of cluster to this end we first derivean objective function that explicitly incorporates this generalization we then evaluate twoschemes for model selection one using this objective function a bayesian estimation schemethat selects the best model structure using the marginal or integrated likelihood and the secondbased on a technique using a 
when designing a two alternative classifier one ordinarily aim to maximize the classifier s ability to discriminate between member of the two class we describe a situation in a real world business application of machine learning prediction in which an additional constraint is placed on the nature of the solution that the classifier achieve a specified correct acceptance or correct rejection rate i e that it achieve a fixed accuracy on member of one class or the other our domain is predicting churn in the telecommunication industry churn refers to customer who switch from one service provider to another we propose four algorithm for training a classifier subject to this domain constraint and present result showing that each algorithm yield a reliable improvement in performance although the improvement is modest in magnitude it is nonetheless impressive given the difficulty of the problem and the financial return that it achieves to the service provider when designing a classifier one must specify an objective measure by which the classifier s performance is to be evaluated one simple objective measure is to minimize the number of misclassifications if the cost of a classification error depends on the target and or response class one might utilize a risk minimization framework to reduce the expected loss a more general approach is to maximize the classifier s ability to discriminate one class from another class e g chang lippmann an roc curve green swets can be used to visualize the discriminative performance of a two alternative classifier that output class posterior to explain the roc curve a classifier can be thought of a making a positive negative judgement a to whether an input is a member of some class two different accuracy measure can be obtained from the classifier the accuracy of correctly identifying an input a a member of the class a correct acceptance or ca and the accuracy of correctly identifying an input a a nonmember of the class a correct rejection or cr to evaluate the ca and cr rate it is necessary to pick a threshold above which the classifier s probability estimate is interpreted a an accept and below which is interpreted a a reject call this the criterion the roc curve plot ca against cr rate for various criterion figure a note that a the threshold is lowered the ca rate increase and the cr rate decrease for a criterion of the ca rate approach and the cr rate for a criterion of the ca rate approach 
in this paper we extend the rao blackwellised particle filtering method to more complex hybrid model consisting of gaussian latent variable and discrete observation this is accomplished by augmenting the model with artificial variable that enable u to apply rao blackwellisation other improvement include the design of an optimal importance proposal distribution and being able to swap the sampling an selection step to handle outlier we focus on sequential binary classifier that 
we present three way of combining linear programming with the kernel trick to find value function approximation for reinforcement learning one formulation is based on svm regression the second is based on the bellman equation and the third seek only to ensure that good move have an advantage over bad move all formulation attempt to minimize the number of support vector while fitting the data experiment in a difficult synthetic maze problem show that all three formulation give 
we combine the replica approach from statistical physic with a variational approach to analyze learning curve analytically we apply the method to gaussian process regression a a main result we derive approximative relation between empirical error measure the generalization error and the posterior variance 
in packet switch packet queue at switch input and contend for output the contention arbitration policy directly affect switch performance the best policy depends on the current state of the switch and current traffic pattern this problem is hard because the state space possible transition and set of action all grow exponentially with the size of the switch we present a reinforcement learning formulation of the problem that decomposes the value function into many small independent value function and enables an efficient action selection 
everybody know that neural network need more than a single layer of nonlinear unit to compute interesting function we show that this is false if one employ winner take all a nonlinear unit any boolean function can be computed by a single winner takeall unit applied to weighted sum of the input variable any continuous function can be approximated arbitrarily well by a single soft winner take all unit applied to weighted sum of the input variable only positive weight are needed in these linear weighted sum this may be of interest from the point of view of neurophysiology since only of the synapsis in the cortex are inhibitory in addition it is widely believed that there are special microcircuit in the cortex that compute winner take all our result support the view that winner take all is a very useful basic computational unit in neural vlsi it is wellknown that winner take all of input variable can be computed very efficiently with transistor and a total wire length and area that is linear in in analog vlsi lazzaro et al we show that winner take all is not just useful for special purpose computation but may serve a the only nonlinear unit for neural circuit with universal computational power we show that any multi layer perceptron need quadratically in many gate to compute winner take all for input variable hence winner take all provides a substantially more powerful computational unit than a perceptron at about the same cost of implementation in analog vlsi 
recently we presented a new approach to the classification problem arising in data mining it is based on the regularization network approach but in contrast to other method which employ ansatz function associated to data point we use a grid in the usually high dimensional feature space for the minimization process to cope with the curse of dimensionality we employ sparse grid thus only o hn nd instead of o hn d grid point and unknown are involved here d denotes the dimension of the feature space and hn n give the mesh size we use the sparse grid combination technique where the classification problem is discretized and solved on a sequence of conventional grid with uniform mesh size in each dimension the sparse grid solution is then obtained by linear combination in contrast to our former work where d linear function were used we now apply linear basis function based on a simplicial discretization this allows to handle more dimension and the algorithm need le operation per data point we describe the sparse grid combination technique for the classification problem give implementational detail and discus the complexity of the algorithm it turn out that the method scale linearly with the number of given data point finally we report on the quality of the classifier built by our new method on data set with up to dimension it turn out that our new method achieves correctness rate which are competitive to that of the best existing method 
in memory consolidation declarative memory which initially require the hippocampus for their recall ultimately become independent of it consolidation ha been the focus of numerous experimental and qualitative modeling study but only little quantitative exploration we present a consolidation model in which hierarchical connection in the cortex that initially instantiate purely semantic information acquired through probabilistic unsupervised learning come to instantiate episodic information a well the hippocampus is responsible for helping complete partial input pattern before consolidation is complete while also training the cortex to perform appropriate completion by itself 
we describe a new algorithm for computing a nash equilibrium in graphical game a compact representation for multi agent system that we introduced in previous work the algorithm is the first to compute equilibrium both efficiently and exactly for a non trivial class of graphical game 
in this paper we derive a second order mean field theory for di rected graphical probability model by using an information theoretic argument it is shown how this can be done in the absense of a partition function this method is a direct generalisation of the well known tap approximation for boltzmann machine in a numerical example it is shown that the method greatly improves the first order mean fie ld approximation for a restricted class of graphical model so called single overlap graph the second order method ha comparable complexity to the first order method for sigmoid belief network the meth od is shown to be particularly fast and effective 
the selection of kernel parameter is an open problem in the training of nonlinear support vector machine the usual selection criterion is the quotient of the radius of the smallest sphere enclosing the training feature and the margin width empirical study on real world data using gaussian and polynomial kernel show that the test error due to this criterion is often much larger than the minimum test error in other word this criterion can be suboptimal or inadequate hence we propose augmenting the usual criterion with a traditional measure of class separability in statistical feature selection this measure employ the within class and betweenclass scatter in feature space which is equivalent to computing the pooled covariance matrix trace and the distance between class mean we show empirically that the new criterion result in improved generalization 
this paper examines the application of reinforcement learning to a wireless communication problem the problem requires that channel utility be maximized while simultaneously minimizing battery usage we present a solution to this multi criterion problem that is able to significantly reduce power consumption the solution us a variable discount factor to capture the effect of battery usage 
outlier detection is an important task in data mining with numerous application including credit card fraud detection video surveillance etc a recent work on outlier detection ha introduced a novel notion of local outlier in which the degree to which an object is outlying is dependent on the density of it local neighborhood and each object can be assigned a local outlier factor lof which represents the likelihood of that object being an outlier although the concept of local outlier is a useful one the computation of lof value for every data object requires a large number of kgr nearest neighbor search and can be computationally expensive since most object are usually not outlier it is useful to provide user with the option of finding only n most outstanding local outlier i e the top n data object which are most likely to be local outlier according to their lofs however if the pruning is not done carefully finding top n outlier could result in the same amount of computation a finding lof for all object in this paper we propose a novel method to efficiently find the top n local outlier in large database the concept of micro cluster is introduced to compress the data an efficient micro cluster based local outlier mining algorithm is designed based on this concept a our algorithm can be adversely affected by the overlapping in the micro cluster we proposed a meaningful cut plane solution for overlapping data the formal analysis and experiment show that this method can achieve good performance in finding the most outstanding local outlier 
abstract this paper is concerned with the problem of detecting outlier from unlabeled data in prior work we have developed smartsifter which is an on line outlier detection algorithm based on unsupervised learning from data on the basis of smartsifter this paper yield a new framework for outlier filtering using both supervised and unsupervised learning technique iteratively in order to make the detection process more effective and more understandable the outline of the framework is a follows in the first round for an initial dataset we run smartsifter to give each data a score with a high score indicating a high possibility of being an outlier next giving positive label to a number of higher scored data and negative label to a number of lower scored data we create labeled example then we construct an outlier filtering rule by supervised learning from them here the rule is generated based on the principle of minimizing extended stochastic complexity in the second round for a new dataset we filter the data using the constructed rule then among the filtered data we run smartsifter again to evaluate the data in order to update the filtering rule applying of our framework to the network intrusion detection we demonstrate that it can significantly improve the accuracy of smartsifter and outlier filtering rule can help 
redundancy reduction on the basis of the second order statisticsof natural image ha been very successful in accounting for thepsychophysics of low level vision here we study the second orderstatistics of natural sound ensemble using principal componentanalysis pca their eigen spectrum exhibit a finite size scalingbehavior a a function of the window size with universality afterthe millisecond range in contrast with natural scene auditory spectrum do not universally 
we introduce total wire length a salient complexity measure for an analysis of the circuit complexity of sensory processing in biological neural system and neuromorphic engineering furthermore we introduce a set of basic computational problem that apparently need to be solved by circuit for translationand scale invariant sensory processing finally we exhibit a number of circuit design strategy for these new benchmark function that can be implemented within realistic complexity bound in particular with linear or almost linear total wire length 
hard disk drive failure are rare but are often costly the ability to predict failure is important to consumer drive manufacturer and computer system manufacturer alike in this paper we investigate the ability of two bayesian method to predict disk drive failure based on measurement of drive internal condition we firstview the problem from an anomaly detection stance we introduce a mixture model of naive bayes submodels i e cluster that is trained using expectation maximization the second method is a naive bayes classifier a supervised learning approach both method are tested on realworld data concerning drive the predictive accuracy of both algorithm is far higher than the accuracy of thresholding method used in the disk drive industry today 
this paper present a method for obtaining class membership probability estimate for multiclass classification problem by coupling the probability estimate produced by binary classifier this is an extension for arbitrary code matrix of a method due to hastie and tibshirani for pairwise coupling of probability estimate experimental result with boosted naive bayes show that our method produce calibrated class membership probability estimate while having similar classification accuracy a loss based decoding a method for obtaining the most likely class that doe not generate probability estimate 
we consider the problem of designing a linear transformation of rank which project the feature of a classifier onto such a to achieve minimum bayes error or probability of misclassification two avenue will be explored the first is to maximize the average divergence between the class density and the second is to minimize the union bhattacharyya bound in the range of while both approach yield similar performance in practice they outperform standard lda feature and show a relative improvement in the word error rate over state of the art cepstral feature on a large vocabulary telephony speech recognition task 
the question of whether the nervous system produce movement through the combination of a few discrete element ha long been central to the study of motor control muscle synergy i e coordinated pattern of muscle activity have been proposed a possible building block here we propose a model based on combination of muscle synergy with a specific amplitude and temporal structure time varying synergy provide a realistic basis for the decomposition of the complex pattern observed in natural behavior to extract time varying synergy from simultaneous recording of emg activity we developed an algorithm which extends existing non negative matrix factorization technique 
we describe an empirical study of an adaptive hierarchical vision system using a simplevision task requiring both low level andhigh level processing we examined how threeschemes of feedback for on line learning affectedthe true positive rate the number ofinstances used for learning and the need foruser feedback the rst scheme used forlearning those instance for which the userprovided feedback the second used all instance assuming that no feedback meantcorrect 
we present a new approach to the supervised learning of lateral interaction for the competitive layer model clm dynamic feature binding architecture the method is based on consistency condition which were recently shown to characterize the attractor state of this linear threshold recurrent network for a given set of training example the learning problem is formulated a a convex quadratic optimization problem in the lateral interaction weight an efficient dimension reduction of the learning problem can be achieved by using a linear superposition of basis interaction we show the successful application of the method to a medical image segmentation problem of fluorescence microscope cell image 
we propose a simple general purpose method that take a input a set of instance and a collection of web page and produce a set of new feature defined over the given instance each generated feature corresponds either to a word from an html header that appears to modify an instance or else to a quot position quot in an html document that appears to contain an instance in learning to classify musical artist for example the generated feature g classical might be true for all instance that 
we present deterministic greedy algorithm for building sparse nonlinear regression model from observational data our objective is to develop ecient numerical scheme for reducing the training and runtime complexity of nonlinear regression technique applied to massive datasets in the spirit of natarajan s greedy algorithm natarajan we iteratively minimize a loss function subject to a speci ed constraint on the degree of sparsity required of the nal model or an upper bound on the 
sequence of event are an important type of data arising in various application including telecommunication bio statistic web access analysis etc a basic approach to modeling such sequence is to find the underlying intensity function describing the expected number of event per time unit typically the intensity function are assumed to be piecewise constant we therefore consider different way of fitting intensity model to event sequence data we start by considering a bayesian approach using markov chain monte carlo mcmc method with varying number of piece these method can be used to produce posterior distribution on the intensity function and they can also accomodate covariates the drawback is that they are computationally intensive and thus are not very suitable for data mining application in which large number of intensity function have to be estimated we consider dynamic programming approach to finding the change point in the intensity function these method can find the maximum likelihood intensity function in o n k time for a sequence of n event and k different piece of intensity we show that simple heuristic can be used to prune the number of potential change point yielding speedup of several order of magnitude the result of the improved dynamic programming method correspond very closely with the posterior average produced by the mcmc method 
imitation is actively being studied a an effective mean of learning in multi agent environment it allows an agent to learn how to act well perhaps optimally by passively observing the action of cooperative teacher or other more experienced agent it environment we propose a straightforward imitation mechanism called model extraction that can be integrated easily into standard model based reinforcement learning algorithm roughly by observing a mentor with similar capability an agent can extract information about it own capability in unvisited part of state space the extracted information can accelerate learning dramatically we illustrate the benefit of model extraction by integrating it with prioritized sweeping and demonstrating improved performance and convergence through observation of single and multiple mentor though we make some stringent assumption regarding observability possible interaction and common ability we briefly comment on extension of the model that relax these 
this paper describes how we used regression rule to improve upon a result previously published in the earth science literature in such a scienti c application of machine learning it is crucially important for the learned model to be understandable and communicable we recount how we selected a learning algorithm to maximize communicability and then describe two visualization technique that we developed to aid in understanding the model by exploiting the spatial nature of the data we also 
abstract we calculate lower bound on the size of sigmoidal neural network thatapproximate continuous function in particular we show that for the approximationof polynomial the network size ha to grow a omega gamma k where ki the degree of the polynomial this bound is valid for any input dimension i e independently of the number of variable the result is obtained by introducinga new method employing upper bound on the vapnik chervonenkisdimension for proving lower 
we introduce and discus a local methodto learn one step ahead predictor for iteratedtime series forecasting for each singleone step ahead prediction our methodselects among different alternative a localmodel representation on the basis of a localcross validation procedure in the literature local learning is generally used forfunction estimation task which do not taketemporal behavior into account our techniqueextends this approach to the problem oflong horizon 
suppose you are given some dataset drawn from an underlying probability distribution and you want to estimate a simple subset of input space such that the probability that a test point drawn from lie outside of equal some a priori specified between and we propose a method to approach this problem by trying to estimate a function which is positive on and negative on the complement the functional form of is given by a kernel expansion in term of a potentially small subset of the training data it is regularized by controlling the length of the weight vector in an associated feature space we provide a theoretical analysis of the statistical performance of our algorithm the algorithm is a natural extension of the support vector algorithm to the case of unlabelled data 
determining the relationship between the activity of a single nervecell to that of an entire population is a fundamental question thatbears on the basic neural computation paradigm in this paperwe apply an information theoretic approach to quantify the levelof cooperative activity among cell in a behavioral context it ispossible to discriminate between synergetic activity of the cell v redundant activity depending on the difference between the informationthey provide when measured 
decision theoretic reasoning and planning algorithmsare increasingly being used for mobilerobot navigation due to the significantuncertainty accompanying the robot perceptionand action such algorithm requiredetailed probabilistic model of the environmentof the robot and it is very desirable toautomate the process of compiling such modelsby mean of autonomous learning algorithm this paper compare experimentallyfour learning method in combination withfour heuristic 
the traditional kalman filter can be viewed a a recursive stochastic algorithm that approximates an unknown function via a linear combination of prespecified basis function given a sequence of noisy sample in this paper we generalize the algorithm to one that approximates the fixed point of an operator that is known to be a euclidean norm contraction instead of noisy sample of the desired fixed point the algorithm update parameter based on noisy sample of function generated by application of the operator in the spirit of robbins monro stochastic approximation the algorithm is motivated by temporal difference learning and our development lead to a possibly more efficient variant of temporal difference learning we establish convergence of the algorithm and explore efficiency gain through computational experiment involving optimal stopping and queueing problem 
learning a complex task can be significantly facilitated by defining a hierarchy of subtasks an agent can learn to choose between various temporally abstract action each solving an assigned subtask to accomplish the overall task in this paper we study hierarchical learning using the framework of option we argue that to take full advantage of hierarchical structure one should perform option specific state abstraction and that if this is to scale to larger task state abstraction should be automated we adapt mccallum s u tree algorithm to automatically build option specific representation of the state feature space and we illustrate the resulting algorithm using a simple hierarchical task result suggest that automated option specific state abstraction is an attractive approach to making hierarchical learning system more effective 
in this contribution we transfer a customer purchase incidence model for consumer product which is based on ehrenberg s repeat buying theory to web based information product ehrenberg s repeat buying theory successfully describes regularity on a large number of consumer product market we show that these regularity exist in electronic market for information good too and that purchase incidence model provide a well founded theoretical base for recommender and alert service the article consists of two part in the first part ehrenberg s repeat buying theory and it assumption are reviewed and adapted for web based information market second we present the empirical validation of the model based on data collected from the information market of the virtual university of the vienna university of economics and business administration at http vu wu wien ac at from september to may i introduction in this article we concentrate on an anonymous recommender service of the correlation type made famous by amazon com applied to an information broker it is based on consumption pattern for information good web site from market basket web browser session which we treat a consumer purchase history with unobserved consumer identity in resnick and varian s design space this recommender service is characterized a the content of a recommendation consists of link to web site it is an implicit service based on observed user behav 
game site on the world wide web draw people from around the world with specialized interest skill and knowledge data from the game often reflects the player expertise and will to win we extract probabilistic forecast from data obtained from three online game the hollywood stock exchange hsx the foresight exchange fx and the formula one pick six f p competition we find that all three yield accurate forecast of uncertain future event in particular price of so called movie stock on hsx are good indicator of actual box office return price of hsx security in oscar emmy and grammy award correlate well with observed frequency of winning fx price are reliable indicator of future development in science and technology collective prediction from player in the f competition serve a good forecast of true race outcome in some case forecast induced from game data are more reliable than expert opinion we argue that web game naturally attract well informed and well motivated player and thus offer a valuable and oft overlooked source of high quality data with significant predictive value 
we present evidence that several higher order statistical propertiesof natural image and signal can be explained by a stochasticmodel which simply varies scale of an otherwise stationary gaussianprocess we discus two interesting consequence the rstis that a variety of natural signal can be related through a commonmodel of spherically invariant random process which havethe attractive property that the joint density can be constructedfrom the one dimensional marginal 
while it ha recently become possible to build spoken dialogue system that interact with user in real time in a range of domain system that support conversational natural language are still subject to a large number of spoken language understanding slu error endowing such system with the ability to reliably distinguish slu error from correctly understood utterance might allow them to correct some error automatically or to interact with user to repair them thereby improving the system s overall performance we report experiment on learning to automatically distinguish slu error in spoken utterance collected in a field trial of at t s how may i help yousystem interacting with live customer traffic we apply the automatic classifier ripper cohen to train an slu classifier using feature that are automatically obtainable in real time the classifer achieves accuracy on this task an improvement of over the majority class baseline we show that the most important feature are those that the natural language understanding module can compute suggesting that integrating the trained classifier into the nlu module of the how may i help you system should be straightforward 
we present an information geometric measure to systematicallyinvestigate neuronal firing pattern taking account not only ofthe second order but also of higher order interaction we beginwith the case of two neuron for illustration and show how to testwhether or not any pairwise correlation in one period is significantlydifferent from that in the other period in order to test such a hypothesisof different firing rate the correlation term need to besingled out orthogonally 
we introduce a new shape descriptor the shape context for correspondence recovery and shape based object recognition the shape context at a point capture the distribution over relative position of other shape point and thus summarizes global shape in a rich local descriptor shape context greatly simplify recovery of correspondence between point of two given shape moreover the shape context lead to a robust score for measuring shape similarity once shape are aligned the shape 
abstract a new paradigm is proposed for sorting spike in multi electrode data using ratio of transfer function between cell and electrode it is assumed that for every cell and electrode there is a stable linear relation these are dictated by the property of the tissue the electrode and their relative geometry the main advantage of the method is that it is insensitive to variation in the shape and amplitude of a spike spike sorting is carried out in two separate step first template describing the statistic of each spike type are generated by clustering transfer function ratio then spike are detected in the data using the spike statistic these technique were applied to data generated in the escape response system of the cockroach 
in many data mining domain misclassification cost are different for different example in the same way that class membership probability are example dependent in these domain both cost and probability are unknown for test example so both cost estimator and probability estimator must be learned after discussing how to make optimal decision given cost and probability estimate we present decision tree and naive bayesian learning method for obtaining well calibrated probability estimate we then explain how to obtain unbiased estimator for example dependent cost taking into account the difficulty that in general probability and cost are not independent random variable and the training example for which cost are known are not representative of all example the latter problem is called sample selection bias in econometrics our solution to it is based on nobel prize winning work due to the economist james heckman we show that the method we propose perform better than metacost and all other known method in a comprehensive experimental comparison that us the well known large and challenging dataset from the kdd data mining contest 
this paper present predictive gain scheduling a technique for simplify ing reinforcement learning problem by decomposition link admission control of self similar call traf fic is used to demonstrate the technique the control problem is decomposed into on line prediction of near future call arrival rate and precomputation of policy for poisson call ar rival process at decision time the prediction are used to select among the policy simulation show that this technique result in sig nificantly faster learning without any performance loss compared to a reinforcement learning controller that doe not decompose the problem 
we present an improved bound on the difference between training and test error for voting classiers this improved averaging bound provides a theoretical justication for popular averaging technique such a bayesian classic ation maximum entropy discrimination winnow and bayes point machine and ha implication for learning algorithm design 
there are many hierarchical clustering algorithm available but these lack a firm statistical basis here we set up a hierarchical probabilistic mixture model where data is generated in a hierarchical tree structured manner markov chain monte carlo mcmc method are demonstrated which can be used to sample from the posterior distribution over tree containing variable number of hidden unit 
hebbian learning rule are generally formulated a static rule under changing condition e g neuromodulation input statistic most rule are sensitive to parameter in particular recent work ha focused on two different formulation of spike timing dependent plasticity rule additive stdp is remarkably versatile but also very fragile whereas multiplicative stdp is more robust but lack attractive feature such a synaptic competition and rate stabilization here we address 
building intelligent system that are capable of extracting high level representation from high dimensional data lie at the core of solving many ai related task including visual object or pattern recognition speech perception and language understanding theoretical and biological argument strongly suggest that building such system requires deep architecture that involve many layer of nonlinear processing many existing learning algorithm use shallow architecture including neural network with only one hidden layer support vector machine kernel logistic regression and many others the internal representation learned by such system are necessarily simple and are incapable of extracting some type of complex structure from high dimensional input in the past few year researcher across many different community from applied statistic to engineering computer science and neuroscience have proposed several deep hierarchical model that are capable of extracting meaningful high level representation an important property of these model is that they can extract complex statistical dependency from data and efficiently learn high level representation by re using and combining intermediate concept allowing these model to generalize well across a wide variety of task the learned high level representation have been shown to give state of the art result in many challenging learning problem and have been successfully applied in a wide variety of application domain including visual object recognition information retrieval natural language processing and speech perception a few notable example of such model include deep belief network deep boltzmann machine deep autoencoders and sparse coding based method the goal of the tutorial is to introduce the recent development of various deep learning method to the kdd community the core focus will be placed on algorithm that can learn multi layer hierarchy of representation emphasizing their application in information retrieval object recognition and speech perception 
we formulate a model for probability distribution on image space we show that any distribution of image can be factored exactly into conditional distribution of feature vector at one resolution pyramid level conditioned on the image information at lower resolution we would like to factor this over position in the pyramid level to make it tractable but such factoring may miss long range dependency to capture long range dependency we introduce hidden class label at each pixel in the pyramid the result is a hierarchical mixture of conditional probability similar to a hidden markov model on a tree the model parameter can be found with maximum likelihood estimation using the em algorithm we have obtained encouraging preliminary result on the problem of detecting various object in sar image and target recognition in optical aerial image 
