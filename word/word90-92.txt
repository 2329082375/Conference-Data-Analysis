traditional application of truth maintenance system tm fail to adequately represent heuristic search in which some path are initially preferred what they miss is the idea of switching context rationally based on heuristic preference we show that it is useful especially for plan with contingency to maintain the validity of the reason for context choice and rejection we demonstrate how to do so with a problem solver tm architecture called redux 
most approach to model based diagnosis describe a diagnosis for a system a a set of failing component that explains the symptom in order to characterize the typically very large number of diagnosis usually only the minimal such set of failing component are represented this method of characterizing all diagnosis is inadequate in general in part because not every superset of the faulty component of a diagnosis necessarily provides a diagnosis in this paper we analyze the notion of diagnosis in depth exploiting the notion of implicate implicant and prime implicate implicant we use these notion to propose two alternative approach for addressing the inadequacy of the concept of minimal diagnosis first we propose a new concept that of kernel diagnosis which is free of the problem of minimal diagnosis second we propose to restrict the axiom used to describe the system to ensure that the concept of minimal diagnosis is adequate 
one of the question in understanding the rela tion between circumscription and consistencybased nonmonotonic logic is can default logic be expressed in circumscription while it seems impossible to express default logic in ex isting form of circumscripti on is it neverthe le possible to express default logic in a certain extension of circumscripti on this paper present a construction of default logic in the spirit of circumscripti on it ha been shown that the new formalism circumscriptive exten sion is indeed an extension of circumscripti on the equivalence of the new formalism and default logic is shown to hold under certain con ditions which demonstrates that default logic can be expressed by merely classical logic with a fixed point operator 
although computer are widely used to simulate complex physical system crafting the underlying model that enable computer analysis remains difficult when a model is created for one task it is often impossible to reuse the model for another purpose because each task requires a different set of simplifying assumption by representing modeling assumption explicitly a approximation reformulations we have developed qualitative technique for switching between model we assume that automated reasoning proceeds in three phase model selection quantitative analysis using the model and validation that the assumption underlying the model were appropriate for the task at hand if validation discovers a serious discrepancy between predicted and observed behavior a new model must be chosen we present a domain independent method for performing this model shift when the model are related by an approximation reformulation and describe a common lisp implementation of the theory 
the author compare the photometric effect of motion which is defined a the variation of a point s imaged intensity a a consequence of motion and the geometric effect of motion which is defined a the variation in projected surface geometry a a consequence of motion it is shown that photometric motion provides a cue to surface shape that is potentially a useful a that provided by geometric motion a simple technique for using this photometric motion information to extract both surface shape and reflectance is developed and a biological implementation is proposed how this photometric motion mechanism can be integrated with and used to enhance existing structure motion algorithm is discussed intensity information is sometimes even more important than geometric distortion when estimating the shape of a single continuous surface that is rotating relative to the observer s frame of reference 
unification based nl parser that copy argument graph to prevent their destruction suffer from inefficiency copying is the most expensive operation in such parser and several method to reduce copying have been devised with varying degree of success lazy unification is presented here a a new conceptually elegant solution that reduces copying by nearly an order of magnitude lazy unification requires no new slot in the structure of node and only nominal revision to the unification algorithm 
conventional blind search technique generally assume that the goal node for a givenproblem are distributed randomly along the fringe of the search tree we argue that thisis often invalid in practice and suggest that a more reasonable assumption is that decisionsmade at each point in the search carry equal weight we go on to show that a new searchtechnique called iterative broadening lead to order of magnitude saving in the time neededto search a space satisfying this assumption the 
the concept of inductive bias can be broken down into the underlying assumption of the domain the particular implementation choice that restrict or order the space of hypothesis considered by the learning program the bias choice and the inductive policy that link the two we define inductive policy a the strategy used to make bias choice based on the underlying assumption inductive policy decision involve addressing trade offs with respect to different bias choice without addressing these tradeoff bias choice will be made arbitrarily from the standpoint of inductive policy we discus two issue not addressed much in the machine learning literature first we discus batch learning with a strict time constraint and present an initial study with respect to trading off predictive accuracy for speed of learning next we discus the issue of learning in a domain where different type of error have different associated cost risk we show that by using different inductive policy accuracy can be traded off for safety we also show how the value for the latter tradeoff can be represented explicitly in a system that adjusts bias choice with respect to a particular inductive policy 
general purpose truth maintenance system have receivedconsiderable attention in the past few year thispaper discus the functionality of truth maintenancesystems and compare various existing algorithm applicationsand direction for future research are also discussed introductionin jon doyle wrote a master thesis at the mit ailaboratory entitled quot truth maintenance system forproblem solving quot doyle in this thesis doyledescribed an independent module called a 
the purpose of this paper is to expand the syntax and semantics of logic program and deductive database to allow for the correct representation of incomplete information in the presence of multiple extension the language of logic program with classical negation epistemic disjunction and negation by failure is further expanded by a new modal operator k where for the set of rule t and formula f kf stand for f is known to a reasoner with a set of premise t theory containing such an operator will be called strongly introspective we will define the semantics of such theory which expands the semantics of deductive database from gelfond and lifschitz bd and demonstrate the applicability of strongly introspective theory to formalization of some form of commonsense reasoning 
plan recognition requires the construction of possible plan which could explain a set of observed action and then selecting one or more of them a providing the belt explanation in this paper we present a formal model of the latter process based upon probability theory our model consists of a knowledge base of fact about the world expressed in a first order language and rule for using that knowledge base to construct a bayesian network the network is then evaluated to find the plan with the highest probability 
constraint satisfaction problem csps involve finding value for variable subject to constraint on which combination of value are permitted this paper develops a concept of interchangeability of csp value fully interchangeable value can be substituted for one another in solution to the problem removing all but one of a set of fully interchangeable value can simplify the search space for the problem without effectively losing solution refinement of the interchangeability concept extend it applicability basic property of interchangeablity and complexity parameter are established a hierarchy of local interchangeability is defined that permit recognition of some interchangeable value with polynomial time local computation computing local interchangeability at any level in this hierarchy to remove value before backtrack search is guaranteed to be cost effective for some csps several form of weak interchangeability are defined that permit eliminating value without losing all solution interchangeability can be introduced by grouping value or variable and can be recalculated dynamically during search the idea of interchangeability can be abstracted to encompass any mean of recovering the solution involving one value from the solution involving another 
we describe a robot control architecture which combine a stimulus response subsystem for rapid reaction with a search based planner for handling unanticipated situation the robot agent continually chooses which action it is to perform using the stimulus response subsystem when possible and falling back on the planning subsystem when necessary whenever it is forced to plan it applies an explanation based learning mechanism to formulate a new stimulus response rule to cover this new situation and others similar to it with experience the agent becomes increasingly reactive a it learning component acquires new stimulus response rule that eliminate the need for planning in similar subsequent situation this theo agent architecture is described and result are presented demonstrating it ability to reduce routine reaction time for a simple mobile robot from minute to under a second 
thanks to two stronger version of predicate circumscription one of the best known non monotonic reasoning method we give a definitive answer to two old open problem the first one is the problem of expressing do main circumscription in term of predicate cir cumscription the second one is the problem of definability of the circumscribed predicate asked by doyle in and never answered since these two result and the way used to obtain them could help an automatic circumscriptor 
this paper investigates the effect of parallelism on blackboard system scheduling a parallel blackboard system is described that allows multiple knowledge source instantiation to execute in parallel using a shared memory blackboard approach new class 
this paper describes the application of explanation based learning a machine learning technique to the sri core language engine a large scale general purpose natural language analysis system the idea is to bypass normal morphological syntactic and 
two independent mechanism of context change have been discussed separately in the literature context change by that make use of it and discus our initial implementation of these idea 
we develop representation for locative and path specifying preposition emphasizing the implementability of the underlying semantic primitive our primitive pertain to mechanical characteristic such a geometric relationship among object kinematic or motional characteristic implied by preposition the representation along with representation for action verb along similar line have been used to successfully animate the performance of task underlying natural language imperative by human agent 
although relevance feedback technique have been investigated for more than year hardly any of these technique ha been implemented in a commercial full text document retrieval system in addition to pure performance problem this is due to the fact that the application of relevance feedback technique increase the complexity of the user interface and thus also the use of a document retrieval system in this paper we concentrate on a relevance feedback technique that allows easily understandable and manageable user interface and at the same time provides high quality retrieval result moreover the relevance feedback technique introduced unifies a well a improves other well known relevance feedback technique 
computer and thought are the two category that together define artificial intelligence a a discipline it is generally accepted that work in artificial intelligence over the last thirty year ha had a strong influence on aspect of computer architecture in this paper we also make the converse claim that the state of computer architecture ha been a strong influence on our model of thought the von neumann model of computation ha lead artificial intelligence in particular direction intelligence in biological system is completely different recent work in behavior based artificial intelligence ha produced new model of intelligence that are much closer in spirit to biological system the non von neumann computational model they use share many characteristic with biological computation 
this paper present an efficient and homogeneous paradigm for automatic acquisition and recognition of nonparametric shape acquisition time varies from linear to cubic in the number of object feature recognition time is linear to cubic in the number of feature in the image and grows slowly with the number of stored model nonparametric shape representation is achieved by spatial autocorrelation transforms both acquisition and recognition are two step process in the first phase spatialautocorrelationoperators are applied to the image data to perform local shape analysis then spatial autocorrelation operator are applied to the local shape descriptor to either create entry acquisition or index recognition into a table containing the distributed shape information the output of the table is used to generate a density function on the space of possible shape with peak corresponding to high confidence in the presence of a particular shape instance the behavior of the system on a set of complex shape is shown with respect to occlusion geometric transformation and cluttered scene 
this paper show how using a nonmonotonic logic to describe the effect of action enables plausible plan to be discovered quickly and then refined if time permit candidate plan are found by allowing them to depend on unproved assumption the nonmonotonic logic make explicit which antecedent of rule have the status of default condition and they are the only one that may be left unproved so only plausible candidate plan are produced these are refined incrementally by trying to justify the assumption on which they depend the new planning strategy ha been implemented with good experimental result 
the paper introduces a dependency based grammar and the associated parser and focus on the problem of determinism in parsing and recovery from error first it is shown how dependency based parsing can be afforded by taking into account the suggestion coming from other approach and the preference criterion for parsing are breifly addressed second the issue of the interconnection between the syntactic analysis and the semantic interpretation in incremental processing are discussed and the adoption of a tm for the recovery of the processing error is suggested 
autoepistemic ae logic is a formal system characterizing agent that have complete introspective access to their own belief ae logic relies on a fixed point definition that ha two significant part the first part is a set of assumption or hypothesis about the content of the fixed point the second part is a set of reflection principle that link sentence with statement about their provability we characterize a family of ideal ae reasoner in term of the minimal hypothesis that they can make and the weakest and strongest reflection principle that they can have while still maintaining the interpretation of ae logic a self belief these result can help in analyzing metatheoretic system in logic programming 
holographic recurrent network hrns are recurrent network which incorporate associative memory technique for storing seq uential structure hrns can be easily and quickly trained using gradient descent technique to generate sequence of discrete output and trajec tory through continuous space the performance of hrns is found to be superior to that of ordinary recurrent network on these sequence gener ation task 
we describe a computer program which understands a greyscale image of a face wellenough to locate individual face feature such a eye and mouth the program ha twodistinct component module designed to locate particular face feature usually in arestricted area and the overall control strategy which activates module on the basis ofthe current solution state and ass and integrates the result of each module 
this paper describes a method by which the epipolar line equation for binocular stereo i e the invariant relating the image coordinate of corresponding image point can be estimated directly by analyzing the image no camera calibration or detailed knowledge of the stereo geometry is required 
existing approach to text generation fail to consider howinteractions with the user may be managed within a coherentexplanation or description this paper present anapproach to generating such interactive explanation basedon two level of discourse planning content planning anddialogue planning the system developed allows aspect ofthe changing context to be monitored with an explanation and the developing explanation to depend on this changingcontext interruption from the 
we investigate the use of interpretation tree to solve the correspondenceproblem for a mobile robot fusing data from a range image intoa world model consisting of planar surface patch uncertainty is handledby stochastic technique where error are represented by normal jointprobability distribution we show that for problem of a typical size thesearch time is too long unless the world model can be structured into partsonly one of which can be occupied by the robot at any given 
one way to speed up reinforcement learning is to enable learning to happen simultaneously at multiple resolution in space and time this paper show how to create a learning managerial hierarchy in which high level manager learn how to set task to their sub manager who in turn learn how to satisfy them sub manager need not initially understand their manager command they simply learn to maximise their reinforcement in the context of the current command we illustrate the system using a simple maze task a the system learns how to get around satisfying command at the multiple level it explores more efficiently than standard flat learning and build a more comprehensive map 
computing surface curvature would seem to be a simple application of differential geometry but problem arise due to noise and the quantized nature of digital image we present a method for determining principal curvature and direction of surface estimated from three dimensional image we use smoothness constraint to connect different surface point and by then comparing information over local neighbourhood we iteratively update the information at each point to ensure that this information is consistent over the estimated surface 
an important and readily available source of knowledge for common sense reasoning is partial description of specific experience knowledge base kb containing such information are called episodic knowledge base ekb aggregation of episodic knowledge provide common sense knowledge about the unobserved property of new experience such knowledge is retrieved by applying statistic to a relevant subset of the ekb called the reference class i study a manner in which a corpus of experience can be represented to allow common sense retrieval which is flexible enough to allow the common sense reasoner to deal with new experience and in the simplest case reduces to efficient database look up i define two first order dialect l and ql l is used to represent experience in an episodic knowledge base an extension ql is used for writing query to ekbs 
two new parsing algorithm for context free phrase structure grammar are presented which perform a bounded amount of processing per word per analysis path independently of sentence length they are thus capable of parsing in real time in a parallel implementation which fork processor in response to non deterministic choice point 
this paper synthesizes general constraint satisfaction and classical ai planning into a theory of incremental change that account for multiple objective and contingency the hypothesis is that this is a new and useful paradigm for problem solving and re solving a truth maintenance based architecture derived from the theory is useful for contingent assignment problem such a logistics planning 
recently there ha been much criticism in the ai community that knowledge based system are not situated we argue that trying to provide for situatedness in a conventional system will lead to the so called model explosion cycle and that for most application environment adaptivity is needed for situated behavior cooperative system are a solution to the model explosion cycle where adaptivity is delegated to the user a hybrid symbolic connectionist system offer self tuning capability and therefore adaptivity but can t cope with the model explosion cycle integrating both approach into a cooperative hybrid system lead to a much more situated behavior than conventional system can achieve our approach is illustrated using a real life expert system in the domain of technical troubleshooting ongoing practical test indicate that a cooperative hybrid design present an attractive architecture for knowledge based system 
in this paper we define the concept of logical consistency of belief among a group of computational agent that are able to reason nonmonotonically we then provide an algorithm for truth maintenance that guarantee local consistency for each agent and global consistency for data shared by the agent furthermore we show the algorithm to be complete in the sense that if a consistent state exists the algorithm will either find it or report failure the algorithm ha been implemented in the rad distributed expert system shell 
researcher have found relevance feedback to be effective in interactive information retrieval although few formal user experiment have been made in order to run a user experiment on a large document collection experiment were performed at nist to complete some of the missing link found in using the probabilistic retrieval model these experiment using the cranfield collection showed the importance of query expansion in addition to query reweighting and showed that adding a few a well selected term could result in performance improvement of over additionally it wa shown that performing multiple iteration of feedback is highly effective 
consideration of when right association work and when it fails lead to a restatement of this parsing principle in term of the notion of heaviness a computational investigation of a syntactically annotated corpus provides evidence for this proposal and suggest circumstance when ra is likely to make correct attachment prediction 
we present an approach to unsupervised concept fonnation based on accumulation of partial regularity using an algorithmic complexity framework we defme regularity a a model that achieves a compressed coding of data we discus induction of model we present induction of finite automaton model for regularity of string and induction of model based on vector translation for set of point the concept we develop are particularly appropriate for natural space structure that accept a decomposition into recurrent recognizable part they are usually hierarchical and suggest that a vocabulary of basic constituent can be learned before focussing on how they are assembled we define and illustrate basic regularity a algorithmically independent building block of structure they are identifiable a local maximum of compression a a function of model complexity stepwise induction consists in finding a model using it to compress the data then applying the same procedure on the code it is a way to induce in polynomial time structure whose basic component have bound complexity library are set of partial regularity a theoretical basis for clustering and concept formation finally we use the above concept to present a new perspective on explanation based generalization we prove it to be a language independent method to specialize the background knowledge 
this paper present the result of converting a standard greham harrison ruzzo ghr parser for a unification grammar into an agenda driven parsing system the agenda is controlled by statistical measure of grammar rule likelihood obtained from a training set the technique in the agenda parser lead to substantial reduction in chart size and parse time and can be applied to any chart based parsing algorithm without hand tuning 
automatic recognition of spoken letter is one of the most challenging task in the field of computer speech recognition the difficulty of the task is due to the acous tic similarity of many of the letter accurate recognition requires the system to perform fine phonetic distinction such a s b v d b v p d v t t v s g c v z v v z m v n and j v k the ability to per form fine phonetic distinction to discriminate among the minimal sound unit of the language is a fundamen tal unsolved problem in computer speech recognition 
this paper present a simple sound complete and systematic algorithm for domain independent strip planning simplicity is achieved by starting with a ground procedure and then applying a general and independently verifiable lifting transformation previous planner have been designed directly a lifted procedure our ground procedure is a ground version of tate s nonlin procedure in tate s procedure one is not required to determine whether a prerequisite of a step in an unfinished plan is guaranteed to hold in all linearizations this allows tate s procedure to avoid the use of chapman s modal truth criterion systematicity is the property that the same plan or partial plan is never examined more than once 
this work pertains to the knuth bendix kb algorithm which try to find a complete set of reduction from a given set of equation in the kb algorithm a term ordering is employed and it is required that every equation be orientable in the sense that the left hand side be greater than the right the kb algorithm halt if a non orientable equation is produced a generalization of the kb algorithm ha recently been developed in which every equation is orientable and which halt only when a complete set is generated in the generalization a constraint is added to each equation the constraint governs when the equation can be used a a reduction the constraint is obtained from the equation by solving the term inequality left hand side right hand side to understand what it mean to solve a term inequality consider the analogy with algebra in which solving term equality i e unification is analogous to solving algebraic equality then solving term inequality is analogous to solving algebraic inequality thus the solution of term inequality relates to unification a the solution of algebraic inequality relates to the solution of algebraic equality we show how to solve term inequality when using the lexicographic path ordering 
we present two concept language called pl and pl which are extension of tc we prove that the subsumption problem in these language can be solved in polynomial time both language include a construct for express ing inverse role which ha not been considered up to now in tractable language in addition pl includes number restriction and negation of primitive concept while pl includes role conjunction and role chaining by exploiting recent complexity result we show that none of the construct usually considered in concept language can be added to pl and pl without losing tractabtlity there fore on the assumption that language are characterized by the set of construct they provide the two language pre sented in this paper provide a solution to the problem of singling out an optimal trade off between expressive power and computational complexity 
we propose a new csp formalism that incorporates hard constraint and preference so that the two are easily distinguished both conceptually and for purpose of problem solving preference are represented a a lexicographic order over variable and domain value respectively constraint are treated in the usual manner therefore these problem can be solved with ordinary csp algorithm with the proviso that complete algorithm cannot terminate search after finding a feasible solution except in the important case of heuristic that follow the preference order lexical order we discus the relation of this problem representation to other formalism that have been applied to preference including soft constraint formalism and cp net we show how algorithm selection can be guided by work on phase transition which serve a a useful marker for a reversal in relative efficiency of lexical ordering and ordinary csp heuristic due to reduction in number of feasible solution we also consider branch and bound algorithm and their anytime property finally we consider partitioning strategy that take advantage of the implicit ordering of assignment in these problem to reduce the search space 
recently linsker and mackay and miller have analysed hebbian correlational rule for synaptic development in the visual system and miller ha studied such rule in the case of two population of fibre particularly two eye miller s analysis ha so far assumed that each of the two population ha exactly the same correlational structure relaxing this constraint by considering the effect of small perturbative correlation within and between eye permit study of the stability of the solution we predict circumstance in which qualitative change are seen including the production of binocularly rather than monocularly driven unit 
temporal projection predicting future state of a changing world ha been studied mainly a a formal problem researcher have been concerned with getting the concept of causality and change right and have ignored the practical issue surrounding projection in planning for example when the effect of a plan s action depend on the prevailing state of the world and that state of the world is not known with certainty projecting the plan may generate an exponential number of possible outcome this problem ha traditionally been eliminated by restricting the domain so the world state is always known and by restricting the action representation so that either the action s intended effect is realized or the action cannot be projected at all we argue against these restriction and instead present a system that represents and reason about an uncertain world support a representation that allows context sensitive action effect and generates projection that reflect only the significant or relevant outcome of the plan where relevance is determined by the planner s query about the resulting world state 
a genetic algorithm is used for learning qualitative model based on the qsim formalism hierarchical representation enables formation of submodels relevant for induction of domain explanation daring the search for better coding of the candidate in parallel with the search for better solution the sise and shape of candidate solution are dynamically created optimisation is based on the maximisation of the number of example covered by a candidate solution combined with the minimisation of the number of constraint used in the solution the result of learning is a set of model of different specificity that explain all given example an experiment in learning a qualitative model of the connected container system u tube is described in detail several solution equivalent to the original model were discovered 
in the right situation a speaker can use an unqualified indefinite description without being misunderstood this use of language is a kind of conversational implicature i e a non truth functional context dependent inference based upon language user awareness of principle of cooperative conversation i present a convention for identifying normal state implicatures which is based upon mutual belief of the speaker and hearer about certain property of the speaker s plan a key property is the precondition that an entity playing a role in the plan must be in a normal state with respect to the plan 
natural language processing nlp is the study of mathematical and computational modeling of various aspect of language and the development of a wide range of system these include spoken language system that integrate speech and natural language cooperative interface to database and knowledge base that model aspect of human human interaction multilingual interface machine translation and message understanding system among others research in nlp is highly interdisciplinary involving concept in computer science linguistics logic and psychology nlp ha a special role in computer science because many aspect of the field deal with linguistic feature of computation and nlp seek to model language computationally 
joint action by a team doe not consist merely of simultaneous and coordinated individual action to act together a team must be aware of and care about the status of the group effort a a whole we present a formal definition of what it could mean for a group to jointly commit to a common goal and explore how these joint commitment relate to the individual commitment of the team member we then consider the case of joint intention where the goal in question involves the team performing some action in both case the theory is formulated in a logical language of belief action and time previously used to characterize individual commitment and intention an important consequence of the theory is the type of communication among the team member that it predicts will often be necessary 
most ai researcher would i believe agree that truly intelligent machine i e machine on a par with human will require at least four order of magnitude more power and memory than are available on any machine today schwartz waltz there is now widespread agreement in the supercomputing community that by the year all supercomputer defined a the most powerful machine available at a given time will be massively parallel fox yet relatively little thought ha been given in ai a to how to utilize such machine with few exception ai s attention ha been limited to workstation minicomputer and pc today s massively parallel machine present ai with a golden opportunity to make an impact especially in the world of commercial application the most striking near term opportunity is in the marriage of research on very large database with case based and memory based ai moreover such application are step on a path that can lead eventually to a class of truly intelligent system 
we consider the case of heuristic search where the location of the goal may change during the course of the search for example the goal may be a target that is actively avoiding the problem solver we present a moving target search algo rithm mt to solve this problem we prove that if the average speed of the target is slower than that of the problem solver then the prob lem solver is guaranteed to eventually reach the target an implementatio n with randomly po sitioned obstacle confirms that the mt algo rithm is highly effective in various situation 
aspect of semantic interpretation such a quantifier scoping and reference resolution are often realised computationally by non monotonic operation involving loss of information and destructive manipulation of semantic representation the paper describes how monotonic reference resolution and scoping can be carried out using a revised quasi logical form qlf representation semantics for qlf are presented in which the denotation of formula are extended monotonically a qlf expression are resolved 
automating proof by induction is important in many computer science and artificial intelligence application in particular in program verification and specification system we present a new method to prove and disprove automatically inductive property given a set of axiom a well suited induction scheme is constructed automatically we call such a scheme a test set then for proving a property we just instanciate it with term from the test set and apply pure algebraic simplification to the result this method avoids completion and explicit induction however it retains their positive feature namely the completeness of the former and the robustness of the latter 
this paper present a plan based model that handle negotiation subdialogues by inferring both the communicative action that people pursue when speaking and the belief underlying these action we contend that recognizing the complex discourse action pursued in negotiation subdialogues e g expressing doubt requires both a multistrength belief model and a process model that combine different knowledge source in a unified framework we show how our model identifies the structure of negotiation subdialogues including recognizing expression of doubt implicit acceptance of communicated proposition and negotiation subdialogues embedded within other negotiation subdialogues 
this thesis investigates the problem of estimating the three dimensional structure of a scene from a sequence of image structure information is recovered from image continuously using shading motion or other visual mechanism a kalman filter represents structure in a dense depth map with each new image the filter first update the current depth map by a minimum variance estimate that best fit the new image data and the previous estimate then the structure estimate is predicted for the next time step by a transformation that account for relative camera motion experimental evaluation show the significant improvement in quality and computation time that can be achieved using this technique 
a fundamental assumption made in formulating optical flow algorithm is that motion at any point in an image can be represented a a single pattern component undergoing a simple translation even complex motion will look like uniform displacement when viewed through a sufficiently small window this assumption fails for a number of situation that commonly occur in real world image for example transparent surface moving past one another yield multiple motion component at a point 
the introduction of null unknown value in the relational database call for an extension of the theoretical foundation of the database model null are alien to classical logic in which the relational database model is rooted this ha led to all sort of counterintuitive ad hoc solution reasonable in one place but awkward in others a sound model theoretical foundation of null in a relational database based on modal logic is presented here the modal interpretation of query is easy to comprehend and intuitively correct partial interpretation which are to be preferred from a computational point of view are inadequate for arbitrary query formula that have an identical partial and modal interpretation are called safe safety guarantee on the one hand that the partial answer is meaningful and on the other hand that the modal interpretation is finitely computable the suitability of modal logic to model null is illustrated by a short discussion of the effect of null on database integrity 
example based programming is a form of software reuse in which existing code example are modified to meet current task need example based programming system that have enough exampies to be useful present the problem of finding relevant example a prototype system named codefinder which explores issue of retrieving software object relevant to the design task is presented codefinder support human computer dialogue by providing the mean to incrementally construct a query and by providing associative cue that are compatible with human memory retrieval principle 
most active and passive range finding techniquesyield unstructured and generally noisy d point in order to build useful world representation one must be able to remove spuriousdata point and group the remaining intomeaningful surface in this paper we propose an approach basedon fitting local surface differential propertiesof these surface are first used iterativelyto smooth the point and then to group theminto more global surface while eliminating error we present 
a well constructed thesaurus ha long been recognized a a valuable tool in the effective operation of an information retrieval system this paper report the result of experiment designed to determine the validity of an approach to the automatic construction of global thesaurus described originally by crouch in and based on a clustering of the document collection the author validate the approach by showing that the use of thesaurus generated by this method result in substantial improvement in retrieval effectiveness in four test collection the term discrimination value theory used in the thesaurus generation algorithm to determine a term s membership in a particular thesaurus class is found not to be useful in distinguishing a good from an indifferent or poor thesaurus class in conclusion the author suggest an alternate approach to automatic thesaurus construction which greatly simplifies the work of producing viable thesaurus class experimental result show that the alternate approach described herein in some case produce thesaurus which are comparable in retrieval effectiveness to those produced by the first method at much lower cost 
this paper investigates the effect of parallelism on blackboard system scheduling a parallel blackboard system is described that allows multiple knowledge source instantiation to execute in parallel using a shared memory blackboard approach new class 
recent research on reinforcement learning ha focused on algorithmsbased on the principle of dynamic programming dp one of the most promising area of application for these algorithmsis the control of dynamical system and some impressiveresults have been achieved however there are significant gapsbetween practice and theory in particular there are no convergenceproofs for problem with continuous state and action space or for system involving non linear function 
it ha been widely believed that qualitative analysis guide quantitative analysis while sufficient study ha not been made from technical viewpoint in this paper we present a case study with psx nl a program which autonomously analyzes the behavior of two dimensional nonlinear differential equation by integrating knowledge based method and numerical algorithm psx nl focus on geometric property of solution curve of ordinary differential equation in the phase space psx nl is designed based on a couple of novel idea a a set of flow mapping which is an abstract description of the behavior of solution curve and b a flow grammar which specifies all possible pattern of solution curve enabling psx n l to derive the most plausible interpretation when complete information is not available we describe the algorithm for deriving flow mapping 
this paper present a method of compressing and reconstructing a real image using it feature map and a feature catalogue that conprises of feature template representing the local form of feature found in a number of natural image unlike most context texture based technique that assume all feature profile at feature point to be some form of graded step this method is able to restore the shading in the neighbourhood of a feature point close to it original value whilst maintaining high compression ratio of around 
this paper introduces the causal reconstruction task the task of reading a causal description of a physical system forming an internal model of the specified behavior and answering question demonstrating comprehension and reasoning on the basis of the input description a representation called transition space is introduced in which event are depicted a path fragment in a space of transition or complex of change in the attribute of participating object by identifying partial match between the transition space representation of event a program called pathfinder is able to perform causal reconstruction on short causal description presented in simplified english simple transformation applied to event representation prior to matching enable the program to bridge discontinuity arising from the writer s use of analogy or abstraction the operation of pathfinder is illustrated in the context of a simple causal description extracted from the encyclopedia americana involving exposure of film in a camera 
the goal in automatic programming is to get a computer to perform a task by telling it what need to be done rather than by explicitly programming it this paper considers the task of automatically generating a computer program to enable an autonomous mobile robot to perform the task of moving a box from the middle of an irregular shaped room to the wall we compare the ability of the recently developed genetic programming paradigm to produce such a program to the reported ability of reinforcement learning technique such a q learning to produce such a program in the style of the subsumption architecture the computational requirement of reinforcement learning necessitates considerable human knowledge and intervention whereas genetic programming come much closer to achieving the goal of getting the computer to perform the task without explicitly programming it the solution produced by genetic programming emerges a a result of darwinian natural selection and genetic crossover sexual recombination in a population of computer program the process is driven by a fitness measure which communicates the nature of the task to the computer and it learning paradigm 
many recent object recognition system use a small number of pairing of data and model feature to compute the d transformation from a model coordinate frame into the sensor coordinate system in the case of perfect image data these system seem to work well with uncertain image data however the performance of such method is le well understood in this paper we examine the effect of two dimensional sensor uncertainty on the computation of three dimensional model transformation we 
in previous work zlotkin and rosenschein a we have developed a negotiation protocol and offered some negotiation strategy that are in equilibrium this negotiation process can be used only when the negotiation set n is not empty domain in which the negotiation set are never empty are called cooperative domain in general non cooperative domain the negotiation set is sometimes empty in this paper we present a theoretical negotiation model for rational agent in general non cooperative domain necessary and sufficient condition for cooperation are outlined by redefining the concept of utility we are able to enlarge the number of situation that have a cooperative solution an approach is offered for conflict resolution and it is shown that even in a conflict situation partial cooperative step can be taken by interacting agent that is agent in fundamental conflict might still agree to cooperate up to a certain point a unified negotiation protocol is developed that can be used in all case it is shown that in certain borderline cooperative situation a partial cooperative agreement i e one that doe not achieve all agent goal might be preferred by all agent even though there exists a rational agreement that would achieve all their goal 
a class of pose determination problem in which the sensory data are line and the corresponding reference data are plane is discussed the line considered are different from edge line in that they are not the intersection of boundary face of the object the author describes a polynomial approach that doe not require a priori knowledge about the object location closed form solution for orthogonal parallel and coplanar feature configuration of critical importance in real application are derived finding concerning the necessary and sufficient condition under which the line to plane pose determination problem can be solved are described 
a major obstacle to the widespread use of expert system in real time domain is the non predictability of response time while some researcher have addressed this issue by optimizing response time through better algorithm or parallel hardware there ha been little research towards run time prediction in order to meet user defined deadline to cope with the latter real time expert system must provide mechanism for estimating run time required to react to external event a a starting point for our investigation we chose the rete algorithm which is widely used for real time production system in spite of rete s combinatorial worst case match behavior we introduce a method forestimating match time in the rete network this paper show that simple profiling method do not work well but by going to a finer granularity we can get much better execution time prediction for basic action a well a for complete right hand side of rule our method is dynamically applied during the run time of the production system by using continuously updated statistical data of individual node in the rete network 
abstract classification method from statistical pattern recognition neural net and machine learning were applied to four real world data set each of these data set ha been previously analyzed and reported in the statistical medical or machine learning literature the data set are characterized by statisucal uncertainty there is no completely accurate solution to these problem training and testing or resampling technique are used to estimate the true error rate of the classification method detailed attention is given to the analysis of performance of the neural net using back propagation for these problem which have relatively few hypothesis and feature the machine learning procedure for rule induction or tree induction clearly performed best 
language differ in the concept and real world entity for which they have word and grammatical construct therefore translation must sometimes be a matter of approximating the meaning of a source language text rather than finding an exact counterpart in the target language we propose a translation framework based on situation theory the basic ingredient are an information lattice a representation scheme for utterance embedded in context and a mismatch resolution scheme defined in term of information flow we motivate our approach with example of translation between english and japanese 
we introduce an algorithm for designing a predictive left to right shift reduce non determinisic push down machine corresponding to an arbitrary unrestricted context free grammar and an algorithm for efficiently driving this machine in pseudo parallel the performance of the resulting parser is formally proven to be superior to earley s parser the technique employed consists in constructing before run time a parsing table that encodes a non deterministic machine in the which the predictive behavior ha been compiled out at run time the machine is driven in pseudo parallel with the help of a chart the recognizer behaves in the worst case in space however in practice it is always superior to earley s parser since the prediction step have been compiled before run time finally we explain how other more efficient variant of the basic parser can be obtained by determinizing portion of the basic non deterministic push down machine while still using the same pseudo parallel driver 
an original methodology called backward model tracing to model student performance which feature a profitable integration of the bug collection and bug construction technique is presented this methodology ha been used for building the modelling module of a new version of et english tutor an it aimed at supporting the learning of the english verb system backward model tracing is based on the idea of analyzing the reasoning process of the student by reconstructing step by step and in reverse order the chain of reasoning s he ha followed in giving his her answer in order to do this both correct domain specific knowledge and a catalogue of stereotyped error malrules are utilized when the system is unable to explain the student behavior by exploiting it previous knowledge new malrules are generated dynamically by utilizing explanation based learning technique the overall process is based on a deep modelling of the student problem solving and the discrimination among possible explicative hypothesis about the reason underlying the student behavior is carried on nonmonotonically through a truth maintenance system the proposed approach ha been fully implemented in a student modelling module developed in prolog 
merging operator in a plan can yield significant saving in the cost to execute a plan past research in planning ha concentrated on handling harmful interaction among plan but the understanding of positive one ha remained at a qualitative heuristic level this paper provides a quantitative study for plan optimization and present both optimal and approximate algorithm for finding minimum cost merged plan with worst and average case complexity analysis and empirical test we demonstrate that efficient and wellbehaved approximation algorithm are applicable for optimizing general plan with large size 
the purpose of this paper is to characterize a constituent boundary parsing algorithm using an information theoretic measure called generalized mutual information which serf a an alternative to traditional grammar based parsing method this method is based on the hypothesis that constituent boundary can be extracted from a given sentence or word sequence by analyzing the mutual information value of the part of speech n gram within the sentence this hypothesis is supported by the performance of an implementation of this parsing algorithm which determines a recursive unlabeled bracketing of unrestricted english text with a relatively low error rate this paper derives the generalized mutual information statistic describes the parsing algorithm and present result and sample output from the parser 
the candidate elimination algorithm for inductive learning with version space can require both exponential time and space this article describes the incremental non backtracking focusing inbf algorithm which learns strictly tree structured concept in polynomial space and time specifically it learns in time o pnk and space o nk where p is the number of positive n the number of negative and k the number of feature inbf is an extension of an existing batch algorithm avoidance focusing af although af also learns in polynomial time it assumes a convergent set of positive example and handle additional example inefficiently inbf ha neither of these restriction both the af and inbf algorithm assume that the positive example plus the near miss will be sufficient for convergence if the initial set of example is convergent this article formally prof that for treestructured concept this assumption doe in fact hold 
in this paper a resolution method for propositional temporal logic is presented temporal formula incorporating both past time and future time temporal operator are converted to separated normal form snf then both non temporal and temporal resolution rule are applied the resolution method is based on classical resolution but incorporates a temporal resolution rule that can be implemented efficiently using a graph theoretic approach 
this paper report a case study on a large scale and corporate wide case based system unlike most paper for the aaai conference which exclusively focus on algorithm and model executed on computer system this paper heavily involves organizational activity and structure a a part of algorithm in the system it is our claim that successful corporate wide deployment of the case base system must involve organizational effort a a part of an algorithmic loop in the system in a broad sense we have established a corporate wide case acquisition algorithm which is performed by person and developed the squad software quality control advisor system which facilitates sharing and spreading of experience corporate wide the major finding were that the key for the success is not necessary in complex and sophisticated ai theory in fact we use very simple algorithm but the integration of mechanism and algorithm executed by machine and person involved 
all major approach to qualitative reasoning rely on the existence of a model of the physical system however the task of finding a model is usually far from trivial within the area of electrical engineering model building method have been developed to automatically deduce model from measurement in this paper we explicitly show how to incorporate qualitative knowledge in order to apply these method to situation where they do not behave satisfactorily a program ha been developed and applied to a non trivial example the qualitative input in term of an incomplete bond graph and the resulting output can be used to form a more complete bond graph this more informative model is suitable for further reasoning 
we report here on our experiment with post part of speech tagger to address problem of ambiguity and of understanding unknown word part of speech tagging perse is a well understood problem our paper report experiment in three important area handling unknown word limiting the size of the training set and returning a set of the most likely tag for each word rather than a single tag we describe the algorithm that we used and the specific result of our experiment on wall street journal article and on muc terrorist message 
example are often used along with textual description to help convey particular idea especially in instructional or explanatory context these accompanying example reflect information in the surrounding text and in turn also influence the text sometimes example replace possible textual elaboration in the description it is thus clear that if object description are to be generated the system must incorporate strategy to handle example in this work we shall investigate some of these issue in the generation of object description 
some problem are discussed that arise for incremental processing using certain flexible categorial grammar which involve either undesirable parsing property or failure to allow combination useful to incrementality we suggest a new calculus which though designed in relation to categorial interpretation of some notion of dependency grammar seems to provide a degree of flexibility that is highly appropriate for incremental interpretation we demonstrate how this grammar may be used for efficient incremental parsing by employing normalisation technique 
we formulate the figure ground discrimination problem a a combinatorial optimization problem we suggest a cost function that make explicit a definition of shape based on interaction between image edge these interaction have some mathematical analogy with interacting spin system a model that is well suited for solving combinatorial optimization problem we devise a mean field annealing method for finding the global minimum of such a spin system and the method successfully solves for the figure ground problem 
projectively invariant shape descriptor allow fastindexing into model library without the need for posecomputation or camera calibration this paper describesprogress in building a model based vision systemfor plane object that us algebraic projective invariant we give a brief account of these descriptor andthen describe the recognition system giving examplesof the invariant technique working on real image introduction a major unsolved problem in model based visionis the 
the task of obtaining a line labeling from a greyscale image of trihedral object present difficulty not found in the classical line labeling problem a originally formulated the line labeling problem assumed that each junction wa correctly pre classified a being of a particular junction type e g t y arrow the success of the algorithm proposed have depended critically upon getting this initial junction classification correct in real image however junction of different type may actually look quite similar and this preclassification is often difficult to achieve this issue is addressed by recasting the line labeling problem in term of a coupled probabilistic system which label both line and junction this result in a robust system in which prior knowledge of acceptable configuration can serve to overcome the problem of misleading or ambiguous evidence 
a fundamental assumption made in formulatingoptical flow mgorithms is that motion at any point in an imagecan be represented a a single pattern undergoing a simpletranslation even complex motion will appear a a uniformdisplacement when viewed through a sufficiently smm window this assumption fails in a number of common sitnations for example transparent sttr ace moving pastone another yield two motion component at each point more important the assunption fails along the 
vector quantization is useful for data compression competitive learning which minimizes reconstruction error is an appropriate algorithm for vector quantization of unlabelled data vector quantizati on of labelled data for classification ha a different objective to minimize the number of misclassifications and a different algorithm is appropriate we show that a variant of kohonen s lvq algorithm can be seen a a multiclass extension of an algorithm which in a restricted class case can be proven to converge to the bayes optimal classification boundary we compare the performance of the lvq algorithm to that of a modified version having a decreasing window and normalized step size on a ten class vowel classification problem 
simple indoor navigation subtasks such a moving an autonomousplatform robot in a corridor parallel to a wall or correcting it trajectoryto avoid small obstacle can be accomplished using reflexivebehaviours without the need of a navigation planner we describe adynamic vision module in which reflex are implemented a feedbacksystems measurement on the image data provide the position of thefloor boundary in the coordinate system of the onboard camera thesystem state vector 
a novel method for feature extraction ha been applied to a problem of three dimensionalobject recognition intrator and gold the method is related to recent statistical theory huber friedman and is derived from a biologically motivated computationaltheory bienenstock et al result of an initial study replicating recent psychophysicalexperiments bulthoff and edelman demonstrated the utility of the proposed methodfor feature extraction we describe 
a class of concept learning algorithm cl augments standard similarity based technique by performing feature construction based on the sbl output pagallo and hausslcr s fringe pagallo s extension symmetric fringe sym fringe and a refinement we call dcfringe are all instance of this class using decision tree a their underlying representation these method use pattern at the fringe of the tree to guide their construction but dcfringe us limited construction of conjunction and disjunction experiment with small dnf and cnf concept show that dcfringe outperforms both the purely conjunctive fringe and the le restrictive symfringe in term of accuracy conciseness and efficiency further the gain of these method is linked to the size of the training set we discus the apparent limitation of current method to concept exhibiting a low degree of feature interaction and suggest way to alleviate it this lead to a feature construction approach based on a wider variety of pattern restricted by statistical measure and optional knowledge 
we present a novel object centered formalization of action which allows u to define an interesting class of task called cooking task which can be performed without backtracking since backtracking is unnecessary action can be selected incrementally using a greedy method without having to precompute a plan such an approach is efficient and rapidly adjusts to unforeseen circumstance our argument is that cooking task are widely encountered in everyday life because of the special property of a given culture s artifact in other word culture ha structured the world so a to make it easier to live in we present an implementation of these idea experimental result and control experiment using a standard nonlinear planner 
i describe several computational complexity result for planning some of which identify tractable planning problem the model of planning called propositional planning is simple condition within operator are literal with no variable allowed the different plan ning problem are defined by different restriction on the precondition and postconditions of operator the main result are proposi tional planning is pspace complete even if operator are restricted to two positive nonnegated precondition and two postconditions or if operator are restricted to one postcondi tion with any number of precondition it is np complete if operator are restricted to positive postconditions even if operator are restricted to one precondition and one posi tive postcondition it is tractable in a few re stricted case one of which is if each opera tor is restricted to positive precondition and one postcondition the block world problem slightly modified is a subproblem of this re stricted planning problem 
we present a method to construct real time system using a component anytime algorithm whose quality of result degrades grace fully a computation time decrease introduc ing computation time a a degree of freedom defines a scheduling problem involving the ac tivation and interruption of the anytime com ponents this scheduling problem is especially complicated when trying to construct interruptible algorithm whose total run time is un known in advance we introduce a framework to measure the performance of anytime algo rithms and solve the problem of constructing interruptible algorithm by a mathematical re duction to the problem of constructing con tract algorithm which require the determi nation of the total run time when activated we show how the composition of anytime algo rithms can be mechanized a part of a compiler for a lisp like programming language for realtime system the result is a new approach to the construction of complex real time sys tems that separate the arrangement of the per formance component from the optimization of their scheduling and automates the latter task 
a faulty component that behaves consistently over time is said to behave non intermittently for any given set of input such a component will always generate the same output assuming that component fail non intermittently is a common simplifying strategy used by diagnostician because many real world device often fail this way this strategy remove the need to repeat experiment and this strategy allows information from independent example of system behavior to be combined in relatively simple way this paper extends the formal framework for diagnosis developed in to allow nonintermittency assumption in addition we show how the definition can be easily integrated into atm based diagnosis engine within our formulation component can be individually assumed to be intermittent or nonintermittent 
a new kind of rms based on a close merge of tm and atm is proposed it us the tm graph and interpretation and the atm multiple context labelling procedure in order to fill in the problem of the atm environment in presence of nonmonotonic inference a new kind of environment able to take into account hypothesis that do not hold is defined these environment can inherit formula that hold a in the atm context lattice the dependency graph can be interpreted with regard to these environment so every node can be labelled furthermore this lead to consider several possible interpretation of a query 
constraint satisfaction csp is a powerful and extensively used framework for describing search problem a csp is typically defined a the problem of finding consistent assignment of value to a fixed set of variable given some constraint over these variable however for many synthesis task such a configuration and model composition the set of variable that are relevant to a solution and must be assigned value change dynamically in response to decision made during the course of problem solving in this paper we formalize this notion a a dynamic constraint satisfaction problem that us two type of constraint compatibility constraint correspond to those traditionally found in csps namely constraint over the value of variable activity constraint describe condition under which a variable may or may not be actively considered a a part of a final solution we present a language for expressing four type of activity constraint in term of variable value and variable being considered we then describe an implemented algorithm that enables tight interaction between constraint about variable activity and constraint about variable value the utility of this approach is demonstrated for configuration and model composition task 
causality play an important role in qualitative reasoning about physical system in this paper we show that the bond graph method can be fruitfully applied to represent and generate causal order on a formal basis both physical and computational aspect of bond graph causality are discussed in particular we show that it provides a inner phys ical foundation for a causal order along the line of iwasaki and simon bond graph causality also generates more information than doe the causal ordering theory including better causal resolution an improved definition of exogeneity in term of parameter and source automatic checking of self containment and a more detailed physical treat ment of feedback the bond graph method originates corresponds to the device ontology the topology of the bond graph can be employed to obtain important qualitative physical information top and akkermans ai thus bond graph provide a formal and generic language for modelling and representing physical system in this paper we will deal with the topic of causality according to study of human reasoning about technical device iforbus and gentner causal explanation are based on elementary mechanism that relate individual variable in a directed way we propose that bond graph formalise these intuitive idea in a physically appropriate way and can be fruitfully employed to obtain causal information based on expert knowledge about physical system theory in particular we show that bond graph causality yield a causal ordering method for physical system that is similar to but more powerful than the theory of iwasaki and simon i iwasaki and simon ai 
we argue that current plan based theory of discourse do not by themselves explain prevalent phenomenon in even simple task oriented dialogue the purpose of this paper is to show how one difhcult to explain feature of these dialogue confirmation follows from the joint or team nature of the underlying task specifically we review the concept of a joint intention and we argue that the conversants in a task oriented dialogue jointly intend to accomplish the task from this basis we derive the goal underlying the pervasive use of confirmation observed in a recent experiment we conclude with a discussion on generalizing the analysis presented here to characterize dialogue itself a a joint activity 
official testing will be done in may and the third message understanding conference muc will be held may at the naval ocean system center a proceeding will be published on the basis of this conference the result of the evaluation will be analyzed to discover whether conclusion can be drawn c o n c e r n i n g the c o r r e l a t i o n among task performance text analysis capability and theoretical approach r e c e n t r e s u l t s 
we present an efficient method for inferring fact from a propositional knowledge base which is not required to be in conjunctive normal form this logically incomplete method called propositional fact propagation is more powerful and efficient than some form of boolean constraint propagation hence it can be used for tractable deductive reasoning in many ai application including various truth maintenance system we also use propositional fact propagation to define a weak logical entailment relation that is more powerful and efficient than some others presented in the literature among other application this new entailment relation can be used for efficiently answering query posed to a knowledge base and for modeling belief held by a resource limited agent 
this paper present a new approach for resolving lexical ambiguity in one language using statistical data on lexical relation in another language this approach exploit the difference between mapping of word to sens in different language we concentrate on the problem of target word selection in machine translation for which the approach is directly applicable and employ a statistical model for the selection mechanism the model wa evaluated using two set of hebrew and german example and wa found to be very useful for disambiguation 
in many formalization of a changing world thing do not change all the time but are persistent thoughout a time interval often this persistency is represented by fact refering to interval int which are still valid if int is replaced by any subinterval int approach like episode propagation or penberthy s temporal unification try to employ this property for efficient reasoning however these approach lack formality in this paper their way of reasoning about persistency is reconstructed a inference rule that combine appropriate timeboxes with standard resolution in many case burckert s constrained resolution may be used more complex example may be handled by a new inference rule called persistency resolution an analysis of this rule lead to a more general notion of persistency 
there are two common but quite distinct interpretation of probability they can be interpreted a a measure of the extent to which an agent belief an assertion i e a an agent s degree of belief or they can be interpreted a an assertion of relative frequency i e a a statistical measure used a statistical measure probability can represent various assertion about the objective statistical state of the world while used a degree of belief they can represent various assertion about the subjective state of an agent s belief in this paper we examine how an agent who know certain statistical fact about the world might infer probabilistic degree of belief in other assertion from these statistic for example an agent who know that most bird fly a statistical fact may generate a degree of belief greater than in the assertion that tweety fly given that tweety is a bird this inference of degree of belief from statistical fact is known a direct inference we develop a formal logical mechanism for performing direct inference some of the inference possible via direct inference are closely related to default inference we examine some feature of this relationship 
a significant problem in designing mobile robot control system involves coping with the uncertainty that arises in moving about in an unknown or partially unknown environment and relying on noisy or ambiguous sensor data to acquire knowledge about that environment we describe a control system that chooses what activity to engage in next on the basis of expectation about how the information returned a a result of a given activity will improve it knowledge about the spatial layout of it environment certain of the higher level component of the control system are specified in term of probabilistic decision model whose output is used to mediate the behavior of lower level control component responsible for movement and sensing the control system is capable of directing the behavior of the robot in the exploration and mapping of it environment while attending to the real time requirement of navigation and obstacle avoidance 
incorporation of default in grammar formalism is important for reason of linguistic adequacy and grammar organization in this paper we present an algorithm for handling default information in unification grammar the algorithm specifies a logical operation on feature structure merging with the non default structure only those part of the default feature structure which are not constrained by the non default structure we present various linguistic application of default unification 
the probabilistic network technology is a knowledge based technique which focus on reasoning under uncertainty because of it well defined semantics and solid theoretical foundation the technology is finding increasing application in field such a medical diagnosis machine vision military situation assessment petroleum exploration and information retrieval however like other knowledge based technique acquiring the qualitative and quantitative information needed to build these network can be highly labor intensive constructqr integrates technique and concept from probabilistic network artificial intelligence and statistic in order to induce markov network i e undirected probabilistic network the resulting network are useful both qualitatively for concept organization and quantitatively for the assessment of new data the primary goal of constructor is to find qualitative structure from data constructor find structure by first modeling each feature in a data set a a node in a markov network and secondly by finding the neighbor of each node in the network in markov network the neighbor of a node have the property of being the smallest set of node which shield the node from being affected by other node in the graph this property is used in a heuristic search to identify each node s neighbor the traditional test for independence is used to test if a set of node shield another node cross validation is used to estimate the quality of alternative structure 
in the logical semantics of knowledge base k the handling of contradiction pose a problem not solvable by standard logic an adequate logic for kb must be capable of tolerating inconsistency in a kb without losing it deductive content this is also the bottom line of so called paraconsistent logic but paraconsistent logic doe not address the question whether contradictory information should be accepted or not in the derivation of further information depending on it we propose two computational logic based on the notion of support and acceptance handling contradiction in a conservative resp skeptical manner they neither lead to the break down of the system nor are they accepted a valid piece of information 
this paper describes a series of experiment aimed at producing a bottom up parser that will produce partial par suitable for use in robust interpretation and still be reasonably efficient in the course of these experiment we improved parse time by a factor of over our first attempt ending with a system that wa twice a fast a our previous parser which relied on strong top down constraint the major algorithmic variation we tried are described along with the corresponding performance result 
we propose a new stereo vision algorithm for finding circle in a scene in both d image ellipsis are found the ellipsis are matched in order to find circle in d space the method doe not require a special camera alignment instead both camera matrix must be known some result are presented showing that the method is sufficiently fast and accurate for object recognition after edge detection a few second of cpu time are sufficient to find full circle with standard deviation of the order of of the radius of the circle 
this paper identifies two fundamentally different kind of training information for learning search control in term of an evaluation function each kind of training information suggests it own set of method for learning an evaluation function the paper show that one can integrate the method and learn simultaneously from both kind of information 
we present bfl a hybrid logic for representing uncertain knowledge bfl attache a quantified notion of belief based on dempster shafer s theory of belief function to classical first order logic the language of bfl is composed of object of the form f a b where f is a first order sentence and a and b are number in the interval with a b intuitively a measure the strength of our belief in the truth of f and b that in it falseness a number of property of first order logic nicely generalize to bfl in return bfl give u a new perspective on some important point of dempster shafer theory e g the role of dempster s combination rule 
review of adaptive mesh adaptive mesh are dynamic network of nodal mass intercon nected by adjustable spring they are useful for nonuniformly sampling and reconstructing visual data this paper extends the adaptive mesh model in the following way it i develops open adaptive mesh and closed adaptive shell based on triangular and rectangular element ii proposes a discontinuity detection and preservation algorithm suitable for the model and iii devel ops technique for adaptive hierarchical subdivision of adaptive mesh and shell the extended model is applied to image and d surface data 
natural language processing nlp system the effort focus particularly on automatically inferring the meaning of new word in context and on developing partial interpretation of language that is either fragmentary or beyond the capability of the nlp system to understand the technique are being evaluated in a message processing domain such a automatic data base update based on article from the wall street journal on corporate takeover bid 
personalized knowledge based system have not yet become widespread despite their potential for valuable assistance in many daily task this is due in part to the high cost of developing and maintaining customized knowledge base the construction of personal assistant a learning apprentice interactive assistant that learn continually from their user is one approach which could dramatically reduce the cost of knowledge based advisor we present one such personal learning apprentice called cap which assist in managing a meeting calendar cap ha been used since june by a secretary in our work place to manage a faculty member s meeting calendar and is the first instance of a fielded learning apprentice in routine use this paper describes the organization of cap it performance in initial field test and more general lesson learned from this effort about learning apprentice system 
psychological evidence indicates that human chess player base their assessment of chess position on structural perceptual pattern learned through experience morph is a computer chess program that ha been developed to be more consistent with the cognitive model the learning mechanism used by morph combine weight updating genetic explanation based and temporal difference learning to create delete generalize and evaluate chess position an associative pattern retrieval system organizes the database for efficient processing the main objective of the project are to demonstrate capacity of the system to learn to deepen our understanding of the interaction of knowledge and search and to build bridge in this area between ai and cognitive science to strengthen connection with the cognitive literature limitation have been place on the system such a restriction to ply search to little domain knowledge and to no supervised training 
whereas in the united state work in machine translation mt ha only recently been reinstated a a respectable natural language processing nlp application it ha long been considered a worthwhile and interesting topic for research and development in both europe and japan in term of number of project in one sub field of computational linguistics mt is currently perhaps the most important application one obvious reason for this is simply the daily awareness that people communicate in language other than english a situation that naturally encourages an interest in translation on a practical level for example every television cable system in europe broadcast station from numerous country and on the political level the european community ec is committed to protecting the language of each of the member state which implies providing numerous translation service from an economic viewpoint every company know that in order to market it product the documentation must be in the language of the target country and a last motivation for interest in mt which wa also the origin of mt activity in the u and an important concern for japan is the desire for better access to information important document often exist in some foreign language 
in this paper we demonstrate an important role for model based reasoning incase adaptation model based reasoning can allow a case based reasoner to applycases to a wider range of problem than would otherwise be possible we focus on case adaptation in brainstormer a planner that us abstractadvice to help it plan in the domain of political and military policy a it relatesto terrorism we show that by equipping a case adapter with an explicit causalmodel of the planning process case 
massively parallel artificial intelligence is a new and growing area of ai research enabled by the emergence of massively parallel machine it is a new paradigm in ai research a high degree of par allelism not only affect computing performance but also trigger drastic change in the approach to ward building intelligent system memory based reasoning and parallel marker passing are example of new and redefined approach these new ap proaches fostered by massively parallel machine offer a golden opportunity for ai in challenging the vastness and irregularity of real world data that are encountered when a system access and process very large data base and knowledge base this article describes the current status of massively par allel artificial intelligence research and position of each panelist 
binocular stereo is the process of obtaining depth information from a pair of left and right camera in the past occlusion have been region where stereo algorithm have failed we show that on the contrary they can help stereo computation by providing cue for depth discontinuity 
this paper is a report of a study investigating the validity of the multiple poisson np model of word distribution in document collection an np distribution is a mixture of n poisson distribution with different mean we describe a practical algorithm for determining if a certain word is distributed acording to an np distribution and computing the distribution parameter the algorithm wa applied to every word in four different document collection it wa found that over of frequently occurring word and term indeed behave according to the np distribution the result indicate that the proportion of np word depends on the collection size document length and the frequency of the individual word most of the np word recognised are distributed according to the mixture of relatively few single poisson distribution two three or four there is an indication that the number of single poisson component in the mixture of relatively few single poisson distribution two three or four there is an indication that the number of single poisson component in the mixture depends on the collection frequency of word 
this paper descnbes a knowledge representation and reasoning system that performs a limited but interesting class of inference over a restricted class of first order sentence with optimal efficiency the proposed system can answer yes no a well a wh query in time that is only proportional to the length of the shortest derivation of the query and is independent of the size of the knowledge base this work suggests that the expressiveness and the inferential ability of a representation and reasoning system may be limited in unusual way to arrive at extremely efficient yet fairly powerful knowledge based system 
first order learning system e g foil focl forte generally rely on hill climbing heuristic in order to avoid the combinatorial explosion inherent in learning first order concept however hill climbing leaf these system vulnerable to local maximum and local plateau we present a method called relational pathfinding which ha proven highly effective in escaping local maximum and crossing local plateau we present our algorithm and provide learning result in two domain family relationship and qualitative model building 
we describe an algorithm which allows a behavior based robot to learn on the basis of positive and negative feedback when to activate it behavior in accordance with the philosophy of behavior based robot the algorithm is completely distributed each of the behavior independently try to sensor find out i whether it is relevant i e whether it is at all correlated to positive feedback and ii what the condition are under which it becomes reliable i e the condition under which it maximises the probability of receiving positive feedback and minimises the probability of receiving negative feedback the algorithm ha been tested successfully on an autonomous legged robot which had to learn how to coordinate it leg so a to walk forward 
achieving goal despite uncertainty in control and sensing may require robot toperform complicated motion planning and execution monitoring this paper describesa reduced version of the general planning problem in the presence of uncertainty anda complete polynomial algorithm solving it the planner computes a guaranteed plan for given uncertainty bound by backchaining omnidirectional backprojections of thegoal until one fully contains the set of possible initial position of the robot 
feature structure play an important role in linguistic knowledge representation in computational linguistics given the proliferation of different feature structure formalism it is use ful to have a common language to express them in this paper show how a variety of fea ture structure and constraint on them can be expressed in predicate logic except for the use of circumscription for non monotonic device including sorted feature value subsumption constraint and the non monotonic any val ues and constraint equation many feature system can be completely axiomatized in the schonfinkel bernays class of first order formu lae so the decidability of the satisfiability and validity problem for these system follows im mediately 
the ability to identify and reason about novel aspect of their input would greatly enhance the capability of artificial neural network the extent of the novelty could be used to judge the appropriateness of individual network for the task to be performed the location and shape of novel feature could be employed to identify the unusual component of the input and to choose an appropriate response 
this paper discus the recent view on knowl edge representation and memory a pre sented by different researcher under the flag of situated cognition the situated view im ply a radical shift of paradigm we argue that there are no strong reason to leave the traditional paradigm of cognitive science and ai four main issue are addressed the role of computational model in theory of cogni tion theory on knowledge and memory the frame of reference problem and implication for learning and instruction the main conclusion of the paper is that situationism is throwing out the baby with the bath water consoli dated achievement of cognitive science and ai still stand even if the architecture that are assumed to underly traditional model of cog nition can be challenged 
this paper describes a method of analysing rule based system which model the procedural semantics of such language through a process of abstract interpretation the program absps derives a description of the mapping between a rule base s input and output in contrast to earlier approach absps can analyse the effect of conflict resolution closed world negation and the retraction of fact this considerably reduces the size of the search space because in the abstract domain absps take advantage of the very same control information which guide the inference engine in the concrete domain absps can detect redundancy which would be missed if the procedural semantics were ignored furthermore the abstract description of a rule base s input output mapping can be used to prove that the rule base meet it specification 
we identify and illustrate five important kind of dialectical example standard configuration of case which enable an arguer to justify rhetorical assertion effectively by example our computer program generates argument context collection of case that instantiate dialectical example from an on line database of case according to a user s general specification the argument context generation program provides a human or automated tutor a stock of dialectical example to teach novice advocate first year law student how to recognize carry out and respond to the associated rhetorical move although generating such example is very hard for human even when dealing with small number of case our program generates and organizes such example quickly and effectively in a preliminary experiment we employed program generated argument context manually to teach basic argument skill to first year law student with good result our ability to define such complex example declaratively in term of logical expression of loom concept and relation affords a number of advantage over previous work 
model based diagnosis is based on first principle reasoning using the behavioral specification of the primitive component of a device unless the computational architecture of the model based reasoning engine is carefully designed combinatorial explosion render the approach useless for device consisting of more than a handful of component this paper analyzes the diverse origin of this combinatorial explosion a nd outline strategy to cope with each one the resulting computational architecture for model based diagnosis provides order of magnitude performance improvement on large example making model based approach practical for device consisting of on the order of component 
a discussion is presented of how to understand assembly illustration in an assembly manual without basing it on any model of mechanical part the discussion is based on the following two basic prerequisite the first is that at the present stage mechanical part are assumed to be in the shape of or composed of cylinder the second is to recognize the d structure of mechanical part by using the assembly information obtained from auxiliary line a hierarchical approach is developed to understand an assembly illustration also presented is a generated structure understanding scheme the scheme find the generated structure in the succeeding illustration by using preliminary information on the generated structure from the current assembly illustration the preliminary information is obtained by understanding the current assembly illustration 
an intelligent agent us known fact including statistical knowledge to assign degree of belief to assertion it is uncertain about we investigate three principled technique for doing this all three are application of the principle of indifference because they assign equal degree of belief to all basic situation consistent with the knowledge base they differ because there are competing intuition about what the basic situation are various natural pattern of reasoning such a the preference for the most specific statistical data available turn out to follow from some or all of the technique this is an improvement over earlier theory such a work on direct inference and reference class which arbitrarily postulate these pattern without offering any deeper explanation or guarantee of consistency the three method we investigate have surprising characterization there are connection to the principle of maximum entropy a principle of maximal independence and a center of mass principle there are also unexpected connection between the three that help u understand why the specific language chosen for the knowledge base is much more critical in inductive reasoning of the sort we consider than it is in traditional deductive reasoning 
we present an approach to the detection and identification of human face and describe a working near real time face recognition system which track a subject s head and then recognizes the person by comparing characteristic of the face to those of known individual our approach treat face recognition a a two dimensional recognition problem taking advantage of the fact that face are are normally upright and thus may be described by a small set of d characteristic view face image are projected onto a feature space face space that best encodes the variation among known face image the face space is defined by the eigenfaces which are the eigenvectors of the set of face they do not necessarily correspond to isolated feature such a eye ear and nose the framework provides the ability to learn to recognize new face in an unsupervised manner 
we describe a planner that work on the description of a multi path environment and generates a conditional plan the resulting plan is guaranteed to fulfill it goal whatever path of the description the environment follows during the plan execution 
this paper present pac learning analysis for instance based learning algorithm for both symbolic and numeric prediction task the algorithm analyzed employ a variant of the k nearest neighbor pattern classifier the main result of these analysis are that the ib instance based learning algorithm can learn using a polynomial number of instance a wide range of symbolic concept and numeric function in addition we show that a bound on the degree of difficulty of predicting symbolic value may be obtained by considering the size of the boundary of the target concept and a bound on the degree of difficulty in predicting numeric value may be obtained by considering the maximum absolute value of the slope between instance in the instance space moreover the number of training instance required by ib is polynomial in these parameter the implication of these result for the practical application of instance based learning algorithm are discussed 
this paper study sorted generalization the generalization with respect to an arbitrary taxonomic theory of atomic formula containing sorted variable it develops an algorithm for the task discus the algorithm and task complexity and present semantic property of sorted generalization based on it semantic property we show how sorted generalization is applicable to such problem a abduction induction knowledge base vivification and analogical reasoning significant distinction between this work and related work with taxonomic information arise from the generality of the taxonomic theory we allow which may be any first order taxonomic theory and the semantic completeness property of sorted generalization 
many problem solving approach are based on the assumption that a problem can be precisely defined before it is solved these approach are inadequate for dealing with ill defined problem which require the coevolution of problem setting and problem solving in this paper we describe integrated domain oriented knowledge based design environment and their underlying multifaceted architecture the environment empower human to cope with ill defined problem such a design by supporting an incremental approach to problem setting and problem solving we focus on the integration of specification construction and a catalog of prestored design object in those environment the synergy of integration enables the environment to make those object relevant to the task at hand taking architectural design a a domain to illustrate our approach we describe an operational prototype system catalogexplorer that assist designer in locating example in the catalog that are relevant to the task at hand a articulated by a partial specification and a partial construction user are thereby relieved of the task of forming query and navigating in information space 
troubleshooting problem in real manufacturing environment impose constraint on admissible solution that make the computational solution offered by troubleshooting from first principle and the conventional experience based expert system approach infeasible in this paper we present a computational theory for a solution to these problem that is based on the principle of locality and exploit the domain specific weak method of troubleshooter and debugging knowledge of the designer the computational theory is evaluated by generating focus of attention heuristic for a moderately complex digital device 
explanation based learning ebl fails to accelerate problem solving in some problem space how do these problem space differ from the one in minton s experiment b can minute modification to problem space encoding drastically alter ebl s performance will prodigy ebl s success scale to real world domain this paper present a formal theory of problem space structure that answer these question the central observation is that prodigy ebl relies on finding nonrecursive explanation of prodigy s problem solving behavior the theory explains and predicts prodigy ebl s performance in a wide range of problem space the theory also predicts that a static program transformer called static can match prodigy ebl s performance in some case the paper report on an array of experiment that confirms this prediction static match prodigy ebl s performance in each of minton s problem space 
the hybrid phenomenon theory hpt isa framework forformalizinghow dynamic state spacemodelsofphysicalsystemsarebuiltfrom firstprinciples the hpt descendsfrom the qualitativeprocesstheory qpt forbus fromwhich itinherits basicconceptslike view phenomenon and influence however thehpt redefinessome oftheseconceptsina more strictmanner inordertorepresentknowledge of physicswith the accuracyneeded to developfullparametricmodels specifically influencesmay specifyquantifiednon linearfunctionsofseveralvariables a mechanism denotedsubsumptionisintroducedtoensureconsistencyintheemergingmodelswhen different simplifyingassumptionsaremade the hpt hasbeen implementedinclos 
in this paper we describe progress toward the development of an x ray image analysis system for industrial inspection here the goal is to check part dimension and identify geometric flaw against known tolerance specification from an image analysis standpoint this pose challenge to devise robust method to extract low level feature develop deformable parameterized template and perform statistical tolerancing test for geometry verification we illustrate aspect of our current system and how knowledge of expected object geometry is used to guide the interpretation of geometry from image 
using abstraction in planning doe not guarantee an im provement in search efficiency it is possible for an abstract planner to display worse performance than one that doe not use abstraction analysis and experiment have shown that good abstraction hierarchy have or are close to having the downward refinement property whereby given that a concrete level solution exists every abstract solution can be refined to a concrete level solu tion without backtracking across abstract level work ing within a semantics for abstrips style abstraction we provide a characterizati on of the downward refinement property after discussing it effect on search efficiency we develop a semantic condition sufficient for guarantee ing it presence in an abstraction hierarchy using the semantic condition we then provide a set of sufficient and polynomial time checkable syntactic condition that can be used for checking a hierarchy for the downward refinement property 
the esprit project polyglot aim at developing multi lingual speech to text and text to speech conversion and to integrate this technology in a number of commercially viable prototype application speech to text conversion is mainly concerned with very large vocabulary isolated word recognition it us a statistical knowledge based approach that wa pioneered for italian and is now being extended to other language work on continuous speech recognition ha the character of an in depth feasibility study for textto speech conversion a new multi level data structure is developed that facilitates rule writing by offering a semi graphical rule format the recognition and synthesis technology is used to build a number of generic prototype application that mainly address office automation 
current complex feature based grammar use a single procedure unification for a multitude of purpose among them enforcing formal agreement between purely syntactic feature this paper present evidence from several natural language that unification variable matching combined with variable substitution is the wrong mechanism for effecting agreement the view of grammar developed here is one in which unification is used for semantic interpretation while purely formal agreement involves only a check for non distinctness i e variable matching without variable substitution 
this is a connected scries of argument concern ing paraconsistent logic it is argued first that paraconsistency is an option worth pursuing in automated reasoning then that the most popular paraconsistent logic fde is inadequate for the reconstruction of essential first order argu ments after a case is made for regarding quan tifiers a dyadic rather than monadic operator it is shown that the addition of such quantifier to fde allows an implication connective to be defined yielding the known logic bn refining the treatment of implication in a manner similar to that found in intuitionist logic lead to the more interesting system bn 
this paper present an edge finder for textured image using rough constraint on the size of image region it estimate the local amount of variation in image value these estimate are constructed so that they do not rise at boundary this enables subsequent smoothing and edge detection to find coarse scale boundary to the full available resolution while ignoring change within uniformly textured region this method extends easily to vector valued image e g color image or texture feature significant group of outlier value are also identified enabling the edge finder to detect crack separating region a well a certain change in texture phase 
if the back propagation network can produce an inference structure with high and robust performance then it is sensible to extract rule from it the kt algonthm is a novel algonthm for generating rule from an adapted net efficiently the algorithm is able to deal with both single layer and multi layer network and can learn both confirming and disconfirming rule empirically the algorithm is demonstrated in the domain of wind shear detection by infrared sensor with success 
this paper describes the application of explanation based learning a machine learning technique to the sri core language engine a large scale general purpose natural language analysis system the idea is to bypass normal morphological syntactic and 
traditional approach to neural coding characterize the encoding of known stimulus in average neural response organism face nearly the opposite task extracting information about an unknown time dependent stimulus from short segment of a spike train here the neural code wa characterized from the point of view of the organism culminating in algorithm for real time stimulus estimation based on a single example of the spike train these method were applied to an identified movement sensitive neuron in the fly visual system such decoding experiment determined the effective noise level and fault tolerance of neural computation and the structure of the decoding algorithm suggested a simple model for real time analog signal processing with spiking neuron 
student modelling is not typically concerned with representing the deep mental model a student employ in dealing with the world around him her in this research we discus an intelligent tutoring system presto whose goal is to understand the mental model a student ha of a physical device and then use this mental model in providing help to overcome misunderstanding related to the functioning of the device the mental model is extracted from the student by asking question about the relationship of variable affecting the physic of the device the mental model is represented using dekleer and brown s qualitative confluence equation the mental model can then be compared to a set of confluence representing a correct perspective on how the device function a variety of pedagogical choice can be made to explain contradiction implicit in the student s understanding of the device to show the student a simpler physical device that by analogy illustrates anomaly in the student s understanding or to let the student witness his her version of the device in operation so the misunderstanding become obvious experiment in running presto with a number of student show this approach to mental modelling to be promising 
finding the configuration of a set of rigid bodiesthat satisfy a set of geometric constraint isa problem traditionally solved by reformulatingthe geometry and constraint a algebraic equationswhich are solved symbolically or numerically but many such problem can be solved by reasoningsymbolically about the geometric bodiesthemselves using a new technique called degreesof freedom analysis in this approach a sequenceof action is devised to satisfy each constraintincrementally 
this paper considers the problem of computing placement of point in dimensional space given two uncalibrated perspective view the main theorem show that the placement of the point is determined only up to an arbitrary projective transformation of space given additional ground control point however the location of the point and the camera parameter may be determined the method is linear and non iterative whereas previously known method for solving the camera calibration and placement to take proper account of both ground control point and image correspondence are unsatisfactory in requiring either iterative method or model restriction a a result of the main theorem it is possible to determine projective invariant of dgeometric configuration from two perspective view 
human subject easily perceive and extensively use shape regularity such a symmetry or periodicity when they are confronted with tile task of object description and recognition a computer vision algorithm is presented that emulates such behavior in that it similarly make use of shape redundancy for the concise description and meaningful segmentation of planar object contour the contour are analyzed in so called arc length space this parameter space facilitates the detection of regularity under a broad range of viewing condition several of the irregularity which have traditionally been treated in isolation are given a unified substrate for their detection and use in building compact model regularity consistency check are made and if necessary altered version are inferred 
it is obvious to anyone familiar with the rule of the game of chess that a king on an empty board can reach every square it is true but not obvious that a knight can reach every square why is the first fact obvious but the second fact not this paper present an analytic theory of a class of obviousness judgment of this type whether or not the specific of this analysis are correct it seems that the study of obviousness judgment can be used to construct integrated theory of linguistics knowledge representation and inference 
developing large scale system are major effort which require careful planning and solid methodological foundation this paper describes case method the methodology for building large scale case based system casemethod defines the procedure which manager engineer and domain expert should follow in developing case based system and provides a set of supporting tool an empirical study show that the use of case method attains significant workload reduction in system development and maintenance more than a well a qualitative change in corporate activity 
in this paper we first give a formal semantics of nonmonotonic tm theory with cp justification then we prove that the model of a theory j is also a model of theory j i next we conclude thai for every tm theory j there must be a theory j such that j ha no cp justification and all the model of j is also j s finally we prove that the concept of extension defined by u junker and kun konolige is also correct under our definition 
we provide an algorithm that nd optimal search strategy for and tree and or tree our model includes three outcome when a node is explored nding a solution not nding a solution and realizing that there are no solution beneath the current node pruning and not nding a solution but not pruning the node below the expected cost of examining a node and the probability of the three outcome are given based on this input the algorithm generates an order that minimizes the expected search cost 
the calculus of time interval defined by allen ha been extended in various way in order to accomodate the need for considering other time object than convex interval eg time point and interval non convex interval this paper introduces and investigates the calculus of generalized interval which subsumes these extension in an algebraic setting the set of p q relation which generalizes the set of relation in the sense of allen ha both an order structure and an algebraic structure we show that a an order it is a distributive lattice whose property express the topological property of the set of p q relation we also determine in what sense the algebraic operation of transposition and composition act continuously on this set in allen s algebra the subset of relation which can be translated into conjunctive constraint on the endpoint using only ha special computational significance the constraint propagation algorithm is complete when restricted to such relation we give a geometric characterization of a similar subset in the general case and prove that it is stable under composition a a consequence of this last fact we get a very simple explicit formula for the composition of two element in this subset 
this paper present empirical evidence for five hypothesis about learning from large noisy domain that tree built from very large training set are larger and more accurate than tree built from even large subset that this increased accuracy is only in part due to the extra size of the tree and that the extra training instance allow both better choice of attribute while building the tree and better choice of the subtrees to prune after it ha been built for the practitioner with the common goal of maximising the accuracy and minimising the size of induced tree these conclusion prompt new technique for induction on large training set although building huge tree from huge training set is computational ly expensive pruning smaller tree on them is not yet it improves accuracy where a pruned tree is considered too large for human or machine limitation it can be overpruncd to an acceptable size although this requires far more time than building a tree of that size from a correspondingl y small training set it will usually be more accurate the paper also describes an algorithm for overpruning tree to user specifie d size limit it is evaluated in the course of testing the above hypothesis 
a diversity of phenomenon can produce image intensitydiscontinuities information about the physicalcause for image edge can provide crucial input to imageunderstanding algorithm we present a physicallybasedapproach using polarization to distinguish type of image edge limb edge specular edge andalbedo physical edge assuming general imaging conditionsand smooth dielectric surface we develop a labelingscheme which enables u to distinguish amongthese edge type the 
this paper survey some recent theoretical result on the efficiency of machine learning algorithm the main tool described is the notion of probably approximately correct pac learning introduced by valiant we define this learning model and then look at some of the result obtained in it we then consider some criticism of the pac model and the extension proposed to address these criticism finally we look briefly at other model recently proposed in computational learning theory 
we propose a language for programming in autoepistemic logic that extends the standard logic programming and incorporates incomplete information by syntactically distinguishing the true negation from the lack of information we also provide a way to define negative information explicitly a fixpoint semantics can be defined for stratified and conservative program in this paper we investigate definite autoepistemic program we investigate fixpoints of definite autoepistemic program and show that they coincide with the declarative semantics of these program we also define a resolution procedure called slsae resolution for such program slsae resolution is sound and complete for stratified conservative and solvable program 
we describe the modification of a grammar to take advantage of prosodic information provided by a speech recognition system this initial study is limited to the use of relative duration of phonetic segment in the assignment of syntactic structure specifically in ruling out alternative par in otherwise ambiguous sentence taking advantage of prosodic information in parsing can make a spoken language system more accurate and more efficient if prosodic syntactic mismatch or unlikely match can be pruned we know of no other work that ha succeeded in automatically extracting speech information and using it in a parser to rule out extraneous par 
goal a typically conceived in ai planning provide an insufficient basis for choice of action and hence are deficient a the sole expression of an agent s objective decision theoretic utility offer a more adequate basis yet lack many of the computational advantage of goal we provide a preferential semantics for goal that ground them in decision theory and preserve the validity of some but not all common goal operation performed in planning this semantic account provides a criterion for verifying the design of goal based planning strategy thus providing a new framework for knowledge level analysis of planning system planning to achieve goal in the predominant ai planning paradigm planner construct plan designed to produce state satisfying particular condition called goal each goal represents a partition of possible state of the world into those satisfying and those not satisfying the goal though planner use goal to guide their reasoning the crude binary distinction defined by goal provide no basis for choosing among alternative plan that ensure achievement of goal and no guidance whatever when no such plan can be found these lacuna pose significant problem for planning in all realistic situation where action have uncertain effect or objective can be partially satisfied to overcome these widely recognized expressive limitation of goal many ai planner make ad hoc use of heuristic evaluation function these augment the guidance provided by goal but lack the semantic justification needed to evaluate their true efficacy we believe that heuristic evaluation function should not be viewed a mere second order refinement on the primary goal based representation of objective supporting a separate optimizing phase of planning our thesis is that relative preference over the possible result of a plan constitutes the fundamental concept underlying the objective of planning with goal serv jon doyle is supported by the usaf rome laboratory and darpa under contract f c ing a a computationally useful heuristic approximation to these preference doyle our purpose here is to provide a formal semantics for goal in term of decision theoretic preference that support rational justification for planning principle the grounding in decision theory enables designer to determine whether their planning system act rationally in accord with their goal and provides a principled basis for integrating goal with other type of preference information we begin by summarizing some basic concept of preference we then develop formal decision theoretic semantics for goal and examine some standard planning operation in light of the semantics we conclude by discussing some related work and offering some direction for future investigation preference and utility 
a method is described for deriving rule of inference from relation between probability of sentence in nilsson s probabilistic logic 
this paper present a rapid and robust parsing system currently used to learn from large body of unedited text the system contains a multivalued part of speech disambiguator and a novel parser employing bottom up recognition to find the constituent phrase of larger structure that might be too difficult to analyze the result of applying the disambiguator and parser to large section of the lancaster oslo bergen corpus are presented 
this paper present a rapid and robust parsing system currently used to learn from large body of unedited text the system contains a multivalued part of speech disambiguator and a novel parser employing bottom up recognition to find the constituent phrase of larger structure that might be too difficult to analyze the result of applying the disambiguator and parser to large section of the lancaster oslo bergen corpus are presented 
graph unification is the most expensive part of unification based grammar parsing it often take over of the total parsing time of a sentence we focus on two speed up element in the design of unification algorithm elimination of excessive copying by only copying successful unification finding unification failure a soon a possible we have developed a scheme to attain these two element without expensive overhead through temporarily modifying graph during unification to eliminate copying during unification we found that parsing relatively long sentence requiring about top level unification during a parse using our algorithm is approximately twice a fast a parsing the same sentence using wroblewski s algorithm 
this paper investigates the effect of parallelism on blackboard system scheduling a parallel blackboard system is described that allows multiple knowledge source instantiation to execute in parallel using a shared memory blackboard approach new class 
to speed up production system many researcher have turned to parallel implementation we describe a system called par that executes production rule in parallel par is novel because it executes many rule simultaneously run in a highly asynchronous fashion and run on a distributed memory machine item improves available concurrency over system that only perform the match step in parallel item reduces bottleneck over synchronous parallel production system item make the technique more available given the lower cost of distributed versus shared memory machine the two main problem regarding correctness namely serialization and the maintenance of consistent distributed database are addressed and solved estimate of the effectiveness of this approach are also given 
the notion of minimality is widely used in three different area of artificial intelligence nonmonotonic reasoning belief revision and conditional reasoning however it is difficult for the reader of the literature in these area to perceive the similarity clearly because each formalization in those area us it own language sometimes without referring to other formalization we define ordered structure and family of ordered structure a the common ingredient of the semantics of all the work above we also define the logic for ordered structure and family we present a uniform view of how minimality is used in these three area and shed light on deep reciprocal relation among different approach of the area by using the ordered structure and the family of ordered structure 
this work us an alignment approach for classifying object according to their shape similarity previous alignment method were mostly limited to the recognition of specific rigid object allowing only for rigid transformation between the model and the viewed object the current work extends previous alignment scheme in two main direction extending the set of allowed transformation between the model and the viewed object and using structural aspect of the internal model namely their part decomposition the compensating transformation is divided into two part the first rough alignment compensates approximately for change in viewpoint and is derived by matching tangen tial point on the silhouette of the model and the viewed object the second the adjustment transformation is derived by matching local feature discontinuity of the contour orientation and curvature principal aspect of the scheme suggested here are also relevant for the recognition of flexible object 
we propose a syntactic filter for identifying non coreferential pronoun np pair within a sentence the filter applies to the output of a slot grammar parser and is formulated in term of the head argument structure which the parser generates it handle control and unbounded dependency construction without empty category or binding chain by virtue of the unificational nature of the parser the filter provides constraint for a discourse semantics system reducing the search domain to which the inference rule of the system s anaphora resolution component apply 
we present a nonlinear forward search method suitable for planning the reaction of an agent operating in a highly unpredictable environment we show that this method is more efficient than existing linear method we then introduce the notion of safety and liveness rule this make possible a sharper exploitation of the information retrieved when exploring the future of the agent 
this paper describes the application of explanation based learning a machine learning technique to the sri core language engine a large scale general purpose natural language analysis system the idea is to bypass normal morphological syntactic and 
we present a representational format for observed movement the representation ha a temporal structure relating component of a single complex movement we also present oxbow an unsupervised learning system which construct class of these movement empirical result indicate that the system build abstract movement concept with appropriate component structure allowing it to predict the latter portion of a partially observed movement 
in order to navigate autonomously most robot system are provided with some sort of global terrain map to make storage practical these map usually have a high level symbolic representation of the terrain the robot s symbolic map is then used to plan a local path this paper describes a system which us the reverse and perhaps more natural process this system process local sensor data in such a way a to allow efficient reactive local navigation a byproduct of this navigation process is an abstraction of the terrain information which form a global symbolic terrain map of the terrain through which the robot ha passed since this map is in the same format a that used by the local navigation system the map is easy for the system to use augment or correct compared with the data from which the map are created the map are very space efficient and can be modified or used for navigation in real time experiment with this system both in simulation and with a real robot operating in natural terrain are described 
know how is an important concept in artificial intelligence it ha been argued previously that it cannot be successfully reduced to the knowledge of fact in this paper i present sound and complete axiomatizations for two non reductive and intuitively natural formal definition of the knowhow of an agent situated in a complex environment i also present some theorem giving useful property of know how and discus and resolve an interesting paradox which is described within this is done using a new operator in the spirit of dynamic logic that is introduced herein and whose semantics and proof theory are given 
an underlying assumption of research on learning from planning and activity is that agent can exploit regularity they find in the world for agent that interact with a world over an extended period of time there is another possibility the exploited regularity can be created and maintained rather than discovered we explore the way in which agent can actively stabilize the world to increase the predictability and tractability of acting within in it 
it ha been hypothesized that tree adjoining grammar tag is particularly well suited for sentence generation it is unclear however how a sentence generation system based on tag should choose among the syntactic possibility made available in the grammar in this paper we consider the question of what need to be done to generate with tag and explain a generation system that provides the necessary feature this approach is compared with other tag based generation system particular attention is given to mumble which like our system make syntactic choice on sophisticated functional ground 
evaluation of information retrieval system should be based on measure of the information provided by the retrieval process informativeness measure which take into account the interactive and full text nature of present day system and the different type of question which are asked of them desirable property for an informativeness measure are developed including context sensitivity user centrality and logarithmic response a hypergraph based framework for measuring the informativeness of a retrieval process is presented and a measure developed which satisfies the desired property the measure is compared to previously developed information measure and illustrated via an application 
a crucially important adequacy test of any theory of speech act is it ability to handle performatives this paper provides a theory of performatives a a test case for our rationally based theory of illocutionary act we show why i request you is a request and i lie to you that p is self defeating the analysis support and extends earlier work of theorist such a bach and harnish and take issue with recent claim by searle that such performative a declarative analysis are doomed to failure 
we present the system abtweak which extends the precondition elimination abstraction of ab strip to hierarchical planner using the nonlinear plan representation a defined in tweak we show that abtweak satisfies the monotonic property whereby the existence of a lowest level solution ii implies the existence of a highest level solution that is structurally similar to ii this property enables one to prune a considerable amount of the search space without loss of completeness 
we describe a method for classifying news story using memory based reasoning mbr a k nearest neighbor method that doe not require manual topic definition using an already coded training database of about story from the dow jones press release news wire and seeker stanfill a text retrieval system that support relevance feedback a the underlying match engine code are assigned to new unseen story with a recall of about and precision of about there are about different code to be assigned using a massively parallel supercomputer we leverage the information already contained in the thousand of coded story and are able to code a story in about second given seeker the text retrieval system we achieved these result in about two person month we believe this approach is effective in reducing the development time to implement classification system involving large number of topic for the purpose of classification message routing etc 
this paper present a connectionist syntactic parser which us structure unification grammar a it grammatical framework the parser is implemented in a connectionist architecture which store and dynamically manipulates symbolic representation but which can t represent arbitrary disjunction and ha bounded memory these problem can be overcome with structure unification grammar s extensive use of partial description 
we show that the familiar explanation based generalization ebg procedure is applicable to a large family of programming language including three family of importance to ai logic programming such a prolog lambda calculus such a lisp and combinator language such a fp the main application of this result is to extend the algorithm to domain for which predicate calculus is a poor representation in addition many issue in analytical learning become clearer and easier to reason about 
we present a logic for belief revision in which revision of a theory by a sentence is represented using a conditional connective the conditional is not primitive but rather defined using two unary modal operator our approach capture and extends the classic agm model without relying on the limit assumption reasoning about counterfactual or hypothetical situation is also crucial for ai existing logic for such subjunctive query are lacking in several respect however primarily in failing to make explicit the epistemic nature of such query we present a logical model for subjunctive based on our logic of revision that appeal explicitly to the ramsey test we discus a framework for answering subjunctive query and show how integrity constraint on the revision process can be expressed 
syntactic phrase indexing and term clustering have been widely explored a text representation technique for text retrieval in this paper we study the property of phrasal and clustered indexing language on a text categorization task enabling u to study their property in isolation from query interpretation issue we show that optimal effectiveness occurs when using only a small proportion of the indexing term available and that effectiveness peak at a higher feature set size and lower effectiveness level for a syntactic phrase indexing than for word based indexing we also present result suggesting that traditional term clustering method are unlikely to provide significantly improved text representation an improved probabilistic text categorization method is also presented 
this paper show that a first order unification based semantic interpretation for various coordinate construct is possible without an explicit use of lambda expression if we slightly modify the standard montagovian semantics of coordination this modification along with partial execution completely eliminates the lambda reduction step during semantic interpretation 
in this paper we address the issue of evaluating decision tree generated from training example by a learning algorithm we give a set of performance measure and show how some of them relate to others we derive result suggesting that the number of leaf in a decision tree is the important measure to minimize minimizing this measure will in a probabilistic sense improve performance along the other measure notably it is expected to produce tree whose error rate are le likely to exceed some acceptable limit the motivation for deriving such result is two fold to better understand what constitutes a good measure of performance and to provide guidance when deciding which aspect of a decision tree generation algorithm should be changed in order to improve the quality of the decision tree it generates the result presented in this paper can be used a a basis for a methodology for formally proving that one decision tree generation algorithm is better than another this would provide a more satisfactory alternative to the current empirical evaluation method for comparing algorithm 
this paper address the integration of service for rule based reasoning in knowledge representation server based on term subsumption language a an alternative to previous construction of rule a concept concept link a mechanism is proposed based on intensional role implementing the axiom of comprehension in set theory this ha the benefit of providing both rule a previously defined and set aggregation using a simple mechanism that is of identical computational complexity to that for rule alone the extension proposed have been implemented a part of kr a knowledge representation server written a a class library in c the paper give an example of their application to the ripple down rule technique for large scale knowledge base operation acquisition and maintenance 
struct is a system that learns structural decision tree from positive and negative example the algorithm us a modification of pagallo and haussler s fringe algorithm to construct new feature in a first order representation experiment compare the effect of different hypothesis evaluation strategy domain representation and feature construction struct is also compared with quinlan s foil on two domain the result show that a modified fringe algorithm improves accuracy but that it is sensitive to the distribution of the example 
the kind of resource limitation that is most evident in commonsense reasoner is the passage of time while the reasoner reason there is not necessarily any fixed and final set of consequence with which such a reasoning agent end up in formalizing commonsense reasoner then one must be able to take into account that time is passing a the reasoner is reasoning the reasoner can then make use of such information in subsequent deduction step logic is such a formalism it wa developed in elgot drapldn to model the on going process of deduction conclusion are drawn step by step there is no final state of reasoning the emphasis is on intermediate conclusion in this paper we use step logic to model the three wisemen problem although others have formalized this problem they have ignored the time aspect that is inherent in the problem a correct assessment of the situation is made by recognizing that the reasoning process take time and determining that the other wise men would have concluded such and such by now this is an important aspect of the problem that need to be addressed 
the overfit problem in inductive learning and the utility problem in speedup learning both describe a common behavior of machine learning method the eventual degradation of performance due to increasing amount of learned knowledge plotting the performance of the changing knowledge during execution of a learning method the performance response reveals similar curve for several method the performance response generally indicates an increase to a single peak followed by a more gradual decrease in performance the similarity in performance response suggests a model relating performance to the amount of learned knowledge this paper provides empirical evidence for the existence of a general model by plotting the performance response of several learning program formal model of the performance response are also discussed these model can be used to control the amount of learning and avoid degradation of performance 
the distributed associative memory dam ha been described previously a a powerful method for pattern recognition we show that it also can be used for preattentive and attentive vision the basis for the preattentive system is that both the visual input feature a well a the memory are arranged in a pyramid this enables the system to provide fast preselection of region of visual interest the selected area of interest are used in an attentive recognition stage where the memory and the feature work at full resolution the reason for application of dam is based on a statistical theory of rejection the availability of a reject option in the dam is the prerequisite for novelty detection and preattentive selection we demonstrate the performance of the method on two diverse application 
agent in multiagent system interact to a large extent by communicating such communication may be fruitfully studied from the point of view of speech act theory in order for multiagent system to be formally and rigorously designed and analyzed a semantics of speech act that give their objective model theoretic condition of satisfaction is needed however most research into multiagent system that deal with communication provides only informal description of the different message 
this paper discus the application of algorithmic spelling correction technique to the identification of those word in a database of th century english text that are most similar to a query word in modern english the experiment have used n gram matching non phonetic coding and dynamic programming method for spelling correction and have demonstrated that high recall search can be carried out although some of the search are very demanding of computational resource the method are in principle applicable to historical text in many language and from many diffeent period 
the aim of this paper is to provide a hasis for a theory of event and process that can be used for reasoning about arbitrarily complex dynamic domain involving multiple agent the approach is based on a model of event that explicitly represents the domain of influence of each event by scoping an event s domain of influence most of the problem that have plagued the more conventional stated transition model of event can be avoided the effect of performing event either in isolation or concurrently with other event is described to represent constraint among event a model of process is developed this allows the modelling of arbitrarily complex behaviour finally a representation of causal influence is provided that allows the ramification of any given event occurrence to be modelled 
binocular difference in orientation and foreshortening are systematically related to surface slant and tilt and could potentially be exploited by biological and machine vision system indeed human stereopsis may posse a mechanism that specifically make use of these orientation and spatial frequency disparity in addition to the usual cue of horizontal disparity in machine vision algorithm orientation and spatial frequency disparity are a source of error in finding stereo correspondence because one seek to find feature or area which are similar in the two view when in fact they are systematically different in other word it is common to treat a noise what is useful signal 
this paper describes the integration of abstraction and explanation based learning ebl in the context of the prodigy system prodigy s abstraction module creates a hierarchy of abstract problem space so problem solving can proceed in a more directed fashion the ebl module acquires search control knowledge by analyzing problemsolving trace when the two module are integrated they tend to complement each other s capability resulting in performance improvement that neither system can achieve independently we present empirical result showing the effect of combining the two module and describe the factor that influence the overall performance of the integrated system 
in this paper we address the problem of making correct decision in the context of game playing specifically we address the problem of reducing or eliminating pathology in game tree however the framework used in the paper applies to decision making that depends on evaluating complex boolean expression the main contribution of this paper is in casting general evaluation of game tree a belief propagation in causal tree this allows u to draw several theoretically and practically interesting corollary in the bayesian framework we typically do not want to ignore any evidence even if it may be inaccurate therefore we evaluate the game tree on several level rather than just the deepest one choosing the correct move in a game can be implemented in a straightforward fashion by an efficient linear time algorithm adapted from the procedure for belief propagation in causal tree we propose a probabilistic ally sound heuristic that allows u to reduce the effect of pathology significantly 
we address the problem of selecting an attribute and some of it value for branching during the top down generation of decision tree we study the class of impurity measure member of which are typically used in the literature for selecting attribute during decision tree generation e g entropy in id gid and cart gini index in cart we argue that this class of measure is not particularly suitable for use in classification learning we define a new class of measure called c sep that we argue is better suited for the purpose of class separation a new measure from c sep is formulated and some of it desirable property are shown finally we demonstrate empirically that the new algorithm o btree that us this measure indeed produce better decision tree than algorithm that use impurity measure 
during the past decade knowledge representation research in ai ha generated a class of language called term subsumption language tsl which is a knowledge representation formalism with a well defined logic based semantics due to it formal semantics a term subsumption system can automatically infer the subsumption relationship between concept defined in the system however these system are very limited in handling vague concept in the knowledge base in contrast fuzzy logic directly deal with the notion of vagueness and imprecision using fuzzy predicate fuzzy quantifier linguistic variable and other construct hence fuzzy logic offer an appealing foundation for generalizing the semantics of term subsumption language based on a test score semantics in fuzzy logic this paper first generalizes the semantics of term subsumption language then we discus impact of such a generalization to the reasoning capability of term subsumption system the generalized knowledge representation framework not only alleviates the difficulty of conventional ai knowledge representation scheme in handling imprecise and vague information but also extends the application of fuzzy logic to complex intelligent system that need to perform highlevel analysis using conceptual abstraction 
the inverse kinematics problem for redundant manipulator is ill posed and nonlinear there are two fundamentally different issue which result in the need for some form of regularization the existence of multiple s olution branch global ill posedness and the existence of excess degree of freedom local illposedness for certain class of manipulator learning method applied to input output data generated from the forward function can be used to globally regularize the problem by partitioning the domain of the for ward mapping into a finite set of region over which the inverse problem is well posed local regularization can be accomplished by an appropriate parameterization of the redundancy consistently over each region a a result the i ll posed problem can be transformed into a finite set of well posed problem eachcan then be solved separately to construct approximate direct inverse functi ons 
in spite of the popularity of explanation based learning ebl it theoretical basis is not well understood using a generalization of probably approximately correct pac learning to problem solving domain this paper formalizes two form of explanation based learning of macro operator and prof the sufficient condition for their success these two form of ebl called macro caching and serial parsing respectively exhibit two distinct source of power or bias the sparseness of the solution space and the decomposability of the problem space the analysis show that exponential speedup can be achieved when either of these bias is suitable for a domain somewhat surprisingly it also show that computing the precondition of the macro operator is not necessary to obtain these speedup the the oretical result are confirmed by experiment in the domain of eight puzzle our work suggests that the best way to address the utility problem in ebl is to implement a bias which exploit the problem space structure of the set of domain that one is interested in learning 
this paper present an extension to line of ullman s incremental rigidity scheme originally formulated for a set of point the formulation is based on the angular and distance invariance of rigid configuration of line it is shown that the line structure can be recovered incrementally from it motion 
interval consistency problem deal with event each of which is assumed to be an interval on the real line or on any other linearly ordered set this paper deal with problem in reasoning about such interval when the precise topological relationship between them is unknown or only partially specified this work unifies notion of interval algebra for temporal reasoning in artificial intelligence with those of interval order and interval graph in combinatorics obtaining new algorithmic and complexity result of interest to both discipline several version of the satisjiability minimum labeling and all consistent solution problem for temporal interval data are investigated the satisfiability question is shown to be np complete even when restricting the possible interval relationship to subset of the relation intersection and precedence only on the other hand we give efficient algorithm for several other restriction of the problem many of these problem are also important in molecular biology archaeology and resolving mutual exclusion constraint in circuit design 
there are many new application field for automated deduction where we have to apply abductive reasoning in these application we have to generate consequence of a given theory having some appropriate property in particular we consider the case where we have to generate the clause containing instance of a given literal l the negation of the other literal in such clause are hypothesis allowing to derive l in this paper we present an inference rule called l inference which wa designed in order to derive those clause and a l strategy the l inference rule is a sort of input hyper resolution the main result of the paper is the proof of the soundness and completeness of the l inference rule the l strategy associated to the l inference rule is a saturation by level with deletion of the tautology and of the subsumed clause we show that the l strategy is also complete 
many learning from experience system use information extracted from problem solving experience to modify a performance element pe forming a new element pe that can solve these and similar problem more efficiently however a transformation that improve performance on one set of problem can degrade performance on other set the new pe is not always better than the original pe this depends on the distribution of problem we therefore seek the performance element whose expected performance over this distribution is optimal unfortunately the actual distribution which is needed to determine which element is optimal is usually not known moreover the task of finding the optimal element even knowing the distribution is intractable for most interesting space of element this paper present a method palo that side step these problem by using a set of sample to estimate the unknown distribution and by using a set of transformation to hill climb to a local optimum this process is based on a mathematically rigorous form of utility analysis in particular it us statistical technique to determine whether the result of a proposed transformation will be better than the original system we also present an efficient way of implementing this learning system in the context of a general class of performance element and include empirical evidence that this approach can work effectively 
although version space provide a useful conceptual tool for inductive concept learning they often face severe computational difficulty when implemented for example the g set of traditional boundary set implementation of version space can have size exponential in the amount of data for even the most simple conjunctive description language haussler this paper present a new representation for version space that is more general than the traditional boundary set representation yet ha worst case time complexity that is polynomial in the amount of data when used for learning from attribute value data with tree structured feature hierarchy which includes language like haussler s the central idea underlying this new representation is to maintain the traditional s boundary set a usual but use a list n of negative data rather than keeping a g set a is typically done 
we establish the motion equation for rigidly moving d line and the structure equation that relate a temporal match of d line in three consecutive picture in an image sequence we also analyse in detail the numerical stability of such estimation 
we present here a new formalization of belief which ha a simpler semantics than the previous formalization and develop an inference method for it by generalizing the resolution method the usual prepositional formula are embedded in our logic a a special type of belief formula one can obtain a non monotonic logic of belief by applying say circumscription to the basic belief logic developed here which is monotonic in nature one can also apply the technique repeatedly to construct a hierarchy of belief logic blk k where blk blk and blkcan handle formula involving up to level k nested application of the belief operator b 
the first part of this paper briefly describes a mathematical framework called the containment model that provides the operation and data structure for a text dominated database with a hierarchical structure the database is considered to be a hierarchical collection of continuous extent each extent being a word word phrase text element or non text element the filter operation making up a search command are expressed in term of containment criterion that specify whether a contiguous extent will be selected or rejected during a search this formalism comprised of the mathematical framework and it associated language defines a conceptual layer upon which we can construct a well defined higher level layer specifically the user interface that serf to provide a level of functionality that is closer to the need of the user and the application domain with the conceptual layer established we go on to describe the design and implementation of a versatile interface which handle query that search and navigate a heterogeneous collection of structured document interface functionality is provided by a set of worker module supported by an environment that is the same for all interface the interface environment allows a worker to communicate with the underlying text retrieval engine using a well defined command protocol that is based on a small set of filter operator the overall design emphasizes a interface flexibility for a variety of search and browsing capability b the modular independence of the interface with respect to it underlying retrieval engine and c the advantage to be accrued by defining retrieval command using operator that are part of a text algebra that provides a sound theoretical foundation for the database 
this paper investigates the effect of parallelism on blackboard system scheduling a parallel blackboard system is described that allows multiple knowledge source instantiation to execute in parallel using a shared memory blackboard approach new class 
membership query extended with the meta query concept is proposed a a method to acquire complex classification rule furthermore relevent concept class where a small number of query is sufficient are characterized in this paper we advocate and present the benefit of the use of query in order to learn a target concept efficiently thus providing the foundation for automating the knowledge acquisition process based on these result we developed a knowledge acquisition tool kac z which us query about specific domain object the system usefulness ha been demonstrated by it application in the domain of manufacturing cutting industry 
this paper describes a real time d vision system which us stereo matching of vertical edge segment the system is designed to permit a mobile robot to avoid obstacle and to position itself within an indoor environment the system us real time edge tracking to lock onto stereo match stereo matching is performed using a global version of dynamic programming for matching stereo segment 
we present a new compositional tense aspect deindexing mechanism that make use of a component of discourse context the mechanism allows reference episode to be correctly identified even for embedded clause and for discourse that involves shift in temporal perspective and permit deindexed logical form to be automatically computed with a small number of deindexing rule 
we investigate the complexity of reasoning with monotonic inheritance hierarchy that contain beside isa edge also role or function edge a role edge is an edge labelled with a name such a spouse of or brother of we call such network isar network given a network with n vertex and m edge we consider two problem p determining whether the network implies an isa relation between two particular node and p determining all isa relation implied by the network a is well known without role edge the time complexity of p is o m and the time complexity of p is o n unfortunately the result do not extend naturally to isar network except in a very restricted case for general isar network we first give an polynomial algorithm by an easy reduction to proposional horn theory a the degree of the polynomial is quite high o m n for p o m n for p we then develop a more direct algorithm for both p and p it complexity is o n m actually a finer analysis of the algorithm reveals a complexity of o nr log r n r n where r is the number of different role label one corolary is that if we fix the number of role label the complexity of our algorithm drop back to o n 
of all the possible way of computing abductive explanation the atm procedure is one of the most popular while this procedure is known to run in exponential time in the worst case the proof actually depends on the existence of query with an exponential number of answer but how much of the difficulty stem from having to return these large set of explanation here we explore abduction task similar to that of the atm but which return relatively small answer the main result is that although it is possible to generate some non trivial explanation quickly deciding if there is an explanation containing a given hypothesis is np hard a is the task of generating even one explanation expressed in term of a given set of assumption letter thus the method of simply listing all explanation a employed by the atm probably cannot be improved upon an interesting result of our analysis is the discovery of a subtask that is at the core of generating explanation and is also at the core of generating extension in reiter s default logic moreover it is this subtask that account for the computational difficulty of both form of reasoning this establishes for the first time a strong connection between computing abductive explanation and computing extension in default logic 
the manuscript submitted for the hypertext conference were assigned to member of the review committee using a variety of automated method based on information retrieval principle and latent semantic indexing fifteen reviewer provided exhaustive rating for the submitted abstract indicating how well each abstract matched their interest the automated method do a fairly good job of assigning relevant paper for review but they are still somewhat poorer than assignment made manually by human expert and substantially poorer than an assignment perfectly matching the reviewer own ranking of the paper a new automated assignment method called n of n achieves better performance than human expert by sending reviewer more paper than they actually have to review and then allowing them to choose part of their review load themselves 
biological retina extract spatial and temporal feature in an attempt to reduce the complexity of performing visual task we have built and tested a silicon retina which encodes several useful temporal feature found in vertebrate retina the cell in our silicon retina are selective to direction highly sensitive to positive contrast change around an ambient light level and tuned to a particular velocity inhibitory connection in the null direction perform the direction selectivity we desire this silicon retina is on a mm die and consists of a array of photoreceptors 
we describe an implemented system that learns to recognize human face under varyingpose and illumination condition the system relies on symmetry operation to detectthe eye and the mouth in a face image us the location of these feature to normalizethe appearance of the face performs simple but effective dimensionality reduction by aconvolution with a set of gaussian receptive field and subject the vector of activity ofthe receptive field to a radial basis function 
quot trajectory extension learning quot is a new technique for learningcontrol in robot which assumes that there exists some parameterof the desired trajectory that can be smoothly varied from a regionof easy solvability of the dynamic to a region of desired behaviorwhich may have more difficult dynamic by gradually varying theparameter practice movement remain near the desired path whilea neural network learns to approximate the inverse dynamic forexample the average speed of motion 
the parameter of the parameterized modal operator p and p usually represent agent in the epistemic interpretation or action in the dynamic logic interpretation or the like in this paper the application of the idea of parametrized modal operator is extended in in two way first of all a modified neighbourhood semantics is defined which permit among others the interpretation of the parameter a probability value a formula f may for example express the fact that in at least of all case world f hold these probability value can be number qualitative description and even arbitrary term secondly a general theory of the parameter and in particular of the characteristic operation on the parameter is developed which unifies for example the multiplication of number in the probabilistic interpretation of the parameter and the sequencing of action in the dynamic logic interpretation 
cost based abduction attempt to find the best explanation for a set of fact by finding a minimal cost proof for the fact the cost are computed by summing the cost of the assumption necessary for the proof plus the cost of the rule we examine existing method for constructing explanation proof a a minimization problem on a dag we then define a probabilistic semantics for the cost and prove the equivalence of the cost minimization problem to the bayesian network map solution of the system 
when autonomous agent attempt to coordinate action it is often necessary that they reach some kind of consensus reaching such a consensus ha traditionally been dealt with in the distributed artificial intelligence literature via the mechanism of negotiation another alternative is to have agent bypass negotiation by using a voting mechanism each agent express it preference and a group choice mechanism is used to select the result some choice mechanism are better than others and ideally we would like one that cannot be manipulated by an untruthful agent one such non manipulable choice mechanism is the clarke tax clarke though theoretically attractive the clarke tax present a number of difficulty when one attempt to use it in a practical implementation this paper examines how the clarke tax could be used a an effective preference revealer in the domain of automated agent reducing the need for explicit negotiation 
partial constraint are often available in visual processing task requiring the matching of contour in two image we propose a noniterative scheme to determine contour match using locally affine transformation the method assumes that contour are approximated by the orthographic projection of planar patch within oriented neighborhood of varying size for degenerate case a minimal matching solution is chosen closest to the minimal pure translation performance on noisy synthetic and natural contour imagery is reported 
universal attachment is a general purpose mechanism for integrating diverse representation structure and their associated inference program into a framework built on logical representation and theorem proving the integration is achieved by link referred to a universal attachment that connect logical expression to these structure and program in this paper we describe a compilation based method for automatically generating new program and new universal attachment to those program given a base set of existing program and universal attachment the generation method provides the mean to obtain large collection of attachment and attached program without the traditional specification overhead a well the method simplifies the task of validating that a collection of attachment is correct 
this paper describes the learning part of a system which ha been developed to provide expert system capability augmented with learning the learning scheme is a hybrid connectionist symbolic one a network representation is used learning may be done incrementally and requires only one pas through the data set to be learned attribute value pair are supported a a variable implementation variable are represented by group of connected cell in the network the learning algorithm is described and an example given current result are discussed which include learning the well known iris data set the result show that the system ha promise 
this paper describes the application of explanation based learning a machine learning technique to the sri core language engine a large scale general purpose natural language analysis system the idea is to bypass normal morphological syntactic and 
in this paper we describe a method for hybridiz ing a genetic algorithm and a k nearest neighbor classification algorithm we use the genetic algo rithm and a training data set to learn real valued weight associated with individual attribute in the data set we use the k nearest neighbor algo rithm to classify new data record based on their weighted distance from the member of the train ing set we applied our hybrid algorithm to three test case classification result obtained with the hybrid algorithm exceed the performance of the k nearest neighbor algorithm in all three case 
this paper investigates the effect of parallelism on blackboard system scheduling a parallel blackboard system is described that allows multiple knowledge source instantiation to execute in parallel using a shared memory blackboard approach new class 
we present an algebraic approach to geometric reasoning and learning the purpose of this research is to avoid the usual difficulty in symbolic handling of geometric concept our system grew is grounded on a reasoning scheme that integrate the symbolic reasoning and algebraic reasoning of wu s method the basic principle of this scheme is to describe mathematical knowledge in term of symbolic logic and to execute the subsidiary reasoning for wu s method the validity of our approach and grew is shown by experiment such a applying to learning by example of computer vision heuristic or solving locus problem 
this paper present an approach to learning from noisy data that view the problem a one of reasoning under uncertainty where prior knowledge of the noise process is applied to compute a posteriori probability over the hypothesis space in preliminary experiment this maximum a posteriori map approach exhibit a learning rate advantage over the c algorithm that is statistically significant 
this work address three problem with reinforcement learning and adaptiveneuro control non markovian interface between learner and environment on line learning based on system realization vectorvaluedadaptive critic an algorithm is described which is based on systemrealization and on two interacting fully recurrent continually running networkswhich may learn in parallel problem with parallel learning areattacked by adaptive randomness it is also described how 
collocational knowledge is necessary for language generation the problem is that collocation come in a large variety of form they can involve two three or more word these word can be of different syntactic category and they can be involved in more or le rigid way this lead to two main difficulty collocational knowledge ha to be acquired and it must be represented flexibly so that it can be used for language generation we address both problem in this paper focusing on the acquisition problem we describe a program xtract that automatically acquires a range of collocation from large textual corpus and we describe how they can be represented in a flexible lexicon using a unification based formalism 
batch gradient descent deltaw t gammajde dw t converges to a minimumof quadratic form with a time constant no better than max min where min and max are the minimum and maximum eigenvalue of the hessianmatrix of e with respect to w it wa recently shown that adding amomentum term deltaw t gammajde dw t ff deltaw t gamma improves this to pmax min although only in the batch case here we show that secondordermomentum deltaw t gammajde dw t 
reliability defined a the guarantee that a program satisfies it specification is an important aspect of many application for which rule based program are suited executing rule based program on a series of test case doe not guarantee correct behavior in all possible test case to show a program is reliable it is desirable to construct formal specification for the program and to prove that it obeys those specification this paper present an assertional approach to the verification of a class of rule based program characterized by the absence of conflict resolution the proof logic needed for verification is already in use by researcher in concurrent programming the approach involves expressing the program in a language called swarm and it specification a assertion over the swarm program among model that employ rule based notation swarm is the first to have an axiomatic proof logic a brief review of swarm and it proof logic is given along with an illustration of the formal verification method used on a simple rule based program 
we present a strategy for restricting the application of the inference rule paramodulation the strategy applies to problem in first order logic with equality and is designed to prevent paramodulation into subterms of skolem expression a weak completeness result is presented the functional reflexive axiom are assumed experimental result on problem in set theory combinatory logic tarski geometry and algebra show that the strategy can be useful when searching for refutation and when applying knuth bendix completion the emphasis of the paper is on the effectiveness of the strategy rather than on it completeness 
certain pronoun context are argued to establish a local center lc i e a conventionalized indexical similar to st nd pers pronoun demonstrative pronoun also indexicals are shown to access entity that are not lcs because they lack discourse relevance or because they are not yet in the universe of discourse 
this paper investigates the effect of parallelism on blackboard system scheduling a parallel blackboard system is described that allows multiple knowledge source instantiation to execute in parallel using a shared memory blackboard approach new class 
a number of researcher have investigated the use of planbased approach to generate textual explanation e g appelt hovy moore maybury b this paper extends this approach to generate multimedia explanation by defining three type of communicative act linguistic act illocutionary and locutionary speech act visual act e g deictic act and medium independent rhetorical act e g identify describe this paper formalizes several of these communicative act a operator in the library of a hierarchical planner a computational implementation is described which us these plan operator to compose route plan in coordinated natural language and graphic in the context of a cartographic information system 
a theoretical approach to the problem of intelligent regulation of data processing parameter is proposed in term of joint probability maximization it is shown that under suitable hypothesis the problem can be solved by maximizing in a distributed way the product of computationally more tractable conditional probability a a case study the implementation of an architecture made up of four unit is investigated 
we showed how to exploit motion concept associated with verb of locomotion for top down control in traffic scene two kind of constraint could be derived spatial constraint through knowledge about the applicability of motion concept and motion constraint through knowledge about typical motion we proposed to compute motion constraint using a spatio temporal buffer a a shared representation for bottom up and top down process within the buffer motion concept are expressed a typicality distribution from which prediction about object motion can be derived a local prediction algorithm allows for the computation of search area for low level motion analysis a low level motion representation based on spatio temporal gabor cell is well suited for the integration of this kind of top down information 
in this paper we show that the lvq learning algorithm converges to locally asymptotic stable equilibrium of an ordinary differential equation we show that the learning algorithm performs stochastic approximation convergence of the voronoi vector is guaranteed under the appropriate condition on the underlying statistic of the classification problem we also present a modification to the learning algorithm which we argue result in convergence of the lvq for a larger set of initial condition finally we show that lvq is a general histogram classifier and that it risk converges to the bayesian optimal risk a the appropriate parameter go to infinity with the number of past observation 
abstract all major approach to qualitative reasoning rely on the existence of a model of the physical system however the task of finding a model is usually far from trivial within the area of electrical engineering model building method have been developed to automatically deduce model from measurement in this paper we explicitly show how to incorporate qualitative knowledge in order to apply these method to situation where they do not behave satisfac torily a program ha been developed and ap plied to a non trivial example the qualitative input in term of an incomplete bond graph and the resulting output can be used to form a more complete bond graph this more infor mative model is suitable for further reasoning 
lexical disambiguation can be achieved using different source of information aiming at high performance of automatic disambiguation it is important to know the relative importance and applicability of the various source in this paper we classify several source of information and show how some of them can be achieved using statistical data first evaluation indicate the extreme importance of local information which mainly represents lexical association and selectional restriction for syntactically related word 
appropriate bias is widely viewed a the key to efficient learning and generalization i present a new algorithm the incremental delta bar delta idbd algorithm for the learning of appropriate bias based on previous learning experience the idbd algorithm is developed for the case of a simple linear learning system the lm or delta rule with a separate learning rate parameter for each input the idbd algorithm adjusts the learning rate parameter which are an important form of bias for this system because bias in this approach is adapted based on previous learning experience the appropriate test bed are drifting or non stationary learning task for particular task of this type i show that the idbd algorithm performs better than ordinary lm and in fact find the optimal learning rate the idbd algorithm extends and improves over prior work by jacob and by me in that it is fully incremental and ha only a single free parameter this paper also extends previous work by presenting a derivation of the idbd algorithm a gradient descent in the space of learning rate parameter finally i offer a novel interpretation of the idbd algorithm a an incremental form of hold one out cross validation 
this paper describes the implementation of a d vision algorithm droid on the oxford parallel vision architecture paradox and the result of experiment to gauge the algorithm s effectiveness in providing navigation data for an autonomous guided vehicle the algorithm reconstructs d structure by analysing image sequence obtained from a moving camera in this application the architecture delivers a performance of greater than frame per second time the performance of a sun alone 
in this paper we show a new approach for reasoning about time and probability that combine a formal declarative language with a graph representation of system of random variable for making inference first we provide a continuous time logic for expressing knowledge about time and probability then we introduce the time net a kind of bayesian network for supporting inference with statement in the logic time net encode the probability of fact and event over time we provide a simulation algorithm to compute probability for answering query about a time net finally we consider an incremental probabilistic temporal database based on the logic and time net to support temporal reasoning and planning application the result is an approach that is semantically well founded expressive and practical 
automatic speech understanding and automatic speech recognition extract different kind of information from the input signal the result of the former must be evaluated on the basis of the response of the system while the result of the latter is the word sequence which best match the input signal in both case search ha to be performed based on score of interpretation hypothesis a scoring method is presented based on stochastic context free grammar the method give optimal upper bound for the computation of the best derivation tree of a sentence this method allows language model to be built based on stochastic context free grammar and their use with an admissible search algorithm that interprets a speech signal with left to right or middle out strategy theoretical and computational aspect are discussed 
abstract the use of energy minimizing curve known a snake to extract feature of interest in image ha been introduced by ka witkin and terzopoulos a balloon model wa introduced in a a way to generalize and solve some of the problem encountered with the original method we present a d generalization of the balloon model a a d deformable surface which evolves in d image it is deformed under the action of internal and external force attracting the surface toward detected edgels by mean of an attraction potential to solve the minimization problem for a surface two simplified approach are shown first defining a d surface a a series of d planar curve then we solve the d model using the finite element method yielding greater stability and faster convergence we have applied this model for segmenting magnetic resonance image 
this paper describes a simple heuristic method for solving large scale constraint satisfaction and scheduling problem given an initial assignment for the variable in a problem the method operates by searching though the space of possible repair the search is guided by an ordering heuristic the min conflict heuristic that attempt to minimize the number of constraint violation after each step we demonstrate empirically that the method performs order of magnitude better than traditional backtracking technique on certain standard problem for example the one million queen problem can be solved rapidly using our approach we also describe practical scheduling application where the method ha been successfully applied a theoretical analysis is presented to explain why the method work so well on certain type of problem and to predict when it is likely to be most effective 
this paper present a computaional method of calculating the measure of salience in understanding metaphor we mainly treat metaphor in the form of a is like b in which a is called target concept and b is called source concept in understanding a metaphor some property of the source concept are transferred to the target concept in the transfer process we first have to select the property of the source concept that can be more preferably transferred to the target concept the measure of salience represents how typical or prominent the property is and is used to measure the transferability of the property by introducing the measure of salience we have to consider only the high salient property after the selection the measure of salience wa calculated from smith medin s probabilistic concept l according to tversky s two factor l one is intensity which refers to signal to noise ratio this is calculated from the entropy of property the other is diagnostic factor which refers to the classificatory significance of property this is calculated from the distribution of the property s intensity among similar concept finally we briefly outline the whole process of understanding metaphor using the measure of salience 
we present a new approach to model based monitoring and diagnosis of dynamic system the presented diamon algorithm us hierarchical model to monitor and diagnose dynamic system diamon is based on the integration of teleological parameter based monitoring model and repair oriented device based diagnosis model it combine consistency based diagnosis with model based monitoring and us an extension of the qsim language for the representation of qualitative system model furthermore diamon is able to detect and localize a broad range of nonpermanent fault and thus extends traditional diagnosis which exclusively deal with permanent faulty behavior the operation of diamon will be demonstrated on a real world example in a multiple fault scenario 
a task oriented system is one that performs the minimum effort necessary to solve a specified task depending on the task the system decides which information to gather which operator to use at which resolution and where to apply them we have been developing the basic framework of a task oriented computer vision system called tea that us bayes net and a maximum expected utility decision rule in this paper we present a method for incorporating geometric relation into a bayes net and then show how relational knowledge and evidence enables a task oriented system to restrict visual processing to particular area of a scene by making camera movement and by only processing a portion of the data in an image 
the analysis of large complex situation pose difficult problem for qualitative reasoning due to the complexity of reasoning from first principle and the proliferation of ambiguity abstraction is a promising bolution to these problem in this paper we study a type of abstraction behavioral aggregation the process of grouping a set of individual entity that collectively behave a a unit in particular we show how to build aggregate model of situation involving dynamic equilibrium and how to reason about their behavior finally we demonstrate through several example the benefit of reasoning at the aggregate level a reduction in the complexity of reasoning and a compact easily interpretable description of the behavior 
this paper deal with a neural network model in which each neuron performs a threshold logic function an important property of the model is that it always converges to a stable state when operating in a serial mode this property is the basis of the potential application of the model such a associative memory device and combinatorial optimization one of the motivation for use of the model for solving hard combinatorial problem is the fact that it can be implemented by optical device and thus operate at a higher speed than conventional electronics the main theme in this work is to investigate the power of the model for solving np hard problem and to understand the relation between speed of operation and the size of a neural network in particular it will be shown that for any np hard problem the existence of a polynomial size network that solves it implies that np co np also for the traveling salesman problem tsp even a polynomial size network that get an e approximate solution doe not exist unless p np the above result are of great practical interest because right now it is possible to build neural network which will operate fast but are limited in the number of neuron they contain 
we develop the notion that knowledge editing is a cooperative activity that requires knowledge editor to reach consensus a they represent information in a knowledge base we describe an intelligent knowledge editing tool the hit knowledge editor and illustrate how it assist knowledge editor in reaching consensus 
to design a task independent dialogue system we present a task oriented dialogue analysis in term of finding the referent of definite description and we show how this analysis lead to a goal oriented inferential representation of the task this representation provides a logical generic model of the task which is compatible with a belief system then we show how this task model jointly used with the domain specific user model for which we propose a formalization enables a dialogue system to plan request negotiation dialogue 
passive navigation refers to the ability of an organism or a robot that move in it environment to determine it own motion precisely on the basis of some perceptual input for the purpose of kinetic stabilization the problem ha been treated for the most part a a general recovery from dynamic imagery problem and it ha been formulated a the general d motion estimation or structure from motion module consequently if a robust solution to the passive navigation problem a it ha been formulated in the recovery paradigm is achieved we will immediately be able to solve many other important problem a simple application of the general principle however despite numerous theoretical result no technique ha found application in system that can perform well in the real world in this paper we outline some of the reason behind this and we develop a robust solution to the passive navigation problem which is 
lqms is a knowledge based system that identifies and explains anomaly in data acquired from multiple sensor the knowledge base wa built by a sequence of domain expert it prototype performed with a high level of accuracy and that performance ha been incrementally and significantly improved during development and field testing several point are developed in this paper the combination of an intuitive model sufficient for the task and powerful graphical development tool allowed the domain expert to build a large high performance system the observation situation relation representation illustrates an intermediate point on the simplicity expressiveness spectrum which is understandable to the domain expert while being expressive enough for the diagnostic task the system wa designed a a workbench for the domain expert this enticed them to become more directly involved and resulted in a better system the use of an integrated knowledge base edit tracking system wa important to the project in several way it reassured computer naive expert that they could not damage the overall system which increased their productivity and it also allowed expert located in various place around the world to compare contrast and integrate change in a structured way 
access limited logic all is a language for knowledge representation which formalizes the access limitation inherent in a network structured knowledge base where a deductive method such a resolution would retrieve all assertion that satisfy a given pattern an access limited logic retrieves all assertion reachable by following an available access path in this paper we extend previous work to include negation disjunction and the ability to make assumption and reason by contradiction we show that the extended allneg remains socratically complete thus guaranteeing that for any fact which is a logical consequence of the knowledge base there exists a series of preliminary query and assumption after which a query of the fact will succeed and computationally tractable we show further that the key factor determining the computational difficulty of finding such a series of preliminary query and assumption is the depth of assumption nesting we thus demonstrate the existence of a family of increasingly powerful inference method parameterized by the depth of assumption nesting ranging from incomplete and tractable to complete and intractable 
we obtain here the complexity of solving a type of prolog problem which genesereth and nilsson have called sequential constraint satisfaction such problem are of direct relevance to relational database retrieval a well a providing a tractable first step in analyzing prolog problem solving in the general case the present paper provides the first analytic expression for the expected complexity of solving sequential constraint satisfaction problem these expression provide a basis for the formal derivation of heuristic for such problem analogous to the theory based heuristic obtained by the author for traditional constraint satisfaction problem solving a first application ha been in providing a formal basis for warren s heuristic for optimally ordering the goal in a conjunctive query due to the incorporation of constraint looseness into the analysis the expected complexity obtained here ha the useful property that it is usually quite accurate even for individual problem instance rather than only for the assumed underlying problem class a a whole heuristic based on these result can be expected to be equally instance specific preliminary result for warren s heuristic have shown this to be the case 
this paper describes the application of explanation based learning a machine learning technique to the sri core language engine a large scale general purpose natural language analysis system the idea is to bypass normal morphological syntactic and 
bandwidth is a fundamental concept in graph theory which ha some surprising application to a class of ai search problem graph bandwidth provides a link between the syntactic structure of a constraint satisfaction problem csp and the complexity of the underlying search task bandwidth can be used to define a new class of easy csp s namely those that have limited constraint graph bandwidth these csp s can be solved in polynomial time essentially by divide and conquer this in turn suggests that bandwidth provides a mathematical measure of the decomposability of a search problem in addition bandwidth supply a measure for comparing different search ordering for a given csp statistical analysis suggests that backtracking with small bandwidth ordering lead to a more efficient search than that obtained under ordering with larger bandwidth small bandwidth ordering also limit the pruning that can be done by intelligent backtracking if small bandwidth ordering are indeed advantageous then a large number of heuristic developed in numerical analysis to find such ordering may find applicability to solving constraint satisfaction problem 
we propose a d interpretation system where knowledge is represented by a set of proposition and where interpretation and truth maintenance are based on a consistent labeling of this set of proposition the basic concept are illustrated on the problem of d interpretation of image straight line correspondence 
conventional envisioners proposed in qualitative physic have two difficulty in common ambiguity in prediction and inability of reasoning about global behavior we take a geometric approach to overcome these difficulty and have implemented a program psx nl which can reason about global behavior by analyzing geometry and topology of solution curve of ordinary differential equation in the phase space in this paper we highlight a flow grammar which specifies possible pattern of solution curve one may see in the phase space the role of a flow grammar in psx nl is twofold firstly it allows psx nl to reason about complex pattern in a uniform manner secondly it allows psx nl to switch to an approximate top down algorithm when complete geometric clue are not available due to the difficulty of mathematical problem encountered 
we propose a new approach to build a fuzzy inference system of which the parameter can be updated to achieve a desired input output mapping the structure of the proposed fuzzy inference system is called generalized neural network and it learning procedure rule to update parameter is basically composed of a gradient descent algorithm and kalman filter algorithm specifically we first introduce the concept of generalized neural network gnn s and develop a gradient descent based supervised learning procedure to update the gnn s parameter secondly we observe that if the overall output of a gnn is a linear combination of some of it parameter then these parameter can be identified by one time application of kalman filter algorithm to minimize the squared error according to the simulation result it is concluded that the proposedl new fuzzy inference system can not only incorporate prior knowledge about the original system but also fine tune the membership function of the fuzzy rule a the training data set varies 
in the best case using an abstraction hierarchy in problem solving can yield an exponential speed up in search efficiency such a speed up is predicted by various analytical model developed in the literature and efficiency gain of this order have been confirmed empirically however these model assume that the downward refinement property drp hold when this property hold backtracking never need occur across abstraction level when it fails search may have to consider many different abstract solution before finding one that can be refined to a concrete solution in this paper we provide an analysis of the expected search complexity without assuming the drp we find that our model predicts a phase boundary where abstraction provides no benefit if the probability that an abstract solution can be refined is very low or very high search with abstraction yield significant speed up however in the phase boundary area where the probability take on an intermediate value search efficiency is not necessarily improved the phenomenon of a phase boundary where search is hardest agrees with recent empirical study of cheeseman et al ckt 
when rule are executed in a parallel production system the goal of control is to ensure both that a high quality solution is achieved and that processing resource are used effectively we argue that the conventional conflict resolution algorithm is not suitable a a control mechanism for parallel rule firing system the necessity for examining all eligible rule within a system imposes a synchronization delay which limit processor utilization rather than perform conflict resolution we propose that rule should be executed asynchronously a soon a they become enabled however this approach leaf the problem of controlling the computation unsolved we have identified three distinct type of control program sequencing heuristic control and dynamic scheduling which are required for efficient and correct parallel execution of rule we discus the issue involved in implementing each type of control without undue overhead within the context of our system a parallel rule firing system with an augmented agenda manager 
we address the problem of generating adjective in a text generation system we distinguish between usage of adjective informing the hearer of a property of an object and usage expressing an intention of the speaker or an argumentative orientation for such argumentative usage we claim that a generator cannot simply map from information in the knowledge base to adjective instead we identify various knowledge source necessary to decide whether to use an adjective what adjective should be selected and what syntactic function it should have we show how these decision interact with lexical property of adjective and the syntax of the clause we propose a mechanism for adjective selection and illustrate it in the context of the explanation component of the advisor expert system we describe an implementation of adjective selection using a version of functional unification grammar 
evidential reasoning is a body of technique that support automated reasoning from evidence it is based upon the dempster shafer theory of belief function both the formal basis and a framework for the implementation of automated reasoning system based upon these technique are presented the formal and practical approach are divided into four part specifying a set of distinct propositional space each of which delimits a set of possible world situation specifying the interrelationship among these propositional space representing body of evidence a belief distribution over these propositional space and establishing path for the body of evidence to move through these propositional space by mean of evidential operation eventually converging on space where the target question can be answered 
dictionary contain a rich set of relationship between their sens but often these relationship are only implicit we report on our experiment to automatically identify link between the sens in a machine readable dictionary in particular we automatically identify instance of zero affix morphology and use that information to find specific linkage between sens this work ha provided insight into the performance of a stochastic tagger 
in this paper we report on our use of zero morpheme in unification based combinatory categorial grammar after illustrating the benefit of this approach with several example we describe the algorithm for compiling zero morpheme into unary rule which allows u to use zero morpheme more efficiently in natural language processing then we discus the question of equivalence of a grammar with these unary rule to the original grammar lastly we compare our approach to zero morpheme with possible alternative 
in this paper we describe a formal semantic model that applies to many classical planning system this give a unifying framework in which to study diverse planner and motivates formal logic that can be used to study their property a an example of the model s utility we present a general truth criterion which test for the necessary truth of a proposition at arbitrary point in the planning process 
many auditory theorist consider the temporal adaptation of theauditory nerve a key aspect of speech coding in the auditory periphery experiment with model of auditory localization and pitchperception also suggest temporal adaptation is an important elementof practical auditory processing i have designed fabricated and successfully tested an analog integrated circuit that modelsmany aspect of auditory nerve response including temporal adaptation introductionwe are modeling 
in this paper we describe the concept of physical impossibility a an alternative to the specification of fault model these axiom can be used to exclude impossible diagnosis similar to fault model we show for horn clause theory while the complexity of finding a first diagnosis is worst case exponential for fault model it is polynomial for physical impossibility axiom even for the case of finding all diagnosis using physical impossibility axiom instead of fault model is more efficient although both are exponential in the worst case these result are used for a polynomial diagnosis and measurement strategy which find a final sufficient diagnosis 
a compositional semantics for focusing subjuncts word such a only even and also is developed from rooth s theory of association with focus by adapting the theory so that it can be expressed in term of a frame based semantic formalism a semantics that is more computationally practical is arrived at this semantics capture pragmatic subtlety by incorporating a two part representation and recognizes the contribution of intonation to meaning 
decomposing a difficult problem into simpler subproblems is a classic problem solving technique unfortunately the most difficult subproblems can be a difficult if not more difficult than the original problem this is not an obstacle to problem solving if the difficult subproblems recur in other problem if the difficult subproblems recur often then it solution need only be learned once and reused steppingstone is a learning problem solver that decomposes a problem into simple and difficult but recurring subproblems it solves the simple subproblems with an inexpensive constrained problem solver to solve the difficult subproblems steppingstone us an unconstrained problem solver once it solves a difficult subproblem it us the solution to generate a sequence of subgoals or steppingstones that can be used by the constrained problem solver to solve this difficult subproblem when it occurs again in this paper we provide analytical evidence for steppingstone s capability a well a empirical result from our work with the domain of logic synthesis 
we present an approach to the semantic labelling of edge and reconstruction of range data by the fusion of registered range and intensity data this is achieved by using bayesian estimation within coupled markov random field mrf employing the constraint of surface smoothness and edge continuity 
this paper present a computational model that segment image based on the textural property of object surface the proposed coupled membrane model applies the weak membrane approach to an image wl o y derived from the power response of a family of selfsimilar quadrature gabor wavelet while segmentation break are allowed in x and y only coupling is introduced to in all dimension the resulting spatial and spectral diffusion prevents minor variation in local texture from producing segmentation boundary experiment showed that the model is adequate in segmenting a class of synthetic and natural texture image 
this paper describes a rational reconstruction of einstein s discovery of special relativity validated through an implementation the erlanger program einstein s discovery of special relativity revolutionized both the content of physic and the research strategy used by theoretical physicist this research strategy entail a mutual bootstrapping process between a hypothesis space for bias defined through different postulated symmetry of the universe and a hypothesis space for physical theory the invariance principle mutually constrains these two space the invariance principle enables detecting when an evolving physical theory becomes inconsistent with it bias and also when the bias for theory describing different phenomenon are inconsistent structural property of the invariance principle facilitate generating a new bias when an inconsistency is detected after a new bias is generated this principle facilitates reformulating the old inconsistent theory by treating the latter a a limiting approximation 
this paper investigates the effect of parallelism on blackboard system scheduling a parallel blackboard system is described that allows multiple knowledge source instantiation to execute in parallel using a shared memory blackboard approach new class 
this paper present a scene interpretation system in a context of multi sensor fusion we present how the real world and the interpreted scene are modeled knowledge about sensor and multiple view notion shot are taken into account some result are shown from an application to sar spot image interpretation 
a method of determining the similarity of noun on the basis of a metric derived from the distribution of subject verb and object in a large text corpus is described the resulting quasi semantic classification of noun demonstrates the plausibility of the distributional hypothesis and ha potential application to a variety of task including automatic indexing resolving nominal compound and determining the scope of modification 
this session will explore the reason for the lack of impact in four important area in which ai ha been expected to significantly affect real world software engineering the panelist each representing one of these area will respond to the conjecture that these failure rest upon a common cause reliance on isolationist technology and approach rather than upon creating additive technology and approach that can be integrated with other existing capability 
we have addressed the problem of analyzing image containing multiple sparse overlapped pattern this problem arises naturally when analyzing the composition of organic macromolecule using data gathered from their nmr spectrum using a neural network approach we have obtained excellent result in using nmr data to analyze the presence of various amino acid in protein molecule we have achieved high correct classification percentage about for image containing a many a five sub stantially distorted overlapping pattern 
although block world planning is well known it complexity ha not previously been analyzed and different planning researcher have expressed conflicting opinion about it difficulty in this paper we present the following result finding optimal plan in a well known formulation of the block world planning domain is np hard even if the goal state is completely specified classical example of deleted condition interaction such a sussman s anomaly and creative destruction are not difficult to handle in this domain provided that the right planning algorithm is used instead the np hardness of the problem result from difficulty in determining which of several different action will best help to achieve multiple goal 
we describe an envisionment based qualitative simulation program the program implement part of an axiomatic first order theory that ha been developed to represent and reason about space and time topological information from the modelled domain is expressed a set of distinct topological relation holding between set of object these form the qualitative state in the underlying theory and simulation process in the theory are represented a path in the envisionment tree the algorithm is illustrated with an example of a simulation of phagocytosis and exocytosis two process used by unicellular organism for garnering food and expelling waste material respectively 
acp is a fully implemented constraint propagation system that computes numeric interval for variable davis along with an atm label de kleer a for each such interval the system is built within a focused atm architecture forbus and de kleer dressler and farquhar and incorporates a variety of technique to improve efficiency 
structural aggreg ation is an inherent aspect of our ability in reasoning about the real world in this paper we present our investigation of structural aggregation to simplify domain model and suppress irrelevant detail of complex physical system we address the role of clustering cluster orientation and creation of black box in qualitative causal reasoning we describe algorithm that automate structural aggregation to achieve efficiency and clarity in planning and explanation of physical system behavior 
the performance of production program can be improved by firing multiple rule in a production cycle although considerable amount of research ha been done on parallel processing of production program the problem of multiple rule firing ha not been thoroughly investigated yet in this paper we begin by identifying the problem associated with multiple rule firing system the compatibility problem and the convergence problem and present three multiple rule firing model which address them the rule dependence model rdm address the compatibility problem using inter rule data dependence analysis the single context multiple rule scmrj model and the multiple context multiple rule mcmr model address both the compatibility and the convergence problem a production program executed under the scmr and the mcmr model is guaranteed to reach a solution which is equivalent to the sequential execution these three multiple rule firing model have been simulated on the rubic simulator and the mcmr model which ha the highest performance ha been implemented on the intel ipsc hypercube the simulation and implementation result are reported 
model based computer vision system which recognize object in single grayscale image require the matching of stored object model and the image data resulting from the perspective projection if object may be located arbitrarily in relation to each other occlusion can occur thereby creating different line configuration in the projected image with different viewing direction for a collection of polyhedral object containing n vertex there are of the order o n different general view and o n degenerate view and the algoritmic complexity for constructing a view list is o n where n is the sum of polyhedral bounding face part of the assembly convex hull and plane arising from visual interaction of polyhedral part inside the convex hull 
this contribution address the problem of detection and tracking of moving vehicle in image sequence from traffic scene recorded by a stationary camera in order to exploit the a priori knowledge about the shape and the physical motion of vehicle in traffic scene a parameterized vehicle model is used for an intraframe matching process and a recursive estimator based on a motion model is used for motion estimation the initial guess about the position and orientation for the model are computed with the help of a clustering approach of moving image feature shadow edge of the model are taken into account in the matching process this enables tracking of vehicle under complex illumination condition and within a small effective field of view result on real world traffic scene are presented and open problem are outlined 
this paper introduces an approach to visual sampling and reconstruction motivated by concept from numerical grid generation we develop adaptive mesh that can nonuniformly sample and reconstruct intensity and range data adaptive mesh are dynamic model which are assembled by interconnecting nodal mass with adjustable spring acting a mobile sampling site the node observe interesting property of the input data such a intensity depth gradient and curvature based on these nodal observation the spring automatically adjust their stiffness so a to distribute the available degree of freedom of the reconstructed model in accordance with the local complexity of the input data the adaptive mesh algorithm run at interactive rate with continuous d display on a graphic workstation we apply it to the adaptive sampling and reconstruction of image and surface 
a new approach to bottom up parsing that extends augmented context free grammar to a process grammar is formally presented a processor grammar pg defines a set of rule suited for bottom up parsing and conceived a process that are applied by a pg processor the matching phase is a crucial step for process application and a parsing structure for efficient matching is also presented the pg processor is composed of a process scheduler that allows immediate constituent analysis of structure and behaves in a non deterministic fashion on the other side the pg offer mean for implementing specific parsing strategy improving the lack of determinism innate in the processor 
in this paper we compare the utility of different commitment strategy in planning under a least commitment strategy plan are represented a partial order and operator are ordered only when interaction are detected we investigate claim of the inherent advantage of planning with partial order a compared to planning with total order by focusing our analysis on the issue of operator ordering commitment we are able to carry out a rigorous comparative analysis of two planner we show that partial order planning can be more efficient than total order planning but we also show that this is not necessarily so 
the geometric hashing paradigm for model based recognition of object in cluttered scene is discussed this paradigm enables a unified approach to rigid object recognition under different viewing transformation assumption both for d and d object obtained by different sensor e g vision range tactile it is based on an intensive off line model preprocessing learning stage where model information is indexed into a hash table using minimal transformation invariant feature this enables the on line recognition algorithm to be particularly efficient the algorithm is straightforwardly parallelizable initial experimentation of the technique ha led to successful recognition of both d and d object in cluttered scene from an arbitrary viewpoint we also compare the geometric hashing with the hough transform and the alignment technique extension of the basic paradigm which reduce it worst case recognition complexity are discussed 
the success of case based reasoning depends on effective retrieval of relevant prior case if retrieval is expensive or if the case retrieved are inappropriate retrieval and adaptation cost will nullify many of the advantage of reasoning from prior experience we propose an indexing vocabulary to facilitate retrieval of explanation in a casebased explanation system the explanation we consider are explanation of anomaly conflict between new situation and prior expectation or belief our vocabulary group anomaly according to the type of information used to generate the expectation or belief that failed and according to how the expectation failed we argue that by using this vocabulary to characterize anomaly and retrieving explanation that were built to account for similarly characterized past anomaly a case based explanation system can restrict retrieval to explanation likely to be relevant in addition the vocabulary can be used to organize general explanation strategy that suggest path for explanation in novel situation 
the problem of finding the internal orientation of a camera camera calibration is extremely important for practical application in this paper a complete method for calibrating a camera is presented in contrast with existing method it doe not require a calibration object with a known d shape the new method requires only point match from image sequence it is shown using experiment with noisy data that it is possible to calibrate a camera just by pointing it at the environment selecting point of interest and then tracking them in the image a the camera move it is not necessary to know the camera motion the camera calibration is computed in two step in the first step the epipolar transformation is found two method for obtaining the epipoles are discussed one due to sturm is based on projective invariant the other is based on a generalisation of the essential matrix the second step of the computation us the so called kruppa equation which link the epipolar transformation to the image of the absolute conic after the camera ha made three or more movement the kruppa equation can be solved for the coefficient of the image of the absolute conic the solution is found using a continuation method which is briefly described the intrinsic parameter of the camera are obtained from the equation for the image of the absolute conic the result of experiment with synthetic noisy data are reported and possible enhancement to the method are suggested 
the goal of perception is to extract invariant property of the underlyingworld by computing contrast at edge the retina reduces incidentlight intensity spanning twelve decade to a twentyfold variation in onestroke it solves the dynamic range problem and extract relative reflectivity bringing u a step closer to the goal we have built a contrast sensitive silicon retina that model all major synaptic interaction in theouter plexiform layer of the vertebrate retina using 
the backpropagation algorithm can be used for both recognition and generationof time trajectory when used a a recognizer it ha been shownthat the performance of a network can be greatly improved by addingstructure to the architecture the same is true in trajectory generation in particular a new architecture corresponding to a quot reversed quot tdnn isproposed result show dramatic improvement of performance in the generationof hand written character acombination of tdnn and 
we developed a prototype information retrieval system which us advanced natural language processing technique to enhance the effectiveness of traditional key word based document retrieval the backbone of our system is a statistical retrieval engine which performs automated indexing of document then search and ranking in response to user query this core architecture is augmented with advanced natural language processing tool which are both robust and efficient in early experiment the augmented system ha displayed capability that appear to make it superior to the purely statistical base 
we extend reiter s general theory of model based diagnosis reiter to a theory of reconfiguration the generality of reiter s theory readily support an extension in which the problem of reconfiguration is viewed a a close analogue of the problem of diagnosis using a reconfiguration predicate rcfg analogous to the abnormality predicate ab we formulate a strategy for reconfiguration by transforming that for diagnosis a benefit of this approach is that algorithm for diagnosis can be exploited a algorithm for reconfiguration thereby promoting an integrated approach to fault detection identification and reconfiguration 
cogin is a system designed for induction of symbolic decision model from pre classed example based on the use of genetic algorithm gas much research in symbolic induction ha focused on technique for reducing classification inaccuracy that arise from inherent limit of underlying incremental search technique genetic algorithm offer an intriguing alternative to stepwise model construction relying instead on model evolution through global competition the difficulty is in providing an effective framework for the ga to be practically applied to complex induction problem cogin merges traditional induction concept with genetic search to provide such a framework and recent experimental result have demonstrated it advantage relative to basic stepwise inductive approach in this paper we describe the essential element of the cogin approach and present a favorable comparison of cogin result with those produced by a more sophisticated stepwise approach with support post processing on standardized multiplexor problem 
this paper present an approach to identifying conjuncts of coordinate conjunction appearing in text which ha been labelled with syntactic and semantic tag the overall project of which this research is a part is also briefly discussed the program wa tested on a word chapter of the merck veterinary manual the algorithm is deterministic and domain independent and it performs relatively well on a large real life domain construct not handled by the simple algorithm are also described in some detail 
we describe an application of the discovery system fahrenheit in a chemistry laboratory our emphasis is on automation of the discovery process a oposed to human intervention and on computer control over real experiment and data collection a opposed to the use of simulation fahrenheit performs automatically many cycle of experimentation data collection and theory formation we report on electrochemistry experiment of several hour duration in which fahrenheit ha developed empirical equation quantitative regularity equivalent to those developed by an analytical chemist working on the same problem the theoretical capability of fahrenheit have been expanded allowing the system to find maximum in a dataset evaluate error for all concept and determine reproducibility of result after minor adjustment fahrenheit ha been able to discover regularity in maximum location and height and to analyse repeatability of measurement by the same mechanism adapted from bacon by which all numerical regularity are detected 
we define fuzzy constraint network and prove a theorem about their relationship to fuzzy logic then we introduce khayyam a fuzzy constraint based programming language in which any sentence in the first order fuzzy predicate calculus is a well formed constraint statement finally using khayyam to address an equipment selection application we illustrate the expressive power of fuzzy constraint based language 
the structure imposed upon spoken sentence by intonation seems frequently to be orthogonal to their traditional surface syntactic structure however the notion of intonational structure a formulated by pierrehumbert selkirk and others can be subsumed under a rather different notion of syntactic surface structure that emerges from a theory of grammar based on a combinatory extension to categorial grammar interpretation of constituent at this level are in turn directly related to information structure or discourse related notion of theme rheme focus and presupposition some simplification appear to follow for the problem of integrating syntax and other high level module in spoken language system 
while every shafer belief function corresponds to a set of interval belief on the atom of the frame of di cernment an arbitrarily specified set of interval of belief may not correspond to any belief function even when it doe correspond to bound imposed by set of probability function this paper prof necessary and sufficient condition which must be met by a set of belief interval over atom if a corresponding belief function exists the sufficiency is proved via an an o n algorithm which will always construct an corresponding belief function if one exit for a specific set of interval 
this paper present a polynomial time algorithm pruned correspondencesearch pc for solving a wide class of geometricmaximal matching problem including the problem of recognizing d object from a single d image the pc algorithmis connected with the geometry of the underlying recognitionproblem only through call to a verification algorithm 
abstract we describe a decision theoretic method that an au tonomous agent can use to model multiagent situ ations and behave rationally based on it model our approach which we call the recursive modeling method explicitly account for the recursive nature of multiagent reasoning our method let an agent recursively model another agent s decision based on probabilistic view of how that agent perceives the multiagent situation which in turn are derived from hypothesizing how that other agent perceives the initial agent s possible decision and so on fur ther we show how the possibility of multiple inter action can affect the decision of agent allowing cooperative behavior to emerge a a rational choice of selfish agent that otherwise might behave uncooperatively 
this paper describes a situated reasoning architecture originally used with ground mobile robot which is shown to easily integrate control theoretic algorithm navigation heuristic and human supervision for semiautonomous robot control in underwater field environment the control architecture produce reaction plan that exploit low level competence a operator the low level competence include both obstacle avoidance heuristic and control theoretic algorithm for generating and following a velocity acceleration trajectory experiment with an undersea remotely piloted robot in a test tank at the deep submergence laboratory at wood hole ma are described the robot performed both pilot aided and autonomous exploration task robustly during normal change in the task environment the architecture wa implemented in the gapps rex situated automaton programming language the guaranteed constant cycle time of the synchronous rex circuit allowed for rapid tuning of the parameter of the control theoretic and heuristic algorithm to obtaln smooth safe motion 
building a large scale system often involves creating a large knowledge store and a these grow and are maintained by a number of individual error are inevitable exploring database a a specialization of knowledge store this paper study the hypothesis that descriptive learned model can be prescriptively used to find error to that end it describes an implemented system called carper applying carper to a real world database demonstrates the viability of the approach and establishes a baseline of performance for future research 
in this article we present a novel hybrid graph spatial representation for robot navigation this representation enables our mobile robot to build a model of it surroundings which it can then use for navigation the model or map that use this representation are hybrid graph the node being analogical local map of landmark location in the robot s environment the arc being the action the robot executes to travel between the location this representation yield a reliable navigation tool one which ensures that the robot can re orient itself to recover from error in path execution and encounter with unexpected obstacle the lognet approach also mesh with human s natural approach of mapping with landmark instead of using angular and translational data 
the cmu phoenix system is an experiment in understanding spontaneous speech it ha been implemented for the air travel information service task in this task casual user are asked to obtain information from a database of air travel information user are not given a vocabulary grammar or set of sentence to read they compose query themselves in a spontaneous manner this task present speech recognizers with many new problem compared to the resource management task not only is the speech not fluent but the vocabulary and grammar are open also the task is not just to produce a transcription but to produce an action retrieve data from the database taking such action requires parsing and understanding the utteraoce word error rate is not a important a utterance understanding rate phoenix attempt to deal with phenomenon that occur in spontaneous speech unknown word restarts repeat and poody formed or unusual grammar are common is spontaneous speech and are very disruptive to standard recognizers these event lead to misrecognitions which often cause a total parse failure our strategy is to apply grammatical constraint at the phrase level and to use semantic rather than lexical grammar semantics provide more constraint than part of speech and must ultimately be delt with in order to take action applying constraint at the phrase level is more flexible than recognizing sentence a a whole while providing much more constraint than word spotting restarts and repeat are most often between phase occurences so individual phrase can still be recognized correctly poorly constructed grammar often consists of well formed phrase and is often semantically well formed it is only syntactically incorrect we associate phrase by frame based semantics phrase represent word string that can fill slot in frame the slot represent information which the frame is able to act on the current phoenix system us a bigram language model with the sphinx speech recognition system the top scoring word string is passed to a flexible frame based parser the parser assigns phrase word string from the input to slot in frame the slot represent information content needed for the frame a beam of frame hypothesis is produced and the best scoring one is used to produce an sql query 
saw the lifting of the security restriction on large number of scientific and technical report which had been written during world war two pre war virtually all publication had been in journal and the report format wa strange and unfamiliar both for the scientific community and for librarian a such they presented new challenge the administrative problem of actually being able to obtain copy of the report wa tackled by setting up new government agency with direct 
this paper present an algorithm for first order horn clause abduction that us an atm to avoid redundant computation this algorithm is either more efficient or more general than any other previous abduction algorithm since computing all minimal abductive explanation is intractable we also present a heuristic version of the algorithm that us beam search to compute a subset of the simplest explanation we present empirical result on a broad range of abduction problem from text understanding plan recognition and device diagnosis which demonstrate that our algorithm is at least an order of magnitude faster than an alternative abduction algorithm that doe not use an atm 
the blackboard instructional planner is a blackboard based dynamic planner for intelligent tutoring system it generates a sequence of lesson plan customized to a student s background and adaptively replans to handle student request and unexpected change to the student model or time remaining the planner is designed to be generic to tutor that teach troubleshooting for complex physical device it control the lower hoist tutor a prototype tutor for the mark naval gun mount this tutor teach troubleshooting of the lower hoist a complex hydraulic electronic mechanical assembly of the mark the tutor implementation demonstrates the planner s operation and mean of integration this research contributes to an understanding of dynamic instructional planner planner controlled tutor and it control architecture the planner implementation show precisely how a blackboard architecture can be used to realize a dynamic instructional planner although experimental the tutor implementation demonstrates how such a planner can be embedded in an intelligent tutoring system and what the respective role of the different component of a planner controlled tutor are finally the analysis of the planner s use of the blackboard architecture clarifies requirement for control architecture in intelligent tutoring system and trade offs made in choosing alternative 
this paper explores the problem of learning from example when feature measurement cost are significant it then extends two effective and familiar learning method id and ibl to address this problem the extension c id and c ibl are described in detail and are tested in a natural robot domain and a synthetic domain empirical study support the hypothesis that the extended method are indeed sensitive to feature cost they deal effectively with varying cost distribution and with irrelevant feature 
in this study we map out a way to build event representation incrementally using information which may be widely distributed across a discourse an enhanced discourse representation kamp provides the vehicle both for carrying open event role through the discourse until they can be instantiated by np and for resolving the reference of these otherwise problematic np by binding them to the event role 
this paper describes the application of explanation based learning a machine learning technique to the sri core language engine a large scale general purpose natural language analysis system the idea is to bypass normal morphological syntactic and 
a theory of perspectivity is proposed to establish the foundation of the theory of situated agent an account is then given based on the theory of perspectivity of the use of a seemingly perspectivity related expression japanese long range reflexive zibun the theory we proposed for perspectival mental state incorporates two independent notion indexicality and world view the first capture the situatedness of agent within physical environment and the second capture the mode of reasoning adopted by agent in interacting with other agent the relationship between these two notion were also discussed based on the proposed theory of perspectivity we argued that contrary to wide spread belief the usage of zibzsn is not directly related to perspectivity we gave an alternative explication for the interaction of the usage of zibun with perspectivity sensitive expression and the indexical pronoun watashi i in term of the coreference rule for zibun the constraint on the two component of perspectivity and the agent awareness default principle for the world view 
this paper investigates a problem of natural language processing from the perspective of reasoning work on constraint satisfaction we formulate the task of computing singular definite reference to a known contextual entity a a constraint satisfaction problem we argue that such referential constraint problem have a structure which is often simpler than the general case and can therefore often be solved by the sole use of low power network consistency technique to illustrate we define a linguistic fragment which provably generates tree structured constraint problem this enables u to conclude that the limited operation of strong arc consistency is sufficient to resolve the class of noun phrase defined by the fragment 
this paper present a formal relationship for probability ken satoh generation computer technology minato ku tokyo japan ksatoh icot jp for probability theory and a class of nonmonotomc reasoning which we call daxy nonmonotonic reusoning in lazy nonmonotonic reasoning nonmonotonicity emerges only when new added knowledge is contradictory to the previous belief in this paper we consider nonmonotonic reasoning in term of consequence relation a consequence relation is a binary relation over formula which express that a formula is derivable from another formula under inference rule of a considered system a consequence relation which ha lazy nonmonotonicity is called a rutionad consequence relation studied by lehmann and magidor we provide a probabilistic semantics which characterizes a rational consequence relation exactly then we show a relationship between propositional circumscription and consequence relation and apply this semantics to a consequence relation defined by propositional circumscription which ha lazy nonmonotonicity 
researcher have long argued that an attentional mechanism is requiredto perform many vision task this paper introduces an attentional prototypefor early visual processing our model is composed of a processinghierarchy and an attention beam that traverse the hierarchy passingthrough the region of greatest interest and inhibiting the region that arenot relevant the type of input to the prototype is not limited to visualstimuli simulation using high resolution digitized image 
recently conditional logic have been developed for application to problem in default reasoning we present a uniform framework for the development and investigation of conditional logic to represent and reason with normality and demonstrate these logic to be equivalent to extension of the modal system s we also show that two conditional logic recently proposed to reason with default knowledge are equivalent to fragment of two logic developed in this framework 
this paper is concerned with making precise the notion that recognizing plan is much like parsing text to this end it establishes a correspondence between kautz plan recognition formalism and existing grammatical framework this mapping help isolate subset of kautz formalism in which plan recognition can be efficiently performed by parsing 
this paper investigates the effect of parallelism on blackboard system scheduling a parallel blackboard system is described that allows multiple knowledge source instantiation to execute in parallel using a shared memory blackboard approach new class 
an algorithm is described which computes stable model of propositional logic program with negation a failure using the assumption based truth maintenance mechanism since stable model of logic program are closely connected to stable expansion of a class of autoepistemic theory this algorithm point to a link between stable expansion of a class of auto epistemic theory and atm structure 
one kind of temporal reasoning is temporal projection the computation of the consequence of a set of event this problem is related to a number of other temporal reasoning task such a story understanding planning and plan validation we show that one particular simple case of temporal projection on partially ordered event turn out to be harder than previously conjectured however given the restriction of this problem story understanding planning and plan validation appear to be easy in fact we show that plan validation one of the intended application of temporal projection is tractable for an even larger class of plan 
constraint satisfaction problem csps provide a model often used in artificial intelligence since the problem of the existence of a solution in a csp is an np complete task many filtering technique have been developed for csps the most used filtering technique are those achieving arc consistency nevertheless many reasoning problem in ai need to be expressed in a dynamic environment and almost all the technique already developed to solve csps deal only with static csps so in this paper we first define what we call a dynamic csp and then give an algorithm achieving arc consistency in a dynamic csp the performance of the algorithm proposed here and of the best algorithm achieving arc consistency in static csps are compared on randomly generated dynamic csps the result show there is an advantage to use our specific algorithm for dynamic csps in almost all the case tested 
cognitive ability of fifty university student were tested using eight test from the kit of factor referenced cognitive test all student searched for reference on the same topic using a standard computerized index and performance in the search wa analyzed using a variety of measure effect for cognitive difference a well a for difference in demographic characteristic and knowledge were identified using multiple regression perceptual speed had an effect on the quality of search and logical reasoning verbal comprehension and spatial scanning ability influenced search tactic it is suggested that information retrieval system can be made more accessible to user with different level of cognitive ability through improvement that will assist user to scan list of term choose appropriate vocabulary for searching and select useful reference 
we propose a software framework for integrating people and computer system in large geographically dispersed manufacturing enterprise underlying the framework is an enterprise model that is built by dividing complex business process into elementary task or activity each such task is then modeled in cognitive term e g what to look for what to do who to tell and entrusted to an intelligent agent ia for execution the ia interact with each other directly via a message bus or through a shared distributed knowledge base they can also interact with human through personal assistant pa a special type of ia that know how to communicate with people through multi medium interface preliminary experimental result suggest that this model based man machine approach provides a viable path for applying dai to realworld enterprise 
in recent year there is much interest in word cooccurrence relation such a n gram verb object combination or cooccurrence within a limited context this paper discus how to estimate the probability of cooccurrences that do not occur in the training data we present a method that make local analogy between each specific unobserved cooccurrence and other cooccurrences that contain similar word a determined by an appropriate word similarity metric our evaluation suggests that this method performs better than existing smoothing method and may provide an alternative to class based model 
reinforcement learning rl algorithm have traditionally been thought of a trial and error learning method that use actual control experience to incrementally improve a control policy sutton s dyna architecture demonstrated that rl algorithm can work a well using simulated experience from an environment model and that the resulting computation wa similar to doing one step lookahead planning inspired by the literature on hierarchical planning i propose learning a hierarchy of model of the environment that abstract temporal detail a a mean of improving the scalability of rl algorithm i present h dyna hierarchical dyna an extension to sutton s dyna architecture that is able to learn such a hierarchy of abstract model h dyna differs from hierarchical planner in two way first the abstract model are learned using experience gained while learning to solve other task in the same environment and second the abstract model can be used to solve stochastic control task simulation on a set of compositionally structured navigation task show that h dyna can learn to solve them faster than conventional rl algorithm the abstract model also serve a mechanism for achieving transfer of learning across multiple task 
the algorithm presented performs gradient descent on the weight spaceof an artificial neural network ann using a finite difference toapproximate the gradient the method is novel in that it achieves a computationalcomplexity similar to that of node perturbation o n butdoes not require access to the activity of hidden or internal neuron this is possible due to a stochastic relation between perturbation at theweights and the neuron of an ann the algorithm is also similar to 
agent collaborating to achieve a goal bring to their joint activity different belief about way in which to achieve the goal and the action necessary for doing so thus a model of collaboration must provide a way of representing and distinguishing among agent belief and of stating the way in which the intention of different agent contribute to achieving their goal furthermore in collaborative activity collaboration occurs in the planning process itself thus rather than modelling plan recognition per se what must be modelled is the augmentation of belief about the action of multiple agent and their intention in this paper we modify and expand the sharedplan model of collaborative behavior grosz sidner we present an algorithm for updating an agent s belief about a partial sharedplan and describe an initial implementation of this algorithm in the domain of network management 
a parallel distributed computational model for reasoning and learning is discussed based on a belief network paradigm issue like reasoning and learning for the proposed model are discussed comparison between our method and other method are also given 
a basic feature of intelligent tutoring system it is their ability to represent domain knowledge that can be attributed to the student at each stage of the learning process in this paper we present a general first order logic framework for the representation of this kind of knowledge acquired by the system through the analysis of the student answer this represantation make it possible to describe the behaviour of well known it and to provide a direct implementation in a logic programming language moreover we point out several improvement that can be easily achieved by exploiting the feature of a declarative approach in particular we address the representation and use of the knowledge that the system know not to be possessed by the student 
i describe a head driven parser for a class of grammar that handle discontinuous constituency by a richer notion of string combination than ordinary concatenation the parser is a generalization of the left corner parser matsumoto et al and can be used for grammar written in powerful formalism such a non concatenative version of hpsg pollard reape 
an important problem in visual motion analysis is to determine the parameter of egomotion we present a simple fast method that computes the translational motion of a sensor that is generating a sequence of image this procedure computes a scalar function from the optical flow field induced on the image plane due to the motion of the sensor and us the norm of this function a an error measure appropriate value of the parameter used in the computation of the scalar function yield zero error this observation is used to locate the focus of expansion which is directly related to the translational motion 
an inventorwho isskilledat constructinginnovative designsisdistinguished notjustby thefirstprinciples he know butby theway he usestheseprinciples and how he focussesthesearchfornoveldevicesamong an overwhelmingspaceofpossibilities we proposethatan appropriatefocusfordesignisthenetworkofqualitative interaction betweenquantities calledan interaction topology usedby adevicetoachieveitsdesiredbehavior we presentan approach calledinteraction based invention which viewsdesignas a processof buildinginteraction topology inthispaperdirectly from firstprinciples the programibis whichembodiesthis approach designssimplehydro mechanicalregulators analogoustodevicesthatwerefundamentaltothedevelopmentoffeedbackcontroltheory abstract in highlycompetitivemarkets with rapid technology shiftsa designermust continuallyexploitnew technologiesand existingtechnologiesinnonobviousways currentai designresearchfocuseson heirarchicalrefinement using librariesof design fragment mcdermott roylance mitchelletal ressler mitchelletal mittaletal while librarybased techniqueslikeconfiguration mcdermott have been highlysuccessfulforsome routinetasks they ignoretheseinnovativeaspectsof the designprocess to achievethegeneralitynecessarytomaintainacompetitiveedge the designermight have to consider not justwhat isintheroutinelibrary but any possibledevicestructure evaluatingthebehaviorofany structure requiresreasoningfrom firstprinciples the generality affordedby theseprinciplespresentsthe designerwith an overwhelming space ofpossibilities to avoidbeing losttheinventormust useeverymeans athisdisposalto focusthesearch this isthe abilitytoinnovate thus a robusttheory must capture not only techniquesfor routinedesign but theprocessofinnovationfrom first principle we referto thisas invention discussion withmy advisorrandy davisandcommittee patrickwinstonandtomes lozano perezhad atremendous impacton thisresearch johandekleer brianfalkenhainer leojoskowiczand mark shirleyprovidedvaluablecomments on earlier draft thisresearchwas performedbothatthe mit ai lab and xerox parc mit supportwas provided byan analogdevicesfellowship dec wang and darpa underofficeofnavalresearchcontractn k 
the problem of automatically improving functional program using darlington s unfold fold technique is addressed transformation tactic are formalized a method consisting of preand postconditions expressed within a sorted meta logic predicate and function of this logic induce an abstract program property space within which conventional monotonic planning technique are used to automatically compose method hence tactic into a program improving strategy this meta program reasoning cast the undirected search of the transformation space a a goal directed search of the more abstract space tactic are only weakly specified by method this flexibility is required if they are to be applicable to the class of generalized program that satisfy the pre condition of their method this is achieved by allowing the tactic to generate degenerate script that may require refinement example of tactic and method are given with illustration of their use in automatic program improvement 
this paper describes a model of the complementarity of rule and precedent in the classification task under this model precedent assist rule based reasoning by operationalizing abstract rule antecedent conversely rule assist case based reasoning through case elaboration the process of inferring case fact in order to increase the similarity between case and term reformulation the process of replacing a term whose precedent only weakly match a case with term whose precedent strongly match the case fully exploiting this complementarity requires a control strategy characterized by impartiality the absence of arbitrary ordering restriction on the use of rule and precedent an impartial control strategy wa implemented in grebe in the domain of texas worker s compensation law in a preliminary evaluation grebe s performance wa found to be a good or slightly better than the performance of law student on the same task 
x morf is a language independent morphological component for the recognition and generation of word form based on a lexicon of morphs the approach is based on two level morphology extension are motivated by linguistic data which call into question an underlying assumption of standard two level morphology namely the independence of morphophonology and morphology a exemplified by two level rule and continuation class accordingly i propose a model which allows for interaction between these two part instead of using continuation class word formation is described in a feature based unification grammar two level rule are provided with a morphological context in the form of feature structure information contained in the lexicon and the word formation grammar guide the application of two level rule by matching the morphological context against the morphs i present an efficient implementation of that model where rule are compiled into automaton a in the standard model and where processing of the feature based grammar is enhanced using an automaton derived from that grammar a a filter 
this paper present a methodology based on the estimation of the optical flow to detect static obstacle during the motion of a mobile robot the algorithm is based on a correlation scheme at any time we estimate the position of the focus of expansion and stabilize it by using the kalman filter we use the knowledge of the focus position of the flow field computed in the previous time to reduce the search space of corresponding patch and to predict the flow field in the successive one because of it intrinsic recursive aspect the method can be seen a an on off reflex which detects obstacle lying on the ground during the path of a mobile platform no calibration procedure is required the key aspect of the method is that we compute the optical flow only on one row of the image that is relative to the ground plane 
the focused gamma network is proposed a one of the possible implementation of the gamma neural model the focused gamma network is compared with the focused backpropagation network and tdnn for a time series prediction problem and with adaline in a system identification problem 
latent semantic indexing lsi is a technique for representing document query and term a vector in a multidimensional real valued space the representtions are approximation to the original term space encoding and are found using the matrix technique of singular value decomposition in comparison multidimensional scaling md is a class of data analysis technique for representing data point a point in a multidimensional real valued space the object are represented so that inter point similarity in the space match inter object similarity information provided by the researcher we illustrate how the document representation given by lsi are equivalent to the optimal representation found when solving a particular md problem in which the given inter object similarity information is provided by the inner product similarity between the document themselves we further analyze a more general md problem in which the interdocument similarity information although still in inner product form is arbitrary with respect to the vector space encoding of the document 
most of today s terminological representation system implement hybrid reasoning architecture wherein a concept classifier is employed to reason about concept definition and a separate recognizer is invoked to compute instantiation relation between concept and instance whereas most of the existing recognizer algorithm designed to maximally exploit the reasoning supplied by the concept classifier loom ha experimented with recognition strategy that place le emphasis on the classifier and rely more on the ability of loom s backward chaining query facility this paper present the result of experiment that test the performance of the loom algorithm these result suggest that at least for some application the loom approach to recognition is likely to outperform the classical approach they also indicate that for some application much better performance can be achieved by eliminating the recognizer entirely in favor of a purely backward chaining architecture we conclude that no single recognition algorithm or strategy is best for all application and that an architecture that offer a choice of inference mode is likely to be more useful than one that offer only a single style of reasoning 
how should opinion of control knowledge source be represented and combined these issue are addressed for the case where control knowledge is used to form an agenda i e a proposed knowledge source execution order a formal model is developed in the dempster shafer belief calculus and computational problem are discussed a well the model is applicable to many other problem where it is desired to order a set of candidate using a knowledge based approach 
this paper investigates the effect of parallelism on blackboard system scheduling a parallel blackboard system is described that allows multiple knowledge source instantiation to execute in parallel using a shared memory blackboard approach new class 
we propose that a planner should be provided with an explicit model of it own planning mechanism and show that linking a planner s expectation about the performance of it plan to such a model by mean of explicit justification structure enables the planner to determine which aspect of it planning are responsible for observed performance failure we have implemented the idea presented in this paper in a computer model applied to the game of chess the model is capable of diagnosing planning failure due to incomplete knowledge of the rule improper or overly optimistic focus of attention faulty projection and insufficient lead time for warning about threat and is therefore able to learn such concept a discovered attack and the fork 
this paper describes dynamic trade off evaluation dte a new technique that ha been developed to improve the performance of real time problem solving system the dte technique is most suitable for automation environment in which the requirement for meeting time constraint is of equal importance to that of providing optimally intelligent solution in such environment the demand of high input data volume and short response time can rapidly overwhelm traditional ai system dte is based on the recognition that in time constrained environment compromise to optimal problem solving in favor of timeliness must often be made in the form of trade offs towards this end dte combine knowledge based technique with decision theory to dynamically modify system behavior and adapt the decision criterion that determine how such modification are made the performance of dte ha been evaluated in the context of several type of real time trade offs in spacecraft monitoring problem one such application ha demonstrated that dte can be used to dynamically vary the data that is monitored making it possible to detect and correctly analyze all anomalous data by examining only a subset of the total input data in carefully structured experimental evaluation that use real spacecraft data and real decision making dte provides the ability to handle a three fold increase in input data in realtime without loss of performance 
a formalism is presented for lexical specification in unification based grammar which exploit defeasible multiple inheritance to express regularity sub regularity and exception in classifying the property of word such system are in the general case intractable the present proposal represents an attempt to reduce complexity while retaining sufficient expressive power for the task at hand illustrative example are given of morphological analysis from english and german 
it is well known that for many np complete problem such a k sat etc typical case are easy to solve so that computationally hard case must be rare assuming p np this paper show that np complete problem can be summarized by at least one order parameter and that the hard problem occur at a critical value of such a parameter this critical value separate two region of characteristically different property for example for k colorability the critical value separate overconstrained from underconstrained random graph and it mark the value at which the probability of a solution change abruptly from near to near it is the high density of well separated almost solution local minimum at this boundary that cause search algorithm to thrash this boundary is a type of phase transition and we show that it is preserved under mapping between problem we show that for some p problem either there is no phase transition or it occurs for bounded n and so bound the cost these result suggest a way of deciding if a problem is in p or np and why they are different 
this paper describes the integration of a learning mechanism called complementary discrimination learning with a knowledge representation schema called decision list there are two main result of such an integration one is an efficient representation for complementary concept that is crucial for complementary discrimination style learning the other is the first behaviorally incremental algorithm called cdl for learning decision list theoretical analysis and experiment in several domain have shown that cdl is more efficient than many existing symbolic or neural network learning algorithm and can learn multiple concept from noisy and inconsistent data 
in this paper we define six parameter addressed to parametrize the texture characteristic of an image towards it segmentation with the aim to operate at high speed these parameter have been defined looking for an acceptable compromise between discrimination capacity and easyness to implement a specific architecture for them 
i present a modular network architecture and a learning algorithm basedon incremental dynamic programming that allows a single learning agentto learn to solve multiple markovian decision task mdts with significanttransfer of learning across the task i consider a class of mdts called composite task formed by temporally concatenating a number ofsimpler elemental mdts the architecture is trained on a set of compositeand elemental mdts the temporal structure of a composite task is 
this paper present a novel approach to incrementally estimating visual motion over a sequence of image we start by formulating constraint on image motion to account for the possibility of multiple motion this is achieved by exploiting the notion of weak continuity and robust statistic in the formulation of a minimization problem the resulting objective function is non convex traditional stochastic relaxation technique for minimizing such function prove inappropriate for the task 
we describe a statistical technique for assigning sens to word an instance of a word is assigned a sense by asking a question about the context in which the word appears the question is constructed to have high mutual information with the translation of that instance in another language when we incorporated this method of assigning sens into our statistical machine translation system the error rate of the system decreased by thirteen percent 
we describe a neural network called rulenet that learns explicit symbolic condition action rule in a formal string manipulation domain rulenet discovers functional category over element of the domain and at various point during learning extract rule that operate on these category the rule are then injected back into rulenet and training continues in a process called iterative projection by incorporating rule in this way rulenet exhibit enhanced learning and generalization performance over alternative neural net approach by integrating symbolic rule learning and subsymbolic category learning rulenet ha capability that go beyond a purely symbolic system we show how this architecture can be applied to the problem of case role assignment in natural language processing yielding a novel rule based solution 
we present an approach to contour grouping based on classical tracking technique edge point are segmented into smooth curve so a to minimize a recursively updated bayesian probability measure the resulting algorithm employ local smoothness constraint and a statistical description of edge detection and can accurately handle corner bifurcation and curve intersection experimental result demonstrate good performance 
this paper describes a parallel associative processor ixm developed mainly for semantic network processing ixm consists of associative processor and network processor having a total of k word of associative memory the large associative memory enables semantic network node to be processed in parallel and reduces the order of algorithmic complexity to o in basic semantic net operation we claim that intensive use of associative memory provides far superior performance in carrying out the basic operation necessary for semantic network processing intersection marker propagation and arithmetic operation 
this paper report experimental result of a high performance real time memory based translation memory based translation is a new approach to machine translation which us example or case of past translation to carry out translation of sentence this idea is counter to traditional machine translation system which rely on extensive use of rule in parsing transfer and generation although there are some preliminary report on the superiority of the memory based translation in term of it scalability quality of translation and easiness of grammar writing we have not seen any report on it performance this is perhaps the first report discussing the feasibility and problem of the approach based on actual massively parallel implementation using real data we also claim that the architecture of the ixm associative processor is highly suitable for memory based translation task parsing performance of the memory based translation system attained a few millisecond per sentence 
existing formalism for default reasoning capture some aspect of the nonmonotonicity of human commonsense reasoning however perlis ha shown that one of these formalism circumscription is subject to certain counterintuitive limitation kraus and perlis suggested a partial solution but significant problem remain in this paper we observe that the unfortunate limitation of circumscription are even broader than perlis originally pointed out moreover these problem are not confined to circumscription they appear to be endemic in current nonmonotonic reasoning formalism we develop a much more general solution than that of kraus and perlis involving restricting the scope of nonmonotonic reasoning and show that it remedy these problem in a variety of formalism 
a central goal of qualitative physic is to provide a framework for organizing and using quantitative knowledge one important use of quantitative knowledge is numerical simulation while current numerical simulator are powerful they are often hard to construct do not reveal the assumption underlying their construction and do not produce explanation of the behavior they predict this paper show how to combine qualitative and quantitative model to produce a new class of self explanatory simulation which combine the advantage of both kind of reasoning self explanatory simulation provide the accuracy of numerical model and the interpretive power of qualitative reasoning we define what self explanatory simulation are and show how to construct them automatically we illustrate their power with some example generated with an implemented system simgen we analyze the limitation of our technique and discus plan for future work 
abstract ebmt example based machine translation is proposed ebmt retrieves similar example pair of source phrase sentence or text and their translation from a d t hase of example adapting the example to translate a new input ebmt ha the following feature it is easily upgraded simply by inputting appropriate example to the database it assigns a reliability factcr to the translation result it is acoelerated effectively by both indexing and parallel computing it is robust because of best match reasoning d it well utilizes translator expertise a prototype system ha been implemented to deal with a difficult translation problem for conventional rule based machine translation rbmt i e translating japanese noun phrase of the form n no n into english the system ha achieved about a success rate on average this paper explains the basic idea of ebmt illustrates the experiment in detail explains the broad applicability of ebmt to several difficult translation problem for rbmt and discus the advantage of integrating ebmt with rbmt 
multichannel filtering technique are presented for obtaining both regionand edge based segmentation of textured image the channel are represented by a bank of even symmetric gabor filter that nearly uniformly cover the spatial frequency domain feature image are obtained by subjecting each selected filtered image to a nonlinear transformation and computing a measure of energy around each pixel region based segmentation are obtained by using a square error clustering algorithm edge based segmentation are obtained by applying an edge detector to each feature image and combining their magnitude response an integrated segmentation technique that combine the strength of the previous two technique while eliminating their weakness is proposed the integrated approach is truly unsupervised since it eliminates the need for knowing the exact number of texture category in the image 
chromatic aberration is due to refraction affecting eachcolor channel differently this paper address the useof image warping to reduce the impact of these aberrationsin vision application the warp is determined usingedge displacement which are fit with cubic spline a new image reconstruction algorithm is used for nonlinearresampling the main contribution of this work isto analyze the quality of the warping approach by comparingit with active lens control two different imaging 
a stochastic model based on insight of mandelbrot and simon is discussed against the background of new criterion of adequacy that have become available recently a a result of study of the similarity relation between word a found in large computerized text corpus 
speaker independent system is desirable in many application where speaker specific data do not exist however if speakerdependent data are available the system could be adapted to the specific speaker such that the error rate could be significantly reduced in this paper darpa resource management task is used a the domain to investigate the performance of speaker adaptive speech recognition since adaptation is based on speaker independent system with only limited adaptation data a good adaptation algorithm should be consistent with the speaker independent parameter estimation criterion and adapt those parameter that are le sensitive to the limited training data two parameter set the codebook mean vector and the output distribution are regarded to be most important they are modified in the framework of maximum likelihood estimation criterion according to the characteristic of each speaker in order to reliably estimate those parameter output distribution are shared with each other if they exhibit certain acoustic similarity in addition to modify these parameter speaker normalization with neural network is also studied in the hope that acoustic data normalization will not only rapidly adapt the system but also enhance the robustness of speakerindependent speech recognition preliminary result indicate that speaker difference can be well minimized in comparison with speaker independent speech recognition the error rate ha been reduced from to by only using parameter adaptation technique with adaptation sentence for each speaker when the number of speaker adaptation sentence is comparable to that of speaker dependent training speaker adaptive recognition work better than the best speaker dependent recognition result on the same test set which indicates the robustness of speaker adaptive speech recognition 
abstract we identify the three principle factor affecting the performance oflearning by network with localized unit unit noise sample density and the structure of the target function we then analyze the effect of unit receptive field parameter on these factor and use this analysis to propose a new learning algorithm which dynamically alters receptive field property during learning how receptive field parameter affect neural learning bartlett w mel stephen m omohundro 
animal locomotion pattern are controlled by recurrent neural networkscalled central pattern generator cpgs although a cpg can oscillateautonomously it rhythm and phase must be well coordinated with thestate of the physical system using sensory input in this paper we proposea learning algorithm for synchronizing neural and physical oscillator withspecific phase relationship sensory input connection are modified by thecorrelation between cellular activity and input signal 
we have previously argued that the syntactic structure of natural language can be exploited to construct powerful polynomial time inference procedure this paper support the earlier argument by demonstrating that a natural language based polynomial time procedure can solve schubert s steamroller in a single step 
artificial creature are autonomous mobile agent that have to react in real time to sensor and plan and perform action in the real world current effective architecture for artificial creature are behavior based and use variant of the subsumption architecture the paper proposes an extension to these architecture in term of a layer which introduces cognitive capability to artificial creature this extension is frame based and us emergent frame recognition to determine which frame should become active 
tree have played a key role in the study of constraint satisfaction problem because problem with tree structure can be solved efficiently it is shown here that a family of generalized tree k tree can offer increasing representational complexity for constraint satisfaction problem while maintaining a bound on computational complexity linear in the number of variable and exponential in k additional result are obtained for larger class of graph known a partial k tree these method may be helpful even when the original problem doe not have k tree or partial k tree structure specific tradeoff are suggested between representational power and computational complexity 
this paper present an implemented psychologically plausible parsing model for government binding theory grammar i make use of two main idea a generalization of the licensing relation of abney allows for the direct encoding of certain principle of grammar e g theta criterion case filter which drive structure building the working space of the parser is constrained to the domain determined by a tree adjoining grammar elementary tree all dependency and constraint are localized within this bounded structure the resultant parser operates in linear time and allows for incremental semantic interpretation and determination of grammaticality 
the power of knowledge acquisition system that employ failure driven learning derives from two main source an effective global credit assignment process that determines when to acquire new knowledge by watching an expert s behavior and an efficient local credit assignment process that determines what new knowledge will be created for completing a failed explanation of an expert s action because an input e g observed action to a failure driven learning system can generate multiple explanation a learning opportunity to extend the incomplete domain theory can go unobserved this paper describes a failure driven learning with a context analysis mechanism a a method to constrain explanation and thereby increase the number of learning opportunity experimentation using a synthetic expert system a the observed expert show that the use of context analysis increase the number of learning opportunity by about and increase the overall amount of improvement to the expert system by around 
admissible heuristic are an important class of heuristic worth discovering they guarantee shortest path solution in search algorithm such a a and they guarantee le expensively produced but boundedly longer solution in search algorithm such a dynamic weighting unfortunately effective accurate and cheap to compute admissible heuristic can take year for people to discover several researcher have suggested that certain transformation of a problem can be used to generate admissible heuristic this article defines a more general class of transformation called abstraction that are guaranteed to generate only admissible heuristic it also describes and evaluates an implemented program absolver io that us a mean end analysis search control strategy to discover abstracted problem that result in effective admissible heuristic absolver i discovered several well known and a few novel admissible heuristic including the first known effective one for rubik s cube thus concretely demonstrating that effective admissible heuristic can be tractably discovered by a machine 
our previous research on one probe access to large collection of data indexed by alphanumeric key ha produced the first practical minimal perfect hash function for this problem here a new algorithm is described for quickly finding minimal perfect hash function whose specification space is very close to the theoretical lower bound i e around bit per key the various stage of processing are detailed along with analytical and empirical result including timing for a set of over million key that wa processed on a nextstation in about hour 
structure mapping of this type are a general mean of information processing in the vertebrate visual system in this paper we present an application of a special topographic mapping termed the inverse perspective mapping for the processing of stereo and motion more specifically we study a class of algorithm for the detection of deviation from an expected normal situation these expectation concern the global space variance of certain image parameter e g disparity or speed of feature motion and can thus easily be implemented in the mapping rule the resulting algorithm are minimal in the sense that no irrelevant information is extracted from the scene in a technical application we use topographic mapping for a stereo obstacle detection system the implementation ha been tested on an automatically guided vehicle agv in an industrial environment image 
the issue of optimal motion and structure estimation from monocular image sequence with a rigidity scene is addressed the method ha the following characteristic the dimension of the search space in the nonlinear optimization is drastically reduced by exploiting the relationship between structure and motion parameter the degree of reliability of the observation and estimate is effectively taken into account the proposed formulation allows arbitrary interframe motion and the information about the structure of the scene acquired from previous image is systematically integrated into the new estimation it is shown that any scale factor associated with two consecutive image in a monocular sequence is determined by the scale factor of the first two image the simulation and experiment with long image sequence of real world scene indicate that the optimization method developed greatly reduces the computational complexity and substantially improves the motion and structure estimation over that produced by linear algorithm 
this research attempt to span the gap between the ai in medicine aim and consistency based diagnosis cbd community by applying cbd to physiology the highly regulated nature of physiological system challenge standard cbd algorithm which are not tailored for complex dynamic system to combat this problem we separate static from dynamic analysis so that cbd is performed over the steady state constraint at only a selected set of time slice regulatory model help link static inter slice diagnosis into a complete dynamic account of the physiological progression this provides a simpler approach to cbd in dynamic system that a preserve information reuse capability b extends information theoretic probing and c add a new capability to cbd the detection of dynamic fault i e those that do not necessarily persist throughout diagnosis 
this paper present an approach to retranslation the third and final step of the theory reduction approach to solving theory revision problem retranslation involves putting a modified operationalized or reduced version of the desired revised theory back into the entire language of the original theory this step is desirable for a number of reason not least of which is the need to compress what are generally very large reduced theory into much smaller and thus more efficiently evaluated unreduced theory empirical result for the retranslation method are presented 
a new approach to determine motion from multiple image of a sequence is presented motion is regarded a orientation in a three dimensional space with one time and two space coordinate the algorithm is analogous to an eigenvalue analysis of the inertia tensor besides the determination of the displacement vector field it allows the classification of four region with regard to motion a constant region where no velocity determination is possible b edge where the velocity component perpendicular to the edge is determined c corner where both component of the velocity vector are calculated d motion discontinuity which are used to mark the boundary between object moving with different velocity 
this paper provides an account of the representation of default in cyc and their semantics in term of first order logic with reification default reasoning is a complex thing and we have found it beneficial to separate various complex issue whose current best solution is likely to change now and then such a deciding between extension preferring one default to another etc and deal with them explicitly in the knowledge base thus allowing u to adopt a simple and hopefully fixed logical mechanism to handle the basic nonmonotonicity itself we also briefly describe how this default reasoning scheme is implemented in cyc 
this paper present a new method for evaluating the spatial attitude position orientation of a d object by matching a d static model of this object with sensorial data describing the scene d projection or d sparse coordinate this method is based on the pre computation of a force field derived from d distance map designed to attract any d point toward the surface of the model the attitude of the object is infered by minimizing the energy necessary to bring all of the d point or projection line in contact with the surface geometric configuration of the scene to quickly and accurately compute the d distance map a precomputed distance map is represented using an octree spline whose resolution increase near the surface 
analogical planning provides a mean of solving problem where other machine learning method fail because it doe not require numerous previous example or a rich domain theory given a problem in an unfamiliar domain the target case an analogical planning system locates a successful plan in a similar domain the bast case and us the similarity to generate the target plan unfortunately the analogical planning process is expensive and inflexible many of the limiting factor reside in the base selection step which drive the analogy formation process this paper describes two way of increasing the effectiveness and efficiency of analogical planning first a parallel graph match base selection algorithm is presented a parallel implementation on the connection machine is described and shown to substantially decrease the complexity of base selection second a base case merge algorithm is shown to increase the flexibility of analogical planning by combining the benefit of several base case when no single plan contributes enough information to the analogy the effectiveness of this approach is demonstrated with example from the domain of automatic programming 
this paper present a comprehensive approach to automatic theory refinement in contrast to other system the approach is capable of modifying a theory which contains multiple fault and fault which occur at intermediate point in the theory the approach us explanation to focus the correction to the theory with the correction being supplied by an inductive component in this way an attempt is made to preserve the structure of the original theory a much a possible because the approach begin with an approximate theory learning an accurate theory take fewer example than a purely inductive system the approach ha application in expert system development where an initial approximate theory must be refined the approach also applies at any point in the expert system lifecycle when the expert system generates incorrect result the approach ha been applied to the domain of molecular biology and show significantly better result then a purely inductive learner 
to allow efficient parallel processing of prolog program on distributed multiprocessor a nonshared variable binding approach is required such that binding environment can be independently distributed among processor this paper present a binding scheme which realises the independence of a clause s binding environment by eagerly instantiating variable across clause argument the application of the scheme on a prolog virtual machine ha illustrated it feature of efficiency in execution and simplicity in implementation the preliminary performance evaluation ha demonstrated the feasibility of the scheme 
using case to find innovative solution to problem is mainly the result of two process cross contextual rem in ding and composition of multiple case or case part although the ability to use case taken from across contextual boundary is desirable there is a tension between representing and accessing case across context and in using part of multiple case to synthesize a solution one way of alleviating this difficulty is through index transformation in this paper we represent two index transformation technique that facilitate both cross contextual remindings and the access of multiple appropriate case part the mechanism are general and principled based on a qualitative calculus they are also behavior preserving a needed requirement for case synthesis in many domain of interest the transformation technique have been implemented in cadet a case based problem solver mat operates in the domain of mechanical design 
this paper describes a method of classifying semantically similar noun the approach is based on the distributional hypothesis our approach is characterized by distinguishing among sens of the same word in order to resolve the polysemy issue the classification result demonstrates that our approach is successful 
sensor interpret ation involves the determination of high level explalliations of sensor data blackboard based interpretation system have usually been limited to incremental hypothesize and test strategy for resolving uncertainty we have developed a new interpretation framework that support the use of more sophisticated strat egies like differential diagnosis the resun framework ha two key component an evidential represent ation that includes explicit symbolic encoding of the source of uncertainty sou in the evidence for hypothesis and a script based incremental control planner interpretation is viewed a an incremental process of gathering evidence to resolve particular source of uncertainty control plan invoke action that examine the symbolic sou associated with hypothesis and use the resulting information to post goal to resolve uncertadnty these goal direct the system to expand method appropriate for resolving the current source of uncertllinty in the hypothesis the planner s refocusing mechanism make it possible to postpone focusing decision when there is insufficient information to make decision and provides opportunistic control capability the resun framework ha been implemented and experimentally verified using a simulated aircraft monitorilllg application 
most natural language based document retrieval system use the syntax structure of constituent phrase of document a index term many of these system also attempt to reduce the syntactic variability of natural language by some normalisation procedure applied to these syntax structure however the retrieval performance of such system remains fairly disappointing some system therefore use a meaning representation language to index and retrieve document in this paper a system is presented that us horn clause logic a meaning representation language employ advanced technique from natural language processing to achieve incremental extensibility and us method from logic programming to achieve robustness in the face of insufficient data 
this paper present a contour tracing algorithm based on a priori knowledge about searched edge the algorithm is destined to trace edge having long fragment of approximately constant direction this enables the implementation of one edge detector mask only in a given area and simplifies the thinning procedure the way of searching for starting point is discussed a well a choosing and joining fragment of edge assuring the best correspondence between the found edge and the knowledge possessed the algorithm show good insensitivity to noise and local edge distortion 
a board is described that contains the anna neural network chip anda dsp c digital signal processor the anna analog neural networkarithmetic unit chip performs mixed analog digital processing the combination of anna with the dsp allows high speed end toendexecution of numerous signal processing application includingthe preprocessing the neural net calculation and the postprocessingsteps the anna board evaluates neural network to time faster than the dsp alone 
a new stochastic motion estimation method based on the maximum a posteriori probability map criterion is developed deterministic algorithm approximating the map estimation over discrete and continuous state space are proposed these approximation result in known motion estimation algorithm the theoretical superiority of the stochastic algorithm over deterministic approximation in locating the global optimum is confirmed experimentally 
in this paper we discus a mechanism for modifying context in a tutorial dialogue the context mechanism imposes a pedagogically motivated misrepresentation pmm on a dialogue to achieve instructional goal in the paper we outline several type of pmms and detail a particular pmm in a sample dialogue situation while the notion of pmms are specifically oriented towards tutorial dialogue misrepresentation ha interesting implication for context in dialogue situation generally and also suggests that grice s maxim of quality need to be modified 
we have recently reported on two new word sense disambiguation system one trained on bilingual material the canadian hansard and the other trained on monolingual material roget s thesaurus and grolier s encyclopedia after using both the monolingual and bilingual classifier for a few month we have convinced ourselves that the performance is remarkably good nevertheless we would really like to be able to make a stronger statement and therefore we decided to try to develop some more objective evaluation measure although there ha been a fair amount of literature on sense disambiguation the literature doe not offer much guidance in how we might establish the success or failure of a proposed solution such a the two system mentioned in the previous paragraph many paper avoid quantitative evaluation altogether because it is so difficult to come up with credible estimate of performance this paper will attempt to establish upper and lower bound on the level of performance that can be expected in an evaluation an estimate of the lower bound of averaged over ambiguous type is obtained by measuring the performance produced by a baseline system that ignores context and simply assigns the most likely sense in all case an estimate of the upper bound is obtained by assuming that our ability to measure performance is largely limited by our ability obtain reliable judgment from human informant not surprisingly the upper bound is very dependent on the instruction given to the judge jorgensen for example suspected that lexicographer tend to depend too much on judgment by a single informant and found considerable variation over judgment only agreement a she had suspected in our own experiment we have set out to find word sense disambiguation task where the judge can agree often enough so that we could show that they were outperforming the baseline system under quite different condition we have found agreement over judge 
expert system in complex domain require rich knowledge representation formalism and problem solving paradigm a typical framework may involve a blackboard architecture and a reason maintenance system rms to guarantee the consistency of the link between the blackboard node however in order to satisfy computational feasibility and become operational the resulting expert system must often be rewritten using le expressive tool we propose an architecture integrating efficiently an ops like inference engine and an assumption based truth maintenance system atm these paradigm have been separately investigated and extended role distribution between an atm and an inference engine integrated in a single framework is one of the major issue to obtain good overall performance two architecture will be studied loose coupling where the atm and the inference engine are clearly separated and tight coupling where the atm is intimately integrated with the match phase of a rete based inference engine the advantage and drawback of both solution are described in detail finally future work is discussed 
document clustering ha not been well received a an information retrieval tool objection to it use fall into two main category first that clustering is too slow for large corpus with running time often quadratic in the number of document and second that clustering doe not appreciably improve retrieval we argue that these problem arise only when clustering is used in an attempt to improve conventional search technique however looking at clustering a an information access tool in it own right obviates these objection and provides a powerful new access paradigm we present a document browsing technique that employ document clustering a it primary operation we also present fast linear time clustering algorithm which support this interactive browsing paradigm 
plan fail for many reason during planner development failure can often be traced to action of the planner itself failure recovery analysis is a procedure for analyzing execution trace of failure recovery to discover how the planner s action may be causing failure the four step procedure involves statistically analyzing execution data for dependency between action and failure mapping those dependency to plan structure explaining how the structure might produce the observed dependency and recommending modification the procedure is demonstrated by applying it to explain how a particular recovery action may lead to a particular failure in the phoenix planner the planner is modified based on the recommendation of the analysis and the modification are shown to improve the planner s performance by removing a source of failure and so reducing the overall incidence of failure 
this paper characterizes connectionist type architecture that allow a distributed solution for class of constraint satisfaction problem the main issue addressed is whether there exists a uniform model of computation where all node are indistinguishable that guarantee convergence to a solution from every initial state of the system whenever such a solution exists we show that even for relatively simple constraint network such a ring there is no general solution using a completely uniform asynchronous model however some restricted topology like tree can accommodate the uniform asynchronous model and a protocol demonstrating this fact is presented an almost uniform asynchronous network consistency protocol is also presented we show that the algorithm are guaranteed to be self stabilizing which make them suitable for dynamic or error prone environment 
in baader a we have considered different type of semantics for terminologicial cycle in the concept language tlq which allows only conjunction of concept and value restriction it turned out that greatest fixed point semantics gfp semantics seems to be most appropriate for cycle in this language in the present paper we shall show that the concept defining facility of flo with cyclic definition and gfp semantics can also be obtained in a different way one may replace cycle by role definition involving union composition and transitive closure of role this proposes a way of retaining in an extended language the pleasant feature of gfp semantics for flq with cyclic definition without running into the trouble caused by cycle in larger language starting with the language alc of schmidt schau b smolka which allows negation conjunction and disjunction of concept a well a value restric tions and exists in restriction we shall disallow cyclic concept definition but instead shall add the possibility of role definition involving union composition and transitive closure of role in contrast to other terminological kr system which incorporate the transitive closure operator for role we shall be able to give a sound and complete algorithm for concept subsumption 
identifying the regularity underlying speaker decision to emphasize or de emphasize an item intonationally ha long been the subject of speculation and controversy this paper describes a study of accent assignment based upon the analysis of natural recorded read speech result are being incorporated in newspeak an interface to the bell laboratory text to speech system which varies intonational feature based upon syntactic structure and higher level discourse information inferred from unrestricted text in order to generate more natural synthetic speech implication of the work for the evaluation of discourse model for automatic labeling of prosodic feature and for speech synthesis are discussed 
this paper describes a linguistic knowledge representation technique suitable for reducing analysis time and memory requirement in a parser for continuous speech parsing speech having to process a lattice of word hypothesis instead of a string of word involves a tremendous amount of search and the generation of a high number of phrase hypothesis the aim is while using powerful and flexible formalism for syntax and semantics to generate compact phrase hypothesis each one accounting for many syntactic rule simultaneously the proposed method is able to cope with and to take advantage from the fact that short word are often missing from the lattice a detailed example is given to clarify this method finally experimental data arc presented and discussed showing the effectiveness of the proposed technique 
the dominant approach to information retrieval system design are based on rational theory and cognitive engineering however these theory a well a approach in other discipline reviewed in this paper do not account for communication or interaction among design participant which is critical to design outcome this research attempt to develop a descriptive design model that account for communication among user designer and developer throughout the design process a pilot study ha been completed and a preliminary model that represents a first step in understanding participant evolving perception and expectation of the design process and it outcome is described in this paper 
this paper describes an agent architecture and it implementation for situated robot control in field environment the architecture draw from the idea of universal plan and subsumption s layered control producing reaction plan that exploit low level competence a operator the architecture ha been implemented in an extended version of the gapps rex situated automaton programming language this language produce synchronous virtual circuit which have been shown to have formal epistemic property the resulting architecture exhibit robust task execution ha high level goal representation and maintains consistent semantics between agent state and the environment ongoing experiment using the architecture with two land mobile robot and one undersea mobile robot are described the robot perform their task robustly during normal change in the task environment 
a system is described for acquiring a context sensitive phrase structure grammar which is applied by a best path bottom up deterministic parser the grammar wa based on english news story and a high degree of success in parsing in reported overall this research concludes that csg is a computationally and conceptually tractable approach to the construction of phrase structure grammar for news story text 
this paper describes a representation and computational model for deriving three dimensional articulated volumetric description of object from laser rangefinder data what differentiates this work from other approach is that it is purely bottom up relying on general assumption cast in term of differential geometry 
i present a semantic analysis of collective distributive ambiguity and resolution of such ambiguity by model based reasoning this approach go beyond scha and stallard whose reasoning capability wa limited to checking semantic type my semantic analysis is based on link and robert where distributivity come uniformly from a quantificational operator either explicit e g each or implicit e g the d operator i view the semantics module of the natural language system a a hypothesis generator and the reasoner in the pragmatic module a a hypothesis filter cf simmons and davis the reasoner utilizes a model consisting of domain dependent constraint and domain independent axiom for disambiguation there are two kind of constraint type constraint and numerical constraint and they are associated with predicate in the knowledge base whenever additional information is derived from the model the contradiction checker is invoked to detect any contradiction in a hypothesis using simple mathematical knowledge cdcl collective distributive constraint language is used to represent hypothesis constraint and axiom in a way isomorphic to diagram representation of collective distributive ambiguity 
neuron in area mt of primate visual cortex encode the velocity of moving object we present a model of how mt cell aggregate response from v to form such a velocity representation two different set of unit with local receptive field receive input from motion energy filter one set of unit form estimate of local motion while the second set computes the utility of these estimate output from this second set of unit gate the output from the first set through a gain control mechanism this active process for selecting only a subset of local motion response to integrate into more global response distinguishes our model from previous model of velocity estimation the model yield accurate velocity estimate in synthetic image containing multiple moving target of varying size luminance and spatial frequency profile and deal well with a number of transparency phenomenon 
this paper report several experimental result on the speed of convergence of neural network training using genetic algorithm and back propagation recent excitement regarding genetic search lead some researcher to apply it to training neural network there are report on both successful and faulty result and unfortunately no systematic evaluation ha been made this paper report result of systematic experiment designed to judge whether use of genetic algorithm provides any gain in neural network training over existing method experimental result indicate that genetic search is at best equally efficient to faster variant of back propagation in very small scale network but far le efficient in larger network 
this paper describes the application of explanation based learning a machine learning technique to the sri core language engine a large scale general purpose natural language analysis system the idea is to bypass normal morphological syntactic and 
explanation requires a dialogue user must be allowed to ask question about previously given explanation however building an interface that allows user to ask follow up question pose a difficult challenge for natural language understanding because such question often intermix meta level reference to the discourse with object level reference to the domain we propose a hypertext like interface that allows user to point to the portion of the system s explanation they would like clarified by allowing user to point many of the difficult referential problem in natural language analysis can be avoided however the feasibility of such an interface rest on the system s ability to understand what the user is pointing at i e the system must understand it own explanation to solve this problem we employ a planning approach to explanation generation which record the design process that produced an explanation so that it can be used in later reasoning in this paper we show how synergy arises from combining a pointing style interface with a text planning generation system making explanation dialogue more feasible 
our eminent researcher including john mccarthy allen newell claude shannon herb simon ken thompson and alan turing put significant effort into computer chess research now that computer have reached the grandmaster level and are beginning to vie for the world championship the ai community should pause to evaluate the significance of chess in the evolving objective of ai evaluate the contribution made to date and ass what can be expected in the future despite the general interest in chess amongst computer scientist and the significant progress in the last twenty year there seems to be a jack of appreciation for the field in the ai community on one hand this is the fruit of success brute force work why study anything else but also the result of a focus on performance above all else in the chess community also chess ha proved to be too challenging for many of the ai technique that have been thrown at it we wish to promote chess a the fundamental test bed recognized by our founding researcher and increase awareness of it contribution to date 
experiment are reported on the use of an assumption based truth maintenance system atm to establish a match between a d model and a single d image we show that the atm improves the efficiency of the search for maximal combination of consistently labelled feature a memory cost is incurred associated with the recording system of the atm this can be reduced by simple heuristic empirical evidence is presented quantifying the cost and benefit of the method 
the high resolution field of view of the human eyeonly cover a tiny fraction of the total field of view while this arrangement allows for great economy incomputational resource it force the visual systemto solve other problem that would not exist with uniformlyhigh resolution one of these problem is howto determine where to redirect the fovea given onlythe low resolution information available in the periphery the advent of spatially variant receptor arraysfor camera ha made it 
this paper explores the effect of initial weight selection on feed forward network learning simple function with the back propagation technique we first demonstrate through the use of monte carlo technique that the magnitude of the initial condition vector in weight space is a very significant parameter in convergence time variability in order to further understand this result additional deterministic experiment were performed the result of these experiment demonstrate the extreme sensitivity of back propagation to initial weight configuration 
we develop a qualitative method for under standing and representing phase space struc tures of complex system to demonstrate this method a program called map ha been con structed that understands qualitatively differ ent region of a phase space and represents and extract geometric shape information about these region using deep domain knowledge of dynamical system theory given a dynamical system specified a a system of governing equa tions map applies a successive sequence of operation to incrementally extract the qual itative information and generates a complete high level symbolic description of the phase space structure through a combination of nu merical combinatorial and geometric compu tations and spatial reasoning technique the high level description is sensible to human be ings and manipulable by other program we are currently applying the method to a difficult engineering design domain in which controller for complex system are to be automatically synthesized to achieve desired property based on the knowledge of the phase space shape of the system 
we are interested in the analysis of non polyhedral scene we will present an original egomotion algorithm based on the tracking of arbitrary curve in a sequence of gray scale image this differential method analysis the spatiotemporal surface and extract a simple equation relating the motion parameter and measure on the spatiotemporal surface when a curve contour line is tracked in image sequence this equation allows to extract the d motion parameter of the object attached to the contour when rigid motion is assumed experiment on synthetic a well a real data show the validity of this method 
symmetry is usually viewed a a discrete feature an object is either symmetric or non symmetric wepropose to view symmetry a a continuous feature anddefine a continuous symmetry measure csm toquantify the symmetry of object some applicalionsare also presented 
in a bayesian framework we give a principled account of how domainspecificprior knowledge such a imperfect analytic domain theory can beoptimally incorporated into network of locally tuned unit by choosinga specific architecture and by applying a specific training regimen ourmethod proved successful in overcoming the data deficiency problem ina large scale application to devise a neural control for a hot line rollingmill it achieves in this application significantly higher 
this paper give an overview of a natural language dialogue system called pragma this system contains a number of novel and important feature a well a integrating previous work into a unified mechanism the most important advance that pragma represents compared with previous system is the high degree of bidirectionality employed in it design a single grammar is used for interpretation and generation and the same knowledge source are used for plan recognition and response generation the system is also flexible in that it generates useful extended response not only to query which allow the user s plan to be inferred but also to query which do not allow this 
in baader a we have considered different type of semantics for terminologicial cycle in the concept language tlq which allows only conjunction of concept and value restriction it turned out that greatest fixed point semantics gfp semantics seems to be most appropriate for cycle in this language in the present paper we shall show that the concept defining facility of flo with cyclic definition and gfp semantics can also be obtained in a different way one may replace cycle by role definition involving union composition and transitive closure of role this proposes a way of retaining in an extended language the pleasant feature of gfp semantics for flq with cyclic definition without running into the trouble caused by cycle in larger language starting with the language alc of schmidt schau smolka which allows negation conjunction and disjunction of concept a well a value restriction and exists in restriction we shall disallow cyclic concept definition but instead shall add the possibility of role definition involving union composition and transitive closure of role in contrast to other terminological kr system which incorporate the transitive closure operator for role we shall be able to give a sound and complete algorithm for concept subsumption 
we present a method for extracting geometric and relational structure from raw intensity data on one hand low level image processing extract isolated feature on the other hand image interpretation us sophisticated object description in representation framework such a semantic network we suggest an intermediate level description between lowand high level vision this description is produced by grouping image feature into more and more abstract structure first we motivate our choice with respect to what should be represented and we stress the limitation inherent with the use of sensory data second we describe our current implementation and illustrate it with various example 
we describe an approach to training a statistical parser from a bracketed corpus and demonstrate it use in a software testing application that translates english specification into an automated testing language a grammar is not explicitly specified the rule and contextual probability of occurrence are automatically generated from the corpus the parser is extremely successful at producing and identifying the correct parse and nearly deterministic in the number of par that it produce to compensate for undertraining the parser also us general linguistic subtheories which aid in guessing some type of novel structure 
this paper describes the geometrical limitation of algorithm for d reconstruction which use corresponding line token in addition to announcing a description of the general critical set we analyse the configuration defeating the liu huang algorithm and study the relation between these set 
although generalization and discrimination are commonly used together in machine learning little ha been understood about how these two method are intrinsically related this paper describes the idea of complementary discrimination which exploit semantically the syntactic duality between the two approach discriminating a concept is equivalent to generalizing the complement of the concept and vice versa this relation brings together naturally generalization and discrimination so that learning program may utilize freely the advantage of both approach such a learning by analogy and learning from mistake we will give a detailed description of the complementary discrimination learning cdl algorithm and extend the previous result by considering the effect of noise and analyzing the complexity of the algorithm cdl s performance on both perfect and noisy data and it ability to manage the tradeoff between simplicity and accuracy of concept have provided some evidence that complementary discrimination is a useful and intrinsic relation between generalization and discrimination 
efficient syntactic and semantic parsing for ambiguous context free language are generally characterized a complex specialized highly formal algorithm in fact they are readily constructed from straightforward recursive transition network rtns in this paper we introduce lr rtns and then computationally motivate a uniform progression from basic lr parsing to earley s chart parsing concluding with tomita s parser these apparently disparate algorithm are unified into a single implementation which wa used to automatically generate all the figure in this paper 
introductionessential process in computer vision consist in drawing quantitative information about a threedimensionalscene by mean of two dimensional perspective image here it belongs to everyonessubjective experience that depth information is available from comparison of the ishapesj of correspondingobject and image con gurations unfortunately however the concept of shape ha nocanonical mathematical description in section a de nition convenient for the above purpose will 
simple second order recurrent network are shown to readily learn small knownregular grammar when trained with positive and negative string example weshow that similar method are appropriate for learning unknown grammar fromexamples of their string the training algorithm is an incremental real time recurrentlearning rtrl method that computes the complete gradient and updatesthe weight at the end of each string after or during training a dynamic clusteringalgorithm extract 
we compare two strategy for training connectionist a well a non connectionist model for statistical pattern recognition the probabilistic strategyis based on the notion that bayesian discrimination i e o ptimal classification is achieved when the classifier learns thea posterioriclass distribution of the random feature vector the differential strategy is based on the notion that the identity of the largest class a posteriori probability of the feature vector is all that is needed to achieve bayesian discrim ination each strategy is directly linked to a family of objective function that can be used in the supervised train ing procedure we prove that the probabilistic strategy linked with error measure objective function such a mean squared error and cross entropy typically used to train classifier necessarily requires larger training set and more complex classifier architecture than those needed to approximate the bayesian discriminant function in contrast we prove that the differential strategy linked with classification figure of merit objective function cfmmono requires the minimum classifier functional complexity and the fewest training example necessary to approximate the bayesian discriminant function with specified precision measured in probability of error we present our proof in the context of a game of chance in which an unfair sided die is tossed repeatedly we show that this rigged game of dice is a paradigm at the root of all statistical pattern recognition task and demonstrate how a simple extension of the concept lead u to a general information theoretic model of sample complexity for statistical pattern recognition 
we describe a system of reversible grammar in which given a logic grammar specification of a natural language two efficient prolog program are derived by an off line compilation process a parser and a generator for this language the centerpiece of the system is the inversion algorithm designed to compute the generator code from the parser s prolog code using the collection of minimal set of essential argument msea for predicate the system ha been implemented to work with definite clause grammar dcg and is a part of an english japanese machine translation project currently under development at nyu s courant institute 
when explanation based learning ebl is used for knowledge level learning kll training example are essential and ebl is not simply reducible to partial evaluation a key enabling factor in this behavior is the use of domain theory in which not every element is believed a priori when used with such domain theory ebl provides a basis for rote learning deductive kll and induction from multiple example nondeductive kll this article lay the groundwork for using ebl in kll by describing how ebl can lead to increased belief and describes new result from using soar s chunking mechanism a variation on ebl a the basis for a task independent rote learning capability and a version space based inductive capability this latter provides a compelling demonstration of nondeductive kll in soar and provides the basis for an integration of conventional ebl with induction however it also reveals how one of soar s key assumption the non penetrable memory assumption make this more complicated than it would otherwise be this complexity may turn out to be appropriate or it may point to where modification of soar are needed 
stadard reinforcement learning method assume they can identify each state distinctly before making an action decision in reality a robot agent only ha a limited sensing capability and identifying each state by extensive sensing can be time consuming this paper describes an approach that learns active perception strategy in reinforcement learning and considers sensing cost explicitly the approach integrates cost sensitive learning with reinforcement learning to learn an efficient internal state representation and a decision policy simultaneously in a finite deterministic environment it not only maximizes the long term discounted reward per action but also reduces the average sensing cost per state the initial experimental result in a simulated robot navigation domain are encouraging 
traditional syntactic model of parsing have been inadequate for task driven processing of extended text because they spend most of their time on misdirected linguistic analysis leading to problem with both efficiency and coverage statistical and domain driven processing offer compelling possibility but only a a complement to syntactic processing for semanticallyoriented task such a data extraction from text the problem is how to combine the coverage of these weaker method with the detail and accuracy of traditional lingusitic analysis a good approach is to focus linguistic analysis on relation that directly impact the semantic result detaching these relation from the complete constituent to which they belong this approach result in a faster more robust and potentially more accurate parser for real text 
many design task have search space that are vague and evaluation criterion that are subjective we present a model of design that can solve such problem using a method of plausible design adaptation in our approach adaptation transformation are used to modify the component and structure of a design and constraint on the design problem this adaptation process play multiple role in design it is used a part of case based reasoning to modify previous design case it accommodates constraint that arrive late in the design process by adapting previous decision rather than by retracting them it resolve impasse in the design process by weakening preference constraint this model of design ha been implemented in a computer program called julia that design the presentation and menu of a meal to satisfy multiple interacting constraint 
this paper describes a series of experiment that were performed on the rocky iii robot rocky iii is a small autonomous rover capable of navigating through rough outdoor terrain to a predesignated area searching that area for soft soil acquiring a soil sample and depositing the sample in a container at it home base the robot is programmed according to a reactive behavior control paradigm using the alfa programming language this style of programming produce robust autonomous performance while requiring significantly le computational resource than more traditional mobile robot control system the code for rocky iii run on an bit processor and us about k of memory 
there exist method in automated theorem proving for non classical logic based on translation of logic from a non classical source logic abbreviated henceforth sl into a classical target logic abbreviated henceforth tl these valuable method do not address the important practical problem of presenting proof in sl we propose a framework applicable at least to s p k t k for presenting proof of theorem of these logic found in a familiar tl order sorted predicate logic abbreviated henceforth ospl the method backward translates lemma in a deduction in tl either a into lemma in a corresponding deduction in sl in the best case or b into formula semantically related to lemma in a corresponding deduction in the worst case a a natural consequence we bring to the fore the fact that this framework can also be used to help in solving another important and very difficult problem the transfer of strategy from one logic to another one conjecture with corresponding theorem which is a particular case of itis stated when b above hold we give sufficient and in general satisfactory condition in order to obtain the lemma in sl two example are treated in full detail the well known problem of the wise man puzzle and another one which show how our method can be used to transfer strategy no additional theoretical result is given in this direction but it is clear from the example how the proposed framework can help to transfer strategy 
we present the mapping unit approach to representing subeategorization information a computational framework for encoding subcategorization information which ha been developed and implemented for bbn s delphi system the nl component of the harc spoken language system the advantage of our approach to subeategorization lie in it flexibility a flexibility which in turn offer greater robustness of coverage with respect to unanticipated variation of a verbal argument pattern and easier extension of coverage to new pattern it handle in a quite natural way argument order variation optionality of guments and metonymy 
a novel multiresolution image analysis technique based on hierarchy of irregular tessellation generated in parallel by independent stochastic process is presented like traditional image pyramid these hierarchy are constructed in a number of step on the order of log image size step however the structure of a hierarchy is adapted to the image content and artifact of rigid resolution reduction are avoided two application of these technique are presented connected component analysis of labeled image and segmentation of gray level image in labeled image every connected component is reduced to a separate root with the adjacency relation among the component also extracted in gray level image the output is a segmentation of the image into a small number of class a well a the adjacency graph of the class 
designing a user interface is an ill defined problem making cooperative problem solving system a promising approach to support user interface designer cooperative problem solving system are modular system that support the human designer with multiple independent system component we present a system architecture and an implemented system framer that demonstrate the cooperative problem solving approach framer represents design knowledge in formal machine interpretable knowledge source such a critic and dynamic specification sheet and in semi formal knowledge source such a a palette of user interface building block and a checklist each of these component contributes significantly to the overall usefulness of the system while requiring only limited resource to be designed and implemented 
explanation based learning ebl can be used to significantly speed up problem solving is there sufficient structure in the definition of a problem space to enable a static analyzer using ebl style optimization to speed up problem solving without utilizing training example if so will such an analyzer run in reasonable time this paper demonstrates that for a wide range of problem space the answer to both question is yes the static program speed up problem solving for the prodigy problem solver without utilizing training example in minton s problem space static acquires control knowledge from twenty six to seventy seven time faster and speed up prodigy up to three time a much a prodigy ebl this paper present static s algorithm derives a condition under which static is guaranteed to achieve polynomial time problem solving and contrast static with prodigy ebl 
the development of a formal logic for reasoning about change ha proven to be surprisingly difficult furthermore the logic that have been developed have found surprisingly little application in those field such a qualitative reasoning that are concerned with building program that emulate human common sense reasoning about change in this paper we argue that a basic tenet of qualitative reasoning practice the separation of modeling and simulation obviates many of the difficulty faced by previous attempt to formalize reasoning about change our analysis help explain why the qr community ha been nonplussed by some of the problem studied in the nonmonotonic reasoning community further the formalism we present provides both the beginning of a formal foundation for qualitative reasoning and a framework in which to study a number of open problem in qualitative reasoning 
the author attempt to determine what can be inferred from ambiguity in process of visual interpretation they discus this question in a specific context the interpretation of scene geometry in the form of parametrized volumetric model ambiguity is described a a local probabilistic property of the misfit error surface in the parameter space of superellipsoid model namely a an ellipsoid of confidence in which there is a given probability that the true parameter can be found the author show how to project the ellipsoid of confidence back into d space to obtain the shell in which the true d surface most probably lie and introduce what they call the uncertainty a a local property of the fitted model s surface they propose a technique that can use this information to plan a new direction of view that minimizes the ambiguity of subsequent interpretation 
linear precedence lp rule are widley used for stating word order principle they have been adopted a constraint by hpsg but no encoding in the formalism ha been provided since they only order sibling they are not quite adequate at least not for german we propose a notion of lp constraint that applies to linguistically motivated branching domain such a head domain we show a type based encoding in an hpsg style formalism that support processing the encoding can be achieved by a compilation step 
several different technique have been proposedfor computer recognition of human face this paperpresents the first result of an ongoing project to compareseveral recognition strategy on a common database a set of algorithm ha been developed to ass the feasibilityof recognition using a vector of geometrical feature such a nose width and length mouth position and chinshape the performance of a nearest neighbor classifier with a suitably defined metric is reported a a 
based on psychological study which show that metaphor and other nonliteral construction are comprehended in the same amount of time a comparable literal construction some researcher have concluded that literal meaning is not computed during metaphor comprehension in this paper we suggest that the empirical evidence doe not rule out the possibility that literal meaning is constructed we present a computational model of metaphor comprehension which is consistent with the data but in which literal meaning is computed this model ha been implemented a part of a unification based natural language processing system called link 
in contrast to the designer logic approach this paper show how the attribute value feature structure of unification grammar and constraint on them can be axiomatized in classical first order logic which can express disjunctive and negative constraint because only quantifier free formula are used in the axiomatization the satisfiability problem is np complete 
function sharing is the simultaneous implementation of several function by a single structural element this article describes how the idea of function sharing can be used in a computational design procedure that produce efficient design from modular design these idea have been implemented a a computer program for the domain of mechanical device that can be described functionally a a network of lumped parameter idealized element 
model based recognition method generally search for geometrically consistent pair of model and image feature the quality of an hypothesis is then measured using some function of the number of model feature that are paired with image feature the most common approach is to simply count the number of pair of consistent model and image feature however this may yield a large number of feature pair due to a single model feature being consistent with several image feature and vice versa a better quality measure is provided by the size of a maximal bipartite matching which eliminates the multiple counting of a given feature computing such a matching is computationally expensive but under certain condition it is well approximated by the number of distinct feature consistent with a given hypothesis 
this paper describes cabot a case based system that is able to adjust it retrieval and adaptation metric in addition to storing case it ha been applied to the game of othello experiment show that cabot save about half a many case a similar system that do not adjust their retrieval and adaptation mechanism it also consistently beat these system these result suggest that existing case based system could save fewer case without reducing their current level of performance they also demonstrate that it is beneficial to distinguish failure due to missing information faulty retrieval and faulty adaptation 
family of kernel that are useful in a variety of early vision algorithm may be obtained by rotating and scaling in a continuum a template kernel these multi scale multi orientation family may be approximated by linear interpolation of a discrete finite set of appropriate basis kernel a scheme for generating such a basis together with the appropriate interpolation weight is described unlike previous scheme by perona and simoncelli et al it is guaranteed to generate the most 
in machine learning there is considerable interest in technique which improve planning ability initial investigation have identified a wide variety of technique to address this issue progress ha been hampered by the utility problem a basic tradeoff between the benefit of learned knowledge and the cost to locate and apply relevant knowledge in this paper we describe the composer system which embodies a probabilistic solution to the utility problem we outline the statistical foundation of our approach and compare it against four other approach which appear in the literature 
a central problem in text understanding research is the indeterminacy of natural language two related issue that arise in confronting this problem are the need to make complex interaction possible among the system component that search for cue and the need to control the amount of reasoning that is done once cue have been discovered we identify a key difficulty iu enabling true interaction among system component and we propose an architectural framework that minimizes this difficulty a concrete example of a reasoning task encountered in an actual text understanding application is used to motivate the design principle of our framework 
the author argue for a generalisation of inference from the standard account in term of truth preservation to one which countenance preservation of other desirable metalinguistic property the development is partly historical and partly analytic a relational account of preservation is then presented and from this the two notion of inferential structure and inferential model are derived to illustrate the generality of the relational conception of inference we show how such structure and model can be realised in the development of a legal advisory system 
this paper considers the determination of internal camera parameter from two view of a point set in three dimension a non iterative algorithm is given for determining the focal length of the two camera a well a their relative placement assuming all other internal camera parameter to be known it is shown that this is all the information that may be deduced from a set of image correspondence 
this paper describes an algorithm for estimation of directionality in d and d vector field and how that feature relates to the curvature of curve in d image and surface in d image 
one of the grand challenge for machine learning is the problem of learning from textbook this paper address the problem of learning from text including omission and inconsistency that are clarified by illustrative example to avoid problem in natural language understanding we consider a simplification of this problem in which the text ha been manually translated into a logical theory this learning problem is solvable by a technique that we call analogical abductive explanation based learning ana ebl formal evidence and experimental result in the domain of contract bridge show that the learning technique is both efficient and effective 
focus of attention is extremely important in human visual perception if computer vision system are to perform task in a complex dynamic world they will have to be able to control processing in a way that is analogous to visual attention in human 
the development of larger scale natural language system ha been hampered by the need to manually create mapping from syntactic structure into meaning representation a new approach to semantic interpretation is proposed which us partial syntactic structure a the main unit of abstraction for interpretation rule this approach can work for a variety of syntactic representation corresponding to directed acyclic graph it is designed to map into meaning representation based on frame hierarchy with inheritance we define semantic interpretation rule in a compact format the format is suitable for automatic rule extension or rule generalization when existing hand coded rule do not cover the current input furthermore automatic discovery of semantic interpretation rule from input output example is made possible by this new rule format the principle of the approach are validated in a comparison to other method on a separately developed domain instead of relying purely on painstaking human effort this paper combine human expertise with computer learning strategy to successfully overcome the bottleneck of semantic interpretation 
this paper present a new approach for exploiting truth maintenance system tm which make them simpler to use without necessarily incurring a substantial performance penalty the basic intuition behind this approach is to convey the locality of the knowledge representation of the problem solver to the tm the tm then us this locality information to control and restrict it inference the new tm accept arbitrary propositional formula a input and use general boolean constraint propagation bcp to answer query about whether a particular literal follows from the formula our tm exploit the observation that if the set of propositional formula are converted to their prime implicates then bcp is both efficient and logically complete this observation allows the problem solver to influence the degree of completeness of the tm by controlling how many implicates are constructed this control is exerted by using the locality in the original task to guide which combination of formula should be reduced to their prime implicates this approach ha been implemented and tested both within assumption based truth maintenance system and logic based truth maintenance system 
this paper investigates the effect of parallelism on blackboard system scheduling a parallel blackboard system is described that allows multiple knowledge source instantiation to execute in parallel using a shared memory blackboard approach new class 
this paper will present computer model of three robotic motion planning and learning system which use a multi sensory learning strategy for learning and control in these system machine vision input is used to plan and execute movement utilizing an algorithmic controller while at the same time neural network learn the control of those motion using feedback provided by position and velocity sensor in the actuator a specific advantage of this approach is that in addition to the system leaming a more automatic behavior it employ a computationally le costly sensory system more tightly coupled from perception to action 
scene analysis especially for real data is a complex problem there are two main explanation which interest u in this paper 
we evaluate current explanation scheme these are either insufficiently general or suffer from other serious drawback we propose a domain independent explanation system that is based on ignoring irrelevant variable in a probabilistic setting we then prove important property of some specific irrelevance based scheme and discus how to implement them 
this paper concern the interactive construction of geometric model of object from image sequence we show that when the object are constrained to move on the ground plane a simple direct sfm algorithm is possible which is vastly superior to conventional method the proposed algorithm is non iterative and in general requires a minimum of three point in two frame experimental comparison with other method are presented in the paper it is shown to be greatly superior to general linear sfm algorithm not only in computational cost but also in accuracy and noise robustness it provides a practical method for modelling moving object from monocular monochromatic image sequence 
machine translation of locative preposition is not straightforward even between closely related language this paper discus a system of translation of locative preposition between english and french the system is based on the premise that english and french do not always conceptualize object in the same way and that this account for the major difference in the way that locative preposition are used in these language this paper introduces knowledge representation of conceptualization of object and a method for translating preposition based on these conceptual representation 
this paper describes an operational system which can acquire the core meaning of word without any prior knowledge of either the category or meaning of any word it encounter the system is given a input a description of sequence of scene along with sentence which describe the event taking place a those scene unfold and produce a output a lexicon consisting of the category and meaning of each word in the input that allows the sentence to describe the event it is argued that each of the three main component of the system the parser the linker and the inference component make only linguistically and cognitively plausible assumption about the innate knowledge needed to support tractable learning the paper discus the theory underlying the system the representation and algorithm used in the implementation the semantic constraint which support the heuristic necessary to achieve tractable learning the limitation of the current theory and the implication of this work for language acquisition research 
knowledge compilation speed inference by creating tractable approximation of a knowledgebase but this advantage is lost if the approximation are too large we show how learningconcept generalization can allow for a more compact representation of the tractabletheory we also give a general induction rule for generating such concept generalization finally we prove that unless np non uniform p not all theory have small horn leastupper bound approximation introductionwork in 
tracking multiple moving object in image sequence involves a combination of motion detection and segmentation this task can become complicated a image motion may change significantly between frame like with camera vibration such vibration make tracking in longer sequence harder a temporal motion constancy can not be assumed 
edge element defined a maximum of the gradient magnitude in gradient direction of a gaussian smoothed image are usually thresholded to suppress edge element due to noise in low contrast image region thresholding may suppress also edge element which are part of a significant image structure and may thus result in the fragmentation or total loss of such structure 
this paper describes an implemented program that take a raw untagged text corpus a it only input no open class dictionary and generates a partial list of verb occurring in the text and the subcategorization frame sfs in which they occur verb are detected by a novel technique based on the case filter of rouvret and vergnaud the completeness of the output list increase monotonically with the total number of occurrence of each verb in the corpus false positive rate are one to three percent of observation five sfs are currently detected and more are planned ultimately i expect to provide a large sf dictionary to the nlp community and to train dictionary for specific corpus 
bonsai a model based d object recognition system is described it identifies and localizes d object in range image of one or more part that have been designed on a computer aided design cad system recognition is performed via constrained search of the interpretation tree using unary and binary constraint derived automatically from the cad model to prune the search space attention is focused on the recognition procedure but the model building image acquisition and segmentation procedure are also outlined experiment with over image demonstrate that the constrained search approach to d object recognition ha an accuracy comparable to that of previous system 
recent development in generation algorithm have enabled work in unification based computational linguistics to approach more closely the ideal of grammar a declarative statement of linguistic fact neutral between analysis and synthesis from this perspective however the situation is still far from perfect all known method of generation impose constraint on the grammar they assume we briefly consider a number of proposal for generation outlining their consequence for the form of grammar and then report on experience arising from the addition of a generator to an existing unification environment the algorithm in question based on that of shieber et al though among the most permissive currently available excludes certain class of parsable analysis 
we consider the problem of matching model and sensory data feature when there is geometric uncertainty in the data feature for the purpose of object localization and identification the problem is to construct set of model feature and sensory data feature correspondence that are geometrically consistent within the limit of the geometric uncertainty in the data feature the major new contribution is to demonstrate a polynomial time algorithm for constructing set of geometrically consistent feature correspondence in the presence of geometric uncertainty existing system which handle uncertainty carefully and guarantee finding a correct matching have not demonstrated polynomial complexity bound 
we discus algorithm for generation within the lambek theorem proving framework efficient algorithm for generation in this framework take a semantics driven strategy this strategy can be modeled by mean of rule in the calculus that are geared to generation or by mean of an algorithm for the theorem prover the latter possibility enables processing of a bidirectional calculus therefore lambek theorem proving is a natural candidate for a uniform architecture for natural language parsing and generation 
semantic based approach to information retrieval make a query evaluation similar to an inference process based on semantic relation semantic based approach find out hidden semantic relationship between a document and a query but quantitative estimation of the correspondence between them is often empiric on the other hand probabilistic approach usually consider only statistical relationship between term it is expected that improvement may be brought by integrating these two approach this paper demonstrates using some particular probabilistic model which are strongly related to modal logic that such an integration is feasible and natural a new model is developed on the basis of an extended modal logic it ha the advantage of augmenting a semantic based approach with a probabilistic measurement and augmenting a probabilistic approach with finer semantic relation than just statistical one it is shown that this model verifies most of the condition for an absolute probability function 
this paper investigates the effect of parallelism on blackboard system scheduling a parallel blackboard system is described that allows multiple knowledge source instantiation to execute in parallel using a shared memory blackboard approach new class 
consider an infinite binary search tree in which the branch have independent random cost suppose that we must find an optimal cheapest or nearly optimal path from the root to a node at depth n karp and pearl show that a bounded lookahead backtracking algorithm a usually find a nearly optimal path in linear expected time when the cost take only the value or from this successful performance one might conclude that similar heuristic should be of more general use but we find here equal success for a simpler non backtracking bounded lookahead algorithm so the search model cannot support this conclusion if however the search tree is generated by a branching process so that there is a possibility of node having no son or branch having prohibitive cost then the non backtracking algorithm is hopeless while the backtracking algorithm still performs very well these result suggest the general guideline that backtracking becomes attractive when there is the possibility of dead end or prohibitively costly outcome 
we investigate various contextual effect on text interpretation and account for them by providing contextual constraint in a logical theory of text interpretation on the basis of the way these constraint interact with the other knowledge source we draw some general conclusion about the role of domain specific information top down and bottom up discourse information flow and the usefulness of formalisation in discourse theory 
we present a new algorithm sie for designing lumped parameter model from first principle like the ibis system of williams sle us a qualitative representation of parameter interaction to guide it search and speed the test for working design but sie s interaction set representation is considerably simpler than ibis s space of potential and existing interaction furthermore si is both complete and systematic it explores the space of possible design in an nonredundant manner 
in this work we look at mean field annealing mfa from two different perspective information theory and statistical mechanic an iterative deterministic algorithm is developed to obtain the mean field solution for disparity calculation in stereo image 
in this paper we examine how the complexity of domain independent planning with strip style operator depends on the nature of the planning operator we show how the time complexity varies depending on a wide variety of condition whether or not delete list are allowed whether or not negative precondition are allowed whether or not the predicate are restricted to be proposition i e ary whether the planning operator are given a part of the input to the planning problem or instead are fixed in advance 
different approach to visual object recognition can be divided into two general class model based v non model based scheme in this paper we establish some limitation on the class of non model based recognition scheme we show that every function that is invariant to viewing position of all object is the trivial constant function it follows that every consistent recognition scheme for recognizing all d object must in general be model based the result is extended to recognition scheme that are imperfect allowed to make mistake or restricted to certain class of object 
this paper introduces an approach to build a qualitative description of scene along a route which is used in route recognition by a mobile robot the description consists of a series of landmark autonomously selected by the robot from a panoramic view which ha been generated a a visual memory of scene along route the basic idea to bridge the quantitative panoramic view to qualitative landmark is to examine the distinctiveness of pattern in the image and select landmark from unique pattern that are remarkably by which to navigate 
the construction of the semantic representation for a natural language sentence or a piece of discourse cannot be covered by the so called compositional semantics alone in the general case the non compositional construction step of generating quantifier scoping and of anaphora resolution have to be included in order to filter out unnecessary information a soon a possible it is desirable to merge these three phase into one processing step we describe how the rule for extended compositional semantics a presented in pereira can be integrated into a parser for categorial grammar the inspection of the data flow show where concurrency can come into play 
this paper report on an indexing system supporting retrieval of past case a advice about everyday social problem it ha been implemented in the abby lovelorn advising system two point are emphasized d in ice are description of problem and their cause couched in a vocabulary centered on intentional causality and index fit a fixed format that allows reification of identity and thematic relationship a feature abby answer several of the central question that any indexing system must address and ha advantage over le restrictive system 
in this paper we present a novel explanation of the source of indefinite information in common sense reasoning indefinite information arises from report about the world expressed in term of concept that have been defined using only definite rule adopting this point of view we show that first order logic is insufficiently expressive to handle important example of common sense reasoning a a remedy we propose the use of circumscribed definite rule and we then investigate the proof theory of this more expressive framework we consider two approach first prototypical proof a special type of proof by induction which yield a sound proof theory second we describe case in which there exists a decision procedure for answering query a particularly significant result because it show that it is possible to have decidable query processing in circumscribed theory that are not equivalent to any first order theory 
this paper describes the application of explanation based learning a machine learning technique to the sri core language engine a large scale general purpose natural language analysis system the idea is to bypass normal morphological syntactic and 
an equational approach to the synthesis of functional and logic program is taken typically a target program contains equation that are only true in the standard model of the given domain rule to synthesize such program induction is necessary we propose heuristic for generalizing from a sequence of deductive consequence these are combined with rewrite based method of inductive proof to derive provably correct program 
this paper formally present a class of planning problem which allows non binary state variable and parallel execution of action the class is proven to be tractable and we provide a sound and complete polynomial time algorithm for planning within this class this result mean that we are getting closed to tackling realistic planning problem in sequential control where a restricted problem representation is often sufficient but where the size of the problem make tractability an important issue 
robot act upon and perceive the world from a particular perspective it is important to recognize this relativity to perspective if one is not to be overly demanding in specifying what they need to know in order to be able to achieve goal through action in this paper we show how a formal theory of knowledge and action proposed in lesperance can be used to formalize several kind of situation drawn from a robotics domain where indexical knowledge is involved several example treated deal with the fact that ability to act upon an object doe not require de re knowledge of the object or it absolute position knowledge of it relative position is sufficient it is shown how the fact that perception yield indexical knowledge can be captured we also point out the value of being able to relate indexical knowledge and objective knowledge within the same formalism through an example involving the use of a map for navigation finally we discus a problem raised by some higher level parametrized action and propose a solution 
we study what kind of data may ease the computational complexity of learning of horn clause theory in gold s paradigm and boolean function in pac learning paradigm we give several definition of good data basic and generative representative set and develop data driven algorithm that learn faster from good example and degenerate to learn in the limit from the worst possible example we show that horn clause theory k term dnf and general dnf boolean function are polynomially learnable from generative representative presentation 
this paper describes an inductive learning method in probabilistic domain it acquires an appropriate probabilistic model from a small amount of observation data in order to derive an appropriate probabilistic model a presumption tree with least description length is constructed description length of a presumption tree is defined a the sum of it code length and log likelihood using a constructed presumption tree the probabilistic distribution of future event can be presumed appropriately from observation of occurrence in the past this capability enables the efficiency of certain kind of performance system such a diagnostic system that deal with probabilistic problem the experimental result show that a model based diagnostic system performs efficiently by making good use of the learning mechanism in comparison with a simple probability estimation method it is shown that the proposed approach requires fewer observation to acquire an appropriate probabilistic model 
information retrieval is concerned with selecting document from a collection that will be of interest to a user with a stated information need or query research aimed at improving the performance of retrieval system that is selecting those document most likely to match the user s information need remains an area of considerable theoretical and practical importance this dissertation describes a new formal retrieval model that us probabilistic inference network to represent document 
this paper describes the application of explanation based learning a machine learning technique to the sri core language engine a large scale general purpose natural language analysis system the idea is to bypass normal morphological syntactic and 
large case database are numerous and packed with information the largest of them are potentially rich source of domain knowledge the fgp machine is a software architecture that can make this knowledge explicit and bring it to bear on classification and prediction problem the architecture provides much of the functionality of traditional expert system without requiring the system builder to preprocess the database into rule frame or any other fixed abstraction implementation of the fgp machine use similarity based reminding and the case themselves to drive the inference engine by having the system calculate and incorporate a measure of feature salience into it distance calculation the fgp architecture smoothly cope with incomplete data and is particularly well suited to weak theory domain we explain the model describe a particular implementation of it and present test result for a classification task in three application area 
in this paper we explore the use of an adaptive search technique genetic algorithm to construct a system gabel which continually learns and refines concept classification rule from it interaction with the environment the performance of the system is measured on a set of concept learning problem and compared with the performance of two existing system id r and c preliminary result support that despite minimal system bias gabil is an effective concept learner and is quite competitive with id r and c a the target concept increase in complexity 
description logic are a popular formalism for knowledge representation and reasoning this paper introduces a new operation for description logic computing the least common subsumer of a pair of description this operation computes the largest set of commonality between two description after arguing for the usefulness of this operation we analyze it by relating computation of the least common subsumer to the well understood problem of testing subsumption a close connection is shown in the restricted case of structural subsumption we also present a method for computing the least common subsumer of attribute chain equality and analyze the tractability of computing the least common subsumer of a set of description an important operation in inductive learning 
a method is presented for acquiring perceptually grounded semantics for spatial term in a simple visual domain a a part of the l miniature language acquisition project two central problem in this learning task are a ensuring that the term learned generalize well so that they can be accurately applied to new scene and b learning in the absence of explicit negative evidence solution to these two problem are presented and the result discussed 
in this paper we present an approach to the incorporation of object versioning into a distributed full text information retrieval system we propose an implementation based on partially versioned index set arguing that it space overhead and query time performance make it suitable for full text ir with it heavy dependence on inverted indexing we develop algorithm for computing both historical query and time range query and show how these algorithm can be applied to a number of problem in distributed information management such a data replication caching transactional consistency and hybrid medium repository 
a real time ai problem solver performs a task or a set of task in two phase planning and execution under real time constraint a real time ai problem solver must balance the planning and the execution phase of it operation to comply with deadline this paper provides a methodology for specification and analysis of real time ai problem and problem solver this methodology is demonstrated via domain analysis of the real time path planning problem and via algorithm analysis of dynoraii and rta we provide new result on worst case complexity of the problem we also provide experimental evaluation of dynoraii and rta for deadline compliance 
this paper describes a methodology for the design of shape starting from an initial shape a geometric reasoning kernel is used to generate and control a sequence of numerical optimization subproblems that converges to a final design a topology and associated geometry that can be significantly different from the starting shape a subproblem in the sequence is systematically formulated from a geometric abstraction of current shape and it objective function constraint and bound are dynamically derived the geometric representation of the shape is adaptive and change throughout the problem solving process to accommodate the shape change trend that occur shape evolution take place within each subproblem and between subproblems intrasubproblem evolution is responsible for geometric modification while inter subproblem evolution handle topology modification the combination of geometric reasoning and numerical optimization technique provides a robust and systematic methodology for shape synthesis that can generate new design shape without relying on heuristic or domain spedfic knowledge 
we present a method for approximating the expected number of step required by a heuristic search algorithm to reach a goal from any initial state in a problem space the method is based on a mapping from the original state space to an abstract space in which state are characterized only by a syntactic distance from the nearest goal modeling the search algorithm a a markov process in the abstract space yield a simple system of equation for the solution time for each state we derive some insight into the behavior of search algorithm by examining some closed form solution for these equation we also show that many problem space have a clearly delineated easy zone inside which problem are trivial and outside which problem are impossible the theory is borne out by experiment with both markov and non markov search algorithm our result also bear on recent experimental data suggesting that heuristic repair algorithm can solve large constraint satisfaction problem easily given a preprocessor that generates a sufficiently good initial state 
in this paper a model for combining text and fact retrieval is described a query is a set of condition where a single condition is either a text or fact condition fact condition can be interpreted a being vague thus leading to nonbinary weight for fact condition with respect to database object for text condition we use description of the occurence of term in document instead of precomputed indexing weight thus treating term similar to attribute probabilistic indexing weight for condition are computed by introducing the notion of correctness or acceptability of a condition w r t an object these indexing weight are used in retrieval for a probabilistic ranking of object based on the retrieval for a probabilistic ranking of object based on the retrieval with probabilistic indexing rpi model for which a new derivation is given here 
we demonstrate the use of a genetic algorithm ga to match a flexible template model to image evidence the advantage of the ga is that plausible interpretation can be found in a relatively small number of trial it is also possible to generate multiple distinct interpretation hypothesis the method ha been applied to the interpretation of ultrasound image of the heart and it performance ha been assessed in quantitative term 
surface curvature along extremal boundary is potentially useful information for navigation grasping and object identification task previous theory have shown that qualitative information about curvature can be obtained from a static view furthermore it is known that for orthographic projection under planar viewer motion quantitative curvature information is available from spatio temporal derivative of flow this theory is extended here to arbitrary curvilinear viewer motion and perspective projection 
we present a methodological framework enabling a detailed descriptionof the performance of hopfield like attractor neural network ann in the first two iteration using the bayesian approach wefind that performance is improved when a history based term is includedin the neuron s dynamic a further enhancement of the network s performance is achieved by judiciously choosing the censoredneurons those which become active in a given iteration on the basisof the magnitude of 
this paper present a corpus based approach for deriving heuristic to locate the antecedent of relative pronoun the technique duplicate the performance of hand coded rule and requires human intervention only during the training phase because the training instance are built on parser output rather than word cooccurrences the technique requires a small number of training example and can be used on small to medium sized corpus our initial result suggest that the approach may provide a general method for the automated acquisition of a variety of disambiguation heuristic for natural language system especially for problem that require the assimilation of syntactic and semantic knowledge 
referring expression and other object description should be maximal under the local brevity no unnecessary component and lexical preference preference rule otherwise they may lead hearer to infer unwanted conversational implicatures these preference rule can be incorporated into a polynomial time generation algorithm while some alternative formalization of conversational implicature make the generation task np hard 
this paper present fit an intelligent tutoring system it for the domain of addition of fraction it wa developed with the aim of improving on many of the shortcoming of existing tutor in the mathematical domain the paper largely describes it functioning in order to give the reader a better feel of the tutor s capability than obtained from it description an actual student tutor protocol extract is given more significantly the tutor ha also been evaluated in several way with seemingly very encouraging result so far however due to length restriction they are not reported in this paper the paper concludes by briefly highlighting some of fit s improved feature over other existing tutor in the domain a well a some of it shortcoming 
a machine vision algorithm to find the longest common subcurve of two d curve is presented the curve are represented by spline fitted through sequence of sample point extracted from dense range data the approximated d curve are transformed into d numerical string of rotation and translation invariant shape signature based on a multi resolution representation of the curvature and torsion value of the space curve the shape signature string are matched using an efficient hashing technique that find longest matching substring the result of the string matching stage are later verified by a robust least square d curve matching technique which also recovers the euclidean transformation between the curve being matched this algorithm is of average complexity o n where n is the number of the sample point on the two curve the algorithm ha application in assembly and object recognition task result of assembly experiment are included 
in the literature tree adjoining grammar tag are propagated to be adequate for natural language description analysis a well a generation in this paper we concentrate on the direction of analysis especially important for an implementation of that task is how efficiently this can be done i e how readily the word problem can be solved for tag up to now a parser with o n step in the worst case wa known where n is the length of the input string in this paper the result is improved to o n log n a a new lowest upper bound the paper demonstrates how local interpretion of tag tree allows this reduction 
we show that the terminological logic acc comprising boolean operation on concept and value restriction is a notational variant of the propositional modal logic k m to demonstrate the utility of the correspondence we give two of it immediate by product namely we axiomatize acc and give a simple proof that subsumption in acc is pspace complete replacing the original six page one furthermore we consider an extension of acc additionally containing both the identity role and the composition union transitive reflexive closure range restriction and inverse of role it turn out that this language called tsl is a notational variant of the propositional dynamic logic converse pdl using this correspondence we prove that it suffices to consider finite tsl model show that tsl subsumption is decidable and obtain an axiomatization of tsl by discovering that feature correspond to deterministic program in dynamic logic we show that adding them to tsc preserve decidability although violates it finite model property additionally we describe an algorithm for deciding the coherence of inverse free tsc concept with feature finally we prove that universal implication can be expressed within tsc 
i present algorithm for automated long term behavior prediction which can recognize when a simulation ha run long enough to produce a representative behavior sample characterize the behavior and determine whether this behavior will continue forever or eventually terminate or otherwise change it charader i have implemented these algorithm in a working program which doe longterm behavior prediction for mechanical device 
resolution reasoner when applied to set theory problem typically suffer from lack of focus mar is a program that attempt to rectify this difficulty by exploiting the definition like character of the set theory axiom a in the case of it predecessor slim it employ a tableau proof procedure based on binary resolution but mar is enhanced by an equality substitution rule and a device for introducing previously proved theorem a lemma mar s performance compare favorably with that of other existing automated reasoner for this domain mar find proof for many basic fact about function construed a set of ordered pair mar is being used to attack the homomorphism test problem the theorem that the composition of two group homomorphism is a group homomorphism 
we present a heuristic based approach to deep space mission scheduling which is modeled on the approach used by expert human scheduler in producing schedule for planetary encounter new chronological evaluation technique are used to focus the search by using information gained during the scheduling process to locate classify and resolve region of conflict our approach is based on the assumption that during the construction of a schedule there exist several disjunct temporal region where the demand for one resource type or a single temporal constraint dominates bottleneck region if the scheduler can identify these region and classify them based on their dominant constraint then the scheduler can select the scheduling heuristic 
this paper present a parsing method for identifying word in mandarin chinese sentence the identification system is composed of a tomita s parser augmented with test originally a part of the english chinese machine translation system ccl ecmt together with the associated augmented context free grammar for word composition the simple augmented grammar with the score function effectively capture the intuitive idea of longest possible composition of chinese word in sentence and at the same time take into consideration the frequency count of word the identification rate of this system for the corpus taken from book and a newspaper is this identification system is simple but the identification rate is relatively high the minimum element for word composition parsing is down to character a opposed to sentence parsing down to chinese word it ha the potential of incorporating phrase structure and semantic checking into the system in this way word identification syntactic and even semantic analysis can be organized into a single phase the result of testing the word identification on corpus taken from book and a chinese newspaper are also presented 
since knowledge is usually incomplete agent need to introspect on what they know and do not know the best known model of introspective reasoning suffer from intractability or even undecidability if the underlying language is first order to better suit the fact that agent have limited resource we recently proposed a model of decidable introspective reasoning in first order knowledge base kb however this model is deficient in that it doe not allow for quantifying in which is needed to distinguish between knowing that and knowing who in this paper we extend our earlier work by adding quantifying in and equality to a model of limited belief that integrates idea from possible world semantics and relevance logic 
although the detection of invariant structure in a given set of input patternsis vital to many recognition task connectionist learning rule tend to focus ondirections of high variance principal component the prediction paradigm isoften used to reconcile this dichotomy here we suggest a more direct approach toinvariant learning based on an anti hebbian learning rule an unsupervised twolayernetwork implementing this method in a competitive setting learns to extractcoherent depth 
much of the theoretical research on nonmonotonic inheritance ha concentrated on formalism involving only is a link between primitive node however it is hard to imagine a useful network representation of commonsense or expert knowledge that would not involve node representing negative conjunctive or disjunctive property certain node of this kind were included in some of the earliest formalism for defeasible inheritance but were omitted in later work either to secure tractability or to simplify the task of theoretical analysis the purpose of the present paper is to extend the theoretical analysis of defeasible inheritance to network incorporating these expressive enhancement 
we describe work on the visualization of bibliographic data and to aid in this task the application of numerical technique for multidimensional scaling many area of scientific research involve complex multivariate data one example of this is information retrieval document comparison may be done using a large number of variable such condition do not favour the more well known method of visualization and graphical analysis a it is rarely feasible to map each variable onto one aspect of even a three dimensional coloured and textured space bead is a prototype system for the graphically based exploration of information in this system article in a bibliography are represented by particle in space by using physically based modelling technique to take advantage of fast method for the approximation of potential field we represent the relationship between article by their relative spatial position inter particle force tend to make similar article move closer to one another and dissimilar one move apart the result is a d scene which can be used to visualize pattern in the high d information space 
recent research in real time artificial intelligence ha focussed upon the design of situated agent and in particular how to achieve effective and robust behaviour with limited computational resource a range of architecture and design principle ha been proposed to solve this problem this ha led to the development of simulated world that can serve a testbeds in which the effectiveness of different agent can be evaluated we report here an experimental program that aimed to investigate how commitment to goal contributes to effective behaviour and to compare the property of different strategy for reacting to change our result demonstrate the feasibility of developing system for empirical measurement of agent performance that are stable sensitive and capable of revealing the effect of high level agent characteristic such a commitment 
this paper deal with edge detection in d image such a scanner magnetic resonance nmr or spatio temporal data we propose an unified formalism for d edge detection using optimal recursive and separable filter recently introduced for d edge detection then we obtain some efficient d edge detection algorithm having a low computational cost we also show that d edge tracking closing enables to extract many edge not provided by the filtering stage without introducing noisy edge experimental result obtained on nmr image are shown 
this paper describes a layered control system for a binocular stereo head it begin with a discussion of the principle of layered control and then describes the mechanical device for a binocular camera head a device level controller is presented which permit an active vision system to command the position of the gaze point the final section describes experiment with reflexive control of focus iris and vergence 
this paper investigates the effect of parallelism on blackboard system scheduling a parallel blackboard system is described that allows multiple knowledge source instantiation to execute in parallel using a shared memory blackboard approach new class 
we propose a representation of concurrent action rather than invent a new formalism we model them within the standard situation calculus by introducing the notion of global action and primitive action whose relationship is analogous to that between situation and fluents the result is a framework in which situation and action play quite symmetric role the rich structure of action give rise to a new problem which due to this symmetry between action and situation is analogous to the traditional frame problem in lin and shoham we provided a solution to the frame problem based on a formal adequacy criterion called epistemological completeness here we show how to solve the new problem based on the same adequacy criterion 
an algorithm is presented that integrates segmentation map using both region and edge segmentation map a input the result of integration is a region map in which each region is large and compact the operation is efficient and independent of image source a well a segmentation technique the proposed algorithm allows multiple input map and applies user selected weight on various information source the scope of integration is parametrically controlled for the desired spatial resolution a maximum likelihood estimator provides initial solution of edge position and strength from multiple input an iterative procedure is then used to smooth the resultant edge pattern the edge map is converted to a region map using closed edge contour if desired finally region are merged to ensure that every region ha the required property experimental result are demonstrated using various segmentation technique and real data from laser radar and thermal sensor 
strategic planner for robot designed to operate in a dynamic environment must be able to decide i how often a sensory request should be granted and ii how to recover from a detected error this paper derives closed form formula for the appropriate frequency of sensor integration a a function of parameter of the equipment the domain and the type of error from which the system wish to recover 
this paper describes how a competitive tree learning algorithm can be derived from first principle the algorithm approximates the bayesian decision theoretic solution to the learning task comparative experiment with the algorithm and the several mature ai and statistical family of tree learning algorithm currently in use show the derived bayesian algorithm is consistently a good or better although sometimes at computational cost using the same strategy we can design algorithm for many other supervised and model learning task given just a probabilistic representation for the kind of knowledge to be learned a an illustration a second learning algorithm is derived for learning bayesian network from data implication to incremental learning and the use of multiple model are also discussed 
this paper provides a quantitative analysis of domain structure and it effect on the complexity of diagnostic problem solving it introduces a hypothesis about the modular structure of domain and proposes a measured called explanatory power the distribution of explanatory power reveals the inherent structure of domain we conjecture that such structure might facilitate problem solving even when the problem solver doe not exploit it explicitly to test this hypothesis we create a domain without structure by randomizing the distribution of explanatory power we use the structured and randomized knowledge base to study the effect of domain structure on two diagnostic algorithm candidate generation and symptom clustering the result indicate that inherent domain structure even when not encoded explicitly can facilitate problem solving such facilitation occurs for both the candidate generation and symptom clustering algorithm moreover domain structure appears to benefit symptom clustering more than candidate generation suggesting that the efficiency of symptom clustering derives in part from exploiting domain structure 
the goal of this paper is to present a theorem prover where the underlying code ha been written to behave a the procedural metalevel of the object logic we have then defined a logical declarative metatheory mt which can be put in a one to one relation with the code and automatically generated from it mt is proved correct and complete in the sense that for any object level deduction the wff representing it is a theorem of mt and viceversa such theorem can be translated back in the underlying code this open up the possibility of deriving control strategy automatically by metatheoretic theorem proving of mapping them into the code and thus of extending and modifying the system itself this seems a first step towards really self reflective system it system able to reason deductively about and modify their underlying computation mechanism we show that the usual logical reflection rule so called reflection up and down are derived inference rule of the system 
best first model merging is a general technique for dynamically choosing the structure of a neural or related architecture while avoiding overfitting it is applicable to both learning and recognition task and often generalizes significantly better than fixed structure w e demonstrate the approach applied to the task of choosing radial basis function for function learning choosing local af fine model for curve and constraint surface modelling and choosing the structure of a balltree or bumptree to maximize efficiency of access 
few text longer than a paragraph are written without appropriate formatting to ensure readability automated text generation program must not only plan and generate their text but be able to format them a well we describe how work on the automated planning of multisentence text and on the display of information in a multimedia system led to the insight that text formatting device such a footnote italicized region enumeration etc can be planned automatically by a text structure planning process this is achieved by recognizing that each formatting device fulfills a specific communicative function in a text and that such function can be defined in term of the text structure relation used a plan in a text planning system an example is presented in which a text is planned from a semantic representation to a final form that includes english sentence and latex formatting command intermingled a appropriate 
stereo vision and motion analysis have been frequently used to infer scene structure and to control the movement of a mobile vehicle or a robot arm unfortunately when considered separately these method present intrinsic difficulty and a simple fusion of the respective result ha been proved to be insufficient in practice 
we analyse the effect of analog noise on the synaptic arithmeticduring multilayer perceptron training by expanding the cost functionto include noise mediated penalty term prediction are madein the light of these calculation which suggest that fault tolerance generalisation ability and learning trajectory should be improvedby such noise injection extensive simulation experiment on twodistinct classification problem substantiate the claim the resultsappear to be perfectly 
in this paper we describe an automated method of classifying research project description a human expert classifies a sample set of project into a set of disjoint and pre defined class and then the computer learns from this sample how to classify new project into these class both textual and non textual information associated with the project are used in the learning and classification phase textual information is processed by two method of analysis a natural language analysis followed by a statistical analysis non textual information is processed by a symbolic learning technique we present the result of some experiment done on real data two different classification of our research project 
subtle difference in the method of constructing argument in inheritance system can result in profound difference in both the conclusion reached and the efficiency of inference this paper focus on issue surrounding the defeat of argument in nonmonotonic inheritance looking primarily at skeptical reasoner we analyze several type of defeat that may be encountered especially the defeat of defeaters finally we raise some question specific to network that mix strict and defeasible link 
in this paper we shall discus how to treat the automatic generation of assembly task specification a a constraint satisfaction problem csp over finite and infinite domain conceptually it is straightforward to formulate assembly planning in term of csp however the choice of constraint representation and of the order in which the constraint are applied is nontrivial if a computationally tractable system design is to be achieved this work investigates a subtle interaction between a pair of interleaving constraint namely the kinematic and the spatial occupancy constraint while finding one consistent solution to a general csp is np complete our work show how to reduce the combinatorics in problem arising in assembly using the symmetry of assembly component group theory being the standard mathematical theory of symmetry is used extensively in this work since both robot and assembly component are threedimensional rigid body whose feature have certain symmetry this form part of our high level robot assembly task planner in which geometric solid modelling group theory and csp are combined into one computationally effective framework 
the majority of potential vision application such a robotic guidance and visual surveillance involve the real time analysis and description of object behaviour from image sequence in the view project we are developing advanced visual surveillance capability for situation where the scene structure object and much of the expected behaviour is known this combine competence from image understanding knowledge based processing and real time technology in this paper we discus the spatio temporal reasoning which is of central importance to the system allowing behavioral feedback in particular we will elaborate the analysis of occlusion behaviour where we need knowledge of the camera geometry to invoke the occlusion region monitoring of vehicle plus knowledge of the scene geometry to maintain high level model of possible trajectory for the occluded vehicle and to recognise the re emerging vehicle s 
an algorithm is proposed to determine antecedent for vp ellipsis the algorithm eliminates impossible antecedent and then imposes a preference ordering on possible antecedent the algorithm performs with accuracy on a set of example of vp ellipsis collected from the brown corpus the problem of determining antecedent for vp ellipsis ha received little attention in the literature and it is shown that the current proposal is a significant improvement over alternative approach 
we describe a method of automatically abducing qualitative model from description of behavior we generate from either quantitative or qualitative data model in the form of qualitative differential equation suitable for use by qsim constraint are generated and filtered both by comparison with the input behavior and by dimensional analysis if the user provides complete information on the input behavior and the dimension of the input variable the resulting model is unique maximally constrained and guaranteed to reproduce the input behavior if the user provides incomplete information our method will still generate a model which reproduces the input behavior but the model may no longer be unique incompleteness can take several form missing dimension value of variable or entire variable 
in this paper we present an average case analysisof the bayesian classifier a simple induction algorithmthat fare remarkably well on many learningtasks our analysis assumes a monotone conjunctivetarget concept and independent noise freeboolean attribute we calculate the probabilitythat the algorithm will induce an arbitrary pair ofconcept description and then use this to computethe probability of correct classification over the instancespace the analysis take into account 
most geometric model are quantitative making it difficult to abstract the underlying spatial information needed for task such a planning learning or vision furthermore the precision used in a typical quantitative system often exceeds the actual accuracy of the data in this work we describe a systematic representation that build spatial map based on local qualitative relation between object it derives relation that are more functionally relevant i e those that involve accidental alignment or can be described based on such alignment in one dimension interval logic allen provides a mechanism for representing these type of relation in this work we propose a formalism that enables u to perform alignment based reasoning in two and higher dimension with object at angle the principal advantage of this representation is that a it is free of subjective bias and b it is complete in the qualitative sense of distinguishing all overlap tangency nocontact geometry in addition the model is capable of handling uncertainty in the initial system e g the fuse box is somewhere behind the compressor by constructing bounded inference from disjunctive input data two kind of uncertainty can be handled those arising from deliberate imprecision in the interest of compactness down the road from or those caused by an inadequacy of data sensor spatial description or map 
one of the key tool in physic based vision ha beencolor histogram analysis but to date histogram have onlybeen used for pixel grouping color analysis and materialtype labeling in this paper we present a new quantitativemodel of histogram that yield a more complete descrip tion of scene property 
the multi state time delay neural network m tdnn integratesa nonlinear time alignment procedure dtw and the highaccuracyphoneme spotting capability of a tdnn into a connectionistspeech recognition system with word level classification anderror backpropagation we present an m tdnn for recognizingcontinuously spelled letter a task characterized by a small buthighly confusable vocabulary our m tdnn achieves word accuracy on speaker dependent independent task 
this paper discus how a two level knowledge representation model for machine translation integrates aspectual information with lexical semantic information by mean of parameterization the integration of aspect with lexical semantics is especially critical in machine translation because of the lexical selection and aspectual realization process that operate during the production of the target language sentence there are often a large number of lexical and aspectual possibility to choose from in the production of a sentence from a lexical semantic representation aspectual information from the source language sentence constrains the choice of target language term in turn the target language term limit the possibility for generation of aspect thus there is a two way communication channel between the two process this paper will show that the selection realization process may be parameterized so that they operate uniformly across more than one language and it will describe how the parameter based approach is currently being used a the basis for extraction of aspectual information from corpus 
this paper investigates the effect of parallelism on blackboard system scheduling a parallel blackboard system is described that allows multiple knowledge source instantiation to execute in parallel using a shared memory blackboard approach new class 
abstract the probabilistic neural network pnn algorithm represents the likelihood function of a given class a the sum of identical isotropic gaussians in practice pnn is often an excellent pattern classier outperforming other classiers including backpropagation however it is not robust with respect to ane transformation of feature space and this can lead to poor performance on certain data we have derived an extension of pnn called weighted pnn wpnn which compensates for this aw by allowing anisotropic gaussians i e gaussians whose covariance is not a multiple of the identity matrix the covariance is optimized using a genetic algorithm some interesting feature of which are it redundant logarithmic encoding and large population size experimental result validate our claim 
this paper describes recent work on the unisys atis spoken language system and report benchmark result on natural language spoken language and speech recognition we describe enhancement to the system s semantic processing for handling non transparent argument structure and enhancement to the system s pragmatic processing of material in art swers displayed to the user we found that the system s score on the natural language benchmark test decreased from o to without these enhancement we also report result for three spoken language system unisys natural language coupled with mit summit speech recognition unisys natural language coupled wish mit lincoln lab speech recognition and unisys natural language coupled with bbn speech recognition speech recognition result are reported on the result of the unisys natural language selecting a candidate from the mitsummit n best n 
this paper describes the application of explanation based learning a machine learning technique to the sri core language engine a large scale general purpose natural language analysis system the idea is to bypass normal morphological syntactic and 
in visual processing the ability to deal with missing and noisy information is crucial occlusion and unreliable feature detector often lead to situation where little or no direct information about feature is available however the available information is usually sufficient to highly constrain the output we discus bayesian technique for extracting class probability given partial data the optimal solution involves integrating over the missing dimension weighted by the local probability density the framework extends naturally to the case of noisy information we show how to obtain closed form approximation to the bayesian solution using gaussian basis function network simulation on a complex task d hand gesture recognition validate the theory when both integration and weighting by input density are used performance decrease gracefully with the number of missing or noisy feature performance is substantially degraded if either step is omitted 
computer program that access significant amount of text usually include code that manipulates the textual object that comprise it such program include electronic mail reader typesetter and in particular full text information retrieval system such code is often unsatisfying in that access to textual object is either efficient or flexible but not both a programming language like awk or perl provides very general facility for describing textual object but at the cost of rescanning the text for every textual object at the other extreme full text information retrieval system usually offer access to a very limited number of kind of textual object but this access is very efficient the system described in this paper is a programming tool for managing textual object it provides a great deal of flexibility giving access to very complex document structure with a large number of constituent kind of textual object further it provides access to these object very efficiently both in term of time and auxiliary space by being very careful to access secondary storage only when absolutely necessary 
we present a theory of plan modification applicable to hierarchical nonlinear planning our theory utilizes the validation structure of the stored plan to yield a flexible and conservative plan modification framework the validation structure which constitutes a hierarchical explanation of correctness of the plan with respect to the planner s own knowledge of the domain is annotated on the plan a a by product of initial planning plan modification is characterized a a process of removing inconsistency in the validation structure of a plan when it is being reused in a new changed planning situation the repair of these inconsistency involves removing unnecessary part of the plan and adding new non primitive task to the plan to establish missing or failing validation the resultant partially reduced plan with a consistent validation structure is sent to the planner for complete reduction we discus the development of this theory in friar system and characterize it completeness coverage efficiency and limitation 
a connectionist unification algorithm is presented it utilizes the fact that the most general unifier of two term corresponds to a finest valid equivalence relation defined on a occurrence label representation of the unification problem the algorithm exploit the maximal parallelism inherent in the computation of such a finest valid equivalence relation while using only computational feature of connectionism it can easily be restricted to solve special form of the unification problem such a the word problem the matching problem or the unification problem over infinite tree 
in this paper we describe a statistical technique for aligning sentence with their translation in two parallel corpus in addition to certain anchor point that are available in our data the only information about the sentence that we use for calculating alignment is the number of token that they contain because we make no use of the lexical detail of the sentence the alignment computation is fast and therefore practical for application to very large collection of text we have used this technique to align several million sentence in the english french hansard corpus and have achieved an accuracy in excess of in a random selected set of sentence pair that we checked by hand we show that even without the benefit of anchor point the correlation between the length of aligned sentence is strong enough that we should expect to achieve an accuracy of between and thus the technique may be applicable to a wider variety of text than we have yet tried 
the problem of finding a set of assumption which explain a given proposition is in general np hard even when the background theory is an acyclic horn theory in this paper it is shown that when the background theory is acyclic horn and it pseudo completion 
in this paper we represent singular definite noun phrase a function in logical form this representation is designed to model the behavior of both anaphoric and non anaphoric distributive definites it is also designed to obey the computational constraint suggested in harper har our initial representation of a definite place an upper bound on it behavior given it structure and location in a sentence later when ambiguity is resolved the precise behavior of the definite is pinpointed 
research in distributed ai ha led to computational technique for providing ai system with rudimentary social skill this paper give a brief survey of distributed ai describing the work that strives for social skill that a person might acquire in kindergarten and highlighting important unresolved problem facing the field 
we define a set of deterministic bottom up left to right parser which analyze a subset of tree adjoining language the lr parsing strategy for context free grammar is extended to tree adjoining grammar tag we use a machine called bottom up embedded push down automation bepda that recognizes in a bottom up fashion the set of tree adjoining language and exactly this set each parser consists of a finite state control that drive the move of a bottom up embedded pushdown automaton the parser handle deterministically some context sensitive tree adjoining language in this paper we informally describe the bepda then given a parsing table we explain the lr parsing algorithm we then show how to construct an lr parsing table no lookahead an example of a context sensitive language recognized deterministically is given then we explain informally the construction of slr parsing table for bepda we conclude with a discussion of our parsing method and current work 
the feasibility of derivational analogy a a mechanism for improving problem solving behavior ha been shown for a variety of problem domain by several researcher however most of the implemented system have been empirically evaluated in the restricted context of an already supplied base analog or on a few isolated example in this paper we address the utility of a derivational analogy based approach when the cost of retrie ving analog from a sizable case library and the cost of retrieving inappropriate analog is factored in 
a novel method called height from motion hfm is developed to estimate the motion and structure under planar motion by using this method both translational and rotational motion three degree of freedom can be treated in a unified manner based on the hfm method the correspondence problem becomes easy to deal with especially under translational motion experiment of real scene image sequence and the error analysis theoretically and experimentally have shown the efficiency and robustness of the method 
a large number of statistical measure have been postulated for the description and discrimination of texture while most are useful in some situation none are totally effective in all of them an alternative approach is to learn which measure are best for particular circumstance in this paper the distributed learning system of constraint motion is used to learn relevant texture descriptor from a set of well known first and second order grey level statistic using this system a network of distributed unit partition itself into set of unit that detect one and only one of the given class of texture each of these set is further partitioned into individual unit that detect natural subtypes of these texture class one which do not necessarily produce the same type of statistic at the local level together these unit form a network capable of determining the texture classification of an image 
we present a dynamic algorithm for map calculation the algorithm is based upon santos s technique santos b of transforming minimal cost proof problem into linear programming problem the algorithm is dynamic in the sense that it is able to use the result from an earlier near by problem to lessen it search time result are presented which clearly suggest that this is a powerful technique for dynamic abduction problem 
this paper present a formal account of the temporal interpretation of text the distinct natural interpretation of text with similar syntax are explained in term of defeasible rule characterising causal law and gricean style pragmatic maxim intuitively compelling pattern of defeasible entailment that are supported by the logic in which the theory is expressed are shown to underly temporal interpretation 
visual processing is very important for robot navigation it ha been demonstrated that manycomplex operation which deserve an intelligent behaviour can be performed relying only onreflexes to visual stimulus in this framework the detection of corridor of free space along the robot trajectory is certainly avery important capability to safely navigate stereo vision and motion parallax can be used a cuesto infer scene structure and determine free space area in this paper we propose a 
we describe a system called tileworld which consists of a simulated robot agent and a simulated environment which is both dynamic and unpredictable both the agent and the environment are highly parameterized enabling one to control certain characteristic of each we can thus experimentally investigate the behavior of various meta level reasoning strategy by tuning the parameter of the agent and can ass the success of alternative strategy in different environment by tuning the environmental parameter our hypothesis is that the appropriateness of a particular meta level reasoning strategy will depend in large part upon the characteristic of the environment in which the agent incorporating that strategy is situated we describe our initial experiment using tileworld in which we have been evaluating a version of the meta level reasoning strategy proposed in earlier work by one of the author bratman et al 
fundamental instability have been observed in the performance of the majority of thealgorithms for three dimensional motion estimation from two view many geometric andintuitive interpretation have been offered to explain the error sensitivity of the estimatedparameters in this paper we address the importance of the form of the error norm to beminimized with respect to the motion parameter we describe the error norm used by theexisting algorithm in a unifying notation and give a 
the form of rule in combinatory categorial grammar ccg is constrained by three principle called adjacency consistency and inheritance these principle have been claimed elsewhere to constrain the combinatory rule of composition and type raising in such a way a to make certain linguistic universal concerning word order under coordination follow immediately the present paper show that the three principle have a natural expression in a unification based interpretation of ccg in which directional information is an attribute of the argument of function grounded in string position the universal can thereby be derived a consequence of elementary assumption some desirable result for grammar and parser follow concerning type raising rule 
this article show a way of using a stereo vision system a a logical sensor to perform mobile robot navigation task such a obstacle avoidance we describe our system from which the implementation of a task described by an automaton can be done very easily then we show an example of a navigation task 
existing metric for the learning performance of feed forward neural network do not provide a satisfactory basis for comparison because the choice of the training epoch limit can determine the result of the comparison i propose new metric which have the desirable property of being independent of the training epoch limit the efficiency measure the yield of correct network in proportion to the training effort expended the optimal epoch limit provides the greatest efficiency the learning performance is modelled statistically and asymptotic performance is estimated implementation detail may be found in hamey 
the ability to generalize remains one of the central issue of concept learning a general generalization algorithm the candidate elimination algorithmexists but practical application of this algorithm are still limited due to it low convergence the issue ha shifted to the design of a useful bias limiting the size of the version space this paper proposes a new kind of bias called empirical bias and a new general algorithm ice for generalization in presence of bias this proposition is founded on the concept of focus set which provides a very flexible way to express expectation or constraint on the space of generalization 
phrase structure grammar are an effective representation for important syntactic and semantic aspect of natural language but are computationally too demanding for use a language model in real time speech recognition an algorithm is described that computes finite state approximation for context free grammar and equivalent augmented phrase structure grammar formalism the approximation is exact for certain context free grammar generating regular language including all left linear and right linear context free grammar the algorithm ha been used to construct finite state language model for limited domain speech recognition task 
the problem of computing maximally specific generalization mscg s of relational description can be modelled a tree search we describe several transformation and pruning method for reducing the complexity of the problem based on this analysis we have implemented a search program x search for finding the mscg s experiment compare the separate and combined effect of pruning method on search efficiency with effective pruning method full width search appears feasible for moderately sized relational description 
this paper describes a natural language processing system developed for the semantic network array processor snap the goal of our work is to develop a scalable and high performance natural language processing system which utilizes the high degree of parallelism provided by the snap machine we have implemented an experimental machine translation system a a central part of a real time speech to speech dialogue translation system it is a snap version of the dmdiaiog speech to speech translation system memory based natural language processing and syntactic constraint network model ha been incorporated using parallel marker passing which is directly supported from hardware level experimental result demonstrate that the parsing of a sentence is done in the order of millisecond 
this paper describes the application of explanation based learning a machine learning technique to the sri core language engine a large scale general purpose natural language analysis system the idea is to bypass normal morphological syntactic and 
this paper discus the logic lkm which extends circumscription into an epistemic domain this extension will allow u to define circumscription of predicate that appear within the context of a modal operator in fact lkm can be seen a a method of extending any first order nonmonotonic logic whose semantic definition is based on a partial order among model into a new nonmonotonic logic defined for a modal language whose modal operator k follows an undedying s or weak s semantics one interesting use of this nonmonotonic logic is to model nonmonotonic aspect of the communication between agent 
this paper describes a method for camera calibration the system consists of a static camera which take a sequence of image of a calibration plane rotating around a fixed axis there is no requirement for any exact positioning of the camera or calibration plane 
we analyze the computational complexity of phonological model a they have developed over the past twenty year the major result are that generation and recognition are undecidable for segmental model and that recognition is np hard for that portion of segmental phonology subsumed by modern autosegmental model formal restriction are evaluated 
three method for improving the performance of gaussian radial basisfunction rbf network were tested on the nettalk task in rbf anew example is classified by computing it euclidean distance to a set ofcenters chosen by unsupervised method the application of supervisedlearning to learn a non euclidean distance metric wa found to reduce theerror rate of rbf network while supervised learning of each center s varianceresulted in inferior performance the best improvement in 
the purpose of this paper are threefold the first is to provide a crisp formalization of abstrips style abstraction since the lack of such formalization ha made it difficult to ascertain the us and value of this type of abstraction in previous research second we define the refinement relationship between solution at different level of the abstraction hierarchy such definition are crucial to developing efficient search strategy with this type of hierarchical planning and third we provide a restriction on the abstraction mapping that provides a criterion for generating useful abstraction 
in this paper we show how to use common knowledge computationally in solving problem involving cooperation of multiple agent when common knowledge is available we will explain why a procedural approach to common knowledge is better suited to solving multiple agent problem than a static one we show even if one can never prove that common knowledge ha been attained halpern and moses that assuming it ha been attained is often safe and efficacious the ability to detect fairly reliably when certain condition are not met suffices a a guideline for when to assume something is common knowledge in principle the problem of when one ha individual knowledge is about a difficult we use the situation oriented programming language prosit by combining reasoning about situation and in situation prosit make possible an especially intuitive and simple solution of hypothetical reasoning problem involving common knowledge nakashima and tutiya 
the kbann knowledge based artificial neural network approach us neural network to refine knowledge that can be written in the form of simple propositional rule we extend this idea further by presenting the manncon multivariable artificial neural network control algorithm by which the mathematical equation governing a pid proportional integral derivative controller determine the topology and initial weight of a network which is further trained using backpropagation we apply this method to the task of controlling the outflow and temperature of a water tank producing statistically significant gain in accuracy over both a standard neural network approach and a nonlearning pid controller furthermore using the pid knowledge to initialize the weight of the network produce statistically le variation in test set accuracy when compared to network initialized with small random number 
many classification algorithm require that the training data contain only discrete attribute to use such an algorithm when there are numeric attribute all numeric value must first be converted into discrete value a process called discretization this paper describes chimerge a general robust algorithm that us the statistic to discretize quantize numeric attribute 
verification method and tool developed so far have assumed a very simple model of rule based expert system rbes current rbes often do not comply this model and require more sophisticated verification technique a rbes model including uncertainty and control ha been used to analyze four verification issue inconsistency redundancy circularity and useless rb object identifying a number of new verification problem the concept of label and environment dekleer have been extended to incorporate uncertainty and control information obtaining the construct extended label and extended environment they have been used to express and solve these new verification problem 
we study concept language also called terminological language a mean for both defining a knowledge base and expressing query in particular we investigate on the possibility of using two different concept language one for asserting fact about individual object and the other for querying a set of such assertion contrary to many negative result on the complexity of terminological reasoning our work show that provided that a limited language is used for the assertion it is possible to employ a richer query language while keeping the reasoning process tractable we also show that on the other hand there are construct that make query answering inherently intractable 
one obstacle to wider use of inductive learning algorithm in problem solving system is the sensitivity of the algorithm to the way in which example of the concept are represented human normally decide how the example will be represented so success in incorporating inductive learning algorithm varies from person to person constructive induction reduces but doe not eliminate this sensitivity an ideal solution would eliminate the need for any human intervention in determining how a problem solving system and an inductive learning algorithm are integrated this paper show how a problem solver can use it domain knowledge to automatically create a representation of example that is adequate for learning search control knowledge the resulting representation describes the example in term of how and how well they satisfy the problemsolver s goal experimental evidence from two domain is presented to support the claim that this approach is generally useful 
in this paper we present algorithm for the interpretation and generation of a kind of particularized conversational implicature occurring in certain indirect reply our algorithm make use of discourse expectation discourse plan and discourse relation the algorithm calculate implicatures of discourse unit of one or more sentence our approach ha several advantage first by taking discourse relation into account it can capture a variety of implicatures not handled before second by treating implicatures of discourse unit which may consist of more than one sentence it avoids the limitation of a sentence at a time approach third by making use of property of discourse which have been used in model of other discourse phenomenon our approach can be integrated with those model also our model permit the same information to be used both in interpretation and generation 
the projection of depth or orientation discontinuity in a physical scene result in image intensity edge which are not ideal step edge but are more typically a combination of step peak and roof profile most edge detection scheme ignore the composite nature of these edge resulting in systematic error in detection and localization the problem of detecting and localizing these edge is addressed along with the problem of false response in smoothly shaded region with constant gradient of the image brightness a class of nonlinear filter known a quadratic filter is appropriate for this task while linear filter are not performance criterion are derived for characterizing the snr localization and multiple response of these filter in a manner analogous to canny s criterion for linear filter a two dimensional version of the approach is developed which ha the property of being able to represent multiple edge at the same location and determine the orientation of each to any desired precision this permit junction to be localized without rounding experimental result are presented 
despite the fact that complex visual scene contain multiple overlapping object peopleperform object recognition with ease and accuracy one operation that facilitates recognitionis an early segmentation process in which feature of object are grouped and labeled accordingto which object they belong current computational system that perform this operation arebased on predefined grouping heuristic we describe a system called magic that learns howto group feature based on a set 
text processing for complex domain such a terrorism is complicated by the difficulty of being able to reliably distinguish relevant and irrelevant text we have discovered a simple and effective filter the relevancy signature algorithm and demonstrated it performance in the domain of terrorist event description the relevancy signature algorithm is based on the natural language processing technique of selective concept extraction and relies on text representation that reflect predictable pattern of linguistic context this paper describes text classification experiment conducted in the domain of terrorism using the muc text corpus a customized dictionary of about word provides the lexical knowledge base needed to discriminate relevant text and the circus sentence analyzer generates relevancy signature a an effortless side effect of it normal sentence analysis although we suspect that the training base available to u from the muc corpus may not be large enough to provide optimal training we were nevertheless able to attain relevancy discrimination for significant level of recall ranging from to with precision in half of our test run 
causal theory are default theory which explicitly accommodate a distinction between explained and unexplained proposition this is accomplished by mean of an operator c in the language for which proposition are assumed explained when literal of the form c hold the behavior of causal theory is determined by a preference relation on model based on the minimization of unexplained abnormality we show that causal network general logic program and theory for reasoning about change can be all naturally expressed a causal theory we also develop a proof theory for causal theory and discus how they relate to autoepistemic theory prioritized circumscription and pearl s c e calculus 
the key concept of autoepistemic logic introduced by moore is a stable expansion of a set of premise i e a set of belief adopted by an agent with perfect introspection capability on the basis of the premise moore s formalization of a stable expansion however is nonconstructive and produce set of belief which are quite weakly grounded in the premise a new more constructive definition of the set of belief of the agent is proposed it is based on classical logic and enumeration of formula considering only a certain subclass of enumeration l hierarchic enumeration an attractive class of expansion is captured to characterize the set of belief of a fully introspective agent these l hierarchic expansion are stable set minimal very tightly grounded in the premise and independent of the syntactic representation of premise furthermore reiter s default logic is shown to be a special case of autoepistemic logic based on l hierarchic expansion 
knowledge processing is very demanding on computer architecture knowledge processing generates subcomputation path at an exponential rate it is memory intensive and ha high communication requirement marker passing architecture are good candidate to solve knowledge processing problem in this paper we justify the design decision made for the semantic network array processor snap important aspect of snap are the instruction set marker relation propagation rule interconnection network and granularity these feature are compared to those in netl and the connection machine 
the instantaneous image motion field due to a camera moving through a static environment encodes information about ego motion and environmental layout for pure translational motion the motion field ha a unique point termed focus of expansion contraction where the image velocity vanishes we reveal the fact that for an arbitrary d motion the zero velocity point whose number can be large have the regularity of being approximately cocircular more generally all the image point with the same velocity u are located approximately on a circle termed the isovelocity circle ivc determined solely by u and the ego motion except for the pathological case in which the circle degenerate into a straight line while ivcs can be recovered from or more pair of iso velocity point in the motion field using a linear method estimating ego motion reduces to solving system of linear equation constraining iso velocity point pair yang 
a formalism will be presented in this paper which make it possible to realise the idea of assigning only one scope ambiguous representation to a sentence that is ambiguous with regard to quantifier scope the scope determination result in extending this representation with additional context and world knowledge condition if there is no scope determining information the formalism can work further with this scope ambiguous representation thus scope information doe not have to be completely determined 
korf present a method for learning macro operator and show that the method is applicable to serially decomposable problem in this paper i analyze the computational complexity of serial decomposability assuming that operator take polynomial time it is np complete to determine if an operator or set of operator is not serially decomposable whether or not an ordering of state variable is given in addition to serial decomposability of operator a serially decomposable problem requires that the set of solvable state is closed under the operator it is pspace complete to determine if a given finite state variable problem is serially decomposable in fact every solvable instance of a pspace problem can be converted to a serially decomposable problem furthermore given a bound on the size of the input every problem in pspace can be transformed to a problem that is nearly serially decomposable i e the problem is serially decomposable except for closure of solvable state or a unique goal state 
reasoning about causality is an interesting application area of formal nonmonotonic theory here we focus our attention on a certain aspect of causal reasoning namely causal asymmetry in order to provide a qualitative account of causal asymmetry we present a justification based approach that us circumscription to obtain the minimality of cause we define the notion of causal and evidential support in term of a justification change with respect to a circumscriptive theory and show how the definition provides desirable interaction between causal and evidential support 
this work investigates the effect of parallelism on blackboard system scheduling a parallel blackboard system is described that allows multiple knowledge source instantiation to execute in parallel using a shared memory blackboard approach new class of control knowledge are defined that use information about the relationship between system goal to schedule task this control knowledge is implemented using bb style control heuristic the usefulness of the heuristic is demonstrated by comparing the effectiveness of problem solving with and without the heuristic problem solving with the new control knowledge resulted in increased processor utilization and decreased total execution time 
a method is developed for the computation of depth map modulo scale from one single image of a polyhedral scene only affine shape property of the scene and image are used hence no metrical information result from simple experiment show good performance both what concern exactness and robustness it is also shown how the underlying theory may be used to single out and characterise certain singular situation that may occur in machine interpretation of line drawing 
to achieve our goal of building a comprehensive lexical database out of various on line resource it is necessary to interpret and disambiguate the information found in these resource in this paper we describe a disambiguation module which analyzes the content of dictionary definition in particular definition of the form to verb with np we discus the semantic relation holding between the head and the prepositional phrase in such structure a well a our heuristic for identifying these relation and for disambiguating the sens of the word involved we present some result obtained by the disambiguation module and evaluate it rate of success a compared with result obtained from human judgment 
we present a feed forward network architecture for recognizing an unconstrainedhandwritten multi digit string this is an extension of previouswork on recognizing isolated digit in this architecture a single digit recognizeris replicated over the input the output layer of the network iscoupled to a viterbi alignment module that chooses the best interpretationof the input training error are propagated through the viterbi module the novelty in this procedure is that segmentation is 
i establish fundamental equation that relate the three dimensional motion of a curve to it observed image motion i introduce the notion of spatio temporal surface and study it differential property up to the second order in order to do this i only make the assumption that the d motion of the curve preserve arc length a more general assumption than that of rigid motion i show that contrarily to what is commonly believed the full optical flow of the curve can never be recovered from this surface i nonetheless then show that the hypothesis of a rigid d motion allows in general to recover the structure and the motion of the curve in fact without explicitely computing the tangential optical flow 
a model of the elementary particle of a domain and their rudimentary interaction is essential for sophisticated reasoning about the macroscopic behavior of physical system a microscopic theory can make explicit the deeper mechanism underlying causal model collapse a great variety of macroscopic phenomenon into a few rudimentary interaction elaborate upon or validate macroscopic explanation and so forth this paper describes a qualitative representation for microscopic theory and a method for reasoning with microscopic particle to obtain the macroscopic behavior the representation and reasoning are illustrated using implemented example from the fluid domain 
conventional method for the parametric design of engineering structure rely on the iterative re use of analysis program in order to converge on a satisfactory solution since finite element and other analysis program require considerable computer resource this research proposes a general method to minimize their use by utilizing constraint based reasoning to carry out redesign a problem solver consisting of constraint network which express basic relationship between individual design parameter and variable is attached to the analysis program once an initial design description ha been set out using the conventional analysis program the network can then reason about required adjustment in order to find a consistent set of parameter value we describe how global constraint representing standard design behavioral equation are decomposed to form binary constraint network the network use approximate reasoning to determine dependency between key parameter and after an adjustment ha been made use exact relationship information to update only those part of the design description that are affected by the adjustment we illustrate the idea by taking a an example the design of a continuous prestressed concrete beam 
an investigation is conducted of the relationship that exist between the three dimensional structure and kinematics of a line moving rigidly in space and the two dimensional structure and kinematics optical flow of it image in one or two camera the author establish the fundamental equation that relate it three dimensional motion to it observed image motion they then assume that stereo match have been established between image segment and show how the estimation of the optical flow in the two image can be used to compute part of the kinematic screw of the corresponding d line the equation are linear and provide a very simple way to estimate the full kinematic screw if several line of the same object are available experimental result using synthetic and real data are presented 
a system called skordos ha been implemented for model based diagnosis of analog circuit one of the difficulty of model based diagnosis for analog circuit is managing t he tremendous numher of prediction which may he generated by a constraint propagation system fortunately not all of those prediction are valuable for the diagnostic process a process called hibernation which is used in skordos to prevent generation of useless prediction is introduced and described here another technique is introduced and described whcih further assist in controlling the inequality reasoning by exploiting hibernation this technique involves changing the structure in which value are combined it us hibernation a an early filter to reduce the number of interaction resulting from kirchhoff s current law from exponential to quadratic in the number of interacting variable 
an analysis of the property of qualitative differential equation involving feedback structure is presented the topological interpretation of this theory serf a the basis for a simulator of qualitative differential equation quaf quaf predicts the initial trend and final state of each variable thereby elucidating the general character of the response the approach requires that causal differential equation replace algebraic form derived from pseudo steady state moving equilibrium assumption quaf is compared to the qualitative simulator qsim on an example involving interconnected tank and a significant narrowing of the number of interpretation of system behavior is observed 
the classical approach to shape from shading problem is to find a numerical solution of the image irradiance partial differential equation it is always assumed that the parameter of this equation the light source direction and surface albedo can be estimated in advance for image which contain shadow and occluding contour this decoupling of problem is artificial we develop a new approach to solving these equation it is based on modern differential geometry and solves for light source surface shape and material change concurrently local scene element scenels are estimated from the shading flow field and smoothness material and light source compatibility condition resolve them into consistent scene description shadow and related difficulty for the classical approach are discussed 
we are interested in matching stereoscopic image involving both natural object vegetation sky relief and man made object building road vehicle in this context we have developed a pyramidal stereovision algorithm based on contour chain point the matching process is performed at different step corresponding to the different resolution the nature of the primitive allows the algorithm to deal with rich and complex scene good result are obtained for extremely fast computing time 
it is known that perceptual aliasing may significantly diminish the effectiveness of reinforcement learning algorithm whitehead and ballard perceptual aliasing occurs when multiple situation that are indistinguishable from immediate perceptual input require different response from the system for example if a robot can only see forward yet the presence of a battery charger behind it determines whether or not it should backup immediate perception alone is insufficient for determining the most appropriate action it is problematic since reinforcement algorithm typically learn a control policy from immediate perceptual input to the optimal choice of action this paper introduces the predictive distinction approach to compensate for perceptual aliasing caused from incomplete perception of the world an additional component a predictive model is utilized to track aspect of the world that may not be visible at all time in addition to the control policy the model must also be learned and to allow for stochastic action and noisy perception a probabilistic model is learned from experience in the process the system must discover on it own the important distinction in the world experimental result are given for a simple simulated domain and additional issue are discussed 
in order to be taken seriously connectionist natural language processing system must be able to parse syntactically complex sentence current connectionist parser either ignore structure or impose prior restriction on the structural complexity of the sentence they can process either number of phrase or the depth of the sentence structure xeric network presented here are distributed representation connectionist parser which can analyze and represent syntactically varied sentence including one with recursive phrase structure construct no a priori limit are placed on the depth or length of sentence by the architecture xeric network use recurrent network to read word one at a time raam style reduced description and x bar grammar are used to make an economical syntactic representation scheme this is combined with a training technique which allows xeric to use multiple virtual copy of it raam decoder network to learn to parse and represent sentence structure using gradient descent method xeric network also perform number person disambiguation and lexical disambiguation result show that the network train to a few percent error for sentence up to a phrase nesting depth of ten or more and that this performance generalizes well 
we report result from large scale experiment in satisfiability testing a ha been observed by others testing the satisfiability of random formula often appears surprisingly easy here we show that by using the right distribution of instance and appropriate parameter value it is possible to generate random formula that are hard that is for which satisfiability testing is quite difficult our result provide a benchmark for the evaluation of satisfiability testing procedure 
closed world reasoning is a common nonmonotonic technique that allows for dealing with negative information in knowledge and data base we present a detailed analysis of the computational complexity of the different form of closed world reasoning for various fragment of propositional logic the analysis allows u to draw a complete picture of the tractability intractability frontier for such a form of nonmonotonic reasoning we also discus how to use our result in order to characterize the computational complexity of other problem related to nonmonotonic inheritance diagnosis and default reasoning 
this paper describes a technique for learning both the number of state and the topology of hidden markov model from example the induction process start with the most specific model consistent with the training data and generalizes by successively merging state both the choice of state to merge and the stopping criterion are guided by the bayesian posterior pro bability we compare our algorithm with the baum welch method of estimating fixed size model and find that it can induce minimal hmms from data in case where fix ed estimation doe not converge or requires redundant parameter to converge 
in this paper we introduce a logic for describing tree which allows u to reason about both the parent and domination relationship the use of domination ha found a number of application such a in deterministic parser based on description theory marcus hindle fleck in a compact organization of the basic structure of tree adjoining grammar vijay shanker schabes and in a new characterization of the adjoining operation that allows a clean integration of tag into the unification based framework vijay shanker our logic serf to formalize the reasoning on which these application are based 
we present a comparison of three well known heuristic search algorithm best first search bfs iterative deepening id and depth first branch and bound dfbb we develop a model to analyze the time and space complexity of these three algorithm in term of the heuristic branching factor and solution density our analysis identifies the type of problem on which each of the search algorithm performs better than the other two these analytical result are validated through experiment on different problem we also present a new algorithm dfs which is a hybrid of iterative deepening and depth first branch and bound and show that it outperforms the other three algorithm on some problem 
if stereopsis is to be used in a dynamic environment it make little sense to re compute the entire representation of disparity space from scratch at each time step one simple approach would be to use the result from the current solution to prime the algorithm for the next solution if three dimensional trajectory information wa available this information could be used to first update the previous solution and then this updated solution could be used to prime the algorithm for the following stereo pair recent work ha demonstrated that it is possible to measure such trajectory information very quickly without complex token or feature extraction this paper demonstrates how raw disparity measurement made by this earlier technique can be integrated into a single trajectory measurement at each image point a mechanism is then proposed that update a stereopsis algorithm operating in a dynamic environment using this trajectory information 
surface discontinuity are detected in a sequence of image by exploiting physical constraint at early stage in the processing of visual motion to achieve accurate early discontinuity detection we exploit five physical constraint on the presence of discontinuity i the shape of the sum of squared difference ssd error surface in the presence of surface discontinuity ii the change in the shape of the ssd surface due to relative surface motion iii distribution of optic flow in a neighborhood of a discontinuity iv spatial consistency of discontinuity v temporal consistency of discontinuity the constraint are described and experimental result on sequence of real and synthetic image are presented the work ha application in the recovery of environmental structure from motion and in the generation of dense optic flow field 
this paper investigates the effect of parallelism on blackboard system scheduling a parallel blackboard system is described that allows multiple knowledge source instantiation to execute in parallel using a shared memory blackboard approach new class 
we argue for hyper logicism the view hitherto unarticulated that ai can succeed in creating a genuine robot agent by building a symbol system of the appropriate sort which ha no sub symbolic interaction whatsoever with the external world 
existing shape from shading algorithm assume constant reflectance across the shaded surface multi colored surface are excluded because both shading and reflectance affect the measured image intensity given a standard rgb color image we describe a method of eliminating the reflectance effect in order to calculate a shading field that depends only on the relative position of the illuminant and surface of course shading recovery is closely tied to lightness recovery and our method follows from the work of land horn and blake in the luminance image r g b shading and reflectance are confounded reflectance change are located and removed from the luminance image by thresholding the gradient of it logarithm at location of abrupt chromaticity change thresholding can lead to gradient field which are not conservative do not have zero curl everywhere and are not integrable and therefore do not represent realizable shading field by applying a new curl correction technique at the thresholded location the thresholding is improved and the gradient field are forced to be conservative the resulting poisson equation is solved directly by the fourier transform method experiment with real image are presented 
in the general framework of a constraint based grammar formalism often some sort of feature logic serf a the constraint language to describe linguistic object we investigate the extension of basic feature logic with subsumption or matching constraint based on a weak notion of subsumption this mechanism of one way information flow is generally deemed to be necessary to give linguistically satisfactory description of coordination phenomenon in such formalism we show that the problem whether a set of constraint is satisfiable in this logic is decidable in polynomial time and give a solution algorithm 
this paper describes the application of explanation based learning a machine learning technique to the sri core language engine a large scale general purpose natural language analysis system the idea is to bypass normal morphological syntactic and 
interval and point algebra have been proposed for representing qualitativetemporal information about the relationshipsbetween pair of intervalsand pairsof point respectively in thispaper we address two relatedreasoningtasksthat arisein these algebra given possibly indefinite knowledge of the relationshipsbetween some intervalsor point findone or more scenariosthat are consistentwith the information provided and find allthe feasiblerelationsbetween every pair of intervalsor point solutionsto these problem have application in natural language processing planning and a knowledge representationlanguage we definecomputationallyefficientprocedures forsolvingthese tasksforthe pointalgebraand forum correspondingsubset of the intervalalgebra our algorithm are marked improvement over the previouslyknown algorithm we alsoshow how the resultsfor the point algebrahelp u to design a backtracking algorithmforthe full intervalalgebrathat isusefulinpractice 
a car like indoor mobile robot is a kinematically constrained robot that can be modelled a a d object translating and rotating in the horizontal plane among well defined obstacle the kinematic constraint impose that the linear velocity of the robot point along it main axis no sidewise motion is possible and restrict the range of admissible value for the steering angle in this paperl we describe a fast path planner for such a robot this planner is one to two order of magnitude faster than previously implemented planner for the same type of robot in addition it ha an anytime flavor that allows it to return a path in a short amount of time and to improve that path through iterative optimization according to the amount of time that is devoted to path planning the planner is essentially a combination of preexisting idea it efficiency derives from the good match between these idea and from various technical improvement brought to them 
we address the problem of scale selection in texture analysis two different scale parameter feature scale and statistical scale are defined statistical scale is the size of the region used to compute average we define the class of homogeneous random function a a model of texture a dishomogeneity function is defined and we prove that it ha usefulasymptotic property in the limit of infinite statistical scale we describean algorithm for image partitioning which ha performed 
in we defined the concept of agent oriented programming aop which can be viewed a a specialization of object oriented programming oop aop view object a agent with mental state and in the spirit of speech act theory identifies a number of message type informing requesting offering and so on aop is a general framework in this paper we present a specific and simple language called agento we define it syntax present it interpreter and illustrate both through an example 
since knowledge base kb are usually incomplete they should be able to provide information regarding their own incompleteness which requires them to introspect on what they know and do not know an important area of research is to devise model of introspective reasoning that take into account resource limitation under the view that a kb is completely characterized by the set of belief it represents it epistemic state it seems natural to model kb in term of belief reasoning can then be understood a the problem of computing membership in the epistemic state of a kb the best understood model of belief are based on possible world semantics however their computational property are unacceptable in particular they render reasoning in firstorder kb undecidable in this paper we propose a novel model of belief which preserve many of the advantage of possible world semantics yet at the same time guarantee reasoning to be decidable where a kb may contain sentence in full first order logic moreover such kb have perfect knowledge about their own belief even though their belief about the world are limited 
current explanation based generalization ebg technique can perform badly when the problem being solved involves recursion often an infinite series of learned concept are generated that correspond to the expansion of recursive solution over every finite depth previous attempt to address the problem such a shavlik s generalization to n ebg method are overly reluctant to expand recursion this reluctance can lead to inefficient rule in this paper ebg is viewed a a program transformation technique on logic program within that framework an improved operationality criterion for controlling the expansion of recursion is presented this criterion prevents certain infinite and combinatorially explosive rule class from being generated yet permit expansion in some useful circumstance allowing more efficient rule to be learned 
a novel method that us vanishing point of horizontal line but not parallel to each other for estimating the change of camera azimuth is proposed although indoor scene contain many such line there also exist non horizontal line which should be discarded from the group of line for estimating the azimuth change distinguishing non horizontal line from horizontal one is difficult for a static image but possible for the dynamic imagery because non horizontal line show different motion pattern a the camera move 
neural network model have been criticized for their inability to make useofcompositional representation in this paper we describe a series of psychological phenomenon that demonstrate the role of structured representation in cognition these finding suggest that people compare relational representation via a process of structural alignment this process will have to be captured by any model of cognition symbolic or subsymbolic 
this paper analysis the criterion necessary for a knowledge representation kr language for implementing high level vision hlv recognition system we show the importance of introducing a specific kr language for specification and possibly for implementation of hlv system in particular we examine the adequacy tractability and suitability of implementing a hlv system using logic the kr language most commonly used in area of artificial intelligence isomorphic to hlv in addition we use this analysis of classical logic to identify the criterion necessary for any hlv kr language logic is seen to be at least a good a language for specification of hlv system a any other kr language however using evidence obtained from an object recognition system implemented using propositional logic evidence which is supported by theoretical analysis we argue that classical logic is an inadequate kr language for implementing hlv system it cannot identify preferred interpretation and is computationally intractable even for simple propositional language 
this paper present a process model of plan inference for use in natural language consultation system it includes a strategy that can both defer unwarranted decision about the relationship of a new action to the user s overall plan and sanction rational default inference the paper describes an implementation of this strategy using the dempster shafer theory of evidential reasoning our process model overcomes a limitation of previous plan recognition system and produce a richer model of the user s plan and goal yet one that can be explained and justified to the user when discrepancy arise between it and what the user is actually trying to accomplish 
there have been many recent attempt to incorporate default into unification based grammar formalism what these attempt have in common is that they all lose one of the most desirable property of feature system namely presentation order independence this paper describes a method of dealing with default that retains order independence the method work by making a strong distinction between strict and default information the addition of nonmonotonic sort allows default information to be carried in the feature structure while retaining a simple deterministic unification operation monotonic feature structure are rederived through a satisfaction relation that is abstract in that it depends only on the ordering information for sort 
a parameter mapping well suited for line segmentation is described we start with discussing some intuitively natural mapping for line segmentation including the popular hough transform then we proceed with describing the novel parameter mapping and argue for it property the topology of the mapping introduces it name the m bius strip parameterization this mapping ha topological advantage over previously proposed mapping 
we investigate a model in which excitatory neuron have dynamical thresholdswhich display both fatigue and potentiation the fatigue propertyleads to oscillatory behavior it is responsible for the ability of the modelto perform segmentation i e decompose a mixed input into staggeredoscillations of the activity of the cell assembly memory affected byit potentiation is responsible for sustaining these staggered oscillationsafter the input is turned off i e the system 
we present a new approach to the problem of matching d curve the approach ha an algorithmic complexity sublinear with the number of model and can operate in the presence of noise and partial occlusion 
three key component of an autonomous intelligent system are planning execution and learning this paper describes how the soar architecture support planning execution and learning in unpredictable and dynamic environment the tight integration of these component provides reactive execution hierarchical execution interruption on demand planning and the conversion of deliberate planning to reaction these capability are demonstrated on two robotic system controlled by soar one using a puma robot arm and an overhead camera the second using a small mobile robot with an arm 
gradient method are widely used in the computation of optical flow we discus extension of these method which compute probability distribution of optical flow the use of distribution allows representation of the uncertainty inherent in the optical flow computation facilitating the combination with information from other source we compute distributed optical flow for a synthetic image sequence and demonstrate that the probabilistic model account for the error in the flow estimate 
this paper considers the interpretation a a three dimensional velocity field of the changing intensity pattern induced by a smoothly deforming lambertian surface of uniform albedo illuminated by a distant point light source the requisite intensity rate constraint which is derived contains no term relating to the tangential component of surface velocity so the determination of the velocity field is ill posed exhibiting a form of aperture problem a stretch based regulariser is applied to enable estimation of the velocity field and test with synthetic data show a requirement for high accuracy 
full text retrieval system often use either a bitmap or an inverted file to identify which document contain which term so that the document containing any combination of query term can be quickly located bitmap of term occurrence are large but are usually sparse and thus are amenable to a variety of compression technique here we consider technique in which the encoding of each bitvector within the bitmap is parameterised so that a different code can be used for each bitvector our experimental result show that the new method yield better compression than previous technique 
both the tangential and normal component of the flow can be computed reliably where the image hessian is well conditioned a fast algorithm to propagate flow along contour from such location is proposed experimental result for an intrinsically parallel algorithm for computing the flow along zero crossing contour are presented 
many researcher believe that certain aspect of natural language processing such a word sense disambiguation and plan recognition in story constitute abductive inference we have been working with a specific model of abduction called parsimonious covering applied in diagnostic problem solving word sense disambiguation and logical form generation in some restricted setting diagnostic parsimonious covering ha been extended into a dual route model to account for syntactic and semantic aspect of natural language the two route of covering are integrated by defining open class linguistic concept aiding each other the diagnostic model ha dealt with set while the extended version where syntactic consideration dictate word order deal with sequence of linguistic concept here we briefly describe the original model and the extended version and briefly characterize the notion of covering and different criterion of parsimony finally we examine the question of whether parsimonious covering can serve a a general framework for parsing 
determining the relationship between the intonational characteristic of an utterance and other feature inferable from it text is important both for speech recognition and for speech synthesis this work investigates the use of text analysis in predicting the location of intonational phrase boundary in natural speech through analyzing utterance from the darpa air travel information service database for statistical modeling we employ classification and regression tree cart technique we achieve success rate of just over representing a major improvement over other attempt at boundary prediction from unrestricted text 
a qualitative reasoning planner for determining robot control parameter to drive manipulation action ha been developed integrated into a telerobot system and demonstrated for a match striking task the planner consists of a qualitative reasoner and a numerical execution history which interact to jointly direct and narrow the search for reliable numerical control parameter value the planner algorithm implementation and an execution example are described the relationship to previous qualitative reasoning work is also discussed 
we present a computational framework for stereopsis basedon the output of linear spatial filter tuned to a range of orientation andscales this approach go beyond edge based and area based approachesby using a richer image description and incorporating several stereo cuesthat have previously been neglected in the computer vision literature 
this empirical study attempt to find answer to the question of how a natural language henceforth nl system could resolve attachment of prepositional phrase henceforth pps by examining naturally occurring pp attachment in typed dialogue examination includes testing predictive power of existing attachment theory against the data the result of this effort will be an algorithm for interpreting pp attachment 
it is shown that the formalism of bayesian network provides an elegant solution in a probabilistic framework to the problem of integrating top down and bottom up visual process a well serving a a knowledge base the formalism is modified to handle spatial data and thus extends the applicability of bayesian network to visual processing the modified form is called the perceptual inference network pin the theoretical background of a pin is presented and it viability is demonstrated in the context of perceptual organization the pin imparts an active inferential and integrating nature to perceptual organization 
junction are the intersection point of three or more intensity surface in an image an analysis of zero crossing and the gradient near junction demonstrates that gradient based edge detection scheme fragment edge at junction this fragmentation is caused by the intrinsic pairing of zero crossing and a destructive interference of edge gradient at junction using the previous gradient analysis we propose a junction detector that find junction in edge map by following gradient ridge and using the minimum direction of saddle point in the gradient the junction detector is demonstrated on real imagery and previous approach to junction detection are discussed 
ionpaul r cooperinstitute for the learning sciencesnorthwestern universityevanston ilcooper il nwu edupeter n prokopowiczinstitute for the learning sciencesnorthwestern universityevanston ilprokopowicz il nwu eduabstractnetwork vision system must make inference from evidential informationacross level of representational abstraction from low level invariant through intermediate scene segment to high level behaviorally relevantobject description this paper show 
this paper deal with the application of a multichannelfiltering based texture segmentation methodto a variety of document image processing problem text graphic separation address block location andbar code localization both supervised and unsupervisedmethods have been used to identify region of textor bar code in the input gray level document image the performance of our segmentation and classificationscheme is shown on a variety of document imageswhich were scanned using a 
reasoning about one s personal schedule of appointment is a common but surprisingly complex activity motivated by the novel application of planning and temporal reasoning technique to this problem we have extended the formalization of the temporal distance model of dechter meiri and pearl we have developed method for using date a reference interval and for meeting the challenge of repeated activity such a weekly recurring appointment 
causal reasoning is an essential part of a number of task that have been central to many endeavour in ai notably planning and prediction diagnosis and explanation recently it ha become an object of study in it own right drawing inspiration from the work of philosopher and logician a well a more immediately ai oriented concern in this paper i shall examine just one approach to causal reasoning that advocated by yoav shoham in a recent book and article in particular i shall try to lay bare a number of assumption underlying shoham s work all of which i shall call into question key assumption are that causality is an epistemic notion that causal reasoning is inherently non monotonic and that epistemic reasoning should be handled by mean of modal logic while arguing against these assumption i do not offer a specific causal theory of my own but shall conclude with some suggestion a to the general line which i feel such a theory ought to follow 
a general formalism for detecting geometric configuration of image data is presented the author first estimate an ideal geometric configuration that supposedly exists and then check to what extent the original edge must be displaced in order to support the hypothesis all type of test are reduced to computing a single measure of edge displacement which provides a universal measure of uncertainty applicable to all type of decision making 
one usually writes a i program to be used on a range of example which although similar in kind differ in detail this paper show how to predict where in a space of problem instance the hardest problem are to be found and where the fluctuation in difficulty are greatest our key insight is to shift emphasis from modelling sophisticated algorithm directly to modelling a search space which capture their principal effect this allows u to analyze complex a i problem in a simple and intuitive way we present a sample analysis compare our model s quantitative prediction with data obtained independently and describe how to exploit the result to estimate the value of preprocessing finally we circumscribe the kind problem to which the methodology is suited 
the limited capacity of working memory is intrinsic to human sentence processing and therefore must be addressed by any theory of human sentence processing this paper give a theory of garden path effect and processing overload that is based on simple assumption about human short term memory capacity 
investigating the character of scientific discovery using computational model is a growing area in artificial intelligence and cognitive science scientific discovery involves both theory and experiment but existing discovery system have mainly considered the formation and modification of theory this paper focus on the modelling of experiment a general characterization of the nature of experiment is given and more specifically galileo s motion experiment are examined the stern scientific discovery system ha been used to model galileo s investigation of free fall and is introduced here the system ha an extensive representation for experiment and us experiment to i confirm existing hypothesis ii find new hypothesis ii enhance it own performance and iv make intractable hypothesis tractable 
a multidimensional edge model is established and a first order estimation for multidimensional edge profile is proposed an optimal edge location detection algorithm is developed the advantage of the algorithm are that it ha little dependence on assumption of edge model noise model or smoothing filter it ha better ability for detecting very weak edge and making le edge orientation error than other edge detector it can handle corner and complicated multidimensional image structure and it detects different edge type at the same time 
we present a reliable and efficient method for extracting simple geometric structure i e straight line parabola and ellipsis from edge image the reliability of the recovery procedure which build the parametric model is ensured by an iterative procedure through simultaneous data classification and parameter estimation the overall relative insensitivity to noise and minor change in input data is achieved by considering many competitive solution and selecting those that produce the simplest description i e the one that account for the largest number of data point with the smallest number of parameter while keeping the deviation between data point and model low the presented method is efficient for two reason firstly it is designed a a search which utilizes intermediate result a a guidance toward the final result and secondly it combine model recovery and model selection in a computationally efficient procedure 
this paper describes a cognitively plausible mechanism for systematically handling complex syntactic construction within a semantic parser more specifically we show how these construction are handled without a global syntactic grammar or syntactic parse tree representation and without sacrificing the benefit of semantically oriented parsing we evaluate the psychological validity of our architecture and conclude that it is a plausible computational model of human processing for an important class of embedded clause construction a a result we achieve robust sentence processing capability not found in other parser of it class 
in many domain of interest to distributed artificial intelligence the problem solving environment may be viewed a a collection of loosely coupled intelligent agent each of which reason based on it own incomplete knowledge of the state of the world no agent ha sufficient knowledge to solve the problem at hand so that coordinated cooperative problem solving is required to satisfy system goal in this paper we present dare a distributed reasoning system in which agent have the ability to focus their attention on selective information interchange to facilitate cooperative problem solving the experimental result we present demonstrate that agent in a loosely coupled network of problem solver can work semi independently yet focus their attention with the aid of relatively simple heuristic when cooperation is appropriate these result suggest that we have developed an effective cooperation strategy which is largely independent of initial knowledge distribution 
you want your neural net algorithm to learn sequence do not just use conventionalgradient descent or approximation thereof in recurrent net time delay net etc instead use your sequence learning algorithm to implement the following method no matter whatyour final goal are train a network to predict it next input from the previous one sinceonly unpredictable input convey new information ignore all predictable input but let allunexpected input plus information about the 
in this paper we show how a natural language system can learn to find the antecedent of relative pronoun we use a well known conceptual clustering system to create a case based memory that predicts the antecedent of a wh word given a description of the clause that precedes it our automated approach duplicate the performance of hand coded rule in addition it requires only minimal syntactic parsing capability and a very general semantic feature set for describing noun human intervention is needed only during the training phase thus it is possible to compile relative pronoun disambiguation heuristic tuned to the syntactic and semantic preference of a new domain with relative ease moreover we believe that the technique provides a general approach for the automated acquisition of additional disambiguation heuristic for natural language system especially for problem that require the assimilation of syntactic and semantic knowledge 
one of the most common modification made to the standard strip action representation is the inclusion of filter condition a key function of such filter condition is to distinguish between operator that represent different context dependent effect for the same action we consider how filter condition may be used to provide this functionality in a complete and correct partial order planner we conclude that they are not effective and that in general the use of filter condition is incompatible with the basic assumption that lie behind partial order planning we present an alternative mechanism using the secondary precondition of pednault to represent context dependent effect the use of secondary precondition is effective and preserve completeness and correctness 
there is a need for highly redundant manipulator to work in complex cluttered environment our goal is to plan path for such manipulator efficiently the path planning problem ha been shown to be psp ace complete in term of the number of degree of freedom dof of the manipulator we present a method which overcomes the complexity with a strong heuristic utilizing redundancy by mean of a continuous manipulator model the continuous model allows u to change the complexity of the problem from a function of both the dof of the manipulator believed to be exponential and the complexity of the environment polynomial to a polynomial function of the complexity of the environment only 
decision tree are widely used in machine learning and knowledge acquisition system however there is no optimal or even unanimously accepted strategy of obtaining good such tree and most of the generated tree suffer from impropriety i e inadequacy in representing knowledge the final goal of the research reported here is to formulate a theory for the decision tree domain that is a set of heuristic on which a majority of expert will agree which will describe a good decision tree a well a a set of heuristic specifying how to obtain optimal tree in order to achieve this goal we have designed a recursive architecture learning system which monitor an interactive knowledge acquisition system based on decision tree and driven by explanatory reasoning and incrementally acquires from the expert using it the knowledge used to build the decision tree domain theory this theory is also represented a a set of decision tree and may be domain dependent our system acquires knowledge to define the notion of good bad decision tree and to measure their quality a well a knowledge needed to guide domain expert in constructing good decision tree the partial theory acquired at each moment is also used by the basic knowledge acquisition system in it tree generation process thus constantly improving it performance 
this paper present some complexity result for deductive recognition in the framework of language such a kl one in particular it focus on classification operation that are usually performed in these language through subsumption computation the paper present a simple language that encompasses and extends earlier kl one based recognition framework by relying on parsing algorithm the paper show that a significant class of recognition problem in this language can be performed in polynomial time this is in marked contrast to the exponentiality and undecidability result that have recently been obtained for subsumption in even some of the most restricted variant of kl one 
this paper analyzes the property structure and limitation of vector based model for information retrieval from the computational geometry point of view it is shown that both the pseudo cosine and the standard vector space model can be viewed a special case of a generalized linear model more importantly both the necessary and sufficient condition have been identified under which ranking function such a the inner product cosine pseudo cosine dice covariance and product moment correlation measure can be used to rank the document the structure of the solution region for acceptable ranking is analyzed and an algorithm for finding all the solution vector is suggested 
in this paper we examine the relationship between belief goal and intention in particular we consider the formalization of the asymmetry thesis a proposed by bratman we argue that the semantic characterization of this principle determines if the resulting logic is capable of handling other important problem such a the side effect problem of beliefgoal intention interaction while cohen and levesque s formalization faithfully model some aspect of the asymmetry thesis it doe not solve all the side effect problem on the other hand the formalization provided by rao and georgeff solves all the side effect problem but only model a weak form of the asymmetry thesis in this paper we combine the intuition behind both these approach and provide a semantic account of the asymmetry thesis in both linear time and branching time logic for solving many of these problem 
this paper present a general mathematically rigorous approach to nonlinear planning that handle both complex goal and action with context dependent effect a goal can be any arbitrary well formed formula containing conjunction disjunction negation and quantifier action are likewise not constrained and can have an unrestricted number of complex situation dependent effect the approach presented here can thus be used to solve a wider range of problem than previous approach to nonlinear planning the approach is based on previous work by the author on linear planning the same mathematical framework is used with the result extended to nonlinear plan 
this paper describes the localized search mechanism of the gemplan multiagent planner both formal complexity result and empirical result are provided demonstrating the benefit of localized search a localized domain description is one that decomposes domain activity and requirement into a set of region this description is used to infer how domain requirement are semantically localized and a a result to enable the decomposition of the planning search space into a set of space one for each domain region benefit of localization include a smaller and cheaper overall search space a well a heuristic guidance in controlling search such benefit are critical if current planning technology and other type of reasoning are to be scaled up to large complex domain 
the problem of determining the intrinsic and extrinsic parameter of a mobile camera is addressed an optimal solution which consists of the following step is presented first the camera is calibrated in several working position and for each position the corresponding transformation matrix is computed using a method developed by o d faugeras and g toscani next optimal intrinsic parameter are searched for all position finally for each separate position optimal extrinsic parameter are computed by minimizing a mean square error through a closed form solution experimental result show that such a technique yield a very large reduction of calibration error and a considerable gain relative to other existing on site calibration technique 
recognising a curved surface from it outline in a single view isa major open problem in computer vision this paper show technique forrecognising a significant class of surface from a single perspective view theapproach us geometrical fact about bitangencies crease and inflectionsto compute description of the surface s shape from it image outline thesedescriptions are unaffected by the viewpoint or the camera parameter weshow using image of real scene that these 
this paper present an explanation based learning strategy for learning general plan for use in an integrated approach to planning the integrated approach augments a classical planner with the ability to defer achievable goal thus preserving the construction of provably correct plan while gaining the ability to utilize runtime information in planning proving achievability is shown to be possible without having to determine the action to achieve the associated goal a learning strategy called contingent explanation based learning us conjectured variable to represent the eventual value of plan parameter with unknown value a priori and completers to determine these value during execution an implemented system demonstrates the use of contingent ebl in learning a general completable reactive plan for spaceship acceleration 
we introduce and demonstrate a bootstrap method for construction of an inverse function for the robot kinematic mapping using only sample configurationspace workspace data unsupervised learning clustering technique are used on pre image neighborhood in order to learn to partition the configuration space into subset over which the kinematic mapping is invertible supervised learning is then used separately on each of the partition to appro ximate the inverse function the ill posed inverse kinematics function is thereby regularized and a global inverse kinematics solution for the wristless puma manipulator is developed 
a language processor is to find out a most promising sentence hypothesis for a given word lattice obtained from acoustic signal recognition in this paper a new language processor is proposed in which unification grammar and markov language model are integrated in a word lattice parsing algorithm based on an augmented chart and the island driven parsing concept is combined with various preference first parsing strategy defined by different construction principle and decision rule test result show that significant improvement in both correct rate of recognition and computation speed can be achieved 
this paper address the problem of determining the kind of three dimensional reconstruction that can be obtained from a binocular stereo rig for which no three dimensional metric calibration data is available the only information at our disposal is a set of pixel correspondence between the two retina which we assume are obtained by some correlation technique or any other mean we show that even in this case some very rich non metric reconstruction of the environment can nonetheless be obtained 
this paper extends yeap s computational theory of cognitive map focusing on the problem of computing a raw cognitive map by an autonomous agent in addition to having a view of the environment a input the agent also maintains a representation of her immediate surroundings this representation is referred to a an mfis a memory for one s immediate surroundings argument for the use of the mfis are presented the main question that we ask in implementing our idea are i what frame of reference is appropriate for the mfis and ii how doe the mfis change a the agent move through the environment a program ha been implemented successfully and the main algorithm used and the result of running the program are presented 
we investigate the utility of explanation based learning in recursive domain theory and examine the cost of using macro rule in these theory the compilation option in a recursive domain theory range from constructing partial unwindings of the recursive rule to converting recursive rule into iterative one we compare these option against using appropriately ordered rule in the original domain theory and demonstrate that unless we make very strong assumption about the nature of the distribution of future problem it is not profitable to form recursive macro rule via explanation based learning in these domain 
a peg in hole insertion task is used a an example to illustratethe utility of direct associative reinforcement learning method forlearning control under real world condition of uncertainty andnoise task complexity due to the use of an unchamfered holeand a clearance of le than mm is compounded by the presenceof positional uncertainty of magnitude exceeding to time theclearance despite this extreme degree of uncertainty our resultsindicate that direct 
most previous work in analytic generalization of plan dealt with totally ordered plan these method cannot be directly applied to generalizing partially ordered plan since they do not capture all interaction among plan operator for all total order of such plan in this paper we introduce a new method for generalizing partially ordered plan this method is based on providing ebg with explanation which systematically capture the interaction among plan operator for all the total order of a partially ordered plan the explanation are based on the modal truth criterion which state the necessary and sufficient condition for ensuring the truth of a proposition at any point in a plan for a class of partially ordered plan the generalization obtained by this method guarantee successful and ineraction free execution of any total order of the generalized plan in addition the systematic derivation of the generalization algorithm from the modal truth criterion obviates the need for carrying out a separate formal proof of correctness of the ebg algorithm 
model based object recognition is typically addressed by first deriving structure from image and then matching that structure with stored object while recognition should be facilitated through the derivar tion of a much structure a possible most researcher have found that a compromise is necessary a the process for deriving that structure are not sufficiently robust we present a technique for the extraction and subsequent recognition of d object model from passively sensed image model extraction is performed using a depth from camera motion technique followed by simple interpolation between the determined depth value the resultant model are recognised using a new technique implicit model matching which wa originally developed for use with model derived from actively sensed range data the technique performs object recognition using secondary representation of the d model hence overcoming the problem frequently associated with deriving stable model primitive this paper then describes a technique for deriving d structure from passively sensed image introduces a new approach to object recognition test the approach robustness of the approach and hence demonstrates the potential for object recognition using d structure derived from passively sensed data 
thematic analysis is best manifested by contrasting collocation such a shipping pacemaker v shipping department while in the first pair the pacemaker are being shipped in the second one the department are probably engaged in some shipping activity but are not being shipped text pre processor intended to inject corpus based intuition into the parsing process must adequately distinguish between such case although statistical tagging church et al meteer et al brill cutting et al ha attained impressive result overall the analysis of multiple contentword string i e collocation ha presented a weakness and caused accuracy degradation to provide acceptable coverage i e of collocation a tagger must have accessible a large database i e pair of individually analyzed collocation consequently training must be based on a corpus ranging well over million word since such a large corpus doe not exist in a tagged form training must be from raw corpus in this paper we present an algorithm for text tagging based on thematic analysis the algorithm yield high accuracy result we provide empirical result the program nlcp nl corpus processing acquired a thematic relation database through the million word wall street journal corpus it wa tested over the tipster word joint venture corpus 
the approach to database query evaluation developed by levesque and reiter treat database a first order theory and query a formula of the language which includes in addition to the language of the database an epistemic modal operator in this epistemic query language one can express question not only about the external world described by the database but also about the database itself about what the database know on the other hand epistemic formula are used in knowledge representation for the purpose of expressing default autoepistemic logic is the best known epistemic nonmonotonic formalism the logic of grounded knowledge proposed recently by lin and shoham is another such system this paper brings these two direction of research together we describe a new version of the lin shoham logic similar in spirit to the levesque reiter theory of epistemic query using this formalism we can give meaning to epistemic query in the context of nonmonotonic database including logic program with negation a failure 
unification of disjunctive feature description is important for efficient unification based parsing this paper present constraint projection a new method for unification of disjunctive feature structure represented by logical constraint constraint projection is a generalization of constraint unification and is more efficient because constraint projection ha a mechanism for abandoning information irrelevant to a goal specified by a list of variable 
the ease of learning concept from example in empirical machine learning depends on the attribute used for describing the training data we show that decision tree based feature construction can be used to improve the performance of back propagation bp an artificial neural network algorithm both in term of the convergence speed and the number of epoch taken by the bp algorithm to converge we use disjunctive concept to illustrate feature construction and describe a measure of feature quality and concept difficulty we show that a reduction in the difficulty of the concept to be learned by constructing better representation increase the performance of bp considerably 
the author address the motion tracking problem that arises in the context of a mobile vehicle navigating in an unknown environment where other mobile body such a human being or robot may also be moving a stereo rig mounted on the mobile vehicle provides a sequence of d map of the environment the current stereo system is trinocular and the d token used at the moment are line segment corresponding to significant intensity discontinuity in the image although the framework to solve the motion tracking problem developed in this work arises in this specific context the author believe it should be applicable in other context in particular they could use other d primitive for example point combination of point and line curve 
the planar thallium tl myocardial perfusion scintigram is a widely used diagnostic technique for detecting and estimating the risk of coronary artery disease interpretation is currently based on visual scoring of myocardial defect combined with image quantitation and is known to have a significant subjective component neural network learned to interpret thallium scintigrams a determined by both individual and multiple consensus expert rating four different type of network were explored single layer two layer backpropagation bp bp with weight smoothing and two layer radial basis function rbf the rbf network wa found to yield the best performance generalization by region and compare favorably with human expert we conclude that this network is a valuable clinical tool that can be used a a reference diagnostic support system to help reduce inter and intraobserver variability this system is now being further developed to include other variable that are expected to improve the final clinical diagnosis 
this paper describes a technique for building a geometric description of a scene from the motion of a camera mounted on a robot arm the movement of edge line in a sequence of image are tracked to maintain an image plane flow model tracking perserves the correspondance of segment even when the camera displaces makeing possible a inexpensive form of motion stereo three dimensional structure is computed using the match provided by the segment tracking process and the displacement parameter provided by the robot controller by fusion of d data from different view point we obtain an accurate and complete representation of the scene 
previously we have introduced the idea of neural network transfer where learning on a target problem is sped up by using the weightsobtained from a network trained for a related source task here we present a new algorithm called discriminability based transfer dbt which us an information measure to estimate the utilityof hyperplanes defined by source weight in the target network and rescales transferred weight magnitude accordingly severalexperiments demonstrate that target 
this paper present a computer algorithm which given a dense temporal sequence of intensity image of multiple moving object will separate the image into region showing distinct object and for those object which are rotating will calculate the three dimensional structure and motion the method integrates the segmentation of trajectory into subset corresponding to different object with the determination of the motion and structure of the object trajectory are partitioned into group corresponding to the different object by fitting the trajectory from each group to a hierarchy of increasingly complex motion model this grouping algorithm us an efficient motion estimation algorithm based on the factorization of a measurement matrix into motion and structure component experiment are reported using two real image sequence of frame each to test the algorithm 
we previously proposed the moving target search mt algorithm where the location of the goal may change during the course of the search mt is the first search algorithm concerned with problem solving in a dynamically changing environment however since we constructed the algorithm with the minimum operation necessary for guaranteeing it completeness the algorithm a proposed is neither efficient nor intelligent in this paper we introduce innovative notion created in the area of resource bounded planning into the formal search algorithm mt our goal is to improve the efficiency of mt while retaining it completeness notion that are introduced are commitment to goal and deliberation for selecting plan evaluation result demonstrate that the intelligent mt is to time more efficient than the original mt in uncertain situation 
although considerable interest ha been shown in language inferenceand automaton induction using recurrent neural network success ofthese model ha mostly been limited to regular language we havepreviously demonstrated that neural network pushdown automaton nnpda model is capable of learning deterministic context free language e g anbnand parenthesis language from example however the learning task is computationally intensive in this paper wediscuss some way in 
this paper show how a new approach in the use of ai technique ha been successfully used for the design of an effective it in the domain of diagnosis training the originality of this approach wa to take into account three complex problem simultaneously teaching diagnosis method to student giving the mean to the teacher of maintaining the system by themselves and providing a tool easy to insert in the context of university laboratory the architecture of the system is based on a distinct use of two kind of knowledge representation all the knowledge liable to modification is gathered within library under descriptive form easily maintained by the educational staff general diagnosis knowledge independent of hardware circuit and even application field is described with basic production rule and control metarule the development of the system wa based on the precise analysis of the expert s behaviour and of the user s need with the aim of making extensive use of the descriptive form in order to minimize the static knowledge embedded in the rule the system can work on a microcomputer and is used in an engineering school 
the human face is an object that is easily located in complex scene by infant and adult alike yet the development of an automated system to perform this task is extremely challenging this paper is concerned with the development of a computational model for locating human face in newspaper photograph based on cognitive research in human perceptual development in the process of learning to recognize object in the visual world one could assume that natural growth favor the development of the ability to detect the more essential feature first hence a study of the progress of an infant s visual ability can be used to categorize the potential feature in term of their importance the face locator developed by the author take a hypothesis generate and test approach to the task of finding the location of people s face in digitized picture information from the accompanying caption is used in the verification phase the system successfully located all face in of the test newspaper photograph 
this paper discus a radically new scheme of natural language processing called massively parallel memory based parsing most parsing scheme are rule based or principle based which involves extensive serial rule application thus it is a time consuming task which requires a few second or even a few minute to complete the parsing of one sentence also the degree of parallelism attained by mapping such a scheme on parallel computer is at most medium so that the existing scheme can not take advantage of massively parallel computing the massively parallel memory based parsing take a radical departure from the traditional view it view parsing a a memory intensive process which can be sped up by massively parallel computing although we know of some study in this direction we have seen no report regarding implementation strategy on actual massively parallel machine on performance or on practicality accessment based on actual data thus this paper focus on discussion of the feasibility and problem of the approach based on actual massively parallel implementation using real data the degree of parallelism attained in our model reach a few thousand and the performance of a few millisecond per sentence ha been accomplished in addition parsing time grows only linearly or sublincarly to the length of the input sentence the experimental result show the approach is promising for real time parsing and bulk text processing 
qualitative reasoner have been hamstrung by the inability to analyze large model this includes self explanatory simulator which tightly integrate qualitative and numerical model to provide both precision and explanatory power while they have important potential application in training instruction and conceptual design a critical step towards realizing this potential is the ability to build simulator for medium sized system i e on the order of ten to twenty independent parameter this paper describes a new method for developing self explanatory simulator which scale up while our method involves qualitative analysis it doe not rely on envisioning or any other form of qualitative simulation we describe the result of an implemented system which us this method and analyze it limitation and potential 
commitment to an ontological perspective is a primary aspect of reasoning about the physical world for complex analytic task the ability to switch between different ontology to represent the same target system can be critical supplementing the standard device ontology for electronic circuit we outline element of a charge carrier cc ontology for reasoning about electronics having two ontology extends our range of reasoning but raise the issue of how to control their application we propose a set of ontological choice rule to govern the process of ontological shift and demonstrate it effectiveness with example involving the two ontology in reasoning about electronic circuit 
this study report result from three patient with bilateral brain lesion a f c d and o s and normal observer on psychophysical task which examined the contribution of motion mechanism to the extraction of image discontinuity the data do not support the suggestion that the visual system extract motion discontinuity by comparing fully encoded velocity signal nl clo moreover the data do not support the suggestion that the computation underlying discontinuity localization must occur simultaneously with the spatial integration of motion signal kea we propose a computational scheme that can account for the data 
this paper is about orienting that is establishing and maintaining a spatial relation between a motorized pair of camera the eye head system and a static or a moving object tracked over time motivated by physiological evidence the paper proposes a simple set of vision based strategy aimed to perform head eye and body movement in a complex environment fixation is shown to be an essential feature in visual servoing and it is used to decouple control on head rotational degree of freedom making possible a metric le approach to the orientation problem a running implementation of these strategy using a binocular camera system mounted on a puma demonstrates the effectiveness of the approach 
in the paper we study a new and natural modal interpretation of default we show that under this interpretation there are whole family of modal nonmonotonic logic that accurately represent default reasoning one of these logic is used in a definition of possible world semantics for default logic this semantics yield a characterization of default extension similar to the characterization of stable expansion by mean of autoepistemic interpretation we also show that the disjunctive information can easily be handled if disjunction is represented by mean of modal disjunctive default modal formula that we use in our interpretation our result indicate that there is no single modal logic for describing default reasoning on the contrary there exist whole range of modal logic each of which can be used in the embedding a a host logic 
assumption based truth maintenance system have developed into powerful and popular mean for considering multiple context simultaneously during problem solving unfortunately increasing problem complexity can lead to explosive growth of node label in this paper we present a new atm algorithm catms which avoids the problem of label explosion while preserving most of the querytime efficiency resulting from label compilation catms generalizes the standard atm subsumption relation allowing it to compress an entire label into a single assumption this compression of label is balanced by an expansion of environment to include any implied assumption the result is a new dimension of flexibility allowing catms to trade off the query time efficiency of uncompressed label against the cost of computing them to demonstrate the significant computational gain of catms over de kleer s atm we compare the performance of the atm based qpe problem solver using each 
this paper present a method for interpreting metaphoric language in the context of a portable natural language inferface the method license metaphoric us via coercion between incompatible ontological sort the machinery allows both previously known and unexpected metaphoric us ot be correctly interpreted and evaluated with respect to the backend expert system 
quantitative prediction are typically obtained by characterizing a system in term of algebraic relationship and then using these relationship to compute quantitative prediction from numerical data for real life system such a mainframe operating system an algebraic characterization is often difficult if not intractable this paper proposes a statistical approach to obtaining quantitative prediction from monotone relationship non parametric interpolative prediction for monotone function nimf nimf us monotone relationship to search historical data for bound that provide a desired level of statistical confidence we evaluate nimf by comparing it prediction to those of linear least square regression a widely used statistical technique that requires specifying algebraic relationship for memory contention in an ibm computer system our result suggest that using an accurate monotone relationship can produce better quantitative prediction than using an approximate algebraic relationship 
classical constraint system require that the set of variable whichexist in a problem be known ab initio however there are some applicationsin which the existence of certain variable is dependent on conditionswhose truth or falsity can only be determined dynamically in this paper we show how this conditional existence of variable can be handled in amathematically well founded fashion by viewing a constraint network asa set of sentence in free logic based on these idea we have 
prosody can be useful in resolving certain lexical and structural ambiguity in spoken english in this paper we present some result of employing two type of prosodic information namely pitch and pause to assist syntactic and semantic analysis during parsing 
prosody can be useful in resolving certain lexical and structural ambiguity in spoken english in this paper we present some result of employing two type of prosodic information namely pitch and pause to assist syntactic and semantic analysis during parsing 
semantic disambiguation is a difficult problem in natural language analysis a better strategy for semantic disambiguation is to accumulate constraint obtained during the analytical process of a sentence and disambiguate a early a possible the meaning incrementally using the constraint we propose such a computational model of natural language analysis and call it the incremental disambiguation model the semantic disambiguation process can be equated with the downward traversal of a discrimination network however the discrimination network ha a problem in that it cannot be traversed unless constraint are entered in an a priori fixed order in general the order in which constraint are obtained cannot be a priori fixed so it is not always possible to traverse the network downward during the analytical process in this paper we propose a method which can traverse the discrimination network according to the order in which constraint are obtained incrementally during the analytical process this order is independent of the a priori fixed order of the network 
abstract this paper present a new method for producing a dictionary of subcategorization frame from unlabelled text corpus it is shown that statistical filtering of the result of a finite state parser running on the output of a stochastic tagger produce high quality result despite the error rate of the tagger and the parser further it is argued that this method can be used to learn all subcategorization frame whereas previous method are not extensible to a general solution to the problem 
in the functionally accurate cooperative fa c distributed problem solving paradigm agent exchange tentative and partial result in order to converge on correct solution the key question for fa c problem solving are how should cooperation among agent be structured and what capability are required in the agent to support the desired cooperation to date the fa c paradigm ha been explored with agent that did not have sophisticated evidential reasoning capability we have implemented a new framework in which agent maintain explicit representation of the reason why their hypothesis are uncertain and explicit representation of the state of the action being taken to meet their goal in this paper we will show that agent with more sophisticated model of their evidence and their problem solving state can support the complex dynamic interaction between agent that are necessary to fully implement the fa c paradigm our framework make it possible for agent to have directed dialogue among agent for distributed differential diagnosis make use of a variety of problem solving method in response to changing situation transmit information at different level of detail and drive local and global problem solving using the notion of the global consistency of local solution these capability have not been part of previous implementation of the fa c paradigm 
since linear resolution with clause ordering is incomplete for consequence finding it ha been used mainly for proof finding in this paper we re evaluate consequence finding firstly consequence finding is generalized to the problem in which only interesting clause having a certain property called characteristic clause should be found then we show how adding a skip rule to ordered linear resolution make it complete for consequence finding in this general sense compared with set of support resolution the proposed method generates fewer clause to find such a subset of consequence in the propositional case this is an elegant tool for computing the prime implicants implicates the importance of the result lie in their applicability to a wide class of ai problem including procedure for nonmonotonic and abductive reasoning and truth maintenance system 
a unified framework is developed for efficiently solving a wide range of computational vision problem by performing adaptive scale space tracking where a solution at a coarse resolution is tracked to solution at ever increasing resolution this approach is motivated by physical smoothness model deformable sheet based on thin elastic membrane and plate the inherent smoothness property of the deformable sheet act against externally applied problem specific force derived from image the author also developed the necessary relation for quantitative control of scale based parameter so that the scale space tracking process can be completely automated finally they present solution to different problem in computational vision using the framework applied to real image 
a visual language is defined equivalent in expressive power to term subsumption language expressed in textual form to each knowledge representation primitive there corresponds a visual form expressing it concisely and completely the visual language and textual language are intertranslatable expression in the language are graph of labeled node and directed or undirected arc the node are labeled textually or iconically and their type are denoted by six different outline computer readable expression in the language may be created through a structure editor that ensures that syntactic constraint are obeyed the editor export knowledge structure to a knowledge representation server computing subsumption and recognition and maintaining a hybrid knowledge base of concept definition and individual assertion the server can respond to query graphically displaying the result in the visual language in editable form knowledge structure can be entered directly in the editor or imported from knowledge acquisition tool such a those supporting repertory grid elicitation and empirical induction knowledge structure can be exported to a range of knowledge based system 
when intelligent agent who have different knowledge and capability must work together they must communicate the right information to coordinate their action developing technique for deciding what to communicate however is problematic because it requires an agent to have a model of a message recipient and to infer the impact of a message on the recipient based on that model we have developed a method by which agent build recursive model of each other where the model are probabilistic and decision theoretic in this paper we show how an agent can compute the impact of a message in term of how it increase or decrease it expected utility by treating the imperfect communication channel probabilistically our method allows agent to account for risk in committing to nonintuitive course of action and to compute the utility of acknowledging message 
this paper concern the task of removing redundant information from a given knowledge base and restructuring it in the form of a tree so a to admit efficient problem solving routine we offer a novel approach which guarantee the removal of all redundancy that hide a tree structure we develop a polynomial time algorithm that given an arbitrary constraint network generates a precise tree representation whenever such a tree can be extracted from the input network otherwise the fact that no tree representation exists is acknowledged and the tree generated may serve a a good approximation to the original network 
in this paper we introduce an extension of the probably approximately correct pac learning model to study the problem of learning inclusion hierarchy of concept sometimes called is a hierarchy from random example using only the hypothesis representation output over many different run of a learning algorithm we wish to reconstruct the partial order with respect to generality among the different target concept used to train the algorithm we give an efficient algorithm for this problem with the property that each run is oblivious of all other run each run can take place in isolation without access to any example except those of the current target concept and without access to the current pool of hypothesis representation thus additional mechanism providing shared information between run are not necessary for the inference of some nontrivial hierarchy 
in order to control the motion of a mobile robot it is necessary to have accurate egomotion parameter in addition egomotion parameter are useful in determining environmental depth and structure we present a computationally inexpensive method that rapidly and robustly determines both the translational vector and rotational component of robot motion through the use of an active camera we employ gaze control consisting of two type of camera motion first the camera fixates on an item in the environment while measuring motion parallax based on the measured motion parallax the camera then rapidly saccade to a different fixation point the algorithm iteratively seek out fixation point that are closer to the translational direction of motion rapidly converging so that the active camera will always point in the instantaneous direction of motion at that point the tracking motion of the camera is equal but opposite in sign to the robot s rotational component of motion experiment are carried out both in simulation and in the real world giving result that are close to the actual motion parameter of the robot 
this paper describes the application of explanation based learning a machine learning technique to the sri core language engine a large scale general purpose natural language analysis system the idea is to bypass normal morphological syntactic and 
the problem of finding a set of assumption which explain a given proposition is in general np hard even when the background theory is an acyclic horn theory in this paper it is shown that when the background theory is acyclic horn and it pseudo completion 
a model of plan recognition in discourse must be based on intended recognition distinguish each agent s belief and intention from the other s and avoid assumption about the correctness or completeness of the agent belief in this paper we present an algorithm for plan recognition that is based on the shared plan model of collaboration grosz and sidner lochbaum et al and that satisfies these constraint 
motivated by an anomaly in branch and bound bnb search we analyze it average case complexity we first delineate exponential v polynomial average case complexity of bnb when best first bnb is of linear complexity we show that depth first bnb ha polynomial complexity for problem on which best first bnb ha exponential complexity we obtain an expression for the heuristic branching factor next we apply our analysis to explain an anomaly in lookahead search on sliding tile puzzle and to predict the existence of an average case complexity transition of bnb on the asymmetric traveling salesman problem finally by formulating ida a costbounded bnb we show it average case optimality which also implies that rbfs is optimal on average 
this paper present a method for incrementally segmenting image over time using both intensity and motion information this is done by formulating a model of physically significant image resgions using local constraint on intensity and motion and then finding the optimal segmentation over time using an incremental stochastic minimization technique the result is a robust and dynamic segmentation of the scene over a sequence of image the approach ha a number of benefit first 
reification of proposition expressing state event and property ha been widely advocated a a mean of handling temporal reasoning in ai the author proposes that such reification is both philosophically suspect and technically unnecessary the reified theory of allen and shoham are examined and it is shown how they can be unreified the resulting loss of expressive power can be rectified by adopting davidson s theory in which event token rather than event type are reified this procedure is illustrated by mean of kowalski and sergot s event calculus the additional type reification of the latter system being excised by mean of a general procedure proposed by the author for converting type reification into token reification some example are given to demonstrate the expressive power of the resulting theory 
we present a new approach to developing fast and efficient knowledge representation system previous approach to the problem of tractable inference have used restricted language or incomplete inference mechanism problem include lack of expressive power lack of inferential power and or lack of a formal characterization of what can and cannot be inferred to overcome these disadvantage we introduce a knowledge compilation method we allow the user to enter statement in a general unrestricted representation language which the system compiles into a restricted language that allows for efficient inference since an exact translation into a tractable form is often impossible the system search for the best approximation of the original information we will describe how the approximation can be used to speed up inference without giving up correctness or completeness we illustrate our method by studying the approximation of logical theory by horn theory following the formal definition of horn approximation we present anytime algorithm for generating such approximation we subsequently discus extension to other useful class of approximation 
abstract in the last year many logic of nonmonotonicity have been developedusing various different formalism and axiomatizations whichmakes them very difficult to compare we develop a classificationscheme for these logic using only a few simple concept and axiomsbased on conditional logic property of partial pre order of possibleworlds and centering assumption our framework the p system allows u to discus the similarity main difference and possible extensionsof these 
prime implicates have become a widely used tool in ai the prime implicates of a set of clause can be computed by repeatedly resolving pair of clause adding the resulting resolvent to the set and removing subsumed clause unfortunately this brute force approach performs far more resolution step than necessary tison provided a method to avoid many of the resolution step and kean and tsiknis developed an optimized incremental version unfortunately both these algorithm focus only on reducing the number of resolution step required to compute the prime implicates the actual running time of the algorithm depends critically on the number and expense of the subsumption check they require this paper describes a method based on a simplification of kean and tsiknis algorithm using an entirely different data structure to represent the data base of clause the new algorithm us it form of discrimination net called try to represent the clausal data base which produce an improvement in running time on all known example with a dramatic improvement in running time on larger example 
in this article we outline a basic approach to treating metonymy properly in a multilingual machine translation system this is the first attempt at treating metonymy in an machine translation environment the approach is guided by the difference of acceptability of metonymy which were obtained by our comparative survey among three language english chinese and japanese the characteristic of the approach are a follows influence of the context individual and familiality with metonymy are not used an actual acceptability of each metonymic expression is not realized directly grouping metonymic example into pattern is determined by the acceptability judgement of the speaker surveyed a well a the analyst intuition the analysis and generation component treat metonymy differently using the pattern the analysis component accepts a wider range of metonymy than the actual result of the survey and the generation component treat metonymy more strictly than the actual result we think that the approach is a starting point for more sophisticated approach to translation in a multilingual machine translation environment 
this paper describes an application of an analytical learning technique plausible explanation based learning pebl that dynamically acquires search control knowledge for a constraint based scheduling system in general the efficiency of a scheduling system suffers because of resource contention among activity our system learns the general condition under which chronic contention occurs and us search control to avoid repeating mistake because it is impossible to prove that a chronic contention will occur with only one example traditional ebl technique are insufficient we extend classical ebl by adding an empirical component that creates search control rule only when the system gain enough confidence in the plausible explanation this extension to ebl wa driven by our observation about the behavior of our scheduling system when applied to the real world problem of scheduling task for nasa space shuttle payload processing we demonstrate the utility of this approach and provide experimental result 
this article present an algorithm for computing the exact aspect graph of an opaque solid bounded by a smooth algebraic surface orthographic projection is assumed the algorithm is based on a catalog of visual event available from singularity theory it us curve tracing cell decomposition homotopy continuation and ray tracing to construct the region of the view sphere delineated by visual event curve the algorithm ha been fully implemented and example are presented 
this paper introduces a new approach for computing the exact aspect graph of curved object observed under orthographic projection curve corresponding to various visual event partition the gaussian sphere into region where the image structure is stable a catalogue of these event for piecewise smooth object is available from singularity theory for a solid bounded by rational parametric patch and their intersection curve it is shown that each visual event is characterized by a system of n polynomial in n variable whose solution can be found by numerical curve tracing method combining these method with ray tracing it is possible to characterize the stable image structure within each region result from a preliminary implementation are presented 
diagnosis of multiple disorder can be made efficient using a new representation and algorithm based on symptom clustering the symptom clustering approach partition symptom into causal group in contrast to the existing candidate generation approach which assembles disorder or candidate symptom clustering achieves efficiency by generating aggregate of candidate rather than individual candidate and by representing them implicitly in a cartesian product form search criterion of parsimony subsumption and spanning narrow the symptom clustering search space and a problem reduction search algorithm explores this space efficiently experimental result on a large knowledge base indicate that symptom clustering yield a near exponential increase in performance compared to candidate generation 
the internal representation of the training pattern of multi layer perceptrons wa examined and we demonstrated that the connection weight between layer are effectively transforming the representation format of the information from one layer to another one in a meaningful way the internal code which can be in analog or binary form is found to be dependent on a number of factor including the choice of an appropriate representation of the training pattern the similarity between the pattern a well a the network structure i e the number of hidden layer and the number of hidden unit in each layer 
in this paper we report progress towards a flexible visually driven object manipulation system the aim is that a robot arm with a camera and gripper mounted on it tip should be able to transport object across an obstacle strewn environment our system is based on the analysis of moving image contour which can provide direct estimate of the shape of curved surface recently we have elaborated on this basis in two respect first we have developed real time visual tracking method using dynamic contour with lagrangian dynamic allowing direct generation of approximation to geodesic path around obstacle secondly we have built a d system for incremental active exploration of free space 
this paper extends shapiro s model inference system for synthesizing logic program from example of input output behavior a new refinement operator for clause generation based upon the decomposition of prolog program into skeleton basic prolog program with a well understood control flow and technique standard prolog programming practice is described shapiro s original system is introduced skeleton and technique are discussed and simple example are provided to familiarize the reader with the necessary terminology the model inference system equipped with this new refinement operator is compared and contrasted with the original version presented by shapiro the strength and weakness of applying skeleton and technique to synthesizing prolog program is discussed 
this paper present a heterogeneous asynchronous architecture for controlling autonomous mobile robot which is capable of controlling a robot performing multiple task in real time in noisy unpredictable environment the architecture produce behavior which is reliable task directed and taskable and reactive to contingency experiment on real and simulated realworld robot are described the architecture smoothly integrates planning and reacting by performing these two function asynchronously using heterogeneous architectural element and using the result of planning to guide the robot s action but not to control them directly the architecture can thus be viewed a a concrete implementation of agre and chapman s plan ascommunications theory the central result of this work is to show that completely unmodified classical ai programming methodology using centralized world model can be usefully incorporated into real world embedded reactive system 
this paper describes the result of using a reactive control software architecture for a mobile robot retrieval task in an outdoor environment the software architecture draw from the idea of universal plan and subsumption s layered control producing reaction plan that exploit low level competence a operator the retrieval task requires the robot to locate and navigate to a donor agent receive an object from the donor and return the implementation employ the concept of navigation template nats to construct and update an obstacle space from which navigation plan are developed and continually revised selective perception is employed among an infrared beacon detector which determines the bearing to the donor a real time stereo vision system which obtains the range and ultrasonic sensor which monitor for obstacle en route the perception routine achieve a robust controlled switching among sensor mode a defined by the reaction plan of the robot in demonstration run in an outdoor parking lot the robot located the donor object while avoiding obstacle and executed the retrieval task among a variety of moving and stationary object including moving car without stopping it traversal motion the architecture wa previously reported to be effective for simple navigation and pick and place task using ultrasonics thus the result reported herein indicate that the architecture will scale well to more complex task using a variety of sensor 
we consider the problem of object recognition via local geometric feature matching in the presence of sensor uncertainty occlusion and clutter we present a general formulation of the problem and a polynomial time algorithm which guarantee finding all geometrically feasible interpretation of the data modulo uncertainty in term of the model this formulation applies naturally to problem involving both d and d object 
a knowledge based system for text understanding will incorporate both lexical and encyclopaedic information the lexical information is the basis of the parsing process while the encyclopaedic information form the target representation and is used in the knowledge acquisition process this paper describes twig a text understanding system where these two knowledge base arc integrated into one representation there is some theoretical justification for this and it ha the advantage of reducing duplication of information in the system this integration also ha the advantage of making conceptual information available during the parsing process most of all this integration of diverse information form a natural basis for a blackboard architecture 
many current recognition system terminate a search once an interpretation that is good enough is found the author formally examines the combinatorics of this approach showing that choosing correct termination procedure can dramatically reduce the search in particular the author provides condition on the object model and the scene clutter such that the expected search is at most quartic the analytic result are shown to be in agreement with empirical data for cluttered object recognition these result imply that it is critical to use technique that select subset of the data likely to have come from a single object before establishing a correspondence between data and model feature 
in this paper we explore the idea of representing csps using technique from formal language theory the solution set of a csp can be expressed a a regular language we propose the minimized deterministic finite state automaton mdfa recognizing this language a a canonical representation for the csp this representation ha a number of advantage explicit enumerated constraint can be stored in lesser space than traditional technique implicit constraint and network of constraint can be composed from explicit one by using a complete algebra of boolean operator like and or not etc applied in an arbitrary manner such constraint are stored in the same way a explicit constraint by using mdfas this capability allows our technique to construct network of constraint incrementally after constructing this representation answering query like satisfiability validity equivalence etc becomes trivial a this representation is canonical thus mdfas serve a a mean to represent constraint a well a to reason with them while this technique is not a panacea for solving csps experiment demonstrate that it is much better than previously known technique on certain type of problem 
for free text search over rapidly evolving corpus dynamic update of inverted index is a basic requirement b tree are an effective tool in implementing such index the zipfian distribution of posting suggests space and time optimization unique to this task in particular we present two novel optimization merge update which performs better than straight forward block update and pulsing which significantly reduces space requirement without sacrificing performance inverted index 
effective monitoring of device supported patient in the intensive care unit icu is complex involving interpretation of many variable comparative evaluation of many therapy option and control of many patient management parameter even skilled clinician make error that limit the quality of care harm patient or cause life threatening situation a growing body of research aim to improve icu monitoring with computer technology most of this research fall in two area a short term engineering of practical solution to narrowly defined immediate problem e g smart alarm system or b basic research on fundamental issue potentially relevant to icu monitoring e g temporal reasoning by contrast our project aim to develop a more comprehensive intelligent agent having a broad range of capability to cooperate on the icu team we do not aim to produce a practical system suitable for near term deployment in the icu but rather a proof of concept an experimental system that a demonstrably performs and coordinate a range of intelligent reasoning task of use in icu monitoring b doe so reliably in a significant range of medical situation and c arguably will scale up to meet the comprehensive set of practical requirement with an appropriate development effort we have developed an experimental system called guardian which exhibit several of the required capability and utilizes an underlying architecture hypothesized to support the full range of required capability in this paper we describe the guardian system it architecture and it current knowledge base we describe it performance and summarize the result of preliminary evaluation finally we discus ongoing and planned research on guardian 
the author consider the problem of occlusion in computing stereo disparity from a pair of image usual approach to stereo matching e g area based feature based etc can perform poorly in the neighborhood of occluding boundary if no surface model are assumed qualitative improvement are possible based on condition derived from the geometry of occlusion a pair of correspondence process using information near an occlusion boundary to localize the boundary can determine the sign of associated depth discontinuity unambiguously the proposed method is able to identify the presence and extent of occlusion region and assign disparity in a consistent way near the occlusion region 
there are still very few system performing a similarity based learning and using a first order logic fol representation this limitation come from the intrinsic complexity of the learning process in fol and from the difficulty to deal with numerical knowledge in this representation in this paper we show that major learning process namely generalizatiorl and clustering can be solved in a homogeneous way by using a similarity measure a this measure is defined the similarity computation come down to a problem of solving a set of equation in several unknown the representation language used to express our example is a subset of fol allowing to express both quantitative knowledge and a relevance scale on the predicate 
a computational framework for extracting edge with anarbitrary profile function and keypoints such a corner vertex andterminations is presented using oriented filter with even and odd symmetrywe combine their convolution output to oriented energy resulting in aunified representation of edge line and combination thereof we derivean quot edge quality quot measure which allows to test the validity of a general edgemodel a detection scheme for keypoints is proposed based 
dynamic knowledge base are a fact of life in many artificial intelligence application using current technique however it is not always possible to provide the desired level of associative access to them whilst meeting real time or even near real time performance criterion this paper argues the case for a hardware associative storage system that us symbolic pattern matching a it access mechanism a working prototype of such a system designed a a co processor for a workstation host is then described the coprocessor is based on an array of custom designed vlsi smart memory chip these combine storage and search processing logic on the same die parallelism is exploited both on chip and between chip to yield a high system performance the paper concludes with some example of how this hardware can be used to support real application 
this paper investigates the effect of parallelism on blackboard system scheduling a parallel blackboard system is described that allows multiple knowledge source instantiation to execute in parallel using a shared memory blackboard approach new class 
we address the problem of d reconstruction of a set of heterogeneous edge primitive from two perspective view the edge primitive that are taken into account are contour point line segment quadratic curve and closed curve we illustrate the existence of analytic solution for the d reconstruction of the above edge primitive knowing the relative geometry between the two perspective view 
operational definition link scientific attribute to experimental situation prescribing for the experimenter the action and measurement needed to measure or control attribute value while very important in real science operational procedure have been neglected in machine discovery we argue that in the preparatory stage of the empirical discovery process each operational definition must be adjusted to the experimental task at hand this is done in the interest of error reduction and repeatability of measurement both small error and high repeatability are instrumental in theory formation we demonstrate that operational procedure refinement is a discovery process that resembles the discovery of scientific law we demonstrate how the discovery task can be reduced to an application of the fahrenheit discovery system a new type of independent variable the experiment refinement variable have been introduced to make the application of fahrenheit theoretically valid this new extension to fahrenheit us simple operational procedure a well a the system s experimentation and theory formation capability to collect real data in a science laboratory and to build theory of error and repeatability that are used to refine the operational procedure we present the application of fahrenheit in the context of dispensing liquid in a chemistry laboratory 
reinforcement learning algorithm when used to solve multi stage decision problem perform a kind of online incremental search to find an optimal decision policy the time complexity of this search strongly depends upon the size and structure of the state space and upon a priori knowledge encoded in the learner initial parameter value when a priori knowledge is not available search is unbiased and can be excessive cooperative mechanism help reduce search by providing the learner with shorter latency feedback and auxiliary source of experience these mechanism are based on the observation that in nature intelligent agent exist in a cooperative social environment that help structure and guide learning within this context learning involves information transfer a much a it doe discovery by trial and error two cooperative mechanism are described learning with an external critic or lec and learning by watching or lbw the search time complexity of these algorithm along with unbiased q learning are analyzed for problem solving task on a restricted class of state space the result indicate that while unbiased search can be expected to require time moderately exponential in the size of the state space the lec and lbw algorithm require at most time linear in the size of the state space and under appropriate condition are independent of the state space size altogether requiring time proportional to the length of the optimal solution path while these analytic result apply only to a restricted class of task they shed light on the complexity of search in reinforcement learning in general and the utility of cooperative mechanism for reducing search 
this paper describes the application of explanation based learning a machine learning technique to the sri core language engine a large scale general purpose natural language analysis system the idea is to bypass normal morphological syntactic and 
in this paper we describe a design of wafer scale integration for massively parallel memory based reasoning wsi mbr wsi mbr attains about million parallelism on a single inch wafer using the state of the art fabrication technology while wsi mbr is specialized to memory based reasoning which is one of the mainstream approach in massively parallel artificial intelligence research the level of parallelism attained far surpasses any existing massively parallel hardware combination of memory array and analog weight computing circuit enable u to attain super high density implementation with nanosecond order inference time simulation result indicates that inherent robustness of the memory based reasoning paradigm overcomes the possible precision degradation and fabrication defect in the wafer scale integration also the wsi mbr provides a compact desk top size massively parallel computing environment 
we present a model for flexible extruded object such a wire tube or grommet and demonstrate a novel self adjusting seven dimensional hough transform that derives and analyzes their three space curved ax from position and surface normal infor mation the method is purely local and is very cheap to compute the model considers such object a piecewise toroidal and decomposes the seven pa rameters of a torus into three nested subspace the structure of which counteract the error implicit in the analysis of object of great size and or small curvature we believe it is the first example of a parameter space structure designed to cluster ill con ditioned hypothesis together so that they can be easily detected and ignored this work complement existing shape from contour approach for analyz ing torus it us no edge information and it doe not require the solution of high degree non linear equa tions by iterative technique most of the result including the condition for the existence of more that one solution phantom anti torus have been verified using a symbolic mathematical analysis sys tem we present in the environment of the ibm convex system robust result on both synthetic cad cam range data the hasp of a lock and actual range data a knotted piece of coaxial cable and discus several system tuning issue 
consider the problem of exploring a large state space for a goal state although many such state may exist finding anyone state satisfying the requirement is sufficient all method known until now for conducting such search in parallel fail to provide consistent linear speedup over sequential execution the speedup vary between sublinear to superlinear and from run to run further adding processor may sometimes lead to a slow down rather than speedup giving rise to speedup anomaly we present prioritizing strategy which yield consistent linear speedup and requires substantially smaller memory over other method the performance of these strategy is demonstrated on a multiprocessor 
normally constraint network are undirected since constraint merely tell u which set of value are compatible and compatibility is a symmetrical relationship in contrast causal model use directed link conveying cause effect asymmetry in this paper we give a relational semantics to this directionality thus explaining why prediction is easy while diagnosis and planning are hard we use this semantics to show that certain relation posse intrinsic directionality similar to those characterizing causal influence we also use this semantics to decide when and how an unstructured set of symmetrical constraint can be configured so a to form a directed causal theory 
this paper is that in an active system which canfocus it attention these problem become rather simplified and do therefore allow for robustsolutions in particular simulated foveation 
a recent system foil construct horn clause program from numerous example compu tational efficiency is achieved by using greedy search guided by an information based heuristic greedy search tends to be myopic but de terminate term an adaptation of an idea in troduced by another new system golem ha been found to provide many of the benefit of lookahead without substantial increase in computation this paper sketch key idea from foil and golem and discus the use of determinate literal in a greedy search con text the efficacy of this approach is illus trated on the task of learning the quicksort procedure and other small but non trivial listmanipulation function 
in this paper we give a new demonstration in which it is proved that the symmetric exponential filter is the optimal edge detection filter in the criterion of the signal to noise ratio localization precision and unique maximum then we deduce the first and the second directional derivative operator for symmetric exponential filter and realize them by first order recursive algorithm and propose to detect the edge by maximum of gradient gef or by the zero crossing of second directional derivative along the gradient direction sdef 
ladkin and maddux lama showed how to interpret the calculus of time interval defined by allen all in term of representation of a particular relation algebra and proved that this algebra ha a unique countable representation up to isomorphism in this paper we consider the algebra an of n interval which coincides with allen s algebra for n and prove that an ha a unique countable representation up to isomorphism for all n we get this result which implies that the first order theory of an is decidable by introducing the notion of a weak representation of an interval algebra and by giving a full classification of the connected weak representation of an we also show how the topological property of the set of atom of an can be represented by a n dimensional polytope 
we formulate the dempster shafer formalism of belief function shafer in the spirit of logical inference system our formulation called the belief calculus explicitly avoids the use of set theoretic notation a such it serf a an alternative for the use of the dempster shafer formalism for uncertain reasoning 
taking example from english and french idiom this paper show that not only constituent structure rule but also most syntactic rule such a topicalization wh question pronominalization are subject to lexical constraint on top of syntactic and possibly semantic one we show that such puzzling phenomenon are naturally handled in a lexicalized formalism such a tree adjoining grammar the extended domain of locality of tag also allows one to jexicalize syntactic rule while defining them at the level of constituent structure 
this paper present a model of failure recovery from which we have designed and tested set of failure recovery method in the phoenix system we derive the model document it assumption and then test the validity of the assumption and prediction of the model we present three experiment one derives baseline for failure recovery in the phoenix environment the second compare the performance of two strategy for selecting failure recovery method the third compare the performance of an initial set of failure recovery method with a redesigned set that is predicted to have lower expected cost 
constraint satisfaction problem involve finding value for variable subject to constraint on which combination of value are permitted they arise in a wide variety of domain ranging from scene analysis to temporal reasoning we present a new representation for partial solution a cross product of set of value this representation can be used to improve the performance of standard algorithm especially when seeking all solution or discovering that none exist 
ebl can learn justified generalization from only one example when the domain theory is perfect however it doe not work when the domain theory is imperfect imperfectness of the domain theory can be classified into four level i e incomplete intractable inconsistent and non operational one it is necessary to unify ebl and sbl to solve these problem in this paper we propose a framework of an augmented ebl to handle plural example simultaneously we formalize it on logic program and introduce a concept of least ebg to extract similarity from plural example we discus on an approach to solve utility problem with the augmented ebl utility problem is a problem to learn more efficient description under complete tractable consistent but nonoperational domain theory we define operationality criterion with maximizing usage degree and minimizing backtracking number and show they increase partial monotonically by generalization since this partial monotinicity is not preferable to search operational generalization least ebgs are more operational than usual ebgs we design a simple incremental learner based on least ebgs and show it usefulness in recursive domain theory we also discus on other imperfect theory problem 
this paper describes the application of explanation based learning a machine learning technique to the sri core language engine a large scale general purpose natural language analysis system the idea is to bypass normal morphological syntactic and partly semantic processing for most input sentence instead using a set of learned rule explanation based learning is used to extract the learned rule automatically from sample sentence submitted by a user and thus tune the system for that particular user by indexing the learned rule efficiently it is possible to achieve dramatic speedup performance measurement were carried out using a training set of sentence and a separate test set of sentence all from the atis corpus a set of learned rule wa derived from the training set these rule covered percent of the test sentence and reduced the total processing time to a third an overall speed up of percent wa accomplished using a set of only learned rule 
in this paper we are interested in using a first order theorem prover to prove theorem that are formulated in some higher order logic to this end we present translation of higher order logic into first order logic with flat sort and equality and give a sufficient criterion for the soundness of these translation in addition translation are introduced that are sound and complete with respect to l henkin s general model semantics our higher order logic are based on a restricted type structure in the sense of a church they have typed function symbol and predicate symbol but no sort 
we introduce a framework for training architecture composed of several module this framework which us a statistical formulation of learning system provides a unique formalism for describing many classical connectionist algorithm a well a complex system where several algorithm interact it allows to design hybrid system which combine the advantage of connectionist algorithm a well a other learning algorithm 
it is often the case that linguistic and pictorial information are jointly provided to communicate information in situation where the text describes salient aspect of the picture it is possible to use the text to direct the interpretation i e labelling object in the accompanying picture this paper focus on the implementation of a multi stage system piction that us caption to identify human in an accompanying photograph this provides a computationally le expensive alternative to traditional method of face recognition it doe not require a pre stored database of face model for all people to be identified a key component of the system is the utilisation of spatial constraint derived from the caption in order to reduce the number of possible label that could be associated with face candidate generated by a face locator a rule based system is used to further reduce this number and arrive at a unique labelling the rule employ spatial heuristic a well a distinguishing characteristic of face e g male versus female the system is noteworthy since a broad range of ai technique are brought to bear ranging from natural language parsing to constraint satisfaction and computer vision 
we present a mapping from a class of default theory to sentence in propositional logic such that each model of the latter corresponds to an extension of the former using this mapping we show that many property of default theory can be determined by solving propositional satisfiability in particular we show how csp technique can be used to identify analyze and solve tractable subset of reiter s default logic 
this paper present an algorithm for detecting the occluding contour generated by a surface and for reconstructing depth along them it also describes an algorithm for computing the two main curvature of the surface in the neighborhood of the occluding contour we have used these algorithm on synthetic and real data 
the paper describes work on applying a general purpose natural language processing system to transfer based interactive translation transfer take place at the level of quasi logical form qlf a contextually sensitive logical form representation which is deep enough for dealing with cross linguistic difference theoretical argument and experimental result are presented to support the claim that this framework ha good property in term of modularity compositionality reversibility and monotonicity 
the self organization of recurrent feature discovery network isstudied from the perspective of dynamical system bifurcationtheory reveals parameter regime in which multiple equilibrium orlimit cycle coexist with the equilibrium at which the networksperform principal component analysis introductionoja made the remarkable observation that a simple model neuron with anhebbian adaptation rule develops into a filter for the first principal component ofthe input distribution 
in this paper we develop a method of matching and recognizing aerial road network image based on road network model we use attributed relational graph to describe image and model the correspondence are found using a relaxation labelling algorithm which optimises a criterion of similarity 
motion sensitive cell in the primary visual cortex are not selective to velocity but rather are directionally selective and tuned to spatiotemporal frequency this paper describes physiologically plausible theory for computing velocity from the output of spatiotemporally oriented filter and prof several theorem showing how to combine the output of a class of frequency tuned filter to detect local image velocity furthermore it can be shown grzywacz and yuille that the filter combination may simulate pattern cell in the middle temporal area mt while each filter simulates primary visual cortex cell this suggests that mt s role is not to solve the aperture problem but to estimate velocity from primary cortex information the spatial integration that account for motion coherence may be postponed to a later cortical stage 
abstract inspired by a visual motion detection model for the rabbit retina and by a computational architecture used for early audition in the barn owl we have designed a chip that employ a correlation model to report the one dimensional field motion of a scene in real time using subthreshold analog vlsi technique we have fabricated and successfully tested a transistor chip using a standard mosis process 
this paper describes a general approach for automatically programming a behavior based robot new behavior are learned by trial and error using a performance feedback function a reinforcement two algorithm for behavior learning are described that combine technique for propagating reinforcement value temporally across action and spatially across state a behavior based robot called obelix see figure is described that learns several component behavior in an example task involving pushing box an experimental study using the robot suggests two conclusion one the learning technique are able to learn the individual behavior sometimes outperforming a hand coded program two using a behavior based architecture is better than using a monolithic architecture for learning the box pushing task 
we describe how a behavior hierarchy can be used in a protocol that allows ai agent to discover and resolve interaction flexibly agent that initially do not know with whom they might interact use this hierarchy to exchange abstraction of their anticipated behavior by comparing behavior agent iteratively investigate interaction through more focused exchange of successively detailed information they can also modify their behavior along different dimension to either avoid conflict or promote cooperation we explain why our protocol give agent a richer language for coordination than they get through exchanging plan or goal and we use a prototype implementation to illustrate our protocol we argue that our hierarchical protocol for coordinating behavior provides a powerful representation for negotiation and can act a a common foundation for integrating theory about plan and organization 
we have analyzed sentence of spontaneous human computer speech data containing repair drawn from a total corpus of sentence we present here criterion and technique for automatically detecting the presence of a repair it location and making the appropriate correction the criterion involve integration of knowledge from several source pattern matching syntactic and semantic analysis and acoustic 
neural network with binary weight are very important from both the theoretical and practical point of view in this paper we investigate the learnability of single binary perceptrons and union of binary perceptron network i e an or of binary perceptrons where each input unit is connected to one and only one perceptron we give a polynomial time algorithm that pac learns these network under the uniform distribution the algorithm is able to identify both the network connectivity and the weight value necessary to represent the target function these result suggest that under reasonable distribution perceptron network may be easier to learn than fully connected network 
we address in this paper how to find cluster based on proximity and planar facet based on coplanarity from d line segment obtained from stereo the proposed method are efficient and have been tested with many real stereo data these procedure are indispensable in many application including scene interpretation object modeling and object recognition we show their application to d motion determination we have developed an algorithm based on the hypothesize and verify paradigm to register two consecutive d frame and estimate their transformation motion by grouping d line segment in each frame into cluster and plane we can reduce effectively the complexity of the hypothesis generation phase 
we present a general framework for plan recognition whose formulation is motivated by a general purpose algorithm for effective abduction the knowledge representation is a restricted form of first order logic which is made computationally explicit a a graph structure in which plan are manifest a a special kind of graph walk intuitively plan are fabricated by searching an action description graph for relevant connection amongst instance of observed action the class of plan for which our method is applicable is wider than those previously proposed a both recursive and optional plan component can be represented despite the increased generality the proposed message passing algorithm ha an asymptotic upper bound that is an improvement on previous related work 
we consider the approach to game playing where one look ahead in a game tree evaluates heuristically the probability of winning at the leaf and then propagates this evaluation up the tree we show that minimax doe not make optimal use of information contained in the leaf evaluation and in fact misvalues the position associated with all node this occurs because when actually playing a position down the game tree a player would be able to search beyond the boundary of the original search and so ha access to additional information the remark that such extra information will exist allows better use of the information contained in the leaf evaluation even though we do not have access to the extra information itself our analysis implies that while minimax is approximately correct near the top of the game tree near the bottom a formula closer to the probability product formula is better we propose a simple model of how deep search yield extra information about the chance of winning in a position within the context of this model we write down the formula for propagating information up the tree which is correct at all level we generalize our result to the case when the outcome at the leaf are correlated and also to game like chess where there are three possible outcome win lose and draw experiment demonstrate our formula s superiority to minimax and probability product in the game of kalah 
block world cube world ha been one of the most popular model domain in artificial intelligence search and planning the operation and effectiveness of alternative heuristic strategy both basic and complex can be observed easily in this domain we show that finding an optimal solution is np hard in an important variant of the domain and po cr ular extension this enlarges the range of mo el domain whose complexity ha been explored mathematically and it demonstrates that the complexity of search in block world is on the same level a for sliding block problem the traveling salesperson problem binpacking problem and the like these result also support the practice of using block world a a tutorial search domain in course on artificial intelligence to reveal both the value and limitation of heuristic search when seeking optimal solution 
we proposed a novel method to analyze a sequence of color image a series of colorimages are examined in a four dimensional space which we call the temporal color space whose ax are the three color ax rgb and one temporal axis the significance of thetemporal color space lie in it ability to represent the change of image color with time aconventional color space analysis yield the histogram of the color in an image only at aninstance of time conceptually the two reflection 
a methodology is presented whereby a nominal trajectory for an assembly operation computed from kinematic constraint alone is augmented with a fine motion strategy synthesized through uncertainty and force analysis insertion clearance and size tolerance are introduced into the assembly part model in parallel with the manual selection of a perturbed nominal trajectory in contact space the selection of small clearance and in turn small insertion angle allows u to linearize contact space about discrete point in the nominal trajectory contact state are represented a affine space in a generalized c space of model error and pose variable the feasibility of proposed command velocity to be executed in the presence of position control and model error is determined through an uncertainty analysis technique based on the forward projection of convex polytopes in contact space our approach further the automates the so called manual method of motion planning with uncertainty 
this paper investigates the semantics of conditional term rewriting system with negation which may not satisfy desirable property like termination it is shown that the approach used by fitting for prolog style logic program is applicable in this context a monotone operator is developed whose fixpoints describe the semantics of conditional rewriting several example illustrate this semantics for non terminating rewrite system which could not be easily handled by previous approach 
this paper describes the application of explanation based learning a machine learning technique to the sri core language engine a large scale general purpose natural language analysis system the idea is to bypass normal morphological syntactic and 
we characterize the complexity of several typical problem in propositional default logic in particular we examine the complexity of extension membership extension existence and extension entailment problem we show that the extension existence problem is p complete even for semi normal theory and that the extension membership and entailment problem are p complete and p complete respectively even when restricted to normal default theory these result contribute to our understanding of the computational relationship between propositional default logic and other formalism for nonmonotonic reasoning e g autoepistemic logic and mcdermott and doyle s nml a well a their relationship to problem outside the realm of nonmonotonic reasoning 
the retrieval capability of the signature file access method have become very attractive for many data processing application dealing with both formatted and unformatted data however performance is still a problem mainly when large file are used and fast response required in this paper a high performance signature file organization is proposed integrating the latest development both in storage structure and parallel computing architecture it combine horizontal and vertical approach to the signature file fragmentation in this way a new mixed decomposition scheme particularly suitable for parallel implementation is achieved the organization based on this fragmentation scheme is called fragmented signature file performance analysis show that this organization provides very good and relatively stable performance covering the full range of possible query for the same degree of parallelism it outperforms any other parallel signature file organization that ha been defined so far the proposed method also ha other important advantage concerning processing of dynamic file adaptability to the number of available processor load balancing and to some extent fault tolerant query processing 
we combine simple retrieval with domain specific validation of retrieved case to produce a useful practical tool for case based reasoning based on real world case we retrieve between three and six case over a wide range of new problem this represents a selectivity ranging from to compared to an average selectivity of only from simple retrieval alone 
in this paper we have sketched out an algorithm for reconstructing a sequence of overlapping range image based on two key constraint minimizing the local variation of curvature across adjacent view and minimizing the variation of motion parameter across adjacent surface point it operates without explicitly computing correspondence and without the invoking a global rigidity assumption preliminary result indicate that the resulting surface reconstruction is both robust and accurate 
two new theorem proving procedure for equational horn clause are presented the largest literal is selected for paramodulation in both strategy except that one method treat positive literal a larger than negative one and result in a unit strategy both use term ordering to restrict paramodulation to potentially maximal side of equation and to increase the amount of allowable simplification demodulation completeness is shown using proof ordering 
standard value function approach to finding policy for partially observablemarkov decision process pomdps are intractable for large model the intractabilityof these algorithm is due to a great extent to their generating an optimalpolicy over the entire belief space however in real pomdp problem most beliefstates are unlikely and there is a structured low dimensional manifold of plausiblebeliefs embedded in the high dimensional belief space 
we describe a program beatrix that can understand textbook physic problem specified by a combination of english text and a diagram the result of the understanding process is a unified internal model that represents the problem including information derived from both the english text and the diagram the system is implemented a two opportunistic coparsers one for english and one for diagram within a blackboard architecture a central problem is establishing coreference that is determining when part of the text and diagram refer to the same object constraint supplied by the text and diagram mutually reduce ambiguity in interpretation of the other modality 
we present an approach to grammar development where the task is decomposed into two separate subtasks the first task linguistic with the goal of producing a set of rule that have a large coverage in the sense that the correct parse is among the proposed par on a blind test set of sentence the second task is statistical with the goal of developing a model of the grammar which assigns maximum probability for the correct parse we give parsing result on text from computer manual 
abstract we present a general theory that capture the relationship between certain domain and negotiation mechanism the analysis make it possible to categorize precisely the kind of do main in which agent find themselves and to use the category to choose appropriate negoti ation mechanism the theory presented here both generalizes previous result and allows agent designer to characterize new domain ac curately the analysis thus serf a a critical step in using the theory of negotiation in realworld application weshow that in certain task oriented do main there exist distributed consensus mech anisms with simple and stable strategy that lead to efficient outcome even when agent have incomplete information about their envi ronment we also present additional novel re sults in particular that in concave domain u ing all or nothing deal no lying by an agent can be beneficial and that in subadditive do main there often exist beneficial decoy lie that do not require full information regarding the other agent s goal 
the curvature method developed by basri and ullman approximates the appearance of object with smooth bounding surface from different viewpoint for their recognition in this paper we analyze the curvature method we apply the method to ellipsoid and compute analytically the error obtained the error depends on the exact shape of the ellipsoid namely the relative length of it ax and it increase a the ellipsoid becomes deep elongated in the z direction we show that the error are usually small and that in general a small number of model is required to predict the appearance of an ellipsoid from all possible view 
this paper is a discussion of machine learning theory on empirically learning classification rule the paper proposes six myth in the machine learning community that address issue of bias learning a search computational learning theory occam s razor universal learning algorithm and interactive learning some of the problem raised are also addressed from a bayesian perspective the paper concludes by suggesting question that machine learning researcher should be addressing both theoretically and experimentally 
this paper present two method for adding domain knowledge to similarity based learning through feature construction a form of representation change in which new feature are constructed from relationship detected among existing feature in the first method domain knowledge constraint are used to eliminate le desirable new feature before they are constructed in the second method domain dependent transformation generalize new feature in way meaningful to the current problem these two us of domain knowledge are illustrated in citre where they are shown to improve hypothesis accuracy and conciseness on a tic tat toe classification problem 
for real world concept learning problem feature selection is important to speed up learning and to improve concept quality we review and analyze past approach to feature selection and note their strength and weakness we then introduce and theoretically examine a new algorithm rellef which selects relevant feature using a statistical method relief doe not depend on heuristic is accurate even if feature interact and is noise tolerant it requires only linear time in the number of given feature and the number of training instance regardless of the target concept complexity the algorithm also ha certain limitation such a nonoptimal feature set size way to overcome the limitation are suggested we also report the test result of comparison between relief and other feature selection algorithm the empirical result support the theoretical analysis suggesting a practical approach to feature selection for real world problem 
march computation on and among data set mapped to irregular non uniform aggregate of processing element pe is a very important but largely ignored problem in parallel vision processing associative processing is an effective mean of applying parallel processing to these computation but is often restricted to operating on one data set at a time what we propose is an additional level of parallelism we call multi associativity a a framework for performing associative computation on these data set simultaneously in this paper we introduce algorithm developed for the content addressable array parallel processor caapp to simulate efficiently within aggregate of pe simultaneously the associative algorithm typically supported in hardware at the array level some of the result are the efficient application of existing associative algorithm e g to arbitrary aggregate of pe in parallel and the development of new multi associative algorithm among them parallel prefix and convex hull the multi associative framework also extends the associative paradigm by allowing operation on and among aggregate themselves operation not defined when the entity in question is always an entire array two consequence are support of divide and conquer algorithm within aggregate and communication among aggregate the rest of the paper describes a mapping of multi associativity onto the caapp and numerous multi associative algorithm 
abstract we present the information theoretic derivation of a learn ing algorithm that cluster unlabelled data with linear discriminants i n contrast to method that try to preserve information about the input pat tern we maximize the information gained from observing the output of robust binary discriminator implemented with sigmoid node we derive a local weight adaptation rule via gradient ascent in this objectiv e demonstrate it dynamic on some simple data set relate our approach to previous work and suggest direction in which it may be extended 
a case based reasoner can frequently benefit from using piece of multiple previous case in the course of solving a single problem in our model case piece called snippet are organized around the pursuit of a goal and there are link between the piece that preserve the structure of reasoning the advantage of our representational approach include the step taken in a previous case can be followed a long a they are relevant since the connection between step are preserved there is easy access to all part of previous case so they can be directly accessed when appropriate 
we have shown a multiscale coarse to fine hierarchical matching of stereo pair which us adaptive smoothing to extract the matching primitive the number of matching primitive at coarse scale is small therefore reducing the number of potential match which in return increase the reliability of the matching result a dense disparity can be obtained at a fine scale where the density of edgels is very high the control strategy is very simple compared to other multiscale approach such a the one using gaussian scale space this result from the accuracy of edge detected by adaptive smoothing at different scale the simplicity of the control strategy is especially important for low level processing and make parallel implementation quite simple 
the paper present a novel method of robust skeletonization based on the voronoi diagram vd ofboundary point which is characterized by correct euclideanmetrics and inherent preservation of connectivity the regularization of the voronoi medial axis vma in the sense of blum s prairie fire analogy isdone by attributing each component of the vma witha measure of prominence and stability the resultingvoronoi skeleton vsk appear largely invariant withrespect to typical noise 
program such a bacon abacus coper kepler and others are designed to find functional relationship of scientific significance in numerical data without relying on the deep domain knowledge scientist normally bring to bear in analytic work whether these system actually perform a intended is an open question however to date they have been supported only by anecdotal evidence report that a desirable answer ha been found in one or more handselected and often artificial case in this paper i describe a function finding algorithm which differs radically from previous candidate in three respect first it concentrate rather on reliable identification of a few functional form than on heuristic search of an infinite space of potential relation second it introduces the use of distinction significance and lack of fitthree general concept of value in evaluating apparent functional relationship finally and crucially the algorithm ha been tested prospectively on an extensive collection of real scientific data set though i claim much le than previous investigator about the power of my approach these claim may be considered to a degree quite unfamiliar in function finding research a conclusively proven 
the author show that there are two basic way to eliminate rotation either you eliminate actual effective rotation r or abstract rotation r the transformation of visual direction p is decomposed into pure deformation pure strain and abstract rotation method can be grouped a to whether they eliminate actual or abstract rotation an interesting elimination of r can be done for equatorial motion field on the image hemisphere using equator normal flow having a moving sensor in a fixed environment then rotation can be eliminated by detecting the phase and amplitude of a sine wave hidden in a d signal the signal is a function of longitude both sparse velocity and normal flow for nearly equator parallel image contour can be used directly a input data this approach seems robust and can be used in conjunction with the algorithm by r c nelson and j aloimonos 
to make informed decision in a multiagent environment an agent need to model itself the world and the other agent including the model that those other agent might be employing we present a framework for recursive modeling that us possible world semantics and is based on extending the kripke structure so that an agent can model the information it think that another agent ha in each of the possible world which in turn can be modeled with kripke structure using recursive nesting we can define the propositional attitude of agent to distinguish between the concept of knowledge and belief through the three wise men example we show how our framework is useful for deductive reasoning and we suggest that it might provide a meeting ground between decision theoretic and deductive method for multiagent reasoning 
we use connectionist modeling to develop an analysis of stress system in term of ease of learnability in traditional linguistic analysis learnability argument determine default parameter setting based on the feasibilty of logically deducing correct setting from an initial state our approach provides an empirical alternative to such argument based on perceptron learning experiment using data from nineteen human language we develop a novel characterization of stress pattern in term of six parameter these provide both a partial description of the stress pattern itself and a prediction of it learnability without invoking abstract theoretical construct such a metrical foot this work demonstrates that machine learning method can provide a fresh approach to understanding linguistic phenomenon 
this paper present a model and an algorithm for the detection of specularities from lambertian reflection using multiple color image from different viewing direction the algorithm called spectral differencing is based on the lambertian consistency that color image irradiance from lambertian reflection at an object surface doe not change depending on viewing direction but color image irradiance from specular reflection or from a mixture of lambertian and specular reflection doe change the spectral differencing is a pixelwise parallel algorithm and it detects specularities by color difference between a small number of image without using any feature correspondence or image segmentation applicable object include uniformly or nonuniformly colored dielectric and metal under extended and multiply colored scene illumination experimental result agree with the model and the algorithm performs well within the limitation discussed 
conversation between two people is usually of mixed initiative with control over the conversation being transferred from one person to another we apply a set of rule for the transfer of control to set of dialogue consisting of a total of turn the application of the control rule let u derive domain independent discourse structure the derived structure indicate that initiative play a role in the structuring of discourse in order to explore the relationship of control and initiative to discourse process like centering we analyze the distribution of four different class of anaphora for two data set this distribution indicates that some control segment are hierarchically related to others the analysis suggests that discourse participant often mutually agree to a change of topic we also compared initiative in task oriented and advice giving dialogue and found that both allocation of control and the manner in which control is transferred is radically different for the two dialogue type these difference can be explained in term of collaborative planning principle 
when dictionary for specific application or subject field are derived from a text collection the frequency distribution of the term in the collection give information about the expected completeness of the dictionary if only a subset of the term in the collection is to be included in the dictionary the completeness of the dictionary can be optimized with respect to dictionary size in this paper formula for the relationship between the frequency distribution of the term in the collection and expected dictionary completeness are derived first we regard one dimensional dictionary where the non trivial term occurring in the text are to be included in the dictionary then we describe the case of two dimensional dictionary which are needed for example for automatic indexing with a controlled vocabulary here relationship between text term and descriptor from the prescribed vocabulary have to be stored in the dictionary for both case formula for the interpolation and extrapolation with respect to different collection size are derived we give experimental result for one dimensional dictionary and show how the completeness can be estimated and optimized 
we describe a generalization of equivalence between constraint set called weak equivalence this new equivalence relation take into account that not all variable have the same function in a constraint set and therefore distinguishes between restriction variable and intermediate variable we explore the property of weak equivalence and it underlying notion of weak implication with an axiomatic approach in addition a complete set of axiom for weak implication is presented with example derived from the declarative rule language rl we show the applicability of weak equivalence to constraint solving 
this paper investigates the effect of parallelism on blackboard system scheduling a parallel blackboard system is described that allows multiple knowledge source instantiation to execute in parallel using a shared memory blackboard approach new class 
we define a model theoretic reasoning formalism that is naturally implemented on symmetric neural network like hopfield network or boltzman machine we show that every symmetric neural network can be seen a performing a search for a satisfying model of some knowledge that is wired into the network s weight several equivalent language are then shown to describe the knowledge embedded in these network among them is propositional calculus extended by augmenting propositional assumption with penalty the extended calculus is useful in expressing default knowledge preference between argument and reliability of assumption in an inconsistent knowledge base every symmetric network can be described by this language and any sentence in the language is translatable into such a network a sound and complete proof procedure supplement the model theoretic definition and give an intuitive understanding of the nonmonotonic behavior of the reasoning mechanism finally we sketch a connectionist inference engine that implement this reasoning paradigm 
recent research suggests the utility of performing induction over explanation this process identifies commonality across explanation that cannot be extracted solely by explanation based technique this ha important implication for the correctness of learned knowledge flann and dietterich and a we show on the efficiency with which learned knowledge can be reused specifically we illustrate that inductive concept formation can abstract and organize explanatory knowledge for efficient reuse in a domain of algebra story problem 
effective reasoning about complex engineered device requires device model that are both adequate for the task and computationally efficient this paper present a method for constructing simple and adequate device model by selecting appropriate model for each of the device s component appropriate component model are determined by the context in which the device operates we introduce context dependent behavior cdbs a component behavior model representation for encapsulating contextual modeling constraint we show how cdbs are used in the model selection process by exploiting constraint from three source the structural and behavioral context of the component and the expected behavior of the device we describe an implemented program for selecting a simplest adequate model the input are the structure of the device the expected device behavior and a library of cdbs the output is a set of component cdbs forming a structurally and behaviorally consistent device model that achieves the expected behavior 
there is no consensus on how syntax and semantic pragmatic should interact in natural language processing this paper focus on one issue concerning interaction order of processing two approach are compared empirically an interleaved syntax first approach in which semantic interpretation is performed at intermediate point during parsing and a semantics first approach in which semantic consideration drive the rule selection process during parsing the study provides empirical evidence that the semantics first approach is more efficient than the syntax first approach in processing text in narrow domain 
this paper i will consider parsing a a discretecombinatorial problem which consists in constructing alabeled graph that satisfies a set of linguisticconstraints i will identify some property of linguisticconstraints which allow this problem to be solvedefficiently using constraint satisfaction algorithm ithen describe briefly a modular parsing algorithmwhich construct a syntactic graph using a set ofgenerative operation and applies a filtering algorithmto eliminate inconsistent 
programming robot is a tedious task so there is growing interest in building robot which can learn by themselves self improving which involves trial and error however is often a slow process and could be hazardous in a hostile environment by teaching robot how task can be achieved learning time can be shortened and hazard can be minimized this paper present a general approach to making robot which can improve their performance from experience a well a from being taught based on this proposed approach and other learning speedup technique a simulated learning robot wa developed and could learn three moderately complex behavior which were then integrated in a subsumption style so that the robot could navigate and recharge itself interestingly a real robot could actually use what wa learned in the simulator to operate in the real world quite successfully 
we present a new grammatical formalism called constraint dependency grammar cdg in which every grammatical rule is given a a constraint on word to word modification cdg parsing is formalized a a constraint satisfaction problem over a finite domain so that efficient constraint propagation algorithm can be employed to reduce structural ambiguity without generating individual parse tree the weak generative capacity and the computational complexity of cdg parsing are also discussed 
the surface reconstruction problem is formulated a a two stage reconstruction procedure the first stage is a robust local fit to the data in a multiresolution scheme and the second is a regularized least square fit with the addition of an adaptive mechanism in the smoothness functional in order to make the solution well behaved the author present the detail of the second stage in which they use the weighted bicubic spline a a surface representation in a regularization framework with a tikhonov stabilizer a the smoothness norm it is shown how the adaptive weight in the stabilizer help the surface bend across discontinuity by varying the energy of the surface 
we propose that ambiguous prepositional phrase attachment can be resolved on the basis of the relative strength of association of the preposition with noun and verb estimated on the basis of word distribution in a large corpus this work suggests that a distributional approach can be effective in resolving parsing problem that apparently call for complex reasoning 
this paper describes the application of explanation based learning a machine learning technique to the sri core language engine a large scale general purpose natural language analysis system the idea is to bypass normal morphological syntactic and 
abstract itsdefinition quot syndrome property are not alwaysof the static kind though sometimes dynamicproperties are also used e g an agent is theperceived instigator of the action since one of the desired characteristic of aroles system is the power to discriminate event another quot desired quot property being to offer aneasier selection of grammatical function therecognition of semantic role should be linked tothe interpretation of the event that is to their dymmic 
we describe arachne a concept formation system that us explicit constraint on tree structure and local restructuring operator to produce well formed probabilistic concept tree we also present a quantitative measure of tree quality and compare the system s performance in artificial and natural domain to that of cobweb a well known concept formation algorithm the result suggest that arachne frequently construct higher quality tree than cobweb while still retaining the ability to make accurate prediction 
we present in this paper a new deterministic and massively parallel algorithm for combinatorial optimization in a markov random field this algorithm is an extension of previous relaxation labeling by optimization algorithm first the a posteriori probability of a tentative labeling defined in term of a markov random field is generalized to continuous labelings this merit function of probabilistic vector is then convexified by changing it domain global optimization is performed and the maximum is tracked down while the original domain is restaured on an application to contextual pixel quantization it compare favorably to recent stochastic simulated annealing or deterministic graduated non convexity method popularized for low level vision 
robot performing complex task in rich environment need very good perception module in order to understand their situation and choose the best action robot planning system have typically assumed that perception wa so good that it could refresh the entire world model whenever the planning system needed it or whenever anything in the world changed unfortunately this assumption is completely unrealistic in many real world domain because perception is far too difficult robot in these domain cannot use the traditional planner paradigm but instead need a new system design that integrates reasoning with perception our research is aimed at showing how a robot can reason about perception how task knowledge can be used to select perceptual target and how this selection dramatically reduces the computational cost of perception 
a primary problem facing real world robot is the question of which sensing action should be performed at any given time it is important that an agent be economical with it allocation of sensing when sensing is expensive or when there are many possible sensing operation available sensing is rational when the expected utility from the information obtained outweighs the execution cost of the sensing operation itself this paper outline an approach to the efficient construction of plan containing explicit sensing operation with the objective of finding nearly optimal cost effective plan with respect to both action and sensing the scheduling of sensing operation in addition to the usual scheduling of physical action potentially result in an enornous increase in the computational complexity of planning our approach avoids this pitfall through strict adherence to a static sensing policy the approach based upon the markov decision process paradigm handle a significant amount of uncertainty in the outcome of action 
in this paper we analyze a particular model of control among intelligent agent that of nonabsolute control non absolute control involves a supervisor agent that issue order to a subordinate agent an example might be a human agent on earth directing the activity of a mar based semi autonomous vehicle both agent operate with essentially the same goal the subordinate agent however is assumed to have access to some information that the supervisor doe not have the agent is thus expected to exercise it judgment in following order i e following the true intent of the supervisor to the best of it ability after presenting our model we discus the planning problem how would a subordinate agent choose among alternative plan our solution focus on evaluating the distance between candidate plan 
in the following paper we view applying default reasoning a a construction of an argument supporting agent s belief this yield a slight reformulation of the notion of an extension for default theory the proposed formalism enjoys a property which we call rational maximization of belief 
a computer model of the musculoskeletal system of the lobstergastric mill wa constructed in order to provide a behavioral interpretationof the rhythmic pattern obtained from isolated stomatogastricganglion the model wa based on hill s muscle modeland quasi static approximation of the skeletal dynamic and couldsimulate the change of chewing pattern by the effect of neuromodulators the stomatogastric nervous systemthe crustacean stomatogastric ganglion stg is a circuit of 
this paper present a logically complete assumption based truth maintenance system atm that is part of a complex blast furnace computer aided piloting system this system is built on an efficient and logically complete propositional constraint solver that ha been successfully used for industrial application in computer aided design 
abstract the bayesian model comparison framework is reviewed and the bayesianoccam s razor is explained this framework can be applied to feedforwardnetworks making possible objective comparison between solutionsusing alternative network architecture objective choice of magnitudeand type of weight decay term quantied estimate of the error barson network parameter and on network output the framework also generatesa measure of the eective number of parameter 
abduction is an important inference process underlying much of human intelligent activity including text understanding plan recognition disease diagnosis and physical device diagnosis in this paper we describe some problem encountered using abduction to understand text and present some solution to overcome these problem the solution we propose center around the use of a different criterion called explanatory coherence a the primary measure to evaluate the quality of an explanation in addition explanatory coherence play an important role in the construction of explanation both in determining the appropriate level of specificity of a preferred explanation and in guiding the heuristic search to efficiently compute explanation of sufficiently high quality 
this paper describes the application of explanation based learning a machine learning technique to the sri core language engine a large scale general purpose natural language analysis system the idea is to bypass normal morphological syntactic and 
abstract this paper describes a new method based on the extended gaussian image gi which 
in this paper we present a purely semantic view on non monotonic reasoning we follow the direction pointed in and claim that any nonmonotonic logic can be viewed a a result of transforming some base standard logic by a selection strategy defined on model the generalized theory of model preference is shortly outlined here together with it use in modeling non monotonic belief 
a user may typically need to combine the strength of more than one system in order to perform a task in this paper we describe a component of the janus natural language interface that translates intensional logic expression representing the meaning of a request into executable code for each application program chooses which combination of application system to use and design the transfer of data among them in order to provide an answer the complete janus natural language system ha been ported to two large command and control decision support aid 
this paper present an enhanced model of plan based dialogue understanding most plan based dialogue understanding model derived from litman and allen assume that the dialogue speaker have access to the same domain plan library and that the active domain plan are shared by the two speaker we call these feature shared domain plan constraint these assumption however ae too strict to account for mixed initiative dialogue where each speaker ha a different set of domain plan that are housed in his or her own plan library and where an individual speaker s domain plan may be activated at any point in the dialogue we propose an extension to the litman and allen model by relaxing the shared domain plan constraint our extension improves the ability to track the currently active plan the ability to explain the planning behind speaker utterance and the ability to track which speaker control the conversational initiative in the dialogue 
understanding a text requires two basic task making inference at several level of knowledge and composing a global interpretation of the given text from those various type of inference since making inference at each level demand an extensive computation there have been several attempt to use parallel inference mechanism such a parallel marker passing pmp to increase the productivity of the inference mecha nism such a mechanism when used with many local processor is capable of making inference in parallel however it often pose a large burden on the task of composing the global interpretation by producing a number of meaningless inference which should be filtered out therefore the increased productivity of the inference mechanism cause the slow down of the task of forming the global interpretation and make it the bottleneck of the whole system our system true effectively solves this problem with the constrained marker passing mechanism the new mechanism not only allows the system to make necessary inference in parallel but also provides a way to compose the global interpretation in parallel therefore the system is truly parallel and doe not suffer from any single bottleneck 
consolidation is inferring the behavioral description of a device by composing the behavioral description of it component e g deriving the qualitative differential equation qdbs of a device from those of it component in previous work dormoy and raiman described the qualitative resolution rule which is a general rule for deriving qdes of combination of component however the qualitative resolution rule is intractable in general a a step toward understanding tractable qualitative reasoning i present a new qde resolution rule the qualitative difference resolution rule that support the tractable consolidation of component in which direction of flow is dependent on the sign of pressure difference pipe and container are general type of component that match this rule the pressure regulator example also match this rule 
this report discus what it mean to claim that a representation is an effective encoding of knowledge we first present dimension of merit for evaluating representation based on the view that usefulness is a behavioral property and is necessarily relative to a specified task we then provide method based on result from mathematical statistic for reliably measuring effectiveness empirically and hence for comparing different representation we also discus weak but guaranteed method of improving inadequate representation our result are an application of the idea of formal learning theory to concrete knowledge representation formalism 
this paper present a strengthened algorithm for temporal reasoning during plan recognition which improves on a straightforward application of allen s reasoning algorithm this is made possible by viewing plan a both hierarchical structure and temporal network a a result we can show how to use a constraint the temporal relation explicitly given in input to improve the result of plan recognition we also discus how to combine the given constraint with those prestored in the system s plan library to make more specific the temporal constraint indicated in the plan being recognised 
abstract recurrent cascade correlation rcc is a recurrent versio n of the cascade correlation learning architecture of fahlman and lebiere fahlman rcc can learn from example to map a sequence of input into a desired sequence of output new hidden unit with recurrent connection are added to the network one at a time a they are needed during training in effect the n etwork build up a finite state machine tailored specifically for the current problem rcc retains the advantage of cascade correlation fast learning good generalization automatic construction of a near min imal multi layered network and the ability to learn complex behavior through a sequence of simple lesson the power of rcc is demonstrated on two task learning a finite state grammar from example of legal string and learning to recognize character in morse code this research wa sponsored in part by the national science foundation contract eet the view and conclusion 
i describe a neural network which decomposes a set of input into a sequence of generative parameter it us a series of coupled parameter finding and removing network and requires the input to be in a particular temporal format 
term clustering and syntactic phrase formation are method for transforming natural language text both have had only mixed success a strategy for improving the quality of text representation for document retrieval since the strength of these method are complementary we have explored combining them to produce superior representation in this paper wc discus our implementation of a syntactic phrase generator a well a our preliminary experiment with producing phrase cluster these experiment show small improvement in retrieval effectiveness resulting from the use of phrase cluster but it is clear that corpus much larger than standard information retrieval test coliections will be required to thoroughly evaluate the use of thin technique 
cascade correlation is a new architecture and supervised l earning algorithm for artificial neural network instead of just adjusting the weight in a network of fixed topology cascade correlation begin with a minimal network then automatically train and add new hidden unit one by one creating a multi layer structure once a new hidden unit ha been added to the network it input side weight are frozen this unit then becomes a permanent feature detector in the network available for producing output or for creating other more complex feature detector the cascade correlation architecture ha several advantage over existing algorithm it learns very quickly the network de termines it own size and topology it retains the structure it ha built even if the training set change and it requires no back propagation of error signal through the connection of the network 
cascade correlation is a new architecture and supervised learning algorithm for artificial neural network instead of just adjusting the weight in a network of fixed topology cascade correlation begin with aminimal network then automatically train and add new hidden unit one by one creating a multi layerstructure once a new hidden unit ha been added to the network it input side weight are frozen this unitthen becomes a permanent feature detector in the network available for 
we describe a technique for improving problem solving performance by creating concept that allow problem state to be evaluated through an efficient recognition process a temporal difference td method is used to bootstrap a collection of useful concept by backing up evaluation from recognized state to their predecessor this procedure is combined with explanationbased generalization ebg and goal regression to use knowledge of the problem domain to help generalize the new concept definition this maintains the efficiency of using the concept and accelerates the learning process in comparison to knowledge free approach also because the learned definition may describe negative condition it becomes possible to use ebg to explain why some instance is not an example of a concept the learning technique ha been elaborated for minimax gameplaying and tested on a tic tat toe system t given only concept defining the end game state and constrained to a two ply search bound experiment show that t learns concept for achieving near perfect play t s total searching time including concept recognition is within acceptable performance limit while perfect play without the concept requires search taking well over time longer than t s 
a new algorithm for computing the hough transform ha been presented it us information present in the location of the feature point to reduce the generation of evidence in the transform plane the algorithm give improved performance compared with the standard hough transform the improvement is in computation time and memory allocation further advantage of using the algorithm are that peak detection is one dimensional and the end point of curve may be detected the algorithm is also inherently parallel 
case based teaching system like good human teacher tell story in order to help student learn a case based teaching system engages a student in a challenging task and monitor his action looking for opportunity to tell story that will assist the learning process in order to produce story at the appropriate moment a casebased teaching system must have a library of story that are indexed according to how they should be used and a set of reminding strategy to retrieve story when they are relevant in this paper i discus creanimate a biology tutor that us story to help teach elementary school student about animal morphology in particular i discus the reminding strategy and indexing scheme that enable the system to achieve it educational objective these reminding strategy are example remindings similarity based remindings and expectation violation remindings 
knowledge representation kr ha traditionally been thought of a the heart of artificial intelligence anyone who ha ever built an expert system a natural language system almost any ai system at all ha had to tackle the problem of representing it knowledge of the world despite it ubiquity for most of ai s history kr ha been a backstage activity but in the s it emerged a a field unto itself with it own burgeoning literature along with this growth the last decade ha seen major change in kr methodology important technical contribution and challenge to the basic assumption of the field i survey some of these development and then speculate about some of the equally interesting change that appear on the horizon i also look at some of the critical problem facing kr research in the near future both technical and sociological 
we present a new model for the perceptual reasoning involved in hand eye coordination and we show how this model can be developed into a control mechanism for a robot manipulator with a visual sensor this new approach overcomes the hig h computational cost the lack of robustness and the need for precise calibration that plague traditional approach at the heart of our model is the perceptual kinematic map pkm a direct mapping from the control space of the manipulator onto a space defined by a set of measurable image parameter by exploring it workspace the robot learns qualitatively the topology of it pkm and thus acquires the dexterity for future task in a striking parallel to biological system 
functional unification grammar fug are popular for natural language application because the formalism us very few primitive and is uniform and expressive in our work on text generation we have found that it also ha annoying limitation it is not suited for the expression of simple yet very common taxonomic relation and it doe not allow the specification of completeness condition we have implemented an extension of traditional functional unification this extension address these limitation while preserving the desirable property of fug it is based on the notion of typed feature and typed constituent we show the advantage of this extension in the context of a grammar used for text generation 
a domain model in savile represents the step involved in producing and processing financial data in a company using an ontology appropriate for several reasoning task in accounting and auditing savile is an implemented program that demonstrates the adequacy and appropriateness of this ontology of financial data processing for evaluating internal control designing test and other audit planning related task this paper discus the rationale syntax semantics and implementation of the ontology a it stand today 
a discussion is presented of the design of a system that can input a vision task specification and use it knowledge of the operation of mathematical morphology to automatically construct a procedure that can execute the task to do this the author develop a predicate calculus representation to describe the essence of the state of all the image that are created during the execution of the morphological procedure and the state of the relationship among them the author translate the english description of morphological procedure into predicate logic in so doing they gain an understanding of the goal of each procedure and the exact condition under which a procedure achieves it goal with this knowledge of the operation of mathematical morphology represented in predicate logic a search procedure can be used to automatically produce vision procedure 
belief revision for an intelligent system is usually computationally expensive here we tackle this problem by using focus in belief revision that is revision occurs only in a subset of belief under attention or in focus attention can be shifted within the belief base thus allowing use and revision of other subset of belief this attention shifting belief revision architecture show promise to allow efficient and natural revision of belief base 
most research in computer chess ha focussed on creating an excellent chess player with relatively little concern given to modelling how human play chess the research reported in this paper is aimed at investigating knowledge based chess in the context of building a prototype chess tutor umrao which help student learn how to play bishop pawn endgame in tutoring it is essential to take a knowledge based approach since student must learn how to manipulate strategic concept not how to carry out minimax search umrao us an extension of michic s advice language to represent expert and novice chess plan for any given endgame the system is able to compile the plan into a strategy graph which elaborates strategy both well formed and ill formed that student might use a they solve the endgame problem strategy graph can be compiled off line so that they can be used in real time tutoring we show that the normally rigid model tracing tutoring pardigm can be used in a flexible way in this domain 
the ida natural language generation system us a kl one type classifier to perform content determination surface realisation and part of text planning generation by classification allows ida to use a single representation and reasoning component for both domain and linguistic knowledge which is difficult for system based on unification or systemic generation technique 
the approach discussed in this contribution is an example for the interpretation of temporal variation in image sequence recorded by a translating camera the encouraging result show how obstacle can be detected in image sequence taken from a translating camea by evaluation of optical flow vector estimated with independently developed approach further development are necessary to extend the approach to more general motion and more complex environment 
temporal reasoning is widely used in ai especially for natural language processing existing method for temporal reasoning are extremely expensive in time and space because complete graph are used we present an approach of temporal reasoning for expert system in technical application that reduces the amount of time and space by using sequence graph a sequence graph consists of one or more sequence chain and other interval that are connected only loosely with these chain sequence chain are based on the observation that in technical application many event occur sequentially the uninterrupted execution of technical process for a long time is characteristic for technical application to relate the first interval in the application with the last one make no sense in sequence graph only these relation are stored that are needed for further propagation in contrast to other algorithm which use incomplete graph no information is lost and the reduction of complexity is significant additionally the representation is more transparent because the flow of time is modelled 
mitchell s version space approach to inductive concept learning ha been highly influential in machine learning a it formalizes inductive concept learning a a search problem to identify some concept definition out of a space of possible definition this paper lay out some theoretical underpinnings of version space it present the condition under which an arbitrary set of concept definition in a concept description language can be represented by boundary set which is a necessary condition for a set of concept definition to be a version space furthermore although version space can be intersected and unioned version space are simply set albeit with special structure the result need not be a version space this paper also present the condition under which such intersection and union of two version space yield a version space i e representable by boundary set finally the paper show how the resulting boundary set after intersection and union can be computed from the initial boundary set and prof the algorithm correct 
multiclass learning problem involve finding a definition for an unknown function f x whose range is a discrete set containing k value i e k class the definition is acquired by studying large collection of training example of the form xi f xi existing approach to this problem include a direct application of multiclass algorithm such a the decision tree algorithm id and cart b application of binary concept learning algorithm to learn individual binary function for each of the k class and c application of binary concept learning algorithm with distributed output code such a those employed by sejnowski and rosenberg in the nettalk system this paper compare these three approach to a new technique in which bch error correcting code are employed a a distributed output representation we show that these output representation improve the performance of id on the nettalk task and of backpropagation on an isolated letter speech recognition task these result demonstrate that error correcting output code provide a general purpose method for improving the performance of inductive learning program on multiclass problem 
we present our research on defining a correct semantics for forward chaining production system p program a correct semantics ensures that the execution of the program will not produce incorrect answer and execution will terminate it also ensures that the answer are consistent we define a class of stratified p program and propose an operational semantics for these program we define an operator tps which computes the operational fixpoint for the production of the stratified p program the fixpoint capture the meaning of the p program the theory that can be derived from the production of the p program may be inconsistent with the constraint that are also derived from the p program we can then view the constraint a modifying the theory so that the modified theory p is consistent with the constraint however the same answer are obtained in the operational semantics of the stratified p program or from the modified theory p 
we have developed and evaluated a set of tutor construction tool which enabled three computer naive educator to build test and modify an intelligent tutoring system the tool constitute a knowledge acquisition interface for representing and rapid prototyping both domain and tutoring knowledge a formative evaluation is described which lasted nearly two year and involved student this research aim to understand and support the knowledge acquisition process in education and to facilitate browsing and modification of knowledge result of a person hour analysis of throughput factor are provided along with knowledge representation and engineering issue for developing knowledge acquisition interface in education 
in previous paper we presented method for retrieving collocation from large sample of text we described a tool xtract that implement these method and able to retrieve a wide range of collocation in a two stage process these method a well a other related method however have some limitation mainly the produced collocation do not include any kind of functional information and many of them are invalid in this paper we introduce method that address these issue these method are implemented in an added third stage to xtract that examines the set of collocation retrieved during the previous two stage to both filter out a number of invalid collocation and add useful syntactic information to the retained one by combining parsing and statistical technique the addition of this third stage ha raised the overall precision level of xtract from to with a precision of in the paper we describe the method and the evaluation experiment 
recently the relationship between several form of default reasoning based on conditional default ha been investigated in particular the system based on e semantics preferential model and fragment of modally defined conditional logic have been shown to be equivalent these system form a plausible core for default inference but are too weak in general failing to deal adequately with irrelevance we propose an extension of the modal conditional logic in which one can express the truth of sentence at inaccessible possible world and show how this logic can be used to axiomatize a simple preference relation on the modal structure of this logic this preferential semantics is shown to be equivalent to entailment and rational closure we suggest that many metalogical system of default inference can be axiomatized within this logic using the notion of inaccessible world 
this paper present an algorithm which make use of tense interpretation to determine the intended temporal ordering between the state and event mentioned in a narrative this is done by maintaining a temporal focus and interpreting the tense of each new statement of the narrative with respect to this focus in particular we propose heuristic for determining the temporal ordering and constraint for characterizing coherent tense sequence the algorithm is further defended through experiment with naturally occurring example 
the output of a typical multi output classification network do not satisfy theaxioms of probability probability should be positive and sum to one this problem canbe solved by treating the trained network a a preprocessor that produce a feature vectorthat can be further processed for instance by classical statistical estimation technique we find that in case of interest neural network are and should be somewhat underdetermined because the training data is always 
this paper describes a contour extraction scheme which refines a roughly estimated initial contour to outline a precise object boundary in our approach mixture density description which are parametric description of decomposed sub region are obtained from region clustering using these description likelihood that a pixel belongs to the object and it background are evaluated unlike other active contour extraction scheme region and edge based estimation scheme are integrated into an energy minimization process using log likelihood function based on the mixture density description owing to the integration the active contour locates itself precisely to the object boundary for complex background image moreover c discontinuity of the contour is realized a change of the object sub region boundary the experiment show these advantage 
finding the l most probable explanation mpe of a given evidence se in a bayesian belief network is a process to identify and order a set of composite hypothesis his of which the posterior probability are the l largest i e pr h se pr h se pr hl se a composite hypothesis is defined a an instantiation of all the non evidence variable in the network it could be shown that finding all the probable explanation is a np hard problem previously only the first two best explanation i e l in a singly connected bayesian network could be efficiently derived without restriction on network topology and probability distribution this paper present an efficient algorithm for finding l mpe in singly connected network and the extension of this algorithm for multiply connected network this algorithm is based on a message passing scheme and ha a time complexity o lkn for singly connected network where l is the number of mpe to be derived k the length of the longest path in a network and n the maximum number of node state defined a the product of the size of the conditional probability table of a node and the number of the incoming outgoing arc of the node 
this paper present a new approach to constructive induction discrimination based constructive induction dbc which invents useful predicate in learning relation triggered by failure of selective induction dbc find a minimal set of variable forming a new predicate that discriminates between positive and negative example and induces a definition of the invented predicate if necessary it also induces subpredicates for the definition experimental result show that dbc learns meaningful predicate without any interactive guidance 
finding best explanation is often formalized in ai in term of minimal cost proof finding such proof is naturally characterized a a best first search of the proof tree actually a proof dag unfortunately the only known search heuristic for this task is quite poor in this paper we present a new heuristic a proof that it is admissible for certain successor function and some experimental result suggesting that it is a significant improvement over the currently used heuristic 
this paper describes a natural language processing system reinforced by the use of association of word and concept implemented a a neural network combining an associative network with a conventional system contributes to semantic disambiguation in the process of interpretation the model is employed within a and the advantage over conventional one are shown 
in most research on concept formation within machine learning and cognitive psychology the feature from which concept are built are assumed to be provided a elementary vocabulary in this paper we argue that this is an unnecessarily limited paradigm within which to examine concept formation based on evidence from psychology and machine learning we contend that a principled account of the origin of feature can only be given with a grounded model of concept formation i e with a model that incorporates direct access to the world via sensor and manipulator we discus the domain of process control a a suitable framework for research into such model and present a first approach to the problem of developing elementary vocabulary from perceptual sensor data 
a image analysis resort to increasingly powerful algorithm the processing time is correspondingly extended consequently system designer are constantly looking for new technology and new architecture capable of improving processing speed without increasing the complexity and the cost of the machine 
this paper present an analysis of purpose clause in the context of instruction understanding such analysis show that goal affect the interpretation and or execution of action lends support to the proposal of using generation and enablement to model relation between action and shed light on some inference process necessary to interpret purpose clause 
this paper present a unification procedure which eliminates the redundant copying of structure by using a lazy incremental copying approach to achieve structure sharing copying of structure account for a considerable amount of the total processing time several method have been proposed to minimize the amount of necessary copying lazy incremental copying lic is presented a a new solution to the copying problem it synthesizes idea of lazy copying with the notion of chronological dereferencing for achieving a high amount of structure sharing 
we introduce an adaptive search technique that speed up state space search by learning heuristic censor while searching the censor speed up search by pruning away mote and more of the space until a solution is found in the pruned space censor are learned by explaining dead end and other search failure to learn quickly the technique over generalizes by assuming that certain constraint ate preservable i e remain true on at least one solution path a recovery mechanism detects violation of this assumption and selectively relaxes learned censor the technique implemented in an adaptive problem solver named failsafe learns useful heuristic that cannot be learned by other reported method it effectiveness is indicated by a preliminary complexity analysis and by experimental result in three domain including one in which prodigy failed to learn eflective search control rule 
platt s resource allocation network ran platt a b is modified for a reinforcement learning paradigm and to quot restart quot existing hidden unit rather than adding new unit after restarting unit continue to learn via back propagation the resultingrestart algorithm is tested in a q learning network that learns tosolve an inverted pendulum problem solution are found faster onaverage with the restart algorithm than without it introductionthe goal of supervised learning is 
are we justified in inferring a general rule from observation that frequently confirm it this is the usual statement of the problem of induction the present paper argues that this question is relevant for the understanding of machine learning but insufficient research in machine learning ha prompted another more fundamental question the number of possible rule grows exponentially with the size of the example and many of them are somehow confirmed by the data how are we to choose effectively some rule that have good chance of being predictive we analyze if and how this problem is approached in standard account of induction and show the difficulty that are present finally we suggest that the explanation based learning approach and related method of knowledge intensive induction could be a partial solution to some of these problem and help understanding the question of valid induction from a new perspective 
we investigate the logical structure of concept generated by conjunction and disjunction over a monotonic multiple inheritance network where concept node represent linguistic category and link indicate basic inclusion isa and disjointness isnota relation we model the distinction between primitive and defined concept a well a between closed and open world reasoning we apply our logical analysis to the sort inheritance and unification system of hpsg and also to classification in systemic choice system 
system that discover empirical equation from data require large scale testing to become a reliable research tool in the central part of this paper we discus two convergence test for large scale evaluation of equation finder and we demonstrate that our system which we introduce earlier ha the desired convergence property our system can detect a broad range of equation useful in different science and can be easily expanded by addition of new variable transformation previous system such a bacon or abacus disregarded or oversimplified the problem of error analysis and error propagation leading to paradoxical result and impeding the true world application our system treat experimental error in a systematic and statistically sound manner it propagates error to the transformed variable and assigns error to parameter in equation it us error in weighted least square fitting in the evaluation of equation including their acceptance rejection and ranking and us parameter error to eliminate spurious parameter the system detects equivalent term variable and equation and it remove the repetition this is important for convergence test and system efficiency thanks to the modular structure our system can be easily expanded modified and used to simulate other equation finder 
one of the major focus of research in distributed artificial intelligence dai is the design of automated agent which can interact effectively in order to cooperate in problem solving negotiation is recognized a an important mean by which inter agent cooperation is achieved in this paper we suggest a strategic model of negotiation for n agent n that take the passage of time during the negotiation process itself into consideration change in the agent s preference over time will change their strategy in the negotiation and a a result the agreement they are willing to reach we will show that in this model the delay in reaching such agreement can be shortened and in some case avoided altogether 
we introduce method for identifying operator precondition that need not be expanded further the method are proved to be admissible that is they will not cause a solution to be missed when one exists in certain case the method also identify operator reformulations that increase the number of nonexpandable precondition this approach provides effective loop control in common situation moreover the computation required can be performed during a precompilation of the operator in a domain thus there is no significant additional run time overhead during planning 
delayed reinforcement learning is an attractive framework for the unsupervised learning of action policy for autonomous agent some existing delayed reinforcement learning technique have shown promise in simple domain however a number of hurdle must be passed before they are applicable to realistic problem this paper describes one such difficulty the input generalization problem whereby the system must generalize to produce similar action in similar situation and an implemented solution the g algorithm this algorithm is based on recursive splitting of the state space based on statistical measure of difference in reinforcement received connectionist backpropagation ha previously been used for input generalization in reinforcement learning we compare the two technique analytically and empirically the g algorithm s sound statistical basis make it easy to predict when it should and should not work whereas the behavior of back propagation is unpredictable we found that a previous successful use of backpropagation can be explained by the linearity of the application domain we found that in another domain g reliably found the optimal policy whereas none of a set of run of backpropagation with many combination of parameter did 
in traditional natural language system the channel of communication between the user and the system wa a narrow and constraining device in many area natural ian guage based information access requires the possibility for the user of exploring the domain and a system s comfortable habitability the present work present a rationale for building intelligent interface that combine natural language and hypermedia a new mean for human computer interaction and in particular give an outline of a prototype of this kind built for the exploration of italian fresco the alfresco interactive system 
market price mechanism from economics constitute a well understood framework for coordinating decentralized decision process with minimal communication walras is a general market oriented programming environment for the construction and analysis of distributed planning system based on general equilibrium theory the environment provides basic construct for defining computational market structure and a procedure for deriving their corresponding competitive equilibrium in a particular realization of this approach for a simplified form of distributed transportation planning we see that careful construction of the decision process according to economic principle can lead to effective decentralization and that the behavior of the system can be meaningfully analyzed in economic term 
this paper address the problem of bridging the gap between the field of knowledge renresentation kr and uncertain reasoning ur the proposed solution consists of a framework for representing uncertain knowledge in which two component one dealing with categorical knowledge and one dealing with uncertainty about this knowledge are singled out in this sense the framework is hybrid this framework is characterized in both modeltheoretic and proof theoretic term state of belief is represented by belief set defined in term of the functional approach to knowledge representation suggested by levesque example are given using first order logic and a minimal subset of m krypton for the kr side and a yes no trivial case and dempster shafer theory for the ur side 
this paper describes the development of an architecture and implementation of a graphical tracing system for the parallel logic programming language parlog novel feature of the architecture include a graphical execution model of parlog a range of representational technique that allow the user a choice of perspective and granularity of analysis and ongoing work on graphical tool that provide user defined visualisation of their program either before the program is run or afterwards by demonstration from a textual trace the aim of the architecture are threefold to aid program construction and debugging by providing an informative graphical trace of the program s execution to provide the user with a choice of representational technique at a preferred level of granularity and to allow user to define their own visualisation that more truly map onto their conception of the problem and which support the way they wish to view the execution information 
we show that the class of string language generated by linear context free rewriting system is equal to the class of output language of deterministic tree walking transducer from equivalence that have previously been established we know that this class of language is also equal to the string language generated by context free hypergraph grammar multicomponent tree adjoining grammar and multiple context free grammar and to the class of yield of image of the regular tree language under finite copying top down tree transducer 
an algorithm able to locate an object of revolution from it cad model and a single perspective image is proposed geometric property of object of revolution are used in order to simplify the localization problem the axis projection is first computed by a prediction verification scheme it enables to compute a virtual image in which the contour are symmetric a rough localization is done in this virtual image and then improved by an iterative process experiment with real image prove it robustness and it capability to deal with partially occluded contour 
in natural language processing ambiguity resolution is a central issue and can be regarded a a preference assignment problem in this paper a generalized probabilistic semantic model gpsm is proposed for preference computation an effective semantic tagging procedure is proposed for tagging semantic feature a semantic score function is derived based on a score function which integrates lexical syntactic and semantic preference under a uniform formulation the semantic score measure show substantial improvement in structural disambiguation over a syntax based approach 
this paper present an outline of a theory of agency that seek to integrate ongoing understanding planning and activity into a single model of representation and processing our model of agency rise out of three basic piece of work schank s structural model of memory organization schank hammond s work in case based planning and dependency directed repair hammond d and martin s work in direct memory access parsing martin we see this paper a a first step in the production of a memory based theory of agency the active pursuit of goal in the face of a changing environment that can exist within the computational constraint of a computer model 
this dissertation present a model of the human sentence interpretation process which attempt to meet criterion of adequacy imposed by the different paradigm of sentence interpretation these include the need to produce a high level interpretation to embed a linguistically motivated grammar and to be compatible with psycholinguistic result on sentence processing the model includes a theory of grammar called construction based interpretative grammar cig and an interpreter which us the grammar to build an interpretation for single sentence an implementation ofhte interpreter ha been built called sal sal is an on line interpreter reading word one at a time and updating a partial interpretation of the sentence after each constituent this constituent by constituent interpretation is more fine grained and hence more on line than most previous model sal is strongly interactionist in using both bottom up and top down knowledge in an evidential manner to access a set of construction to build interpretation it us a coherence based selection mechanism to choose among these candidate interpretation and allows temporary limited parallelism to handle local ambiguity sal s architecture is consistent with a large number of psycholinguistic result the interpreter embodies a number of strong claim about sentence processing one claim is uniformity with respect to both representation and process in the grammar a single kind of knowledge structure the grammatical construction is used to represent lexical syntactic idiomatic and semantic knowledge cig thus doe not distinguish between the lexicon the idiom dictionary the syntactic rule base and the semantic rule base uniformity in processing mean that there is no distinction between the lexical analyzer the parser and the semantic interpreter because these kind of knowledge are represented uniformly they can be accessed integrated and disambiguated by a single mechanism a second claim the interpreter embodies is that sentence processing is fundamentally knowledge intensive and expectation based the representation and integration of construction us many diverse type of linguistic knowledge similarly the access of construction is sensitive to top down and bottom up syntactic and semantic knowledge and the selection of construction is based on coherence with grammatical knowledge and the interpretation 
we define a restricted domain a class of discrete d convex shape we prove that there is a setof thirteen restricted domain fk k k g suchthat any given restricted domain k is expressible ask k phi phi k k phi phi k k phi delta delta delta phi phi k k where phi k i k i represents the k i fold dilation of k i and k isa translation we show that this entail a linear transformationfrom a thirteen dimensional space in which 
work in distributed artificial intelligence dai ha since it earliest year been concerned with negotiation strategy which can be used in building agent that are able to communicate to reach mutually beneficial agreement in this paper we suggest a strategic model of negotiation that take the passage of time during the negotiation process itself into consideration change in the agent s preference over time will change their strategy in the negotiation and a a resuit the agreement they are willing to reach we will show that in this model the delay in reaching agreement can be avoided 
this paper describes uunk a program designed to understand ungrammatical input while most previous work in the field ha relied on syntactic technique or sublanguage analysis to parse grammatical error uunk us a semantics driven algorithm to process such input the paper give a brief overview of link the unification based system upon which ulink is built special attention is given to those aspect of link which allow ulink to use semantics to process ill formed input the detail of ulink s algorithm are then discussed by considering two example the paper concludes with a discussion of related research and problem which remain to be solved 
we propose a method of extracting and describing the shape of feature from medical image which provides both a skeleton and boundary representation this method doe not require complete closed boundary nor regularly sampled edge point line between edge point are connected into boundary section using a measure of proximity alternatively or in addition known connectivity between point such a that available from traditional edge detector can be incorporated if known the resultant description are objectcentred and hierarchical in nature with an unambiguous mapping between skeleton and boundary section 
an approach to generation system design is described which support maximal expression of commonality across language within this approach it becomes natural to represent inherently multilingual grammar and semantics the approach rest on the linguistic notion of functional similarity and difference by capturing the function language need to perform we achieve a level of linguistic description which carry across language far more effectively than account that are structurally based we demonstrate the general principle implementation and benefit of the approach with respect to three unrelated language english chinese and japanese 
many current recognition system use constrained search to locate object in cluttered environment previous formal analysis ha shown the expected amount of search change from quadratic to exponential when data from more than one object is included terminating the search upon finding a good enough interpretation can make the search cubic only if the data is grouped into subset likely to have come from a single object this paper examines the combinatorics of determining that a candidate object is not present in the data and show this search is again exponential even with termination and grouping implying that the work needed to weed out incorrect model is exponential the analytic result agree with empirical data for cluttered object recognition 
we propose in this paper a new topological classification of point in d image this classification is based on two connected component number computed on the neighborhood of the point these number allow to classify a point a an interior or isolated border curve surface point or a different kind of junction 
iterative sentence such a mary knocked on the door four time john played the sonata every other day and mary wa often busy can be understood a asserting that some situation type is either repeated a certain number of time or with a certain frequency the semantic content of iterative sentence ha been standardly represented by some logical formula which quantifies over instance of a non iterative situation type the principal claim of this paper and the basis of the representation proposed in it is that we also require iterative situation type and instance in order to completely handle the range of possible interpretation of iterative sentence 
the repetitive behavior of a device or system can be described in two way a detailed description of one iteration of the behavior or a summary description of the behavior over many repetition this paper describes an implemented program called ai that transforms the first type of description into the second type ai deal only with behavior where each repetition change parameter by the same amount at present the summary consists of the symbolic average rate of change in parameter value and information on how those rate would be different if various constant and function had been different unlike some other approach ai doe not require that a repeating behavior be described in term of a set of differential equation two example of running ai are given one concern the human heart the other a steam engine 
structure from motion algorithm based on matched point like feature under orthographic projection are explored for use in analysing image motion from small rigid moving object for two frame analysis closed form npoint algorithm are devised that minimize image plane positional error the ba relief ambiguity is shown to exist for arbitrary object rotation the algorithm is applied to real image and good estimate of the projection of the axis of rotation on to the imageplane are obtained 
structure from motion algorithm based on matched point like feature under orthographic projection are explored for use in analysing image motion from small rigid moving object for two frame analysis closed from n point algorithm are devised that minimise image plane positional error the ba relief ambiguity is shown to exist for arbitrary object rotation the algorithm is applied to real image and good estimate of the projection of the axis of rotation onto the image plane are obtained 
this paper describes a new hardware algorithm for morpheme extraction and it implementation on a specific machine mex i a the first step toward achieving natural language parsing accelerator it also show the machine s performance time faster than a personal computer this machine can extract morpheme from character japanese text by searching an morpheme dictionary in second it can treat multiple text stream which are composed of character candidate a well a one text stream the algorithm is implemented on the machine in linear time for the number of candidate while conventional sequential algorithm are implemented in combinational time 
it ha been demonstrated here how some geometrical feature extracted from the brightness image of an object of revolution and coming from the perspective projection of line or point situated on the surface of the object can be used to find the spatial attitude of the object in the viewer coordinate system 
large vc dimension classifier can learn difficult task but are usuallyimpractical because they generalize well only if they are trained with hugequantities of data in this paper we show that even very high order polynomialclassifiers can be trained with a small amount of training data andyet generalize better than classifier with a smaller vc dimension thisis achieved with a maximum margin algorithm the generalized portrait 
we present an incremental configuration space c construction algmithm for mechanism described a collection of subassemblies of rigid part the input are the initial subassembly configuration and the subassembly cs partitioned into uniform motion region in which part contact are constant and motion are monotonic the output is a partition of the mechanism c into uniform motion region the algorithm optimizes c construction by incrementally enumerating and testing only the region reachable from the initial configuration we implement the algorithm for subassemblies whose uniform motion region are polyhedral or are of dimension two or lower the program construct the exact c when possible and an approximate c otherwise the approximate c usually is qualitatively correct and in good quantitative agreement with the true c the program cover most mechanism composed of linkage and fixed ax kinematic pair two subassembly type for which c construction program are available 
to speed up production system researcher have developed parallel algorithm that execute multiple instantiation simultaneously unfortunately without special control such system can produce result that could not have been produced by any serial execution we present and compare three different algorithm that guarantee a serializable result in such system our goal is to analyze the overhead that serialization incurs all three algorithm perform synchronization at the level of instantiation not rule and are targeted for shared memory machine one algorithm operates synchronously while the other two operate asynchronously of the latter two one synchronizes instantiation using compiled test that were determined from an omine analysis while the other us a novel locking scheme that requires no such analysis our examination of performance show that asynchronous execution is clearly faster than synchronous execution and that the locking method is somewhat faster than the method using compiled test moreover we predict that the synchronization and or locking needed to guarantee serializability will limit speedup no matter how many processor are used 
overfitting avoidance in induction ha often been treated a if it statistically increase expected predictive accuracy in fact there is no statistical basis for believing it will have this effect overfitting avoidance is simply a form of bias and a such it effect on expected accuracy depends not on statistic but on the degree to which this bias is appropriate to a problem generating domain this paper identifies one important factor that affect the degree to which the bias of overfitting avoidance is appropriate the abundance of training data relative to the complexity of the relationship to be induced and show empirically how it determines whether such method a pessimistic and cross validated cost complexity pruning will increase or decrease predictive accuracy in decision tree induction the effect of sparse data is illustrated first in an artificial domain and then in more realistic example drawn from the uci machine learning database repository 
an approach for introducing default reasoning into first order horn clause theory is described a default theory is expressed a a set of strict implication of the form n and a set of default rule of the form n where the i and are function free literal a partial order of set of formula is obtained from these set of strict and default implication default reasoning is defined with respect to this ordering and a set of contingent ground fact crucially only strict implication appear in this structure consequently the complexity of default reasoning is that of classical reasoning together with an attendant overhead for manipulating the structure this overhead is o n where n is the number of original formula hence for default in propositional horn clause form time complexity is o n m where m is the total length of the original formula the approach is sound in that default reasoning in this structure is proven to conform to that of an extant system for default reasoning 
the opening of a model signal with a convex zeroheightstructuring element is studied empirically experimentsare performed in which the input signalmodel parameter and the opening length are variedover an acceptable range and the corresponding greylevel distribution in the opened signal are fit to pearsondistributions next regression are used to relatethe pearson distribution parameter to the input parameter resulting in equation that may be used topredict the effect of an 
a pattern in the translation of locative prepositional phrase between english and spanish is presented a way of exploiting this pattern is proposed in the context of a multilingual machine translation system under development 
we present the following result about ida and related algorithm we show that ida is not asymptotically optimal in all of the case where it wa thought to be so in particular there are tree satisfying all of the condition previously thought to guarantee asymptotic optimality for ida such that ida will expand more than o n node where n is the number of node eligible for expansion by a we present a new set of necessary and sufficient condition to guarantee that ida expands o n node on tree on tree not satisfying the above condition there is no best first admissible tree search algorithm that run in s n n where n o memory and always expands o n node there are acyclic graph on which ida expands n node 
recently developed technique have improved the performance of production system several time over however these technique are not yet adequate for continuous problem solving in a dynamically changing environment to achieve adaptive real time performance in such environment we use an organization of distributed production system agent rather than a single monolithic production system to solve problem organization self design is performed to satisfy real time constraint and to adapt to changing resource requirement when overloaded individual agent decompose themselves to increase parallelism and when the load lightens the agent compose with each other to free hardware resource in addition to increased performance generalization of our composition decomposition approach provide several new direction for organization self design a pressing concern in distributed ai 
present seven set of laboratory result testing variable in term position ranking which produce a phrase effect by weighting the distance between proximate term result of the test conducted by this project are included covering variant term position algorithm sentence boundary stopword counting every pair testing field selection and combination of algorithm including collection frequency record frequency and searcher weighted the discussion includes the result of test by fagan and by croft the need for term stemming proximity a a precision device comparison with boolean and the quality of test collection 
production system are an established method for encoding knowledge in an expert system the semantics of production system language and the concomitant algorithm for their evaluation rete and treat enumerate the set of rule instantiation and then apply a strategy that selects a single instantiation for firing often rule instantiation are calculated and never fired in a sense the time and space required to eagerly compute these unfired instantiation is wasted this paper present preliminary result about a new match technique lazy matching the lazy match algorithm fold the selection strategy into the search for instantiation such that only one instantiation is computed per cycle the algorithm improves the worst case asymptotic space complexity of incremental matching moreover empirical and analytic result demonstrate that lazy matching can substantially improve the execution time of production system program 
in this paper we argue that a mobile robot s environment can be determined by computing local map surrounding feature point called fixation point these fixation point are obtained by searching the scene for point which present some interesting cue for robot navigation this d computation is based on a monocular active vision system composed of a camera mounted on a rotating table accurately controlled by a computer which gaze the fixation point a the robot move the system then computes the local map and update it with each new observation in order to increase it accuracy and robustness real experimentation in a complex indoor scene illustrates that the d scene coordinate can be obtained with a good accuracy by integrating several observation 
in a precedent bused domain one appeal to previous case to support a solution decision explanation or an argument expert typically use care in choosing case in precedent based domain and apply such criterion a case relevance prototypicality and importance in domain where both case and rule are used expert use an additional case selection criterion the generalization that a particular group of case support domain expert use their knowledge of case to forge the rule learned from those case in this paper we explore inductive learning in a mixed paradigm setting where both rule based and case based reasoning method are used in particular we consider how the technique of casebased reasoning in an adversarial precedent based domain can be used to aid a decision tree based classification algorithm for training set selection branching feature choice and induction policy preference and deliberate exploitation of inductive bias we focus on how precedentbased argumentation may inform the selection of training example used to build classification tree the resulting decision tree may then be reexpressed a rule and incorporated into the mixed paradigm system we discus the heuristic control problem involved in incorporating an inductive learner into cabaret a mixed paradigm reasoner finally we present an empirical study in a legal domain of the classification tree generated by various training set constructed by a case based reasoning module 
in this paper we propose a new approach to intensional semantics of term subsumption language we introduce concept algebra whose signature are given by set of primitive concept role and the operation of the language for a given set of variable standard result give u free algebra we next define for a given set of concept definition a term algebra a the quotient of the free algebra by a congruence generated by the definition the ordering on this algebra is called descriptive subsumption we also construct a universal concept algebra a a non well founded set given by the greatest fixed point of a certain equation the ordering on this algebra is called structural subsumption we prove there are unique mapping from the free algebra to each of these and establish that our method for classifying cycle in a term subsumption language krep consists of constructing accessible pointed graph representing term in the universal concept algebra and checking a simulation relation between term 
pablo is a nonlinear planner that reason hierarchically by generating abstract predicate pablo s abstract search space are generated automatically using predicate relaxation a new technique for defining hierarchy of abstract predicate for some domain this mechanism generates hierarchy that are more useful than those created by previous technique using abstraction can lead to substantial saving in computation time furthermore pablo can achieve a limited form of reactivity when reasoning with relaxed predicate these abstraction can be viewed a small reactive plan and our method a an approach to dynamically combining these into useful nonlinear plan 
the three tiered discourse representation defined in luperfoy is applied to multimodal human computer interface hci dialogue in the applied system the three tier are a linguistic analysis morphological syntactic sentential semantic of input and output communicative event including keyboard entered command language atom nl string mouse click output text string and output graphical event a discourse model representation containing one discourse object called a peg for each construct each guise of an individual under discussion and the knowledge base kb representation of the computer agent s belief system which is used to support it interpretation procedure i present evidence to justify the added complexity of this three tiered system over standard two tiered representation based on a cognitive process that must be supported for any non idealized dialogue environment e g the agent can discus construct not present in their current belief system including information decay and the need for a distinction between understanding a discourse and believing the information content of a discourse b linguistic phenomenon in particular context dependent np which can be partially or totally anaphoric and c observed requirement of three implemented hci dialogue system that have employed this three tiered discourse representation 
i argue that because of spelling and typing error and other property of typed text the identification of word and word boundary in general requires syntactic and semantic knowledge a lattice representation is therefore appropriate for lexical analysis i show how the use of such a representation in the clare system allows different kind of hypothesis about word identity to be integrated in a uniform framework i then describe a quantitative evaluation of clare s performance on a set of sentence into which typographic error have been introduced the result show that syntax and semantics can be applied a powerful source of constraint on the possible correction for misspelled word 
strategy are proposed for combining different kind of constraint in declarative grammar with a detachable layer of control information the added control information is the basis for parametrized dynamically controlled linguistic deduction a form of linguistic processing that permit the implementation of plausible linguistic performance model without giving up the declarative formulation of linguistic competence the information can be used by the linguistic processor for ordering the sequence in which conjuncts and disjuncts are processed for mixing depth first and breadth first search for cutting off undesired derivation and for constraint relaxation 
this paper is concerned with knowledge representation issue in machine learning in particular it present a representation language that support a hybrid analytical and similarity based classification scheme analytical classification is produced using a kl one like term subsumption strategy while similarity based classification is driven by generalization induced from a training set by an unsupervised learning procedure this approach can be seen a providing an inductive bias to the learning procedure thereby shortening the required training phase and reducing the brittleness of the induced generalization 
accurate computation of image motion enables theenhancement of image sequence motion computationin scene having multiple moving object is performedtogether with object segmentation by using aunique temporal integration approach 
effective mapping and retrieval are important issue in successful deployment of plan reuse strategy in this paper we present a domain independent strategy for ranking a set of plausible reuse candidate in the order of cost of modifying them to solve a new planning problem the cost of modification is estimated by measuring the amount of disturbance caused to the validation structure of a reuse candidate if it were to be reused in the new problem situation this strategy is more informed than the typical feature based retrieval strategy and is more efficient than the method which require partial knowledge of the nature of the plan for the new problem situation to guide the retrieval process we discus the implementation of this retrieval strategy in friar a framework for flexible reuse and modification in hierarchical planning 
we present a method of recovering shape from shadingthat solves directly for the surface height by using a discreteformulation of the problem we are able to achievegood convergence behavior by employing numerical solutiontechniques more powerful than gradient descentmethods derived from variational calculus because wedirectly solve for height we avoid the problem of findingan integrable surface maximally consistent with surfaceorientation furthermore since we do not need 
we demonstrate in this paper how certain form of rule based knowledge can be used to prestructure a neural network of normalized basis function and give a probabilistic interpretation of the network architecture we describe several way to assure that rule based knowledge is preserved during training and present a method for complexity reduction that try to minimize the number of rule and the number of conjuncts after training the re ned rule are extracted and analyzed 
the information state of an agent is changed when a text in natural language is processed the meaning of a text can be taken to be this information state change potential the inference of a consequence make explicit something already implicit in the premise i e that no information state change occurs if the assumed consequence text is processed after the given premise text have been processed elementary logic i e first order logic can be used a a logical representation language for text but the notion of a information state a set of possibility namely first order model is not available from the object language belongs to the meta language this mean that text with other text a part e g propositional attitude with embedded sentence cannot be treated directly traditional intensional logic i e modal logic allow via modal operator access to the information state from the object language but the access is limited and interference with extensional notion like standard identity variable etc is introduced this doe not mean that the idea present in intensional logic will not work possibly improved by adding a notion of partiality but rather that often a formalisation in the simple type theory with sort for entity and index making information state first class citizen like individual is more comprehensible flexible and logically well behaved 
a new algorithm is presented for recognising d polyhedral object in a d segmented image using local geometric constraint between d line segment result demonstrate the success of the algorithm at coping with poorly segmented image that would cause substantial problem for many current algorithm the algorithm adapts to use with either d line data or d polygonal object either case increase it efficiency the conventional approach of searching an interpretation tree and pruning it using local constraint is discarded the new approach accumulates the information available from the local constraint and form match hypothesis subject to two global constraint that are enforced using the competitive paradigm all stage of processing consist of many extremely simple and intrinsically parallel operation this parallelism mean that the algorithm is potentially very fast and contributes to it robustness it also mean that the computation can be guaranteed to complete after a known time 
learning an input output mapping from a set of example of the type that many neural network have been constructed to perform can be regarded a synthesizing an approximation of a multi dimensional function from this point of view this form of learning is closely related to regularization theory the theory developed in poggio and girosi show the equivalence between regularization and a class of threelayer network that we call regularization network or hyper basis function 
this paper describes the initial development of a natural language text processor a the first step in an inr dialogue by voice system the eventual system will accept natural spontaneous speech from user and produce response from the database in the form of synthetic speech this paper report result in processing the textual version of atis air travel information system query the current system programmed in c accepts a input the cleaned up text snor version of the spoken query and produce the desired official airline guide oag information a output it us only the word in the input text and not any punctuation mark on the assumption that such mark are difficult to obtain directly from speech input based on the training text data the system correctly interprets a large majority of the textual query 
there ha been recent interest in applying hillclimbing or iterative improvement method to constraint satisfaction problem an important issue for such method is the likelihood of encountering a non solution equilibrium locally optimal point we present analytic technique for determining the relative density of solution and equilibrium point with respect to these algorithm the analysis explains empirically observed data for the n queen problem and provides insight into the potential effectiveness of these method for other problem 
in recent year many researcher have investigated the use of markov random field mrfs for computer vision the computational complexity of the implementation ha been a drawback of mrfs in this paper we derive deterministic approximation to mrfs model all the theoretical result are obtained in the framework of the mean field theory from statistical mechanic because we use mrfs model the mean field equation lead to parallel and iterative algorithm one of the considered model for image reconstruction is shown to give in a natural way the graduate non convexity algorithm proposed by blake and zisserman 
a new method named dfd f of determining depth range from image defocus and rapid autofocusing of acamera is presented it requires only two image in theory but three image in our implementation in contrastwith a related prior method dfd f is based oncomputing only one dimensional fourier coecients a opposedtwo dimensional fourier coecients thus providingnot only computational advantage but also robustness inpractical application dfd f is independent of the formof the 
in constructing probabilistic network from human judgment we use causal relationship to convey useful pattern of dependency the converse task that of inferring causal relationship from pattern of dependency is far le understood this paper establishes condition under which the directionality of some interaction can be determined from non temporal probabilistic information an essential prerequisite for attributing a causal interpretation to these interaction an efficient algorithm is developed that given data generated by an undisclosed causal polytree recovers the structure of the underlying polytree a well a the directionality of all it identifiable link 
cyclic definition are often prohibited in terminological knowledge representation language because from a theoretical point of view their semantics is not clear and from a practical point of view existing inference algorithm may go astray in the presence of cycle in this paper we shall consider terminological cycle in a very small kl one based language for this language the effect of the three type of semantics introduced by nebel a can be completely described with the help of finite automaton these description provide a rather intuitive understanding of terminology with cyclic definition and give insight into the essential feature of the respective semantics in addition one obtains algorithm and complexity result for subsumption determination a it stand the greatest fixed point semantics come off best the characterization of this semantics is easy and ha an obvious intuitive interpretation furthermore important construct such a value restriction with respect to the transitive or reflexive transitive closure of a role can easily be expressed 
in this paper the application of an anthropomorphic retina like visual sensor for optical flow and depth estimation is presented the main advantage obtained with the non uniform sampling is the considerable data reduction while a high spatial resolution is preserved in the part of the field of view corresponding to the focus of attention 
the economic theory of rationality promise to equal mathematical logic in it importance for the mechaniz tion of reasoning we survey the growing literature on how the basic notion of probability utility and rational choice coupled with practical limitation on information and resource influence the design and analysis of reasoning and representation system 
when explanation include multiple medium such a text and illustration a reference to an object can be made through a combination of medium we call part of a presentation that reference material elsewhere a cross reference we are concerned here with how textual expression can refer to part of accompanying illustration the illustration to which a cross reference refers should also satisfy the specific goal of identifying an object for the user thus producing an effective cross reference not only involves text generation but may also entail modifying or replacing an existing illustration and in some case generating an illustration where previously none wa needed in this paper we describe the different type of cross reference that comet coordinated multimedia explanation testbed generates and show the role that both it text and graphic generator play in this process 
we have developed a conceptual framework and a demonstration system that contextualize or situate learning in the context of real world work situation the conceptual framework is based on the following requirement the choice of task and goal must be under the control of the user not the system the environment must be able to situate learning allow situation to talk back support reflection in action identify the instructional information relevant for task at hand and tum breakdown from disaster into opportunity for learning learning must not disrupt or interfere with solving a problem and new information to be learned must help to accomplish the task at hand our demonstration system janus developed for the domain of architectural design is built on an integrated architecture a knowledge based construction component a hypermedia based argumentation component a set of critic and a catalog of precedent solution contextualized learning is supported by the critic that link construction and argumentation and precedent solution from the catalog that situate argumentation evaluation of janus and the underlying conceptual framework have shown that this approach combine some of the best feature of open ended learning environment and tutoring system 
this paper present a tripartite model of dialogue in which three different kind of action are modeled domain action problem solving action and discourse or communicative action we contend that our process model provides a more finely differentiated representation of user intention than previous model enables the incremental recognition of communicative action that cannot be recognized from a single utterance alone and account for implicit acceptance of a communicated proposition 
the construction of a program that generates crossword puzzle is discussed a in a recent paper by dechter and meiri we make an experimental comparison of various search technique the conclusion to which we come differ from theirs in some area although we agree that directional arc consistency is better than path consistency or other form of lookahead and that backjumping is to be preferred to backtracking we disagree in that we believe dynamic ordering of the constraint to be necessary in the solution of more difficult problem 
this paper concern hardware support for a fast vision engine running an edge based stereo vision system low level processing task are discussed and candidate for hardware acceleration are identified 
we present two method for detecting symmetry in image one based directly on the intensity value and another one based on a discrete representation of local orientation a symmetry finder ha been developed which us the intensity based method to search an image for compact region which display some degree of mirror symmetry due to intensity similarity across a straight axis in a different approach we look at symmetry a a bilateral relationship between local orientation a symmetryenhancing edge detector is presented which indicates edge dependent on the orientation at two different image position seed a we call it is a detector element implemented by a feedforward network that hold the symmetry condition we use seed to find the contour of symmetric object of which we know the axis of symmetry from the intensity based symmetry finder the method presented have been applied to the problem of visually guided car following real time experiment with a system for automatic headway control on motorway have been successful 
we describe a method for obtaining subject dependent word set relative to some subject domain using the subject classification given in the machine redable version of longman s dictionary of contemporary english we established subject dependent co occurrence link between word of the defining vocabulary to construct these neighborhood here we describe the application of these neighborhood to information retrieval and present a method of word sense disambiguation based on these co occurrence an extension of previous work 
this paper describes the application of explanation based learning a machine learning technique to the sri core language engine a large scale general purpose natural language analysis system the idea is to bypass normal morphological syntactic and 
transparency produce visual ambiguity in interpreting motion and stereo recent discovery of a general framework principle of super position for building constraint equation of transparency make it possible to analyze the mathematical property of transparency perception this paper theoretically examines multiple ambiguous interpretation in transparent optical flow and transparent stereo 
the recovery of the d structure of indoor scene from a single image is an important goal of machine vision therefore a simple and reliable solution to this problem will have a great influence on many task in robotics such a the autonomous navigation of a mobile vehicle in indoor environment 
a system is described for semi automatically tagging a large body of technical english with domain specific syntactic semantic label these label have been used to disambiguate prepositional phrase attachment for a word body of text containing more than preposition and to provide case role information for about half of the phrase 
a mechanical assembly is usually described by the geometry of it part and the spatial relation defining their position this model doe not directly provide the information needed to reason about assembly and disassembly motion we propose another representation the non directional blocking graph which describes the qualitative internal structure of the assembly this representation make explicit how the part prevent each other from being moved in every possible direction of motion it derives from the observation that the infinite set of motion direction can be partitioned into a finite arrangement of subset such that over each subset the interference among the part remain qualitatively the same we describe how this structure can be efficiently computed from the geometric model of the assembly the dis assembly motion considered include infinitesimal and extended translation in two and three dimension and infinitesimal rigid motion 
the author present a physically based approach to fitting complex three dimensional shape using a novel class of dynamic model that can deform both locally and globally they formulate the deformable superquadrics which incorporate the global shape parameter of a conventional superellipsoid with the local degree of freedom of a spline the model s six global deformational degree of freedom capture gross shape feature from visual data and provide salient part descriptor for efficient indexing into a database of stored model the local deformation parameter reconstruct the detail of complex shape that the global abstraction miss the equation of motion which govern the behavior of deformable superquadrics make them responsive to externally applied force the author fit model to visual data by transforming the data into force and simulating the equation of motion through time to adjust the translational rotational and deformational degree of freedom of the model model fitting experiment involving d monocular image data and d range data are presented 
much distributed artificial intelligence research on negotiation assumes complete knowledge among the interacting agent and or truthful agent these assumption in many domain will not be realistic and this paper extends previous work to begin dealing with the case of inter agent negotiation with incomplete information a discussion of our existing negotiation framework set out the rule by which agent operate during this phase of their interaction the concept of a solution within this framework is presented the same solution concept serf for interaction between agent with incomplete information a it did for complete information interaction the possibility of incomplete information among agent open up the possibility of deception a part of the negotiation strategy of an agent deception during negotiation among autonomous agent is thus analyzed in the constrained block domain and it is shown that beneficial lie do exist in some scenario the three type of interaction cooperative compromise and conflict are examined an analysis is made of how each affect the possibility of beneficial deception by a negotiating agent 
in this paper we compare two grammar based generation algorithm the semantic head driven generation algorithm shdga and the essential argument algorithm eaa both algorithm have successfully addressed several outstanding problem in grammar based generation including dealing with non monotonic compositionality of representation left recursion deadlock prone rule and nondeterminism we concentrate here on the comparison of selected property generality efficiency and determinism we show that eaa s traversal of the analysis tree for a given language construct include also the one taken on by shdga we also demonstrate specific and common situation in which shdga will invariably run into serious inefficiency and nondeterminism and which eaa will handle in an efficient and deterministic manner we also point out that only eaa allows to treat the underlying grammar in a truly multi directional manner 
human explanatory dialogue is an activity in which participant interactively construct explanatory model of the topic phenomenon however current explanation planning technology doe not support such dialogue in this paper we describe contribution in the area of discourse planning architecture heuristic for knowledge communication and user interface design that take step towards addressing this problem first our explanation planning architecture independently applies various constraint on the content and organization of explanation avoiding the inflexibility and contextual assumption of schematic discourse plan second certain planning operator simulate a human explainer s effort to choose and incrementally develop model of the topic phenomenon third dialogue occurs in the medium of a live information interface designed to serve a the representational medium through which the activity of the machine and human are coupled collectively these contribution facilitate interactive model construction in human machine dialogue 
it is known from biological data that the response pattern of interneurons in the olfactory macroglomerulus mgc of insect are of central importance for the coding of the olfactory signal we propose an analytically tractable model of the mgc which allows u to relate the distribution of response pattern to the architecture of the network 
many existing learning method use incre mental algorithm that construct a general ization in one pas through a set of training data and modify it in subsequent pass e g perceptrons neural net and decision tree most of these method do not store the en tire training set in essence employing a limited storage requirement that abstract the notion of a compressed representation the question we address is how much additional processing time is required for method with limited stor age processing time for learning algorithm is equated in this paper with the number of pass necessary through a data set to obtain a correct generalization for instance neural net require many pass through a data set before converging decision tree require fewer pass but precise bound are unknown we consider limited storage algorithm for a particular concept class nested hyperrectangles we prove bound that illustrate the fundamental trade off between storage require ments and processing time required to learn an optimal structure it turn out that our lower bound apply to other algorithm and concept class e g decision tree a well notably imposing storage limitation on the learning task force one to devise a completely different algorithm to reduce the number of pass we also briefly discus parallel learning algorithm 
in this paper we develop high order non biased spatial derivative operator with subpixel accuracy our approach is discrete and provides a way to obtain some of the spatio temporal parameter from an image sequence in this paper we concentrate on spatial parameter 
this paper describes the application of explanation based learning a machine learning technique to the sri core language engine a large scale general purpose natural language analysis system the idea is to bypass normal morphological syntactic and 
the task of inferring a set of class and class description most likely to explain a given data set can be placed on a firm theoretical foundation using bayesian statistic within this framework and using various mathematical and algorithmic approximation the auto class system search for the most probable classification automatically choosing the number of class and complexity of class description simpler version of autoclass have been applied to many large real data set have discovered new independently verified phenomenon and have been released a a robust software package recent extension allow attribute to be selectively correlated within particular class and allow class to inherit or share model parameter though a class hierarchy 
projective distortion of surface texture observed in a perspective image can provide direct information about the shape of the underlying surface previous theory have generally concerned planar surface in this paper we present a systematic analysis of firstand second order texture distortion cue for the case of a smooth curved surface in particular we analyze several kind of texture gradient and relate them to surface orientation and surface curvature the local estimate obtained from these cue can be integrated to obtain a global surface shape and we show that the two surface resulting from the well known tilt ambiguity in the local foreshortening cue typically have qualitatively different shape a an example of a practical application of the analysis a shape from texture algorithm based on local orientation selective filtering is described and some experimental result are shown 
in a new incremental cascade network architecture ha beenpresented this paper discus the property of such cascadenetworks and investigates their generalization ability under theparticular constraint of small data set the evaluation is done forcascade network consisting of local linear map using the mackeyglasstime series prediction task a a benchmark our result indicatethat to bring the potential of large network to bear on theproblem of extracting information from 
this paper describes an approach to student modeling for intelligent tutoring system based on an explicit representation of the tutor s belief about the student and the argument for and against those belief called endorsement a lexicographic comparison of argument sorted according to evidence reliability provides a principled mean of determining those belief that are considered true false or uncertain each of these belief is ultimately justified by underlying assessment data the endorsement based approach to student modeling is particularly appropriate for tutor controlled by instructional planner these tutor place greater demand on a student model than opportunistic tutor numeric calculus approach are le well suited because it is difficult to correctly assign number for evidence reliability and rule plausibility it may also be difficult to interpret final result and provide suitable combining function when numeric measure of uncertainty are used arbitrary numeric threshold are often required for planning decision such an approach is inappropriate when robust context sensitive planning decision must be made instead the ability to examine belief and justification is required this paper present a tm based implementation of the endorsement based approach to student modeling discus the advantage of this approach for planner controlled tutor and compare this approach to alternative 
we designed implemented and evaluated a new concept for visualizing and searching database utilizing direct manipulation called dynamic query dynamic query allow user to formulate query by adjusting graphical widget such a slider and see the result immediately by providing a graphical visualization of the database and search result user can find trend and exception easily user testing wa done with eighteen undergraduate student who performed significantly faster using a dynamic query interface compared to both a natural language system and paper printout the interface were used to explore a real estate database and find home meeting specific search criterion 
a method is presented that cause a to return high quality solution while solving a set of problem using a non admissible heuristic the heuristic guiding the search change a new information is learned during the search and it converges to an admissible heuristic which contains the insight of the original nonadmissible one after a finite number of problem a return only optimal solution experiment on sliding tile problem suggest that learning occurs very fast beginning with hundred of randomly generated problem and an overestimating heuristic the system learned sufficiently fast that only the first problem wa solved non optimally a an application we show how one may construct heuristic for finding high quality solution at lower cost than those returned by a using available admissible heuristic 
the use of abstraction in problem solving is an effective approach to reducing search but finding good abstraction is a difficult problem even for people this paper identifies a criterion for selecting useful abstraction describes a tractable algorithm for generating them and empirically demonstrates that the abstraction reduce search the abstraction learner called alpine is integrated with the prodigy problem solver minton et al b carbonell et al and ha been tested on large problem set in multiple domain 
a novel architecture is presented for combining rule based and case based reasoning the central idea is to apply the rule to a target problem to get a first approximation to the answer but if the problem is judged to be compellingly similar to a known exception of the rule in any aspect of it behavior then that aspect is modelled after the exception rather than the rule the architecture is implemented for the full scale task of pronouncing surname preliminary result suggest that the system performs almost a well a the best commercial system however of more interest than the absolute performance of the system is the result that this performance wa better than what could have been achieved with the rule alone this illustrates the capacity of the architecture to improve on the rule based system it start with the result also demonstrate a beneficial interaction in the system in that improving the rule speed up the case based component 
this paper describes an initial exploration into large learning system i e system that learn a large number of rule given the well known utility problem in learning system efficiency question are a major concern but the question are much broader than just efficiency e g will the effectiveness of the learned rule change with scale this investigation us a single problem solving and learning system dispatcher soar to begin to get answer to these question dispatcher soar ha currently learned new production on top of an initial system of production so it total size is production this represents one of the largest production system in existence and by far the largest number of rule ever learned by an ai system this paper present a variety of data from our experiment with dispatcher soar and raise important question for large learning system 
during the postwar heyday of physic c p snow wrote a short article entitled the two culture there he pointed out the growing division between the science culture and the non science literary culture he observed that scientist basically had no understanding of nay even any concern for literary culture and vice versa he pointed out the profound loss to society that wa resulting from this dichotomy namely creativity often arises in the interchange of idea sadly the two culture were so polarized even then that snow felt that little real dialogue took place between member of the two culture 
in this paper we propose a framework for integrating fault diagnosis and incremental knowledge acquisition in connectionist expert system a new case solved by the diagnostic function is formulated a a new example for the learning function to learn incrementally the diagnostic function is composed of a neural network based example module and a symbolic based rule module while the example module is always first invoked to provide the shortcut solution the rule module provides extensive coverage of case to handle odd case when example module fails two application based on the proposed framework will also be briefly mentioned 
the objective of this paper is to present a significant improvement to the approach of duncan et al to analyze the deformation of curve in sequence of d image this approach is based on the paradigm that high curvature point usually posse an anatomical meaning and are therefore good landmark to guide the matching process especially in the absence of a reliable physical or deformable geometric model of the observed structure a duncan s team we therefore propose a method based on the minimization of an energy which tends to preserve the matching of high curvature point while ensuring a smooth field of displacement vector everywhere the innovation of our work stem from the explicit description of the mapping between the curve to be matched which ensures that the resulting displacement vector actually map point belonging to the two curve which wa not the case in duncan s approach we have actually implemented the method in d and we present the result of the tracking of a heart structure in a sequence of ultrasound image 
there are many planning application that require an agent to coordinate it activity with process that change continuously over time several proposal have been made for combining a temporal logic of time with the differential and integral calculus to provide a hybrid calculus suitable for planning application we take one proposal and explore some of the issue involved in implementing a practical system that derives conclusion consistent with such a hybrid calculus model for real valued parameter are specified a system of ordinary differential equation and construct are provided for reasoning about how these model change over time for planning problem that require projecting the consequence of a set of event from a set of initial condition and causal rule a combination of numerical approximation and symbolic math routine and a simple default reasoning strategy provide for an efficient inference engine 
in modeling the structure of task related discourse using plan it is important to distinguish between plan that the agent ha adopted and is pursuing and those that are only being considered and explored since the kind of utterance arising from a particular domain plan and the pattern of reference to domain plan and movement within the plan tree are quite different in the two case this paper present a three level discourse model that us separate domain and exploration layer in addition to a layer of discourse metaplans allowing these distinct behavior pattern and the plan adoption and reconsideration move they imply to be recognized and modeled 
researcher in both machine translation e g brown et al and bilingual lexicography e g klavans and tzoukermann have recently become interested in studying bilingual corpus body of text such a the canadian hansard parliamentary proceeding which are available in multiple language such a french and english one useful step is to align the sentence that is to identify correspondence between sentence in one language and sentence in the other language this paper will describe a method and a program align for aligning sentence based on a simple statistical model of character length the program us the fact that longer sentence in one language tend to be translated into longer sentence in the other language and that shorter sentence tend to be translated into shorter sentence a probabilistic score is assigned to each proposed correspondence of sentence based on the scaled difference of length of the two sentence in character and the variance of this difference this probabilistic score is used in a dynamic programming framework to find the maximum likelihood alignment of sentence it is remarkable that such a simple approach work a well a it doe an evaluation wa performed based on a trilingual corpus of economic report issued by the union bank of switzerland ubs in english french and german the method correctly aligned all but of the sentence moreover it is possible to extract a large subcorpus that ha a much smaller error rate by selecting the best scoring of the alignment the error rate is reduced from to there were more error on the english french subcorpus than on the english german subcorpus showing that error rate will depend on the corpus considered however both were small enough to hope that the method will be useful for many language pair to further research on bilingual corpus a much larger sample of canadian hansard approximately million word half in english and and half in french ha been aligned with the align program and will be available through the data collection initiative of the association for computational linguistics acl dci in addition in order to facilitate replication of the align program an appendix is provided with detailed c code of the more difficult core of the align program 
it ha long been recognized that hierarchical problem solving can be used to reduce search yet there ha been little analysis of the problem solving method and few experimental result this paper provides the first comprehensive analytical and empirical demonstration of the effectiveness of hierarchical problem solving first the paper show analytically that hierarchical problem solving can reduce the size of the search space from exponential to linear in the solution length and identifies a sufficient set of assumption for such reduction in search second it present empirical result both in a domain that meet all of these assumption a well a in domain in which these assumption do not strictly hold third the paper explores the condition under which hierarchical problem solving will be effective in practice 
we have developed two system fn and andd that use natural language and graphical display respectively to communicate information about object to human user both system must deal with the fundamental problem of ensuring that their output doe not carry unwanted and inappropriate conversational implicatures we describe the type of conversational implicatures that fn and andd can avoid and the computational strategy the two system use to generate output that is free of unwanted implicatures 
many real world planning problem involve substantial amount of domain specific reasoning that is either awkward or inefficient to encode in a general purpose planner previous approach for planning in such domain have either been largely domain specific or have employed shallow model of the domain specific consideration in this paper we investigate a hybrid planning model that utilizes a set of specialist to complement both the overall expressiveness and the reasoning power of a traditional hierarchical planner such a model retains the flexibility and generality of classical planning framework while allowing deeper and more efficient domain specific reasoning through specialist we describe a preliminary implementation of a planning architecture based on this model in a manufacturing planning domain and use it to explore issue regarding the effect of the specialist on the planning and the interaction and interface between them and the planner 
a formal equivalence between propositional expert system and decision table is proved and a practicable procedure given to perform the transformation between propositional expert system and decision table the method gave an order of magnitude speed increase for a well known expert system in routine use the method is very general adaptation are shown for forward and backward chaining inferencing engine inexact reasoning and system where some fact have a high cost and must be determined only if necessary a particular application for the decision table representation is in real time expert system since a simple hardware implementation is available which give further order of magnitude increase in performance finally the decision table representation greatly simplifies the problem of completeness and consistency checking 
we report on the development and application of an efficient unsupervised learning procedure for the classification of an unsegmented datastream given a set of probabilistic binary similarity judgment between region in the stream our method is effective on very large database and tolerates the presence of noise in the similarity judgement and in the extent of similar region we applied this method to the problem of finding the sequence level building block of protein after verifying the effectiveness of the clusterer by testing it on synthetic protein data with known evolutionary history we applied the method to a large protein sequence database a datastream of more than element and found about protein sequence class the motif defined by these class are of biological interest and have the potential to supplement or replace the existing manual annotation of protein sequence database 
we develop a formalism for reasoning with default that are expressed with different level of firmness necessary and sufficient condition for consistency are established and a unique ranking of the rule is found called z which render model a normal a possible subject to the consistency condition we provide the necessary machinery for testing consistency computing the z ranking and drawing the set of plausible conclusion it entail 
speedup learning seek to improve the efficiency of search based problem solver in this paper we propose a new theoretical model of speedup learning which capture system that improve problem solving performance by solving a user given set of problem we also use this model to motivate the notion of batch problem solving and argue that it is more congenial to learning than sequential problem solving our theoretical result are applicable to all serially decomposable domain we empirically validate our result in the domain of eight puzzle 
this paper report a handful of experiment designed to test the feasibility of applying well known partial parsing technique to the problem of automatic data base update from an open ended source of message and the feasiblity of automatically learning semantic knowledge from annotated example the challenge arise from the incompleteness of any lexicon sentence that average over word in length and the lack of a complete semantics 
the class of linear context free rewriting system ha been introduced a a generalization of a class of grammar formalism known a mildly context sensitive the recognition problem for linear context free rewriting language is studied at length here presenting evidence that even in some restricted case it cannot be solved efficiently this entail the existence of a gap between for example tree adjoining language and the subclass of linear context free rewriting language that generalizes the former class such a gap is attributed to crossing configuration a few other interesting consequence of the main result are discussed that concern the recognition problem for linear context free rewriting language 
integration of language constraint into a large vocabulary speech recognition system often lead to prohibitive complexity we propose to factor the constraint into two component the first is characterized by a covering grammar which is small and easily integrated into existing speech recognizers the recognized string is then decoded by mean of an efficient language post processor in which the full set of constraint is imposed to correct possible error introduced by the speech recognizer 
a recognition system is reported which recognizes name spelled over thetelephone with brief pause between letter the system us separateneural network to locate segment boundary and classify letter theletter score are then used to search a database of name to find the bestscoring name the speaker independent classification rate for spoken lettersis the system retrieves the correct name spelled with pausesbetween letter of the time from a database of name 
symmetric network that are based on energy minimization such a boltzmann machine or hopfield net are used extensively for optimization constraint satisfaction and approximation of np hard problem nevertheless finding a global minimum for the energy function is not guaranteed and even a local minimum may take an exponential number of step we propose an improvement to the standard activation function used for such network the improved algorithm guarantee that a global minimum is found in linear time for tree like subnetworks the algorithm is uniform and doe not assume that the network is a tree it performs no worse than the standard algorithm for any network topology in the case where there are tree growing from a cyclic subnetwork the new algorithm performs better than the standard algorithm by avoiding local minimum along the tree and by optimizing the free energy of these tree in linear time the algorithm is self stabilizing for tree cycle free undirected graph and remains correct under various scheduling demon however no uniform protocol exists to optimize tree under a pure distributed demon and no such protocol exists for cyclic network under central demon 
this paper investigates the effect of parallelism on blackboard system scheduling a parallel blackboard system is described that allows multiple knowledge source instantiation to execute in parallel using a shared memory blackboard approach new class 
an information retrieval model is presented for the retrieval of speech document i e audio recording containing speech the indexing vocabulary consists of indexing feature that have the following characteristic first they are easy to recognize by speech recognition method second the number of different indexing feature is small such that a reasonable amount of training data is sufficent to train the hidden markov model that are used by the speech recognition process third the retrieval method based on such indexing feature achieves an acceptable retrieval effectiveness a shown by experiment on text collection fourth these indexing feature cannot only be identified in speech document but also in text document from the last characteristic follows that speech document and text document can be retrieved simultaneously analogously the query may contain either speech or text thus we have a simple multimedia retrieval model where two different medias are indexed coherently we also describe a prototype retrieval system under development 
we present a parallel perceptual organization scheme based on a novel ridge detector that work without edge the scheme computes perceptual group and potential point which can serve a focus of attention for further processing the ridge detector find ridge on vector field and it is designed to automatically find the right scale of a ridge even in the presence of noise multiple step and narrow valley one of the key feature of such ridge detector is that it ha a zero response at discontinuity the ridge detector can be applied both to scalar and vector quantity such a color 
we apply the notion that phase difference can be used to interpret disparity between a pair of stereoscopic image indeed phase relationship can also be used to obtain probabilistic measure both edge and corner a well a the directional instantaneous frequency of an image field the method of phase difference is shown to be equivalent to a newton raphson root finding iteration through the resolution of band pas filtering the method doe however suffer from stability problem and in particular stationary phase and aliasing the stability problem associated with this technique are implicitly derived from the mechanism used to interpet disparity which in general requires an assumption of linear phase and the local instantaneous frequency we present two technique firstly we use the centre frequency of the applied band pas filter to interpret disparity this interpretation however suffers heavily from phase error and requires considerable damping prior to convergence secondly we use the derivative of phase to obtain the instantaneous frequency from an image which is then used to improve the disparity estimate these idea are extended into d where it is possible to extract both vertical and horizontal disparity 
numerical simulation phase space analysis and analytic techmques are three method used to solve quantitative differential equation most work in qualitative reasoning ha dealt with analog of the first two technique producing capability applicable to a wide range of system although potentially of benefit little ha been done to provide closed form analytic solution technique for qualitative differential equation qdes this paper present one such technique for the solution of a class of ordinary linear and nonlinear differential equation the technique is capable of deriving closed form description of the qualitative temporal behavior represented by such equation a language qfl for describing qualitative temporal behavior is presented and procedure and an implementation qdiff that solves equation in this form are demonstrated 
one of the main reason why computer generated proof are not widely accepted is often their complexity and incomprehensibility especially proof of mathematical theorem with equation are normally presented in an inadequate and not intuitive way this is even more of a problem for the presentation of inference drawn by automated reasoning component in other ai system for first order logic proof transformation procedure have been designed in order to structure proof and state them in a formalism that is more familiar to human mathematician in this report we generalize these approach so that proof involving equational reasoning can also be handled to this end extended refutation graph are introduced to represent combined resolution and paramodulation proof in the process of transforming these proof into natural deduction proof with equality the inherent structure can also be extracted by exploiting topological property of refutation graph 
this paper present viewpoint dependent constraint that relate image feature such a t junction and inflection to the pose of curved d object these constraint can be used to recognize and locate object instance in the imperfect line drawing obtained by edge detection from a single image for object modelled by implicit algebraic equation the constraint equation are polynomial and method for solving these system of constraint are briefly discussed an example of pose recovery is presented 
we investigate the use of information from all second order derivative of the error function to perform network pruning i e removing unimportant weight from a trained network in order to improve generalization and increase the speed of further training our method optimal brain surgeon ob is significantly better than magnitude based method which can often remove the wrong weight ob also represents a major improvement over other method such a optimal brain damage le cun denker and solla because ours us the full off diagonal information of the hessian matrix h crucial to ob is a recursion relation for calculating h from training data and structural information of the net we illustrate ob on standard benchmark problem the monk s problem the most successful method in a recent competition in machine learning thrun et al wa backpropagation using weight decay which yielded a network with weight for one monk s problem ob requires only weight for the same performance accuracy on two other monk s problem our method required only and of the weight found by magnitude based pruning 
this paper describes a novel method to measure the differential invariant of the image velocity field robustly by computing average value from the integral of normal image velocity around image contour this is equivalent to measuring the temporal change in the area of a closed contour this avoids having to recover a dense image velocity field and taking partial derivative it also doe not require point or line correspondence moreover integration provides some immunity to image measurement noise 
this paper describes and demonstrates a view independent relational model virm in a vision system designed for recognising a known d object from single monochromatic image the aim is to derive a model of an object able to effect recognition without invoking pose information the system inspects a cad model of the object from a number of different viewpoint to identify relatively view independent relationship among component part of the object these relation are represented in the form of a hypergraph the virm can be searched using a best first technique to obtain hypothesis of vehicle pose which match image feature 
this paper investigates design issue associated with representing relation in binary network augmented with hidden variable the trade off between the number of variable required and the size of their domain is discussed we show that if the number of value available to each variable is just two then hidden variable cannot improve the expressional power of the network regardless of their number however for k we can always find a layered network using k valued hidden variable that represent an arbitrary relation we then provide a scheme for decomposing an arbitrary relation using k hidden variable each having k value k 
the symbolic probabilistic inference spi algorithm d ambrosio provides an efficient framework for resolving general query on a belief network it applies the concept of dependency directed backward search to probabilistic inference and is incremental with respect to both query and observation unlike most belief network algorithm spi is goal directed performing only those calculation that are required to respond to query the directed graph of the underlying belief network is used to develop a tree structure for recursive query processing this allows effective caching of intermediate result and significant opportunity for parallel computation a simple preprocessing step ensures that given the search tree the algorithm will include no unnecessary distribution the preprocessing step eliminates dimension from the intermediate result and prune the search path 
this paper describes a new algorithm to simultaneously detect and classify straight line according to their orientation in d the fundamental assumption is that the most interesting line in a d scene have orientation which fall into a few precisely defined category the algorithm we propose us this assumption to extract the projection of straight edge from the image and to determine the most likely corresponding orientation in the d scene the extracted d line segment are therefore perceptually grouped according to their orientation in d instead of extracting all the line segment from the image before grouping them by orientation we use the orientation data at the lowest image processing level and detect segment separately for each predefined d orientation a strong emphasis is placed on real world application and very fast processing with conventional hardware 
this paper present searching approach and user interface capability of duo an online public access catalogue opac designed to permit the user of three university of the northeast of italy different subject searching access to the co operative multi discipline library catalogue database the co operative catalogue database is managed by one of the software system developed under the italian national project for library automation the sbn project since the sbn database ha not been designed to be efficiently accessed for end user search the duo database ha been designed to avoid duplication of the sbn database data and to be usable for making efficient subject access to the catalogue document the duo design choice are presented in particular the main choice of designing a virtual document that corresponds to each sbn document and that ha unstructured data usable for subject search purpose the paper present a new kind of user opac dialogue that make available to the user different search approach and on line dictionary in particular the user during the interaction with the search tool can represent his information need with the support of interface capability that are based on retrieval path history and word and code on line dictionary duo is the first italian opac that ha been made openly available to user of university and research institution for this reason it is also the first time that opac log data is going to be collected in italy this work mainly intends to make a modern opac available to the user of a sbn catalogue database but it is going to permit also to build up a knowledge on opac usage in italy 
a new class of data structure called bumptrees is described these structure are useful for ef ficiently implementing a number of neural network related operation an empirical comparison with radial basis function is presented on a robot arm mapping learning task application to density estimation classification and constraint representation and learning are also outlined what is a bumptree a bumptree is a new geometric data structure which is useful for efficiently learning representing and evaluating geometric relationship in a variety of context they are a natural generalization of several hierarchical geometric data structure including oct tree k d tree balltrees and boxtrees they are useful for many geometric learning task including approximating function constraint surface classification region and probability density from sample in the function approximation case the approach is related to radial basis function neural network but support faster construction faster access and more flexible modification we provide empirical data comparing bumptrees with radial basis function in section 
island driven parsing is of great relevance for speech recognition understanding and other natural language processing application a bidirectional algorithm is presented that efficiently solves this problem allowing both any possible determination of the starting word in the input sentence and flexible control in particular a mixed bottom to top and top down approach is followed without leading to redundant partial analysis the algorithm performance is discussed 
this paper discus discovery of mathematical model from engineering data set keds a knowledge based equation discovery system identifies several potentially overlapping region in the problem space each associated with an equation of different complexity and accuracy the minimum description length principle together with the keds algorithm is used to guide the partitioning of the problem space the kedsmdl algorithm ha been tested on discovering model for predicting the performance efficiency of an internal combustion engine 
interpolation of d segment obtained through a trinocular stereo process is achieved by using a d delaunay triangulation on the image plane of one of the vision system camera the resulting two dimensional triangulation is backprojected into the d space generating a surface description in term of triangular face the use of a constrained delaunay triangulation in the image plane guarantee the presence of the d segment a edge of the surface representation 
prosodic structure and syntactic structure are not identical neither are they unrelated knowing when and how the two eorrespoud could yield better quality speech synthesis could aid in the disambiguation of com peting syntactic hypothesis in speech understanding and could lead to a more comprehensive view of human speech processing in a set of exper iments involving pair of phonetically similar sentence s representing seven type of structural contrast the perceptual evidence show that some but not all of the pair can be disambiguated on the basis of pro sodie difference the phonological evidence relates the disambiguation primarily to boundary phenomenon although prominence sometimes play a role finally phonetic analysis describing the attribute of these phonological marker indicate the importance of both absolute and rela tive measure 
abstract in this paper we propose a method for specifying the functionality of an intelligent interface to large scale information retrieval system erational and for implementing those function in an r 
object recognition requires complicated domain specific rule for many problem domain it is impractical for a programmer to generate these rule a method for automatically generating the required object class description is needed this paper present a method to accomplish this goal in our approach the supervisor provides a series of example scene description to the system with accompanying object class assignment generalization rule then produce object class description these rule manipulate non symbolic descriptor in a symbolic framework the resulting class description are useful both for object recognition and for providing clear explanation of the decision process we present a simple method for maintaining an optimal description set a new example possibly of previously unseen class become available providing needed update to the description set finally the system s performance is shown a it learns object class description from realistic scene video image of electronic component 
a touted advantage of symbolic representation is the ease of transferring learned information from one intelligent agent to another this paper investigates an analogous problem how to use information from one neural network to help a second network learn a related task rather than translate such information into symbolic form in which it may not be readily expressible we investigate the direct transfer of information encoded a weight here we focus on how transfer can be used to address the important problem of improving neural network learning speed first we present an exploratory study of the somewhat surprising effect of pre setting network weight on subsequent learning guided by hypothesis from this study we sped up back propagation learning for two speech recognition task by transferring weight from smaller network trained on subtasks we achieved speedup of up to an order of magnitude compared with training starting with random weight even taking into account the time to train the smaller network we include result on how transfer scale to a large phoneme recognition problem 
this paper describes a method which us optical flow that is the apparent motion of the image brightness pattern in time varying image in order to detect and identify multiple motion homogeneous region are found by analysing local linear approximation of optical flow over patch of the image plane which determine a list of the possibly viewed motion and finally by applying a technique of stochastic relaxation the presented experiment on real image show that the method is usually able to identify region which correspond to the different moving object is also rather insensitive to noise and can tolerate large error in the estimation of optical flow 
real time constraint on ai system require guaranteeing bound on these system performance however in the presence of source of uncontrolled combinatorics it is extremely difficult to guarantee such bound on their performance in production system the prirnary source of uncontrolled combinatorics is the production match to eliminate these combinatorics the unique attribute formulation wa introduced in tambe and rosenbloom which achieved a linear bound on the production match this formulation lead to several question is this unique attribute formulation the best conceivable production system formulation in fact are there other alternative production system formulation if there are other formulation how should these alternative be compared with the unique attribute formulation this paper attempt to address these question in the context of soar it identifies independent dimension along which alternative production system formulation can be specified these dimension are based on the fiied class of match algorithm currently employed in production system these dimension create a framework for systematically generating alternative formulation using this framework we show that the unique attribute formulation is the best one within the dimension investigated however if a new class of match algorithm is admitted by relaxing certain constraint other competitor fonnulations emerge the paper indicates which competitor formulation are promising and why although some of the concept such a unique attribute are introduced in the context of soar they should also be relevant to other rule based system 
previous work m i sereno cf m e sereno showed that a feedforward network with area v like input layer unit and a hebb rule can develop area mt like second layer unit that solve the aperture problem for pattern motion the present study extends this earlier work to more complex motion saito et al showed that neuron with large receptive field in macaque visual area mst are sensitive to different sens of rotation and dilation irrespective of the receptive field location of the movement singularity a network with an mt like second layer wa trained and tested on combination of rotating dilating and translating pattern third layer unit learn to detect specific sens of rotation or dilation in a position independent fashion despite having position dependent direction selectivity within their receptive field 
this paper introduces a method to create a hierarchical description of smooth curved surface based on scale space analysis we extend the scale space method used in d signal analysis to d object a d scale space image are segmented by zero crossing of surface curvature at each scale and then linked between consecutive scale based on topological change kh description the kh description is then parsed and translated into the p tree which contains the number and distribution of subregions required for shape matching the kh description contains coarse to fine shape information of the object and the p tree is suitable for shape matching a hierarchical matching algorithm using the description is proposed and example show that the symbolic description is suitable for efficient coarse to fine d shape matching 
research in artificial intelligence on constraint based representation for temporal reasoning ha largely concentrated on two kind of formalism system of simple linear inequality to encode metric relation between time point and system of binary constraint in allen s temporal calculus to encode qualitative relation between time interval each formalism ha certain advantage linear inequality can represent date duration and other quantitive information allen s qualitative calculus can express relation between time interval such a disjointedness that are useful for constraint based approach to planning in this paper we demonstrate how metric and allenstyle constraint network can be integrated in a constraint based reasoning system the highlight of the work include a simple but powerful logical language for expressing both quantitative and qualitative information translation algorithm between the metric and allen sublanguages that entail minimal loss of information and a constraint propagation procedure for problem expressed in a combination of metric and allen constraint 
a performance comparison of two self organizing network the kohonenfeature map and the recently proposed growing cell structuresis made for this purpose several performance criterion forself organizing network are proposed and motivated the modelsare tested with three example problem of increasing difficulty thekohonen feature map demonstrates slightly superior result onlyfor the simplest problem for the other more difficult and also morerealistic problem the growing cell 
in this paper we develop a proof procedure for autoepistemic ael and default logic dl based on translating them into a truth maintenance system tm the translation is decidable if the theory consists of a finite number of default and premise and classical derivability for the base language is decidable to determine all extension of a network we develop variant of doyle s labelling algorithm 
this paper describes the application of explanation based learning a machine learning technique to the sri core language engine a large scale general purpose natural language analysis system the idea is to bypass normal morphological syntactic and 
the virtue of the strip assumption for planning is that it bound the information relevant to determining the effect of action viewing the assumption a a statement about belief we find that it doe not actually assume anything about the world itself we can characterize the assertion about belief in term of probabilistic independence thereby facilitating analysis of representation for planning under uncertainty this interpretation separate the strip assumption from other necessary feature of a planning architecture such a it model of persistence and it inferential policy by isolating these factor we can understand the role of dependence across a wide range of planner and action representation graphical model of dependence developed for probabilistic analysis provide a convenient tool for verifying the strip assumption for a variety of planning system investigation of a few representative system reveals a markovian event structure common to these planning model 
we describe an algorithm for tracking an unknown object in natural scene we require that the object s approximate initial location be available and further assume that the it ean be distinguished from the background by motion or stereo no constraint is placed on the object s shape other than that it not change too rapidly from frame to frame object are tracked and segmented cross temporally using a massively parallel bottom up approach our algorithm ha been implemented on a connection machine and run in real time frame per second 
given a set of training example determining the appropriate numberof free parameter is a challenging problem constructivelearning algorithm attempt to solve this problem automatically byadding hidden unit and therefore free parameter during learning we explore an alternative class of algorithm called metamorphosis algorithm in which the number of unit is fixed butthe number of free parameter gradually increase during learning the architecture we investigate is composed 
we introduce a greedy local search procedure called gsat for solving propositional satisfiability problem our experiment show that this procedure can be used to solve hard randomly generated problem that are an order of magnitude larger than those that can be handled by more traditional approach such a the davis putnam procedure or resolution we also show that gsat can solve structured satisfiability problem quickly in particular we solve encoding of graph coloring problem n queen and boolean induction general application strategy and limitation of the approach are also discussed gsat is best viewed a a model finding procedure it good performance suggests that it may be advantageous to reformulate reasoning task that have traditionally been viewed a theorem proving problem a model finding task 
this paper present a projection algorithm for incremental control rule synthesis the algorithm synthesizes an initial set of goal achieving control rule using a combination of situation probability and estimated remaining work a a search heuristic this set of control rule ha a certain probability of satisfying the given goal the probability is incrementally increased by synthesizing additional control rule to handle error situation the execution system is likely to encounter when following the initial control rule by using situation probability the algorithm achieves a computationally effective balance between the limited robustness of triangle table and the absolute robustness of universal plan 
the goal of a probabilistic retrieval system design is to rank the element of the search universe in descending order of their estimated probability of usefulness to the user previously explored method for computing such a ranking have involved the use of statistical independence assumption and multiple regression analysis on a learning sample in this paper these technique are recombined in a new way to achieve greater accuracy of probabilistic estimate without undue additional computational complexity the novel element of the proposed design is that the regression analysis be carried out in two or more level or stage such an approach allows composite or grouped retrieval clue to be analyzed in an orderly manner first within group and then between it compensates automatically for systematic bias introduced by the statistical simplifying assumption and give rise to search algorithm of reasonable computational efficiency 
the previously described kbann system integrates existing knowledge into neural network by defining the network topology and setting initial link weight standard neural learning technique can then be used to train such network thereby refining the information upon which the network is based however standard neural learning technique are reputed to have difficulty training network with multiple layer of hidden unit kbann commonly creates such network in addition standard neural learning technique ignore some of the information contained in the network created by kbann this paper describes a symbolic inductive learning algorithm for training such network that us this previously ignored information and which help to address the problem of training deep network empirical evidence show that this method improves not only learning speed but also the ability of network to generalize correctly to testing example 
choosing between multiple ontological perspective is crucial for reasoning about the physical world choosing the wrong perspective can make a reasoning task impossible this paper introduces a lagrangian plug flow ontology pf for reasoning about thermodynamic fluid flow we show that this ontology capture continuously changing behavior of flowing fluid not represented in currently implemented ontology these behavior are essential for understanding thermodynamic application such a power cycle refrigeration liquefaction throttling and flow through nozzle we express the ontology within the framework of qualitative process qp theory to derive our qp theory for plug flow we use the method of causal clustering to find causal interpretation of thermodynamic equation we also incorporate qualitative version of standard thermodynamic relation including the second law of thermodynamics and clapeyron s equation 
abstract how can artificial neural net generalize better from fewer example in order to generalize successfully neural network learning method typically require large training data set we introduce a neural network learning method that generalizes rationally from many fewer data point relying instead on prior knowledge encoded in previously learned neural network for example in robot control learning task reported here previously learned network that model the effect of robot action are used to guide subsequent learning of robot control function for each observed training example of the target function e g the robot control policy the learner explains the observed example in term of it prior knowledge then analyzes this explanation to infer additional information about the shape or slope of the target function this shape knowledge is used to bias generalization in the learned target function result are presented applying this approach to a simulated robot task based on reinforcement learning 
in stochastic learning weight are random variable whose timeevolution is governed by a markov process at each time step n the weight can be described by a probability density functionp n we summarize the theory of the time evolution of p andgive graphical example of the time evolution that contrast thebehavior of stochastic learning with true gradient descent batchlearning finally we use the formalism to obtain prediction of thetime required for noise induced 
for a very long time it ha been considered that the only way of automatically extracting similar group of word from a text collection for which no semantic information exists is to use document co occurrence data but with robust syntactic parser that are becoming more frequently available syntactically recognizable phenomenon about word usage can be confidently noted in large collection of text we present here a new system called sextant which us these parser and the finer grained context they produce to judge word similarity 
one aspect of world knowledge essential to information retrieval is knowing when two word are related knowing word relatedness allows a system given a user s query term to retrieve relevant document not containing those exact term two word can be said to be related if they appear in the same context document co occurrence give a measure of word relatedness that ha proved to be too rough to be useful the relatively recent apparition of on line dictionary and robust and rapid parser permit the extraction of finer word context from large corpus in this paper we will describe such an extraction technique that us only coarse syntactic analysis and no domain knowledge this technique produce list of word related to any work appearing in a corpus when the closest related term were used in query expansion of a standard information retrieval testbed the result were much better than that given by document co occurence technique and slightly better than using unexpanded query supporting the contention that semantically similar word were indeed extracted by this technique 
generalized clause differ from ordinary clause by allowing conjunction of literal in the role of ordinary literal i e they are disjunction of conjunction of simple literal an advantage of this clausal form is that implication with conjunctive conclusion or disjunctive premise are not split into multiple clause an extension of lovelands model elimination calculus loveland a loveland is presented able to deal with such generalized clause furthermore we describe a method for generating lemma that correspond to valid instance of conjunctive conclusion using these lemma it is possible to avoid multiple proof of the premise of implication with conjunctive conclusion 
best first search is a general search algorithm that always expands next a frontier node of lowest cost it applicability however is limited by it exponential memory requirement iterative deepening a previous approach to this problem doe not expand node in best first order if the cost function can decrease along a path we present a linear space best first search algorithm rbfs that always explores new node in best first order regardless of the cost function and expands fewer node than iterative deepening with a nondecreasing cost function on the sliding tile puzzle rbfs with a weighted evaluation function dramatically reduces computation time with only a small penalty in solution cost in general rbfs reduces the space complexity of best first search from exponential to linear at the cost of only a constant factor in time complexity in our experiment 
two important computational approach to problem solving are model based reasoning mbr and case based reasoning cbr mbr since it reason from first principle is especially suited for solving novel problem cbr since it reason from previous experience is especially suited for solving frequently encountered problem however large novel problem pose difficulty for both approach mbr rapidly grows intractable and cbr fails to find a relevant previous case in this paper we describe an approach called prototype based reasoning that integrates both approach to solve such problem prototype based reasoning treat a large novel problem a a novel combination of several familiar subproblems it us cbr to find and solve the subproblems formulates a new problem by combining these individual solution and us mbr to solve this new problem we demonstrate the effectiveness of this method on several example involving the causal simulation of complex electronic circuit 
adaptive mesh are dynamic network of nodal mass interconnected by adjustable spring they are useful for nonuniformly sampling and reconstructing visual data this paper extends the adaptive mesh model in the following way it i develops open adaptive mesh and closed adaptive shell based on triangular and rectangular element ii proposes a discontinuity detection and preservation algorithm suitable for the model and iii develops technique for adaptive hierarchical subdivision of adaptive mesh and shell the extended model is applied to image and d surface data 
this paper discus unsupervised learning of orthogonal concept on relational data relational predicate while formally equivalent to the feature of the concept learning literature are not a good basis for defining concept hence the current task demand a much larger search space than traditional concept learning algorithm the sort of space explored by connectionist algorithm however the intended application using the discovered concept in the cyc knowledge base requires that the concept be interpretable by a human an ability not yet realized with connectionist algorithm interpretability is aided by including a characterization of simplicity in the evaluation function for hinton s family relation data we do find cleaner more intuitive feature yet when the solution are not known in advance the difficulty of interpreting even feature meeting the simplicity criterion call into question the usefulness of any reformulation algorithm that creates radically new primitive in a knowledge based setting at the very least much more sophisticated explanation tool are needed 
constraint network have been shown to be useful in formulating such diverse problem a scene labeling natural language parsing and temporal reasoning given a constraint network we often wish to i find a solution that satisfies the constraint and ii find the corresponding minimal network where the constraint are a explicit a possible both task are known to be np complete in the general case task i is usually solved using a backtracking algorithm and task ii is often solved only approximately by enforcing various level of local consistency in this paper we identify a property of binary constraint called row convexity and show it usefulness in deciding when a form of local consistency called path consistency is sufficient to guarantee a network is both minimal and decomposable decomposable network have the property that a solution can be found without backtracking we show that the row convexity property can be tested for efficiently and we show by examining application of constraint network discussed in the literature that our result are useful in practice thus we identify a large class of constraint network for which we can solve both task i and ii efficiently 
the performance of production program can be improved by firing multiple rule in a production cycle in this paper we present the multiple context multiple rule mcmr model which speed up production program execution by firing multiple rule concurrently and guarantee the correctness of the solution the mcmr model is implemented using the rubic parallel inference model on the intel ipsc hypercube the intel ipsc hypercube is chosen because it is a cost effective solution to large scale application to avoid unnecessary synchronization and improve performance rule are executed asynchronously and message are used to update the database preliminary implementation result for the rubic parallel inference environment on the intel ipsc hypercube are reported 
this paper describes the application of explanation based learning a machine learning technique to the sri core language engine a large scale general purpose natural language analysis system the idea is to bypass normal morphological syntactic and 
a half occluded region in a stereo pair pixel in one image representing point in is a set of svace visible to that camera or eye only and not to ihe other these occur typically a part of the background immediately to the left and right side of nearby occluding object and are present in most natural scene previous approach to stereo either ignored these unmatchable point or attempted to weed them out in a second pas our algorithm incorporates them from the start a a strong clue to depth discontinuity psychophysical evidence suggests that the human visual system also exploit these clue we start by deriving a measure for goodness of fit and a prior based on a simplified model of object in space which lead to an energy functional depending both on the depth a measured from a central cyclopean eye and on the region of point occluded from the left and right eye perspective we minimize this using dynamic programming along epipolar line followed by annealing in both dimension experiment indicate that this method is very effective even in difficult scene 
this paper present a methodology which enables the derivation of goal ordering rule from the analysis of problem failure we examine all the planning action that lead to failure if there are restriction imposed by a problem state on taking possible action the restriction manifest themselves in the form of a restricted set of possible binding our method make use of this observation to derive general control rule which are guaranteed to be correct the overhead involved in learning is low because our method examines only the goal stack retrieved from the leaf node of a failure search tree rather than the whole tree empirical test show that the rule derived by our system pal after sufficient training performs a well a or better than those derived by system such a prodigy ebl and static 
the intelligent database interface idi is a cache based interface that is designed to provide artificial intelligence system with efficient access to one or more database on one or more remote database management system dbms it can be used to interface with a wide variety of different dbms with little or no modification since sql is used to communicate with remote dbms and the implementation of the idi provides a high degree of portability the query language of the idi is a restricted subset of function free horn clause which is translated into sql result from the idi are returned one tuple at a time and the idi manages a cache of result relation to improve efficiency the idi is one of the key component of the intelligent system server i knowledge representation and reasoning system and is also being used to provide database service for the unisys spoken language system program 
we investigate the computational complexity of membership problem in a number of propositional default logic we introduce a hierarchy of class of propositional default rule that extends that described in kautz and selman and characterize the complexity of membership problem in these class under various simplifying assumption about the underlying propositional theory our work significantly extends both that presented in kautz and selman and in stillman a 
although most design replay technique have been empirically tested against some performance program there ha been very little empirical evidence published that compare various approach on the same problem to determine the source of power six different design replay algorithm based on approach in the literature are implemented and tested on different design replay problem the resulting data indicate that there is a trade off between efficiency and autonomy for certain type of adaptation strategy based on some of the lesson drawn from this data a new algorithm remaid ha been developed this algorithm recognizes two different type of mi match between previous experience and current problem detour and pretours the remaid strategy take advantage of it knowledge of mi match to improve replay autonomy without sacrificing efficiency the success of the remaid algorithm is empirically verified 
one goal of machine discovery is to automate creative task from human scientific practice this paper describes a project to automate in a general manner the theory driven discovery of reaction pathway in chemistry and biology we have designed a system called mechem that proposes credible pathway hypothesis from data ordinarily available to the chemist mechem ha been applied to reaction drawn from the history of biochemistry from recent industrial chemistry a reported in journal and from organic chemistry textbook the paper first explains the chemical problem and discus previous ai treatment then are presented the architecture of the system the key algorithmic idea and the heuristic used to explore the very large space of chemical pathway the system s efficacy is demonstrated on a biochemical reaction studied earlier by kulkarni and simon in the kekada system and on another reaction from industrial chemistry our project ha also resulted in separate novel contribution to chemical knowledge demonstrating that we have not simplified the task for our convenience but have addressed it full complexity 
recent study on the analysis of intonational function examine a range of material from cue phrase in monologue litman and hirschberg and dialogue hirschberg and litman hockey to longer utterance in both monologue and dialogue mclemore result match specific intonational tune to certain discourse function which are more or le well defined although these result make a convincing case that intonation doe signal a change in discourse structure the specification of discourse function remains vague a suitable taxonomy is needed to fine tune the relationship between intonation and discourse function a recent analysis of dialogue kowtko et al provides a framework of conversational game which allows more fine grained examination of prosodic function the current paper introduces an intonational analysis of mono and di syllabic word based upon such a framework and compare result in progress with previous work on intonation 
recent experimental result by schn rr with an approach based on a simplified oriented smoothness constraint show considerable improvement at expected discontinuity of the optical flow field it thus appears justified to study whether the local gray value variation can be exploited in the temporal a well a in the spatial domain in order to achieve further improvement at discontinuity in the optical flow field associated with the image area of moving object in image sequence an extension of the oriented smoothness constraint into the temporal domain is presented in this context a local estimation approach for the spatio temporal partial derivative of optical flow ha been developed this in turn is used to compare two approach for the definition of optical flow 
document management system are needed for many business application this type of system would combine the functionality of a database system for describing storing and maintaining document with complex structure and relationship with a text retrieval system for effective retrieval based on full text the retrieval model for a document management system is complicated by the variety and complexity of the object that are represented in this paper we describe an approach to complex object retrieval using a probabilistic inference net model and an implementation of this approach using a loose coupling of an object oriented database system iris and a text retrieval system based on inference net inquery the resulting system is used to store long structured document and can retrieve document component section figure etc based on their content or the content of related component the lesson learnt from the implementation are discussed 
this paper address the problem of object tracking in a sequence of monocular image the use of region a primitive for tracking enables to directly handle consistent object level entity a motion based segmentation process based on normal flow and first order motion model provide instantaneous measurement shape position and motion of each region present in such segmented image are estimated with a recursive algorithm along the sequence occlusion situation can be handled we have carried out experiment on sequence of real image depicting complex outdoor scene 
a new generation of knowledge database is emerging these system contain thousand of object densely interconnected and heterogeneously organized entered from many source both human and automated such system present tremendous challenge to their user who must locate relevant information quickly and add new information effectively our research aim to understand and support the knowledge editing task the hit knowledge editor hke is an interface that support browsing and modifying the cyc knowledge base guha lenat hke ha been designed to be a collaborative interface following a set of principle for sharing task between system and user we describe these principle and illustrate how hke provides resource built according to those principle that collaborate with it user on a variety of knowledge editing task 
the real time computation of motion from real imagesusing a single chip with integrated sensor is a hard problem we present two analog vlsi scheme that use pulsedomain neuromorphic circuit to compute motion pulsesof variable width rather than graded potential representa natural medium for evaluating temporal relationship both algorithm measure speed by timing a moving edgein the image our first model is inspired by reichardt salgorithm in the fly and yield a non monotonic 
this paper proposes a set of representation for tense and a set of constraint on how they can be combined in adjunct clause the semantics we propose explains the possible meaning of tense in a variety of sentential context it also support an elegant constraint on tense combination in adjunct clause these semantic representation provide insight into the interpretation of tense and the constraint provide a source of syntactic disambiguation that ha not previously been demonstrated we demonstrate an implemented disambiguator for a certain class of three clause sentence based on our theory 
this paper present a formal theory of multiple agent non monotonic reasoning we introduce the subject of multiple agent non monotonic reasoning for inquiry and motivate the field in term of it application for commonsense reasoning we extend moore s autoepistemic logic to the multiple agent case and show that the resulting logic is too weak for most application of commonsense reasoning we then suggest some possible set of principle for a logic of multipleagent non monotonic reasoning based on the concept of an agent s arrogance towards his knowledge of another agent s ignorance while the principle of arrogance are in general too strong we demonstrate that restricted version of these principle can work quite well for commonsense reasoning in particular we show that a restricted form of the principle of arrogance yield result that are equivalent to emat morgenstern a non monotonic logic which wa designed to reason about temporal projection in epistemic context 
this paper describes an adaptive model based diagnostic mechanism although model based system are more robust than heuristic based expert system they generally require more computation time time consumption can be significantly reduced by using a hierarchical model scheme which present view of the device at several different level of detail we argue that in order to employ hierarchical model effectively it is necessary to make economically rational choice concerning the trade off between the cost of a diagnosis and it precision the mechanism presented here make these choice using a model diagnosability criterion which estimate how much information could be gained by using a candidate model it take into account several important parameter including the level of diagnosis precision required by the user the computational resource available the cost of observation and the phase of the diagnosis experimental result demonstrate the effectiveness of the proposed mechanism 
we propose an approach for building surface from an unsegmented set of d point local surface patch are estimated and their differential property are used iteratively to smooth the point while eliminating spurious data and to group them into more global surface we present result on complex natural scene using stereo data a our source of d information 
consistency technique have been studied extensively in the past a a way of tackling constraint satisfaction problem csp in particular various arc consistency algorithm have been proposed originating from waltz s filtering algorithm and culminating in the optimal algorithm ac of mohr and henderson ac run in o ed in the worst case where e is the number of arc or constraint and d is the site of the largest domain being applicable to the whole class of binary csp these algorithm do not take into account the semantics of constraint in this paper we present a new generic arc consistency algorithm ac the algorithm is parametrised on two specified procedure and can be instantiated to reduce to ac and ac more important ac can be instantiated to produce an o ed algorithm for two important class of constraint functional and monotonic constraint we also show that ac ha an important application in constraint logic programming over finite domain the kernel of the constraint solver for such a programming language is an arc consistency algorithm for a set of basic constraint we prove that ac in conjunction with node consistency provides a decision procedure for these constraint running in time o ed 
in this paper we present a polynomial time parsing algorithm for combinatory categorial grammar the recognition phase extends the cky algorithm for cfg the process of generating a representation of the parse tree ha two phase initially a shared forest is build that encodes the set of all derivation tree for the input string this shared forest is then pruned to remove all spurious ambiguity 
standard algorithm for explanation based learning require complete and correct knowledge base the kbann system relaxes this constraint through the use of empirical learning method to refine approximately correct knowledge this knowledge is used to determine the structure of an artificial neural network and the weight on it link thereby making the knowledge accessible for modification by neural learning kbann is evaluated by empirical test in the domain of molecular biology network created by kbann are shown to be superior in term of their ability to correctly classify unseen example to randomly initialized neural network decision tree nearest neighbor matching and standard technique reported in the biological literature in addition kbann s network improve the initial knowledge in biologically interesting way 
though most translation system have some mechanism for translating certain type of divergent predicate argument structure they do not provide a general procedure that take advantage of the relationship between lexical semantic structure and syntactic structure a divergent predicate argument structure is one in which the predicate e g the main verb or it argument e g the subject and object do not have the same syntactic ordering property for both the source and target language to account for such ordering difference a machine translator must consider language specific syntactic idiosyncrasy that distinguish a target language from a source language while making use of lexical semantic uniformity that tie the two language together this paper describes the mechanism used by the unitran machine translation system for mapping an underlying lexical conceptual structure to a syntactic structure and vice versa and it show how these mechanism coupled with a set of general linking routine solve the problem of thematic divergence in machine translation 
the mating paradigm for automated theorem provers wa proposed by andrew to avoid some of the shortcoming in resolution it facilitates automated deduction in higher order and non classical logic moreover there are procedure which translate back and forth between refutation by the mating method and proof in a natural deduction system we describe a search procedure called path focused duplication for finding refutation by the mating method this procedure which is a complete strategy for the mating method address two crucial issue inadequately handled in current implementation that arise in the search for refutation when and how to expand the search space it focus on a particular path that seems to cause an impasse in the search and expands the search space relative to this path in a way that allows the search to immediately resolve the impasse the search space grows and shrink dynamically to respond to the requirement that have arisen or have been met in the search process thus avoiding an explosion in the size of the search space we have implemented a prototype of this procedure and have been able to easily solve many problem that an earlier program found difficult 
we describe a theoretical formulation for stereo in term of the markov random field and bayesian approach to vision this formulation enables u to integrate the depth information from different type of matching primitive or from different vision module we treat the correspondence problem and surface interpolation a different aspect of the same problem and solve them simultaneously unlike most previous theory we use technique from statistical physic to compute property of our theory and show how it relates to previous work these technique also suggest novel algorithm for stereo which are argued to be preferable to standard algorithm on theoretical and experimental ground it can be shown yuille geiger and b lthoff that the theory is consistent with some psychophysical experiment which investigate the relative importance of different matching primitive 
utterance that include rationale clause and mean clause display a variety of feature that affect their interpretation a well a the subsequent discourse of particular importance it the information that is conveyed about agent belief and intention with respect to the action they talk about or perform hence for a language interpretation system to handle these utterance it must identify the relevant feature of each construction and draw appropriate inference about the agent mental state with respect to the action and action relation that are involved this paper describes an interpretation model that satisfies this need by providing a set of interpretation rule and showing how these rule allow for the derivation of the appropriate set of belief and intention associated with each construction 
a common problem in robotic assembly is that of mating tightly fitting part when the location and the dimension of the part are somewhat uncertain it is neeessary to be able to reason about these uncertainty in conjunction with the geometry of the part involved in order to develop motion plan for assembly operation in this paper we will present a method for the treatment of three type of uncertainty usually prevalent in robotic assembly system uncertainty in the initial location of part uncertainty in the control of the robot used to assemble these part and uncertainty in the dimension of these part the method we will present used by a cad based planning system we have developed discovers which portion of an assembly operation must be carried out using force torque guided motion because the composite uncertainty exceed the clearance during these portion of the operation the method further suggests the type of force torque guided motion that need to be used for these portion with this knowledge our planning system formulates motion plan for assembly operation plan for a variety of assembly have been produced by our planning system and have been experimentally verified on both a cincinnati mi lacron t robot and a puma robot 
a new model called multi component blurring or mcb to account for image blurring effect due to depth discontinuity is presented we show that blurring process operating in the vicinity of large depth discontinuties can give rise to emergent image detail quite distinguishable but nevertheless unexplained by previously available blurring model in other word the maximum principle for scale space doe not hold it is argued that blurring in high relief d scene should be more accurately modelled a a multi component process we present result from extensive and carefully designed experiment with many image of real scene taken by a ccd camera with typical parameter these result have consistently supported our new blurring model due care wa taken to ensure that the image phenomenon observed are mainly due to de focussing and not due to mutual illumination specularity object finer structure coherent diffraction or incidental image noise we also hypothesize on the role of blurring on human depth from blur perception based on correlation with recent result from human blur perception 
in this paper we present a new parsing model of linguistic and computational interest linguistically the relation between the parser and the theory of grammar adopted government and binding gb theory a presented in chomsky a b is clearly specified computationally this model adopts a mixed parsing procedure by using left corner prediction in a modified lr parser 
abstract feature structure are informational element that have been used in several linguistic theory and in computational system for natural language processing a logicaj calculus ha been developed and used a a description language for feature structure in the present work a framework in three valued logic is suggested for defining the semantics of a feature structure description language allowing for a more complete set of logical operator in particular the semantics of the negation and implication operator are examined various proposed interpretation of negation and implication are compared within the suggested framework one particular interpretation of the description language with a negation operator is described and it computational aspect studied 
summary form only give teleological modeling a developing approach for creating abstraction and mathematical representation of physically realistic time dependent object is described in this approach geometric constraint property mechanical property of object the parameter representing an object and the control of the object are incorporated into a single conceptual framework a teleological model incorporates time dependent goal of behavior of purpose a the primary abstraction and representation of what the object is a teleological implementation take a geometrically incomplete specification of the motion position and shape of an object and produce a geometrically complete description of the object s shape and behavior a a function of time teleological modeling technique may be suitable for consideration in computer vision algorithm by extending the current notion about how to make mathematical representation of object teleological description can produce compact representation for many of the physically derivable quantity controlling the shape combining operation and constraint which govern the formation and motion of object 
it is shown that an index space can be a powerful tool for reducing the image model match search by a factor of kg but only when accompanied by some mechanism such a grouping that prevents the system from having to consider all match between image group of size g and model group of size g it is also shown that if image group are to index a single point at recognition time then the index space must contain pointer to each model group over a d sheet and should therefore be g dimensional a simple indexing system ha been implemented to demonstrate these concept and a series of experiment have been conducted to investigate the tradeoff between space and time they indicate that the speedup are achievable but require a large amount of space 
an experimental and theoretical analysis of the noise model in the geometric hashing technique is presented the efficacy of the technique is discussed in the case of affine transformation similarity and grid motion the efficacy of the voting procedure in geometric hashing is discussed and it is shown that it introduction significantly reduces the expected burden of the verification stage the discussion is illustrated by result of simulation which have been performed to test the expected performance of the algorithm 
a planar shape recognition approach is presented which is based on hidden markov model and autoregressive parameter this approach segment closed shape into segment and explores the characteristic relation between consecutive segment to make classification at a finer level the algorithm can tolerate much shape contour perturbation and a moderate amount of occlusion the overall classification scheme is independent of shape orientation excellent recognition result have been reported a distinct advantage of the approach is that the classifier doe not have to be trained all over again when a new class of shape is added 
an approach for describing object for the purpose of recognition is developed the author deal with two key issue building natural description of curved object and making these description compact and abstract typical example of the type of curve one is able to describe a both qualitatively similar yet discriminably different are shown method based on curvature extremum alone are likely to find three of the four of these shape indistinguishable while method based on approach such a shape template may be oblivious to their similarity the central idea of the approach are outlined 
the author solve the stereo correspondence problem in uncalibrated domain using extended edge contour a a source of primitive a opposed to traditional point based algorithm this work represents a novel approach to implementation of the structural stereopsis concept of k l boyr and a c kak in particular with regard to speed judiciously exploiting the contiguity relation among primitive correspondence solution without prior knowledge of the epipolar condition for feature rich stereopairs previously requiring several day of processing by the boyer and kak algorithm are now acquired in ten of second on the same equipment 
an algorithm for tracking multiple feature position in a dynamic image sequence is presented this is achieved using a combination of two trajectory based method with the resulting hybrid algorithm exhibiting the advantage of both an optimizing exchange algorithm is described which enables short feature path to be tracked without prior knowledge of the motion being studied the resulting partial trajectory are then used to initialize a fast predictor algorithm which is capable of rapidly tracking multiple feature path a this predictor algorithm becomes tuned to the feature position being tracked it is shown how the location of occluded or poorly detected feature can be predicted the result of applying this tracking algorithm to data obtained from real world scene are then presented 
an algorithm is presented for the smooth tracking of a target in three dimensional space by a binocular head which is capable of vergence version and tilt eye movement this algorithm utilizes stereomotion channel to obtain a measurement of the three dimensional velocity of the target and then us this velocity within a control loop to keep the target center at the fixation point of the binocular head although stereomotion alone is insufficient to accurately drive binocular eye movement relative stereomotion is a useful measurement and could be easily integrated into a positional error driven tracking system 
the author show how stereo based range data obtained over time by a moving vehicle can be integrated without the explicit knowledge or computation of camera motion a unique aspect of the approach is the transformation of range information into relative distance and encoding these distance in image registered map result of experiment on dynamic stereo image are presented 
image of vehicle which move in traffic scene recorded by a stationary camera have been detected and tracked without operator intervention the resulting vehicle trajectory were projected from the image plane onto the street plane a suitable system internal representation of about german motion verb wa then exploited in order to automatically characterize trajectory segment in term of natural language concept a multiresolution approach for feature matching ha been developed which is robust enough to track vehicle image across hundred of frame despite considerable variation in size and projected velocity result from experiment with image sequence from real world traffic scene are presented 
in a binocular camera system moving through the environment it is possible to obtain a three dimensional field of vector where each vector is parallel to the actual inducing d motion of an imaged point relative to the moving camera and scaled in magnitude by the depth of that point the author derive this d vector field the p field discus it computation from the image quantity of optic flow and disparity and examine it behavior for specific case of motion a well a for general motion they present experimental result in computing this vector field and discus issue related to the determination of the motion parameter from it the result are demonstrated on synthetic and real data 
aspect ratio is a fundamental parameter of an imaging system it determines the extent of nonuniformity in sampling video signal guarantee a one to one match between the camera line and the line in the image buffer the horizontal arrangement of pixel however undergoes a resampling due to the digitization process the vertical spacing between line is given by the vertical distance of the photo element on the sensor array the relationship between the vertical and horizontal spacing is determined by the aspect ratio the author proposes a technique that us power spectrum of the image of two set of parallel line to determine the aspect ratio of the system 
the author give geometric constructive solution for d vision problem like positioning a point in space from two view using reference point in the scene no calibration is needed the method involves only simple geometric computation from the experiment it is concluded that positioning d point relatively to reference point is easy and provides more reliable result than absolute positioning a is usually done 
an approach for determining the location and orientation of a camera mounted on an alv autonomous land vehicle is proposed the author choose the road boundary and the object with vertical edge to be the calibration object during the calibration process the alv is supposed to move with pure translation the camera model are derived by using two or three image under the assumption that the height of the camera and the distance during taking image are known in advance the tilt and swing angle can be computed from the first image the pan angle can then be derived by using the projection of vertical edge in three image if the angle between the moving direction of the alv and the direction of road boundary is large enough two image are enough to find the pan angle two translational parameter can be found by employing the projection of vertical edge in two or three image 
a unified model for camera calibration is presented the model doe not assume any explicit form of imaging process and can perform linear back projection while considering all lens distortion this model is well suited to stereo reconstruction it ha been shown that the original two plane model the linear and nonlinear pin hole model can all be realized a special case of this model with the two plane model optimal orientation of the two reference plane can be found intuitively 
an omnidirectional image sensor copis conic projection image sensor is proposed for guiding navigation of a mobile robot it feature passive sensing of the omnidirectional environment in real time using a conic mirror because the conic mirror is used it image is under conic projection where the azimuth of each point in the scene appears in the image a it direction from the image center the author describe copis and it application to guide the navigation of a mobile robot the copis system acquires an omnidirectional view around the robot in real time by using a conic mirror under the assumption of constant motion of the robot location of object around the robot can be estimated by detecting their azimuth change in the omnidirectional image using this method the robot generates an environmental map of an indoor scene while it is moving in the environment a method to avoid collision against object by detecting their azimuth change is presented 
a theoretical link is established between the d edge detection and the local surface approximation using uncertainty a a practical application of the theory a method is presented for computing typical curvature feature from d medical image the author determine the uncertainty inherent in edge and surface detection and d and d image by quantitatively analyzing the uncertainty in edge position orientation and magnitude produced by the multidimensional d and d version of the monga deriche canny recursive separable edge detector the uncertainty is shown to depend on edge orientation e g the position uncertainty may vary with a ratio larger than in the d case and in the d case these uncertainty are then used to compute local geometric model quadric surface patch of the surface which are suitable for reliably estimating local surface characteristic for example gaussian and mean curvature the author demonstrate the effectiveness of these method compared to previous technique 
the author deal with analysis of the dynamic content of a scene from an image sequence whatever the camera situation is static or mobile this problem involves a motion based segmentation step a method which ensures stable motion based partition owing to a statistical regularization approach is presented it doe not require the explicit estimation of optic flow field and manages to link those partition in time the identification of the kinematical component of the scene relies on an intermediate layer accomplishing a generic qualitative motion labeling this is achieved considering jointly several successive image no d measurement are required result obtained on several real image sequence corresponding to complex outdoor situation are reported 
a framework is presented for segmenting shallow structure from their background over a sequence of image shallowness is first quantified a affine describability this is embedded in a tracking system within which hypothesized model structure undergo a cycle of prediction and model matching structure emerge either a shallow or non shallow based on their affine trackability this paper reject continuity heuristic for purely image motion in favor of temporal continuity defined a the consistency of generic d model namely shallow structure 
for certain application an autonomous underwater system may benefit from a perception of it environment in term of magnetics gravity chemical signature or other sensing modality an introduction is presented for researcher unfamiliar with issue in underwater remote sensing application area include machine perception for intelligent underwater system large scale seafloor mapping survey and environmental characterization 
an edge detection algorithm based on the regularization theory in which the smoothness is controlled spatially over the image space is presented the algorithm start with an oversmoothed regularized solution and iteratively refines the surface around discontinuity using the knowledge on the structure of discontinuity exhibited in the error signal between the image data and the previous regularized solution the spatial control of smoothness is shown to resolve the conflict between detection and localization criterion the adaptive nature of the algorithm eliminates the need to select image dependent parameter and enables the extraction of multiscale feature from the image the computational aspect of the algorithm a well a it performance on real and synthetic image are considered 
an efficient physically based solution for recovering a d solid model from collection of d surface measurement is presented given a sufficient number of independent measurement the solution is overconstrained and unique except for rotational symmetry a physically based object recognition method that allows simple closed form comparison of recovered d solid model is given the performance of these method is evaluated using both synthetic and real laser rangefinder data 
a model based recognition method that run in time proportional to the actual number of instance of a model that are found in an image is presented the key idea is to filter out many of the possible match without having to explicitly consider each one this contrast with the hypothesize and test paradigm commonly used in model based recognition where each possible match is tested and either accepted or rejected for most recognition problem the number of possible match is very large whereas the number of actual match is quite small making output sensitive method such a this one very attractive the method is based on an affine invariant representation of an object that us distance ratio defined by quadruple of feature point a central property of this representation is that it can be recovered from an image using only pair of feature point 
a unified theoretical framework for motion transparency and motion boundary by devising fundamental constraint equation of multiple optical flow is proposed this framework can handle flow discontinuity at motion boundary a well a flow multiplicity due to transparency of object in a unified manner the constraint equation are formulated by a composition of homogeneously parametrized differential operator on the space time image fitting algorithm for the constraint which result in eigensystem analysis are described to determine the number of flow the author use the margin energy a measure of goodness of fit which is the difference between the first and the second lower eigenenergy of the eigensystem they also hypothesize a criterion for multiplicity the measure and the criterion are derived from the analogy of quantum mechanic it is demonstrated that the margin energy can determine the transparency and discontinuity of the flow field a region of more than one flow 
a spatiotemporal st image cube created by stacking a temporally dense sequence of image together is a temporally coherent data representation using st surface flow i e the extension of optical flow to st surface it is shown how st flow curve can be recovered and then used to detect group of flow curve such that each group represents a single object or surface in the scene undergoing motion the algorithm form cluster of similar flow curve and is based on constraint called the temporal uniqueness constraint first a point in an image can only move to at most one point in the next image second a point in an image can come from at most one point in the previous image when these constraint are violated or it appears that they are violated occlusion or disocclusion ha occurred and therefore can also be detected successful grouping of coherent region of the st cube for two gray level image sequence is shown 
a physically based approach to the recovery of nonrigid d motion and the tracking of nonrigid object is presented the approach make use of deformable superquadrics dynamic model that offer global deformation parameter which capture large scale feature and local deformation parameter which capture the detail of complex shape the equation of motion governing the behavior of the model make them responsive to externally applied force the author extend their prior formulation of these equation to include globally parameterized tapering and bending deformation they further generalize the formulation to handle physically based point to point constraint between model such constraint enable one to automatically assemble object model from interconnected deformable superquadric part these composite model may be used to track the motion of articulated flexible object 
a curvature based approach to estimating nonrigid deformation of moving surface is described conformal motion can be characterized by stretching of the surface at each point this stretching is equal in all direction but is different for different point this stretching function can be defined a an additional with global translation and rotation motion parameter an algorithm for local stretching recovery from gaussian curvature based on polynomial approximation of the stretching function is presented these method require point correspondence between time frame but not the complete knowledge of nonrigid transformation 
the elastic property of real material provide constraint on the type of non rigid motion that can occur and thus allow overconstrained estimate of d non rigid motion from optical flow data it is shown that by modeling and simulating the physic of non rigid motion it is possible to obtain good estimate of both object shape and velocity example using grey scale and x ray imagery are presented including an example of tracking a complex articulated figure 
phase portrait are a powerful mathematical model for describing oriented texture an isotangent based approach is presented which is a linear formulation to the problem to locate the critical point and compute the parameter set of this model for the nonsingular two dimensional first order phase portrait the author classify flow pattern by jordan canonical form of the characteristic matrix made up of the estimated parameter for these system they prove that all the isotangent curve are straight line which intersect at a critical point they also apply least median of square lm estimator to find the isotangent line and locate the critical point a linear regression technique is used to estimate the parameter of the two dimensional first order phase portrait of a given flow pattern result of applying the algorithm to synthetic and real image are presented 
a novel approach to computing egomotion and detecting point not moving rigidly with the scene when an observer move with unrestricted motion is presented the approach using collinear image point is based on an exact method for cancelling effect of the observer s rotation from optic flow for each point only the component of velocity normal to the direction of the line joining the collinear point is needed the algorithm is simple appears robust and is ideal for parallel implementation 
we use a new quot aura quot framework to rewrite the nonlinearenergy function of a homogeneous anisotropicmarkov gibbs random field mrf a a linear sum ofaura measure the new formulation relates mrf sto co occurrence matrix it also provides a physicalinterpretation of mrf texture in term of the mixingand separation of graylevel set and in term ofboundary maximization and minimization within this framework we introduce the use of temperaturefor texture modeling and show how the 
a unified framework for object description and correspondence in range image is proposed this is based on modeling the ambiguity with a dynamic system at every level of hierarchy the critical issue in object description is boundary completion the author show how this step can be modeled a the interaction of long and short term variable operating on partial contour represented by parametric spline in a sense this network operates on an evolutionary basis where boundary completion is modeled a a synergistic process and mutation is modeled a differential decay once reliable surface feature are extracted the same framework with appropriate constraint can find the corresponding surface and compute the transformation 
the quantitative aspect of camera fixation for a static scene are addressed in general when the camera undergoes translation and rotation there is an infinite number of point that produce equal optical flow for any instantaneous point in time using a camera centered spherical coordinate system it is shown how to find these point in space for the case where the rotation axis of the camera is perpendicular to the instantaneous translation vector these point lie on cylinder if the elevation component of the optical flow is set to zero then these point form a circle called the equal flow circle or simply efc and a line i e all point that lie on this circle or line are observed a having the same azimuthal optical flow a special case of the efcs is the zero flow circle zfc where both component of the optical flow are equal to zero a fixation point is the intersection of all the zfcs point inside and outside the zfc can be quantitatively mapped using the efcs it is shown how the concept of the efc and zfc can be used to explain the optical flow produced by point near the fixation point 
a direct method called fixation is introduced for solving the general motion vision problem arbitrary motion relative to an arbitrary rigid scene this method result in a linear constraint equation which explicitly express rotational velocity in term of translational velocity by combining this constraint equation with the brightness change constraint equation general solution are found for translational and rotational velocity and the shape avoiding correspondence and optical flow ha been the motivation behind this direct method which us the brightness gradient directly partial implementation of the fixation method on real image ha shown encouraging result which support some of the presented algorithm 
the bayesian segmentation model developed is motivated by consideration of the information needed for higher level visual processing a segmentation is regarded a a collection of parameter defining an image valued stochastic process by separating topological adjacency and metric shape property of the subdivision and intensity property of each region the prior selection is structured accordingly the novel part of the representation the subdivision topology is assigned a prior by universal coding argument using the minimum description length philosophy that the best segmentation allows the most efficient representation of visual data 
a fuzzy adaptive distance dynamic cluster faddc algorithm which is specially designed to search for cluster that lie in subspace such a line and hyper plane is presented one major drawback of all clustering algorithm is that the number of cluster ha to be known a priori a novel compatible cluster merging ccm technique which find the optimum number of cluster in an efficient way is proposed such subspace clustering technique may be used for character recognition and to obtain straight line description of an edge image they may also be used to obtain planar approximation of d range data the effectiveness of the proposed algorithm in several such situation is demonstrated with real data 
a methodology for classifying syntactic pattern that performs a branch and bound search over a set of prototype is proposed the prototype are first clustered hierarchically and the search is performed over the hierarchy the proposed technique is applied to a pattern recognition system in which image are described by the sequence of feature extracted from the chain code of their contour a rotationally invariant string distance measure is defined that compare two feature string the methodology discussed is compared to a nearest neighbor classifier that us prototype the proposed technique decrease the time required to recognize a pattern by and maintains a recognition rate of greater than 
summary form only given a follows in trying to build complete autonomous and self contained mobile robot that can operate in dynamic environment the author have developed a set of constraint on both sensing and action that are somewhat different from the traditional assumption the author have built a number of small on the order of a few pound in weight totally autonomous mobile robot which do real time sensing some including vision a they navigate about this paper focus on the connection of sensing to action and how that constrains each of them 
semidifferential invariant combining coordinate in different point together with their derivative are used for the description of planar contour their use can be seen a a tradeoff between two extreme strategy currently used in shape recognition invariant feature extraction method involving high order derivative and invariant coordinate description leading to the correspondence problem of reference point the method for the derivation of such invariant based on lie group theory and applicable to a wide spectrum of transformation group is described a an example invariant curve parameterizations are developed for affine and projective transformation the usefulness of the approach is illustrated with two example recognition of a test set of planar object viewed under condition allowing affine approximation and the detection of symmetry in perspective projection of curve 
a method for model based recognition of articulated object in cluttered scene is presented the object consist of rigid part connected by rotary or prismatic joint the method is based on an extension of the generalized hough transform approach it is applicable to various viewing transformation unlike previous method there is no significant degradation in performance for recognition of articulated object compared with the recognition of rigid object containing similar amount of information the method wa implemented and successfully tested for recognition of partially overlapping d object with rotary joint which have undergone rotation translation and scaling 
a generic polyhedral model is represented a a network of node and constraint node are d vector representing the location and orientation of the geometric entity or measure variable such a length or cosine constraint are polynomial equation in the node parameter modeling and recognition are viewed a solving for value of the node parameter such that all the constraint equation are satisfied and the mean square error between the model and the observed shape is minimized buchberger s grobner basis algorithm and ritt wu s triangulation algorithm can be used for eliminating dependent parameter a well a for detecting inconsistency among constraint numerical technique are used to find the best fit model subject to constraint 
a simple map model matching criterion that capture important aspect of recognition in controlled situation is described a detailed metrical object model is assumed a probabilistic model of image feature is combined with a simple prior on both the pose and the feature interpretation to yield a mixed objective function the parameter that appear in the probabilistic model can be derived from image in the application domain by extremizing the objective function an optimal matching between model and image feature result within this framework good model of feature uncertainty allow for robustness despite inaccuracy in feature detection in addition the relative likelihood of feature arising from either the object or the background can be evaluated in a rational way the objective function provides a simple and uniform mean of evaluating match and pose hypothesis several linear projection and feature model are discussed an experimental implementation of map model matching among feature derived from low resolution edge image is described 
a variety of architecture have been proposed for integrating vision module into a complete system an examination of these system indicates that they either lack analytic computational tractability or are based on restrictive assumption such a global additivity or sequential decoupled solution of objective the author introduce a framework of integration which overcomes both of these shortcoming this proposed model is based on game theoretic setting in which a correspondence between a vision system and a n player game is established the author demonstrate the power of such a model within a comparative study in the context of a system aimed at delineating d deformable contour from noisy image 
a highly integrated vme based board based on parallel pipeline projection engine pppe architecture containing custom vlsi application specific ic asics that implement the forward and inverse radon transforms in real time ha been designed and fabricated the board contains dsp microcomputer at t dsp c that provide the necessary modularity and programming power to support custom radon transform asics parallel and pipelined processing occurs at both the ic and the board level the asic that executes the highly computationaland i o intensive forward and inverse radon transform algorithm ha been designed and fabricated in a micron scalable cmos process it operates at the required mhz video rate and consists of over transistor 
a vectorization method for line pattern which convert digital binary image into line segment vector is proposed the proposed method suppresses shape distortion a well a pseudo feature point so it produce more compact and natural shaped vector data than existing method the experimental result of applying the proposed method to geographical map show that the vectorization method reduces data volume by and suppresses shape distortion more than is possible with a straightforward method 
three parallel implementation based on three alternate sequential approach are presented the first design is a systolic array based on the known sequential method an execution time of o n m is achieved with nm processing element pe with each pe composed of simple logic element the second design employ broadcast bus feature to speed up the execution of an alternate sequential method linear speedup is achieved by using nm processing element the sequential method ha an execution time of o n m and the proposed parallel design run in o nm time the third design is a modified approach which is well suited for implementation on general purpose machine these design achieve superior performance compared with the existing design in term of their simplicity execution time and domain of application using the proposed design an efficient parallel implementation of stereo matching based on linear segment a primitive is derived 
up to now all the known euclidean distance function algorithm are either excessively slow or inaccurate and even danielsson s method produce error in some configuration the author show that these problem are due to the local way distance are propagated in image by this algorithm to remedy these drawback an algorithm which encodes the object boundary a chain and propagates these structure in the image using rewriting rule is introduced the chain convey euclidean distance and can be written above one another thus yielding exact result in addition the proposed algorithm is particularly efficient some of it application to skeleton and neighborhood graph are described 
the aim of this research is to obtain a morphological pyramid with the aid of alternating sequential filter the author establish a relationship between alternating sequential filter and the morphological sampling theorem developed by r haralick et al an alternative proof for opening and closing in the sampled and unsampled domain using the basis function is shown this decomposition is then used to show the relationship of the compound mapping opening closing in the sampled and unsampled domain an upper and a lower bound for the above relationship are presented under certain circumstance an equivalence is shown for opening closing between the sampled and the unsampled domain an extension to more complicated algorithm using union of opening and intersection of closing is also proposed 
the boundary element method bem for solving poisson s equation is described issue in bem such a green s function boundary condition evaluation of improper integral and continuity up to first derivative of solution function are discussed bem is compared with fem the finite element method in term of storage and time complexity the author discus application to vision height from gradient shape from shading surface interpolation brightness based stereo matching and the optical flow problem brief mention is made of some early experimental result on synthetic image 
the author assume a lambertian shading model with unknown light source direction it is shown that information on the occluding boundary put strong constraint on the light source direction for a small number of light source from sufficiently different direction additional information from intensity extremum will be sufficient to determine the source direction if there are two or three view of the object then the light source can be determined without extremum information the author report experiment performed on synthesized image 
a facial feature classification technique that independently capture both the geometric configuration and the image detail of a particular feature is described the geometric configuration is first extracted by fitting a deformable template to the shape of the feature for example an eye in the image this information is then used to geometrically normalize the image in such a way that the feature in the image attains a standard shape the normalized image of the facial feature is then classified in term of a set of principal component previously obtained from a representative set of training image of similar feature this classification stage yield a representation vector which can be used for recognition matching of the feature in term of image detail alone without the complication of change in facial expression implementation of the system is described and result are given for it application to a set of test face these result show that feature can be reliably recognized using the representation vector obtained 
the use of direct motion vision for determining the depth of a scene is investigated to permit direct comparison of analytical and experimental result only translational motion and planar patch of constant depth are considered the analysis show that the accuracy with which depth can be determined increase with the sum of the square of the temporal derivative over the patch this quantity is referred to a the apparent size of the patch after determining the relationship between relative depth error and apparent size analytically a number of experiment with camera generated image sequence were performed in nearly all of these experiment the agreement between the analytically determined and measured value of the relative depth error is very good 
a camera for extracting depth information from a scene is described it incorporates a single main lens along with a lenticular array placed at the sensor plane the resulting plenoptic camera provides information about how the scene would look when viewed from a continuum of possible viewpoint bounded by the main lens aperture deriving depth information is simpler than in a binocular stereo system because the correspondence problem is minimized the camera extract information about both horizontal and vertical parallax which improves the reliability of the depth estimate 
an algorithm that identifies interreflection by using chromatic information in color image is presented several experimental result from test image show that the proposed method could play the role of an added enhancement stage in the segmentation method using the dichromatic reflection model and could be an independent identification scheme by itself the algorithm doe not consider multiple bounce of light and deal with body to body interreflection only this model must be combined with other intrinsic model of surface property such a metal which exhibit strong surface reflection but not body reflection 
an optimal deformable contour approximation algorithm using a cardinal form piecewise cubic spline pc curve representation is presented the approximation is optimal in the sense of least square error in both location and orientation the knot are set automatically at high curvature position the sample data are generated by a robust edge fragment detection algorithm which is optimal in the sense of a weighted absolute error an initial contour placement algorithm us a penalized maximum likelihood algorithm to group the edge fragment together for an initial contour a controlled deformable contour algorithm refines the initial contour to cover meaningful edge feature 
the relationship between the skeleton and the voronoi diagram for the case of morphologically regular set is described using the discrete point voronoi diagram a practical algorithm is developed to compute an approximate skeleton of a continuous shape from sampled data the analysis also yield a sampling criterion under which the approximation is stable the algorithm is applied to a simple test image 
a formal representation of corner and vertex detection is presented in particular an analytical study is presented which allows one to know exactly what the behavior is of a detector around trihedral vertex it is shown that near three surface two elliptic maximum of det exist and their location is inside extremal contrast surface the intermediate surface always show a hyperbolic minimum it is shown that the detector allows to find the exact position of vertex the approach proposed ha been tested on many noisy synthetic data and real image and it robustness seems promising 
the author present several parallel implementation of the hough transform on a shared memory multiprocessor namely the encore multimax various implementation strategy are described and result are discussed 
a linear generalized hough transform light in which a linear numeric pattern is used to replace the single peak in the original generalized hough transform ght is presented the light is more capable of detecting partially occluded object moreover it is well suited for parallelization especially on simd array processor several sequential and parallel light algorithm have been developed preliminary result for parallel implementation were obtained from and simd one dimensional array processor the ai vision processor with processing element 
the main contribution of this research are the derivation of the probability density function pdf of disparity change in stereo matching based on the pdf of depth change in the world and on the parameter of the stereo image formation process the definition of a match support equation based on the derived pdf and the incorporation of the support equation into a relaxation matching algorithm the derived pdf and support equation are applicable to many existing stereo algorithm 
ellipsis seen in an image may be the d projection of d circle from the scene given this assumption ellipsis can be grouped into perceptual group from which inference about the d structure of object can be made method are proposed for extracting grouping corresponding to surface of revolution a hough transform approach is used for grouping after which the confidence in the plausibility of the perceptual group is improved by detecting symmetry grouping 
the author extend the algorithm for stereo vision on a mobile robot in a unstructured environment to generate the d b rep of the large environment by incrementally integrating local and incomplete stereo data the stereo vision is based on the d b rep of stereo image the d b rep is a d version of the d b rep to describe an image the image is represented in a hierarchy of region boundary and line segment the b rep is obtained for each of stereo image and the two image are matched on this data structure the direct output of stereo is d wireframes a set of d loop boundary composed of d line segment of the scene a unique surface is then defined for each loop to make the d b rep the consistent geometrical model of the scene it is necessary for the mobile robot to change view of the scene to make uninterpretable loop interpretable by finding missing segment and thus make the complete b rep of the scene 
an artificial neural network ann architecture termed a dual network is proposed for pattern classification problem dual network is a network of densely connected simple processing element and it present a structured way to implement polynomial classifier a supervised learning algorithm is developed for the dual network and their ability to solve complex pattern classification problem is verified through experimental study 
an approach for representing signature in an offline environment is presented the approach first make a tracing of a signature similar to the way a human normally doe using hierarchical decision making for stroke identification and ordering based on a set of heuristic rule the dynamic information from the tracing sequence is then incorporated into the representation of the signature a multiresolution critical point segmentation method is used to extract local feature point at varying degree of scale and coarseness for subsequent representation of the signature experimental result are discussed 
a system is presented for modelling solid of revolution from a set of a few monocular image the reconstruction is obtained by assuming only that the viewed object belongs to this particular class of object the knowledge of the camera displacement between each image is not required the modeling is based on the resolution of the inverse perspective problem for the contour point detected in the different image 
the notion of uncertainty manipulation in robot navigation is applied to the task of model based object recognition based on the fact that every detected object feature in an image must be associated with some uncertainty the author show how to iteratively refine the estimate of pose uncertainty of a hypothesized object when a new feature is matched and how to dynamically determine a proper search window to look for the next model feature the advantage of this method over the conventional static search window is demonstrated in an actual object recognition task 
a solution to the correspondence problem using constraint satisfaction is described it us a real time line fitting algorithm to detect change in a point s motion parameter a they happen a trajectory is hypothesized for a single point s motion since the event detected may be wrong multiple trajectory are hypothesized for each point a correspondence is drawn from the set of hypothesized trajectory the algorithm is robust working on noisy and non rigid motion data and doe not use false precision 
the author propose four scale space object detection algorithm for separating object from the background these algorithm do not need thresholding at any of the scale the different algorithm are applicable to image with different noise and clutter characteristic statistical analysis of the four algorithm is conducted for noisy and cluttered background 
rigid d object were modelled automatically from an image sequence taken by a camera that wa rotated around the object the image sequence wa recorded using a calibrated camera which allows one to measure the camera position and to estimate the true object size the d object shape wa obtained in two step the object silhouette were employed to find the enclosing volume of the object the volume wa converted into a flexible surface representation and the d shape wa refined based on the texture information of the object surface texture mapping wa applied to generate a highly realistic d model of the object 
sequence of symbol generated by a visual and action sequence provide information about the natural structure of the world hmms hidden markov model provide one way to learn recover store produce manipulate and analyze both visual sequence and associated knowledge structure for computer vision 
a fast and exact euclidean distance transformation using grayscale mathematical morphology is presented the large structuring element required for this operation is decomposed into iterative application of simple window this is possible because the square of the euclidean distance matrix is easily decomposable non square pixel can also be used in this application 
a solution to extract d shape from a structured light image is presented the d striped image is segmented into region corresponding to quadratic surface a fuzzy analysis of the stripe s property allows one to locate discontinuity and to model stripe part these part are tracked in consecutive image and matched in order to create region segmentation result are presented which were obtained with a real scene consisting of multiple object of arbitrary shape 
the author propose an active solution of optimally recovering d scene depth from a sequence of picture the choice of the difference between camera location for the different picture in the sequence is described the tradeoff between short and long distance between successive location of the camera is shown in term of occlusion perspective transformation and exact triangulation 
the system described is able to extract gripping information for a robot in most of the case occurring in real life for this step no individual object model were necessary however it wa essential to use other general information on the object like their symmetry property it wa computationally advantageous to use contour information and not raster information a method to deal with transparent and specularly reflecting object is discussed 
three dimensional viewpoint invariance is an important requirement in the representation of surface for recognition task parameterized surface posse this desirable property in general the parameter in a parametric surface representation can be arbitrarily defined a canonical intrinsic parameterization provides a consistent invariant form for describing surface the goal of this work is to define and construct such a parameterization 
most approach to simultaneously recovering both model parameter and segmentation have relied on an edge field to represent segmentation this restriction and the implementation it lead to have fundamental limitation in representing occluded stimulus the author develop a framework that overcomes these limitation by using multiple layer of explicit support to represent segmentation result from an initial implementation demonstrate that this method can segment image containing occluded object 
an algorithm is presented which estimate parameter for motion and structure of a rigid planar patch given point correspondence in a monocular image sequence under perspective projection the rotational velocity is assumed constant and the rotation center arbitrary the algorithm mainly consists of two step first the d space of x y z is searched exhaustively and for each x y z all the other parameter with the value of an objective function are computed linearly some of the x y z and the corresponding structure value are used a the initial guess in the second step the objective function is iteratively minimized with respect to five variable for rotation and structure the solution corresponding to the global minimum is used to obtain least square estimate of the remaining unknown for translation and rotation center it wa experimentally found that the objective function converges well so that the d space need not be densely searched result are presented for three image sequence two simulated and one real 
a technique is presented for estimating the position and pose of an autonomous mobile robot navigating in an outdoor urban environment consisting of polyhedral building the d description of the roof top of the building are assumed to be given a a world model a discussion is presented of the us of an intermediate representation of the environment called the edge visibility region evrs in the positional estimation process 
a closed loop image segmentation system that incorporates a genetic algorithm to adapt the segmentation process to change in image characteristic caused by variable environmental condition is presented the genetic algorithm efficiently search the hyperspace of segmentation parameter combination to determine the parameter set which maximizes the segmentation quality criterion a summary of the experimental result that demonstrates the ability to perform adaptive image segmentation and to learn from experience using a collection of outdoor color imagery is given 
an algorithm is described which performs curvilinear grouping of image edge element for detecting object boundary the method work by generating hypothesis and selecting the best one a neighborhood definition based on delaunay graph is used to keep the number of generated hypothesis small an energy minimizing curve is fit to the generated hypothesis to evaluate the grouping and locate discontinuity 
a target location is represented by compressing a image from a spherical mirror into a circular waveform navigation task are specified a a sequence of homing task on target location this is accomplished by matching landmark from the current view with the next target view a method is provided for matching using qualitative geometric feature of each of the waveform in addition a geometric analysis allows one to do d reasoning about the environment which is sufficient for computing the rotation and translation between the current location and the target location it eventually may allow acquisition of a d model of the environment 
the notion of consensus structure a special form of random graph is formally defined the consensus structure represents an extended higher order representation of the random n tuple for statistical and structural pattern recognition an algorithm inferring the consensus structure from a random n tuple is designed based on the detection of statistical interdependency under certain structural constraint in applying structural constraint a circular diagram indicating variable interaction is used to extract global structural feature in a macromolecular modeling problem 
a novel method for character recognition targeted at complex annotation found in engineering document is presented a feasibility study is described in which character extracted from engineering drawing were recognized without error from a class of distinct alphanumeric pattern by a neural network classifier trained with multiscale representation an incremental strategy is presented for resolution which relies upon the continuity between hierarchical level of a novel multiscale decomposition the author observed a fold reduction in the amount of information needed to represent each character for recognition these result suggest high reliability at a reduced cost of representation 
two pattern recognition algorithm are proposed to detect two type of knot in lumber lumped and surface knot using an optical scanner it is shown that lumped knot can be recognized by a log profile approach and surface knot can be detected by an image processing approach a mathematical model for describing the geometry of the knot inside the log based on the surface parameter of the knot is presented by a log breakdown strategy the log can be cut into a maximum number of board with prescribed dimensionality the mathematical knot model can be used to predict the location size orientation and shape of the knot on these board whereby their lumber grade can be determined summing up the value yield of all the board give the value yield of the log by computing all the possible value yield of the log from different log orientation one can determine a maximum value yield of a log 
it is suggested that non smooth or complex boundary are processed by the visual system so a to obtain a simple abstract description called contour texture which contains much le information than that provided by the location of all point of the contour a discussion is presented of what the nature of the contour texture of a curve is and it relation with two dimensional texture which the author contends should be thought of a a separate concept several application are suggested and an implemented filter based scheme is presented 
a system is presented for the simultaneous estimation of surface and motion parameter of a free flying object in a telerobotics experiment the system consists of two main component a vision based invariant surface and motion estimator and a kalman filter an algorithm for invariant surface and motion estimation from sparse multi sensor range data is presented motion estimate from the vision module are input to a kalman filter kf for tracking a free flying object in space the predicted motion parameter from the kf are fed back to the vision module and serve a an initial guess in the search for optimal motion 
a discontinuity detector based on the pervasive noise process near object boundary produced by certain early vision algorithm is described contrary to the assumption embodied in most discontinuity detector the noise in surface property data such a stereo disparity is not spatially white gaussian noise this noise is briefly illustrated to facilitate discontinuity detection the location of intensity edge are used a prior estimate to the true discontinuity location the discontinuity detector is formulated a a markov random field although variation of the approach described should be amenable to hardware implementation 
depth map obtained from focus ranging can have numerous error and distortion due to edge bleeding feature shift image noise and field curvature an improved algorithm that examines an initial high depth of field image of the scene to identify region susceptible to edge bleeding and image noise is given focus evaluation window are adapted to local image content and optimize the tradeoff between spatial resolution and noise sensitivity an elliptical paraboloid field curvature model is used to reduce range distortion in peripheral image area spatio temporal tracking compensates for image feature shift the result is a sparse but reliable depth map 
a closed form solution to the problem of computing d curved surface shape from texture cue is presented an expression showing the dependence of localized image spectral moment on localized surface spectral moment and on local surface orientation is derived the local image spectrum are measured with wavelet and the expression is solved for the surface orientation at each point because the method us localized spectral information it operates at a very low level in the visual hierarchy no extraction of texture or edge element is required the wavelet based computation used is biologically plausible easily parallelized for rapid computation and ha been shown to be the basis for effective solution to a variety of other vision task the method is demonstrated on a number of real world example 
a robust method for describing planar curve in multiple resolution using curvature information is presented the method is developed by taking into account the discrete nature of digital image a well a the discrete aspect of a multiresolution structure pyramid the main contribution lie in the robustness of the technique which is due to the additional information that is extracted from observing the behavior of corner in the whole pyramid furthermore the resulting algorithm is conceptually simple and easily parallelizable theoretical result are developed analyzing the curvature of continuous curve in scale space and showing the behavior of curvature extremum under varying scale the result are used to eliminate any ambiguity that might arise from sampling problem due to the discreteness of the representation experimental result demonstrate the potential of the method 
it is shown how a parameterization of the structure of the observed object interpretable a the space of dimension of generative operation which brought the object into existence entail a lattice that enumerates all the allowable category and subcategories for the class of object along with an inferential preference hierarchy among them each model is constrained to be generic in it parameterization so that each node in the lattice stand in for an entire class of object that all being parameterized the same way can all be treated a equivalent to one another the observed object s natural category 
a direct method is proposed for locating the focus of expansion based on simple parallel computation in selected region of the image each is a circular patch around an estimated focus of expansion foe simple computation allow determining the direction from the estimated to the true foe the best estimate of the intersection of the so called foe constraint line for each region give the location of the foe some analysis allows a confidence measure to be assigned to the information from each local region in order to give more weighting to the most reliable data hence the foe can be located with more accuracy even when the data from various local region lack sufficient information 
a method for the estimation of point correspondence on a surface undergoing nonrigid motion based on change in gaussian curvature is described an approach for estimating the point correspondence and stretching of a surface undergoing conformal motion with constant homothetic linear or polynomial stretching is proposed small motion assumption is utilized to hypothesize all possible point correspondence curvature change are then computed for each hypothesis the difference between computed curvature change and the one predicted by the conformal motion assumption is calculated the hypothesis with the smallest error give point correspondence between consecutive time frame simulation performed on ellipsoidal data illustrate the performance and accuracy of derived algorithm the algorithm is applied to volumetric ct data of the left ventricle of a dog s heart 
an approach for tracing representation and recognition of a handwritten numeral in an offline environment is presented a d spatial representation of a numeral is first transformed into a d spatiotemporal representation by identifying the tracing sequence based on a set of heuristic rule acting a transformation operator given the dynamic information of the tracing sequence a multiresolution critical point segmentation method is proposed to extract local feature point at varying degree of scale and coarseness a neural network architecture the hierarchically self organizing learning hsol network s lee j c pan especially for handwritten numeral recognition is presented experimental result based on a bidirectional hsol network indicated that the method is robust in term of variation deformation and corruption achieving about recognition rate for the test pattern 
a connectionist model for recognizing unconstrained handprinted digit is described instead of treating the input a a static signal the image is canned over time and converted into a time varying signal the temporalized image is processed by a spatiotemporal connectionist network the resulting system offer shift invariance along the temporalized axis a reduction in the number of free parameter and the ability to process image of arbitrary length for a set of real world zip code digit image the system achieved a recognition rate on the training set and a recognition rate on the test with no rejection a recognition rate on the test set wa achieved when of the image were rejected 
a survey and critique of previous work is given and two object based heuristic are developed the structured nature of object is the motivation for the nonaccidental alignment criterion parallel line within the object s bounding contour are related to the object centered coordinate system the regularity and symmetry inherent in many man made object is the motivation for the orthogonal basis constraint an oblique set of coordinate ax in the image is presumed to be the projection of an orthogonal set of d coordinate ax in the scene these heuristic are demonstrated on real and synthetic image contour 
the author attempt to combine result from different vision cue such a contour motion and shading in an image sequence to obtain a complete a d model a possible two problem must be solved first different feature may appear to be the same image data they should be classified so that different method can be applied precisely second output from different approach may supplement or overlap each other they should be qualified and then combined correctly and effectively these issue are addressed by analyzing an image sequence taken when object rotate around an axis the feature are verified according to their behaviour in the continuous image unknown area and area with single or multiple measurement are located a strategy for combining these data is given 
a technique for producing a triangulated irregular network tin from a digital elevation model dem is described the overall goal is to produce an approximate terrain description that preserve the major topographic feature using a greatly reduced set of point selected from the original dem the tin generation process is iterative at each iteration area in the dem that lie outside of a user supplied error tolerance in the tin are identified and point are chosen from the dem to more accurately model these area point selection involves the computation of the difference between the actual dem and an approximate dem this approximate dem is calculated by interpolating elevation point from the tin 
the author report the implementation and evaluation of a function based recognition system that take an uninterrupted d object shape a it input and reason to determine if the object belongs to the superordinate category furniture and if so into which sub category it fall the system ha analyzed over input object and the result largely agree with intuitive human interpretation of the object the study confirms that a relatively small number of knowledge primitive may be used a the basis for defining a relatively broad range of object category the greatest derivation from intuitive human interpretation occurs with object that human would not typically label a one of the known category defined but which have some novel orientation in which they could serve the function of one of these category this is because the system us a purely function based definition of the object category 
progress in building and recognizing model of object for an autonomous vehicle for on road and cross country navigation is reported the object model are stored in a map and are used a landmark for estimating vehicle position the landmark can be used a intermediate control point at which the vehicle must take some prescribed action in the case of a complex mission robust object tracking using sequence of range image and building and updating d object representation is presented tracking us object prediction from one image to the next to accurately compute object location object representation are built by merging set of point from individual image into a single set in an object centered coordinate frame the sparse set of point is then segmented into shape yielding compact and general object representation an algorithm for landmark identification in range image is introduced in the context of map based navigation 
the author present a d measurement method using a projected grid that permit an optimal geometrical exploitation of the calculated d data they propose a reliable method for solving the correspondence that establishes the link between the imaged grid and the original grid curve per curve without any ambiguity the imaged grid is extracted by an efficient method that describes the distorted grid a a network of curve rather than a graph and hence provides regular and precise d information the d projected grid is calculated per curve by using the triangulation principle thus two independent family of d curve that parameterize the object surface can be used for a surface geometrical study 
an approach for autonomous localization of ground vehicle on natural terrain is proposed the localization problem is solved using measurement including attitude heading and distance to specific environmental point the algorithm utilizes random acquisition of distance measurement to prune the possible location s of the viewer the approach is also applicable to airborne localization 
three dimensional edge detection in voxel image is used to locate point corresponding to surface of d structure and the local geometry of these surface is characterized in order to extract point or line which may be used by registration and tracking procedure typically second order differential characteristic of the surface must be calculated to avoid the problem of establishing link between d edge detection and local surface approximation it is proposed to compute the curvature at location designated a edge point using the partial derivative of the image directly by assuming that the surface is defined locally by an iso intensity contour it is possible to calculate directly the curvature and characterize the local curvature extremum ridge point from the first second and third derivative of the gray level function these partial derivative can be computed using the operator of the edge detection experimental result obtained using real x ray scanner data are presented 
probability density function pdfs are derived for many of the geometric measurement upon which stereo matching technique are based including orientation difference between matching line segment or curve the gradient of disparity the directional derivative of disparity and disparity difference between match the pdfs resulting from the transformation are used to critically examine many existing stereo technique several technique based on these pdfs are proposed 
the author investigate the effect of neglecting lens distortion and present a theoretical analysis of the calibration accuracy the error bound derived is a function of a few factor including the number of calibration point the observation error of d image point the radial lens distortion coefficient and the image size and resolution this error bound provides a guideline for selecting both a proper camera calibration configuration and an appropriate camera model while providing the desired accuracy experimental result from both computer simulation and real experiment are given 
a technique for the calibration of an active camera system is presented the calibration of manipulator camera to manipulator camera and base to world is treated in a unified and elegant way in this approach the camera frame and manipulator link frame are all related to the world frame therefore the camera to manipulator and base to world calibration is very straightforward the approach is simple since it us the form of one equation solving one parameter two experiment that verify the accuracy of the technique are reported 
contemporary stereo correspondence algorithm disambiguate multiple candidate match by using a spatial hierarchy mechanism narrow occluding object in d scene cannot be handled by the spatial hierarchy mechanism alone the author propose a dynamic disparity search dd framework that combine the spatial hierarchy mechanism with a new disparity hierarchy mechanism to reduce the stereo matching error caused by narrow occluding object they demonstrate the merit of the dd approach on real stereo image 
a priori knowledge of the relative position of four or more coplanar point or line is used to derive the position of other point and line on the same plane in a manner invariant to camera location and intrinsic camera parameter a framework for data fusion in the projective plane is presented to merge the position estimate of coplanar point and line derived in this way 
under the traditional paradigm of considering vision a a recovery problem visual interception is just another application of the structure from motion module however the inherent difficulty of three dimensional reconstruction have delayed any real time application the author offer a robust solution under the active qualitative vision paradigm from the image intensity function they obtain the locomotive intrinsics of the agent and the target based on this relative information they present a control strategy that decides in real time whether the velocity of the agent should be increased or decreased at any time instant thus guiding the agent to intercept the target the problem of visual interception can thus be solved by simple computation without correspondence 
the author previously introduced a direct method called fixation for the recovery of shape and motion in the general case that us neither feature correspondence nor optical flow instead it directly us the spatio temporal gradient of image brightness the experimental result of applying some of the author s fixation algorithm to a sequence of real image where the motion is a combination of translation and rotation are reported technique for autonomous choice of parameter that result in good estimate for important motion parameter are described 
a method for computing depth from a single shaded image is presented discrete approximation for p and q using finite difference are used and the reflectance in zij is linearized the method is faster since each operation is purely local in addition it give good result for spherical surface in contrast to other linear method 
weak lambertian assumption is proposed and used to determine shape and pose of cylindrical object from a monocular intensity image the method doe not require the knowledge of lighting condition light intensity and lighting direction surface property or albedo experimental result for both synthesized and real image showing the validity of the method are presented 
the problem of recognizing and estimating the shape of object with special marking text symbol drawing etc on their surface using b spline curve modeling and a pair of binocular image is considered a a direct consequence of the invariance of the b spline to affine transformation the computation of the d coordinate of the object curve point from a pair of stereo image becomes a straightforward task and is obtained using minimum mean square error estimation the d curve is recovered from the estimated object curve control point and is subsequently used for the object shape estimation and classification 
the observation of curved contour in image sequence is used in egomotion estimation and in surface reconstruction an egomotion technique that can be applied when no point or straight line correspondence are available is presented it generalizes egomotion to the case of arbitrarily shaped contour which is especially valuable in the case of nonpolyhedral object the computation us a very simple finite difference scheme and quickly provides a good estimation of the motion parameter experiment conducted on synthetic and real data show the validity of the approach 
the author describe how to design color illumination to improve the discriminability of object in color image this procedure is useful in application where the illumination can be controlled such a inspection task from the physic of color image formation the optimal color illumination for discriminating material is derived using a parametrically defined set of illuminant the author suggest how such an approach might be extended to set of material and more general class of light source experiment with painted color patch and live potato plantlet are used to illustrate the usefulness of actively controlling illumination color in machine vision 
it is shown how to actively compute qualitative shape property of a surface directly from image intensity without first quantitatively reconstructing the surface the approach diverges from classical active shape from shading in that two different type of lighting condition a diffuse source and a point source are used rather than two instance of a single type a point source there is no attempt to compute a dense depth map of the surface rather the presence of certain qualitative geometric feature is detected in particular a shape property surface aperture is introduced and it is shown that under diffuse lighting there is typically a spatial correspondence between the minimum of surface aperture and the minimum of image intensity thus the minimum in surface aperture can be located different possible cause of the aperture minimum can subsequently be distinguished by actively illuminating the surface with a point source positioned in the viewing direction 
two problem pertinent to using implicit higher degree polynomial in real world robust system are dealt with characterization and fitting algorithm for the subset of these algebraic curve and surface that is bounded and exists largely in the vicinity of the data a mahalanobis distance for comparing the coefficient of two polynomial to determine whether the curve or surface that they represent are close over a specified region these tool make practical use of geometric invariant for determining whether one implicit polynomial curve or surface is a rotation translation or an affine transformation of another the approach is ideally suited to smooth curve and smooth curved surface that do not have detectable feature 
a methodology for accurate determination of surface normal and light source location from depth and reflectance data is introduced estimation of local surface orientation using depth data alone from range finder with standard depth error can produce significant error while shape from shading using reflectance data alone produce approximate surface orientation result that are highly dependent on the correct initial surface orientation estimate and regularization parameter combining these two source of information give vastly more accurate surface orientation estimate under general condition than either one alone and can also provide better knowledge of local curvature novel iterative method that enforce satisfaction of the image irradiance equation and surface integrability without using regularization are proposed these iterative method work when the light source is any finite distance from the object producing variable incident light orientation over the object 
a formulation of the part decomposition problem motived by the minimum description length mdl criterion is presented unlike previous mdl approach which use analytic function a general geometric constraint convexity is used a a part constraint therefore the method is suitable for complex natural shape such a human face the recovery process consists of a bottom up grouping process and a subsequent optimization process based on the mdl criterion the definite causal relation of part structure between different sensitivity level are used to recover the hierarchy of part structure part decomposition experiment involving real d range image are reported 
algorithm to detect pair of edge that could be end of a straight homogeneous generalized cylinder shgc are proposed geometrical constraint for the end of an shgc are utilized to group edge element and edge segment in a complex image two method are investigated the first algorithm is for a subset of shgcs for which scaling factor of the cross section at both end are the same the second algorithm is for any shgc however a modified version is implemented to reduce computation given a reference and edge it find the edge possibly paired with it several example of end extracted from real image are reported to show the feasibility of the algorithm 
the problem of object segmentation and binding are addressed within a biologically based network model capable of determining depth from occlusion in particular the author discus two subprocesses most relevant to segmentation and binding contour binding and figure direction they propose that these two subprocesses have intrinsic constraint that allow several underdetermined problem in occlusion processing and object segmentation to be uniquely solved simulation that demonstrate the role these subprocesses play in discriminating object and stratifying them in depth are reported the network is tested on illusory stimulus with the network s response indicating the existence of robust psychological property in the system 
a physiologically motivated model of illusory contour perception is examined by simulating a neural network architecture that wa tested with gray level image the result indicate that a model that combine a bottom up feature aggregation strategy with recurrent processing is best suited for describing this type of perceptual completion 
an approach to feature extraction that eliminates binarization by extracting feature directly from gray scale image is presented it not only allows the processing of poor quality input e g low contrast dirty image but also offer the possibility of significantly lower resolution for digitization 
a complete scheme for totally unconstrained handwritten word recognition based on a single contextual hidden markov model hmm is proposed the scheme includes a morphologyand heuristic based segmentation algorithm and a modified viterbi algorithm that search the l st globally best path based on the previous l best path the result of detailed experiment for which the overall recognition rate is up to are reported 
image analysis using texture and multifractal paradigm is addressed multifractal theory and it application to image description are discussed and it is shown that this approach allows the discrete signal to be worked on directly a system for texture classification that is based on a learning scheme and doe not make use of any a priori model is introduced image segmentation is then considered and the notion of mixed class which allows accurate detection of texture boundary on complex image is introduced 
the author randomly sample appropriate range image point and solve equation determined by these point for the parameter of selected primitive type from k sample they measure residual consensus to choose one set of sample point that determines an equation having the best fit for the largest homogeneous surface patch in the current processing region the residual consensus is measured by a compressed histogram method that work at various noise level the estimated surface patch is extracted out of the processing region to avoid further computation a genetic algorithm is used to accelerate the search speed 
the straight homogeneous generalized cylinder shgc comprises a class of object for which recovery of pose and shape from image is generally an underconstrained problem it is shown that for a major subclass of shgcs namely the right straight homogeneous generalized cylinder the d pose tilt and slant and shape cross section and scaling function can be completely determined if the cross section are symmetrical from the mutual orthogonality of the cylinder axis the symmetry axis and transverse axis of the cross section their slant can be determined from their tilt the d orientation of their projection onto the image the d cylinder axis and skewed symmetry axis of the cross section are extracted by using the property that tangent to the image curve at corresponding point meet on the ax once the pose is recovered the cross section and scaling function of the object can also be determined from the cross section contour and extremal contour respectively 
a novel method for programming a robot called the assembly plan from observation apo method is proposed the apo method aim to build a system that ha the capability of observing a human performing an assembly task understanding the task based on the observation and generating the robot program to achieve the same task assembly relation that serve a the basic representation of each assembly task are defined it is verified that such assembly relation can be recovered from the observation of human assembly task and that from such assembly relation it is possible to generate robot motion command to repeat the same assembly task an apo system based on the assembly relation is demonstrated 
tea is a task oriented computer vision system that us bayes net and a maximum expected utility decision rule to choose a sequence of task dependent and opportunistic visual operation on the basis of their cost and present and future benefit the author discus technical problem regarding utility present tea s utility function which approximates a two step lookahead and compare it to various simpler utility function in experiment with real and simulated scene 
the development of the visual process that would be needed by a mobile robot system for visually intercepting a moving target is considered many relevant active visual process are proposed that provide robust input for qualitative motion control strategy the process for detecting independent motion and for monitoring progress toward the moving target are summarized 
for several useful task in photogrammetry and in model based vision noniterative method that require only the inversion of system of linear equation are developed the method are based on the theory of projective invariant the task addressed are resection intersection and transfer or model matching with or without ground control point the following kind of transfer are examined a coplanar object point transfer to image done using four reference point in image b stereo camera system transfer to stereo camera pair done using four reference point in stereo pair c general multicamera configuration transfer of a ninth point to image done using eight tie point in image and 
a method of visual motion recognition applicable to a range of naturally occurring motion that are characterized by spatial and temporal uniformity is described the underlying motivation is the observation that for object that typically move it is frequently easier to identify them when they are moving than when they are stationary specifically it is shown that certain statistical spatial and temporal feature that can be derived from approximation to the motion field have invariant property and can be used to classify regional activity such a windblown tree ripple on water or chaotic fluid flow that are characterized by complex non rigid motion the technique is referred to a temporal texture analysis in analogy to the technique developed to classify gray scale texture the technique are demonstrated on a number of real world image sequence containing complex movement the work ha practical application in monitoring and surveillance and a a component of a sophisticated visual system 
an approach to fitting of implicit algebraic curve and surface to point data is introduced two family of polynomial with bounded zero set are presented member of these family have the same number of degree of freedom a general polynomial of the same degree method for fitting member of these family of polynomial to measured data point are described experimental result for set of point in r and r for curve and surface respectively are presented 
a basic review of the bundle adjustment with self calibration is given the mathematical model parameter estimation and quality analysis of calibration and positioning are covered the calibration procedure hardware and testfield of an accuracy investigation with off the shelf ccd charge coupled device camera lens and frame grabber are described a relative accuracy in object space of part in and an accuracy of in image space were verified by independent measurement with theodolite 
a method of obtaining local projective and affine invariant that is more robust than existing method is presented these shape descriptor are useful for object recognition because they eliminate the search for the unknown viewpoint being local these invariant are much le sensitive to occlusion than the global one used elsewhere the basic idea are i using an implicit curve representation without a curve parameter thus increasing robustness and ii using a canonical coordinate system which is defined by the intrinsic property of the shape regardless of any given coordinate system and is thus invariant several configuration are treated a general curve without any correspondence and curve with known correspondence of feature point or line 
the variation in digitized pixel value that is due to sensor noise and scene variation is quantified using physical model for ccd charge coupled device video camera and material reflectance this analysis form the basis of algorithm for camera characterization and calibration and for surface description the use of these technique for estimating a measure of scene variation is described this measure is independent of image irradiance and can be used to identify a surface from a single sensor band over a range of situation experimental result verify the model presented 
a method for determining the relative view orientation of randomly acquired projection of asymmetric object is described it extends the projection slice theorem by determining the relative orientation of projection by the location of line of intersection among the fourier transforms of the projection in d fourier space the complete algorithm is described and it efficacy is demonstrated using real data 
the registration of multiple d data set obtained with a laser range finder is examined a sensor calibration technique based on the conjunction of a mathematical camera model the n plane b spline npbs with an accurate mechanical calibration setup is proposed an algorithm for recovering the rigid transformation rotation and translation between the two set of d coordinate obtained by the calibrated imaging range sensor is developed it input is set of point lying on the surface of the object this input is converted into an octree spline that allows point to surface distance to be computed quickly a robust nonlinear least square minimization technique then find the optimal pose by minimizing the sum of square distance between the two set of d coordinate the algorithm ha been applied to matching human face with highly accurate result 
the recovery of viewpoint independent description of d shape from two and one half dimensional image a novel d shape representation called the smoothed local generalized cone slgcs is proposed this representation is suitable for recovery of the axis because the local constraint that characterizes a data set corresponding to the same axis point namely the local generalized cone lgc is explicitly defined the extracted axis can be used a a basis for determining a natural parameterization of the object surface using this parameterization the deformable surface fitting problem result in a linear least square problem so stable volumetric recovery is possible recovery experiment involving real d range image are reported 
an algorithm for recovering the scaling function of a straight homogeneous generalized cylinder shgc from an image contour is presented both location and reference cross section are supposed known perspective view assumption and geometric property of shgcs are used to derive the method no additional constraint have been imposed on the object shape the method ha been tested on synthetic image with promising result 
the possibility that what we do not know about the world can be used to guide the way in which they explore it is raised the author previously hypothesized that the spatial structure of uncertainty can be used to guide the acquisition of additional data in a way that significantly improves knowledge of a scene an exploration strategy based on the hypothesis is constructed and experimental result which verified that it work are presented condition which cause it to break down are studied the result indicate that there are inherent degeneracy in certain data configuration but that the author strategy can navigate these area successfully 
an active vision technique for determining the absolute depth of surface is described the algorithm assumes a very general model for the reflectance property of the surface and is valid for most of the shading model commonly used in computer vision work the algorithm relies on the controlled motion of a point light source which is not at infinity but relatively close to the surface and to the camera the sensitivity of the computed depth value to error in the measured quantity is derived allowing a confidence measure for the depth to be determined the confidence measure can aid in the estimation of accurate depth value from multiple image measurement taken over time a method based on robust estimation that permit an unbiased estimate of the depth value to be obtained is presented the result of experiment on synthetic and real world imagery are reported illustrating the efficacy of the active photometric stereo algorithm 
an active approach to the integration of shape from x module here shape from shading and shape from texture is proposed the question of what constitutes a good motion for the active observer is addressed generally the role of the visual system is to provide depth information to an autonomous robot a trajectory module will then interpret it to determine a motion for the robot which in turn will affect the visual information received it is suggested that the motion can also be chosen so a to improve the performance of the visual system 
it is shown how object recognition and optical flow can be captured within a single framework these example have been selected because they illustrate two complementary problem which can be tackled using the same unified approach based on lie theory the object recognition work referred to is based on the extraction of shape invariant and ha been reported elsewhere the present study focus on using the same framework for the calculation of the optical flow besides the introduction of some new method it is shown that several well known scheme can be derived following the same principle 
the problem of a moving robot tracking a moving object with it camera without requiring the ability to recognize the target to distinguish it from distracting surroundings is examined a novel aspect of the approach taken is the use of controlled camera movement to simplify the visual processing necessary to keep the camera locked on the target a gaze holding system implemented on a robot s binocular head demonstrates this approach even while the robot is moving the camera are able to track an object that rotates and move in three dimension the central idea is that localizing attention on d space make simple precategorical visual processing sufficient to hold gaze 
a unified decision theoretic framework for automating the establishment of feature point correspondence in a temporally dense sequence of image is discussed the approach extends a recent sequential detection algorithm to guide the detection and tracking of object feature point through an image sequence the resulting extended feature track provide robust feature correspondence for the estimation of three dimensional structure and motion over an extended number of image frame 
the measurement of multiple velocity using phase based method is discussed in particular phase gradient instantaneous frequency from different bandpass channel quadrature filter output are used to estimate multiple image velocity in a single neighborhood the approach is similar to that of m shizawa and k mase in which nth order differential operator are required to compute n simultaneous velocity estimate however to use instantaneous frequency the output of each channel must be differentiated only once 
adaptive size physically based model suitable for nonrigid motion analysis are presented the mesh size increase or decrease dynamically during the surface reconstruction process to locate node near surface area of interest like high curvature point and to optimize the fitting error a priori information about nonrigidity can be included so that the surface model deforms to fit moving data point while preserving some basic nonrigid constraint e g isometry or conformality implementation of the proposed algorithm with and without isometric conformal constraint is presented performance and accuracy of derived algorithm are demonstrated on data simulating deforming ellipsoidal and bending planar shape the algorithm is applied to the real range data for bending paper and to volumetric temporal left ventricular data 
a methodology for optical flow analysis based on cepstral filtering is introduced the power cepstrum is extended to multiframe analysis a correlative cepstral technique cepscorr is developed it significantly increase the signal to noise ratio reduces ambiguity and it provides a predictive or multievidence approach to visual motion analysis 
the notion of the apparent boundary and the strict apparent boundary of an object which provide an automatic approach of boundary detection are presented it is shown that the apparent boundary and the strict apparent boundary have the same diameter and the same convex hull a the original object it is also shown that the strict apparent boundary is weakly externally visible and is a fixpoint of the two operator that find the apparent boundary and the strict apparent boundary 
the use of a heterogeneous multiple simd m simd architecture with image based measurement and optimal kalman estimator for the analysis of image sequence is illustrated the architecture integrates simd and mimd processing paradigm combining heterogeneity of processor type matched to the computation at each level and operational autonomy within an simd array it is suited to real time simultaneous data parallel iconic and control parallel numeric processing 
an algorithm for generating skeleton of object in a binary image is described the algorithm produce a well centered skeleton with the same simple connectivity a the object and it allows the object to be either exactly or approximately to within a known user selectable error reconstructed it connectivity and reconstructability property can be rigorously proved for approximate reconstruction the skeleton can also be almost always thin and is insensitive to border noise without image prefiltering or skeleton post pruning while maintaining the precise error bound for reconstruction because of these property it robustness to rotation pleasing visual appearance and flexibility it is well suited for such application a data compression image analysis character recognition and circuit board inspection 
in automated feature based motion analysis of multiple frame correspondence data are usually noisy and fragmented a technique that gradually refines the initial noisy correspondence data and link fragment of a single trajectory using feedback from d motion estimation is presented first d motion parameter are estimated using the initial correspondence data then each noisy trajectory is partitioned into subset of point each of which conforms to the estimated motion the best set is used a the input to the next motion estimation this process is repeated and the gap in the refined correspondence data are filled by guidance from the predicted motion test result for a standard real image sequence are presented 
a surface reconstruction method using multilayer feedforward neural network is proposed the parametric form represented by multilayer neural network can model piecewise smooth surface in a way that is more general and flexible than many of the classical method the approximation method is based on a robust backpropagation bp algorithm which extends the basic bp algorithm to handle error especially others in the training data 
a markov random field mrf formulation for the problem of optical flow computation is studied an adaptive window matching scheme is used to obtain a good measure of the correlation between the two image a confidence measure for each match is also used thus the input to the system is the adaptive correlation and the corresponding confidence the mrf model is then used to estimate the velocity field and the velocity discontinuity the problem of occlusion is addressed and a relationship between occlusion and motion discontinuity is established 
an approach to image feature extraction is proposed complex moment of the gabor power spectrum are used to detect linear rectangular hexagonal triangular and other structure with very fine to very coarse resolution when the method is applied to texture segmentation good result are obtained 
the author propose knowledge representation and evidence propagation scheme based on multivariate belief function and present a medical image recognition system to demonstrate the effectiveness of their application to spatial reasoning the proposed system which is based on the blackboard architecture can mimic the reasoning process of a human expert in identifying the anatomical structure in a set of correlated image acquired from x ray ct and pdand t weighted mri in the blackboard oriented system different kind of evidence provided by various knowledge source form a hierarchy of evidential space to which the dempster shafer theory is applied the multivariate belief function are used to represent domain specific knowledge such a rule or fact 
a graph theoretic approach for image segmentation is presented the pixel of the image are represented by the vertex of an undirected adjacency graph g all neighboring pair of pixel are linked by arc with capacity assigned to reflect the strength of an edge element between the linked vertex segmentation is achieved by removing arc corresponding to selected minimum cut of g to form mutually exclusive subgraphs such that the largest intersubgraph maximum flow is minimized this is equivalent to partitioning the image using closed contour of edge element which consist mostly of strong edge the method accurately locates region boundary and at the same time reject contour containing isolated strong edge the minimum cut in g can be computed from a partially cut equivalent tree of g a fast algorithm for constructing partially equivalent tree that can handle graph with several hundred thousand vertex is developed 
a region segmentation algorithm is presented using a model for joint probability density joint probability density can be defined a an n n cooccurrence matrix in which each coordinate i j give the probability for the gray level transition i j between two neighbor pixel the approach consists in modeling the energy distribution within a cooccurrence matrix of a region region are assumed to be stationary a region growing scheme that proceeds in two step is used the first step consists of learning the parameter of the model the second step is the segmentation process starting with a seed pixel new pixel are incorporated in the region if their neighborhood fit the model 
gray scale reconstruction is formally defined for discrete image a brief summary of the existing technique to compute it is provided and a hybrid algorithm that is an order of magnitude faster than any other algorithm is introduced some of it application to image filtering and segmentation are listed 
two morphological method for edge detection in range image are proposed the first method us the opening and closing residue of structuring element in orthogonal direction to detect roof and crease edge and is essentially a morphological implementation of residue analysis technique the more general second method is based on a morphological version of the first derivative operator this method utilizes dilation and erosion residue of structuring element at different scale to reliably extract step edge along with roof edge and crease edge and to classify each pixel a belonging to eight possible structure type positive roof negative roof positive crease negative crease top of step base of step ramp and constant surface this method may be thought of a a morphological multiscale method 
a d generalization of the balloon model a a d deformable surface which evolves in d image is presented it is deformed under the action of internal and external force attracting the surface toward detected edge element by mean of an attraction potential to solve the minimization problem for a surface two simplified approach are shown defining a d surface a a series of d planar curve then the d model is solved using the finite element method yielding greater stability and faster convergence this model ha been used to segment magnetic resonance image 
a mathematical framework for the solution of statistical inference problem on a class of random set is proposed it is based on a new definition of expected pattern the least mean difference estimator restoration filter is proved under certain condition to be equivalent to the minimization of the measure of size area of the set difference between the original pattern and the expected pattern of the estimated restored pattern consequently it is proved that under certain condition if the estimator restoration filter is unbiased then it is the least mean difference estimator restoration filter 
a multiscale filtering scheme based on the three matheron axiom for morphological opening is developed it is shown that opening a signal with a gray scale operator doe not introduce additional zero crossing a one move to coarser scale within this framework the problem of choosing an appropriate structuring element is studied in order to obtain a measure of the performance of different structuring element the statistical property of gray scale opening are studied using a powerful tool in mathematical morphology namely basis function 
the author introduces a framework for investigating the property of energy edge detector and us it to derive some result of interest he show a necessary condition on the form of constituent linear filter in quadratic detector subject to some condition and demonstrates some limitation of such detector it is shown that no quadratic detector can detect an edge at for both a sinewave and a cosine wave which ha implication for detecting narrowband edge with spatially local filter it is also shown that the scale space behavior of energy detector is not well behaved in that it contains bifurcation a scale increase i e new edge can be created a the image is smoothed 
a method for detecting curve segment in a digital image is described the method take a input a set of edge and produce a output the number of and parameter for the segment the method is robust requiring no threshold in place of threshold a model class must be provided using the information theoretic minimum description length mdl principle it evaluates each model in the model class computing the optimal parameter for that model and selects the best model a the one that give the shortest encoding of the data and the model typical of such method the search space is extremely large it is shown how the hough transform ht may be used to reduce this search space greatly yielding an efficient although suboptimal search the result is an algorithm in which mdl overcomes standard problem with the ht while the ht overcomes problem with mdl and which produce a pleasing set of line segment 
the opening transformation on n dimensional discrete space zn is discussed the transform efficiently computes the binary opening closing with any size structuring element it also provides a quick way to calculate the pattern spectrum of an image the pattern spectrum is found to be nothing more than a histogram of the opening transform an efficient two pas recursive opening transform algorithm is developed and implemented the correctness of the algorithm is proved and some experimental result are given the result show that the execution time of the algorithm is a linear function of n where n is the product of the number of point in the structuring element when the input binary image size is and of the image is covered by the binary one pixel it take approximately m to do an arbitrary sized line opening and approximately m to do an arbitrary size box opening on the sun sparc ii workstation with c compiler optimization flag on 
the opening of a model signal with a convex zero height structuring element is studied empirically experiment are performed in which the input signal model parameter and the opening length are varied over an acceptable range and the corresponding grey level distribution in the opened signal are fit to pearson distribution regression are then used to relate the pearson distribution parameter to the input parameter resulting in equation that may be used to predict the effect of an opening characterization experiment show that the maximum absolute error between actual and predicted cumulative distribution using these regression equation have a mean of and a standard deviation of for a range of zero to one the worst case maximum absolute error encountered between the cumulative distribution is 
the author attempt to solve the structuring function decomposition problem where the structuring function refers to the gray scale structuring element a morphologically realizable representation for the structuring function that reduces the structuring function decomposition into a series of binary structuring element decomposition is presented recursive algorithm that are pipelinable for efficiently performing gray scale morphological operation are developed on the basis of proposed representation and decomposition the result are beneficial to real time image analysis in term of computer architecture and software development 
the concept of robustness in statistic is examined starting from the concept of the breakdown point and equivariance property of an estimator the desired equivariance property for shape fitting are defined and high breakdown point method with these property are found 
the problem of improving an initial segmentation of medical data by making use of gray level texture and gradient information is addressed the mathematical environment is that of markov random field and stochastic process this yield two major advantage automatic selection of program parameter and ergonomic software that can be used to test homogeneity property of region the method is applied to echocardiographic image in order to segment cardiac cavity 
an algorithm for extracting and segmenting surface description from stereo image is presented compared with the advance of range image segmentation the progress of surface segmentation from stereo image is slow the algorithm integrates the boundary based and region based approach to achieve more satisfactory segmentation result the algorithm yield such a rich description of a scene in term of global surface patch and closed surface boundary these shape primitive are useful for subsequent high level processing 
it is shown how random perturbation model can be set up for a vision algorithm sequence involving edge finding edge linking and gap filling by starting with an appropriate noise model for the input data the author derive random perturbation model for the output data at each stage of their example sequence these random perturbation model are useful for performing model based theoretical comparison of the performance of vision algorithm parameter of these random perturbation model are related to measure of error such a the probability of misdetection of feature unit probability of false alarm and the probability of incorrect grouping since the parameter of the perturbation model at the output of an algorithm are indicator of the performance of the algorithm one could utilize these model to automate the selection of various free parameter threshold of the algorithm 
a scheme for the estimation of the markov random field mrf line process parameter that us geometric cad model of the object in the scene is presented the model are used to generate synthetic image of the object from random viewpoint the edge map computed from the synthesized image are used a training sample to estimate the line process parameter using a least square method it is shown that this parameter estimation method is useful for detecting edge in range a well a intensity image 
two iterative algorithm for shape reconstruction based on multiple image taken under different lighting condition known a photometric stereo are proposed it is shown that single image shape from shading sfs algorithm have an inherent problem i e the accuracy of the reconstructed surface height is related to the slope of the reflectance map function defined on the gradient space this observation motivates the author to generalize the single image sfs algorithm to two photometric stereo sfs algorithm aiming at more accurate surface reconstruction the two algorithm directly determine the surface height by minimizing a quadratic cost functional which is defined to be the square of the brightness error obtained from each individual image in a parallel or cascade manner the optimal illumination condition that lead to best shape reconstruction is examined 
a feature based stereo matching system that is based on an algorithm for one dimensional waveform matching is described it is intended for use in automated cartography to generate an accurate three dimensional model of man made structure and natural terrain each epipolar line in the stereo pair is represented a a one dimensional intensity waveform the waveform is described a a collection of feature such a peak and valley and represented across a set of hierarchical level computed by approximation from the original waveform these feature are matched using an evaluation function that factor similarity of waveform shape intensity and symbolic feature description waveform match at coarse resolution are used to constrain match at finer level intra inter scanline correction are applied and the actual position of the stereo match is adjusted by using the gradient representation of the original waveform some representative result are presented for a complex urban scene 
the author examine how estimate of three dimensional scene structure a encoded in a scene disparity map can be improved by the analysis of the original monocular imagery they describe the utilization of surface illumination information provided by the segmentation of the monocular image into fine surface patch of nearly homogeneous intensity to remove mismatch generated during stereo matching these patch are used to guide a statistical analysis of the disparity map based on the assumption that such patch correspond closely with physical surface in the scene such a technique is quite independent of whether the initial disparity map wa generated by automated area based or feature based stereo matching refinement result on complex urban scene containing various man made and natural feature are presented and the improvement due to monocular fusion with a set of different region based image segmentation are demonstrated 
an intensity reflectance model is proposed for diffuse reflection originating from subsurface multiple scattering within inhomogeneous dielectric material one of the most common assumption in computer vision from inhomogeneous dielectric is lambertian the proposed model utilizes the subsurface intensity distribution predicted by modeling subsurface multiple scattering based upon radiative transfer theory for an optically smooth surface boundary this subsurface intensity distribution becomes altered by fresnel attenuation upon refraction into air making it become significantly non lambertian this resulting intensity distribution serf a a reflection law for individual optically smooth microfacets that compose an opaque rough surface producing a physical unification of diffuse reflection with the torrance sparrow specular reflection model experimental result that verify the accuracy of the model are presented a relatively simple formula that approximates closely the true behaviour of diffuse reflection and can be used in place of a lambertian term is presented the non lambertian nature of the diffuse component becomes most significant near occluding contour and sometimes produce a previously unnoticed intensity effect that can aid in their detection 
full perspective mapping between d object and d image are more complicated than weak perspective mapping which consider only rotation translation and scaling therefore in d model based robot navigation it is important to understand how and when full perspective must be taken into account a probabilistic combinatorial optimization algorithm is used to search for an optimal match between d landmark and d image feature three variation are considered a weak perspective algorithm rotates translates and scale an initial d projection of the d landmark a full perspective selects a most promising alternative but then update the pose and reprojects the landmark like the full perspective algorithm the hybrid algorithm reliably recovers the true pose of the robot and like the weak perspective algorithm it run to faster than the full perspective algorithm 
an approach to shape from shading that is based on a connection with a calculus of variation optimal control problem is proposed an explicit representation corresponding to a shaded image is given for the surface uniqueness of the surface under suitable condition is an immediate consequence the approach lead naturally to an algorithm for shape reconstruction that is simple fast provably convergent in many case provably convergent to the correct solution and doe not require regularization given a continuous image the algorithm can be proved to converge to the continuous surface solution a the image sampling frequency is taken to infinity experimental result are presented for synthetic and real image 
potential obstacle in the path of a mobile robot that can often be characterized a shallow i e their extent in depth is small compared to their distance from the camera are considered the constraint of affine trackability is applied to automatic identification and d reconstruction of shallow structure in realistic scene it is shown how this approach can handle independent object motion occlusion and motion discontinuity although the reconstructed structure is only a frontal plane approximation to the corresponding real structure the robustness of depth of the approximation might be useful for obstacle avoidance where the exact shape of an object may not be of consequence so long a collision with it can be avoided 
it is shown that the set of d image produced by a group of d point feature of a rigid model can be optimally represented with two line in two high dimensional space this result is used to match image and model group by table lookup the table is efficiently built and accessed through analytic method that account for the effect of sensing error in real image it reduces the set of potential match by a factor of several thousand this representation of a model s image is used to analyze two other approach to recognition it is determined when invariant exist in several domain and it is shown that there is an infinite set of qualitatively similar nonaccidental property 
a cad model based machine vision system for dimensional inspection of machine part is described with emphasis on the theory behind the system the original contribution of this work are the use of precise definition of geometric tolerance suitable for use in image processing the development of measurement algorithm corresponding directly to these definition the derivation of the uncertainty in the measurement task and the use of this uncertainty information in the decision making process initial experimental result have verified the uncertainty derivation statistically and proved that the error probability obtained by propagating uncertainty are lower than those obtainable without uncertainty propagation 
a measure of deformation energy suitable for fitting deformable model to image data is described an object s displacement is constrained to be globally smooth by penalizing the variation of the deformation gradient tensor this homogeneous deformation measure is invariant to arbitrary rigid body motion of object and viewpoint given the correspondence between model and data it remains quadratic in the displacement parameter leading to linear least square fit the method wa used to reconstruct the nonhomogeneous d motion of the heart wall from tomographic magnetic resonance image a finite element model of the left ventricle wa deformed to fit material point tracked in biplanar view only the in plane component were available from each separate image the through plane component being reconstructed in the fit 
the concept and design issue that provide the basis for the i f image interpretation foundation system are described the i f system combine object oriented design for machine vision software and constraint based geometric modeling into a flexible and effective system for automatic template guided visual inspection object oriented design for d geometry based image analysis is discussed and result from processing experimental x ray data are presented 
modeling of the unknown surface a key first step in the perception of surface in range image using the function approximation approach is considered akaike s entropy based information criterion aic is a simple but powerful tool for choosing the best fitting model among several competing model however the aic presupposes a fixed data set and a normality assumption on the error s distribution the aic is extended to a t distribution noise model which more realistically represents anomaly in the data such a outlier and quantization error this criterion is modified to be used with a robust sequential algorithm to accommodate the variable data size resulting from fitting different model the modified criterion is applied to real range data and it performance is compared with that of aic and consistent aic 
a human action recognition method based on a hidden markov model hmm is proposed it is a feature based bottom up approach that is characterized by it learning capability and time scale invariability to apply hmms one set of time sequential image is transformed into an image feature vector sequence and the sequence is converted into a symbol sequence by vector quantization in learning human action category the parameter of the hmms one per category are optimized so a to best describe the training sequence from the category to recognize an observed sequence the hmm which best match the sequence is chosen experimental result for real time sequential image of sport scene show recognition rate higher than the recognition rate is improved by increasing the number of people used to generate the training data indicating the possibility of establishing a person independent action recognizer 
the concept of active object recognition is introduced and a proposal for it solution is described the camera is mounted on the end of a robot arm on a mobile base the system exploit the mobility of the camera by using low level image data to drive the camera to a standard viewpoint with respect to an unknown object from such a viewpoint the object recognition task is reduced to a two dimensional pattern recognition problem the system us an efficient tree based probabilistic indexing scheme to find the model object that is likely to have generated the observed data and for line tracking us a modification of the token based tracking scheme of j l crowley et al the system ha been successfully tested on a set of origami object given sufficiently accurate low level data recognition time is expected to grow only logarithmically with the number of object stored 
