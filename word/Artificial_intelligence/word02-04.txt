we present a general purpose method for dynamically factoring a planning domain whose structure is then exploited by our generic planning method to find sound and complete plan the planning algorithm s time complexity scale linearly with the size of the domain and at worst exponentially with the size of the largest subdomain and interaction between subdomains the factorization procedure divide a planning domain into subdomains that are organized in a tree structure such that interaction between neighboring subdomains in the tree is minimized the combined planning algorithm is sound and complete and we demonstrate it on a representative planning domain the algorithm appears to scale to very large problem regardless of the black box planner used our planning procedure find plan for multiple subgoals in each of the subdomains separately using a generic black box planner it search over possible plan using complex action descriptor from each of the subdomains to form a plan for the overall goal the planning procedure us dynamic programming principle and backtracking occurs only within a subdomain a part of the black box planner we prove that our planning procedure run in time linear in the number of subdomains and take time that is at most exponential in the size of the largest subdomain and the number of dependency between subdomains the type of factoring that we select is justified by this complexity result we also prove that the combined algorithm is sound and complete and that it can be applied to solve any planning problem using any generic black box planner for planning within subdomains the complexity is upper bounded by the complexity of the black box planner on the unpartitioned domain we implemented and tested our planning algorithm on a simple domain to guide further development we created two implementation one with the ipp planning system koehler and hoffmann and one with the ff planner hoffmann and nebel we compared the result of our algorithm with those of ipp and ff and have shown that for a single domain our result scale much better than these planner alone the example validates our analytical result and show that our planner s performance scale linearly with the size of the domain for this problem motivating further development 
provides a listing of current committee member 
in field such a medicine geography and me chanics spatial reasoning involves reasoning about entity for example cavity and invad ing particle that may coincide without over lapping the purpose of this paper is to develop a mereotopology for domain that include coin cident entity such a material object hole geopolitical entity and spatial region in addi tion i construct mathematical model of this mereotopology in which nontrivial coincidence relation are defined 
filtering denotes any method whereby an agent up date it belief state it knowledge of the state of the world from a sequence of action and obser vations in logical filtering the belief state is a log ical formula describing possible world state and the agent ha a possibly nondeterministic logi cal model of it environment and sensor this paper present efficient logical filtering algorithm that maintain a compact belief state representa tion indefinitely for a broad range of environment class including nondeterministic partially ob servable strip environment and environment in which action permute the state space efficient filtering is also possible when the belief state is rep resented using prime implicates or when it is ap proximated by a logically weaker formula 
inverse or identification problem involve decid ing whether or not an explicitly given set of data point have an implicit description for instance in the form of a constraint network such prob lem provide insight into the relationship among various representation of knowledge which may have differing computational property this pa per formalizes and study the inverse circumscrip tion problem which roughly speaking is to de cide given a set of model if there exists a formula whose circumscription describes the input set 
the interested reader model we present a simple easily implemented spectral learning algorithm which applies equally whether we have no supervisory information pairwise link constraint or labeled example in the unsuper vised case it performs consistently with other spec tral clustering algorithm in the supervised case our approach achieves high accuracy on the cate gorization of thousand of document given only a few dozen labeled training document for the newsgroups data set furthermore it classifica tion accuracy increase with the addition of unla beled document demonstrating effective use of unlabeled data by using normalized affinity ma trice which are both symmetric and stochastic we also obtain both a probabilistic interpretation of our method and certain guarantee of performance 
the paper describes on going work on generating dialogue response on two level first generation of factual response from rdf xml information second generation of meta level explanation of an ontology s structure from daml oil domain ontology we also discus way to use an ontology to improve a current dialogue system s response for example by identifying misconception 
we investigate the combination of answer set pro gramming and qualitative optimization technique answer set optimization program aso pro gram have two part the generating program produce answer set representing possible solution the preference program express user preference it induces a preference relation on the answer set of based on the degree to which rule are satisfied we discus possible application of aso program ming give complexity result and propose imple mentation technique we also analyze the relation ship between a so program and cp network 
we present a probabilistic method for path planning that considers trajectory constrained by both the environment and an ensemble of restriction or preference on preferred motion for a moving robot our system learns constraint and preference bias on a robot s motion from example and then synthesizes behavior that satisfy these constraint this behavior can encompass motion that satisfy diverse requirement such a a sweep pattern for floor coverage or in particular in our experiment satisfy restriction on the robot s physical capability such a restriction on it turning radius given an approximate path that may not satisfy the required condition our system computes a refined path that satisfies the constraint and also avoids obstacle our approach is based on a bayesian framework for combining a prior probability distribution on the trajectory with environmental constraint the prior distribution is generated by decoding a hidden markov model which is itself is trained over a particular set of preferred motion environmental constraint are modeled using a potential field over the configuration space this paper pose the requisite theoretical framework and demonstrates it effectiveness with several example 
ontology a a discipline of computer science ha made many claim about it usefulness however to date there ha been very little evaluation of those claim we present the result of an experiment using a hybrid search system with a significant knowledge based component to measure using precision and recall the impact of improving the quality of an ontology on overall performance we demonstrate that improving the ontology using ontoclean guarino and welty doe positively impact performance and that having knowledge of the search domain is more effective than domain knowledge free search technique such a link analysis 
we introduce grid based sensordcsp a geometri cally structured benchmark problem for the study of distributed csp algorithm this domain pro vides realistic structure of the communication and tracking constraint we formally define this prob lem and perform it worst case complexity analy si likewise we provide an average case empirical analysis of the awc algorithm studying it behav ior on tractable and intractable sub class of our problem 
neural symbolic system are hybrid system that integrate symbolic logic and neural network the goal of neural symbolic integration is to benefit from the combination of feature of the symbolic and connectionist paradigm of artificial intelligence this paper introduces a new neural network architecture based on the idea of fibring logical system fibring allows one to combine different logical system in a principled way fibred neural network may be composed not only of interconnected neuron but also of other network forming a recursive architecture a fibring function then defines how this recursive architecture must behave by defining how the network in the ensemble relate to each other typically by allowing the activation of neuron in one network a to influence the change of weight in another network b intuitively this can be seen a training network b at the same time that one run network a we show that in addition to being universal approximators like standard feedforward network fibred neural network can approximate any polynomial function to any desired degree of accuracy thus being more expressive than standard feedforward network 
web search engine struggle to satisfy the need of web user user are notoriously poor at representing their need in the form of a query and search engine are poor at responding to vague query however progress ha been made by introducing context into the search process in this paper we describe and evaluate a novel approach to using context in web search that adapts a generic search engine for the need of a specialist community of user this collaborative search method enjoys significant performance benefit and avoids the privacy and security concern that are commonly associated with related personalization research 
the core of scientific theory are law these law often make use of theoretical term linguistic entity which do not directly refer to observables there is therefore no direct way of determining which theoretical assertion are true this suggests that multiple theory may exist which are incompatible with each other but compatible with all possible observation since such theory make the same empirical claim empirical test cannot be used to differentiate or rank such theory one property that ha been suggested for evaluating rival theory is coherence this wa only understood qualitatively until we kwok et al introduced a coherence measure based on the average use of formula in support set for observation the idea wa to identify highly coherent theory with those whose formula that are tightly coupled to account for observation while low coherence theory contain many disjointed and isolated statement our current approach generalizes that insight to accommodate fundamental idea from the philosophy of science and better mirror scientific practice moreover this new approach is neutral with respect to the philosophy and practice of science and is able to explain notion like modularization using coherence 
we present a generalization of similarity based retrieval in recommender system which ensures that for any case that is acceptable to the user the retrieval set contains a case that is at least a good in an objective sense and so also likely to be acceptable our approach recognizes that similarity to the target query is only one of several possible criterion according to which a given case might be considered at least a good a another 
a pattern database pdb is a heuristic function implemented a a lookup table that store the length of optimal solution for subproblem instance standard pdbs have a distinct entry in the table for each subproblem instance in this paper we investigate compressing pdbs by merging several entry into one thereby allowing the use of pdbs that exceed available memory in their uncompressed form we introduce a number of method for determining which entry to merge and discus their relative merit these vary from domainindependent approach that allow any set of entry in the pdb to be merged to more intelligent method that take into account the structure of the problem the choice of the best compression method is based on domain dependent attribute we present experimental result on a number of combinatorial problem including the four peg tower of hanoi problem the sliding tile puzzle and the top spin puzzle for the tower of hanoi we show that the search time can be reduced by up to three order of magnitude by using compressed pdbs compared to uncompressed pdbs of the same size more modest improvement were observed for the other domain 
we investigate the problem of non covariant behavior of policy gradient reinforcement learning algorithm the policy gradient approach is amenable to analysis by information geometric method this lead u to propose a natural metric on controller parameterization that result from considering the manifold of probability distribution over path induced by a stochastic controller investigation of this approach lead to a covariant gradient ascent rule interesting property of this rule are discussed including it relation with actor critic style reinforcement learning algorithm the algorithm discussed here are computationally quite efficient and on some interesting problem lead to dramatic performance improvement over noncovariant rule 
the paper present a novel expressive logic based formalism intended for reasoning about numerical distance we investigate it computational prop erties in particular show that it is exptimecomplete and devise a tableau based satisfiabilitychecking algorithm to be able to express knowl edge about implicit or unknown distance we then extend the language with variable ranging over distance and prove that the resulting logic is decidable a well 
repetition is an important phenomenon in a variety of domain such a music computer program and architectural drawing a generative model for these domain should account for the possibility of repetition we present repeated observation model rom a framework for modeling sequence that explicitly allows for repetition in a rom an element is either generated by copying a previous element or by using a base model we show how to build rom using gram and hidden markov model a the base model we also describe an extension of rom in which entire subsequence are repeated together result from a music modeling domain show that rom can lead to dramatic improvement in predictive ability 
muhiagent system ma can go down for a large number of reason ranging from system mal function and power failure to malicious attack the placement of agent on node is called a de ployment of the ma we develop a probabilis tic model of survivability of a deployed ma and provide two algorithm to compute the probability of survival of a deployed ma our probabilistic model doc not make independence assumption though such assumption can be added if so de sired an optimal deployment of a ma is one that maximizes it survival probability we provide a mathematical answerto this question an algorithm that computes an exact solution to this problem a well a several algorithm that quickly compute approximate solution to the problem we have implemented our algorithm our implementation demonstrates that computing deployment can be done scalably 
in first order logic a theory t is considered stronger than another theory t if every formula derived from t is also derived from t such an order relation is useful to know relative value between different theory in the context of de fault logic a theory contains default information a well a definite information to order default theory it is necessary to ass the information content of a default theory to this end we intro duce a multi valued interpretation of default the ories based on a nine valued bilattice it distin guishes definite and credulous skeptical default in formation derived from a theory and is used for ordering default theory based on their informa tion content the technique is also applied to or der nonmonotonic logic program the result of this paper provide a method for comparing differ ent default theory and have important application to learning nonmonotonic theory 
this paper present a new boosting arcing algorithm called poca parallel online continuous arcing unlike traditional boosting algorithm such a arc x and adaboost that construct ensemble by adding and training weak learner sequentially on a round by round basis training in poca is performed over an entire ensemble continuously and in parallel since member of the ensemble are not frozen after an initial learning period a in traditional boosting poca is able to adapt rapidly to nonstationary environment and because poca doe not require the explicit scoring of a fixed exemplar set it can perform online learning of non repeating data we present result from experiment conducted using neural network expert that show poca is typically faster and more adaptive than existing boosting algorithm result presented for the uci letter dataset are to our knowledge the best published score to date 
we present a new class of game local effect game leg which exploit structure in a different way from other compact game representation studied in ai we show both theoretically and empirically that these game often but not always have pure strategy nash equilibrium finding a potential function is a good technique for finding such equilibrium we give a complete characterization of which leg have potential function and provide the function in each case we also show a general case where pure strategy equilibrium exist in the absence of potential function in experiment we show that myopic best response dynamic converge quickly to pure strategy equilibrium in game not covered by our positive theoretical result 
we identify a new and important global or non binary constraint this constraint ensures that the value taken by two vector of varia bles when viewed a multisets are ordered this constraint is useful for a numb er of different application including breaking symmetry and fuzzy constraint satisfaction we propose and implement an efficient linear time algorithm for enf orcing generalised arc consistency on such a multiset ordering constraint ex perimental result on several problem domain show considerable promise 
pervasive robotics will require in a near future small light and cheap robot that exhibit complex behavior these demand led to the development of the m m macaco project a robotic active vision head macaco is a portable system capable of emulating the head of different creature both aesthetically and functionally it integrates mechanism for social interaction autonomous navigation and object analysis 
temperature discovery search tds is a new minimaxbased game tree search method designed to compute or approximate the temperature of a combinatorial game tds is based on the concept of an enriched environment where a combinatorial game g is embedded in an environment consisting of a large set of simple game of decreasing temperature optimal play start in the environment but eventually must switch to g tds nd the temperature of g by determining when this switch must happen both exact and heuristic version of tds are described and evaluated experimentally in experiment with sum game in amazon tds outperforms an searcher 
a public virtual laboratory is presented where animats are controlled by mechanism from different cognitive paradigm a brief description of the characteristic of the laboratory and the us it ha had is given mainly it ha been used to contrast philosophical idea related with the notion of cognition and to elucidate debate on proper paradigm in ai and cognitive science 
we present an approach for learning part of speech distinction by induction over the lexicon of the cyc knowledge base this produce good result using a decision tree that incorporates both semantic feature and syntactic feature accurate result are achieved for the special case of deciding whether lexical mapping should use count noun or mass noun headword comparable result are also obtained using opencyc the publicly available version of cyc 
in jegou a decomposition method ha been introduced for improving search efficiency in the area of constraint satisfaction problem this method is based on property of micro structure of csps related to property of triangulated graph this decomposition allows to transform an instance of csp in a collection of sub problem easier to solve and then give a natural and efficient way for a parallel implementation habbas et al in this paper we present a generalization of this approach which is based on a generalization of triangulated graph this generalization allows to define the level of decomposition which can be fixed by a graph parameter the larger this parameter is the more level of decomposition that is the number of sub problem is a a consequence we can then define the level of decomposition with respect to the nature of the parallel configuration used the number of processor first experiment reported here show that this extension increase significantly the advantage of the basic decomposition already shown in habbas et al 
this paper present a task allocation scheme via self organizing swarm coalition for distributed mobile sensor network coverage our approach us the concept of ant behavior to self regulate the regional distribution of sensor in proportion to that of the moving target to be tracked in a non stationary environment a a result the adverse effect of task interference between robot are minimized and sensor network coverage is improved quantitative comparison with other tracking strategy such a static sensor placement potential field and auction based negotiation show that our approach can provide better coverage and greater flexibility to respond to environmental change 
a logic of conditional preference is defined with a language which allows she compact representation of certain kind of conditional preference statement a semantics and a proof theory cp net can be expressed in this language and the semantics and proof theory generalise those of cp net despite being substantially more expressive the formalism maintains important property of cp net there are simple sufficient condition for consistency and under these condition optimal outcome can be efficiently generated it is also then easy to find a total order on outcome which extends the conditional preference order and an approach to constrained optimisation can be used which generalises a natural approach for cp net some result regarding the expressive power of cp net are also given 
we consider from a computational perspective the problem of how to aggregate the ranking preference of a number of alternative by a number of different voter into a single consensus ranking following the majority voting rule social welfare function for aggregating preference in this way have been widely studied since the time of condorcet one drawback of majority voting procedure when three or more alternative are being ranked is the presence of cycle in the majority preference relation the kemeny order is a social welfare function which ha been designed to tackle the presence of such cycle however computing a kemeny order is known to be np hard we develop a greedy heuristic and an exact branch and bound procedure for computing kemeny order we present result of a computational study on these procedure 
abstract the hierarchical hidden markov model hhmm is an ex tension of the hidden markov model to include a hierarchy of the hidden state this form of hierarchical modeling ha been found useful in application such a handwritten char acter recognition behavior recognition video indexing and text retrieval nevertheless the state hierarchy in the original hhmm is restricted to a tree structure this prohibits two different state from having the same child and thus doe not allow for sharing of common substructure in the model in this paper we present a general hhmm in which the state hierarchy can be a lattice allowing arbitrary sharing of sub structure furthermore we provide a method for numerical scaling to avoid underflow an important issue in dealing with long observation sequence we demonstrate the working of our method in a simulated environment where a hierarchical behavioral model is automatically learned and later used for recognition 
a new approach to the text categorization problem is here presented it is called gaussian weighting and it is a supervised learning algorithm that during the training phase estimate two very simple and easily computable statistic which are the presence p how much a term is present in a category c in the expressiveness e how much is present outside c in the rest of the domain once the system ha learned this information a gaussian function is shaped for each term of a category in order to assign the term a weight that estimate the level of it importance for that particular category we tested our learning method on the task of single label classification using the reuters benchmark the outcome of the result wa quite impressive in different experimental setup we reached a micro averaged fl measure of with a peak of moreover a macro averaged recall and precision wa calculated the former reported a the latter a these result reach most of the state of the art technique of machine learning applied to text categorization demonstrating that this new weighting scheme doe perform well on this particular task 
to operate effectively in complex environment learning agent require the ability to selectively ignore irrelevant detail and form useful abstraction in this article we consider the question of what constitutes a useful abstraction in a stochastic sequential decision problem modeled a a semi markov decision process smdps we introduce the notion of smdp homomorphism and argue that it provides a useful tool for a rigorous study of abstraction for smdps we present an smdp minimization framework and an abstraction framework for factored mdps based on smdp homomorphism we also model different class of abstraction that arise in hierarchical system although we use the option framework for purpose of illustration the idea are more generally applicable we also show that the condition for abstraction we employ are a generalization of earlier work by dietterich a applied to the option framework 
a growing number of application seek to incorporate automatically generated narrative structure into interactive virtual environment in this paper we evaluate a representation for narrative structure generated by an automatic planning system by mapping the plan that control plot into conceptual graph used by quest an existing framework for question answering analysis that includes structure for modeling a reader s narrative comprehension and using method originally employed by quest s developer to determine if the plan structure can serve a effective model of the understanding that human user form after viewing corresponding story played out within a virtual world result from our analysis are encouraging though additional work is required to expand the plan language to cover a broader class of narrative structure 
this paper introduces the concept of resource temporal network rtn a constraint network that subsumes both classical attribute used in a i planning and capacity resource traditionally handled in scheduling after giving a formal definition of rtns we analyze their expressive power and study complexity of several fragment of the rtn framework we show that solving an rtn is in general np complete which is not surprising given the expressivity of the framework whereas computing a necessary truth criterion is polynomial this last result open the door for promising algorithm to solve rtns 
speculative execution of information gathering plan can dramatically reduce the effect of source i o latency on overall performance however the utility of speculation is closely tied to how accurately data value are predicted at runtime caching 
this research summary describes some work in progress on using graphical model to represent relational data in computational science portal such a mygrid the objective is to provide a integrative collaborative filtering cf capability to user of data metadata source code and experimental documentation in some domain of interest recent system such a researchindex citeseer provide collaborative recommendation through citation indexing and system such a sourceforge and the open bioinformatics project provide similar tool such a content based indexing of software our current research aim at learning probabilistic relational model prms from data in order to support intellignet retrieval of data source code and experimental record we present a system design and a pr ci of a test bed under development that applies prm structure learning and inference to cf in repository of bioinformatic s data and software 
a representational gap exists between low level measurement segmentation object classification tracking and high level understanding of video sequence in this paper we propose a novel representation of event in video to bridge this gap based on the case representation of natural language the proposed representation ha three significant contribution over existing framework first we recognize the importance of causal and temporal relationship between subevents and extend case to allow the representation of temporal structure and causality between sub event second in order to capture both multi agent and multithreaded event we introduce a hierarchical case representation of event in term of sub event and case list last for purpose of implementation we present the concept of a temporal event tree and pose the problem of event detection a subtree pattern matching by extending case a natural language representation for the representation of event the proposed work allows a plausible mean of interface between user and the computer we show two important application of the proposed event representation for the automated annotation of standard meeting video sequence and for event detection in extended video of railroad crossing 
this paper investigates a new approach for training discriminant classifier when only a small set of labeled data is available together with a large set of unlabeled data this algorithm optimizes the classification maximum likelihood of a set of labeled unlabeled data using a variant form of the classification expectation maximization cem algorithm it originality is that it make use of both unlabeled data and of a probabilistic misclassification model for these data the parameter of the label error model are learned together with the classifier parameter we demonstrate the effectiveness of the approach on four data set and show the advantage of this method over a previously developed semi supervised algorithm which doe not consider imperfection in the labeling process 
a the artificial intelligence ai system in military simulation and computer game become more complex their action become increasingly difficult for user to understand expert system for medical diagnosis have addressed this challenge though the addition of explanation generation system that explain a system s internal process this paper describes the ai architecture and associated explanation capability used by full spectrum command a training system developed for the u s army by commercial game developer and academic researcher 
the aim of this paper is to compare bayesian network classifier to the k nn classifier based on a subset of feature this subset is established by mean of sequential feature selection method experimental result show that bayesian network classifier more often achieve a better classification rate on different data set than selective k nn classifier the k nn classifier performs well in the case where the number of sample for learning the parameter of the bayesian network is small bayesian network classifier outperform selective knn method in term of memory requirement and computational demand this paper demonstrates the strength of bayesian network for classification 
this poster show an artificial neural network capable of learning a temporal sequence directly inspired from a hippocampus model banquet et al this architecture allows an autonomous robot to learn how to imitate a sequence of movement with the correct timing 
in this paper we propose the framework of monte carlo algorithm a a useful one to analyze ensemble learning in particular this framework allows one to guess when bagging will be useful explains why increasing the margin improves performance and suggests a new way of performing ensemble learning and error estimation 
we present a general purpose method for dynamically factoring a planning domain whose structure is then exploited by our generic planning method to find sound and complete plan the planning algorithm s time complexity scale linearly with the size of the domain and at worst exponentially with the size of the largest subdomain and interaction between subdomains the factorization procedure divide a planning domain into subdomains that are organized in a tree structure such that interaction between neighboring subdomains in the tree is minimized the combined planning algorithm is sound and complete and we demonstrate it on a representative planning domain the algorithm appears to scale to very large problem regardless of the black box planner used our planning procedure find plan for multiple subgoals in each of the subdomains separately using a generic black box planner it search over possible plan using complex action descriptor from each of the subdomains to form a plan for the overall goal the planning procedure us dynamic programming principle and backtracking occurs only within a subdomain a part of the black box planner we prove that our planning procedure run in time linear in the number of subdomains and take time that is at most exponential in the size of the largest subdomain and the number of dependency between subdomains the type of factoring that we select is justified by this complexity result we also prove that the combined algorithm is sound and complete and that it can be applied to solve any planning problem using any generic black box planner for planning within subdomains the complexity is upper bounded by the complexity of the black box planner on the unpartitioned domain we implemented and tested our planning algorithm on a simple domain to guide further development we created two implementation one with the ipp planning system koehler and hoffmann and one with the ff planner hoffmann and nebel we compared the result of our algorithm with those of ipp and ff and have shown that for a single domain our result scale much better than these planner alone the example validates our analytical result and show that our planner s performance scale linearly with the size of the domain for this problem motivating further development 
provides a listing of current committee member 
in field such a medicine geography and me chanics spatial reasoning involves reasoning about entity for example cavity and invad ing particle that may coincide without over lapping the purpose of this paper is to develop a mereotopology for domain that include coin cident entity such a material object hole geopolitical entity and spatial region in addi tion i construct mathematical model of this mereotopology in which nontrivial coincidence relation are defined 
filtering denotes any method whereby an agent up date it belief state it knowledge of the state of the world from a sequence of action and obser vations in logical filtering the belief state is a log ical formula describing possible world state and the agent ha a possibly nondeterministic logi cal model of it environment and sensor this paper present efficient logical filtering algorithm that maintain a compact belief state representa tion indefinitely for a broad range of environment class including nondeterministic partially ob servable strip environment and environment in which action permute the state space efficient filtering is also possible when the belief state is rep resented using prime implicates or when it is ap proximated by a logically weaker formula 
inverse or identification problem involve decid ing whether or not an explicitly given set of data point have an implicit description for instance in the form of a constraint network such prob lem provide insight into the relationship among various representation of knowledge which may have differing computational property this pa per formalizes and study the inverse circumscrip tion problem which roughly speaking is to de cide given a set of model if there exists a formula whose circumscription describes the input set 
the interested reader model we present a simple easily implemented spectral learning algorithm which applies equally whether we have no supervisory information pairwise link constraint or labeled example in the unsuper vised case it performs consistently with other spec tral clustering algorithm in the supervised case our approach achieves high accuracy on the cate gorization of thousand of document given only a few dozen labeled training document for the newsgroups data set furthermore it classifica tion accuracy increase with the addition of unla beled document demonstrating effective use of unlabeled data by using normalized affinity ma trice which are both symmetric and stochastic we also obtain both a probabilistic interpretation of our method and certain guarantee of performance 
the paper describes on going work on generating dialogue response on two level first generation of factual response from rdf xml information second generation of meta level explanation of an ontology s structure from daml oil domain ontology we also discus way to use an ontology to improve a current dialogue system s response for example by identifying misconception 
we investigate the combination of answer set pro gramming and qualitative optimization technique answer set optimization program aso pro gram have two part the generating program produce answer set representing possible solution the preference program express user preference it induces a preference relation on the answer set of based on the degree to which rule are satisfied we discus possible application of aso program ming give complexity result and propose imple mentation technique we also analyze the relation ship between a so program and cp network 
we present a probabilistic method for path planning that considers trajectory constrained by both the environment and an ensemble of restriction or preference on preferred motion for a moving robot our system learns constraint and preference bias on a robot s motion from example and then synthesizes behavior that satisfy these constraint this behavior can encompass motion that satisfy diverse requirement such a a sweep pattern for floor coverage or in particular in our experiment satisfy restriction on the robot s physical capability such a restriction on it turning radius given an approximate path that may not satisfy the required condition our system computes a refined path that satisfies the constraint and also avoids obstacle our approach is based on a bayesian framework for combining a prior probability distribution on the trajectory with environmental constraint the prior distribution is generated by decoding a hidden markov model which is itself is trained over a particular set of preferred motion environmental constraint are modeled using a potential field over the configuration space this paper pose the requisite theoretical framework and demonstrates it effectiveness with several example 
ontology a a discipline of computer science ha made many claim about it usefulness however to date there ha been very little evaluation of those claim we present the result of an experiment using a hybrid search system with a significant knowledge based component to measure using precision and recall the impact of improving the quality of an ontology on overall performance we demonstrate that improving the ontology using ontoclean guarino and welty doe positively impact performance and that having knowledge of the search domain is more effective than domain knowledge free search technique such a link analysis 
we introduce grid based sensordcsp a geometri cally structured benchmark problem for the study of distributed csp algorithm this domain pro vides realistic structure of the communication and tracking constraint we formally define this prob lem and perform it worst case complexity analy si likewise we provide an average case empirical analysis of the awc algorithm studying it behav ior on tractable and intractable sub class of our problem 
neural symbolic system are hybrid system that integrate symbolic logic and neural network the goal of neural symbolic integration is to benefit from the combination of feature of the symbolic and connectionist paradigm of artificial intelligence this paper introduces a new neural network architecture based on the idea of fibring logical system fibring allows one to combine different logical system in a principled way fibred neural network may be composed not only of interconnected neuron but also of other network forming a recursive architecture a fibring function then defines how this recursive architecture must behave by defining how the network in the ensemble relate to each other typically by allowing the activation of neuron in one network a to influence the change of weight in another network b intuitively this can be seen a training network b at the same time that one run network a we show that in addition to being universal approximators like standard feedforward network fibred neural network can approximate any polynomial function to any desired degree of accuracy thus being more expressive than standard feedforward network 
web search engine struggle to satisfy the need of web user user are notoriously poor at representing their need in the form of a query and search engine are poor at responding to vague query however progress ha been made by introducing context into the search process in this paper we describe and evaluate a novel approach to using context in web search that adapts a generic search engine for the need of a specialist community of user this collaborative search method enjoys significant performance benefit and avoids the privacy and security concern that are commonly associated with related personalization research 
the core of scientific theory are law these law often make use of theoretical term linguistic entity which do not directly refer to observables there is therefore no direct way of determining which theoretical assertion are true this suggests that multiple theory may exist which are incompatible with each other but compatible with all possible observation since such theory make the same empirical claim empirical test cannot be used to differentiate or rank such theory one property that ha been suggested for evaluating rival theory is coherence this wa only understood qualitatively until we kwok et al introduced a coherence measure based on the average use of formula in support set for observation the idea wa to identify highly coherent theory with those whose formula that are tightly coupled to account for observation while low coherence theory contain many disjointed and isolated statement our current approach generalizes that insight to accommodate fundamental idea from the philosophy of science and better mirror scientific practice moreover this new approach is neutral with respect to the philosophy and practice of science and is able to explain notion like modularization using coherence 
we present a generalization of similarity based retrieval in recommender system which ensures that for any case that is acceptable to the user the retrieval set contains a case that is at least a good in an objective sense and so also likely to be acceptable our approach recognizes that similarity to the target query is only one of several possible criterion according to which a given case might be considered at least a good a another 
a pattern database pdb is a heuristic function implemented a a lookup table that store the length of optimal solution for subproblem instance standard pdbs have a distinct entry in the table for each subproblem instance in this paper we investigate compressing pdbs by merging several entry into one thereby allowing the use of pdbs that exceed available memory in their uncompressed form we introduce a number of method for determining which entry to merge and discus their relative merit these vary from domainindependent approach that allow any set of entry in the pdb to be merged to more intelligent method that take into account the structure of the problem the choice of the best compression method is based on domain dependent attribute we present experimental result on a number of combinatorial problem including the four peg tower of hanoi problem the sliding tile puzzle and the top spin puzzle for the tower of hanoi we show that the search time can be reduced by up to three order of magnitude by using compressed pdbs compared to uncompressed pdbs of the same size more modest improvement were observed for the other domain 
we investigate the problem of non covariant behavior of policy gradient reinforcement learning algorithm the policy gradient approach is amenable to analysis by information geometric method this lead u to propose a natural metric on controller parameterization that result from considering the manifold of probability distribution over path induced by a stochastic controller investigation of this approach lead to a covariant gradient ascent rule interesting property of this rule are discussed including it relation with actor critic style reinforcement learning algorithm the algorithm discussed here are computationally quite efficient and on some interesting problem lead to dramatic performance improvement over noncovariant rule 
the paper present a novel expressive logic based formalism intended for reasoning about numerical distance we investigate it computational prop erties in particular show that it is exptimecomplete and devise a tableau based satisfiabilitychecking algorithm to be able to express knowl edge about implicit or unknown distance we then extend the language with variable ranging over distance and prove that the resulting logic is decidable a well 
repetition is an important phenomenon in a variety of domain such a music computer program and architectural drawing a generative model for these domain should account for the possibility of repetition we present repeated observation model rom a framework for modeling sequence that explicitly allows for repetition in a rom an element is either generated by copying a previous element or by using a base model we show how to build rom using gram and hidden markov model a the base model we also describe an extension of rom in which entire subsequence are repeated together result from a music modeling domain show that rom can lead to dramatic improvement in predictive ability 
muhiagent system ma can go down for a large number of reason ranging from system mal function and power failure to malicious attack the placement of agent on node is called a de ployment of the ma we develop a probabilis tic model of survivability of a deployed ma and provide two algorithm to compute the probability of survival of a deployed ma our probabilistic model doc not make independence assumption though such assumption can be added if so de sired an optimal deployment of a ma is one that maximizes it survival probability we provide a mathematical answerto this question an algorithm that computes an exact solution to this problem a well a several algorithm that quickly compute approximate solution to the problem we have implemented our algorithm our implementation demonstrates that computing deployment can be done scalably 
in first order logic a theory t is considered stronger than another theory t if every formula derived from t is also derived from t such an order relation is useful to know relative value between different theory in the context of de fault logic a theory contains default information a well a definite information to order default theory it is necessary to ass the information content of a default theory to this end we intro duce a multi valued interpretation of default the ories based on a nine valued bilattice it distin guishes definite and credulous skeptical default in formation derived from a theory and is used for ordering default theory based on their informa tion content the technique is also applied to or der nonmonotonic logic program the result of this paper provide a method for comparing differ ent default theory and have important application to learning nonmonotonic theory 
this paper present a new boosting arcing algorithm called poca parallel online continuous arcing unlike traditional boosting algorithm such a arc x and adaboost that construct ensemble by adding and training weak learner sequentially on a round by round basis training in poca is performed over an entire ensemble continuously and in parallel since member of the ensemble are not frozen after an initial learning period a in traditional boosting poca is able to adapt rapidly to nonstationary environment and because poca doe not require the explicit scoring of a fixed exemplar set it can perform online learning of non repeating data we present result from experiment conducted using neural network expert that show poca is typically faster and more adaptive than existing boosting algorithm result presented for the uci letter dataset are to our knowledge the best published score to date 
we present a new class of game local effect game leg which exploit structure in a different way from other compact game representation studied in ai we show both theoretically and empirically that these game often but not always have pure strategy nash equilibrium finding a potential function is a good technique for finding such equilibrium we give a complete characterization of which leg have potential function and provide the function in each case we also show a general case where pure strategy equilibrium exist in the absence of potential function in experiment we show that myopic best response dynamic converge quickly to pure strategy equilibrium in game not covered by our positive theoretical result 
we identify a new and important global or non binary constraint this constraint ensures that the value taken by two vector of varia bles when viewed a multisets are ordered this constraint is useful for a numb er of different application including breaking symmetry and fuzzy constraint satisfaction we propose and implement an efficient linear time algorithm for enf orcing generalised arc consistency on such a multiset ordering constraint ex perimental result on several problem domain show considerable promise 
pervasive robotics will require in a near future small light and cheap robot that exhibit complex behavior these demand led to the development of the m m macaco project a robotic active vision head macaco is a portable system capable of emulating the head of different creature both aesthetically and functionally it integrates mechanism for social interaction autonomous navigation and object analysis 
temperature discovery search tds is a new minimaxbased game tree search method designed to compute or approximate the temperature of a combinatorial game tds is based on the concept of an enriched environment where a combinatorial game g is embedded in an environment consisting of a large set of simple game of decreasing temperature optimal play start in the environment but eventually must switch to g tds nd the temperature of g by determining when this switch must happen both exact and heuristic version of tds are described and evaluated experimentally in experiment with sum game in amazon tds outperforms an searcher 
a public virtual laboratory is presented where animats are controlled by mechanism from different cognitive paradigm a brief description of the characteristic of the laboratory and the us it ha had is given mainly it ha been used to contrast philosophical idea related with the notion of cognition and to elucidate debate on proper paradigm in ai and cognitive science 
we present an approach for learning part of speech distinction by induction over the lexicon of the cyc knowledge base this produce good result using a decision tree that incorporates both semantic feature and syntactic feature accurate result are achieved for the special case of deciding whether lexical mapping should use count noun or mass noun headword comparable result are also obtained using opencyc the publicly available version of cyc 
in jegou a decomposition method ha been introduced for improving search efficiency in the area of constraint satisfaction problem this method is based on property of micro structure of csps related to property of triangulated graph this decomposition allows to transform an instance of csp in a collection of sub problem easier to solve and then give a natural and efficient way for a parallel implementation habbas et al in this paper we present a generalization of this approach which is based on a generalization of triangulated graph this generalization allows to define the level of decomposition which can be fixed by a graph parameter the larger this parameter is the more level of decomposition that is the number of sub problem is a a consequence we can then define the level of decomposition with respect to the nature of the parallel configuration used the number of processor first experiment reported here show that this extension increase significantly the advantage of the basic decomposition already shown in habbas et al 
this paper present a task allocation scheme via self organizing swarm coalition for distributed mobile sensor network coverage our approach us the concept of ant behavior to self regulate the regional distribution of sensor in proportion to that of the moving target to be tracked in a non stationary environment a a result the adverse effect of task interference between robot are minimized and sensor network coverage is improved quantitative comparison with other tracking strategy such a static sensor placement potential field and auction based negotiation show that our approach can provide better coverage and greater flexibility to respond to environmental change 
a logic of conditional preference is defined with a language which allows she compact representation of certain kind of conditional preference statement a semantics and a proof theory cp net can be expressed in this language and the semantics and proof theory generalise those of cp net despite being substantially more expressive the formalism maintains important property of cp net there are simple sufficient condition for consistency and under these condition optimal outcome can be efficiently generated it is also then easy to find a total order on outcome which extends the conditional preference order and an approach to constrained optimisation can be used which generalises a natural approach for cp net some result regarding the expressive power of cp net are also given 
we consider from a computational perspective the problem of how to aggregate the ranking preference of a number of alternative by a number of different voter into a single consensus ranking following the majority voting rule social welfare function for aggregating preference in this way have been widely studied since the time of condorcet one drawback of majority voting procedure when three or more alternative are being ranked is the presence of cycle in the majority preference relation the kemeny order is a social welfare function which ha been designed to tackle the presence of such cycle however computing a kemeny order is known to be np hard we develop a greedy heuristic and an exact branch and bound procedure for computing kemeny order we present result of a computational study on these procedure 
abstract the hierarchical hidden markov model hhmm is an ex tension of the hidden markov model to include a hierarchy of the hidden state this form of hierarchical modeling ha been found useful in application such a handwritten char acter recognition behavior recognition video indexing and text retrieval nevertheless the state hierarchy in the original hhmm is restricted to a tree structure this prohibits two different state from having the same child and thus doe not allow for sharing of common substructure in the model in this paper we present a general hhmm in which the state hierarchy can be a lattice allowing arbitrary sharing of sub structure furthermore we provide a method for numerical scaling to avoid underflow an important issue in dealing with long observation sequence we demonstrate the working of our method in a simulated environment where a hierarchical behavioral model is automatically learned and later used for recognition 
a new approach to the text categorization problem is here presented it is called gaussian weighting and it is a supervised learning algorithm that during the training phase estimate two very simple and easily computable statistic which are the presence p how much a term is present in a category c in the expressiveness e how much is present outside c in the rest of the domain once the system ha learned this information a gaussian function is shaped for each term of a category in order to assign the term a weight that estimate the level of it importance for that particular category we tested our learning method on the task of single label classification using the reuters benchmark the outcome of the result wa quite impressive in different experimental setup we reached a micro averaged fl measure of with a peak of moreover a macro averaged recall and precision wa calculated the former reported a the latter a these result reach most of the state of the art technique of machine learning applied to text categorization demonstrating that this new weighting scheme doe perform well on this particular task 
to operate effectively in complex environment learning agent require the ability to selectively ignore irrelevant detail and form useful abstraction in this article we consider the question of what constitutes a useful abstraction in a stochastic sequential decision problem modeled a a semi markov decision process smdps we introduce the notion of smdp homomorphism and argue that it provides a useful tool for a rigorous study of abstraction for smdps we present an smdp minimization framework and an abstraction framework for factored mdps based on smdp homomorphism we also model different class of abstraction that arise in hierarchical system although we use the option framework for purpose of illustration the idea are more generally applicable we also show that the condition for abstraction we employ are a generalization of earlier work by dietterich a applied to the option framework 
a growing number of application seek to incorporate automatically generated narrative structure into interactive virtual environment in this paper we evaluate a representation for narrative structure generated by an automatic planning system by mapping the plan that control plot into conceptual graph used by quest an existing framework for question answering analysis that includes structure for modeling a reader s narrative comprehension and using method originally employed by quest s developer to determine if the plan structure can serve a effective model of the understanding that human user form after viewing corresponding story played out within a virtual world result from our analysis are encouraging though additional work is required to expand the plan language to cover a broader class of narrative structure 
this paper introduces the concept of resource temporal network rtn a constraint network that subsumes both classical attribute used in a i planning and capacity resource traditionally handled in scheduling after giving a formal definition of rtns we analyze their expressive power and study complexity of several fragment of the rtn framework we show that solving an rtn is in general np complete which is not surprising given the expressivity of the framework whereas computing a necessary truth criterion is polynomial this last result open the door for promising algorithm to solve rtns 
speculative execution of information gathering plan can dramatically reduce the effect of source i o latency on overall performance however the utility of speculation is closely tied to how accurately data value are predicted at runtime caching 
this research summary describes some work in progress on using graphical model to represent relational data in computational science portal such a mygrid the objective is to provide a integrative collaborative filtering cf capability to user of data metadata source code and experimental documentation in some domain of interest recent system such a researchindex citeseer provide collaborative recommendation through citation indexing and system such a sourceforge and the open bioinformatics project provide similar tool such a content based indexing of software our current research aim at learning probabilistic relational model prms from data in order to support intellignet retrieval of data source code and experimental record we present a system design and a pr ci of a test bed under development that applies prm structure learning and inference to cf in repository of bioinformatic s data and software 
a representational gap exists between low level measurement segmentation object classification tracking and high level understanding of video sequence in this paper we propose a novel representation of event in video to bridge this gap based on the case representation of natural language the proposed representation ha three significant contribution over existing framework first we recognize the importance of causal and temporal relationship between subevents and extend case to allow the representation of temporal structure and causality between sub event second in order to capture both multi agent and multithreaded event we introduce a hierarchical case representation of event in term of sub event and case list last for purpose of implementation we present the concept of a temporal event tree and pose the problem of event detection a subtree pattern matching by extending case a natural language representation for the representation of event the proposed work allows a plausible mean of interface between user and the computer we show two important application of the proposed event representation for the automated annotation of standard meeting video sequence and for event detection in extended video of railroad crossing 
this paper investigates a new approach for training discriminant classifier when only a small set of labeled data is available together with a large set of unlabeled data this algorithm optimizes the classification maximum likelihood of a set of labeled unlabeled data using a variant form of the classification expectation maximization cem algorithm it originality is that it make use of both unlabeled data and of a probabilistic misclassification model for these data the parameter of the label error model are learned together with the classifier parameter we demonstrate the effectiveness of the approach on four data set and show the advantage of this method over a previously developed semi supervised algorithm which doe not consider imperfection in the labeling process 
a the artificial intelligence ai system in military simulation and computer game become more complex their action become increasingly difficult for user to understand expert system for medical diagnosis have addressed this challenge though the addition of explanation generation system that explain a system s internal process this paper describes the ai architecture and associated explanation capability used by full spectrum command a training system developed for the u s army by commercial game developer and academic researcher 
the aim of this paper is to compare bayesian network classifier to the k nn classifier based on a subset of feature this subset is established by mean of sequential feature selection method experimental result show that bayesian network classifier more often achieve a better classification rate on different data set than selective k nn classifier the k nn classifier performs well in the case where the number of sample for learning the parameter of the bayesian network is small bayesian network classifier outperform selective knn method in term of memory requirement and computational demand this paper demonstrates the strength of bayesian network for classification 
this poster show an artificial neural network capable of learning a temporal sequence directly inspired from a hippocampus model banquet et al this architecture allows an autonomous robot to learn how to imitate a sequence of movement with the correct timing 
in this paper we propose the framework of monte carlo algorithm a a useful one to analyze ensemble learning in particular this framework allows one to guess when bagging will be useful explains why increasing the margin improves performance and suggests a new way of performing ensemble learning and error estimation 
utility elicitation is a critical function of any automated decision aid allowing decision to be tailored to the preference of a specific user however the size and complexity of utility function often precludes full elicitation requiring that decision be made without full utility information adopting the minimax regret criterion for decision making with incomplete utility information we describe and empirically compare several new procedure for incremental elicitation of utility function that attempt to reduce minimax regret with a few question a possible specifically using the continuous space of standard gamble query we show that myopically optimal query can be computed effectively in polynomial time for several different improvement criterion one such criterion in particular empirically outperforms the others we examine considerably and ha provable improvement guarantee 
this paper delineates the computational complexity of propositional multi context system we establish np membership by translating multi context system into bounded modal kn and obtain more refined complexity result by achieving the so called bounded model property the number of local model needed to satisfy a set of formula in a multi context system m is bounded by the number of context addressed by plus the number of bridge rule in m exploiting this property of multi context system we are able to encode contextual satisfiability into purely propositional satisfiability providing for the implementation of contextual reasoner based on already existing specialized s at solver finally we apply our result to improve complexity bound for mccarthy s propositional logic of context we show that satisfiability in this framework can be settled in nondeterministic polynomial time o 
this paper reconsiders the notion of actual cause and explanation in functional causal model we demonstrate that isomorphic causal model can generate intuitively different causal pronounce ments this occurs because psychological factor not represented in the model determine what cri teria we use to determine causation this par tially explains the difficulty encountered in previ ous attempt to define actual cause freed from trying fit all example to match intuition directly which is not possible using only the information in causal model we provide definition for cau sation matching the different causal criterion we in tuitively apply this formulation avoids difficulty associated with previous definition and allows a more refined discussion of what constitutes a cause in a given situation the definition of actual cause also allow for more refined formulation of expla nation 
abstract we propose a new framework to study propertiesof consistency in a constraint network from theperspective of property of set intersection ourframework come with a proof schema which givesa generic way of lifting a set intersection propertyto one on consistency various well known resultscan be derived with this framework more importantly we use the framework to obtain a numberof new result we identify a new class of treeconvex constraint where local consistency ensures 
a corpus based knowledge representation system consists of a large collection of disparate knowledge fragment or schema and a rich set of statistic computed over the corpus we argue that by collecting such a corpus and computing the appropriate statistic corpus based representation offer an alternative to traditional knowledge representation for a broad class of application the key advantage of corpus based representation is that we avoid the laborious process of building a often brittle knowledge base we describe the basic building block of a corpus based representation system and a set of application for which such a paradigm is appropriate including one application where the approach is already showing promising result 
inconsistency frequently occur in knowledge about the real world some of these inconsistency may be more significant than others and some knowledgebases set of formula may contain more inconsistency than others this creates problem of deciding whether to act on these inconsistency and if so how to address this we provide a general characterization of inconsistency based on quasi classical logic a form of paraconsistent logic with a more expressive semantics than belnap s four valued logic and unlike other paraconsistent logic allows the connective to appear to behave a classical connective we analyse inconsistent knowledge by considering the conflict arising in the minimal quasi classical model for that knowledge this is used for a measure of coherence for each knowledgebase and for a measure of significance of inconsistency in each knowledgebase in this paper we formalize this framework and consider application in managing heterogeneous source of knowledge 
based on the probabilistic reformulation of principal component analysis pca we consider the problem of determining the number of principal component a a model selection problem we present a hierarchical model for probabilistic pca and construct a bayesian inference method for this model using reversible jump markov chain monte carlo mcmc by regarding each principal component a a point in a one dimensional space and employing only birth death move in our reversible jump methodology our proposed method is simple and capable of automatically determining the number of principal component and estimating the parameter simultaneously under the same disciplined framework simulation experiment are performed to demonstrate the effectiveness of our mcmc method 
speculative execution of information gathering plan can dramatically reduce the effect of source i o latency on overall performance however the utility of speculation is closely tied to how accurately data value are predicted at runtime caching 
speculative execution of information gathering plan can dramatically reduce the effect of source i o latency on overall performance however the utility of speculation is closely tied to how accurately data value are predicted at runtime caching 
there are a number of framework for modelling argumentation in logic they incorporate a formal representation of individual argument and technique for comparing conflicting argument a problem with these proposal is that they do not consider the believability of the argument from the perspective of the intended audience in this paper we start by reviewing a logic based framework for argumentation based on argument tree which provide a way of exhaustively collating argument and counter argument we then extend this framework to it model theoretic evaluation of the believability of argument this extension assumes that the belief of a typical member of the audience for argumentation can be represented by a set of classical formula a beliefbase we compare a beliefbase with each argument to evaluate the empathy or similarly the antipathy that an agent ha for the argument we show how we can use empathy and antipathy to define a pre ordering relation over argument tree that capture how nne argument tree is more believable than another we also use these to define criterion for deciding whether an argument at the root of an argument tree is defeated or undeleated given the other argument in the tree 
current mapping algorithm using consistent pose estimation cpe algorithm can successfully map area of square meter using thousand of pose however the computation to construct the map grows a o n log n so larger map get increasingly difficult to build we present an abstraction method for postponing the growth in computation this method solves a much smaller problem in the space of the connection graph of the map 
clark s completion is a simple nonmonotonic formalism and a special case of several nonmonotonic logic recently there ha been work on extending completion with loop formula so that general case of nonmonotonic logic such a logic program under the answer set semantics and mccain turner causal logic can be characterized by propositional logic in the form of completion loop formula in this paper we show that the idea is applicable to mccarthy s circumscription in the propositional case with lifschitz s pointwise circumscription playing the role of completion we also show how to embed propositional circumscription in logic program and in causal logic inspired by the uniform characterization of completion loop formula 
this paper address the question of allocating computational resource among a set of algorithm in order to achieve the best performance on a scheduling problem instance our primary motivation in addressing this problem is to reduce the expertise needed to apply constraint technology therefore we investigate algorithm control technique that make decision based only on observation of the improvement in solution quality achieved by each algorithm we call our approach low knowledge since it doe not rely on complex prediction model we show that such an approach result in a system that achieves significantly better performance than all of the pure algorithm without requiring additional human expertise furthermore the low knowledge approach achieves performance equivalent to a perfect high knowledge classification approach 
we propose and study extension of logic programming with constraint represented a generalized atom of the form c x where x is a finite set of atom and c is an abstract constraint formally a collection of set of atom atom c x are satisfied by an interpretation set of atom m if m x c we focus here on monotone constraint that is those collection c that are closed under the superset they include in particular weight or pseudo boolean constraint studied both by the logic programming and sat community we show that key concept of the theory of normal logic program such a the one step provability operator the semantics of supported and stable model a well a several of their property including complexity result can be lifted to such case 
this poster describes method to enable intelligent access to multimodal information stream we illustrate these method in two integrated system the broadcast news editor bne which incorporates image speech and language processing and the broadcast news navigator bnn which provides search visualization and personalized access to broadcast news video bnn enables user to perform keyword and named entity search temporally and geospatially visualize entity and story cluster story discover entity relation and obtain personalized multimedia summary by transforming access from sequential to direct search and providing hierarchical hyperlinked summary bne and bnn enable user to access topic and entity news cluster nearly three time a fast a direct search of video 
this paper concern the assessment of linear cause effect relationship from a combination of observational data and qualitative causal structure the paper show how technique developed for identifying causal effect in causal bayesian network can be used to identify linear causal effect and thus provides a new approach for assessing linear causal effect in structural equation model using this approach the paper develops a systematic procedure for recognizing identifiable direct causal effect 
knowledge based question answering system have become quite competent and robust at answering a wide range of question in different domain however in order to ask question correctly one need to have intimate knowledge of the structure of the knowledge base and typical user lack this knowledge we address this problem by developing a system that us the content of the knowledge base to automatically align a user s encoding of a query to the structure of the knowledge base our preliminary evaluation show the system detects and corrects most misalignment and user are able to pose most question quickly 
there have been many proposal for first order belief network i e where we quantify over individual but these typically only let u reason about the individual that we know about there are many instance where we have to quantify over all of the individual in a population when we do this the population size often matter and we need to reason about all of the member of the population but not necessarily individually this paper present an algorithm to reason about multiple individual where we may know particular fact about some of them but want to treat the others a a group combining unification with variable elimination let u reason about class of individual without needing to ground out the theory 
the evolution of description logic dl and propositional dynamic logic produced a hierarchy of decidable logic with multiple maximal element it would be desirable to combine different maximal logic into one super logic but then inference may turn out to be undecidable then it is important to characterize the decidability threshold for these logic in this perspective an interesting open question pointed out by sattler and vardi sattler and vardi is whether inference in a hybrid calculus with restricted form of graded modality is decidable and which complexity class it belongs to in this paper we prove that this calculus and the corresponding dl alciof are undecidable second we prove undecidability result for logic that support both a transitive closure operator over role and number restriction 
object recognition and detection represent a relevant component in cognitive computer vision system such a in robot vision intelligent video surveillance system or multimodal interface object identification from local information ha recently been investigated with respect to it potential for robust recognition e g in case of partial object occlusion scale variation noise and background clutter in detection task this work contributes to this research by a thorough analysis of the discriminative power of local appearance pattern and by proposing to exploit local information content to model object representation and recognition we identify discriminative region in the object view from a posterior entropy measure and then derive object model from selected discriminative local pattern for recognition we determine rapid attentive search for location of high information content from learned decision tree the recognition system is evaluated by various degree of partial occlusion and gaussian image noise resulting in highly robust recognition even in the presence of severe occlusion effect 
particle filter are used extensively for tracking the state of non linear dynamic system this paper present a new particle filter that maintains sample in the state space at dynamically varying resolution for computational efficiency resolution within siatespace varies by region depending on the belief that the true state lie within each region where belief is strong resolution is fine where belief is low resolution is coarse abstracting multiple similar state together the resolution of the statespace is dynamically updated a the belief change the proposed algorithm make an explicit bias variance tradeoff to select between maintaining sample in a biased generalization of a region of state space versus in a high variance specialization at fine resolution sample are maintained at a coarser resolution when the bias introduced by the generalization to a coarse resolution is outweighed by the gain in term of reduction in variance and at a finer resolution when it is not maintaining sample in abstraction prevents potential hypothesis from being eliminated prematurely for lack of a sufficient number of particle empirical result show that our variable resolution particle filter requires significantly lower computation for performance comparable to a classical particle filter 
flux belongs to the high level programming language for cognitive agent that have been developed in recent year based on the established general action representation formalism of the fluent calculus flux allows to implement complex strategy in a concise and modular fashion in this paper we extend the flux language to reason about domain involving continuous change and where action occur concurrently using constraint logic programming we show that this reasoning is performed in an efficient way 
there are a number of framework for modelling argumentation in logic they incorporate a formal representation of individual argument and technique for comparing conflicting argument an example is the framework by besnard and hunter that is based on classical logic and in which an argument obtained from a knowledgebase is a pair where the first item is a minimal consistent set of formula that prof the second item which is a formula in the framework the only counter argument defeaters that need to be taken into account are canonical argument a form of minimal undercut argument tree then provide a way of exhaustively collating argument and counter argument a problem with this set up is that some argument tree may be too big to have sufficient impact in this paper we address the need to increase the impact of argumentation by using pruned argument tree we formalize this in term of how argument resonate with the intended audience of the argument for example if a politician want to make a case for raising tax the argument used would depend on what is important to the audience argument based on increased tax are needed to pay for improved healthcare would resonate better with an audience of pensioner whereas argument based on increased tax are needed to pay for improved transport infrastructure would resonate better with an audience of business executive by analysing the resonance of argument we can prune argument tree to raise their impact 
user feedback is vital in many recommender system to help guide the search for good recommendation preference based feedback e g show me more like item a is an inherently ambiguous form of feedback with a limited ability to guide the recommendation process and for this reason it is usually avoided nevertheless we believe that certain domain demand the use of preference based feedback a such we describe and evaluate a flexible recommendation strategy that ha the potential to improve the performance of case based recommenders that rely on preference based feedback 
when searching for multi attribute service or product understanding and representing user s preference is a crucial task however many computer tool do not afford user to adequately focus on fundamental decision objective reveal hidden preference revise conflicting preference or explicitly reason about tradeoff with competing decision goal a a result user often fail to find the best solution from building decision support system for various application domain we have observed some common area of design pitfall which could lead to undesirable user behavior and ineffective use of decision system by incorporating finding from behavior decision theory we have identified and accumulated a set of principle for avoiding these design pitfall provide a flexible order and choice in preference elicitation so that user can focus on fundamental objective include appropriate information in a decision context to guide user in revealing hidden preference and make tradeoff attribute explicit to facilitate preference revision and flexible decision making we describe these principle and the corresponding interface affordances and discus concrete scenario where they have been applied and tested 
the ability to recognize when an agent abandon a plan is an open problem in the plan recognition literature and is a significant problem if these method are to be applied in real system this paper present an explicit formal and implemented solution to the problem of recognizing when an agent ha abandoned one of it goal based on a theory of probabilistic model revision 
this paper deal with automatically learning the spatial distribution of a set of image that is given a sequence of image acquired from well separated location how can they be arranged to best explain their genesis the solution to this problem can be viewed a an instance of robot mapping although it can also be used in other context we examine the problem where only limited prior odometric information is available employing a feature based method derived from a probabilistic pose estimation framework initially a set of visual feature is selected from the image and correspondence are found across the ensemble the image are then localized by first assembling the small subset of image for which odometric confidence is high and sequentially inserting the remaining image localizing each against the previous estimate and taking advantage of any prior that are available we present experimental result validating the approach and demonstrating metrically and topologically accurate result over two large image ensemble finally we discus the result their relationship to the autonomous exploration of an unknown environment and their utility for robot localization and navigation 
we describe a framework for reducing the space complexity of graph search algorithm such a a that use open and closed list to keep track of the frontier and interior node of the search space we propose a sparse representation of the closed list in which only a fraction of already expanded node need to be stored to perform the two function of the closed list preventing duplicate search effort and allowing solution extraction our proposal is related to earlier work on search algorithm that do not use a closed list at all korf and zhang however the approach we describe ha several advantage that make it effective for a wider variety of problem 
in this paper we introduce an approach to exploit knowledge represented in an ontology in answer to query to an information base we assume that the ontology is embedded in a knowledge base covering the domain of the information base the ontology is first of all to influence ranking of object in answer to query a measured by similarity to the query we consider a generative framework where an ontology in combination with a concept language defines a set of well formed concept wellformed concept is assumed to be the basis for an indexing of the information base in the sense that these concept appear a descriptor attached to object in the base concept are thus applied to obtain a mean for description that generalizes simple word based information base indexing in effect query evaluation is generalized to be a matter of comparison at the level of concept rather than word 
the availability of web search ha revolutionised the way people discover information yet a search service maintain larger and larger index they are in danger of becoming a victim of their own success many common search can return a vast number of web page many of which will be irrelevant to the searcher and of which only about ten or twenty of the top ranked result will be browsed the problem is that while page returned by a search may be relevant to the keywords entered the keywords generally give only a partial expression of the searcher s information need personalised web search take keywords from the user a an expression of their information need but also us additional information about the user such a their preference community location or history to assist in determining the relevance of page there are many approach to providing personalised web search each with the aim of returning the result most relevant to the user ranked highest the feature that distinguish the approach are the kind of information about the user that is used the level of interaction with the user explicit or implicit collection of data how the information is stored client side or server side the algorithm used to incorporate the information about the user into the search and how information is presented to the user mobile device present some unique challenge in this respect some of these personalisation method stem from technique previously used in traditional information retrieval whilst others are unique to the web environment this chapter describes the many technique that have been applied to adapt the web search process to the individual user we also present a novel system that we are developing which us a client side user proflle to provide a personalised ranking of result from multiple search portal we conclude with a brief consideration of the future of personalised search and how it may afiect the development of the web 
abstract we describe an approach to machine learning from numerical data that combine both qualitative and numerical learning this approach is carried out in two stage induction of a qualitative model from numerical example of the behaviour of a physical system and induction of a numerical regression function that both respect the qualitative constraint and fit the training data numerically we call this approach q learning to the identification of a car wheel suspension system a complex industrially relevant mechanical system elsevier b v all right reserved keywords automated model building system identification machine learning qualitative reasoning learning 
admissible and consistent heuristic function are usually preferred in single agent heuristic search a they guarantee optimal solution with complete search method such a a and ida larger problem however frequently make a complete search intractable due to space and or time limitation in particular a path planning agent in a real time strategy game may need to take an action before it complete search ha the time to finish in such case incomplete search technique such a rta srta rtdp dta can be used such algorithm conduct a limited ply lookahead and then evaluate the state envisioned using a heuristic function the action selected on the basis of such evaluation can be suboptimal due to the incompleteness of search and inaccuracy in the heuristic it is usually believed that deeper lookahead increase the chance of taking the optimal action in this paper we demonstrate that this is not necessarily the case even when admissible and consistent heuristic function are used 
many real life optimization problem contain both hard and soft constraint a well a qualitative conditional preference however there is no single formalism to specify all three kind of information we therefore propose a framework based on both cp net and soft constraint that handle both hard and soft constraint a well a conditional preference efficiently and uniformly we study the complexity of testing the consistency of preference statement and show how soft constraint can faithfully approximate the semantics of conditional preference statement whilst improving the computational complexity 
the resource constrained project scheduling problem with time window rcpsp max is an important generalization of a number of well studied scheduling problem in this paper we present a new heuristic algorithm that combine the benefit of squeaky wheel optimization with an effective conflict resolution mechanism called bulldozing to address rcpsp max problem on a range of benchmark problem the algorithm is competitive with state of the art systematic and non systematic method and scale well 
this paper demonstrates the applicability of auto mated reasoning to text processing specifically to question answering it is shown that the approach is feasible effective and scalable a logic prover ha been implemented and integrated into a stateof the art question answering system 
information extraction ie aim at extracting specific information from a collection of document a lot of previous work on from semi structured document in xml or html us learning technique based on string some recent work convert the document to a ranked tree and us tree automaton induction this paper introduces an algorithm that us unranked tree to induce an automaton experiment show that this give the best result obtained so far for ie from semi structured document based on learning 
the paper address the problem of automatic abstraction of component variable in the context of model based diagnosis in order to produce model capable of deriving fewer and more general diagnosis when the current observability of the system is reduced the notion of indiscriminability among fault of a set of component is introduced and constitutes the basis for a formal definition of admissible abstraction which preserve all the distinction that are relevant for diagnosis given the current observability of the system the automatic synthesis of abstract model further restricts abstraction such that the behavior of abstract component is expressed in term of a simple and intuitive combination of the behavior of their subcomponents a a validation of our proposal we present experimental result which show the reduction in the number of diagnosis returned by a diagnostic agent for a space robotic arm 
this paper proposes a unique map learning method for mobile robot based on the co visibility information of object i e the information on whether two object are visible at the same time or not from the current position this method first estimate empirical distance among the object using a simple heuristic a pair of object observed at the same time more frequently is likely to be located more closely together then it computes all the coordinate of the object by multidimensional scaling md technique in the latter part of this paper it is shown that the proposed method is able to learn qualitatively very accurate map though it us only such primitive information and that it is robust against some kind of object recognition error 
this paper address the probabilistic inference of geometric structure from image specifically of synthesizing range data to enhance the reconstruction of a d model of an indoor environment by using video image and very partial depth information in our method we interpolate the available range data using statistical inference learned from the concurrently available video image and from those sparse region where both range and intensity information is available the spatial relationship between the variation in intensity and range can be efficiently captured by the neighborhood system of a markov random field mrf in contrast to classical approach to depth recovery i e stereo shape from shading we can afford to make only weak prior assumption regarding specific surface geometry or surface reflectance function since we compute the relationship between existing range data and the image we start with experimental result show the feasibility of our method 
after many success statistical approach that have been popular in the parsing community are now making headway into natural language generation nlg these system are aimed mainly at surface realization and promise the same advantage that make statistic valuable for parsing robustness wide coverage and domain independence a recent experiment aimed to empirically verify the linguistic coverage for such a statistical surface realization component by generating transformed sentence from the penn treebank corpus this article present the empirical result of a similar experiment to evaluate the coverage of a purely symbolic surface realizer we present the problem facing a symbolic approach on the same task describe the result of it evaluation and contrast them with the result of the statistical method to help quantitatively determine the level of coverage currently obtained by nlg surface realizers 
speculative execution of information gathering plan can dramatically reduce the effect of source i o latency on overall performance however the utility of speculation is closely tied to how accurately data value are predicted at runtime caching 
the independent lifestyle assistant i l s a is an agent based monitoring and support system to help elderly people to live longer in their home by reducing caregiver burden i l s a is a multiagent system that incorporates a unified sensing model situation assessment response planning real time response and machine learning this paper describes the some of the lesson we learned during the development and six month field study 
we investigate phase transition for the family of bounded satisfiability problem sat b recently introduced by zhang that ask given a cnfformula is there a truth assignment that violates no more than b of it clause zhang s result were experimental and for a fixed number of variable n and suggested that the location of the phase transition for sat b are separated and move significantly a b increase analysis of these location wa posed a an open question we analytically show that the phase transition of all sat problem must occur within a narrow region regardless of how large the value of b is moreover our experiment reveal that the phase transition for these problem occur in a remarkable way specifically unlike sat the probability curve for sat do not have a quasi common intersection point about which they rotate a they become steeper with increasing n instead they move rapidly to the left toward the narrow region that the analysis predicts 
speculative execution of information gathering plan can dramatically reduce the effect of source i o latency on overall performance however the utility of speculation is closely tied to how accurately data value are predicted at runtime caching 
we present a new algorithm gm sarsa for finding approximate solution to multiple goal reinforcement learning problem that are modeled a composite markov decision process according to our formulation different sub goal are modeled a mdps that are coupled by the requirement that they share action existing reinforcement learning algorithm address similar problem formulation by first finding optimal policy for the component mdps and then merging these into a policy for the composite task the problem with such method is that policy that are optimized separately may or may not perform well when they are merged into a composite solution instead of searching for optimal policy for the component mdps in isolation our approach find good policy in the context of the composite task 
we describe the application of plan recognition technique to support human intelligence analyst in processing national security alert set by automatically identifying the hostile intent behind them identifying the intent enables u to both prioritize and explain the alert set for succinct user presentation our empirical evaluation demonstrates that the approach can handle alert set of a many a element and can readily distinguish between false and true alarm we discus the important opportunity for future work that will increase the cardinality of the alert set supported by the system to the level demanded by a deployable application in particular we outline opportunity to bring the analyst into the process and the opportunity for heuristic improvement to the plan recognition algorithm 
model checking is a promising approach to automatic verification which ha concentrated on specification expressed in temporal logic comparatively little attention ha been given to temporal logic of knowledge although such logic have been proven to be very useful in the specification of protocol for distributed system in this paper we address ourselves to the model checking problem for a temporal logic of knowledge halpern and vardi s logic of c k ln based on the semantics of interpreted system with local proposition we develop an approach to symbolic c k ln model checking via obdds in our approach to model checking specification involving agent knowledge the knowledge modality are eliminated via quantifier over agent non observable variable 
speculative execution of information gathering plan can dramatically reduce the effect of source i o latency on overall performance however the utility of speculation is closely tied to how accurately data value are predicted at runtime caching is one approach that can be used to issue future prediction but it scale poorly with large data source and is unable to make intelligent prediction given previously unseen input data even when there is an obvious relationship between past input and the output it generated in this paper we describe a novel way to combine classification and transduction for a more efficient and accurate value prediction strategy one capable of issuing prediction about previously unseen hint we show how our approach result in significant speedup for plan that query multiple source or source that require multi page navigation 
in this paper we develop a computational learning framework to build a hierarchy of consumer photo category for semantic retrieval two level of visual semantics are learned for image content and image category statistically we evaluate the average precision at top retrieved photo on heterogeneous consumer photo with very good result 
a key feature of modern optimal planner such a graphplan and blackbox is their ability to prune large part of the search space previous partial order causal link pocl planner provide an alternative branching scheme but lacking comparable pruning mechanism do not perform a well in this paper a domain independent formulation of temporal planning based on constraint programming is introduced that successfully combine a pocl branching scheme with powerful and sound pruning rule the key novelty in the formulation is the ability to reason about support precedence and causal link involving action that are not in the plan experiment over a wide range of benchmark show that the resulting optimal temporal planner is much faster than current one and is competitive with the best parallel planner in the special case in which action have all the same duration 
recently conitzer and sandholm conitzer sandholm introduced the concept of automated mechanism design whereby mechanism design problem are solved using constraint satisfaction method traditionally mechanism design ha focused on producing game which yield the desired outcome when played by ideal rational player however actual player are never perfectly rational human irrationality ha been exhaustively studied and computational agent have both resource bound and potentially implementation flaw in this paper we discus extension of the technique of automated mechanism design to produce game which are robust in the face of player imperfection we model limited rationality by examining agent which converge on their strategy by using a simple variant of fictitious play simulation of repeated play singh kearns mansour this model associate to each game a system of differential equation describing the trajectory of the agent s strategy we describe additional constraint which guarantee that automated mechanism design search problem yield stable mechanism in particular we present negative result for structural stability and positive result for asymptotic stability by considering strict bayesian nash equilibrium and by employing lyapunov technique 
the tm lpsat planner can construct plan in domain containing atomic action and durative action event and process discrete real valued and interval valued fluents and continuous linear change to quantity it work in three stage in the first stage a representation of the domain and problem in an extended version of pddl is compiled into a system of propositional combination of propositional variable and linear constraint over numeric variable in the second stage the lpsat constraint engine wolfman weld is used to find a solution to the system of constraint in the third stage a correct parallel plan is extracted from this solution we discus the structure of the planner and show how a real time temporal model is compiled into lpsat constraint 
speculative execution of information gathering plan can dramatically reduce the effect of source i o latency on overall performance however the utility of speculation is closely tied to how accurately data value are predicted at runtime caching 
this paper present a novel promising approach that allows greedy decision tree induction algorithm to handle problematic function such a parity function lookahead is the standard approach to addressing difficult function for greedy decision tree learner nevertheless this approach is limited to very small problematic function or subfunctions or variable because the time complexity grows more than exponentially with the depth of lookahead in contrast the approach presented in this paper carry only a constant run time penalty experiment indicate that the approach is effective with only modest amount of data for problematic function or subfunctions of up to six or seven variable where the example themselves may contain numerous other irrelevant variable a well 
we solve the problem of obtaining answer to query posed to a mediated integration system under the local a view paradigm that are consistent wrt to certain global integrity constraint for this the query program is combined with logic programming specification under the stable model semantics of the class of minimal global instance and of the class of their repair 
huge amount of data are stored in autonomous geographically distributed source the discovery of previously unknown implicit and valuable knowledge is a key aspect of the exploitation of such source in recent year several approach to knowledge discovery and data mining and in particular to clustering have been developed but only a few of them are designed for distributed data source we propose a novel distributed clustering algorithm based on non parametric kernel density estimation which take into account the issue of privacy and communication cost that arise in a distributed environment 
speculative execution of information gathering plan can dramatically reduce the effect of source i o latency on overall performance however the utility of speculation is closely tied to how accurately data value are predicted at runtime caching 
we study the performance of some known algorithm for solving the simple temporal problem stp and the temporal constraint satisfaction problem tcsp in particular we empirically compare the bellman ford bf algorithm and it incremental version incbf by cesta oddi to the stp of xu choueiry a among the tested algorithm we show that stp is the most efficient for determining the consistency of an stp and that incbf combined with the heuristic of xu choueiry b is the most efficient for solving the tcsp we plan to improve stp by exploiting incrementality a in incbf and other new incremental algorithm 
maximum satisfiability max sat is more general and more difficult to solve than satisfiability sat in this paper we first investigate the effectiveness of walksat one of the best local search algorithm designed for sat on max sat we show that walksat is also effective on max sat while it effectiveness degrades a the problem is more constrained we then develop a novel method that exploit the backbone information in the local minimum from walksat and applies the backbone information in different way to improve the performance of the walksat algorithm we call our new algorithm backbone guided walksat bgwalksat on large random sat and max sat problem a well a instance from the satlib bgwalksat significantly improves walksat s performance 
this paper considers online stochastic optimization problem where time constraint severely limit the number of offline optimization which can be performed at decision time and or in between decision it proposes a novel approach which combine the salient feature of the earlier approach the evaluation of every decision on all sample expectatio n and the ability to avoid distributing the sample among decision consensus the key idea underlying the novel algorithm is to approximate the regret of a decision d the regret algorithm is evaluated on two fundamentally different application online packet scheduling in network and online multiple vehicle routing with time window on both application it produce significant benefit over prior approach 
markov decision process mdps and contingency planning cp are two widely used approach to planning under uncertainty mdps are attractive because the model is extremely general and because many algorithm exist for deriving optimal plan in contrast cp is normally performed using heuristic technique that do not guarantee optimality but the resulting plan are more compact and more understandable the inability to present mdp policy in a clear intuitive way ha limited their applicability in some important domain we introduce an anytime algorithm for deriving contingency plan that combine the advantage of the two approach 
speculative execution of information gathering plan can dramatically reduce the effect of source i o latency on overall performance however the utility of speculation is closely tied to how accurately data value are predicted at runtime caching 
speculative execution of information gathering plan can dramatically reduce the effect of source i o latency on overall performance however the utility of speculation is closely tied to how accurately data value are predicted at runtime caching 
sketch recognition system are currently being developed for many domain but can be time consuming to build if they are to handle the intricacy of each domain this paper present the first translator that take symbolic shape description written in the ladder sketch language and automatically transforms them into shape recognizers editing recognizers and shape exhibitor for use in conjunction with a domain independent sketch recognition system this transformation allows u to build a single domain independent recognition system that can be customized for multiple domain we have tested our framework by writing several domain description and automatically created a domain specific sketch recognition system for each domain 
in recent year we have witnessed the success of autonomous agent applying machine learning technique across a wide range of application however agent applying the same machine learning technique in online application have not been so successful even agent based hybrid recommender system that combine information filtering technique with collaborative filtering technique have only been applied with considerable success to simple consumer good such a movie book clothing and food complex adaptive autonomous agent system that can handle complex good such a real estate vacation plan insurance mutual fund and mortgage have yet emerged to a large extent the reinforcement learning method developed to aid agent in learning have been more successfully deployed in offline application the inherent limitation in these method have rendered them somewhat ineffective in online application in this paper we postulate that a small amount of prior knowledge and human provided input can dramatically speed up online learning we will demonstrate that our agent humane with it prior knowledge or experience about the real estate domain can effectively assist user in identifying requirement especially unstated one quickly and unobtrusively 
we propose a node centroid method with hill climbing to solve the well known matrix bandwidth minimization problem which is to permute row and column of the matrix to minimize it bandwidth many heuristic have been developed for this np complete problem including the cuthill mckee cm and the gibbs poole and stockmeyer gps algorithm recently heuristic such a simulated annealing tabu search and grasp have been used where tabu search and the grasp with path relinking have achieved significantly better solution quality than the cm and gps algorithm experimentation show that the node centroid method achieves the best solution quality when compared with these while being much faster than the newly developed algorithm 
in this paper we evaluate the use of implicit interest indicator a the basis for user profiling in the digital tv domain research in more traditional domain such a web browsing or usenet news indicates that some implicit interest indicator e g read time and mouse movement are capable of serving a alternative to explicit profile information such a user rating consequently the key question we wish to answer relates to the type of implicit indicator that can be identified within the dtv domain and the extent to which they can accurately reflect a user s true preference 
this paper describes a jam session system that enables a human player to interplay with virtual player which can imitate the player personality model of various human player previous system have parameter that allow some alteration in the way virtual player react but these system cannot imitate human personality our system can obtain three kind of player personality model from a midi recording of a session in which that player participated a reaction model a phrase model and a groove model the reaction model is the characteristic way that a player reacts to other player and it can be statistically learned from the relationship between the midi data of music the player listens to and the midi data of music improvised by that player the phrase model is a set of player s characteristic phrase it can be acquired through musical segmentation of a midi session recording by using voronoi diagram on a piano roll the groove model is a model that generates onset time deviation it can be acquired by using a hidden markov model experimental result show that the personality model of any player participating in a guitar trio session can be derived from a midi recording of that session 
abstract keyphrases are useful for a variety of purpose including summarizing indexing labeling categorizing clustering highlighting browsing andsearching the task of automatic keyphrase extractionis to select keyphrases from within the text of a givendocument automatic keyphrase extraction make itfeasible to generate keyphrases for the huge number ofdocuments that do not have manually assignedkeyphrases a limitation of previous keyphraseextraction algorithm is that the 
in this paper we examine method for comparing human and agent behavior the result of such a comparison can be used to validate a computer model of human behavior score a turning test or guide an intelligent tutoring system we introduce behavior bounding an automated model based approach for behavior comparison we identify how this approach can be used with both human and agent behavior we demonstrate that it requires minimal human effort to use and that it is efficient when working with complex agent finally we show empirical result indicating that this approach is effective at identifying behavioral problem in certain type of agent and that it ha superior performance when compared against two benchmark 
this paper introduces a new distinctive class of combinatorial auction protocol called price oriented rationing free porf protocol the outline of a porf protocol is a follows i for each bidder the price of each bundle of good is determined independently of his her own declaration while it can depend on the declaration of other bidder ii we allocate each bidder a bundle that maximizes his her utility independently of the allocation of other bidder i e rationing free although a porf protocol appears quite different from traditional protocol description surprisingly it is a sufficient and necessary condition for a protocol to be strategy proof furthermore we show that a porf protocol satisfying additional condition is false name proof at the same time any false name proof protocol can be described a a porf protocol that satisfies the additional condition a porf protocol is an innovative characterization of strategy proof protocol and the first attempt to characterize false name proof protocol such a characterization is not only theoretically significant but also useful in practice since it can serve a a guideline for developing new strategy false name proof protocol we present a new false name proof protocol based on the concept of a porf protocol 
an odd cycle of a logic program is a simple cycle that ha an odd number of negative edge in the dependency graph of the program similarly an even cycle is one that ha an even number of negative edge for a normal logic program that ha no odd cycle while it is known that such a program always ha a stable model and such a stable model can be computed in polynomial time we show in this paper that checking whether an atom is in a stable model is np complete and checking whether an atom is in all stable model is co np complete both are the same a in the general case for normal logic program furthermore we show that if a normal logic program ha exactly one odd cycle then checking whether it ha a stable model is np complete again the same a in the general case for normal logic program with a fixed number of even cycle we show that there is a polynomial time algorithm for computing all stable model furthermore this polynomial time algorithm can be improved significantly if the number of odd cycle is also fixed 
algorithm based on following local gradient information are surprisingly effective for certain class of constraint satisfaction problem unfortunately previous local search algorithm are notoriously incomplete they are not guaranteed to find a feasible solution if one exists and they cannot be used to determine unsatisfiability we present an algorithmic framework for complete local search and discus in detail an instantiation for the propositional satisfiability problem sat the fundamental idea is to use constraint learning in combination with a novel objective function that converges during search to a surface without local minimum although the algorithm ha worst case exponential space complexity we present empirical resulls on challenging sat competition benchmark that suggest that our implementation can perform a well a state of the art solver based on more mature technique our framework suggests a range of possible algorithm lying between tree based search and local search 
an additive fluent is a fluent with numerical value such that the effect of several concurrently executed action on it can be computed by adding the effect of the individual action we propose a method for describing effect of action on additive fluents in the declarative language c an implementation of this language called the causal calculator can be used for the automation of example of commonsense reasoning involving additive fluents 
the coalition search and rescue task support demonstration show cooperative agent supporting a highly dynamic mission in which ai task planning interagent collaboration workflow enactment policy managed communication semantic web query semantic web service matchmaking and knowledge based notification are employed 
senseclusters is a freely available word sense discrimination system that take a purely unsupervised clustering approach it us no knowledge other than what is available in a raw unstructured corpus and cluster instance of a given target word based only on their mutual contextual similarity it is a complete system that provides support for feature selection from large corpus several different context representation scheme various clustering algorithm and evaluation of the discovered cluster 
we overview the development of first order automated reasoning system starting from their early year based on the analysis of current and potential application of such system we also try to predict new trend in first order automated reasoning our presentation will be centered around two main motif efficiency and usefulness for existing and future potential application 
we follow searle s contention that speaking a natural language is to engage in a rule governed form of behaviour and that those rule are conventional institutional rather than natural or physical we show how this analysis can also be used to specify rule of interaction for system of electronic agent communicating with an artificial language we conclude that using constitutive rule to define the semantics of an agent communication language not only distinguishes agent communication from method invocation but also offer significant computational advantage over using intentional state 
animal s rhythmic movement such a locomotion are considered to be controlled by neural circuit called central pattern generator cpgs this article present a reinforcement learning rl method for a cpg controller which is inspired by the control mechanism of animal because the cpg controller is an instance of recurrent neural network a naive application of rl involves difficulty in addition since state and action space of controlled system are very large in real problem such a robot control the learning of the value function is also difficult in this study we propose a learning scheme for a cpg controller called a cpgactor critic model whose learning algorithm is based on a policy gradient method we apply our rl method to autonomous acquisition of biped locomotion by a biped robot simulator computer simulation show our method is able to train a cpg controller such that the learning process is stable 
speculative execution of information gathering plan can dramatically reduce the effect of source i o latency on overall performance however the utility of speculation is closely tied to how accurately data value are predicted at runtime caching 
speculative execution of information gathering plan can dramatically reduce the effect of source i o latency on overall performance however the utility of speculation is closely tied to how accurately data value are predicted at runtime caching 
information extraction method can be used to automatically fill in database form from unstructured data such a web document or email state of the art method have achieved low error rate but invariably make a number of error the goal of an interactive information extraction system is to assist the user in filling in database field while giving the user confidence in the integrity of the data the user is presented with an interactive interface that allows both the rapid verification of automatic field assignment and the correction of error in case where there are multiple error our system take into account user correction and immediately propagates these constraint such that other field are often corrected automatically linear chain conditional random field crfs have been shown to perform well for information extraction and other language modelling task due to their ability to capture arbitrary overlapping feature of the input in a markov model we apply this framework with two extension a constrained viterbi decoding which find the optimal field assignment consistent with the field explicitly specified or corrected by the user and a mechanism for estimating the confidence of each extracted field so that low confidence extraction can be highlighted both of these mechanism are incorporated in a novel user interface for form filling that is intuitive and speed the entry of data providing a reduction in error due to automated correction 
hand crafting effective visual presentation is time consuming and requires design skill here we present a case based graphic sketch generation algorithm which us a database of existing graphic example case to automatically create a sketch of a presentation for a new user request a the first case based learning approach to graphic generation our work offer three unique contribution first we augment a similarity metric with a set of adequacy evaluation criterion to retrieve a case that is most similar to the request and is also usable in sketch synthesis to facilitate the retrieval of case fragment we develop a systematic approach to case request decomposition when a usable case cannot be found second we improve case retrieval speed by organizing case into hierarchical cluster based on their similarity distance and by using dynamically selected cluster representative third we develop a general case composition method to synthesize a new sketch from multiple retrieved case furthermore we have implemented our casebased sketch generation algorithm in a user system cooperative graphic design system called improvise which help user to generate creative and tailored presentation 
in this demo we focus on the engineering of open multi agent system a electronic institution electronic institution are a formalism to define the rule which structure agent interaction establishing what agent are permitted and forbidden to do we present a set of tool that support the specification analysis and execution of institution a well a the implementation of agent our methodology allows for a successive refinement approach to multi agent system engineering 
society need humor not just for entertainment in the web age presentation become more and more flexible and personalized and they will require humor contribution for electronic commerce development e g product promotion getting selective attention help in memorizing name etc more or le a it happened in the world of broadcasted advertisement even if deep modeling of humor in all of it facet is not something for the near future there is something concrete that ha been achieved and that can help in providing attention to the field the paper refers to the result of hahacronym a project devoted to humorous acronym production a circumscribed task that nonetheless requires various generic component the project open the way to development for creative language with application in the world of advertisement 
we examine the approach of encoding planning problem a csps more closely first we present a simple csp encoding for planning problem and then a set of transformation that can be used to eliminate variable and add new constraint to the encoding we show that our transformation uncover additional structure in the planning problem structure that subsumes the structure uncovered by graphplan planning graph we solve the csp encoded planning problem by using standard csp algorithm empirical evidence is presented to validate the effectiveness of this approach to solving planning problem and to show that even a prototype implementation is more effective than standard graphplan our prototype is even competitive with far more optimized planning graph based implementation we also demonstrate that this approach can be more easily lifted to more complex type of planning than can planning graph in particular we show that the approach can be easily extended to planning with resource 
from a computational perspective there is a close connection between various probabilistic reasoning task and the problem of counting or sampling satisfying assignment of a propositional theory we consider the question of whether state of the art satisfiability procedure based on random walk strategy can be used to sample uniformly or nearuniformly from the space of satisfying assignment we first show that random walk sat procedure often do reach the full set of solution of complex logical theory moreover by interleaving random walk step with metropolis transition we also show how the sampling becomes near uniform 
we propose a variable ordering heuristic for sat which is based on a structural analysis of the sat problem we show that when the heuristic is used by a davis putnam sat solver that employ conflict directed backtracking it produce a divide and conquer behavior in which the sat problem is recursively decomposed into smaller problem that are solved independently we discus the implication of this divide and conquer behavior on our ability to provide structure based guarantee on the complexity of davis putnam sat solver we also report on the integration of this heuristic with zchaffa state of the art sat solver showing experimentally that it significantly improves performance on a range of benchmark problem that exhibit structure 
in this paper we provide a polynomial time algorithm for solving an important class of metric temporal problem that involve simple temporal constraint between various event variable and piecewise constant preference function over variable domain we are given a graph g where x x xn is a set of event x is the beginning of the world node and is set to by convention and e xi xj annotated with the bound lb e ub e is a simple temporal constraint between xi and xj indicating that xj must be scheduled between lb e and ub e second after xi is scheduled lb e ub e a family of stepwise constant preference function f fxi t r r specifies the preference attached with scheduling xi at time t the goal is to find a schedule for all the event such that all the temporal constraint are satisfied and the sum of the preference is maximized our polynomial time algorithm for solving such problem which we refer to a extended simple temporal problem estps ha important consequence in dealing with limited form of disjunction and preference in metric temporal reasoning that would otherwise require an exponential search space 
we study the problem of agent negotiating period of time during which they can have use of resource thus allowing for the sharing of resource we define a multi stage negotiation framework where agent in order to obtain resource step through a sequence of stage each characterised by an increased chance of a mutually agreeable deal but at the price of disclosing more and more information in the sequence the agent may agree to move to the next stage if the previous stage fails to produce a deal amongst them in this paper we concentrate on two early negotiation stage characterised by minimal disclosure of information thus the agent negotiating at these stage can be thought of a minimally intrusive 
scheduling observation by coordinated fleet of earth observing satellite eos involves large search space complex constraint and poorly understood bottleneck condition where stochastic algorithm are often effective however there are many such algorithm and the best one to use is not obvious here we compare multiple variant of the genetic algorithm hill climbing simulated annealing squeaky wheel optimization and iterated sampling on ten realistically sized model eos scheduling problem schedule are represented by a permutation non temperal ordering of the observation request a simple greedy deterministic scheduler assigns time and resource to each observation request in the order indicated by the permutation discarding those that violate the constraint created by previously scheduled observation simulated annealing performs best and random mutation outperforms a more intelligent mutator furthermore the best mutator by a small margin wa a novel approach we call temperature dependent swap that make large change in the early stage of the search and smaller change towards the end 
we have developed a robot controller based upon a neural implementation of norman and shallice s model of executive attentional control in human a simulation illustrates how attentional control lead to the suppression of action selection error in neurally controlled robot a related demonstration illustrates how lesioning of the control architecture lead to behavioural pathology that resemble those seen in human patient with damage to the prefrontal cortex 
we develop an exact dynamic programming algorithm for partially observable stochastic game posgs the algorithm is a synthesis of dynamic programming for partially observable markov decision process pomdps and iterated elimination or dominated strategy in normal form game we prove that when applied to finite horizon posgs the algorithm iteratively eliminates very weakly dominated strategy without first forming a normal form representation of the game for the special case in which agent share the same payoff the algorithm can be used to find an optimal solution we present preliminary empirical result and discus way to further exploit pomdp theory in solving posgs 
we study two apparently different but formally similar scheduling problem the first problem involves contract algorithm which can trade off run time for solution quality a long a the amount of available run time is known in advance the problem is to schedule contract algorithm to run on parallel processor under the condition that an interruption can occur at any time and upon interruption a solution to any one of a number of problem can be requested schedule are compared in term of acceleration ratio which is a worst case measure of efficiency we provide a schedule and prove it optimality among a particular class of schedule our second problem involves multiple robot searching for a goal on one of multiple ray search strategics are compared in term of time competitive ratio the ratio of the total search time to the time it would take for one robot to traverse directly to the goal we demonstrate that search strategy and contract schedule are formally equivalent in addition for our class of schedule we derive a formula relating the acceleration ratio of a schedule to the time competitive ratio of the corresponding search strategy 
we consider how to use external memory such a disk storage to improve the scalability of heuristic search in state space graph to limit the number of slow disk i o operation we develop a new approach to duplicate detection in graph search that localizes memory reference by partitioning the search graph based on an abstraction of the state space and expanding the frontier node of the graph in an order that respect this partition we demonstrate the effectiveness of this approach both analytically and empirically 
we investigate the calculation of a bound for sequence and tree model which are the explicit intersection of a set of simpler model or can be bounded by such an intersection we provide a natural viewpoint which unifies various instance of factored a model for tree and sequence some previously known and others novel including multiple sequence alignment weighted finite state transducer composition and lexicalized statistical parsing the specific case of parsing with a product of syntactic pcfg and semantic lexical dependency component is then considered in detail we show that this factorization give a modular lexicalized parser which is simpler than comparably accurate non factored model and which allows efficient exact inference with large treebank grammar 
speculative execution of information gathering plan can dramatically reduce the effect of source i o latency on overall performance however the utility of speculation is closely tied to how accurately data value are predicted at runtime caching 
in this paper we introduce and study shortest path discovery spd problem a generalization of shortest path problem in spd one is given a directed edge weighted graph and the task is to find a the shortest path for fixed source and target node such that initially the edge weight are unknown but they can be queried querying the cost of an edge is expensive and hence the goal is to minimize the total number of edge cost query executed in this article we characterize some common property of sound spd algorithm propose a particular algorithm that is shown to be sound and effective experimental result on real world ocr task demonstrate the usefulness of the approach whereas the proposed algorithm is shown to yield a substantial speed up of the recognition process 
since the state space of most game is a directed graph many game playing system detect repeated position with a transposition table this approach can reduce search effort by a large margin however it suffers from the so called graph history interaction ghi problem which cause error in game containing repeated position this paper present a practical solution to the ghi problem that combine and extends previous technique because our scheme is general it is applicable to different game tree search algorithm and to different domain a demonstrated with the two algorithm and df pn in the two game checker and go our scheme incurs only a very small overhead while guaranteeing the correctness of solution 
recently several lower bound function are proposed for solving the max sat problem optimally in a branch and bound algorithm these lower bound improve significantly the performance of these algorithm based on the study of these lower bound function we propose a new linear time lower bound function we show that the new lower bound function is admissible and it is consistently and substantially better than other known lower bound function the result of this study is a high performance implementation of an exact algorithm for max sat which outperforms any implementation of the same class 
using language technology for text analysis and light weight ontology a a content mediating level we acquire indexing pattern from vast amount of indexing data for english language medical document this is achieved by statistically relating interlingual representation of these document based on text token bigram to their associated index term from these english indexing pattern we then induce the associated index term for german and portuguese document when their interlingual representation match those of english document thus we learn from past english indexing experience and transfer it in an unsupervised way to non english text without ever having seen concrete indexing data for language other than english 
this paper describes a new qualitative quantitative simulator to help buyer learn how to make decision when they purchase good in this paper we propose an e learning support system lsdm for assisting user decision making by applying artificial intelligence technology when buyer purchase expensive item they must carefully select these item from many alternative the learning support system provides useful information that assist consumer in purchasing good we employ qualitative simulation because the output simulation result are useful our system consists of a qualitative processing system and a quantitative calculation system when user use the system they first input information on the good they want to purchase the information input by user is used in the qualitative simulation next they supply the detail of their budget the rate of loan and several other factor on a form the system then integrates the result of simulation and the user s input data and proposes plan to aid in their decision process the system ha several advantage it can be used by simple input the process of simulation is easy to understand user can learn how to make decision by trial and error and the user can base their decision making on synthetic result 
this paper focus on temporal constraint problem where the objective is to optimize a set of local preference for when event occur in previous work a subclass of these problem ha been formalized a a generalization of temporal csps and a tractable strategy for optimization ha been proposed where global optimality is defined a maximizing the minimum of the component preference value this criterion for optimality which we call weakest link optimization wlo is known to have limited practical usefulness because solution are compared only on the basis of their worst value thus there is no requirement to improve the other value to address this limitation we introduce a new algorithm that rc applies wlo iteratively in a way that lead to improvement of all the value we show the value of this strategy by proving that with suitable preference function the resulting solution are pareto optimal 
our knowitall system aim to automate the tedious process of extracting large collection of fact e g name of scientist or politician from the web in an autonomous domain independent and scalable manner in it first major run knowitall extracted over fact with high precision but suggested a challenge how can we improve knowitall s recall and extraction rate without sacrificing precision this paper present three distinct way to address this challenge and evaluates their performance rule learning learns domain specific extraction rule subclass extraction automatically identifies sub class in order to boost recall list extraction locates list of class instance learns a wrapper for each list and extract element of each list since each method bootstrap from knowitall s domain independent method no hand labeled training example are required experiment show the relative coverage of each method and demonstrate their synergy in concert our method gave knowitall a fold to fold increase in recall while maintaining high precision and discovered city missing from the tipster gazetteer 
this poster motivates ai research in the area of real time strategy rts game and describes the current status of a project whose goal are to implement an rts game programming environment and to build ai that eventually can outperform human expert in this challenging and popular domain 
speculative execution of information gathering plan can dramatically reduce the effect of source i o latency on overall performance however the utility of speculation is closely tied to how accurately data value are predicted at runtime caching 
current tool for interactive knowledge capture have little or no learning aptitude they are mostly oblivious to the process or strategy that the user may be following in entering new knowledge unaware of their progress during a session and ignorant of typical skill expected from a good student we present an approach to make acquisition interface more proactive by extending them with goal that represent what remains to be learned strategy to achieve these goal and acquire further knowledge and awareness of the current status of the body of knowledge learned the resulting interaction show that the system is aware of it progress towards acquiring the new knowledge and move forward by understanding what acquisition goal and strategy to pursue 
speculative execution of information gathering plan can dramatically reduce the effect of source i o latency on overall performance however the utility of speculation is closely tied to how accurately data value are predicted at runtime caching 
to support more efficient video database management this paper explores the concept of video association mining with which the association pattern are characterized by sequentially associated video shot and their cluster information given a continuous video sequence v the video shot segmentation mechanism is first introduced to parse it into discrete shot we then cluster shot into visually distinct group and construct a shot cluster sequence by using the class label of each shot an association mining scheme is designed to mine sequentially associated cluster from the sequence those detected association will convey valuable knowledge for video database management the experimental result demonstrate the effectiveness of our design 
plan recognition ha traditionally been developed for logically encoded application domain with a focus on logical reasoning in this paper we present an integrated plan recognition model that combine low level sensory reading with high level goal inference a two level architecture is proposed to infer a user s goal in a complex indoor environment using an rf based wireless network the novelty of our work derives from our ability to infer a user s goal from sequence of signal trajectory and the ability for u to make a trade off between model accuracy and inference efficiency the model relies on a dynamic bayesian network to infer a user s action from raw signal and an n gram model to infer the user goal from action we present a method for constructing the model from the past data and demonstrate the effectiveness of our proposed solution through empirical study using some real data that we have collected 
most recent research of scalable inductive learning on very large dataset decision tree construction in particular focus on eliminating memory constraint and reducing the number of sequential data scan however state of the art decision tree construction algorithm still require multiple scan over the data set and use sophisticated control mechanism and data structure we first discus a general inductive learning framework that scan the dataset exactly once then we propose an extension based on hoeffding s inequality that scan the dataset le than once our framework are applicable to a wide range of inductive learner 
speculative execution of information gathering plan can dramatically reduce the effect of source i o latency on overall performance however the utility of speculation is closely tied to how accurately data value are predicted at runtime caching 
in this paper we study a logistics problem arising in military transport planning a military organization operates a large fleet of vehicle in a depot to serve the request of various operational unit each request ha a fixed start and end time and is served by a prescribed number of vehicle we address the following two problem how many vehicle are at least needed to meet a given service level of request and suppose we allow each request to shift it start time by a constant duration call all the request be met a niche genetic algorithm together with a hybridized variant are applied to the problem 
we present a coherent framework for data clustering starting with a hopfield network we show the solution for several well motivated clustering objective function are principal component for minmaxcut objective motivated for ensuring cluster balance the solution are the nonlinearly scaled principal component using scaled pc a we generalize to multi way clustering constructing a self aggregation network where connection weight between different cluster are automatically suppressed while connection weight within same cluster are automatically enhanced 
speculative execution of information gathering plan can dramatically reduce the effect of source i o latency on overall performance however the utility of speculation is closely tied to how accurately data value are predicted at runtime caching 
speculative execution of information gathering plan can dramatically reduce the effect of source i o latency on overall performance however the utility of speculation is closely tied to how accurately data value are predicted at runtime caching 
we are interested in enabling a generic sketch recognition system that would allow more natural interaction with design tool in various domain such a mechanical engineering military planning logic design etc we would like to teach the system the symbol for a particular domain by simply drawing an example of each one a easy a it is to teach a person study in cognitive science suggest that when shown a symbol people attend preferentially to certain geometric feature relying on such bias we built a system capable of learning description of hand drawn symbol from a single example the generalization power is derived from a qualitative vocabulary reflecting human perceptual category and a focus on perceptually relevant global property of the symbol our user study show that the system agrees with the subject majority classification about a often a any individual subject did 
we describe a computer system that play a responsive and sensitive accompaniment to a live musician in a piece of non improvised music the system of composed of three component listen anticipate and synthesize listen analyzes the soloist s acoustic signal and estimate note onset time using a hidden markov model synthesize play a prerecorded audio file back at variable rate using a phase vocoder anticipate creates a bayesian network that mediates between listen and synthesize the system ha a learning phase analogous to a series of rehearsal in which model parameter for the network are estimated from training data in performance the system synthesizes the musical score the training data and the on line analysis of the soloist s acoustic signal using a principled decision making engine based on the bayesian network a live demonstration will be given using the aria mi chiamano mimi from puccini s opera la boheme 
grid technology provide a robust infrastructure for distributed computing and are widely used in large scale scientific application that generate terabyte soon petabyte of data this data is described with metadata attribute about the data property and provenance and is organized in a variety of metadata catalog distributed over the grid in order to find a collection of data that share certain property these metadata catalog need to be identified and queried on an individual basis this paper introduces artemis a system developed to integrate distributed metadata catalog on the grid artemis exploit several ai technique including a query mediator a query planning and execution system ontology and semantic web tool to model metadata attribute and an intelligent user interface that guide user through these ontology to formulate query we describe our experience using artemis with large metadata catalog from two project in the physic domain 
for probabilistic reasoning one often need to sample from a combinatorial space for example one may need to sample uniformly from the space of all satisfying assignment can state of the art search procedure for sat be used to sample effectively from the space of all solution and if so how uniformly do they sample our initial result find that on the positive side one can find all solution to a given instance nevertheless sampling can be highly biased this research provides a starting point for the development of more balanced procedure 
speculative execution of information gathering plan can dramatically reduce the effect of source i o latency on overall performance however the utility of speculation is closely tied to how accurately data value are predicted at runtime caching 
we are going to present an implementation of an ai system cabma built on top of a commercial project management tool m project project planning is a business process for successfully delivering one of a kind product and service under real world time and resource constraint cabma for case based project management assistant provides the following functionality it capture case from project plan it reuses captured case to refine project plan and generate project plan from the scratch it maintains consistency of piece of a project plan obtained by case reuse it refines the case base to cope with inconsistency resulting from capturing case over a period of time cabma add a knowledge layer on top of m project to assist the user with his project management task 
multi level spatial aggregate are important for data mining in a variety of scientific and engineering application from analysis of weather data aggregating temperature and pressure data into ridge and front to performance analysis of wireless system aggregating simulation result into configuration space region exhibiting particular performance characteristic in many of these application data collection is expensive and time consuming so effort must be focused on gathering sample at location that will be most important for the analysis this requires that we be able to functionally model a data mining algorithm in order to ass the impact of potential sample on the mining of suitable spatial aggregate this paper describes a novel gaussian process approach to modeling multi layer spatial aggregation algorithm and demonstrates the ability of the resulting model to capture the essential underlying qualitative behavior of the algorithm by helping cast classical spatial aggregation algorithm in a rigorous quantitative framework the gaussian process model support diverse us such a directed sampling characterizing the sensitivity of a mining algorithm to particular parameter and understanding how variation in input data field percolate up through a spatial aggregation hierarchy 
predictive accuracy ha been used a the main and often only evaluation criterion for the predictive performance of classification learning algorithm in recent year the area under the roc receiver operating characteristic curve or simply auc ha been proposed a an alternative single number measure for evaluating learning algorithm in this paper we prove that auc is a better measure than accuracy more specifically we present rigourous definition on consistency and discriminancy in comparing two evaluation measure for learning algorithm we then present empirical evaluation and a formal proof to establish that auc is indeed statistically consistent and more discriminating than accuracy our result is quite significant since we formally prove that for the first time auc is a better measure than accuracy in the evaluation of learning algorithm 
we describe the problem of scheduling the television broadcast of the u s national football league nfl unlike traditional round robin tournament scheduling the nfl problem involves assigning game to broadcast slot under various complex constraint while attempting to satisfy a set of user preference a well a mixed initiative functionality wa required to allow the user to control and assist in the scheduling process a prototype system wa developed for the nfl which produced schedule satisfying many of these constraint and preference in this paper we provide an overview of the constraint solving methodology employed and the implementation of the nfl prototype system 
previous algorithm to compute lexical chain suffer either from a lack of accuracy in word sense disambiguation wsd or from computational inefficiency in this paper we present a new linear time algorithm for lexical chaining that adopts the assumption of one sense per discourse our result show an improvement over previous algorithm when evaluated on a wsd task 
in this paper we introduce the language golog htnti for specifying control using procedural and htn based construct together with deadline and time restriction our language start with feature from golog and htn and extends them so that we can deal with action with duration by being able to specify time interval between the start or end of an action or a program and the start or end of another action or program we then discus an off line interpreter based on the answer set planning paradigm such that the answer set of the logic program have a one to one correspondence with the trace of the golog htnti specification 
we present metric for measuring the similarity of state in a finite markov decision process mdp the formulation of our metric is based on the notion of bisimulation for mdps with an aim towards solving discounted infinite horizon reinforcement learning task such metric can be used to aggregate state a well a to better structure other value function approximators e g memory based or nearest neighbor approximators we provide bound that relate our metric distance to the optimal value of state in the given mdp 
in many real world planning scenario agent often do not have enough resource to achieve all of their goal consequently they are forced to find plan that satisfy only a subset of the goal solving such partial satisfaction planning psp problem pose several challenge including an increased emphasis on modeling and handling plan quality in term of action cost and goal utility despite the ubiquity of such psp problem very little attention ha been paid to them in the planning community in this paper we start by describing a spectrum of psp problem and focus on one of the more general psp problem termed psp net benefit we develop three technique i one based on integer programming called optiplan ii the second based on regression planning with reachability heuristic called altaltps and iii the third based on anytime heuristic search for a forward state space heuristic planner called sapaps our empirical study with these planner show that the heuristic planner generate plan that are comparable to the quality of plan generated by optiplan while incurring only a small fraction of the cost 
computing least common subsumers ic and most specific concept msc are inference task that can support the bottom up construction of knowledge base in description logic in description logic with existential restriction the most specific concept need not exist if one restricts the attention to concept description or acyclic tboxes in this paper we extend the notion le and msc to cyclic tboxes for the description logic ec which allows for conjunction existential restriction and the top concept we show that the le and msc always exist and can be computed in polynomial time if we interpret cyclic definition with greatest fixpoint semantics 
best first search is limited by the memory needed to store the open and closed list primarily to detect duplicate node magnetic disk provide vastly more storage but random access of a disk is extremely slow instead of checking generated node immediately against existing node in a hash table delayed duplicate detection ddd appends them to a file then periodically remove the duplicate node using only sequential disk access frontier search save storage in a best first search by storing only the open list and not the closed list the main contribution of this paper are to provide a scalable implementation of ddd to combine it with frontier search and to extend it to more general best first search such a a we illustrate these idea by performing complete breadth first search of sliding tile puzzle up to the fourteen puzzle for the peg tower of hanoi problem we perform complete search with up to disk searching a space of over a trillion node and discover a surprising anomaly concerning the problem space diameter of the and disk problem we also verify the presumed optimal solution length for up to disk in addition we implement a with ddd on the fifteen puzzle finally we present a scalable implementation of ddd based on hashing rather than sorting 
many real life optimization problem contain bothhard and soft constraint a well a qualitative conditionalpreferences however there is no singleformalism to specify all three kind of information 
formalism and axiomatic theory are designed to support reasoning they are often intended with a preferred interpretation and a targeted ontology question of proper interpretation and of the possible challenge of an intended interpretation arise when integrating a particular theory in pre existing formal and ontological setting this paper report on an instance of this general problem of ontological engineering the case study is that of the integration of the region connection calculus for spatial reasoning in the cyc knowledge base we show that given the assumption on the cyc ontology rcc had to be interpreted within a substantivalist metaphysic of space a a boolean algebra of spatial region which are distinct from their occupant the rcc literature suggests such an intended interpretation and this paper intends to show that this wa a necessary condition of integration in cyc s ontology this led to the enrichment of the cyc knowledge base rather than to a radical modification of the upper level ontology 
we consider a general model of stochastic discrete event system with asynchronous event and propose to develop efficient algorithm for verification and control of such system 
intelligent agent often need to ass user utility function in order to make decision on their behalf or predict their behavior when uncertainty exists over the precise nature of this utility function one can model this uncertainty using a distribution over utility function this view lie at the core of game with incomplete information and more recently several proposal for incremental preference elicitation in such case decision or predicted behavior are based on computing the expected expected utility eeu of decision with respect to the distribution over utility function unfortunately decision made under eeu are sensitive to the precise representation of the utility function we examine the condition under which eeu provides for sensible decision by appeal to the foundational axiom of decision theory we also discus the impact these condition have on the enterprise of preference elicitation more broadly 
we describe an innovative solution to the problem of scheduling astronomy observation for the stratospheric observatory for infrared astronomy an airborne observatory the problem contains complex constraint relating the feasibility of an astronomical observation to the position and time at which the observation begin telescope elevation limit and available fuel solving the problem requires making discrete choice e g selection and sequencing of observation and continuous one e g takeoff time and setting up observation by repositioning the aircraft the problem also includes optimization criterion such a maximizing observing time while simultaneously minimizing total flight time we describe a method to search for good flight plan that satisfy all constraint this novel approach combine heuristic search biased stochastic sampling continuous optimization technique and well founded approximation that eliminate feasible solution but greatly reduce computation time 
speculative execution of information gathering plan can dramatically reduce the effect of source i o latency on overall performance however the utility of speculation is closely tied to how accurately data value are predicted at runtime caching 
we have created ladder the first language to describe how sketched diagram in a domain are drawn displayed and edited the difficulty in creating such a language is choosing a set of predefined entity that is broad enough to support a wide range of domain while remaining narrow enough to be comprehensible the language consists of predefined shape constraint editing behavior and display method a well a a syntax for specifying a domain description sketch grammar and extending the language ensuring that shape and shape group from many domain can be described the language allows shape to be built hierarchically e g an arrow is built out of three line and includes the concept of abstract shape analogous to abstract class in an object oriented language shape group describe how multiple domain shape interact and can provide the sketch recognition system with information to be used in top down recognition shape group can also be used to describe chain reaction editing command that effect multiple shape at once to test that recognition is feasible using this language we have built a simple domain independent sketch recognition system that par the domain description and generates the code necessary to recognize the shape 
web intelligence wi shed new light on direction for scientific research and development which explores the fundamental role a well a practical impact of artificial intelligence ai and advanced information technology it on the next generation of web empowered product system service and activity this paper give new perspective on the future wi research and highlight some of the research challenge and initiative 
representing lexicon and sentence with the subsymbolic approach using technique such a self organizing map som or artificial neural network ann is a relatively new but important research area in natural language processing the performance of this approach however is highly dependent on whether representation are well formed so that member within each cluster are corresponding to sentence or phrase of similar meaning despite the moderate success and the rapid advancement of contemporary computing power it is still difficult to establish an efficient learning method so that natural language can be represented in a way close to the benchmark exhibited by human being one of the major problem is due to the general lack of effective method s to encapsulate semantic information into quantitative expression or structure in this paper we propose to alleviate this problem with a novel technique based on tensor product representation and non linear compression the method is capable of encoding sentence into distributed representation that are closely associated with the semantic content being more comprehensible and analyzable from the perspective of human intelligence 
we study the relationship among structural method for identifying and solving tractable class of constraint satisfaction problem csps in particular we first answer a long standing question about the notion of biconnected component applied to an optimal reduct of the dual constraint graph by showing that this notion is in fact equivalent to the hinge decomposition method then we give a precise characterization of the relationship between the treewidth notion applied to the hidden variable encoding of a csp and the same notion applied to some optimal reduct of the dual constraint graph finally we face the open problem of computing such an optimal reduct we provide an algorithm that output an approximation of an optimal tree decomposition and give a qualitative explanation of the difference between this graph based method and more general hypergraph based method 
many autonomous system such a mobile robot uavs or spacecraft have limited resource capacity and move in dynamic environment performing on board mission planning and execution in such a context requires deliberative capability to generate plan achieving mission goal while respecting deadline and resource constraint a well a run time plan adaption mechanism during execution in this paper we propose a framework to integrate deliberative planning plan repair and execution control in a dynamic environment with stringent temporal constraint it is based on lifted partial order temporal planning technique which produce flexible plan and allow under certain condition discussed in the paper plan repair interleaved with plan execution this framework ha been implemented using the ixtet planner and used to control a robotic platform 
the sub symbolic approach on natural language processing nlp is one of the mainstream in artificial intelligence indeed we have plenty of algorithm for variation of nlp such a syntactic structure representation or lexicon classification theoretically the goal of these research is obviously for developing a hybrid architecture which can process natural language a what human doe thus we propose an online intelligent system to extract the semantics utterance interpretation by applying a layer back propagation neural network to classify the encoded syntactic structure into corresponding semantic frame type e g agent action patient the result are generated dynamically according to training set and user input in webpage form it can diminish the manipulating time while using extra tool and share the statistical result with colleague in clear and standard form 
this paper describes the gerona knowledge ontology and the way it support spoken dialogue tutoring of crisis decision making skill gerona allows domain ontology model to be encoded and reasoned over this paper describes how it ha been used to encode three type of tutoring model that are relevant to simulator based training an expert model a student critiquing model and a question answer model these model provide scot dc a spoken conversational tutor with domain specific knowledge needed for reflective tutoring 
in this thesis we document the result of our investigation of four important problem in the domain of telecommunication network routing and traffic engineering the technique we use involve stochastic learning and learning automaton la our first contribution consists of two efficient solution for maintaining single source shortest path routing tree in network where the weight of the link connecting the node of the network change continuously in a random manner following an unknown stochastic distribution in the second problem we were interested in maintaining shortest path between all pair of node in a dynamically changing network again for this problem we have proposed two la based efficient solution our third contribution wa in the area of qos routing and traffic engineering in network more specifically we have proposed an adaptive online routing algorithm that computes bandwidth guaranteed path in mpls based network using a random race based learning scheme that computes an optimal ordering of route the last problem that we studied wa that of designing routing scheme that would successfully operate in the presence of adversarial environment in mobile ad hoc network manet the need for fault tolerant routing protocol for manet wa identified recently by xue and nahrstedt and in our research we have proposed a new fault tolerant routing scheme that us a stochastic learning based weak estimation procedure all our proposed solution have been demonstrated to be superior to the state of the art 
this paper present a new hybrid method for solving constraint optimization problem in anytime context discrete optimization problem are modelled a valued csp our method vns lds cp combine a variable neighborhood search and limited discrepancy search with constraint propagation to efficiently guide the search experiment on the celar benchmark demonstrate significant improvement over other competing method vns lds cp ha been successfully applied to solve a real life anytime resource allocation problem in computer network 
we present a major variant of the graphplan algorithm that employ available memory to transform the depth first nature of graphplan s search into an iterative state space view in which heuristic can be used to traverse the search space when the planner pegg is set to conduct exhaustive search it produce guaranteed optimal parallel plan to time faster than a version of graphplan enhanced with csp speedup method by heuristically pruning this search space pegg produce plan comparable to graphplan s in makespan at speed approaching state of the art heuristic serial planner 
a given entity representing a person a location or an organization may be mentioned in text in multiple ambiguous way understanding natural language requires identifying whether different mention of a name within and across document represent the same entity we present two machine learning approach to this problem which we call the robust reading problem our first approach is a discriminative approach trained in a supervised way our second approach is a generative model at the heart of which is a view on how document are generated and how name of different entity type are sprinkled into them in it most general form our model assumes a joint distribution over entity e g a document that mention president kennedy is more likely to mention oswald or white house than roger clemens an author model that assumes that at least one mention of an entity in a document is easily identifiable and then generates other mention via an appearance model governing how mention are transformed from tile representative mention we show that both approach perform very accurately in the range of f measure for different entity type much better than previous approach to some aspect of this problem our extensive experiment exhibit the contribution of relational and structural feature and somewhat surprisingly that the assumption made within our generative model are strong enough to yield a very powerful approach that performs better than a supervised approach with limited supervised information 
speculative execution of information gathering plan can dramatically reduce the effect of source i o latency on overall performance however the utility of speculation is closely tied to how accurately data value are predicted at runtime caching 
distance based method in pattern recognition and machine learning have to rely on a similarity or dissimilarity measure between pattern in the input space for many application euclidean distance in the input space is not a good choice and hence more complicated distance metric have to be used in this paper we propose a parametric method for metric learning based on class label information we first define a dissimilarity measure that can be proved to be metric it ha the favorable property that between class dissimilarity is always larger than within class dissimilarity we then perform parametric learning to find a regression mapping from the input space to a feature space such that the dissimilarity between pattern in the input space is approximated by the euclidean distance between point in the feature space parametric learning is performed using the iterative majorization algorithm experimental result on real world benchmark data set show that this approach is promising 
this paper illustrates how a multi agent system implement and governs a computational linguistic model of phonology for syllable recognition we describe how the time map model can be recast a a multi agent architecture and discus how constraint relaxation output extrapolation parse tree pruning clever task allocation and distributed processing are all achieved in this new architcture 
speculative execution of information gathering plan can dramatically reduce the effect of source i o latency on overall performance however the utility of speculation is closely tied to how accurately data value are predicted at runtime caching 
one challenge for developing spoken dialogue system in multiple domain is facilitating system component communication using a shared domain ontology since each domain come with it own set of concept and action relevant to the application adapting a system to a new domain requires customizing component to use the ontological representation required for that domain our research in multiple domain development ha highlighted difference in the ontological need of a generalpurpose language interface and a task specific reasoning application although different domain application have their own ontology many aspect of spoken dialogue interaction are common across domain in this paper we present a new method of customizing a broad coverage parser to different domain by maintaining two ontology one that is generalized for language representation and another that is customized to the domain and defining mapping between them in this way we preserve the broad coverage language component across domain a well a produce semantic representation that are optimally suited to the domain reasoner 
future manned space operation are expected to include a greater use of automation cooke and hine this automation will function without human intervention most of the time however human will be required to supervise the automation and they must be on call to respond to anomaly or to perform related task that are not easily automated in such an environment human perform other task most of the time and their interaction with the automation may be remote and asynchronous a automation becomes more prevalent better support for such interaction is needed the distributed collaboration and interaction dci environment being developed at nasa investigates the use of software agent to assist human in this type of remote distributed space operation the dci approach ha been applied for use by control engineer at the johnson space center jsc who are investigating advanced technology for life support such a the water recovery system or wrs schreckenghost et al the wrs recycles wastewater through biological and chemical process to remove impurity and produce potable water managed by an autonomous control program called t bonasso et al the wrs ran unattended in a continuous integrated test from january through april bonasso et al wrs control engineer periodically monitored for network hardware or power failure from remote location while spending the majority of their time carrying out their daily task on unrelated project the current prototype of the dci environment us a simulation of the wrs t system for both demonstration and continuing development the dci implementation creates an environment in which human and the t control automation together form an integrated team to ensure efficient effective operation of the wrs 
in this paper we discus the role of emotion in ai and possible way to determine their utility for the design of artificial agent we propose a research methodology for determining the utility of emotional control and apply it to the study of autonomous agent that compete for resource in an artificial life environment the result show that the emotional control can improve performance in some circumstance 
reactive planning using assumption is a well known approach to tackle complex planning problem for nondeterministic partially observable domain however assumption may be wrong this may cause an assumption based plan to fail in general it is not possible to decide at runtime whether an assumption ha failed and is putting at danger the success of the plan thus plan execution ha to be controlled taking into account every possible success endangering assumption failure the possibility of tracing such failure strongly depends on the action performed by the plan in this paper focusing on a simple assumption language we provide two main contribution first we formally characterize safe assumption based plan i e plan that not only succeed whenever the assumption hold but also guarantee that any success endangering assumption failure is traced by a suitable monitor in this way replanning may be triggered only when actually needed second we extend the planner in a reactive platform in order to produce safe assumption based plan we experimentally show that safe assumption based re planning is a good alternative to it unsafe counterpart minimizing the need for replanning while retaining the efficiency in plan generation 
speculative execution of information gathering plan can dramatically reduce the effect of source i o latency on overall performance however the utility of speculation is closely tied to how accurately data value are predicted at runtime caching 
we introduce anytime mechanism for distributed optimization with self interested agent anytime mechanism retain good incentive property even when interrupted before the optimal solution is computed and provide better quality solution when given additional time anytime mechanism can solve easy instance of a hard problem quickly and optimally while providing approximate solution on very hard instance in a particular instantiation growrange we successively expand the range of outcome considered computing the optimal solution for each range truth revelation remains a dominant strategy equilibrium with a stage based interruption and is a best response with high probability when the interruption is time based 
query answering over commonsense knowledge base typically employ a first order logic theorem prover while first order inference is intractable in general provers can often be hand tuned to answer query with reasonable performance in practice appealing to previous theoretical work on partition based reasoning we propose resolution based theorem proving strategy that exploit the structure of a kb to improve the efficiency of reasoning we analyze and experimentally evaluate these strategy with a testbed based on the snark theorem prover exploiting graph based partitioning algorithm we show how to compute a partition derived ordering for ordered resolution with good experimental result offering an automatic alternative to hand crafted ordering we further propose a new resolution strategy mf resolution that combine partition based message passing with focused sublanguage resolution our experiment show a significant reduction in the number of resolution step when this strategy is used finally we augment partition based message passing partition derived ordering and mf by combining them with the set of support restriction while these combination are incomplete they often produce dramatic improvement in practice this work present promising practical technique for query answering with large and potentially distributed commonsense kb 
because sequential auction have permeated society more than ever it is desirable for participant to have the optimal strategy beforehand however finding closed form solution to various sequential auction game is challenging current literature provides some answer for specific case but not for general case a decision support system that can automate optimal bid for player in different sequential auction game will be useful in solving these complex economic problem which requires not only economic but also computational efficiency this thesis contributes in several direction first this dissertation derives result related to the multiplicity of equilibrium in first price sealed bid fpsb auction and sequential fpsb auction with discrete bid under complete information it also provides theoretical result for fpsb auction with discrete bid under incomplete information these result are applicable to both two person and multi person case second this thesis develops a technique to compute strategy in sequential auction it applies monte carlo simulation to approximate perfect bayesian equilibrium for sequential auction with discrete bid and incomplete information it also utilizes the leveraged substructure of the game tree which can dramatically reduce the memory and computation time required to solve the game this approach is applicable to sequence of a wide variety of auction finally this thesis analyzes the impact of information in sequential auction with continuous bid and incomplete information when bid are revealed it provides theoretical result especially the non existence of pure strategy symmetric equilibrium in both the symmetric sequential fpsb and the symmetric sequential vickrey auction 
this paper proposes a dynamic model supporting multimodal state space probability distribution and present the application of the model in dealing with visual occlusion when tracking multiple object jointly for a set of hypothesis multiple measurement are acquired at each time instant the model switch among a set of hypothesized measurement during the propagation two computationally efficient filtering algorithm are derived for online joint tracking both the occlusion relationship and state of the object are recursively estimated from the history of measurement data the switching hypothesized measurement shm model is generally applicable to describe various dynamic process with multiple alternative measurement method 
in this paper we present an abstract framework for learning a finite domain constraint solver modeled by a set of operator enforcing a consistency the behavior of the consistency to be learned is taken a the set of example on which the learning process is applied the best possible expression of this operator in a given language is then searched we instantiate this framework to the learning of bound consistency in the indexical language of gnu prolog 
the evaluation of incomplete satisabilit y solver depends critically on the availability of hard satisable instance a plausible source of such instance consists of random ksat formula whose clause are chosen uniformly from among all clause satisfying some randomly chosen truth assignment a unfortunately instance generated in this manner tend to be relatively easy and can be solved ecien tly by practical heuristic roughly speaking for a number of dieren t algorithm a act a a stronger and stronger attractor a the formula s density increase motivated by recent result on the geometry of the space of satisfying truth assignment of random k sat and nae k sat formula we introduce a simple twist on this basic model which appears to dramatically increase it hardness namely in addition to forbidding the clause violated by the hidden assignment a we also forbid the clause violated by it complement so that both a and a are satisfying it appears that under this symmetrization the eects of the two attractor largely cancel out making it much harder for algorithm to nd any truth assignment we give theoretical and experimental evidence supporting this assertion 
this paper provides a logical framework for negotiation between agent that are assumed to be rational cooperative and truthful we present a characterisation of the permissible outcome of a process of negotiation in term of a set of rationality postulate a well a a method for constructing exactly the rational outcome the framework is extended by describing two mode of negotiation from which an outcome can be reached in the concessionary mode agent are required to weaken their demand in order to accommodate the demand of others in the adaptationist mode agent are required to adapt to the demand of others in some appropriate fashion both concession and adaptation are characterised in term of rationality postulate we also provide method for constructing exactly the rational concession a well a the rational adaptation the central result of the paper is the observation that the outcome obtained from the concessionary and adaptationist mode both correspond to the rational outcome we conclude by pointing out the link between negotiation and agm belief change and providing a glimpse of how this may be used to define a notion of preference based negotiation 
we introduce our work on the backdoor key a concept that show promise for characterizing problem hardness in backtracking search algorithm the general notion of backdoor wa recently introduced to explain the source of heavy tailed behavior in backtracking algorithm williams gomes selman a b we describe empirical study that show that the key faction i e the ratio of the key size to the corresponding backdoor size is a good predictor of problem hardness of ensemble and individual instance within an ensemble for structure domain with large key fraction 
there is controversy a to whether explicit support for pddl like axiom and derived predicate is needed for planner to handle real world domain effectively many researcher have deplored the lack of precise semantics for such axiom while others have argued that it might be best to compile them away we propose an adequate semantics for pddl axiom and show that they are an essential feature by proving that it is impossible to compile them away if we restrict the growth of plan and domain description to be polynomial these result suggest that adding a reasonable implementation to handle axiom inside the planner is beneficial for the performance our experiment confirm this suggestion 
increasing dialogue efficiency in case based reasoning cbr must be balanced against the risk of commitment to a sub optimal solution focusing on incremental query elicitation in recommender system we examine the limitation of naive strategy such a terminating the dialogue when the similarity of any case reach a predefined threshold we also identify necessary and sufficient condition for recommendation dialogue to be terminated without loss of solution quality finally we evaluate a number of attribute selection strategy in term of dialogue efficiency given the requirement that there must be no loss of solution quality 
simulation based training is increasingly being used within the military to practice and develop the skill of successful soldier for the skill associated with successful military leadership our inability to model human behavior to the necessary degree of fidelity in constructive simulation requires that new interactive design be developed the ict leader project support leadership development through the use of branching storyline realized within a virtual reality environment trainee assume a role in a fictional scenario where the decision that they make in this environment ultimately affect the success of a mission all trainee decision are made in the context of natural language conversation with virtual character the ict leader project advance a new form of interactive training by incorporating a suite of artificial intelligence technology including control architecture agent of mixed autonomy and natural language processing algorithm 
we consider the problem of updating nonmonotonic knowledge base represented by epistemic logic program where disjunctive information and notion of knowledge and belief can be explicitly expressed we propose a formulation for epistemic logic program update based on a principle called minimal change and maximal coherence the central feature of our approach is that during an update procedure contradictory information is removed on a basis of minimal change under the semantics of epistemic logic program and then coherent information is maximally retained in the update result by using our approach we can characterize an update result in both semantic and syntactic form we show that our approach handle update sequence and satisfies the consistency requirement we also investigate important semantic property of our update approach such a reduction persistence and preservation 
speculative execution of information gathering plan can dramatically reduce the effect of source i o latency on overall performance however the utility of speculation is closely tied to how accurately data value are predicted at runtime caching 
preference elicitation is a serious bottleneck in many decision support application and agent spec ification task cp net were designed to make the preference elicitation process simpler and more in tuitive for lay user by graphically structuring a set of ceteris paribus cp preference statement preference statement most people find natural and intuitive in various context cp net with an un derlying cyclic structure emerge naturally often they are inconsistent according to the current se mantics and the user is required to revise them in this paper we show how optimization query can be meaningfully answered in many inconsistent network without troubling the user with request for revision we also describe a method for focus ing user revision process when revision are truly needed in the process we provide a formal seman tic that justifies our approach and we introduce new technique for computing optimal outcome 
over constrained problem can have an exponential number of conflict which explain the failure and an exponential number of relaxation which restore the consistency a user of an interactive application however desire explanation and relaxation containing the most important constraint to address this need we define preferred explanation and relaxation based on user preference between constraint and we compute them by a generic method which work for arbitrary cp sat or dl solver we significantly accelerate the basic method by a divide and conquer strategy and thus provide the technological basis for the explanation facility of a principal industrial constraint programming tool which is for example used in numerous configuration application 
a formal framework for specifying and developing agent robot must handle not only knowledge and sensing action but also time and concurrency researcher have extended the situation calculus to handle knowledge and sensing action other researcher have addressed the issue of adding time and concurrent action here both of these feature are combined into a united logical theory of knowledge sensing time and concurrency the result preserve the solution to the frame problem of previous work maintains the distinction between indexical and objective knowledge of time and is capable of representing the various way in which concurrency interacts with time and knowledge furthermore a method based on regression is developed for solving the projection problem for theory specified in this version of the situation calculus 
for a robot the ability to get from one place to another is one of the most basic skill however locomotion on legged robot is a challenging multidimensional control problem this paper present a machine learning approach to legged locomotion with all training done on the physical robot the main contribution are a specification of our fully automated learning environment and a detailed empirical comparison of four different machine learning algorithm for learning quadrupedal locomotion the resulting learned walk is considerably faster than all previously reported hand coded walk for the same robot platform 
automated image interpretation is an important task with numerous application until recently designing such system required extensive subject matter and computer vision expertise resulting in poor cross domain portability and expensive maintenance recently a machine learned system adore wa successfully applied in an aerial image interpretation domain subsequently it wa re trained for another man made object recognition task in this paper we propose and implement several extension of adore addressing it primary limitation these extension enable the first successful application of this emerging ai technology to a natural image interpretation domain the resulting system is shown to be robust with respect to noise in the training data illumination and camera angle variation a well a competitively adaptive with respect to novel image 
unlike conventional data or text web page typically contain a large amount of information that is not part of the main content of the page e g banner ad navigation bar and copyright notice such irrelevant information which we call web page noise in web page can seriously harm web mining e g clustering and classification in this paper we propose a novel feature weighting technique to deal with web page noise to enhance web mining this method first build a compressed structure tree to capture the common structure and comparable block in a set of web page it then us an information based measure to evaluate the importance of each node in the compressed structure tree based on the tree and it node importance value our method assigns a weight to each word feature in it content block the resulting weight are used in web mining we evaluated the proposed technique with two web mining task web page clustering and web page classification experimental result show that our weighting method is able to dramatically improve the mining result 
super solution are solution in which if a small number of variable lose their value we are guaranteed to be able to repair the solution with only a few change in this paper we stress the need to extend the super solution framework along several dimension to make it more useful practically we demonstrate the usefulness of those extension on an example from jobshop scheduling an optimization problem solved through constraint satisfaction in such a case there is indeed a trade off between optimality and robustness however robustness may be increased without sacrificing optimality 
with the proliferation of heterogeneous device desktop computer personal digital assistant phone multimedia document must be played under various constraint small screen low bandwidth taking these constraint into account with current document model is impossible hence generic source document must be transformed into document compatible with the target context currently the design of transformation is left to programmer we propose here a semantic framework which account for multimedia document adaptation in very general term a model of a multimedia document is a potential execution of this document and a context defines a particular class of model the adaptation should then retain the source document model that belong to the class defined by the context if such model exist otherwise the adaptation should produce a document whose model belong to this class and are close to those of the source document we focus on the temporal dimension of multimedia document and show how adaptation can take advantage of temporal reasoning technique several metric are given for assessing the proximity of model 
this paper address agent intention a building block of imitation learning that abstract local situation of the agent and proposes a hierarchical hidden markov model hmm to represent cooperative behavior of teamwork the key of the proposed model is introduction of gate probability that restrict transition among agent intention according to others intention using these probability the framework can control transition flexibly among basic behavior in a cooperative behavior 
in this paper we consider the solution of scheduling problem that are inherently over subscribed in such problem there are always more task to execute within a given time frame than available resource capacity will allow and hence decision must be made about which task should be included in the schedule and which should be excluded we adopt a controlled iterative repair search approach and focus on improving the result of an initial priority driven solution generation procedure central to our approach is a new retraction heuristic termed max flexibility which is responsible for identifying which task to temporarily retract from the schedule for reassignment in an effort to incorporate additional task into the schedule the max flexibility heuristic chooses those task that have maximum flexibility for assignment within their feasible window we empirically evaluate the performance of max flexibility using problem data and the basic scheduling procedure from a fielded airlift mission scheduling application we show that it produce better improvement result than two contention based retraction heuristic including a variant of min conflict l minton et al with significantly le search and computational cost 
speculative execution of information gathering plan can dramatically reduce the effect of source i o latency on overall performance however the utility of speculation is closely tied to how accurately data value are predicted at runtime caching 
previous work in pruning algorithm for maxn multi player game tree ha produced shallow pruning and alpha beta branch and bound pruning the effectiveness of these algorithm is dependant a much on the range of terminal value found in the game tree a on the ordering of node we introduce last branch and speculative pruning technique which can prune any constantsum multi player game tree their effectiveness depends only on node ordering within the game tree a b grows large these algorithm will in the best case reduce the branching factor of a nplayer game from b to b n n in chinese checker these method reduce average expansion at depth from million to k node and in heart and spade they increase the average search depth by ply 
an interpretation system find the likely mapping from portion of an image to real world object an interpretation policy specifies when to apply which imaging operator to which portion of the image during every stage of interpretation earlier result compared a number of policy and demonstrated that policy that select operator which maximize the information gain per cost worked most effectively however those policy are myopic they rank the operator based only on their immediate reward this can lead to inferior overall result it may be better to use a relatively expensive operator first if that operator provides information that will significantly reduce the cost of the subsequent operator this suggests using some lookahead process to compute the quality for operator non myopically unfortunately this is prohibitively expensive for most domain especially for domain that have a large number of complex state we therefore use idea from reinforcement learning to compute the utility of each operator sequence in particular our system first us dynamic programming over abstract simplification of interpretation state to precompute the utility of each relevant sequence it doe this off line over a training sample of image at run time our interpretation system us these estimate to decide when to use which imaging operator our empirical result in the challenging real world domain of face recognition demonstrate that this approach work more effectively than myopic approach 
in constraint programming one model a problem by stating constraint on acceptable solution the constraint model is then usually solved by interleaving backtracking search and constraint propagation previous study have demonstrated that designing special purpose constraint propagator for commonly occurring constraint can significantly improve the efficiency of a constraintprogramming approach in this paper we present a fast simple algorithm for bound consistency propagation of the alldifferent constraint the algorithm ha the same worst case behavior a the previous best algorithm but is much faster in practice using a variety of benchmark and random problem we show that our algorithm outperforms existing bound consistency algorithm and also outperforms on problem with an easily identifiable property state ofthe art commercial implementationsof propagator for stronger form of local consistency 
speculative execution of information gathering plan can dramatically reduce the effect of source i o latency on overall performance however the utility of speculation is closely tied to how accurately data value are predicted at runtime caching 
it is a common practice that merchant selling product on the web ask their customer to review the product and associated service a e commerce is becoming more and more popular the number of customer review that a product receives grows rapidly for a popular product the number of review can be in hundred this make it difficult for a potential customer to read them in order to make a decision on whether to buy the product in this project we aim to summarize all the customer review of a product this summarization task is different from traditional text summarization because we are only interested in the specific feature of the product that customer have opinion on and also whether the opinion are positive or negative we do not summarize the review by selecting or rewriting a subset of the original sentence from the review to capture their main point a in the classic text summarization in this paper we only focus on mining opinion product feature that the reviewer have commented on a number of technique are presented to mine such feature our experimental result show that these technique are highly effective 
speculative execution of information gathering plan can dramatically reduce the effect of source i o latency on overall performance however the utility of speculation is closely tied to how accurately data value are predicted at runtime caching 
there ha been significant recent progress in reasoning and constraint processing method in area such a planning and finite model checking current solution technique can handle combinatorial problem with up to a million variable and five million constraint the good scaling behavior of these method appears to defy what one would expect based on a worst case complexity analysis in order to bridge this gap between theory and practice we propose a new framework for studying the complexity of these technique on practical problem instance in particular our approach incorporates general structural property observed in practical problem instance into the formal complexity analysis we introduce a notion of backdoor which are small set of variable that capture the overall combinatorics of the problem instance we provide empirical result showing the existence of such backdoor in real world problem we then present a series of complexity result that explain the good scaling behavior of current reasoning and constraint method observed on practical problem instance 
an agent with limited consumable execution resource need policy that attempt to achieve good performance while respecting these limitation otherwise an agent such a a plane might fail catastrophically crash when it run out of resource fuel at the wrong time in midair we present a new approach to constructing policy for agent with limited execution resource that build on principle of real time ai a well a research in constrained markov decision process specifically we formulate solve and analyze the policy optimization problem where constraint are imposed on the probability of exceeding the resource limit we describe and empirically evaluate our solution technique to show that it is computationally reasonable and that it generates policy that sacrifice some potential reward in order to make the kind of precise guarantee about the probability of resource overutilization that are crucial for mission critical application 
the seldon model combine concept from agent based modeling and social network analysis to create a computation model of social dynamic for terrorist recruitment the underlying recruitment model is based on a unique hybrid agent based architecture that contains simple agent individual such a expatriate and abstract agent conceptual entity such a society and mosque interaction between agent are determined by multiple social network which form and dissipate according to the action of the individual we have implemented a java based toolkit to evaluate the dynamic of social behavior and the specific dynamic associated with terrorist recruitment described by expert social scientist creating an architecture for simple adaptation to other group phenomenon 
the broadcast news navigator bnn is a fully implemented system that incorporates image speech and language processing together with visualization and user preference modeling to support intelligent personalized access to broadcast news video the demonstration will illustrate the use of the system s underlying machine learning enabled story segmentation and processing called the broadcast news editor bne a live scenario based demonstration will illustrate the use of named entity search temporal visualization of entity story clustering and geospatial story visualization discovery of entity relation and personalized multimedia summary generation by transforming access from sequential to direct search and providing hierarchical hyperlinked summary we will demonstrate how user can access topic and entity specific news cluster nearly three time a fast a direct search of digital video in short we will demonstrate intelligent news on demand enabled by a suite of ai technology 
speculative execution of information gathering plan can dramatically reduce the effect of source i o latency on overall performance however the utility of speculation is closely tied to how accurately data value are predicted at runtime caching 
combinatorial auction ca are important mechanism for allocating interrelated item unfortunately winner determination is np complete unless there is special structure we study the setting where there is a graph with some desired property with the item a vertex and every bid bid on a connected set of item two computational problem arise clearing the auction when given the item graph and constructing an item graph if one exists with the desired property wa previously solved for the case of a tree or a cycle and for the case of a line graph or a cycle we generalize the first result by showing that given an item graph with bounded treewidth the clearing problem can be solved in polynomial time and every ca instance ha some treewidth the complexity is exponential in only that parameter we then give an algorithm for constructing an item tree treewidth if such a tree exists thus closing a recognized open problem we show why this algorithm doe not work for treewidth greater than but leave open whether item graph of say treewidth can be constructed in polynomial time we show that finding the item graph with the fewest edge is np complete even when a graph of treewidth exists finally we study how the result change if a bid is allowed to have more than one connected component even for line graph we show that clearing is hard even with component and constructing the line graph is hard even with 
this paper present a multimodal learning system that can ground spoken name of object in their physical referent and learn to recognize those object simultaneously from naturally co occurring multisensory input there are two technical problem involved the correspondence problem in symbol grounding how to associate word symbol with their perceptually grounded meaning from multiple cooccurrences between word and object in the physical environment object learning how to recognize and categorize visual object we argue that those two problem can be fundamentally simplified by considering them in a general system and incorporating the spatio temporal and cross modal constraint of multimodal data the system collect egocentric data including image sequence a well a speech while user perform natural task it is able to automatically infer the meaning of object name from vision and categorize object based on teaching signal potentially encoded in speech the experimental result reported in this paper reveal the effectiveness of using multimodal data and integrating heterogeneous technique in machine learning natural language processing and computer vision 
single class classification scc seek to distinguish one class of data from the universal set of multiple class we present a new scc algorithm that efficiently computes an accurate boundary of the target class from positive and unlabeled data without labeled negative data 
we propose a framework for simple causal theory of action and study the computational complexity in it of various reasoning task such a determinism progression and regression under various assumption a it turned out even the simplest one among them one step temporal projection with complete initial state is intractable we also briefly consider an extension of the framework to allow truly indeterministic action and find that this extension doe not increase the complexity of any of the task considered here 
we present a product representation of belief space for planning under partial observability in earlier work we investigated backward plan construction based on a combination operation for belief state the main problem in explicit construction of belief state is their high number to remedy this problem we refrain from representing individual belief state explicitly and instead represent part of the belief space in a factored form the factorization is induced by the division of the state space to observational class each consisting of observationally indistinguishable state finally we show that the representation lead to a simple planning algorithm that is competitive with other algorithm for planning under partial observability 
many of standard practical technique of solving constraint satisfaction problem use various decomposition method to represent a problem a a combination of smaller one we study a general method of decomposing constraint satisfaction problem in which every constraint is represented a a disjunction of two or more simpler constraint defined possibly on smaller set of value we call a problem an amalgam if it can be decomposed in this way some particular case of this construction have been considered in cohen et al b a including amalgam of problem with disjoint set of value and amalgam of independent problem in this paper we concentrate on constraint class determined by relational clone and study amalgam of such class in the general case of arbitrary finite set of value we completely characterise amalgam of this form solvable in polynomial time and provide efficient algorithm 
speculative execution of information gathering plan can dramatically reduce the effect of source i o latency on overall performance however the utility of speculation is closely tied to how accurately data value are predicted at runtime caching 
we introduce a logical formalism of irreflexivc causal production relation that posse both a standard monotonic semantics and a natural non monotonic semantics the formalism is shown to provide a complete characterization for the causal reasoning behind causal theory from mccain and turner it is shown also that any causal re lation is reducible to it horn sub relation with re spect to the nonmonotonic semantics we describe also a general correspondence between causal re lations and abductive system which show in ef fect that causal relation allow to express abductive reasoning the result of the study seem to sug gest causal production relation a a viable general framework for nonmonotonic reasoning 
speculative execution of information gathering plan can dramatically reduce the effect of source i o latency on overall performance however the utility of speculation is closely tied to how accurately data value are predicted at runtime caching 
speculative execution of information gathering plan can dramatically reduce the effect of source i o latency on overall performance however the utility of speculation is closely tied to how accurately data value are predicted at runtime caching 
best first search is limited by the memory needed to store node in order to detect duplicate disk can greatly expand the amount of storage available but randomly accessing a disk is impractical rather than checking newly generated node a soon a they are generated we append them to a disk file then sort the file and finally scan the sorted file in one pas to detect and remove duplicate node this also speed up such search that fit entirely in memory by improving cache performance we implement this idea for breadth first search performing the first complete search of the sliding tile puzzle and the disk peg tower of hanoi puzzle 
speculative execution of information gathering plan can dramatically reduce the effect of source i o latency on overall performance however the utility of speculation is closely tied to how accurately data value are predicted at runtime caching 
building data integration system today is largely done by hand in a very labor intensive and error prone process in this paper we describe a conceptually new solution to this problem that of mass collaboration the basic idea is to think about a data integration system a having a finite set of parameter whose value must be set to build such a system the system administrator can construct and deploy a system shell then ask the user to help the system automatically converge to the correct parameter value this way the enourmous burden of system development is lifted from the administrator and spread thinly over a multitude of user we discus the challenge to this approach and propose solution we then describe our current effort in applying this approach to the problem of schema matching in the context of data integration 
in this paper we present a general technique for taking forward chaining planner for deterministic domain e g hsp tlplan talplanner and shop and adapting them to work in nondeterministic domain our result suggest that our technique preserve many of the desirable property of these planner such a the ability to use heuristic technique to achieve highly ecien t planning in our experimental study on two problem domain the well known mbp algorithm took exponential time conrming prior result by others a nondeterminized version of shop took only polynomial time the polynomial time gures are conrmed by a complexity analysis and a similar complexity analysis show that a nondeterminized version of tlplan would perform similarly 
patient scheduling in hospital is a highly complex task hospital have a distributed organisational structure being divided into several autonomous ward and ancillary unit moreover the treat ment process is dynamic information about the patient disease often varies during treatment causing change in the treatment process current approach are insufficient because they either fo cu only on the single ancillary unit and therefore do not consider the entire treatment process of the patient or they do not account for the distribution and dynamic of the patient scheduling problem therefore we propose an agent based approach in which the patient and hospital resource are mod elled a autonomous agent with their own goal reflecting the decentralised structure in hospital in this multi agent system the patient agent com pete over the scarce hospital resource moreover to improve the overall solution the agent then ne gotiate with one another to this end a market mechanism is described in which each self inter ested agent try to improve it own situation in particular we focus on how the agent can calculate demand and supply price based upon their current schedule further an evaluation of first result of the proposed method is given 
we present a general framework for studying heuristic for planning in the belief space earlier work ha focused on giving implementation of heuristic that work well on benchmark without studying them at a more analytical level existing heuristic have evaluated belief state in term of their cardinality or have used distance heuristic directly based on the distance in the underlying state space neither of these type of heuristic is very widely applicable often goal belief state is not approached through a sequence of belief state with a decreasing cardinality and distance in the state space ignore the main implication of partial observability to remedy these problem we present a family of admissible increasingly accurate distance heuristic for planning in the belief space parameterized by an integer n we show that the family of heuristic is theoretically robust it includes the simplest heuristic based on the state space a a special case and a a limit the exact distance in the belief space 
the structural rigidity property a generalization of laman s theorem which characterizes rigid bar framework in d is generally considered a good approximation of rigidity in geometric constraint satisfaction problem gcsps however it may fail even on simple gcsps because it doe not take geometric property into account in this paper we question the flow based algorithm used by hoffmann et al to identify rigid subgcsps we show that this algorithm may fail because of the structural rigidity but also by design we introduce a new flow based algorithm which us jermann et al s characterization of rigidity we show that this algorithm is correct in d and d and can be used to tackle the major issue related to rigidity deciding whether a gcsp is rigid or not and identifying rigid or over rigid subgcsps 
goal recognition for dialogue system need to be fast make early prediction and be portable we present initial work which show that using statistical corpus based method to build goal recognizers may be a viable way to meet those need our goal recognizer is trained on data from apian corpus and then used to determine the agent s most likely goal based on that data the algorithm is linear in the number of goal and performs very well in term of accuracy and early prediction in addition it is more easily portable to new domain a doe not require a hand crafted plan library 
we address the task of problem determination in a distributed system using probe or test transaction which gather information about system component effective probing requires minimizing the cost of probing while maximizing the diagnostic accuracy of the probe set we show that pre planning an optimal probe set is np hard and present polynomial time approximation algorithm that perform well we then implement an active probing strategy which selects probe dynamically and show that it yield a significant reduction in probe set size in both simulation and a real system environment 
ensemble method like bagging and boosting that combine the decision of multiple hypothesis are some of the strongest existing machine learning method the diversity of the member of an ensemble is known to be an important factor in determining it generalization error this paper present a new method for generating ensemble that directly construct diverse hypothesis using additional artificially constructed training example the technique is a simple general metalearner that can use any strong learner a a base classifier to build diverse committee experimental result using decision tree induction a a base learner demonstrate that this approach consistently achieves higher predictive accuracy than both the base classifier and bagging whereas boosting can occasionally decrease accuracy and also obtains higher accuracy than boosting early in the learning curve when training data is limited 
this paper compare alternative approach to pose estimation using visual cue from the environment we examine approach that derive pose estimate from global image property such a principal component analysis pca versus from local image property commonly referred to a landmark we also consider the failure mode of the different method our work is validated with experimental result 
this article present a high level discussion of some problem in information retrieval that are unique to web search engine the goal is to raise awareness and stimulate research in these area 
speculative execution of information gathering plan can dramatically reduce the effect of source i o latency on overall performance however the utility of speculation is closely tied to how accurately data value are predicted at runtime caching 
refinement operator for theory avoid the problem related to the myopia of many relational learning algorithm based on the operator that refine single clause however the non existence of ideal refinement operator ha been proven for the standard clausal search space based on subsumption or logical implication which scale up to the space of theory by adopting different generalization model constrained by the assumption of object identity we extend the theoretical result on the existence of ideal refinement operator for space of clause to the case of space of theory 
speculative execution of information gathering plan can dramatically reduce the effect of source i o latency on overall performance however the utility of speculation is closely tied to how accurately data value are predicted at runtime caching 
information extraction can be defined a the task of automatically extracting instance of specified class or relation from text we consider the case of using machine learning method to induce model for extracting relation instance from biomedical article we propose and evaluate an approach that is based on using hierarchical hidden markov model to represent the grammatical structure of the sentence being processed our approach first us a shallow parser to construct a multi level representation of each sentence being processed then we train hierarchical hmms to capture the regularity of the par for both positive and negative sentence we evaluate our method by inducing model to extract binary relation in three biomedical domain our experiment indicate that our approach result in more accurate model than several baseline hmm approach 
the intelligence analysis task of anticipating crisis and providing decision maker with reasonable supportable explainable possible future is extremely difficult to perform this task a team of analyst must consider the political economic and military aspect of national government and non governmental organization this paper describes an agent based simulation framework the advanced global intelligence and leadership experiment agile for building executable model for conducting regional analysis agile is a full featured simulation framework that enables the specification of simulation parameter model and agent let the user define and run simulation under varying condition and enables post run analysis of the result in this paper we describe the system implementation and some example of it use in a pseudo operational setting 
searching for parallel solution in state space planner is a challenging problem because it would require the planner to branch on all possible subset of parallel action exponentially increasing their branching factor we introduce a variant of our heuristic state search planner altalt which generates parallel plan by using greedy online parallelization of partial plan empirical result show that our online approach outperforms post processing offline technique in term of the quality of the solution returned 
speculative execution of information gathering plan can dramatically reduce the effect of source i o latency on overall performance however the utility of speculation is closely tied to how accurately data value are predicted at runtime caching 
algorithm for r elational l earning and p ropositional l earning face different statistical challenge in contrast to propositional learner relational l earner often make statistical i nferences about data that exhibit linkage and autocorrelation recent work h a s hown that t hese c haracteristics of relational data ca n bias inference made by relational l earner in this paper we develop a novel variant of a known statistical procedure a randomization test that produce accurate hypothesis test for r elational data we show that our procedure produce unbiased inference in situation where more obvious adaptation of existing randomization test fail 
abstract 
we have produced an ontology specifying a model of computer attack our ontology is based upon an analysis of over class of computer intrusion and their corresponding attack strategy and is categorized according to system component targeted mean of attack consequence of attack and location of attacker we argue that any taxonomic characteristic used to define a computer attack be limited in scope to those feature that are observable and measurable at the target of the attack we present our model a a target centric ontology that is to be refined and expanded over time we state the benefit of forgoing dependence upon taxonomy in favor of ontology for the classification of computer attack and intrusion we have specified our ontology using daml oil and have prototyped it using damljesskb we present our model a a target centric ontology and illustrate the benefit of utilizing an ontology in lieu of a taxonomy by presenting a use case scenario of a distributed intrusion detection system 
collaborative filtering is a popular technique for recommending item to people several method for collaborative filtering have been proposed in the literature and the quality of their prediction compared in empirical study in this paper we argue that the measure of quality used in these study are based on rather simple assumption we propose and apply additional measure for comparing the effectiveness of collaborative filtering method which are grounded in decision theory 
this research concern the comparison of three different artificial evolution approach to the design of cooperative behavior in a group of simulated mobile robot the first and second approach termed single pool and plasticity are characterized by robot that share a single genotype though the plasticity approach includes a learning mechanism the third approach termed multiple pool is characterized by robot that use different genotype the application domain implement a pursuit evasion game in which team of robot of various size termed predator collectively work to capture either one or two others termed prey these artificial evolution approach are also compared with a static rule based cooperative pursuit strategy specified a priori result indicate that the multiple pool approach is superior comparative to the other approach in term of measure defined for prey capture strategy performance that is this approach facilitated specialization of behavioral role allowing it to be effective for all predator team size tested 
we introduce mcp net an extension of the cp net formalism to model and handle the qualitative and conditional preference of multiple agent we give a number of different semantics for reasoning with mcp net the semantics are all based on the idea of individual agent voting we describe how to test optimality and preference ordering within a mcp net and we give complexity result for such task we also discus whether the voting scheme fairly combine together the preference s of the individual agent 
speculative execution of information gathering plan can dramatically reduce the effect of source i o latency on overall performance however the utility of speculation is closely tied to how accurately data value are predicted at runtime caching 
pro active agent typically have multiple simultaneous goal these may interact with each other both positively and negatively in this paper we provide a mechanism allowing agent to detect and avoid a particular kind of negative interaction where the effect of one goal undo condition that must be protected for successful pursuit of another goal in order to detect such interaction we maintain summary information about the definite and potential conditional requirement and resulting effect of goal and their associated plan we use these summary to guard protected condition by scheduling the execution of goal and plan step the algorithm and data structure developed allow agent to act rationally instead of blindly pursuing goal that will conflict 
default logic is used to describe regular behavior and normal property we suggest to exploit the framework of default logic for detecting outlier individual who behave in an unexpected way or feature abnormal property the ability to locate outlier can help to maintain knowledge base integrity and to single out irregular individual we first formally define the notion of an outlier and an outlier witness we then show that finding outlier is quite complex indeed we show that several version of the outlier detection problem lie over the second level of the polynomial hierarchy for example the question of establishing if at least one outlier can be detected in a given propositional default theory is p complete although outlier detection involves heavy computation the query involved can frequently be executed offline thus somewhat alleviating the difficulty of the problem in addition we show that outlier detection can be done in polynomial time for both the class of acyclic normal unary default and the class of acyclic dual normal unary default 
finding desired information on the internet is becoming increasingly difficult internet directory such a yahoo which organize web page into hierarchical category provide one solution to this problem however such directory are of limited use because some bias is applied both in the collection and categorization of page we propose a method for integrating multiple internet directory by instance based learning our method provides the mapping of category in order to transfer document from one directory to another instead of simply merging two directory into one we present herein an effective algorithm for determining similar category between two directory via a statistical method called the k statistic in order to evaluate the proposed method we conducted experiment using two actual internet directory yahoo and google the result show that the proposed method achieves extensive improvement relative to both the naive bayes and enhanced naive bayes approach without any text analysis on document 
fages showed that if a program is tight then every propositional model of it completion is also it stable model recently babovich erdem and lifschitz generalized fages result and showed that this is also true if the program is tight on the given model of the completion a it turned out this is quite a general result among the commonly known benchmark domain only niemelii s normal logic program encoding of the hamiltonian circuit hc problem doe not have this property in this paper we propose a new normal logic program for solving the hc problem and show that the program is tight on every model of it completion experimental result showed that for many graph this new encoding improves the performance of both smodels and assat chaff especially of the latter system which is based on the sat solver chaff we also propose a notion of inherently tight logic program and show that for any program it is inherently tight iff all it completion model are stable model we then propose a polynomial transformation from a logic program to one that is inherently tight thus providing a reduction of stable model semantics to program completion semantics and sat 
the simultaneous localization and mapping problem is a fundamental problem in mobile robotics while a robot navigates in an unknown environment it must incrementally build a map of it surroundings and localize itself within that map traditional approach to the problem are based upon kalman filter but suffer from complexity issue first the belief state grows quadratically in the size of the map and second the filtering operation can take time quadratic in the size of the map i present a linear space filter that maintains a tractable approximation of the belief state a a thin junction tree the junction tree grows under measurement and motion update and is periodically thinned to remain tractable the time complexity of the filter operation is linear in the size of the map i also present simple enhancement that permit constant time approximate filtering 
we consider how much influence a center can exert on a game if it only power is to propose contract to the agent before the original game and enforce the contract after the game if all agent sign it modelling the situation a an extensive form game we note that the outcome that are enforceable are precisely those in which the payoff to each agent is higher than it payoff in at least one of the nash equilibrium of the original game we then show that these outcome can still be achieved without any effort actually expended by the center we propose a mechanism in which the center doe not monitor the game and the contract are written so that in equilibrium all agent sign and obey the contract with no need for center intervention 
in previous work levesque proposed an extension to classical database that would allow for a certain form of incomplete first order knowledge since this extension wa sufficient to make full logical deduction undecidable he also proposed an alternative reasoning scheme with desirable logical property he also claimed without proof that this reasoning could be implemented efficiently using database technique such a projection and join in this paper we substantiate this claim and show how to adapt a bottom up database query evaluation algorithm for this purpose thus obtaining a tractability result comparable to those that exist for database 
we present a description of three different algorithm that use background knowledge to improve text classifier one us the background knowledge a an index into the set of training example the second method us background knowledge to reexpress the training example the last method treat piece of background knowledge a unlabeled example and actually classifies them the choice of background knowledge affect each method s performance and we discus which type of background knowledge is most useful for each specific method 
graphplan planning graph are structure widely used in modern planner the exclusion relation calculated in the planning graph extension provide very useful information especially in temporal planning where action have different duration however graphplan backward search ha some inefficiency that impose limitation when dealing with large temporal problem this paper present a new search process for temporal planning to avoid these inefficiency this search us the information of a planning graph and show beneficial in the scalability of the planner moreover our experiment show that a planner with this new search is competitive with other state of the art planner w r t the plan quality 
random decision tree is an ensemble of decision tree the feature at any node of a tree in the ensemble is chosen randomly from remaining feature a chosen discrete feature on a decision path cannot be chosen again continuous feature can be chosen multiple time however with a different splitting value each time during classification each tree output raw posterior probability the probability from each tree in the ensemble are averaged a the final posterior probability estimate although remarkably simple and somehow counter intuitive random decision tree ha been shown to be highly accurate under loss and cost sensitive loss function preliminary explanation of it high accuracy is due to the error tolerance property of probabilistic decision making our study ha shown that the actual reason for random tree s superior performance is due to it optimal approximation to each example s true probability to be a member of a given class 
a longstanding goal in planning research is the ability to generalize plan developed for some set of environment to a new but similar environment with minimal or no replanning such generalization can both reduce planning time and allow u to tackle larger domain than the one tractable for direct planning in this paper we present an approach to the generalization problem based on a new framework of relational markov decision process rmdps an rmdp can model a set of similar environment by representing object a instance of different class in order to generalize plan to multiple environment we define an approximate value function specified in term of class of object and in a multiagent setting by class of agent this class based approximate value function is optimized relative to a sampled subset of environment and computed using an efficient linear programming method we prove that a polynomial number of sampled environment suffices to achieve performance close to the performance achievable when optimizing over the entire space our experimental result show that our method generalizes plan successfully to new significantly larger environment with minimal loss of performance relative to environment specific planning we demonstrate our approach on a real strategic computer war game 
there ha been a recent swell of interest in the automatic identification and extraction of opinion and emotion in text in this paper we present the first experimental result classifying the strength of opinion and other type of subjectivity and classifying the subjectivity of deeply nested clause we use a wide range of feature including new syntactic feature developed for opinion recognition in fold cross validation experiment using support vector regression we achieve improvement in mean squared error over baseline ranging from to 
negotiation event in industrial procurement involving multiple highly customisable good pose serious challenge to buying agent when trying to determine the best set of providing agent offer typically a buying agent s decision involves a large variety of constraint that may involve attribute of a very same item a well a attribute of multiple item in this paper we describe ibundler an agentaware negotiation service to solve the winner determination problem considering buyer and provider constraint and preference 
reiter s variant of the situation calculus is tightly related to relational database when complete information on the initial situation is available in particular the information on the initial situation can be seen a a relational database and action a specified by the precondition and successor state axiom can be seen a operation that change the state of the database in this paper we show how to exploit such a correspondence to build system for reasoning about action based on standard rdational database technology indeed by exploiting standlrd relational dbms service a system may be able to perform both projection exploiting dbms querying service and progression exploiting dbms update service in very large action theory a key result towards such a realization is that under very natural condition reiter s basic action theory turn out to be made of safe formula where basically negation is used a a form of difference between predicate only and that regression and progression preserve such a safeness this is a fundamental property to efficiently exploit relational database technology for reasoning we then show that even when action theory are not safe they can be made so while trying to retain efficiency a much a possible finally we briefly discus how such result can be extended to certain form of incomplete information 
we describe a formal framework for diagnosis and repair problem that share element of the well known partially observable mop and cost sensitive classification model our cost sensitive fault remediation model is amenable to implementation a a reinforcement learning system and we describe an instance based state representation that is compatible with learning and planning in this framework we demonstrate a system that us these idea to learn to efficiently restore network connectivity after a failure 
this paper summarizes a probabilistic approach for localizing people through the signal strength of a wireless ieee lb network our approach us data labeled by ground truth position to learn a probabilistic mapping from location to wireless signal represented by piecewise linear gaussians it then us sequence of wireless signal data without position label to acquire motion model of individual people which further improves the localization accuracy the approach ha been implemented and evaluated in an office environment 
we are designing and developing a mobile clinical decision support system known a met mobile emergency triage for supporting emergency triage of different type of acute pain presentation met need to interact with an existing hospital information system run on handheld computing device and be suitable for operation in weak connectivity condition with unstable connection between mobile client and a server the met system capture necessary hospital data allows for patient data entry and provides triage support by operating on handheld computer it fit to the regular clinical workflow without introducing any hindrance and disruption it support triage anytime and anywhere directly at the point of care and can be used a an electronic patient chart that facilitates structured data collection 
we will demonstrate the secure wireless agent testbed swat a unique facility developed at drexel university to study integration networking and information assurance for next generation wireless mobile agent system swat is an implemented system that fully integrates mobile agent wireless ad hoc multi hop network and security the demonstration will show the functionality of a number of decentralized agent based application including application for authentication collaboration messaging and remote sensor monitoring the demonstration will take place on a live mobile ad hoc network consisting of approximately a dozen node pda tablet pc and laptop and hundred of mobile software agent 
traditional single agent search algorithm usually make simplifying assumption single search agent stationary target complete knowledge of the state and sufficient time there are algorithm for relaxing one or two of these constraint in this paper we want to relax all four the application domain is to have multiple search agent cooperate to pursue and capture a moving target agent are allowed to communicate with each other for solving multiple agent moving target mamt application we present a framework for specifying a family of suitable search algorithm this paper investigates several effective approach for solving problem instance in this domain 
speculative execution of information gathering plan can dramatically reduce the effect of source i o latency on overall performance however the utility of speculation is closely tied to how accurately data value are predicted at runtime caching 
this article describes preliminary work on a research environment called virtual synergy to represent a shared virtual map of an area for multiple autonomous robot by modifying the gamebots d multi agent system the use of gamebots will allow multiple user to interact with robot and agent at different level of adjustable or dynamic autonomy by interacting with the robot a another team member the user take on different role to suit the situation supervisor peer mechanic teleoperator and spectator 
given a set of number and a set of bin of fixed capacity the np complete problem of bin packing is to find the minimum number of bin needed to contain the number such that the sum of the number assigned to each bin doe not exceed the bin capacity we present two improvement to our previous bin completion algorithm the first speed up the constant factor per node generation and the second prune redundant part of the search tree the resulting algorithm appears to be asymptotically faster than our original algorithm on problem with element it run over time faster furthermore the ratio of node generation and running time both increase with increasing problem size 
tabu search algorithm are amongst the most successful local search based method for the maximum satisfiability problem the practical superiority of tabu search over the local search alone lias been already shown experimentally several time a natural question addressed here is to understand if this superiority hold also from the worst case point of view moreover it is well known that the main critical parameter of tabu technique is the tabu list length focussing on max sat problem the main contribution of this paper is a worst case analysis of tabu search a a function of the tabu list length we give a first theoretical evidence of the advantage of a tabu search strategy over the basic local search alone that critically depends on the tabu list length 
we have recently proposed augmenting clause in a boolean database with group of permutation the augmented clause then standing for the set of all clause constructed by acting on the original clause with a permutation in the group this approach ha many attractive theoretical property including representational generality and reduction from exponential to polynomial proof length in a variety of setting in this paper we discus the issue that arise in implementing a group based generalization of resolution and give preliminary result describing this procedure s effectiveness 
we study the recognized open problem of designing revenue maximizing combinatorial auction it is unsolved even for two bidder and two item for sale rather than pursuing the pure economic approach of attempting to characterize the optimal auction we explore technique for automatically modifying existing mechanism in a way that increase expected revenue we introduce a general family of auction based on bidder weighting and allocation boosting which we call virtual valuation combinatorial auction vvca all auction in the family are based on the vickrey clarke grove vcg mechanism executed on virtual valuation that are linear transformation of the bidder real valuation the restriction to linear transformation is motivated by incentive compatibility the auction family is parameterized by the coefficient in the linear transformation the problem of designing a high revenue mechanism is therefore reduced to search in the parameter space of vvca we analyze the complexity of the search for the optimal such mechanism and conclude that the search problem is computationally hard despite that optimal parameter for vvca can be found at least in setting with few item and bidder the experiment show that vvca yield a substantial increase in revenue over the traditionally used vcg in larger auction locally optimal parameter which still yield an improvement over vcg can be found 
speculative execution of information gathering plan can dramatically reduce the effect of source i o latency on overall performance however the utility of speculation is closely tied to how accurately data value are predicted at runtime caching 
this paper introduces the point based value iteration pbvi algorithm for pomdp planning pbvi approximates an exact value iteration solution by selecting a small set of representative belief point and then tracking the value and it derivative for those point only by using stochastic trajectory to choose belief point and by maintaining only one value hyper plane per point pbvi successfully solves large problem we present result on a robotic laser tag problem a well a three test domain from the literature 
this paper present a machine learning approach to modeling human behavior in one shot game it provides a framework for representing and reasoning about the social factor that affect people s play the model predicts how a human player is likely to react to different action of another player and these prediction are used to determine the best possible strategy for that player data collection and evaluation of the model were performed on a negotiation game in which human played against each other and against computer model playing various strategy a computer player trained on human data outplayed nash equilibrium and nash bargaining computer player a well a human it also generalized to play people and game situation it had not seen before 
minesweeper is a one person game which look deceptively easy to play but where average human performance is far from optimal playing the game requires logical arithmetic and probabilistic reasoning based on spatial relationship on the board simply checking a board state for consistency is an np complete problem given the difficulty of hand crafting strategy to play this and other game ai researcher have always been interested in automatically learning such strategy from experience in this paper we show that when integrating certain technique into a general purpose learning system mio the resulting system is capable of inducing a minesweeper playing strategy that beat the winning rate of average human player in addition we discus the necessary background knowledge present experimental result demonstrating the gain obtained with our technique and show the strategy learned for the game 
cyclic definition in description logic have until now been investigated only for description logic allowing for value restriction even for the most basic language fl which allows for conjunction and value restriction only deciding subsumption in the presence of terminological cycle is a pspace complete problem this paper investigates subsumption in the presence of terminological cycle for the language el which allows for conjunction existential restriction and the topconcept in contrast to the result for fl subsumption in el remains polynomial independent of whether we use least fixpoint semantics greatest fixpoint semantics or descriptive semantics 
the iterative closest point icp algorithm is a popular method for modeling d object from range data the classical icp algorithm rest on a rigid surface assumption building on recent work on nonrigid object model this paper present an icp algorithm capable of modeling nonrigid object where individual scan may be subject to local deformation we describe an integrated mathematical framework for simultaneously registering scan and recovering the surface configuration to tackle the resulting high dimensional optimization problem we introduce a hierarchical method that first match a coarse skeleton of scan point then adapts local scan patch the approach is implemented for a mobile robot capable of acquiring d model of object 
coordinator are coordination manager for fielded first responder each first response team is paired with a coordinator coordination manager which is running on a mobile computing device coordinator provide decision support to first response team by reasoning about who should be doing what when with what resource in support of which other team and so forth coordinator respond to the dynamic of the environment by re coordinating to determine the right course of action for the current circumstance coordinator have been implemented using wireless pda and proprietary first responder location tracking technology this paper describes coordinator the motivation for them the underlying agent architecture implementation issue and first response exercise 
noa is an agent architecture that support the development of agent motivated by norm obligation permission and prohibition obligation motivate a normative agent to act a motive to achieve a state of affair or to perform some action prohibition restrict an agent s behaviour whereas permission allow an agent to pursue certain activity to test the architecture noa agent arc applied to automated business transaction scenario where the correct execution of contract is paramount to create a situation of trust 
this paper present a logical framework for negotiation based on belief revision theory we consider that a negotiation process is a course or multiple course of mutual belief revision a set of agm style postulate are proposed to capture the rationality of competitive and cooperative behavior of negotiation we first show that the agm revision and it iterated extension is a special case of negotiation function then we show that a negotiation function can be constructed by two related iterated belief revision function under a certain coordination mechanism this provides a qualitative method for constructing negotiation space and rational concession it also show glimpse of how to express game theoretical concept in logical framework 
sequence alignment is an important problem in computational biology we compare two different approach to the problem of optimally aligning two or more character string bounded dynamic programming bdp and divide and conquer frontier search dcfs the approach are compared in term of time and space requirement in through dimension with sequence of varying similarity and length while bdp performs better in two and three dimension it consumes more time and memory than dcfs for higher dimensional problem 
speculative execution of information gathering plan can dramatically reduce the effect of source i o latency on overall performance however the utility of speculation is closely tied to how accurately data value are predicted at runtime caching 
biomedical ontology are typically structured in a biaxial way reflecting both a taxonomic and a mereological order common example such a the gene ontology and the unified medical language system umls excel in term of coverage but lack an adequate semantics of the mereological relation they incorporate this shortcoming is particularly evident a far a the non mandatory existence of part for their whole is collcerned on the one hand and the propagation of property acmss part whole hierarchy on the other hand we provide a formal specification of the semantic foundation of mereology in the biomedical domain that is closely linked to the paradigm of description logic in essence we here propose to emulate mereological reasoning by taxonomic reasoning in an attempt to capture much of the shared intuition underlying merelogical reasoning in the biomedical domain we distinguish for each mereologically relevant concept four different class of part and whole which allow for the expression of five different propagation pattern 
we extend multiclass svm to multiple prototype per class for this framework we give a compact constrained quadratic problem and we suggest an efficient algorithm for it optimization that guarantee a local minimum of the objective function an annealed process is also proposed that help to escape from local minimum finally we report experiment where the performance obtained using linear model is almost comparable to that obtained by state of art kernel based method but with a significant reduction of one or two order in response time 
ensemble technique such a bagging and decorate exploit the instability of learner such a decision tree to create a diverse set of model however creating a diverse set of model for stable learner such a na ve bayes is difficult a they are relatively insensitive to training data change furthermore many popular ensemble technique do not have a rigorous underlying theory and often provide no insight into how many model to build we formally define stable learner a having a second order derivative of the posterior density function and propose an ensemble technique specifically for stable learner our ensemble technique bootstrap model averaging creates a number of bootstrap sample from the training data build a model from each and then sum the joint instance and class probability over all model built we show that for stable learner our ensemble technique for infinite bootstrap sample approximates posterior model averaging aka the optimal bayes classifier obc for finite bootstrap sample we estimate the increase over the abc error using chebychev bound we empirically illustrate our approach s usefulness for several stable learner and verify our bound s correctness 
disjunctive logic programming dlp is a very expressive formalism it allows to express every property of finite structure that is decidable in the complexity class p npnp despite the high expressiveness of dlp there are some simple property often arising in real world application which cannot be encoded in a simple and natural manner among these property requiring to apply some arithmetic operator like sum time count on a set of element satisfying some condition cannot be naturally expressed in dlp to overcome this deficiency in this paper we extend dlp by aggregate function we formally define the semantics of the new language named dlpa we show the usefulness of the new construct on relevant knowledge based problem we analyze the computational complexity of dlpa showing that the addition of aggregate doe not bring a higher cost in that respect we provide an implementation of the dlpa language in dlvthe state of the art dlp system and report on experiment which confirm the usefulness of the proposed extension also for the efficiency of the computation 
we introduce the generalized semi markov decision process gsmdp a an extension of continuous time mdps and semi markov decision process smdps for modeling stochastic decision process with asynchronous event and action using phase type distribution and uniformization we show how an arbitrary gsmdp can be approximated by a discrete time mdp which can then be solved using existing mdp technique the technique we present can also be seen a an alternative approach for solving smdps and we demonstrate that the introduction of phase allows u to generate higher quality policy than those obtained by standard smdp solution technique 
this paper present an approach to the approximate description of univariate real valued function in term of precise or imprecise reference point and interpolation between these point it is achieved by mean of gradual rule which express that the closer the variable to the abscissa of a reference point the closer the value of the function to the ordinate of this reference point gradual rule enable u to specify sophisticated gauge under the form of connected area inside of which the function belonging to the class under consideration should remain this provides a simple and efficient tool for categorizing signal this tool can be further improved by making the gauge flexible by mean of fuzzy gradual rule this is illustrated on a benchmark example 
learning reusable sequence can support the development of expertise in many domain either by improving decision making quality or decreasing execution speed this paper introduces and evaluates a method to learn action sequence for generalized state from prior problem experience from experienced sequence the method induces the context that underlies a sequence of action empirical result indicate that the sequence and context learned for a class of problem are actually those deemed important by expert for that particular class and can be used to select appropriate action sequence when solving problem there 
the concept of consistency ha pervaded study of the constraint satisfiction problem we introduce two concept which are inspired by consistency for the more general framework of the quantified constraint satisfaction problem qcsp we use these concept to derive in a uniform fashion proof of polynomial time tractability and corresponding algorithm for certain case of the qcsp where the type of allowed relation are restricted we not only unify existing tractability result and algorithm but also identify new class of tractable qcsps 
we revisit the problem of revising probabilistic belief using uncertain evidence and report result on several major issue relating to this problem how should one specify uncertain evidence how should one revise a probability distribution how should one interpret informal evidential statement should and do iterated belief revision commute and what guarantee can be offered on the amount of belief change induced by a particular revision our discussion is focused on two main method for probabilistic revision jeffrey s rule of probability kinematics and pearl s method of virtual evidence where we analyze and unify these method from the perspective of the question posed above 
we propose a novel method for constructing utility model by learning from observed negotiation action in particular we show how offer and counter offer in negotiation can be transformed into gamble question providing the basis for eliciting utility function result of experiment and evaluation are briefly described 
causality is typically treated an all or nothing concept either a is a cause of b or it is not we extend the definition of causality introduced by halpern and pearl a to take into account the degree of responsibility of a for b for example if someone win an election then each person who vote for him is le responsible for the victory than if he had won we then define a notion of degree of blame which take into account an agent s epistemic state roughly speaking the degree of blame of a for b is the expected degree of responsibility of a for b taken over the epistemic state of an agent 
many description logic dl combine knowledge representation on an abstract logical level with an interface to concrete domain like number and string with built in predicate such a 
conventional method used for the interpretation of activation data provided by functional neuroimaging technique provide useful insight on what the network of cerebral structure are and when and how much they activate however they do not explain how the activation of these large scale network derives from the cerebral information processing mechanism involved in cognitive function at this global level of representation the human brain can be considered a a dynamic biological system dynamic bayesian network seem currently the most promising modeling paradigm our modeling approach is based on the anatomical connectivity of cerebral region the information processing within cerebral area and the causal influence that connected region exert on each other the capability of the formalism s current version are illustrated by the modeling of a phonemic categorization process explaining the different cerebral activation in normal and dyslexic subject the simulation data are compared to experimental result ruff et al 
we present an efficient approach to adding soft constraint in the form of preference to disjunctive temporal problem dtps and their subclass temporal constraint satisfaction problem tcsps specifically we describe an algorithm for checking the consistency of and finding optimal solution to such probkms the algorithm borrows concept from previous algorithm for solving tcsps and simple temporal problem with preference stpps in both case using technique for projecting and solving component sub problem we show that adding preference to dtps and tcsps requires only slightly more time than corresponding algorithm for tcsps and dtps without preference thus for problem where dtps and tcsps make sense adding preference provides a substantial gain in expressiveness for a marginal cost 
context free grammar cannot be identified in the limit from positive example gold yet natural language grammar are more powerful than context free grammar and human learn them with remarkable ease from positive example marcus identifiability result for formal language ignore a potentially powerful source of information available to learner of natural language namely meaning this paper explores the learnability of syntax i e context free grammar given positive example and knowledge of lexical semantics and the learnability of lexical semantics given knowledge of syntax the long term goal is to develop an approach to learning both syntax and semantics that bootstrap itself using limited knowledge about syntax to infer additional knowledge about semantics and limited knowledge about semantics to infer additional knowledge about syntax 
speculative execution of information gathering plan can dramatically reduce the effect of source i o latency on overall performance however the utility of speculation is closely tied to how accurately data value are predicted at runtime caching 
speculative execution of information gathering plan can dramatically reduce the effect of source i o latency on overall performance however the utility of speculation is closely tied to how accurately data value are predicted at runtime caching 
coalition formation is a key problem in automated negotiation among self interested agent in order for coalition formation to be successful a key question that must be answered is how the gain from cooperation are to be distributed various solution concept have been proposed but the computational question around these solution concept have received little attention we study a concise representation of characteristic function which allows for the agent to be concerned with a number of independent issue that each coalition of agent can address for example there may be a set of task that the capacity unconstrained agent could undertake where accomplishing a task generates a certain amount of value possibly depending on how well the task is accomplished given this representation we show how to quickly compute the shapley value a seminal value division scheme that distributes the gain from cooperation fairly in a certain sense we then show that in distributed marginal contribution based value division scheme which are known to be vulnerable to manipulation of the order in which the agent are added to the coalition this manipulation is np complete thus computational complexity serf a a barrier to manipulating the joining order finally we show that given a value division determining whether some subcoalition ha an incentive to break away in which case we say the division is not in the core is np complete so computational complexity serf to increase the stability of the coalition 
automating the scheduling of sport league ha received considerable attention in recent year a these application involve significant revenue and generate challenging combinatorial optimization problem this paper considers the traveling tournament problem ttp which abstract the salient feature of major league baseball mlb in the united state it proposes a simulated annealing algorithm ttsa for the ttp that explores both feasible and infeasible schedule us a large neighborhood with complex move and includes advanced technique such a strategic oscillation and reheats to balance the exploration of the feasible and infeasible region and to escape local minimum at very low temperature ttsa match the best known solution on the small instance of the ttp and produce significant improvement over previous approach on the larger instance moreover ttsa is shown to be robust because it worst solution quality over run is always smaller or equal to the best known solution 
the need for natural language interface to database nlis ha become increasingly acute a more and more people access information through their web browser pda and cell phone yet nlis are only usable if they map natural language question to sql query correctly people are unwilling to trade reliable and predictable user interface for intelligent but unreliable one we describe a reliable nli precise that incorporates a modern statistical paser and a semantic module precise provably handle a large class of natural language question correctly on the benchmark atis data set precise achieves accuracy 
obtaining an accurate multiple alignment of protein sequence is a difficult computational problem for which many heuristic technique sacrifice optimality to achieve reasonable running time the most commonly used heuristic is progressive alignment which merges sequence into a multiple alignment by pairwise comparison along the node of a guide tree to improve accuracy consistency based method take advantage of conservation across many sequence to provide a stronger signal for pairwise comparison in this paper we introduce the concept of probabilistic consistency for multiple sequence alignment we also present probcons an hmm based protein muhiple sequence aligner based on an approximation of the probabilistic consistency objective function on the balibase benchmark alignment database probcons demonstrates a statistically significant improvement in accuracy compared to several leading alignment program while maintaining practical running time source code and program update are freely available under the gnu public license at http probcons stanford edu 
in the web extractor agent process class of page like call for paper page researcher page etc neglecting the relevant fact that some of them are interrelated forming cluster e g science we propose here an architecture for cognitive multi agent system to retrieve and classify page from these cluster based on data extraction to enable cooperation two design requirement are crucial a a web vision coupling a vision for content class and attribute to be extracted to a functional vision the role of page in information presentation b explicit representation of agent knowledge and ability in the form of ontology both about the cluster s domain and agent task employing this web vision and agent cooperation can accelerate the retrieval of useful page we got encouraging result with two agent for the page class of scientific event and article a comparison of result to similar system come up with two requirement for such system functional categorization and a thoroughly detailed ontology of the cluster 
speculative execution of information gathering plan can dramatically reduce the effect of source i o latency on overall performance however the utility of speculation is closely tied to how accurately data value are predicted at runtime caching 
computer system are rapidly becoming so complex that maintaining them with human support staff will be prohibitively expensive and inefficient in response visionary have begun proposing that computer system be imbued with the ability to configure themselves diagnose failure and ultimately repair themselves in response to these failure however despite convincing argument that such a shift would be desirable a of yet there ha been little concrete progress made towards this goal we view these problem a fundamentally machine learning challenge hence this article present a new network simulator designed to study the application of machine learning method from a system wide perspective we also introduce learning based method for addressing the problem of job routing and scheduling in the network we simulate our experimental result verify that method using machine learning outperform heuristic and hand coded approach on an example network designed to capture many of the complexity that exist in real system 
pervasive computing aim to build an aggregated environment around a user by knitting diverse computing and communicating device and software service into a single homogeneous unit our work is to develop a pervasive computing framework which harness the power of semantic web and web service facilitating the development of effective and intelligent pervasive environment this paper present a high level view of the framework and how different pervasive service can be built on this framework 
speculative execution of information gathering plan can dramatically reduce the effect of source i o latency on overall performance however the utility of speculation is closely tied to how accurately data value are predicted at runtime caching 
teamwork demand agreement among teammembers to collaborate and coordinate effectively when a disagreement between teammate occurs due to failure team member should ideally diagnose it cause to resolve the disagreement such diagnosis of social failure can be expensive in communication and computation overhead which previous work did not address we present a novel design space of diagnosis algorithm distinguishing several phase in the diagnosis process and providing alternative algorithm for each phase we then combine these algorithm in different way to empirically explore specific design choice in a complex domain on thousand of failure case the result show that centralizing the diagnosis disambiguation process is a key factor in reducing communication while run time is affected mainly by the amount of reasoning about other agent these result contrast sharply with previous work in disagreement detection in which distributed algorithm reduce communication 
finding an equivalent fsm with minimal number of state is generally referred a state minimization or state reduction sr problem state minimization is an effective approach in logic synthesis to optimize sequential circuit design in term of area and power kam the sr problem of fsm is np complete and can be treated a a special case of the constraint satisfaction problem csp where the transition function defines all the constraint that need to be satisfied interestingly we observe that not all the constraint are re quired to obtain a given sr solution identifying the redun dancy in the fsm will be useful in the following occasion first it help to understand the nature of the np complete sr problem and to build fsm benchmark to test the effec tiveness of sr solver second the redundancy can be uti lized to hide information and thus provide security protec tion to the fsm simulation result on real life fsms reveal the existence of extremely rich redundancy 
given a noisy dataset how to locate erroneous instance and attribute and rank suspicious instance based on their impact on the system performance is an interesting and important research issue we provide in this paper an error detection and impact sensitive instance ranking edir mechanism to address this problem given a noisy dataset d we first train a benchmark classifier t from d the instance that cannot be effectively classified by t are treated a suspicious and forwarded to a subset s for each attribute ai we switch ai and the class label c to train a classifier api for ai given an instance ik in s we use api and the benchmark classifier t to locate the erroneous value of each attribute ai to quantitatively rank instance in s we define an impact measure based on the information gain ratio ir we calculate iri between attribute ai and c and use iri a the impact sensitive weight of ai the sum of impact sensitive weight from all located erroneous attribute of ik indicates it total impact value the experimental result demonstrate the effectiveness of our strategy 
we present an approach for learning criterion for part of speech classification by induction over the lexicon contained within the cyc knowledge base this produce good result using a decision tree that incorporates semantic feature e g cyc ontological type a well a syntactic feature e g headword morphology accurate result are achieved for the special case of deciding whether lexical mapping should use count noun or mass noun headword for this special case comparable result are also obtained using opencyc the publicly available version of cyc and the cyc to wordnet translation of the semantic speech part criterion 
in the paper we introduce a quantitative measure of autonomy in multiagent interaction we quantify and analyse different type of agent autonomy a decision autonomy versus action autonomy b autonomy with respect to an agent s user c autonomy with respect to other agent and group of agent and d a measure of group autonomy that account for the degree with which one group depends on another group we analyse the problem of composing multiagent group with maximum overall autonomy and we prove that this problem is np complete therefore finding the optimal group or agent with whom to share a task or to whom to delegate a task is in general computationally hard 
we present a new method for image retrieval by shape similarity able to deal with real image with not uniform background and possible touching occluding object first of all we perform a sketch driven segmentation of the scene by mean of a deformation tolerant version of the generalized hough transform dtght using the dtght we select in the image some candidate segment to be matched with the user sketch the candidate segment are then matched with the sketch checking the consistency of the corresponding shape 
link discovery is a new challenge in data mining whose primary concern are to identify strong link and discover hidden relationship among entity and organization based on low level incomplete and noisy evidence data to address this challenge we are developing a hybrid link discovery system called kojak that combine state of the art knowledge representation and reasoning kr r technology with statistical clustering and analysis technique from the area of data mining in this paper we report on the architecture and technology of it first fully completed module called the kojak group finder the group finder is capable of finding hidden group and group member in large evidence database our group finding approach address a variety of important ld challenge such a being able to exploit heterogeneous and structurally rich evidence handling the connectivity curse noise and corruption a well a the capability to scale up to very large realistic data set the first version of the kojak group finder ha been successfully tested and evaluated on a variety of synthetic datasets 
in many real world environment automatic speech recognition asr technology fail to provide adequate performance for application such a human robot dialog despite substantial evidence that speech recognition in human is performed in a top down a well a bottom up manner asr system typically fail to capitalize on this instead relying on a purely statistical bottom up methodology in this paper we advocate the use of a knowledge based approach to improving asr in domain such a mobile robotics a simple implementation is presented which us the visual recognition of object in a robot s environment to increase the probability that word and sentence related to these object will be recognized 
the security of a network configuration is based not just on the security of it individual component and their direct interconnection but it is also based on the potential for system to interoperate indirectly across network route such interoperation ha been shown to provide the potential for cascading path that violate security in a circuitous manner across a network in this paper we show how constraint programming provides a natural approach to expressing the necessary constraint to ensure multilevel security across a network configuration in particular soft constraint are used to detect and eliminate the cascading network path that violate security taking this approach result in practical advancement over existing solution to this problem in particular constraint satisfaction highlight the set of all cascading path upon which we can compute in polynomial time an optimal reconfiguration of the network and ensure security 
a wide variety of technique for visual navigation using robot mounted camera have been described over the past several decade yet adoption of optical flow navigation technique ha been slow this demo illustrates what visual navigation ha to offer robust hazard detection including precipice and obstacle high accuracy open loop odometry and stable closed loop motion control implemented via an optical flow based visual odometry system this work is based on open source vision code common computing hardware and inexpensive consumer quality camera and a such should be accessible to many robot builder 
in this paper we introduce coverage map a a new way of representing the environment of a mobile robot coverage map store for each cell of a given grid a posterior about the amount the corresponding cell is covered by an obstacle using this representation a mobile robot can more accurately reason about it uncertainty in the map of the environment than with standard occupancy grid we present a model for proximity sensor designed to update coverage map upon sensory input we also describe how coverage map can be used to formulate a decision theoretic approach for mobile robot exploration we present experiment carried out with real robot in which accurate map are build from noisy ultrasound data finally we present a comparison of different view point selection strategy for mobile robot exploration 
performance profile tree have recently been proposed a a theoretical basis for fully normative deliberation control in this paper we conduct the first experimental study of their feasibility and accuracy in making stopping decision for anytime algorithm on optimization problem using data and algorithm from two different real world domain we compare performance profile tree to other well established deliberation control technique we show that performance profile tree are feasible in practice and lead to significantly better deliberation control decision we then conduct experiment using performance profile tree where deliberation control decision are made using conditioning on multiple feature of the solution to illustrate that such an approach is feasible in practice 
decomposition method are used to convert general constraint satisfaction problem into an equivalent tree structured problem that can be solved more effectively recently diagnosis algorithm for treestructured system have been introduced but the prerequisite of coupling these algorithm to the outcome of decomposition method have not been analyzed in detail thus limiting their diagnostic applicability in this paper we generalize the tree algorithm and show how to use hypertree decomposition outcome a input to the algorithm to compute the diagnosis of a general diagnosis problem 
this paper examines the relative performance of additive and multiplicative clause weighting scheme for propositional satisfiability testing starting with one of the most recently developed multiplicative algorithm sap an experimental study wa constructed to isolate the effect of multiplicative in comparison to additive weighting while controlling other key feature of the two approach namely the use of random versus flat move deterministic versus probabilistic weight smoothing and multiple versus single inclusion of literal in the local search neighborhood a a result of this investigation we developed a pure additive weighting scheme paw which can outperform multiplicative weighting on a range of difficult problem whtle requiring considerably le effort in term of parameter turning we conclude that additive weighting show better scaling property because it make le distinction between cost and so considers a larger domain of possible move 
we present an emerging indoor assisted navigation system for the visually impaired the core of the system is a mobile robotic base with a sensor suite mounted on it the sensor suite consists of an rfid reader and a laser range finder small passive rfid sensor are manually inserted in the environment we describe how the system wa deployed in two indoor environment and evaluated by visually impaired participant in a series of pilot experiment 
dynamically balancing robot have recently been made available by segway llc in the form of the segway rmp robot mobility platform we have addressed the challenge of using these rmp robot to play soccer building up upon our extensive previous work in this multi robot research domain in this paper we make three contribution first we present a new domain called segway soccer for investigating the coordination of dynamically formed mixed human robot team within the realm of a team task that requires real time decision making and response segway soccer is a game of soccer between two team consisting of both segway riding human and segway rmps we believe segway soccer is the first game involving both human and robot in cooperative role and with similar capability in conjunction with this new domain we present our work towards developing a soccer playing robot using the rmp platform with vision a it primary sensor our third contribution is that of skill acquisition from a human teacher where the learned skill is then used seamlessly during robot execution a part of it control hierarchy skill acquisition and use address the challenge or rapidly developing the low level action that are environment dependent and are not transferable across robot 
pearl s probabilistic causal model ha been used in many domain to reason about causality pearl s treatment of action is very diffewnt from the way action are represented explicitly in action language in this paper we show how to encode pearl s probabilistic causal model in the action language pal thus relating this two distinct approach to reasoning about action 
multi agent system result from interaction between individual agent through these interaction different kind of relationship are formed which can impact substantially on the overall system performance however the behaviour of agent cannot always be anticipated especially when dealing with open and complex system open agent system must incorporate relationship management mechanism to constrain agent action and allow only desirable interaction in consequence in this paper we tackle two important issue firstly in addressing management we identify the range of different control mechanism that are required and when they should be applied secondly in addressing relationship we present a model for identifying and characterising relationship in a manner that is application neutral and amenable to automation 
a default conditional ha most often been informally interpreted a a defeasible version of a classical conditional usually the material conditional that is the intuition is that a default should behave implicitly or explicitly a it say material counterpart by default or unless explicitly overridden in this paper we develop an alternative interpretation in which a default is regarded more like a rule leading from premise to conclusion to this end a general semantic framework under a rule based interpretation is developed and a family of weak conditional logic is specified along with associated proof theory nonmonotonic inference is defined very easily in these logic one obtains a rich set of nonmonotonic inference concerning the incorporation of irrelevant property and of property inheritance moreover this interpretation resolve problem that have been associated with previous approach 
we provide a reformulation of the constraint hierarchy chs framework based on the notion of error indicator adapting the generalized view of local consistency in semiring based constraint satisfaction problem s csps we define constraint hierarchyk consistency ch k c and give a ch c enforcement algorithm we demonstrate how the ch c algorithm can be seamlessly integrated into the ordinary branch and bound algorithm to make it a finite d omain ch solver experimentation confirms the efficiency and robustness of our proposed solver prototype unlike other finite domain ch solver our pr oposed method work for both local and global comparators in addition our solver can support arbitrary error function 
a memory based heuristic is a heuristic function that is stored in a lookup table very accurate heuristic have been created by building very large lookup table sometimes called pattern database most previous work assumes that a memory based heuristic is computed for the entire state space and the cost of computing it is amortized over many problem instance but in some case it may be useful to compute a memory based heuristic for a single problem instance if the start and goal state of the problem instance are used to restrict the region of the state space for which the heuristic is needed the time and space used to compute the heuristic may be substantially reduced in this paper we review recent work that us this idea to compute space efficient heuristic for the multiple sequence alignment problem we then describe a novel development of this idea that is simpler and more general our approach lead to improved performance in solving the multiple sequence alignment problem and is general enough to apply to other domain 
despite increasing deployment of agent technology in several business and industry domain user confidence in fully automated agent driven application is noticeably lacking the main reason for such lack of trust in complete automation are scalability and nonexistence of reasonable guarantee in the performance of selfadapting software in this paper we address the latter issue in the context of learning agent in a multiagent system ma performance guarantee for most existing on line multiagent learning mal algorithm are realizable only in the limit thereby seriously limiting it practical utility our goal is to provide certain meaningful guarantee about the performance of a learner in a ma while it is learning in particular we present a novel mal algorithm that i converges to a best response against stationary opponent ii converges to a nash equilibrium in self play and iii achieves a constant bounded expected regret at any time no average regret asymptotically in arbitrary sized general sum game with non negative payoff and against any number of opponent 
speculative execution of information gathering plan can dramatically reduce the effect of source i o latency on overall performance however the utility of speculation is closely tied to how accurately data value are predicted at runtime caching 
typically markov decision problem mdps assume a single action is executed per decision epoch but in the real world one may frequently execute certain action in parallel this paper explores concurrent mdps mdps which allow multiple non conflicting action to be executed simultaneously and present two new algorithm our first approach exploit two provably sound pruning rule and thus guarantee solution optimality our second technique is a fast sampling based algorithm which produce c ose to optimal solution extremely quickly experiment show that our approach outperform the existing algorithm producing up to two order of magnitude speedup 
abstract recursive conditioning rc is an any space algorithm for exact inference in bayesian network which can trade space for time in increment of the size of a floating point number this smooth tradeoff is possible by varying the algorithm s cache size when rc is run with a constrained cache size an important problem arises which specific result should be cached in order to minimize the running time of the algorithm rc is driven by a structure known a a dtree and many such dtrees exist for a given bayesian network in this paper we examine the problem of searching for an optimal caching scheme for a given dtree and present some optimal time space tradeoff curve for given dtrees of several published bayesian network we also compare these curve to the memory requirement of state of the art algorithm based on jointrees our result show that the memory requirement of these network can be significantly reduced with only a minimal cost in time allowing for exact inference in situation previously impractical they also show that probabilistic reasoning system can be efficiently designed to run under varying amount of memory 
multi robot learning face all of the challenge of robot learning with all of the challenge of multiagent learning there ha been a great deal of recent research on multiagent reinforcement learning in stochastic game which is the intuitive extension of mdps to multiple agent this recent work although general ha only been applied to small game with at most hundred of state on the other hand robot task have continuous and often complex state and action space robot learning task demand approximation and generalization technique which have only received extensive attention in single agent learning in this paper we introduce grawolf a general purpose scalable multiagent learning algorithm it combine gradient based policy learning technique with the wolf win or learn fast variable learning rate we apply this algorithm to an adversarial multi robot task with simultaneous learning we show result of learning both in simulation and on the real robot these result demonstrate that grawolf can learn successful policy overcoming the many challenge in multi robot learning 
we describe scot a spoken conversational tutor which ha been implemented in order to investigate the advantage of natural language in tutoring especially spoken language scot us a generic architecture for conversational intelligence which ha capability such a turn management and coordination of multi modal input and output scot also includes a set of domain independent tutorial recipe a domain specific production rule knowledge base and many natural language component including a bi directional grammar a speech recognizer and a text to speech synthesizer scot lead a reflective tutorial discussion based on the detail of a problem solving session with a real time navy shipboard damage control simulator the tutor attempt to identify and remediate gap in the student s understanding of damage control doctrine by decomposing it tutorial goal into dialogue act which are then acted on by the dialogue manager to facilitate the conversation 
to model combinatorial decision problem involving uncertainty and probability we extend the stochastic constraint programming framework proposed in walsh along a number of important dimension e g to multiple chance constraint and to a range of new objective we also provide a new but equivalent semantics based on scenario using this semantics we can compile stochastic constraint program down into conventional nonstochastic constraint program this allows u to exploit the full power of existing constraint solver we have implemented this framework for decision making under uncertainty in stochastic opl a language which is based on the opl constraint modelling language hentenryck et al to illustrate the potential of this framework we model a wide range of problem in area a diverse a finance agriculture and production 
the relation between answer set programming asp and propositional satisfiability sat is at the center of many research paper partly because of the tremendous performance boost of sat solver during last year various translation from asp to sat are known but the resulting sat formula either includes many new variable or may have an unpractical size there are also well known result showing a one to one correspondence between the answer set of a logic program and the model of it completion unfortunately these result only work for specific class of problem in this paper we present a sat based decision procedure for answer set programming that i deal with any non disjunctive logic program ii work on a sat formula without additional variable and iii is guaranteed to work in polynomial space further our procedure can be extended to compute all the answer set still working in polynomial space the experimental result of a prototypical implementation show that the approach can pay off sometimes by order of magnitude 
abstract cycorp ha developed a knowledge acquisitionsystem based on cyc that can engage a user in anatural language mixed initiative dialogue inorder to achieve a intelligent dialogue with theuser it employ explicit topicand user modeling a system of prioritized interaction and atransparent agenda to which either the user or thesystem can add interaction at any time 
abstract many researcher in artificial intelligence are beginning to explore the use of soft constraint to express a set of possibly conflicting problem requirement a soft constraint is a function defined on a collection of variable which associate some measure of desirability with each possible combination of value for those variable however the crucial question of the computational complexity of finding the optimal solution to a collection of soft constraint ha so far received very 
this paper study the dynamic of agent mediated combinatorial trading at the macroscopic level the combinatorial marketplace consists of a retailer who wish to sell bundle of item and a large number of agent with different purchasing goal these agent dynamically form coalition to exploit the benefit of grouping based on their complementary need a novel physic based dynamic equation is proposed to capture the essence of the movement of agent among different sized coalition simulation experiment are performed to study the global behavior of the agent and the effectiveness of the agent mediated combinatorial trading 
this paper present a new measure of semantic relatedness between concept that is based on the number of shared word overlap in their definition gloss this measure is unique in that it extends the gloss of the concept under consideration to include the gloss of other concept to which they are related according to a given concept hierarchy we show that this new measure reasonably correlate to human judgment we introduce a new method of word sense disambiguation based on extended gloss overlap and demonstrate that it fare well on the senseval lexical sample data 
this paper address the problem of outdoor terrain modeling for the purpose of mobile robot navigation we propose an approach in which a robot acquires a set of terrain model at differing resolution our approach address one of the major shortcoming of bayesian reasoning when applied to terrain modeling namely artifact that arise from the limited spatial resolution of robot perception limited spatial resolution cause small obstacle to be detectable only at close range hence a bayes filter estimating the state of terrain segment must consider the range at which that terrain is observed we develop a multi resolution approach that maintains multiple navigation map and derive rational argument for the number of layer and their resolution we show that our approach yield significantly better result in a practical robot system capable of acquiring detailed d map in large scale outdoor environment 
we study the computational complexity of reasoning with global constraint we show that reasoning with such constraint is intractable in general we then demonstrate how the same tool of computational complexity can be used in the design and analysis of specific global constraint in particular we illustrate how computational complexity can be used to determine when a lesser level of local consistency should be enforced when decomposing constraint will lose pruning and when combining constraint is tractable we also show how the same tool can be used to study symmetry breaking meta constraint like the cardinality constraint and learning nogoods 
traditionally text classifier are built from labeled training example labeling is usually done manually by human expert or the user which is a labor intensive and time consuming process in the past few year researcher investigated various form of semi supervised learning to reduce the burden of manual labeling in this paper we propose a different approach instead of labeling a set of document the proposed method label a set of representative word for each class it then us these word to extract a set of document for each class from a set of unlabeled document to form the initial training set the em algorithm is then applied to build the classifier the key issue of the approach is how to obtain a set of representative word for each class one way is to ask the user to provide them which is difficult because the user usually can only give a few word which are insufficient for accurate learning we propose a method to solve the problem it combine clustering and feature selection the technique can effectively rank the word in the unlabeled set according to their importance the user then selects label some word from the ranked list for each class this process requires le effort than providing word with no help or manual labelillg of document our result show that the new method is highly effective and promising 
the information resource on the web are vast but much of the web is based on a browsing paradigm that requires someone to actively seek information instead one would like to have information agent that continuously attend to one s personal information need such agent need to be able to extract the relevant information from web source integrate data across site and execute efficiently in a networked environment in this paper i describe the technology we have developed to rapidly construct and deploy information agent on the web this includes wrapper learning to convert online source into agent friendly resource query planning and record linkage to integrate data across different site and streaming dataflow execution to efficiently execute agent plan i also describe how we applied this work within the electric elf project to deploy a set of agent for continuous monitoring of travel itinerary 
a binary constraint network is tree convex if we can construct a tree for the domain of the variable so that for any constraint no matter what value one variable take all the value allowed for the other variable form a subtree of the constructed tree it is known that a tree convex network is globally consistent if it is path consistent however if a tree convex network is not path consistent enforcing path consistency on it may not make it globally consistent in this paper we identify a subclass of tree convex network which are locally chain convex and union closed this class of problem can be made globally consistent by path consistency and thus is tractable more interestingly we also find that some scene labeling problem can be modeled by tree convex constraint in a natural and meaningful way 
ontology alignment is a foundational problem area for semantic interoperability we discus the complexity faced by automated alignment solution and describe an ontology based approach for describing and evaluating alignment 
different qualitative model have been proposed for decision under uncertainty in artificial intelligence but they generally fail to satisfy the principle of strict pareto dominance or principle of efficiency in contrast to the classical numerical criterion expected utility among the most prominent example of qualitative model are the qualitative possibilistic utility qpu and the order of magnitude expected utility omeu they are both appealing but inefficient in the above sense the question is whether it is possible to reconcile qualitative criterion and efficiency the present paper show that the answer is yes and that it lead to special kind of expected utility it is also shown that although numerical these expected utility remain qualitative they lead to different decision procedure based on min max and reverse operator only generalizing the leximin and leximax ordering of vector 
speculative execution of information gathering plan can dramatically reduce the effect of source i o latency on overall performance however the utility of speculation is closely tied to how accurately data value are predicted at runtime caching 
in this work we present a genetic algorithm ga system evolutionary counterpoint evoc that generates contrapuntal music counterpoint is the construction of a musical piece by superimposing multiple melody indirectly forming an underlying harmonic structure here we include a description of the underlying algorithm fitness function and overall system module 
this paper present a parametric system devised and implemented to perform hierarchical planning by delegating the actual search to an external planner the parameter at any level of abstraction including the ground one aimed at giving a better insight of whether or not the exploitation of abstract space can be used for solving complex planning problem comparison have been made between instance of the hierarchical planner and their non hierarchical counterpart to improve the significance of the result three different planner have been selected and used while performing experiment to facilitate the setting of experimental environment a novel semi automatic technique used to generate abstraction hierarchy starting from ground level domain description is also described 
unknown environment where unexpected condition can with each new rover mission to mar rover are traveling significantly longer distance in some case distance are increasing by order of magnitude from previous mission this increase enables not only the collection of more science data but cause a large rise in the number of new and different science collection opportunity in this paper we describe the oasis system which provides autonomous capability for dynamically pursuing these science collection opportunity during longrange rover traverse oasis utilizes technique from both machine learning and planning and scheduling to address this goal machine learning technique are applied to analyze data a it is collected and quickly determine new science task and priority on these task planning and scheduling technique are used to alter the rover s behavior so new science measurement can be performed while still obeying resource and other mission constraint in addition to describing our system we also discus how we are testing oasis including the use of mar rover prototype and validation using data gathered from expert planetary geologist 
this paper considers vehicle routing problem vrp where customer location and service time are random variable that are realized dynamically during plan execution it proposes a multiple scenario approach msa that continuously generates plan consistent with past decision and anticipating future request the approach which combine ai and or technique in novel way is compared with the best available heuristic that model long distance courier mail service larsen et al experimental result show that msa may significantly decrease travel time and is robust wrt reasonably noisy distribution 
speculative execution of information gathering plan can dramatically reduce the effect of source i o latency on overall performance however the utility of speculation is closely tied to how accurately data value are predicted at runtime caching 
we present a sufficient a well a a necessary condition for the equivalence between answer set and model of completion for logic program with nested expression in the body of rule this condition is the weakest among all that we are aware of even for normal logic program to obtain this result we present a polynomial time reduction from this class of nested logic program to extended program consequently answer set for these nested program can be computed by an answer set generator for extended program on the one hand and characterized in term of model of completion on the other 
this paper report work on automated meta data creation for multimedia content the approach result in the generation of a conceptual index of the content which may then be searched via semantic category instead of keywords the novelty of the work is to exploit multiple source of information relating to video content in this case the rich range of source covering important sport event news commentary and web report covering international football game in multiple language and multiple modality is analysed and the resultant data merged this merging process lead to increased accuracy relative to individual source 
evolutionary computation is a useful technique for learning behavior in multiagent system among the several type of evolutionary computation one natural and popular method is to coevolve multi agent behavior in multiple cooperating population recent research ha suggested that revolutionary system may favor stability rather than performance in some domain in order to improve upon existing method this paper examines the idea of modifying traditional coevolution biasing it to search for maximal reward we introduce a theoretical justification of the improved method and present experiment in three problem domain we conclude that biasing can help coevolution find better result in some multiagent problem domain 
we perform a systematic comparison of sat and csp model for a challenging combinatorial problem quasigroup completion qcp our empirical result clearly indicate the superiority of the d sat encoding kautz et al with various solver over other sat and csp model we propose a partial explanation of the observed performance analytically we focus on the relative conciseness of the d model and the pruning power of unit propagation empirically the focus is on the role of the unit propagation heuristic of the best performing solver satz li anbulagan which prof crucial to it success and result in a significant improvement in scalability when imported into the csp solver our result strongly suggest that sat encoding of permutation problem hnich smith walsh may well prove quite competitive in other domain in particular when compared with the currently preferred channeling csp model 
in traditional text classification a classifier is built using labeled training document of every class this paper study a different problem given a set p of document of a particular class called positive class and a set u of unlabeled document that contains document from class p and also other type of document called negative class document we want to build a classifier to classify the document in u into document from p and document not from p the key feature of this problem is that there is no labeled negative document which make traditional text classification technique inapplicable in this paper we propose an effective technique to solve the problem it combine the rocchio method and the svm technique for classifier building experimental result show that the new method outperforms existing method significantly 
we study local interchangeability of value in constraint network based on a new approach where a single value in the domain of a variable can be treated a a combination of subvalues we present an algorithm for breaking up value and combining identical fragment experimental result show that the transformed problem take le time to solve for all solution and yield more compactly representable but equivalent solution set we obtain new theoretical result on context dependent interchangeability and full interchangeability and suggest some other application 
in this paper we develop two method for improving the performance of the standard distributed breakout algorithm yokoo et al using the notion of interchangeability we study the performance of this algorithm on the problem of distributed sensor network in particular we consider how neighborhood interchangeability and neighborhood partial interchangeability freuder can be used to keep conflict localized and avoid chain reaction where a conflict originating in one part of the problem spread to neighboring area we see from the experimental result that such technique can bring about significant improvement in term of the number of cycle required to solve the problem and therefore improvement in term of communication and time requirement especially for difficult problem moreover the improved algorithm are able to solve a higher proportion of the test problem 
we present an algorithm pref ac that limit arc consistency ac to the preferred choice of a tree search procedure and that make constraint solving more efficient without changing the pruning and shape of the search tree arc consistency thus becomes more scalable and usable for many realworld constraint satisfaction problem such a configuration and scheduling moreover pref ac directly computes a preferred solution for treelike constraint satisfaction problem 
there have been many proposal to compute similarity between word based on their distribution in context however these approach do not distinguish between synonym and antonym we present two method for identifying synonym among distributionally similar word 
speculative execution of information gathering plan can dramatically reduce the effect of source i o latency on overall performance however the utility of speculation is closely tied to how accurately data value are predicted at runtime caching 
speculative execution of information gathering plan can dramatically reduce the effect of source i o latency on overall performance however the utility of speculation is closely tied to how accurately data value are predicted at runtime caching 
speculative execution of information gathering plan can dramatically reduce the effect of source i o latency on overall performance however the utility of speculation is closely tied to how accurately data value are predicted at runtime caching 
travel and tourism is the leading application field in the b c e commerce it represents nearly of the total b c turnover already in the past travel application were at the forefront of information technology i e the airline computerized reservation system in the early s the industry and it product have rather specific feature which explain this circumstance the product is a confidence good consumer decision are solely based on information beforehand and the industry is highly networked based on world wide cooperation of very different type of stakeholder consequently this industry depends on advanced it application a such travel and tourism may serve a an example of what happens and will happen in the emerging e market pointing at structural change a well a challenging application scenario the paper provides an overview about the industry describes ongoing structural change outline domain specific requirement and discus achievement and challenge in the field following an ai and e commerce point of view it finish with consideration regarding a future it scenario 
symmetry breaking in csps ha attracted considerable attention in recent year various general scheme have been proposed to eliminate symmetry during search in general these scheme may take exponential space or time to eliminate all symmetry this paper study class of csps for which symmetry breaking is tractable it identifies several csp class which feature various form of value interchangeability and show that symmetry breaking can be performed in constant time and space during search using dedicated search procedure experimental result also show the benefit of symmetry breaking on these csps which encompass many practical application 
speculative execution of information gathering plan can dramatically reduce the effect of source i o latency on overall performance however the utility of speculation is closely tied to how accurately data value are predicted at runtime caching 
this paper present a new scenario recognition algorithm for video interpretation we represent a scenario model by specifying the character involved in the scenario the sub scenario composing the scenario and the constraint combining the sub scenario various type of constraint can be used including spatio temporal and logical constraint in this paper we focus on the performance of the recognition algorithm our goal is to propose an efficient algorithm for processing temporal constraint and combining several actor defined within the scenario by efficient we mean that the recognition process is linear in function of the number of sub scenario and in most of the case in function of the number of character to validate this algorithm in term of correctness robustness and processing time in function of scenario and scene property e g number of person in the scene we have tested the algorithm on several video of a bank branch and of an office in on line and off line mode and on simulated data we conclude by comparing our algorithm with the state of the art and showing how the definition of scenario model can influence the result of the real time scenario recognition 
the search for finite state controller for partially observable markov decision process pomdps is often based on approach like gradient ascent attractive because of their relatively low computational cost in this paper we illustrate a basic problem with gradient based method applied to pomdps where the sequential nature of the decision problem is at issue and propose a new stochastic local search method a an alternative the heuristic used in our procedure mimic the sequential reasoning inherent in optimal dynamic programming dp approach we show that our algorithm consistently find higher quality controller than gradient ascent and is competitive with and for some problem superior to other state of the art controller and dp based algorithm on large scale pomdps 
we present cameo the camera assisted meeting event observer which is a physical awareness system designed for use by an agent based electronic assistant cameo is used to observe formal meeting environment and infer the activity of people attending them 
common sense knowledge can be efficiently collected from non expert over the web in a similar fashion to the open mind family of distributed knowledge capture project we describe the collection of common sense data through the open mind indoor common sense omics website we restrict the domain to indoor home and office environment to obtain dense knowledge the knowledge wa collected through sentence template that were generated dynamically based on previous user input entry were converted into relation and saved into a database we discus the result of this online collaborative effort and describe two application of the collected data to indoor mobile robot we discus active desire selection based on current belief and command and a room labeling application based on probability estimate from the common sense knowledge base 
speculative execution of information gathering plan can dramatically reduce the effect of source i o latency on overall performance however the utility of speculation is closely tied to how accurately data value are predicted at runtime caching 
abstract in this paper we identify the main requisite ofan infrastructure for the integrated management ofprofile data and propose a high level description ofits implementation the main goal of our systemis to offer to mobile user content targeted to theirneeds using a presentation suited to their device 
the problem of deriving joint policy for a group of agent that maximize some joint reward function can be modeled a a decentralized partially observable markov decision process pomdp yet despite the growing importance and application of decentralized pomdp model in the multiagents arena few algorithm have been developed for efficiently deriving joint policy for these model this paper present a new class of locally optimal algorithm called joint equilibrium based search for policy jesp we first describe an exhaustive version of jesp and subsequently a novel dynamic programming approach to jesp our complexity analysis reveals the potential for exponential speedup due to the dynamic programming approach these theoretical result are verified via empirical comparison of the two jesp version with each other and with a globally optimal brute force search algorithm finally we prove piece wise linear and convexity pwlc property thus taking step towards developing algorithm for continuous belief state 
evidence from psychology suggests that human process definite description that refer to object present in a visual scene incrementally upon hearing them rather than constructing explicit parse tree after the whole sentence wa said which are then used to determine the referent in this paper we describe a real time distributed robotic architecture for human reference resolution that demonstrates various interaction of auditory visual and semantic processing component hypothesized to underlie human process 
ensemble learning scheme such a adaboost and bagging enhance the performance of a single classifier by combining prediction from multiple classifier of the same type the prediction from an ensemble of diverse classifier can be combined in related way e g by voting or simply by selecting the best classifier via cross validation a technique widely used in machine learning however since no ensemble scheme is always the best choice a deeper insight into the structure of meaningful approach to combine prediction is needed to achieve further progress in this paper we offer an operational reformulation of common ensemble learning scheme voting selection by crossvalidation x val grading and bagging a a stacking scheme with appropriate parameter setting thus from a theoretical point of view all these scheme can be reduced to stacking with an appropriate combination method this result is an important step towards a general theoretical framework for the field of ensemble learning 
speculative execution of information gathering plan can dramatically reduce the effect of source i o latency on overall performance however the utility of speculation is closely tied to how accurately data value are predicted at runtime caching 
recent algorithm like rtdp and lao combine the strength of heuristic search h and dynamic programming dp method by exploiting knowledge of the initial state and an admissible heuristic function for producing optimal policy without evaluating the entire space in this paper we introduce and analyze three new h dp algorithm a first general algorithm schema that is a simple loop in which inconsistent reachable state i e with residual greater than a given c are found and updated until no such state are found and serf to make explicit the basic idea underlying h dp algorithm leaving other commitment aside a second algorithm that build on the first and add a labeling mechanism for detecting solved state based on tarjan s strongly connected component procedure which is very competitive with existing approach and a third algorithm that approximates the latter by enforcing the consistency of the value function over the likely reachable state only and lead to great time and memory saving with no much apparent loss in quality when transition have probability that differ greatly in value 
an advising agent a coach provides advice to other agent about how to act in this paper we contribute an advice generation method using observation of agent acting in an environment given an abstract state definition and partially specified abstract action the algorithm extract a markov chain infers a markov decision process and then solves the mdp given an arbitrary reward signal to generate advice we evaluate our work in a simulated robot soccer environment and experimental result show improved agent performance when using the advice generated from the mdp for both a sub task and the full soccer game 
this paper is about the evolutionary design of multi agent system an important part of recent research in this domain ha been focusing on collaborative revolutionary method we expose possible drawback of these method and show that for a non trivial problem called the blind mouse problem a classical ga approach in which whole population are evaluated selected and crossed together with a few tweak find an elegant and non intuitive solution more efficiently than cooperative coevolution the difference in efficiency grows with the number of agent within the simulation we propose an explanation for this poorer performance of cooperative coevolution based on the intrinsic fragility of the evaluation process this explanation is supported by theoretical and experimental argument 
speculative execution of information gathering plan can dramatically reduce the effect of source i o latency on overall performance however the utility of speculation is closely tied to how accurately data value are predicted at runtime caching 
this paper present a discussion of the theoretical complexity of plan recognition on the basis of an analysis of the number of explanation that any complete plan recognition algorithm must consider given various property of the plan library on the basis of these result it point out property of plan library that make them computationally expensive 
speculative execution of information gathering plan can dramatically reduce the effect of source i o latency on overall performance however the utility of speculation is closely tied to how accurately data value are predicted at runtime caching 
multi view learner reduce the need for labeled data by exploiting disjoint sub set of feature view each of which is sufficient for learning such algorithm assume that each view is a strong view i e perfect learning is possible in each view we extend the multi view framework by introducing a novel algorithm aggressive co testing that exploit both strong and weak view in a weak view one can learn a concept that is strictly more general or specific than the target concept aggressive co testing us the weak view both for detecting the most informative example in the domain and for improving the accuracy of the prediction in a case study on wrapper induction task our algorithm requires significantly fewer labeled example than existing state of the art approach 
speculative execution of information gathering plan can dramatically reduce the effect of source i o latency on overall performance however the utility of speculation is closely tied to how accurately data value are predicted at runtime caching 
the aim of this paper is to present a system that us both inducted hypothesis and expert knowledge for recognition of object within digital image classification of pixel by spectral and spatial feature is learned via example recognition of object within resulting class image is performed by a novel method of search that take advice on object property and enables error in object recognition to be systematically isolated and rectified guided by the system a user selects whether error correction is machine learned or user defined the adaptability of mixed initiative error correction lead to wide applicability the system is demonstrated using image from a robot soccer domain 
word sense discrimination is an unsupervised clustering problem which seek to discover which instance of a word s are used in the same meaning this is done strictly based on information found in raw corpus without using any sense tagged text or other existing knowledge source our particular focus is to systematically compare the efficacy of a range of lexical feature context representation and clustering algorithm when applied to this problem 
w c round and g q zhang have recently proposed to study a form of resolution on algebraic domain round and zhang this framework allows reasoning with knowledge which is hierarchically structured and form a suitable domain more precisely a coherent algebraic cpo a studied in domain theory in this paper we give condition under which a resolution theorem in a form underlying resolution based logic programming system can be obtained the investigation bear potential for engineering new knowledge representation and reasoning system on a firm domain theoretic background 
qualitative assessment of scientific computation is an emerging application area that applies a data driven approach to characterize at a high level phenomenon including conditioning of matrix sensitivity to various type of error propagation and algorithmic convergence behavior this paper develops a spatial aggregation approach that formalizes such analysis in term of model selection utilizing spatial structure extracted from matrix perturbation datasets we focus in particular on the characterization of matrix eigenstructure both analyzing sensitivity of computation with spectral portrait and determining eigenvalue multiplicity with jordan portrait our approach employ spatial reasoning to overcome noise and sparsity by detecting mutually reinforcing interpretation and to guide subsequent data sampling it enables quantitative evaluation of property of a scientific computation in term of confidence in a model explainable in term of the sampled data and domain knowledge about the underlying mathematical structure not only is our methodology more rigorous than the common approach of visual inspection but it also is often substantially more efficient due to well defined stopping criterion result show that the mechanism efficiently sample perturbation space and successfully uncovers high level property of matrix 
oversubscribed scheduling problem require removing or partially satisfying task when enough resource are not available for a particular oversubscribed problem air force satellite control network scheduling we find that the best approach make long leap in the search space we find this is in part due to large plateau in the search space algorithm moving only one task at a time are impractical both a genetic algorithm and squeaky wheel optimization swo make long leap in the search space and produce good solution almost time faster than local search greedy initialization is shown to be critical to good performance but is not a important a directed leap when using fewer than evaluation swo show superior performance with evaluation a genetic algorithm using a population seeded with greedy solution further improves on the swo result 
member of multi robot team may need to collaborate to accomplish a task due to difference in capability this paper describes an extension of the alliance architecture that enables agent recruitment within a decentralized uav ugv robot team without task preemption but us a formal model of emotion and handle heterogeneity affective computing allows recruitment to be robust under loss of communication between agent and minimizes the number of message passed data from simulation show that the affective strategy succeeds with a random message loss rate up to and requires fewer message to be sent compared to greedy and random and that of these affective scale best with team size comparison of broadcast to unicast messaging are also made in simulation 
collective choice setting are the heart of society game theory provides a basis for engineering the incentive into the interaction mechanism e g rule of an election or auction so that a desirable system wide outcome e g president resource allocation or task allocation is chosen even though every agent act based on self interest however there are a host of computer science issue not traditionally addressed in game theory that have to be addressed in order to make mechanism work in the real world those computing communication and privacy issue are deeply intertwined with the economic incentive issue for example the fact that agent have limited computational capability to determine their own and others preference ruin the incentive property of established auction mechanism and give rise to new issue on the positive side computational complexity can be used a a barrier to strategic behavior in setting where economic mechanism design fall short novel computational approach also enable new economic institution for example market clearing technology with specialized search algorithm is enabling a form of interaction that i call expressive competition a another example selective incremental preference elicitation can determine the optimal outcome while requiring the agent to determine and reveal only a small portion of their preference furthermore automated mechanism design can yield better mechanism than the best known to date 
automated verification is one of the most successful application of automated reasoning in computer science in automated verification one us algorithmic technique to establish the correctness of the design with respect to a given property automated verification is based on a small number of key algorithmic idea tying together graph theory automaton theory and logic in this self contained talk i will describe how this holy trinity gave rise to automated verification tool and mention some application to planning 
we are creating an environment for investigating the role of advanced ai in interactive story based computer game this environment is based on the unreal tournament ut game engine and the soar ai engine unreal provides a d virtual environment while soar provides a flexible architecture for developing complex ai character this paper describes our progress to date starting with our game haunt which is designed so that complex ai character will be critical to the success or failure of the game it address design issue with constructing a plot for an interactive storytelling environment creating synthetic character for that environment and using a story director agent to tell the story with those character 
speculative execution of information gathering plan can dramatically reduce the effect of source i o latency on overall performance however the utility of speculation is closely tied to how accurately data value are predicted at runtime caching 
recent experiment have indicated the possibility to use the brain electrical activity to directly control the movement of robotics or prosthetic device in this paper we report result with a portable non invasive brain computer interface that make possible the continuous control of a mobile robot in a house like environment the interface us surface electrode to measure electroencephalogram eeg signal from which a statistical classifier recognizes different mental state until now brain actuated control of robot ha relied on invasive approach requiring surgical implantation of electrode since eeg based system have been considered too slow for controlling rapid and complex sequence of movement here we show that after a few day of training two human subject successfully moved a robot between several room by mental control only furthermore mental control wa only marginally worse than manual control on the same task 
this paper present an action selection framework based on an assemblage of self organizing neural network called cooperative extended kohonen map this framework encapsulates two feature that significantly enhance a robot s action selection capability self organization in the continuous state and action space to provide smooth efficient and fine motion control action selection via the cooperation and competition of extended kohonen map to achieve more complex motion task qualitative and quantitative comparison for singleand multi robot task show our framework can provide better action selection than do potential field method 
wordnet similarity is a freely available software package that make it possible to measure the semantic similarity or relatedness between a pair of concept or word sens it provides six measure of similarity and three measure of relatedness all of which are based on the lexical database wordnet these measure are implemented a perl module which take a input two concept and return a numeric value that represents the degree to which they are similar or related 
a standard intuition underlying traditional account of belief change is the principle of minimal change in this paper we introduce a novel account of belief change in which the agent s belief state is modified minimally to incorporate exactly the new information thus a revision by p q will result in a new belief state in which p q is believed but a stronger proposition such a p q is not regardless of the initial form of the belief state 
recent algorithm provide powerful solution to the problem of determining cost minimizing or revenue maximizing allocation of item in combinatorial auction however in many setting criterion other than cost e g the number of winner the delivery date of item etc are also relevant in judging the quality of an allocation furthermore the bid taker is usually uncertain about her preference regarding tradeoff between cost and nonprice feature we describe new method that allow the bid taker to determine approximately optimal allocation despite this these method rely on the notion of minimax regret to guide the elicitation of preference from the bid taker and to measure the quality of an allocation in the presence of utility function uncertainty computational experiment demonstrate the practicality of minimax computation and the efficacy of our elicitation technique 
current description logic reasoning system provide only limited support for debugging logically erroneous knowledge base in this paper we propose new non standard reasoning service which we designed and implemented to pinpoint logical contradiction when developing the medical terminology dice we provide complete algorithm for unfoldable acc tboxes based on minimisation of axiom using boolean method for minimal unsatisfiability presening sub tboxes and an incomplete bottom up method for generalised incoherence preserving terminology 
speculative execution of information gathering plan can dramatically reduce the effect of source i o latency on overall performance however the utility of speculation is closely tied to how accurately data value are predicted at runtime caching 
we propose an answer to the what is ai question namely that al is really or at least really ought in significant part to be psychometric ai pai along the way we set out and rebut five objection to pai describe peri a robot in our lab who exemplifies pai and briefly treat the future of psychometric ai first by pointing toward some promising pai based application and then by raising some of the big philosophical question the success of psychometric ai will raise 
controlling the sensing of an environment by an agent ha been accepted a necessary for effective operation within most practical domain usually however agent operate in partially observable domain where not all parameter of interest are accessible to direct sensing in such circumstance sensing action must be chosen for what they will reveal indirectly through an axiomatized model of the domain causal structure including ramification this article show how sensing can be chosen so a to acquire and use indirectly obtained information to meet goal not otherwise possible classical logic event calculus is extended with both a knowledge formalism and causal ramification and is used to show how inferring unknown information about a domain lead to conditional sensing action 
this paper discus the specific of planning in multiagent environment it present the formal framework mapl maple for describing multiagent planning domain mapl allows to describe both qualitative and quantitative temporal relation among event thus subsuming the temporal model of both pddl and pop other feature are different level of control over action modeling of agent ignorance of fact and plan synchronization with communicative action for single agent planning in multi agent domain we present a novel forward search algorithm synthesizing mapl s partially ordered temporal plan finally we present a general distributed algorithm scheme for solving mapl problem with several coordinating planner these different contribution are intended a a step towards a simple yet expressive standard for the description of multiagent planning domain and algorithm such a standard could in the future allow cross evaluation of multi agent planning algorithm on standardized benchmark 
the intelligent tutoring system autotutor us latent semantic analysis to evaluate student answer to the tutor s question by comparing a student s answer to a set of expected answer the system determines how much information is covered and how to continue the tutorial despite the success of lsa in tutoring conversation the system sometimes ha difficulty determining at an early stage whether or not an expectation is covered a new lsa algorithm significantly improves the precision of autotutor s natural language understanding and can be applied to other natural language understanding application 
coalition formation is a key problem in automated negotiation among self interested agent and other electronic commerce application a coalition of agent can sometimes accomplish thing that the individual agent cannot or can do thing more efficiently however motivating the agent to abide to a solution requires careful analysis only some of the solution are stable in the sense that no group of agent is motivated to break off and form a new coalition this constraint ha been studied extensively in cooperative game theory however the computational question around this constraint have received le attention when it come to coalition formation among software agent that represent real world party these question become increasingly explicit in this paper we define a concise general representation for game in characteristic form that relies on superadditivity and show that it allows for efficient checking of whether a given outcome is in the core we then show that determining whether the core is nonempty is np complete both with and without transferable utility we demonstrate that what make the problem hard in both case is determining the collaborative possibility the set of outcome possible for the grand coalition by showing that if these are given the problem becomes tractable in both case however we then demonstrate that for a hybrid version of the problem where utility transfer is possible only within the grand coalition the problem remains np complete even when the collaborative possibility are given 
the family of vickrey clarke grove vcg mechanism is arguably the most celebrated achievement in truthful mechanism design however vcg mechanism have their limitation they only apply to optimization problem with a utilitarian objective function and their output should optimize the objective function for many optimization problem finding the optimal output is computationally intractable if we apply vcg mechanism to polynomialtime algorithm that approximate the optimal solution the resulting mechanism may no longer be truthful in light of these limitation it is useful to study whether we can design a truthful non vcg payment scheme that is computationally tractable for a given output method o in this paper we focus our attention on binary demand game in which the agent only available action are to take part in the a game or not to for these problem we prove that a truthful mechanism m o p exists with proper payment method p if and only if o satisfies a certain monotone property we also provide several general algorithm to compute the payment efficiently for various type of output in particular we show how a truthful payment can be computed through or and combination round based combination and some more complex combination of output from subgames 
knapsack constraint are a key modeling structure in discrete optimization and form the core of many real life problem formulation only recently a cost based filtering algorithm for knapsack constraint wa published that is based on some previously developed approximation algorithm for the knapsack problem in this paper we provide an empirical evaluation of approximated consistency for knapsack constraint by applying it to the market split problem and the automatic recording problem 
the general motor variation reduction adviser is a knowledge system built on case based reasoning principle that is currently in use in a dozen general motor assembly center this paper review the overall characteristic of the system and then focus on various ai element critical to support it deployment to a production system a key ai enabler is ontology guided search using domain specific ontology 
there is an evident correlation between semantic distance and creativity in the treatment of metaphor when the tenor and vehicle concept of a metaphor are semantic neighbor the local structure of the taxonomy can be used to constrain any interpretation and thus significantly curtail the breadth of the search space this reliance on taxonomy make wordnet ideally suited to the treatment of local metaphor however the distance involved in creative metaphor mean that the tenor and vehicle rarely belong to the same semantic category and interpretation must involve more than a simple recognition of a common taxonomic parent because creative metaphor effectively involves reconceptualization interpretation must instead exploit the internal relational structure of the tenor and vehicle in this paper we describe how certain key element of the qualia structure of a concept pertaining to it agentive and telic property can be automatically extracted from the sense gloss in wordnet 
in this paper we introduce and study a logic of desire the semantics of our logic is defined by mean of two ordering relation representing preference and normality a in boutilier s logic qdt however the desire are interpreted in a different way in context a i desire b is interpreted a the best among the most normal a b world are preferred to the most normal a b world we study the formal property of these desire illustrate their expressive power on several class of example and position them with respect to previous work in qualitative decision theory 
our goal is to mesh the symbolic reasoning capability of a cognitive model with the constrained optimization possibility inherent in optimal control we plan to develop and test such a system for several different dynamical model in environment of differing certainty and differing efficiency requirement 
we present two simple search method for computing a sample nash equilibrium in a normal form game one for player game and one for n player game both algorithm bias the search towards support that are small and balanced and employ a backtracking procedure to efficiently explore these support making use of a new comprehensive testbed we test these algorithm on many class of game and show that they perform well against the state of the art the lemke howson algorithm for player game and simplicial subdivision and govindan wilson for n player game 
the computation of the first complete approximation of game theoretic optimal strategy for full scale poker is addressed several abstraction technique are combined to represent the game of player texas hold em having size o using closely related model each having size o o despite the reduction in size by a factor of billion the resulting model retain the key property and structure of the real game linear programming solution to the abstracted game are used to create substantially improved poker playing program able to defeat strong human player and be competitive against world class opponent 
bayesian network model are widely used for discriminative prediction task such a classification usually their parameter are determined using unsupervised method such a maximization of the joint likelihood the reason is often that it is unclear how to find the parameter maximizing the conditional supervised likelihood we show how the discriminative learning problem can be solved efficiently for a large class of bayesian network model including the naive bayes nb and tree augmented naive bayes tan model we do this by showing that under a certain general condition on the network structure the discriminative learning problem is exactly equivalent to logistic regression with unconstrained convex parameter space hitherto this wa known only for naive bayes model since logistic regression model have a concave log likelihood surface the global maximum can be easily found by local optimization method 
in this paper we present a programming language approach for the assembly of arbitrary two dimensional shape by decentralized identically programmed agent our system compiles a predetermined global shape into a program that instructs these agent to grow the shape via replication and location based control mechanism in the global to local compilation phase an input shape is decomposed into a network of covering disc the disc network parameterizes the agent program a biologically inspired framework allowing agent to amorphously produce the shape using replication and local interaction our system is robust to random agent failure and regenerates in the event of region death 
