pisa is the town where computer science wa born in italy and where many prestigious institution in this field are located starting from the department of computer science of it university on to the institute of information science and technology of the italian national research council from the seventy there ha been a hive of activity around the outstanding figure of professor antonina starita or tonina for most of u aimed at the development of model of information processing based on biological metaphor and at their application in biomedicine involving master and doctoral student in computer science in pisa her italian and european colleague and their student these activity at the beginning of the seventy were still known a cybernetics they have then taken different name according to the main perspective adopted such a bioengineering neural network fuzzy logic and evolutionary computation more recently they have one again been reunited under almost equivalent denomination such a soft computing computational intelligence and natural computing prof starita wa born on january st in she studied physic in naples with prof eduardo caianiello and then moved to pisa a a researcher at the national research ouncil before becoming associate professor of bioengineering and then professor of computer science at the university of pisa after a long illness tonina left u on august nowadays the seminal activity of tonina continues in the different field of computational intelligence machine learning and their application to innovative interdisciplinary field of research through the computational intelligence machine learning group http www di unipi it group ciml and the neurolab founded by her at the department of computer science of pisa and through her alumnus and collaborator outside of pisa this volume presented during a symposium in memory of antonina starita which wa held in pisa at the department of computer science is to be a witness a well a a big thanks to this extraordinary researcher and guide from her former student and from those who have had the privilege of meeting her in the path of their life and to collaborate scientifically with her it gather scientific contribution by tonina s alumnus and main collaborator in the three main area where tonina wa most active in the last period of her research activity clustering and learning application biomedical application and motor control and evaluation the clustering and learning application part open with a contribution by d a ingaramo m l errecalde l c cagnina and p rosso concerning the clustering of short text corpus by particle swarm optimization pso short text collection are becoming more and more frequent due to the recent development of new communication modality e g blog text messaging snippet etc and thus it is important to develop computational tool for dealing with them the contribution show how pso based approach can be highly competitive alternative for clustering short text corpus tonina wa fascinated by pso due to it evocation of computational mechanism embedded in nature and herself contributed to explore the application of this approach e g to the alignment of medical image the second contribution in this part by d sona e olivetti p avesani and s veeramachaneni investigates the use of neural network nn and specifically of recurrent nn to interpret brain image obtained by functional magnetic resonance imaging fmri this technology aim to map the cognitive state of a human subject to specific functional area of the brain unfortunately the interpretation of fmri image is not easy and computational tool able to help in such a task are highly desirable the author of the contribution disclose how by using recurrent neural network they were able to win the pittsburgh brain activity interpretation contest edition consisting of a brain decoding problem based on free design protocol of stimulus tonina wa very interested in neural network and in particular to the kind of model exploited by the above mentioned contribution in fact she contributed to the definition of a class of neural network model the recursive neural network which are generalization of recurrent neural network to the treatment of structured input such a tree and acyclic graph e g this class of neural network model is the subject of the third contribution where a micheli c bertinetto c duce r solaro and m r tin report on the latest advance on an innovative cheminformatics approach based on them which help into the development of new molecule and material of biomedical interest the contribution present a comprehensive survey of the result obtained on acrylic and methacrylic polymer including statistical copolymer previous pioneering result in the field of cheminformatics were introduced with the contribute of tonina e g the first part of the book is closed by f aiolli and a ciula in their contribution they describe the system for paleographic inspection spi software suite developed at the university of pisa thanks also to tonina s contribution the goal of the spi system based on tangent distance and related learning method is to help in dating and localizing book produced by hand through the analysis of image of their ancient script the author discus how the system ha been used by paleographer in their attempt to classify and identify script and how spi can be improved further to meet the research need of paleographer the spi system is another concrete example of tonina s attitude to put innovative computational approach at the service of other discipline tonina also contributed to the development of new learning model using tangent distance the second part of this book is devoted to contribution in the medical and biological area these area constituted tonina s main research interest and much of her body of work is devoted to the definition of automatic efficient and effective computational tool for addressing problem in these area with a focus on computational intelligent method and technique e g a well a more traditional artificial intelligence tool such a expert system e g the contribution by g c manikis m g kounelakis and m zervakis address the prediction of response to induction treatment in acute myeloid leukemia aml different supervised learning technique are benchmarked and evaluated and most significant indicator that contribute to the improvement of diagnosis are examined a further contribution to the study of the effect of treatment in aml is given by p j g lisboa i h jarman t a etchells f ambrogi i ardoino m vignetti and e biganzoli since aml may require aggressive systemic treatment it is important to characterize quantitatively the response to treatment to this aim a time to event model with competing risk using the framework of partial logistic artificial neural network with automatic relevance determination planncr ard is applied to a significant cohort of patient diagnosed with aml de novo and treated according to a strict protocol defined by the gruppo italiano malattie ematologiche dell adulto gimema in order to follow the disease progression m pelosini f baronti and m petrini investigate the application of a partitioning recursive algorithm known a hypothesis testing classifier system algorithm hcs for the characterization of patient affected by non hodgkin lymphoma nhl the aim of the study is to discover feature potentially useful to detect patient subset with different clinical behavior and prognosis so to be able to personalize the treatment in addition to that multi objective analysis is proposed a a tool to ass follow up schedule the use of bayesian model to distinguish between benign and adnexal ovarian tumor is the topic of the contribution by b van calster o gevaert c van holsbeke b de moor s van huffel and d timmerman the author describe the result obtained by the many project supported by the international ovarian tumor analysis iota study group the aim of these project wa to explore advanced mathematical modeling option for ovarian tumor diagnosis through interdisciplinary collaboration involving clinician statistician and engineer the study reported by p aretini and g bevilacqua focus on the problem to automate the analysis of fish image a novel automated system developed by the aristotle university of thessaloniki with this aim is used for the evaluation of her status in breast cancer case obtaining improved result with respect to semi automated analysis which ha the drawback of requiring substantial user intervention the contribution by l fiaschi j m garibaldi and n krasnogor investigates the possibility of discovering a correlation between variation of the individual nucleotide in dna single nucleotide polymorphism of a person and his her response to drug therapy or personal susceptibility or resistance to a certain disease specifically the contribution exploit the transmission disequilibrium test tdt to perform a multiple test analysis of snp for the assessment of susceptibility to pre eclampsia combination of snp of interest with respect to pre eclampsia are identified the closing contribution of the second part report on how new information and communication technology such a computational intelligence algorithm and tool for biodata analysis grid computing and web service and clinical user interface can be exploited to create a knowledge infrastructure to support personalised care for alzheimer s disease ad the author e ifeachor p hu l sun n hudson and m zervakis report the result obtained within the eu funded project biopattern to which tonina actively participated the contribution provides highlight and insight into the requirement and challenge of personalized care for ad the characteristic feature and requirement of bioprofiles within the context of ad technique for the acquisition of useful parameter for inclusion in the bioprofile for ad and a grid based prototype system to demonstrate the concept of bioprofiling for ad within the eu setting the third part of the book cover a topic to which tonina devoted a relevant portion of her research time motor control and evaluation motor control wa studied by tonina in the context of robotics e g while motor evaluation for medical application wa the research subject of many of tonina s project e g the first contribution is by p morasso v mohan g metta g sandini they describe a method of motion planning that is based on an artificial potential field approach passive motion paradigm combined with terminal attractor dynamic besides holding interesting computational characteristic the proposed approach address in a satisfactory way a feature that is crucial for complex motion pattern in humanoid robot such a bimanual coordination or interference avoidance precise control of the reaching time the second and last contribution of this part by a cappozzo v camomilla u della croce c mazz and g vannozzi report on the result obtained over a decade of work within the vama italian acronym for evaluation of motor ability in the elderly project the objective of vama wa to devise through a biomechanical analysis quantitative method for assessing the locomotor functional limitation of a given individual and a a further step to investigate the relationship between relevant impairment and disability the contribution describes the step constituting the successful methodology originated from this research the book is closed by a contribution covering an additional dimension of tonina research activity i e her effort in trying to support a much a possible the transfer of successful research product to the market so that the quality of life of everybody could be improved in a significant way the author of the contribution d majidi using a very personal perspective report on how tonina supported majidi s journey from her safe and comfortable lab to the difficult and sometimes dangerous world of business we completely agree with majidi s statement about tonina everybody who knew her appreciated her wisdom her creativity her love for life and her love for research this article is for tonina this book is for tonina francesco masulli dipartimento di informatica e scienze dell informazione universit di genova italy alessio micheli dipartimento di informatica universit di pisa italy alessandro sperduti dipartimento di matematica pura ed applicata universit di padova italy acknowledgment the editor would like to thank the dipartimento di informatica of pisa for their support in the organization of the symposium and the many people who have contributed to this book and to the organization of the symposium sincere thanks to paulo lisboa elia biganzoli davide bacciu umberto barcaro franco alberto cardillo katuscia cerbioni claudio gallicchio darya majidi stefania pellegrini and k brent venable reference g da san martino f a cardillo a starita a new swarm intelligence coordination model inspired by collective prey retrieval and it application to image alignment parallel problem solving from nature a sperduti a starita supervised neural network for the classification of structure ieee transaction on neural network a m bianucci a micheli a sperduti a starita application of cascade correlation network for structure to chemistry appl int f masulli d sona a sperduti a starita g zaccagnini a system for the automatic morphological analysis of medieval manuscript journal of forensic document examination d sona a sperduti and a starita a constructive learning algorithm for discriminant tangent model nip d sona a sperduti and a starita discriminant pattern recognition using transformation invariant neuron neural computation b rossi f sartucci a starita automatic analysys of the spontaneous emg activity during ischaemic test in tetany electromyogr clin neurophysiol s la manna darya majidi antonina starita davide caramella a cilotti magnetic resonance in mammography a tool for the automatic detection of the region of interest in contrast enhanced magnetic resonance of the breast computer assisted radiology and surgery f a cardillo a starita d caramella a cilotti a hybrid method for breast mr image processing and classification ieee international symposium on biomedical imaging f baronti a micheli a passaro a starita machine learning contribution to solve prognosis medical problem in outcome prediction in cancer a f g taktak and a c fisher ed elsevier science a starita d majidi a giordano m battaglia r cioni neurex a tutorial expert system for the diagnosis of neurogenic disease of the lower limb journal of artificial intelligence in medicine f leoni m guerrini c laschi d taddeucci p dario a starita implementing robotic grasping task using a biological approach icra g asuni f leoni e guglielmelli a starita p dario a neuro controller for robotic manipulator based on biologically inspired visuo motor co ordination neural model international ieee embs conference on neural engineering g vannozzi u della croce a starita f benvenuti a cappozzo knowledge discovery in data base of biomechanical variable application to the sit to stand motor task journal of neuroengineering rehabil 
the accuracy of a cross document coreference system depends on the amount of context available which is a parameter that varies greatly from corpus to corpus this paper present a statistical model for computing name perplexity class for each perplexity class the prior probability of coreference is estimated the amount of context required for coreference is controlled by the prior coreference probability we show that the prior probability coreference is an important factor for maintaining a good balance between precision and recall for cross document coreference system 
this paper study transliteration alignment it evaluation metric and application we propose a new evaluation metric alignment entropy grounded on the information theory to evaluate the alignment quality without the need for the gold standard reference and compare the metric with f score we study the use of phonological feature and affinity statistic for transliteration alignment at phoneme and grapheme level the experiment show that better alignment consistently lead to more accurate transliteration in transliteration modeling application we achieve a mean reciprocal rate mrr of on xinhua personal name corpus a significant improvement over other reported result on the same corpus in transliteration validation application we achieve equal error rate on a large ldc corpus 
we define a probabilistic morphological analyzer using a data driven approach for syriac in order to facilitate the creation of an annotated corpus syriac is an under resourced semitic language for which there are no available language tool such a morphological analyzer we introduce novel probabilistic model for segmentation dictionary linkage and morphological tagging and connect them in a pipeline to create a probabilistic morphological analyzer requiring only labeled data we explore the performance of model with varying amount of training data and find that with about labeled token we can outperform a reasonable baseline trained on over token and achieve an accuracy of just over when trained on all available training data our joint model achieves accuracy a reduction in error rate over the baseline 
it is a challenging task to identify sentiment polarity of chinese review because the resource for chinese sentiment analysis are limited instead of leveraging only monolingual chinese knowledge this study proposes a novel approach to leverage reliable english resource to improve chinese sentiment analysis rather than simply projecting english resource onto chinese resource our approach first translates chinese review into english review by machine translation service and then identifies the sentiment polarity of english review by directly leveraging english resource furthermore our approach performs sentiment analysis for both chinese review and english review and then us ensemble method to combine the individual analysis result experimental result on a dataset of chinese product review demonstrate the effectiveness of the proposed approach the individual analysis of the translated english review outperforms the individual analysis of the original chinese review and the combination of the individual analysis result further improves the performance 
among syntax based translation model the tree based approach which take a input a parse tree of the source sentence is a promising direction being faster and simpler than it string based counterpart however current tree based system suffer from a major drawback they only use the best parse to direct the translation which potentially introduces translation mistake due to parsing error we propose a forest based approach that translates a packed forest of exponentially many par which encodes many more alternative than standard n best list large scale experiment show an absolute improvement of bleu point over the best baseline this result is also point higher than decoding with best par and take even le time 
we present an efficient algorithm for computing the weakest reading of semantically ambiguous sentence a corpus based evaluation with a large scale grammar show that our algorithm reduces over of sentence to one or two reading in negligible runtime and thus make it possible to work with semantic representation derived by deep large scale grammar 
automated mining of novel document or sentence from chronologically ordered document or sentence is an open challenge in text mining in this paper we describe the preprocessing technique for detecting novel chinese text and discus the influence of different part of speech po filtering rule on the detection performance experimental result on apwsj and trec novelty track data show that the chinese novelty mining performance is quite different when choosing two dissimilar po filtering rule thus the selection of word to represent chinese text is of vital importance to the success of the chinese novelty mining moreover we compare the chinese novelty mining performance with that of english and investigate the impact of preprocessing step on detecting novel chinese text which will be very helpful for developing a chinese novelty mining system 
combining the best output of multiple parser via parse selection or parse hybridization improves f score over the best individual parser henderson and brill sagae and lavie we propose three way to improve upon existing method for parser combination first we propose a method of parse hybridization that recombines context free production instead of constituent thereby preserving the structure of the output of the individual parser to a greater extent second we propose an efficient linear time algorithm for computing expected f score using minimum bayes risk parse selection third we extend these parser combination method from multiple best output to multiple n best output we present result on wsj section and also on the english side of a chinese english parallel corpus 
we explore a stacked framework for learning to predict dependency structure for natural language sentence a typical approach in graph based dependency parsing ha been to assume a factorized model where local feature are used but a global function is optimized mcdonald et al b recently nivre and mcdonald used the output of one dependency parser to provide feature for another we show that this is an example of stacked learning in which a second predictor is trained to improve the performance of the first further we argue that this technique is a novel way of approximating rich non local feature in the second parser without sacrificing efficient model optimal prediction experiment on twelve language show that stacking transition based and graphbased parser improves performance over existing state of the art dependency parser 
this paper provides a unified learningtheoretic analysis of several learnable class of language discussed previously in the literature the analysis show that for these class an incremental globally consistent locally conservative set driven learner always exists additionally the analysis provides a recipe for constructing new learnable class potential application include learnable model for aspect of natural language and cognition 
in this paper we focus on a recent web trend called microblogging and in particular a site called twitter the content of such a site is an extraordinarily large number of small textual message posted by million of user at random or in response to perceived event or situation we have developed an algorithm that take a trending phrase or any phrase specified by a user collect a large number of post containing the phrase and provides an automatically created summary of the post related to the term we present example of summary we produce along with initial evaluation 
most existing algorithm for learning latentvariable model such a em and existing gibbs sampler are token based meaning that they update the variable associated with one sentence at a time the incremental nature of these method make them susceptible to local optimum slow mixing in this paper we introduce a type based sampler which update a block of variable identified by a type which span multiple sentence we show improvement on part of speech induction word segmentation and learning tree substitution grammar 
we present an approach to multilingual grammar induction that exploit a phylogeny structured model of parameter drift our method doe not require any translated text or token level alignment instead the phylogenetic prior couple language at a parameter level joint induction in the multilingual model substantially outperforms independent learning with larger gain both from more articulated phylogeny and a well a from increasing number of language across eight language the multilingual approach give error reduction over the standard monolingual dmv averaging and reaching a high a 
topic model are a useful tool for analyzing large text collection but have previously been applied in only monolingual or at most bilingual context meanwhile massive collection of interlinked document in dozen of language such a wikipedia are now widely available calling for tool that can characterize content in many language we introduce a polylingual topic model that discovers topic aligned across multiple language we explore the model s characteristic using two large corpus each with over ten different language and demonstrate it usefulness in supporting machine translation and tracking topic trend across language 
we report on investigation into hierarchical phrase based translation grammar based on rule extracted from posterior distribution over alignment of the parallel text rather than restrict rule extraction to a single alignment such a viterbi we instead extract rule based on posterior distribution provided by the hmm word to word alignment model we define translation grammar progressively by adding class of rule to a basic phrase based system we ass these grammar in term of their expressive power measured by their ability to align the parallel text from which their rule are extracted and the quality of the translation they yield in chinese to english translation we find that rule extraction from posterior give translation improvement we also find that grammar with rule with only one nonterminal when extracted from posterior can outperform more complex grammar extracted from viterbi alignment finally we show that the best way to exploit source to target and target to source alignment model is to build two separate system and combine their output translation lattice 
we present a novel method for discovering and modeling the relationship between informal chinese expression including colloquialism and instant messaging slang and their formal equivalent specifically we proposed a bootstrapping procedure to identify a list of candidate informal phrase in web corpus given an informal phrase we retrieve contextual instance from the web using a search engine generate hypothesis of formal equivalent via this data and rank the hypothesis using a conditional log linear model in the log linear model we incorporate a feature function both rule based intuition and data co occurrence phenomenon either a an explicit or indirect definition or through formal informal usage occurring in free variation in a discourse we test our system on manually collected test example and find that the formal informal relationship discovery and extraction process using our method achieves an average best precision of given the ubiquity of informal conversational style on the internet this work ha clear application for text normalization in text processing system including machine translation aspiring to broad coverage 
we present a game theoretic model of bargaining over a metaphor in the context of political communication find it equilibrium and use it to rationalize observed linguistic behavior we argue that game theory is well suited for modeling discourse a a dynamic resulting from a number of conflicting pressure and suggest application of interest to computational linguist 
we present a unified view of two state of the art non projective dependency parser both approximate the loopy belief propagation parser of smith and eisner and the relaxed linear program of martin et al by representing the model assumption with a factor graph we shed light on the optimization problem tackled in each method we also propose a new aggressive online algorithm to learn the model parameter which make use of the underlying variational representation the algorithm doe not require a learning rate parameter and provides a single framework for a wide family of convex loss function including crfs and structured svms experiment show state of the art performance for language 
keyphrases are widely used a a brief summary of document since manual assignment is time consuming various unsupervised ranking method based on importance score are proposed for keyphrase extraction in practice the keyphrases of a document should not only be statistically important in the document but also have a good coverage of the document based on this observation we propose an unsupervised method for keyphrase extraction firstly the method find exemplar term by leveraging clustering technique which guarantee the document to be semantically covered by these exemplar term then the keyphrases are extracted from the document using the exemplar term our method outperforms sate of the art graph based ranking method textrank by in f measure 
mitchell et al demonstrated that corpus extracted model of semantic knowledge can predict neural activation pattern recorded using fmri this could be a very powerful technique for evaluating conceptual model extracted from corpus however fmri is expensive and imposes strong constraint on data collection following on experiment that demonstrated that eeg activation pattern encode enough information to discriminate broad conceptual category we show that corpus based semantic representation can predict eeg activation pattern with significant accuracy and we evaluate the relative performance of different corpus model on this task 
we introduce the relative rank differential statistic which is a non parametric approach to document and dialog analysis based on word frequency rank statistic we also present a simple method to establish semantic saliency in dialog document and dialog segment using these word frequency rank statistic application of our technique include the dynamic tracking of topic and semantic evolution in a dialog topic detection automatic generation of document tag and new story or event detection in conversational speech and text our approach benefit from the robustness simplicity and efficiency of non parametric and rank based approach and consistently outperformed term frequency and tf idf cosine distance approach in several experiment conducted 
a a prerequisite to translation of poetry we implement the ability to produce translation with meter and rhyme for phrase based mt examine whether the hypothesis space of such a system is flexible enough to accomodate such constraint and investigate the impact of such constraint on translation quality 
many algorithm extract term from text together with some kind of taxonomic classification is a link however the general approach used today and specifically the method of evaluating result exhibit serious shortcoming harvesting without focusing on a specific conceptual area may deliver large number of term but they are scattered over an immense concept space making recall judgment impossible regarding precision simply judging the correctness of term and their individual classification link may provide high score but this doesn t help with the eventual assembly of term into a single coherent taxonomy furthermore since there is no correct and complete gold standard to measure against most work invents some ad hoc evaluation measure we present an algorithm that is more precise and complete than previous one for identifying from web text just those concept below a given seed term comparing the result to wordnet we find that the algorithm miss term but also that it learns many new term not in wordnet and that it classifies them in way acceptable to human but different from wordnet 
the automatic interpretation of noun noun compound is an important subproblem within many natural language processing application and is an area of increasing interest the problem is difficult with disagreement regarding the number and nature of the relation low inter annotator agreement and limited annotated data in this paper we present a novel taxonomy of relation that integrates previous relation the largest publicly available annotated dataset and a supervised classification method for automatic noun compound interpretation 
in this paper we systematically ass the value of using web scale n gram data in state of the art supervised nlp classifier we compare classifier that include or exclude feature for the count of various n gram where the count are obtained from a web scale auxiliary corpus we show that including n gram count feature can advance the state of the art accuracy on standard data set for adjective ordering spelling correction noun compound bracketing and verb part of speech disambiguation more importantly when operating on new domain or when labeled training data is not plentiful we show that using web scale n gram feature is essential for achieving robust performance 
despite it long history and a great deal of research producing many useful algorithm and observation research in cooperative response generation ha had little impact on the recent commercialization of dialogue technology particularly within the spoken dialogue community we hypothesize that a particular type of cooperative response intensional summary are eective for when user are unfamiliar with the domain we evaluate this hypothesis with two experiment with cruiser a d for in car or mobile user to access restaurant information first we compare cruiser with a baseline system initiative d and show that user prefer cruiser then we experiment with four algorithm for constructing intensional summary in cruiser and show that two summary type are equally eective summary that maximize domain coverage and summary that maximize utility with respect to a user model 
information extraction ie system that extract role filler for event typically look at the local context surrounding a phrase when deciding whether to extract it often however role filler occur in clause that are not directly linked to an event word we present a new model for event extraction that jointly considers both the local context around a phrase along with the wider sentential context in a probabilistic framework our approach us a sentential event recognizer and a plausible role filler recognizer that is conditioned on event sentence we evaluate our system on two ie data set and show that our model performs well in comparison to existing ie system that rely on local phrasal context 
in this article an original view on how to improve phrase translation estimate is proposed this proposal is grounded on two main idea first that appropriate example of a given phrase should participate more in building it translation distribution second that paraphrase can be used to better estimate this distribution initial experiment provide evidence of the potential of our approach and it implementation for effectively improving translation performance 
dominance link were introduced in grammar to model long distance scrambling phenomenon motivating the definition of multiset valued linear indexed grammar mligs by rambow b and inspiring quite a few recent formalism it turn out that mligs have since been rediscovered and reused in a variety of context and that the complexity of their emptiness problem ha become the key to several open question in computer science we survey complexity result and open issue on mligs and related formalism and provide new complexity bound for some linguistically motivated restriction 
this paper present a comparative evaluation of several state of the art english parser based on different framework our approach is to measure the impact of each parser when it is used a a component of an information extraction system that performs protein protein interaction ppi identification in biomedical paper we evaluate eight parser based on dependency parsing phrase structure parsing or deep parsing using five different parse representation we run a ppi system with several combination of parser and parse representation and examine their impact on ppi identification accuracy our experiment show that the level of accuracy obtained with these different parser are similar but that accuracy improvement vary when the parser are retrained with domain specific data 
this paper introduces a new parser evaluation corpus containing around sentence annotated with unbounded dependency from seven different grammatical construction we run a series of off the shelf parser on the corpus to evaluate how well state of the art parsing technology is able to recover such dependency the overall result range from accuracy to these low score call into question the validity of using parseval score a a general measure of parsing capability we discus the importance of parser being able to recover unbounded dependency given their relatively low frequency in corpus we also analyse the various error made on these construction by one of the more successful parser 
nlp task are often domain specific yet system can learn behavior across multiple domain we develop a new multi domain online learning framework based on parameter combination from multiple classifier our algorithm draw from multi task learning and domain adaptation to adapt multiple source domain classifier to a new target domain learn across multiple similar domain and learn across a large number of disparate domain we evaluate our algorithm on two popular nlp domain adaptation task sentiment classification and spam filtering 
in this paper we propose a novel class of graph the tripartite directed acyclic graph tdags to model first order rule feature space for sentence pair classification we introduce a novel algorithm for computing the similarity in first order rewrite rule feature space our algorithm is extremely efficient and a it computes the similarity of instance that can be represented in explicit feature space it is a valid kernel function 
graph based method have gained attention in many area of natural language processing nlp including word sense disambiguation wsd text summarization keyword extraction and others most of the work in these area formulate their problem in a graph based setting and apply unsupervised graph clustering to obtain a set of cluster recent study suggest that graph often exhibit a hierarchical structure that go beyond simple flat clustering this paper present an unsupervised method for inferring the hierarchical grouping of the sens of a polysemous word the inferred hierarchical structure are applied to the problem of word sense disambiguation where we show that our method performs significantly better than traditional graph based method and agglomerative clustering yielding improvement over state of the art wsd system based on sense induction 
we present a system that find short definition of term on web page it employ a maximum entropy classifier but it is trained on automatically generated example hence it is in effect unsupervised we use rouge w to generate training example from encyclopedia and web snippet a method that outperforms an alternative centroid based one after training our system can be used to find definition of term that are not covered by encyclopedia the system outperforms a comparable publicly available system a well a a previously published form of our system 
measuring the similarity between two text is a fundamental problem in many nlp and ir application among the existing approach the cosine measure of the term vector representing the original text ha been widely used where the score of each term is often determined by a tfidf formula despite it simplicity the quality of such cosine similarity measure is usually domain dependent and decided by the choice of the term weighting function in this paper we propose a novel framework that learns the term weighting function given the labeled pair of text a training data the learning procedure tune the model parameter by minimizing the specified loss function of the similarity score compared to traditional tfidf term weighting scheme our approach show a significant improvement on task such a judging the quality of query suggestion and filtering irrelevant ad for online advertising 
experiment are reported that investigate the effect of various source document representation on the accuracy of the sentence extraction phase of a multi document summarisation task a novel representation is introduced based on generic relation extraction gre which aim to build system for relation identification and characterisation that can be transferred across domain and task without modification of model parameter result demonstrate performance that is significantly higher than a non trivial baseline that us tf idf weighted word and at least a good a a comparable but le general approach from the literature analysis show that the representation compared are complementary suggesting that extraction performance could be further improved through system combination 
we apply the hypothesis of one sense per discourse yarowsky to information extraction ie and extend the scope of discourse from one single document to a cluster of topically related document we employ a similar approach to propagate consistent event argument across sentence and document combining global evidence from related document with local decision we design a simple scheme to conduct cross document inference for improving the ace event extraction task without using any additional labeled data this new approach obtained higher f measure in trigger labeling and higher f measure in argument labeling over a state of the art ie system which extract event independently for each sentence 
pisa is the town where computer science wa born in italy and where many prestigious institution in this field are located starting from the department of computer science of it university on to the institute of information science and technology of the italian national research council from the seventy there ha been a hive of activity around the outstanding figure of professor antonina starita or tonina for most of u aimed at the development of model of information processing based on biological metaphor and at their application in biomedicine involving master and doctoral student in computer science in pisa her italian and european colleague and their student these activity at the beginning of the seventy were still known a cybernetics they have then taken different name according to the main perspective adopted such a bioengineering neural network fuzzy logic and evolutionary computation more recently they have one again been reunited under almost equivalent denomination such a soft computing computational intelligence and natural computing prof starita wa born on january st in she studied physic in naples with prof eduardo caianiello and then moved to pisa a a researcher at the national research ouncil before becoming associate professor of bioengineering and then professor of computer science at the university of pisa after a long illness tonina left u on august nowadays the seminal activity of tonina continues in the different field of computational intelligence machine learning and their application to innovative interdisciplinary field of research through the computational intelligence machine learning group http www di unipi it group ciml and the neurolab founded by her at the department of computer science of pisa and through her alumnus and collaborator outside of pisa this volume presented during a symposium in memory of antonina starita which wa held in pisa at the department of computer science is to be a witness a well a a big thanks to this extraordinary researcher and guide from her former student and from those who have had the privilege of meeting her in the path of their life and to collaborate scientifically with her it gather scientific contribution by tonina s alumnus and main collaborator in the three main area where tonina wa most active in the last period of her research activity clustering and learning application biomedical application and motor control and evaluation the clustering and learning application part open with a contribution by d a ingaramo m l errecalde l c cagnina and p rosso concerning the clustering of short text corpus by particle swarm optimization pso short text collection are becoming more and more frequent due to the recent development of new communication modality e g blog text messaging snippet etc and thus it is important to develop computational tool for dealing with them the contribution show how pso based approach can be highly competitive alternative for clustering short text corpus tonina wa fascinated by pso due to it evocation of computational mechanism embedded in nature and herself contributed to explore the application of this approach e g to the alignment of medical image the second contribution in this part by d sona e olivetti p avesani and s veeramachaneni investigates the use of neural network nn and specifically of recurrent nn to interpret brain image obtained by functional magnetic resonance imaging fmri this technology aim to map the cognitive state of a human subject to specific functional area of the brain unfortunately the interpretation of fmri image is not easy and computational tool able to help in such a task are highly desirable the author of the contribution disclose how by using recurrent neural network they were able to win the pittsburgh brain activity interpretation contest edition consisting of a brain decoding problem based on free design protocol of stimulus tonina wa very interested in neural network and in particular to the kind of model exploited by the above mentioned contribution in fact she contributed to the definition of a class of neural network model the recursive neural network which are generalization of recurrent neural network to the treatment of structured input such a tree and acyclic graph e g this class of neural network model is the subject of the third contribution where a micheli c bertinetto c duce r solaro and m r tin report on the latest advance on an innovative cheminformatics approach based on them which help into the development of new molecule and material of biomedical interest the contribution present a comprehensive survey of the result obtained on acrylic and methacrylic polymer including statistical copolymer previous pioneering result in the field of cheminformatics were introduced with the contribute of tonina e g the first part of the book is closed by f aiolli and a ciula in their contribution they describe the system for paleographic inspection spi software suite developed at the university of pisa thanks also to tonina s contribution the goal of the spi system based on tangent distance and related learning method is to help in dating and localizing book produced by hand through the analysis of image of their ancient script the author discus how the system ha been used by paleographer in their attempt to classify and identify script and how spi can be improved further to meet the research need of paleographer the spi system is another concrete example of tonina s attitude to put innovative computational approach at the service of other discipline tonina also contributed to the development of new learning model using tangent distance the second part of this book is devoted to contribution in the medical and biological area these area constituted tonina s main research interest and much of her body of work is devoted to the definition of automatic efficient and effective computational tool for addressing problem in these area with a focus on computational intelligent method and technique e g a well a more traditional artificial intelligence tool such a expert system e g the contribution by g c manikis m g kounelakis and m zervakis address the prediction of response to induction treatment in acute myeloid leukemia aml different supervised learning technique are benchmarked and evaluated and most significant indicator that contribute to the improvement of diagnosis are examined a further contribution to the study of the effect of treatment in aml is given by p j g lisboa i h jarman t a etchells f ambrogi i ardoino m vignetti and e biganzoli since aml may require aggressive systemic treatment it is important to characterize quantitatively the response to treatment to this aim a time to event model with competing risk using the framework of partial logistic artificial neural network with automatic relevance determination planncr ard is applied to a significant cohort of patient diagnosed with aml de novo and treated according to a strict protocol defined by the gruppo italiano malattie ematologiche dell adulto gimema in order to follow the disease progression m pelosini f baronti and m petrini investigate the application of a partitioning recursive algorithm known a hypothesis testing classifier system algorithm hcs for the characterization of patient affected by non hodgkin lymphoma nhl the aim of the study is to discover feature potentially useful to detect patient subset with different clinical behavior and prognosis so to be able to personalize the treatment in addition to that multi objective analysis is proposed a a tool to ass follow up schedule the use of bayesian model to distinguish between benign and adnexal ovarian tumor is the topic of the contribution by b van calster o gevaert c van holsbeke b de moor s van huffel and d timmerman the author describe the result obtained by the many project supported by the international ovarian tumor analysis iota study group the aim of these project wa to explore advanced mathematical modeling option for ovarian tumor diagnosis through interdisciplinary collaboration involving clinician statistician and engineer the study reported by p aretini and g bevilacqua focus on the problem to automate the analysis of fish image a novel automated system developed by the aristotle university of thessaloniki with this aim is used for the evaluation of her status in breast cancer case obtaining improved result with respect to semi automated analysis which ha the drawback of requiring substantial user intervention the contribution by l fiaschi j m garibaldi and n krasnogor investigates the possibility of discovering a correlation between variation of the individual nucleotide in dna single nucleotide polymorphism of a person and his her response to drug therapy or personal susceptibility or resistance to a certain disease specifically the contribution exploit the transmission disequilibrium test tdt to perform a multiple test analysis of snp for the assessment of susceptibility to pre eclampsia combination of snp of interest with respect to pre eclampsia are identified the closing contribution of the second part report on how new information and communication technology such a computational intelligence algorithm and tool for biodata analysis grid computing and web service and clinical user interface can be exploited to create a knowledge infrastructure to support personalised care for alzheimer s disease ad the author e ifeachor p hu l sun n hudson and m zervakis report the result obtained within the eu funded project biopattern to which tonina actively participated the contribution provides highlight and insight into the requirement and challenge of personalized care for ad the characteristic feature and requirement of bioprofiles within the context of ad technique for the acquisition of useful parameter for inclusion in the bioprofile for ad and a grid based prototype system to demonstrate the concept of bioprofiling for ad within the eu setting the third part of the book cover a topic to which tonina devoted a relevant portion of her research time motor control and evaluation motor control wa studied by tonina in the context of robotics e g while motor evaluation for medical application wa the research subject of many of tonina s project e g the first contribution is by p morasso v mohan g metta g sandini they describe a method of motion planning that is based on an artificial potential field approach passive motion paradigm combined with terminal attractor dynamic besides holding interesting computational characteristic the proposed approach address in a satisfactory way a feature that is crucial for complex motion pattern in humanoid robot such a bimanual coordination or interference avoidance precise control of the reaching time the second and last contribution of this part by a cappozzo v camomilla u della croce c mazz and g vannozzi report on the result obtained over a decade of work within the vama italian acronym for evaluation of motor ability in the elderly project the objective of vama wa to devise through a biomechanical analysis quantitative method for assessing the locomotor functional limitation of a given individual and a a further step to investigate the relationship between relevant impairment and disability the contribution describes the step constituting the successful methodology originated from this research the book is closed by a contribution covering an additional dimension of tonina research activity i e her effort in trying to support a much a possible the transfer of successful research product to the market so that the quality of life of everybody could be improved in a significant way the author of the contribution d majidi using a very personal perspective report on how tonina supported majidi s journey from her safe and comfortable lab to the difficult and sometimes dangerous world of business we completely agree with majidi s statement about tonina everybody who knew her appreciated her wisdom her creativity her love for life and her love for research this article is for tonina this book is for tonina francesco masulli dipartimento di informatica e scienze dell informazione universit di genova italy alessio micheli dipartimento di informatica universit di pisa italy alessandro sperduti dipartimento di matematica pura ed applicata universit di padova italy acknowledgment the editor would like to thank the dipartimento di informatica of pisa for their support in the organization of the symposium and the many people who have contributed to this book and to the organization of the symposium sincere thanks to paulo lisboa elia biganzoli davide bacciu umberto barcaro franco alberto cardillo katuscia cerbioni claudio gallicchio darya majidi stefania pellegrini and k brent venable reference g da san martino f a cardillo a starita a new swarm intelligence coordination model inspired by collective prey retrieval and it application to image alignment parallel problem solving from nature a sperduti a starita supervised neural network for the classification of structure ieee transaction on neural network a m bianucci a micheli a sperduti a starita application of cascade correlation network for structure to chemistry appl int f masulli d sona a sperduti a starita g zaccagnini a system for the automatic morphological analysis of medieval manuscript journal of forensic document examination d sona a sperduti and a starita a constructive learning algorithm for discriminant tangent model nip d sona a sperduti and a starita discriminant pattern recognition using transformation invariant neuron neural computation b rossi f sartucci a starita automatic analysys of the spontaneous emg activity during ischaemic test in tetany electromyogr clin neurophysiol s la manna darya majidi antonina starita davide caramella a cilotti magnetic resonance in mammography a tool for the automatic detection of the region of interest in contrast enhanced magnetic resonance of the breast computer assisted radiology and surgery f a cardillo a starita d caramella a cilotti a hybrid method for breast mr image processing and classification ieee international symposium on biomedical imaging f baronti a micheli a passaro a starita machine learning contribution to solve prognosis medical problem in outcome prediction in cancer a f g taktak and a c fisher ed elsevier science a starita d majidi a giordano m battaglia r cioni neurex a tutorial expert system for the diagnosis of neurogenic disease of the lower limb journal of artificial intelligence in medicine f leoni m guerrini c laschi d taddeucci p dario a starita implementing robotic grasping task using a biological approach icra g asuni f leoni e guglielmelli a starita p dario a neuro controller for robotic manipulator based on biologically inspired visuo motor co ordination neural model international ieee embs conference on neural engineering g vannozzi u della croce a starita f benvenuti a cappozzo knowledge discovery in data base of biomechanical variable application to the sit to stand motor task journal of neuroengineering rehabil 
the accuracy of a cross document coreference system depends on the amount of context available which is a parameter that varies greatly from corpus to corpus this paper present a statistical model for computing name perplexity class for each perplexity class the prior probability of coreference is estimated the amount of context required for coreference is controlled by the prior coreference probability we show that the prior probability coreference is an important factor for maintaining a good balance between precision and recall for cross document coreference system 
this paper study transliteration alignment it evaluation metric and application we propose a new evaluation metric alignment entropy grounded on the information theory to evaluate the alignment quality without the need for the gold standard reference and compare the metric with f score we study the use of phonological feature and affinity statistic for transliteration alignment at phoneme and grapheme level the experiment show that better alignment consistently lead to more accurate transliteration in transliteration modeling application we achieve a mean reciprocal rate mrr of on xinhua personal name corpus a significant improvement over other reported result on the same corpus in transliteration validation application we achieve equal error rate on a large ldc corpus 
we define a probabilistic morphological analyzer using a data driven approach for syriac in order to facilitate the creation of an annotated corpus syriac is an under resourced semitic language for which there are no available language tool such a morphological analyzer we introduce novel probabilistic model for segmentation dictionary linkage and morphological tagging and connect them in a pipeline to create a probabilistic morphological analyzer requiring only labeled data we explore the performance of model with varying amount of training data and find that with about labeled token we can outperform a reasonable baseline trained on over token and achieve an accuracy of just over when trained on all available training data our joint model achieves accuracy a reduction in error rate over the baseline 
it is a challenging task to identify sentiment polarity of chinese review because the resource for chinese sentiment analysis are limited instead of leveraging only monolingual chinese knowledge this study proposes a novel approach to leverage reliable english resource to improve chinese sentiment analysis rather than simply projecting english resource onto chinese resource our approach first translates chinese review into english review by machine translation service and then identifies the sentiment polarity of english review by directly leveraging english resource furthermore our approach performs sentiment analysis for both chinese review and english review and then us ensemble method to combine the individual analysis result experimental result on a dataset of chinese product review demonstrate the effectiveness of the proposed approach the individual analysis of the translated english review outperforms the individual analysis of the original chinese review and the combination of the individual analysis result further improves the performance 
among syntax based translation model the tree based approach which take a input a parse tree of the source sentence is a promising direction being faster and simpler than it string based counterpart however current tree based system suffer from a major drawback they only use the best parse to direct the translation which potentially introduces translation mistake due to parsing error we propose a forest based approach that translates a packed forest of exponentially many par which encodes many more alternative than standard n best list large scale experiment show an absolute improvement of bleu point over the best baseline this result is also point higher than decoding with best par and take even le time 
we present an efficient algorithm for computing the weakest reading of semantically ambiguous sentence a corpus based evaluation with a large scale grammar show that our algorithm reduces over of sentence to one or two reading in negligible runtime and thus make it possible to work with semantic representation derived by deep large scale grammar 
automated mining of novel document or sentence from chronologically ordered document or sentence is an open challenge in text mining in this paper we describe the preprocessing technique for detecting novel chinese text and discus the influence of different part of speech po filtering rule on the detection performance experimental result on apwsj and trec novelty track data show that the chinese novelty mining performance is quite different when choosing two dissimilar po filtering rule thus the selection of word to represent chinese text is of vital importance to the success of the chinese novelty mining moreover we compare the chinese novelty mining performance with that of english and investigate the impact of preprocessing step on detecting novel chinese text which will be very helpful for developing a chinese novelty mining system 
combining the best output of multiple parser via parse selection or parse hybridization improves f score over the best individual parser henderson and brill sagae and lavie we propose three way to improve upon existing method for parser combination first we propose a method of parse hybridization that recombines context free production instead of constituent thereby preserving the structure of the output of the individual parser to a greater extent second we propose an efficient linear time algorithm for computing expected f score using minimum bayes risk parse selection third we extend these parser combination method from multiple best output to multiple n best output we present result on wsj section and also on the english side of a chinese english parallel corpus 
we explore a stacked framework for learning to predict dependency structure for natural language sentence a typical approach in graph based dependency parsing ha been to assume a factorized model where local feature are used but a global function is optimized mcdonald et al b recently nivre and mcdonald used the output of one dependency parser to provide feature for another we show that this is an example of stacked learning in which a second predictor is trained to improve the performance of the first further we argue that this technique is a novel way of approximating rich non local feature in the second parser without sacrificing efficient model optimal prediction experiment on twelve language show that stacking transition based and graphbased parser improves performance over existing state of the art dependency parser 
this paper provides a unified learningtheoretic analysis of several learnable class of language discussed previously in the literature the analysis show that for these class an incremental globally consistent locally conservative set driven learner always exists additionally the analysis provides a recipe for constructing new learnable class potential application include learnable model for aspect of natural language and cognition 
in this paper we focus on a recent web trend called microblogging and in particular a site called twitter the content of such a site is an extraordinarily large number of small textual message posted by million of user at random or in response to perceived event or situation we have developed an algorithm that take a trending phrase or any phrase specified by a user collect a large number of post containing the phrase and provides an automatically created summary of the post related to the term we present example of summary we produce along with initial evaluation 
most existing algorithm for learning latentvariable model such a em and existing gibbs sampler are token based meaning that they update the variable associated with one sentence at a time the incremental nature of these method make them susceptible to local optimum slow mixing in this paper we introduce a type based sampler which update a block of variable identified by a type which span multiple sentence we show improvement on part of speech induction word segmentation and learning tree substitution grammar 
we present an approach to multilingual grammar induction that exploit a phylogeny structured model of parameter drift our method doe not require any translated text or token level alignment instead the phylogenetic prior couple language at a parameter level joint induction in the multilingual model substantially outperforms independent learning with larger gain both from more articulated phylogeny and a well a from increasing number of language across eight language the multilingual approach give error reduction over the standard monolingual dmv averaging and reaching a high a 
topic model are a useful tool for analyzing large text collection but have previously been applied in only monolingual or at most bilingual context meanwhile massive collection of interlinked document in dozen of language such a wikipedia are now widely available calling for tool that can characterize content in many language we introduce a polylingual topic model that discovers topic aligned across multiple language we explore the model s characteristic using two large corpus each with over ten different language and demonstrate it usefulness in supporting machine translation and tracking topic trend across language 
we report on investigation into hierarchical phrase based translation grammar based on rule extracted from posterior distribution over alignment of the parallel text rather than restrict rule extraction to a single alignment such a viterbi we instead extract rule based on posterior distribution provided by the hmm word to word alignment model we define translation grammar progressively by adding class of rule to a basic phrase based system we ass these grammar in term of their expressive power measured by their ability to align the parallel text from which their rule are extracted and the quality of the translation they yield in chinese to english translation we find that rule extraction from posterior give translation improvement we also find that grammar with rule with only one nonterminal when extracted from posterior can outperform more complex grammar extracted from viterbi alignment finally we show that the best way to exploit source to target and target to source alignment model is to build two separate system and combine their output translation lattice 
we present a novel method for discovering and modeling the relationship between informal chinese expression including colloquialism and instant messaging slang and their formal equivalent specifically we proposed a bootstrapping procedure to identify a list of candidate informal phrase in web corpus given an informal phrase we retrieve contextual instance from the web using a search engine generate hypothesis of formal equivalent via this data and rank the hypothesis using a conditional log linear model in the log linear model we incorporate a feature function both rule based intuition and data co occurrence phenomenon either a an explicit or indirect definition or through formal informal usage occurring in free variation in a discourse we test our system on manually collected test example and find that the formal informal relationship discovery and extraction process using our method achieves an average best precision of given the ubiquity of informal conversational style on the internet this work ha clear application for text normalization in text processing system including machine translation aspiring to broad coverage 
we present a game theoretic model of bargaining over a metaphor in the context of political communication find it equilibrium and use it to rationalize observed linguistic behavior we argue that game theory is well suited for modeling discourse a a dynamic resulting from a number of conflicting pressure and suggest application of interest to computational linguist 
we present a unified view of two state of the art non projective dependency parser both approximate the loopy belief propagation parser of smith and eisner and the relaxed linear program of martin et al by representing the model assumption with a factor graph we shed light on the optimization problem tackled in each method we also propose a new aggressive online algorithm to learn the model parameter which make use of the underlying variational representation the algorithm doe not require a learning rate parameter and provides a single framework for a wide family of convex loss function including crfs and structured svms experiment show state of the art performance for language 
keyphrases are widely used a a brief summary of document since manual assignment is time consuming various unsupervised ranking method based on importance score are proposed for keyphrase extraction in practice the keyphrases of a document should not only be statistically important in the document but also have a good coverage of the document based on this observation we propose an unsupervised method for keyphrase extraction firstly the method find exemplar term by leveraging clustering technique which guarantee the document to be semantically covered by these exemplar term then the keyphrases are extracted from the document using the exemplar term our method outperforms sate of the art graph based ranking method textrank by in f measure 
mitchell et al demonstrated that corpus extracted model of semantic knowledge can predict neural activation pattern recorded using fmri this could be a very powerful technique for evaluating conceptual model extracted from corpus however fmri is expensive and imposes strong constraint on data collection following on experiment that demonstrated that eeg activation pattern encode enough information to discriminate broad conceptual category we show that corpus based semantic representation can predict eeg activation pattern with significant accuracy and we evaluate the relative performance of different corpus model on this task 
we introduce the relative rank differential statistic which is a non parametric approach to document and dialog analysis based on word frequency rank statistic we also present a simple method to establish semantic saliency in dialog document and dialog segment using these word frequency rank statistic application of our technique include the dynamic tracking of topic and semantic evolution in a dialog topic detection automatic generation of document tag and new story or event detection in conversational speech and text our approach benefit from the robustness simplicity and efficiency of non parametric and rank based approach and consistently outperformed term frequency and tf idf cosine distance approach in several experiment conducted 
a a prerequisite to translation of poetry we implement the ability to produce translation with meter and rhyme for phrase based mt examine whether the hypothesis space of such a system is flexible enough to accomodate such constraint and investigate the impact of such constraint on translation quality 
many algorithm extract term from text together with some kind of taxonomic classification is a link however the general approach used today and specifically the method of evaluating result exhibit serious shortcoming harvesting without focusing on a specific conceptual area may deliver large number of term but they are scattered over an immense concept space making recall judgment impossible regarding precision simply judging the correctness of term and their individual classification link may provide high score but this doesn t help with the eventual assembly of term into a single coherent taxonomy furthermore since there is no correct and complete gold standard to measure against most work invents some ad hoc evaluation measure we present an algorithm that is more precise and complete than previous one for identifying from web text just those concept below a given seed term comparing the result to wordnet we find that the algorithm miss term but also that it learns many new term not in wordnet and that it classifies them in way acceptable to human but different from wordnet 
the automatic interpretation of noun noun compound is an important subproblem within many natural language processing application and is an area of increasing interest the problem is difficult with disagreement regarding the number and nature of the relation low inter annotator agreement and limited annotated data in this paper we present a novel taxonomy of relation that integrates previous relation the largest publicly available annotated dataset and a supervised classification method for automatic noun compound interpretation 
in this paper we systematically ass the value of using web scale n gram data in state of the art supervised nlp classifier we compare classifier that include or exclude feature for the count of various n gram where the count are obtained from a web scale auxiliary corpus we show that including n gram count feature can advance the state of the art accuracy on standard data set for adjective ordering spelling correction noun compound bracketing and verb part of speech disambiguation more importantly when operating on new domain or when labeled training data is not plentiful we show that using web scale n gram feature is essential for achieving robust performance 
despite it long history and a great deal of research producing many useful algorithm and observation research in cooperative response generation ha had little impact on the recent commercialization of dialogue technology particularly within the spoken dialogue community we hypothesize that a particular type of cooperative response intensional summary are eective for when user are unfamiliar with the domain we evaluate this hypothesis with two experiment with cruiser a d for in car or mobile user to access restaurant information first we compare cruiser with a baseline system initiative d and show that user prefer cruiser then we experiment with four algorithm for constructing intensional summary in cruiser and show that two summary type are equally eective summary that maximize domain coverage and summary that maximize utility with respect to a user model 
information extraction ie system that extract role filler for event typically look at the local context surrounding a phrase when deciding whether to extract it often however role filler occur in clause that are not directly linked to an event word we present a new model for event extraction that jointly considers both the local context around a phrase along with the wider sentential context in a probabilistic framework our approach us a sentential event recognizer and a plausible role filler recognizer that is conditioned on event sentence we evaluate our system on two ie data set and show that our model performs well in comparison to existing ie system that rely on local phrasal context 
in this article an original view on how to improve phrase translation estimate is proposed this proposal is grounded on two main idea first that appropriate example of a given phrase should participate more in building it translation distribution second that paraphrase can be used to better estimate this distribution initial experiment provide evidence of the potential of our approach and it implementation for effectively improving translation performance 
dominance link were introduced in grammar to model long distance scrambling phenomenon motivating the definition of multiset valued linear indexed grammar mligs by rambow b and inspiring quite a few recent formalism it turn out that mligs have since been rediscovered and reused in a variety of context and that the complexity of their emptiness problem ha become the key to several open question in computer science we survey complexity result and open issue on mligs and related formalism and provide new complexity bound for some linguistically motivated restriction 
this paper present a comparative evaluation of several state of the art english parser based on different framework our approach is to measure the impact of each parser when it is used a a component of an information extraction system that performs protein protein interaction ppi identification in biomedical paper we evaluate eight parser based on dependency parsing phrase structure parsing or deep parsing using five different parse representation we run a ppi system with several combination of parser and parse representation and examine their impact on ppi identification accuracy our experiment show that the level of accuracy obtained with these different parser are similar but that accuracy improvement vary when the parser are retrained with domain specific data 
this paper introduces a new parser evaluation corpus containing around sentence annotated with unbounded dependency from seven different grammatical construction we run a series of off the shelf parser on the corpus to evaluate how well state of the art parsing technology is able to recover such dependency the overall result range from accuracy to these low score call into question the validity of using parseval score a a general measure of parsing capability we discus the importance of parser being able to recover unbounded dependency given their relatively low frequency in corpus we also analyse the various error made on these construction by one of the more successful parser 
nlp task are often domain specific yet system can learn behavior across multiple domain we develop a new multi domain online learning framework based on parameter combination from multiple classifier our algorithm draw from multi task learning and domain adaptation to adapt multiple source domain classifier to a new target domain learn across multiple similar domain and learn across a large number of disparate domain we evaluate our algorithm on two popular nlp domain adaptation task sentiment classification and spam filtering 
in this paper we propose a novel class of graph the tripartite directed acyclic graph tdags to model first order rule feature space for sentence pair classification we introduce a novel algorithm for computing the similarity in first order rewrite rule feature space our algorithm is extremely efficient and a it computes the similarity of instance that can be represented in explicit feature space it is a valid kernel function 
graph based method have gained attention in many area of natural language processing nlp including word sense disambiguation wsd text summarization keyword extraction and others most of the work in these area formulate their problem in a graph based setting and apply unsupervised graph clustering to obtain a set of cluster recent study suggest that graph often exhibit a hierarchical structure that go beyond simple flat clustering this paper present an unsupervised method for inferring the hierarchical grouping of the sens of a polysemous word the inferred hierarchical structure are applied to the problem of word sense disambiguation where we show that our method performs significantly better than traditional graph based method and agglomerative clustering yielding improvement over state of the art wsd system based on sense induction 
we present a system that find short definition of term on web page it employ a maximum entropy classifier but it is trained on automatically generated example hence it is in effect unsupervised we use rouge w to generate training example from encyclopedia and web snippet a method that outperforms an alternative centroid based one after training our system can be used to find definition of term that are not covered by encyclopedia the system outperforms a comparable publicly available system a well a a previously published form of our system 
measuring the similarity between two text is a fundamental problem in many nlp and ir application among the existing approach the cosine measure of the term vector representing the original text ha been widely used where the score of each term is often determined by a tfidf formula despite it simplicity the quality of such cosine similarity measure is usually domain dependent and decided by the choice of the term weighting function in this paper we propose a novel framework that learns the term weighting function given the labeled pair of text a training data the learning procedure tune the model parameter by minimizing the specified loss function of the similarity score compared to traditional tfidf term weighting scheme our approach show a significant improvement on task such a judging the quality of query suggestion and filtering irrelevant ad for online advertising 
experiment are reported that investigate the effect of various source document representation on the accuracy of the sentence extraction phase of a multi document summarisation task a novel representation is introduced based on generic relation extraction gre which aim to build system for relation identification and characterisation that can be transferred across domain and task without modification of model parameter result demonstrate performance that is significantly higher than a non trivial baseline that us tf idf weighted word and at least a good a a comparable but le general approach from the literature analysis show that the representation compared are complementary suggesting that extraction performance could be further improved through system combination 
we apply the hypothesis of one sense per discourse yarowsky to information extraction ie and extend the scope of discourse from one single document to a cluster of topically related document we employ a similar approach to propagate consistent event argument across sentence and document combining global evidence from related document with local decision we design a simple scheme to conduct cross document inference for improving the ace event extraction task without using any additional labeled data this new approach obtained higher f measure in trigger labeling and higher f measure in argument labeling over a state of the art ie system which extract event independently for each sentence 
we propose a novel objective function for discriminatively tuning log linear machine translation model our objective explicitly optimizes the bleu score of expected n gram count the same quantity that arise in forest based consensus and minimum bayes risk decoding method our continuous objective can be optimized using simple gradient ascent however computing critical quantity in the gradient necessitates a novel dynamic program which we also present here assuming bleu a an evaluation measure our objective function ha two principle advantage over standard max bleu tuning first it specifically optimizes model weight for downstream consensus decoding procedure an unexpected second benefit is that it reduces overfitting which can improve test set bleu score when using standard viterbi decoding 
syntactic consistency is the preference to reuse a syntactic construction shortly after it appearance in a discourse we present an analysis of the wsj portion of the penn tree bank and show that syntactic consistency is pervasive across production with various left hand side nonterminals then we implement a reranking constituent parser that make use of extra sentential context in it feature set using a linear chain conditional random field we improve parsing accuracy over the generative baseline parser on the penn treebank wsj corpus rivalling a similar model that doe not make use of context we show that the context aware and the context ignorant rerankers perform well on different subset of the evaluation data suggesting a combined approach would provide further improvement we also compare par made by model and suggest that context can be useful for parsing by capturing structural dependency between sentence a opposed to lexically governed dependency 
research on coreference resolution and summarization ha modeled the way entity are realized a concrete phrase in discourse in particular there exist model of the noun phrase syntax used for discourse new versus discourse old referent and model describing the likely distance between a pronoun and it antecedent however model of discourse coherence a applied to information ordering task have ignored these kind of information we apply a discourse new classifier and pronoun coreference algorithm to the information ordering task and show significant improvement in performance over the entity grid a popular model of local coherence 
weblogs are a source of human activity knowledge comprising valuable information such a fact opinion and personal experience in this paper we propose a method for mining personal experience from a large set of weblogs we define experience a knowledge embedded in a collection of activity or event which an individual or group ha actually undergone based on an observation that experience revealing sentence have a certain linguistic style we formulate the problem of detecting experience a a classification task using various feature including tense mood aspect modality experiencer and verb class we also present an activity verb lexicon construction method based on theory of lexical semantics our result demonstrate that the activity verb lexicon play a pivotal role among selected feature in the classification performance and show that our proposed method outperforms the baseline significantly 
knowing the degree of antonymy between word ha widespread application in natural language processing manually created lexicon have limited coverage and do not include most semantically contrasting word pair we present a new automatic and empirical measure of antonymy that combine corpus statistic with the structure of a published thesaurus the approach is evaluated on a set of closest opposite question obtaining a precision of over along the way we discus what human consider antonymous and how antonymy manifest itself in utterance 
this paper describes a novel bayesian approach to unsupervised topic segmentation unsupervised system for this task are driven by lexical cohesion the tendency of well formed segment to induce a compact and consistent lexical distribution we show that lexical cohesion can be placed in a bayesian context by modeling the word in each topic segment a draw from a multinomial language model associated with the segment maximizing the observation likelihood in such a model yield a lexically cohesive segmentation this contrast with previous approach which relied on hand crafted cohesion metric the bayesian framework provides a principled way to incorporate additional feature such a cue phrase a powerful indicator of discourse structure that ha not been previously used in unsupervised segmentation system our model yield consistent improvement over an array of state of the art system on both text and speech datasets we also show that both an entropy based analysis and a well known previous technique can be derived a special case of the bayesian framework 
tree to string system and their forest based extension have gained steady popularity thanks to their simplicity and efficiency but there is a major limitation they are unable to guarantee the grammaticality of the output which is explicitly modeled in string to tree system via target side syntax we thus propose to combine the advantage of both and present a novel constituency to dependency translation model which us constituency forest on the source side to direct the translation and dependency tree on the target side a a language model to ensure grammaticality medium scale experiment show an absolute and statistically significant improvement of bleu point over a state of the art forest based tree to string system even with fewer rule this is also the first time that a tree to tree model can surpass tree to string counterpart 
this paper present efficient algorithm for expected similarity maximization which coincides with minimum bayes decoding for a similarity based loss function our algorithm are designed for similarity function that are sequence kernel in a general class of positive definite symmetric kernel we discus both a general algorithm and a more efficient algorithm applicable in a common unambiguous scenario we also describe the application of our algorithm to machine translation and report the result of experiment with several translation data set which demonstrate a substantial speed up in particular our result show a speed up by two order of magnitude with respect to the original method of tromble et al and by a factor of or more even with respect to an approximate algorithm specifically designed for that task these result open the path for the exploration of more appropriate or optimal kernel for the specific task considered 
strictly piecewise sp language are a subclass of regular language which encode certain kind of long distance dependency that are found in natural language like the class in the chomsky and subregular hierarchy there are many independently converging characterization of the sp class rogers et al to appear here we define sp distribution and show that they can be efficiently estimated from positive data 
we describe an approach to domain adaptation that is appropriate exactly in the case when one ha enough target data to do slightly better than just using only source data our approach is incredibly simple easy to implement a a preprocessing step line of perl and outperforms stateof the art approach on a range of datasets moreover it is trivially extended to a multidomain adaptation problem where one ha data from a variety of different domain 
document in language such a chinese japanese and korean sometimes annotate term with their translation in english inside a pair of parenthesis we present a method to extract such translation from a large collection of web document by building a partially parallel corpus and use a word alignment algorithm to identify the term being translated the method is able to generalize across the translation for different term and can reliably extract translation that occurred only once in the entire web our experiment on chinese web page produced more than million pair of translation which is over two order of magnitude more than previous result we show that the addition of the extracted translation pair a training data provides significant increase in the bleu score for a statistical machine translation system 
detecting conflicting statement is a foundational text understanding task with application in information analysis we propose an appropriate definition of contradiction for nlp task and develop available corpus from which we construct a typology of contradiction we demonstrate that a system for contradiction need to make more fine grained distinction than the common system for entailment in particular we argue for the centrality of event coreference and therefore incorporate such a component based on topicality we present the first detailed breakdown of performance on this task detecting some type of contradiction requires deeper inferential path than our system is capable of but we achieve good performance on type arising from negation and antonymy 
hierarchical discourse segmentation is a useful technology but it is difficult to evaluate i propose an error measure based on the word error rate of beeferman et al i then show that this new measure not only reliably distinguishes baseline segmentation from lexically informed hierarchical segmentation and more informed segmentation from le informed segmentation but it also offer an improvement over previous linear error measure 
finding a class of structure that is rich enough for adequate linguistic representation yet restricted enough for efficient computational processing is an important problem for dependency parsing in this paper we present a transition system for planar dependency tree tree that can be decomposed into at most two planar graph and show that it can be used to implement a classifier based parser that run in linear time and outperforms a state of the art transition based parser on four data set from the conll x shared task in addition we present an efficient method for determining whether an arbitrary tree is planar and show that or more of the tree in existing treebanks are planar 
we outline the problem of ad hoc rule in treebanks rule used for specific construction in one data set and unlikely to be used again these include ungeneralizable rule erroneous rule rule for ungrammatical text and rule which are not consistent with the rest of the annotation scheme based on a simple notion of rule equivalence and on the idea of finding rule unlike any others we develop two method for detecting ad hoc rule in flat treebanks and show they are successful in detecting such rule this is done by examining evidence across the grammar and without making any reference to context 
active learning is a machine learning approach to achieving high accuracy with a small amount of label by letting the learning algorithm choose instance to be labeled most of previous approach based on discriminative learning use the margin for choosing instance we present a method for incorporating confidence into the margin by using a newly introduced online learning algorithm and show empirically that confidence improves active learning 
this paper proposes a probabilistic model for associative anaphora resolution in japanese associative anaphora is a type of bridging anaphora in which the anaphor and it antecedent are not coreferent our model regard associative anaphora a a kind of zero anaphora and resolve it in the same manner a zero anaphora resolution using automatically acquired lexical knowledge experimental result show that our model resolve associative anaphora with good performance and the performance is improved by resolving it simultaneously with zero anaphora 
it is well known that parsing accuracy drop significantly on out of domain data what is le known is that some parser suffer more from domain shift than others we show that dependency parser have more difficulty parsing question than constituency parser in particular deterministic shift reduce dependency parser which are of highest interest for practical application because of their linear running time drop to labeled accuracy on a question test set we propose an uptraining procedure in which a deterministic parser is trained on the output of a more accurate but slower latent variable constituency parser converted to dependency uptraining with k unlabeled question achieves result comparable to having k labeled question for training with k unlabeled and k labeled question uptraining is able to improve parsing accuracy to closing the gap between in domain and out of domain performance 
this paper address a new task in sentiment classification called multi domain sentiment classification that aim to improve performance through fusing training data from multiple domain to achieve this we propose two approach of fusion feature level and classifier level to use training data from multiple domain simultaneously experimental study show that multi domain sentiment classification using the classifier level approach performs much better than single domain classification using the training data individually 
this paper present ongoing research on computational model for non cooperative dialogue we start by analysing different level of cooperation in conversation then inspired by finding from an empirical study we propose a technique for measuring non cooperation in political interview finally we describe a research programme towards obtaining a suitable model and discus previous account for conflictive dialogue identifying the difference with our work 
many named entity contain other named entity inside them despite this fact the field of named entity recognition ha almost entirely ignored nested named entity recognition but due to technological rather than ideological reason in this paper we present a new technique for recognizing nested named entity by using a discriminative constituency parser to train the model we transform each sentence into a tree with constituent for each named entity and no other syntactic structure we present result on both newspaper and biomedical corpus which contain nested named entity in three out of four set of experiment our model outperforms a standard semi crf on the more traditional top level entity at the same time we improve the overall f score by up to over the flat model which is unable to recover any nested entity 
a parsing make best search efficient by suppressing unlikely best item existing kbest extraction method can efficiently search for top derivation but only after an exhaustive best pas we present a unified algorithm for k best a parsing which preserve the efficiency of k best extraction while giving the speed ups of a method our algorithm produce optimalk best par under the same condition required for optimality in a best a parser empirically optimal k best list can be extracted significantly faster than with other approach over a range of grammar type 
this paper present a new unsupervised algorithm wordends for inferring word boundary from transcribed adult conversation phone ngrams before and after observed pause are used to bootstrap a simple discriminative model of boundary marking this fast algorithm delivers high performance even on morphologically complex word in english and arabic and promising result on accurate phonetic transcription with extensive pronunciation variation expanding training data beyond the traditional miniature datasets push performance number well above those previously reported this suggests that wordends is a viable model of child language acquisition and might be useful in speech understanding 
while traditional work on text clustering ha largely focused on grouping document by topic it is conceivable that a user may want to cluster document along other dimension such a the author s mood gender age or sentiment without knowing the user s intention a clustering algorithm will only group document along the most prominent dimension which may not be the one the user desire to address this problem we propose a novel way of incorporating user feedback into a clustering algorithm which allows a user to easily specify the dimension along which she want the data point to be clustered via inspecting only a small number of word this distinguishes our method from existing one which typically require a large amount of effort on the part of human in the form of document annotation or interactive construction of the feature space we demonstrate the viability of our method on several challenging sentiment datasets 
traditional wisdom hold that once document are turned into bag of word unigram count vector word order are completely lost we introduce an approach that perhaps surprisingly is able to learn a bigram language model from a set of bag of word document at it heart our approach is an em algorithm that seek a model which maximizes the regularized marginal likelihood of the bagof word document in experiment on seven corpus we observed that our learned bigram language model i achieve better test set perplexity than unigram model trained on the same bag of word document and are not far behind oracle bigram model trained on the corresponding ordered document ii assign higher probability to sensible bigram word pair iii improve the accuracy of ordereddocument recovery from a bag of word our approach open the door to novel phenomenon for example privacy leakage from index file 
this paper present a supervised approach for identifying generic noun phrase in context generic statement express rule like knowledge about kind or event therefore their identification is important for the automatic construction of knowledge base in particular the distinction between generic and non generic statement is crucial for the correct encoding of generic and instance level information generic expression have been studied extensively in formal semantics building on this work we explore a corpus based learning approach for identifying generic np using selection of linguistically motivated feature our result perform well above the baseline and existing prior work 
word sense disambiguation is typically phrased a the task of labeling a word in context with the best fitting sense from a sense inventory such a wordnet while question have often been raised over the choice of sense inventory computational linguist have readily accepted the best fitting sense methodology despite the fact that the case for discrete sense boundary is widely disputed by lexical semantics researcher this paper study graded word sense assignment based on a recent dataset of graded word sense annotation 
word lattice decoding ha proven useful in spoken language translation we argue that it provides a compelling model for translation of text genre a well we show that prior work in translating lattice using finite state technique can be naturally extended to more expressive synchronous context free grammarbased model additionally we resolve a significant complication that non linear word lattice input introduce in reordering model our experiment evaluating the approach demonstrate substantial gain for chineseenglish and arabic english translation 
this paper investigates a new task subjectivity word sense disambiguation swsd which is to automatically determine which word instance in a corpus are being used with subjective sens and which are being used with objective sens we provide empirical evidence that swsd is more feasible than full word sense disambiguation and that it can be exploited to improve the performance of contextual subjectivity and sentiment analysis system 
this paper introduces a new method for identifying named entity ne transliteration in bilingual corpus recent work have shown the advantage of discriminative approach to transliteration given two string w wt in the source and target language a classifier is trained to determine if wt is the transliteration of w this paper show that the transliteration problem can be formulated a a constrained optimization problem and thus take into account contextual dependency and constraint among character bi gram in the two string we further explore several method for learning the objective function of the optimization problem and show the advantage of learning it discriminately our experiment show that the new framework result in over improvement in translating english ne to hebrew 
we present an inexact search algorithm for the problem of predicting a two layered dependency graph the algorithm is based on a k best version of the standard cubictime search algorithm for projective dependency parsing which is used a the backbone of a beam search procedure this allows u to handle the complex nonlocal feature dependency occurring in bistratal parsing if we model the interdependency between the two layer we apply the algorithm to the syntacticsemantic dependency parsing task of the conll shared task and we obtain a competitive result equal to the highest published for a system that jointly learns syntactic and semantic structure 
conventional wisdom dictate that synchronous context free grammar scfgs must be converted to chomsky normal form cnf to ensure cubic time decoding for arbitrary scfgs this is typically accomplished via the synchronous binarization technique of zhang et al a drawback to this approach is that it inflates the constant factor associated with decoding and thus the practical running time denero et al tackle this problem by defining a superset of cnf called lexical normal form lnf which also support cubic time decoding under certain implicit assumption in this paper we make these assumption explicit and in doing so show that lnf can be further expanded to a broader class of grammar called scope that also support cubic time decoding by simply pruning non scope rule from a ghkm extracted grammar we obtain better translation performance than synchronous binarization 
we study self training with product of latent variable grammar in this paper we show that increasing the quality of the automatically parsed data used for self training give higher accuracy self trained grammar our generative self trained grammar reach f score of on the wsj test set and surpass even discriminative reranking system without self training additionally we show that multiple self trained grammar can be combined in a product model to achieve even higher accuracy the product model is most effective when the individual underlying grammar are most diverse combining multiple grammar that were self trained on disjoint set of unlabeled data result in a final test accuracy of on the wsj test set and on our broadcast news test set 
we challenge the nlp community to participate in a large scale distributed effort to design and build resource for developing and evaluating solution to new and existing nlp task in the context of recognizing textual entailment we argue that the single global label with which rte example are annotated is insufficient to effectively evaluate rte system performance to promote research on smaller related nlp task we believe more detailed annotation and evaluation are needed and that this effort will benefit not just rte researcher but the nlp community a a whole we use insight from successful rte system to propose a model for identifying and annotating textual inference phenomenon in textual entailment example and we present the result of a pilot annotation study that show this model is feasible and the result immediately useful 
this paper contributes a formalization of frame semantic parsing a a structure prediction problem and describes an implemented parser that transforms an english sentence into a frame semantic representation it find word that evoke framenet frame selects frame for them and locates the argument for each frame the system us two feature based discriminative probabilistic log linear model one with latent variable to permit disambiguation of new predicate word the parser is demonstrated to significantly outperform previously published result 
most existing system for chinese semantic role labeling srl make use of full syntactic par in this paper we evaluate srl method that take partial par a input we first extend the study on chinese shallow parsing presented in chen et al by raising a set of additional feature on the basis of our shallow parser we implement srl system which cast srl a the classification of syntactic chunk with iob representation for semantic role i e semantic chunk two labeling strategy are presented directly tagging semantic chunk in one stage and identifying argument boundary a a chunking task and labeling their semantic type a a classification task lor both method we present encouraging result achieving significant improvement over the best reported srl performance in the literature additionally we put forward a rule based algorithm to automatically acquire chinese verb formation which is empirically shown to enhance srl 
we extend the classical single task active learning al approach in the multi task active learning mtal paradigm we select example for several annotation task rather than for a single one a usually done in the context of al we introduce two mtal metaprotocols alternating selection and rank combination and propose a method to implement them in practice we experiment with a twotask annotation scenario that includes named entity and syntactic parse tree annotation on three different corpus mtal outperforms random selection and a stronger baseline onesided example selection in which one task is pursued using al and the selected example are provided also to the other task 
conventional n best reranking technique often suffer from the limited scope of the nbest list which rule out many potentially good alternative we instead propose forest reranking a method that reranks a packed forest of exponentially many par since exact inference is intractable with non local feature we present an approximate algorithm inspired by forest rescoring that make discriminative training practical over the whole treebank our final result an f score of outperforms both best and best reranking baseline and is better than any previously reported system trained on the treebank 
strictly corpus based measure of semantic distance conflate co occurrence information pertaining to the many possible sens of target word we propose a corpus thesaurus hybrid method that us soft constraint to generate word senseaware distributional profile dp from coarser concept dp derived from a roget like thesaurus and sense unaware traditional word dp derived from raw text although it us a knowledge source the method is not vocabulary limited if the target word is not in the thesaurus the method fall back gracefully on the word s co occurrence information this allows the method to access valuable information encoded in a lexical resource such a a thesaurus while still being able to effectively handle domain specific term and named entity experiment on word pair ranking by semantic distance show the new hybrid method to be superior to others 
we present a novel learning framework for pipeline model aimed at improving the communication between consecutive stage in a pipeline our method exploit the confidence score associated with output at any given stage in a pipeline in order to compute probabilistic feature used at other stage downstream we describe a simple method of integrating probabilistic feature into the linear scoring function used by state of the art machine learning algorithm experimental evaluation on dependency parsing and named entity recognition demonstrate the superiority of our approach over the baseline pipeline model especially when upstream stage in the pipeline exhibit low accuracy 
this paper explores chinese semantic role labeling srl for nominal predicate besides those widely used feature in verbal srl various nominal srl specific feature are first included then we improve the performance of nominal srl by integrating useful feature derived from a state of the art verbal srl system finally we address the issue of automatic predicate recognition which is essential for a nominal srl system evaluation on chinese nombank show that our research in integrating various feature derived from verbal srl significantly improves the performance it also show that our nominal srl system much outperforms the state of the art one 
bilingual lexicon are fundamental resource modern automated lexicon generation method usually require parallel corpus which are not available for most language pair lexicon can be generated using non parallel corpus or a pivot language but such lexicon are noisy we present an algorithm for generating a high quality lexicon from a noisy one which only requires an independent corpus for each language our algorithm introduces non aligned signature na a cross lingual word context similarity score that avoids the over constrained and inefficient nature of alignment based method we use na to eliminate incorrect translation from the generated lexicon we evaluate our method by improving the quality of noisy spanish hebrew lexicon generated from two pivot english lexicon our algorithm substantially outperforms other lexicon generation method 
this work investigates design choice in modeling a discourse scheme for improving opinion polarity classification for this two diverse global inference paradigm are used a supervised collective classification framework and an unsupervised optimization framework both approach perform substantially better than baseline approach establishing the efficacy of the method and the underlying discourse scheme we also present quantitative and qualitative analysis showing how the improvement are achieved 
we describe a novel approach to unsupervised learning of the event that make up a script along with constraint on their temporal ordering we collect natural language description of script specific event sequence from volunteer over the internet then we compute a graph representation of the script s temporal structure using a multiple sequence alignment algorithm the evaluation of our system show that we outperform two informed baseline 
human linguistic annotation is crucial for many natural language processing task but can be expensive and time consuming we explore the use of amazon s mechanical turk system a significantly cheaper and faster method for collecting annotation from a broad base of paid non expert contributor over the web we investigate five task affect recognition word similarity recognizing textual entailment event temporal ordering and word sense disambiguation for all five we show high agreement between mechanical turk non expert annotation and existing gold standard label provided by expert labelers for the task of affect recognition we also show that using non expert label for training machine learning algorithm can be a effective a using gold standard annotation from expert we propose a technique for bias correction that significantly improves annotation quality on two task we conclude that many large labeling task can be effectively designed and carried out in this method at a fraction of the usual expense 
in this paper we present an algorithm for extracting translation of any given multiword expression from parallel corpus given a multiword expression to be translated the method involves extracting a short list of target candidate word from parallel corpus based on score of normalized frequency generating possible translation and filtering out common subsequence and selecting the top n possible translation using the dice coefficient experiment show that our approach outperforms the word alignment based and other naive association based method we also demonstrate that adopting the extracted translation can significantly improve the performance of the moses machine translation system 
expert search in which given a query a ranked list of expert instead of document is returned ha been intensively studied recently due to it importance in facilitating the need of both information access and knowledge discovery many approach have been proposed including metadata extraction expert profile building and formal model generation however all of them conduct expert search with a coarse grained approach with these further improvement on expert search are hard to achieve in this paper we propose conducting expert search with a fine grained approach specifically we utilize more specific evidence existing in the document an evidence oriented probabilistic model for expert search and a method for the implementation are proposed experimental result show that the proposed model and the implementation are highly effective 
statistical machine translation smt model require bilingual corpus for training and these corpus are often multilingual with parallel text in multiple language simultaneously we introduce an active learning task of adding a new language to an existing multilingual set of parallel text and constructing high quality mt system from each language in the collection into this new target language we show that adding a new language using active learning to the europarl corpus provides a significant improvement compared to a random sentence selection baseline we also provide new highly effective sentence selection method that improve al for phrase based smt in the multilingual and single language pair setting 
we propose a language model based on a precise linguistically motivated grammar a hand crafted head driven phrase structure grammar and a statistical model estimating the probability of a parse tree the language model is applied by mean of an n best rescoring step which allows to directly measure the performance gain relative to the baseline system without rescoring to demonstrate that our approach is feasible and beneficial for non trivial broad domain speech recognition task we applied it to a simplified german broadcast news transcription task we report a significant reduction in word error rate compared to a state of the art baseline system 
we ass the current state of the art in speech summarization by comparing a typical summarizer on two different domain lecture data and the switchboard corpus our result cast significant doubt on the merit of this area s accepted evaluation standard in term of baseline chosen the correspondence of result to our intuition of what summary should be and the value of adding speechrelated feature to summarizers that already use transcript from automatic speech recognition asr system 
this paper demonstrates that the use of ensemble method and carefully calibrating the decision threshold can significantly improve the performance of machine learning method for morphological word decomposition we employ two algorithm which come from a family of generative probabilistic model the model consider segment boundary a hidden variable and include probability for letter transition within segment the advantage of this model family is that it can learn from small datasets and easily generalises to larger datasets the first algorithm promodes which participated in the morpho challenge an international competition for unsupervised morphological analysis employ a lower order model whereas the second algorithm promodes h is a novel development of the first using a higher order model we present the mathematical description for both algorithm conduct experiment on the morphologically rich language zulu and compare characteristic of both algorithm based on the experimental result 
this paper study textual inference by investigating comma structure which are highly frequent element whose major role in the extraction of semantic relation ha not been hitherto recognized we introduce the problem of comma resolution defined a understanding the role of comma and extracting the relation they imply we show the importance of the problem using example from textual entailment task and present a sentence transformation rule learner astrl a machine learning algorithm that us a syntactic analysis of the sentence to learn sentence transformation rule that can then be used to extract relation we have manually annotated a corpus identifying comma structure and relation they entail and experimented with both gold standard par and par created by a leading statistical parser obtaining f score of and respectively 
we present an integrated dependency based semantic role labeling system for english from both nombank and propbank by introducing assistant argument label and considering much more feature template two optimal feature template set are obtained through an effective feature selection procedure and help construct a high performance single srl system from the evaluation on the date set of conll shared task the performance of our system is quite close to the state of the art a to our knowledge this is the first integrated srl system that achieves a competitive performance against previous pipeline system 
this paper study the problem of mining entity translation specifically mining english and chinese name pair existing effort can be categorized into a a transliteration based approach leveraging phonetic similarity and b a corpus based approach exploiting bilingual co occurrence each of which suffers from inaccuracy and scarcity respectively in clear contrast we use unleveraged resource of monolingual entity co occurrence crawled from entity search engine represented a two entity relationship graph extracted from two language corpus respectively our problem is then abstracted a finding correct mapping across two graph to achieve this goal we propose a holistic approach of exploiting both transliteration similarity and monolingual co occurrence this approach building upon monolingual corpus complement existing corpus based work requiring scarce resource of parallel or comparable corpus while significantly boosting the accuracy of transliteration based work we validate our proposed system using real life datasets 
automated summarization method can be defined a language independent if they are not based on any language specific knowledge such method can be used for multilingual summarization defined by mani a processing several language with summary in the same language a input in this paper we introduce muse a language independent approach for extractive summarization based on the linear optimization of several sentence ranking measure using a genetic algorithm we tested our methodology on two language english and hebrew and evaluated it performance with rouge recall v state of the art extractive summarization approach our result show that muse performs better than the best known multilingual approach textrank in both language moreover our experimental result on a bilingual english and hebrew document collection suggest that muse doe not need to be retrained on each language and the same model can be used across at least two different language 
several recent discourse parser have employed fully supervised machine learning approach these method require human annotator to beforehand create an extensive training corpus which is a time consuming and costly process on the other hand unlabeled data is abundant and cheap to collect in this paper we propose a novel semi supervised method for discourse relation classification based on the analysis of co occurring feature in unlabeled data which is then taken into account for extending the feature vector given to a classifier our experimental result on the rst discourse tree bank corpus and penn discourse treebank indicate that the proposed method brings a significant improvement in classification accuracy and macro average f score when small training datasets are used for instance with training set of c a labeled instance the proposed method brings improvement in accuracy and macro average f score up to compared to a baseline classifier we believe that the proposed method is a first step towards detecting low occurrence relation which is useful for domain with a lack of annotated data 
we introduce tiered clustering a mixture model capable of accounting for varying degree of shared context independent feature structure and demonstrate it applicability to inferring distributed representation of word meaning common task in lexical semantics such a word relatedness or selectional preference can benefit from modeling such structure polysemous word usage is often governed by some common background metaphoric usage e g the sens of line or run and likewise modeling the selectional preference of verb relies on identifying commonality shared by their typical argument tiered clustering can also be viewed a a form of soft feature selection where feature that do not contribute meaningfully to the clustering can be excluded we demonstrate the applicability of tiered clustering highlighting particular case where modeling shared structure is beneficial and where it can be detrimental 
model of language learning play a central role in a wide range of application from psycholinguistic theory of how people acquire new word knowledge to information system that can automatically match content to user reading ability we present a novel statistical approach that can infer the distribution of a word s likely acquisition age automatically from authentic text collected from the web we then show that combining these acquisition age distribution for all word in a document provides an effective semantic component for predicting reading difficulty of new text we also compare our automatically inferred acquisition age with norm from existing oral study revealing interesting historical trend a well a difference between oral and written word acquisition process 
this paper describes the application of so called topic model to selectional preference induction three model related to latent dirichlet allocation a proven method for modelling document word cooccurrences are presented and evaluated on datasets of human plausibility judgement compared to previously proposed technique these model perform very competitively especially for infrequent predicate argument combination where they exceed the quality of web scale prediction while using relatively little data 
most attempt to integrate framenet in nlp system have so far failed because of it limited coverage in this paper we investigate the applicability of distributional and wordnet based model on the task of lexical unit induction i e the expansion of framenet with new lexical unit experimental result show that our distributional and wordnet based model achieve good level of accuracy and coverage especially when combined 
in this paper we investigate temporal pattern of web search query we carry out several evaluation to analyze the property of temporal profile of query revealing promising semantic and pragmatic relationship between word we focus on two application query suggestion and query categorization the former show a potential for time series similarity measure to identify specific semantic relatedness between word which result in state of the art performance in query suggestion while providing complementary information to more traditional distributional similarity measure the query categorization evaluation suggests that the temporal profile alone is not a strong indicator of broad topical category 
this paper present two approach to ranking reader emotion of document past study assign a document to a single emotion category so their method cannot be applied directly to the emotion ranking problem furthermore whereas previous research analyzes emotion from the writer s perspective this work examines reader emotional state the first approach proposed in this paper minimizes pairwise ranking error in the second approach regression is used to model emotional distribution experiment result show that the regression method is more effective at identifying the most popular emotion but the pairwise loss minimization method produce ranked list of emotion that have better correlation with the correct list 
this paper present a translation model that is based on tree sequence alignment where a tree sequence refers to a single sequence of subtrees that cover a phrase the model leverage on the strength of both phrase based and linguistically syntax based method it automatically learns aligned tree sequence pair with mapping probability from word aligned biparsed parallel text compared with previous model it not only capture non syntactic phrase and discontinuous phrase with linguistically structured feature but also support multi level structure reordering of tree typology with larger span this give our model stronger expressive power than other reported model experimental result on the nist mt chinese english translation task show that our method statistically significantly outperforms the baseline system 
extant statistical machine translation smt system are very complex software which embed multiple layer of heuristic and embark very large number of numerical parameter a a result it is difficult to analyze output translation and there is a real need for tool that could help developer to better understand the various cause of error in this study we make a step in that direction and present an attempt to evaluate the quality of the phrase based translation model in order to identify those translation error that stem from deficiency in the phrase table pt we propose to compute the oracle bleu score that is the best score that a system based on this pt can achieve on a reference corpus by casting the computation of the oracle bleu a an integer linear programming ilp problem we show that it is possible to efficiently compute accurate lower bound of this score and report measure performed on several standard benchmark various other application of these oracle decoding technique are also reported and discussed 
this paper describes a series of experiment to test the hypothesis that the parallel application of multiple nlp tool and the integration of their result improves the correctness and robustness of the resulting analysis it is shown how annotation created by seven nlp tool are mapped onto tool independent description that are defined with reference to an ontology of linguistic annotation and how a majority vote and ontological consistency constraint can be used to integrate multiple alternative analysis of the same token in a consistent way for morphosyntactic part of speech and morphological annotation of three german corpus the resulting merged set of ontological description are evaluated in comparison to ontological representation of existing reference annotation 
the research question treated in this paper is centered on the idea of exploiting rich resource of one language to enhance the performance of a mention detection system of another one we successfully achieve this goal by projecting information from one language to another via a parallel corpus we examine the potential improvement using various degree of linguistic information in a statistical framework and we show that the proposed technique is effective even when the target language model ha access to a significantly rich feature set experimental result show up to f improvement in performance when the system ha access to information obtained by projecting mention from a resource rich language mention detection system via a parallel corpus 
the adoption of machine translation technology for commercial application is hampered by the lack of trust associated with machine translated output in this paper we describe trustrank an mt system enhanced with a capability to rank the quality of translation output from good to bad this enables the user to set a quality threshold granting the user control over the quality of the translation we quantify the gain we obtain in translation quality and show that our solution work on a wide variety of domain and language pair 
syntactic word reordering is essential for translation across different grammar structure between syntactically distant language pair in this paper we propose to embed local and non local word reordering decision in a synchronous context free grammar and leverage the grammar in a chart based decoder local word reordering is effectively encoded in hiero like rule whereas non local word reordering which allows for long range movement of syntactic chunk is represented in tree based reordering rule which contain variable correspond to source side syntactic constituent we demonstrate how these rule are learned from parallel corpus our proposed shallow tree to string rule show significant improvement in translation quality across different test set 
to date few attempt have been made to develop and validate method for automatic evaluation of linguistic quality in text summarization we present the first systematic assessment of several diverse class of metric designed to capture various aspect of well written text we train and test linguistic quality model on consecutive year of nist evaluation data in order to show the generality of result for grammaticality the best result come from a set of syntactic feature focus coherence and referential clarity are best evaluated by a class of feature measuring local coherence on the basis of cosine similarity between sentence coreference information and summarization specific feature our best result are accuracy for pairwise comparison of competing system over a test set of several input and for ranking summary of a specific input 
predicting possible code switching point can help develop more accurate method for automatically processing mixed language text such a multilingual language model for speech recognition system and syntactic analyzer we present in this paper exploratory result on learning to predict potential code switching point in spanish english we trained different learning algorithm using a transcription of code switched discourse to evaluate the performance of the classifier we used two different criterion measuring precision recall and f measure of the prediction against the reference in the transcription and rating the naturalness of artificially generated code switched sentence average score for the code switched sentence generated by our machine learning approach were close to the score of those generated by human 
existing graph based ranking method for keyphrase extraction compute a single importance score for each word via a single random walk motivated by the fact that both document and word can be represented by a mixture of semantic topic we propose to decompose traditional random walk into multiple random walk specific to various topic we thus build a topical pagerank tpr on word graph to measure word importance with respect to different topic after that given the topic distribution of the document we further calculate the ranking score of word and extract the top ranked one a keyphrases experimental result show that tpr outperforms state of the art keyphrase extraction method on two datasets under various evaluation metric 
we present a simple but accurate parser which exploit both large tree fragment and symbol refinement we parse with all fragment of the training set in contrast to much recent work on tree selection in data oriented parsing and tree substitution grammar learning we require only simple deterministic grammar symbol refinement in contrast to recent work on latent symbol refinement moreover our parser requires no explicit lexicon machinery instead parsing input sentence a character stream despite it simplicity our parser achieves accuracy of over f on the standard english wsj task which is competitive with substantially more complicated state of the art lexicalized and latent variable parser additional specific contribution center on making implicit all fragment parsing efficient including a coarse to fine inference scheme and a new graph encoding 
we explore the task of automatically classifying dialogue act in on online chat forum an increasingly popular mean of providing customer service in particular we investigate the effectiveness of various feature and machine learner for this task while a simple bag of word approach provides a solid baseline we find that adding information from dialogue structure and inter utterance dependency provides some increase in performance learner that account for sequential dependency crfs show the best performance we report our result from testing using a corpus of chat dialogue derived from online shopping customer feedback data 
we present a natural language generation approach which model exploit and manipulates the non linguistic context in situated communication using technique from ai planning we show how to generate instruction which deliberately guide the hearer to a location that is convenient for the generation of simple referring expression and how to generate referring expression with context dependent adjective we implement and evaluate our approach in the framework of the challenge on generating instruction in virtual environment finding that it performs well even under the constraint of realtime generation 
we propose cmsms a novel type of generic compositional model for syntactic and semantic aspect of natural language based on matrix multiplication we argue for the structural and cognitive plausibility of this model and show that it is able to cover and combine various common compositional nlp approach ranging from statistical word space model to symbolic grammar formalism 
hand coded script were used in the s a knowledge backbone that enabled inference and other nlp task requiring deep semantic knowledge we propose unsupervised induction of similar schema called narrative event chain from raw newswire text a narrative event chain is a partially ordered set of event related by a common protagonist we describe a three step process to learning narrative event chain the first us unsupervised distributional method to learn narrative relation between event sharing coreferring argument the second applies a temporal classifier to partially order the connected event finally the third prune and cluster self contained chain from the space of event we introduce two evaluation the narrative cloze to evaluate event relatedness and an order coherence task to evaluate narrative order we show a improvement over baseline for narrative prediction and for temporal coherence 
this paper present preliminary result on the detection of cultural difference from people s experience in various country from two perspective tourist and local our approach is to develop probabilistic model that would provide a good framework for such study thus we propose here a new model cclda which extends over the latent dirichlet allocation lda blei et al and cross collection mixture ccmix zhai et al model on blog and forum we also provide a qualitative and quantitative analysis of the model on the cross cultural data 
in this paper we address the task of crosslingual semantic relatedness we introduce a method that relies on the information extracted from wikipedia by exploiting the interlanguage link available between wikipedia version in multiple language through experiment performed on several language pair we show that the method performs well with a performance comparable to monolingual measure of relatedness 
this study present a novel approach to the problem of system portability across different domain a sentiment annotation system that integrates a corpus based classifier trained on a small set of annotated in domain data and a lexicon based system trained on wordnet the paper explores the challenge of system portability across domain and text genre movie review news blog and product review highlight the factor affecting system performance on out of domain and smallset in domain data and present a new system consisting of the ensemble of two classifier with precision based vote weighting that provides significant gain in accuracy and recall over the corpus based classifier and the lexicon based system taken individually 
we employ statistical method to analyze generate and translate rhythmic poetry we first apply unsupervised learning to reveal word stress pattern in a corpus of raw poetry we then use these word stress pattern in addition to rhyme and discourse model to generate english love poetry finally we translate italian poetry into english choosing target realization that conform to desired rhythmic pattern 
in this paper we present a novel method based on crf s in response to the two special characteristic of contextual dependency and label redundancy in sentence sentiment classification we try to capture the contextual constraint on sentence sentiment using crfs through introducing redundant label into the original sentimental label set and organizing all label into a hierarchy our method can add redundant feature into training for capturing the label redundancy the experimental result prove that our method outperforms the traditional method like nb svm maxent and standard chain crfs in comparison with the cascaded model our method can effectively alleviate the error propagation among different layer and obtain better performance in each layer 
the paper present a novel sentence trimmer in japanese which combine a non statistical yet generic tree generation model and conditional random field crfs to address improving the grammaticality of compression while retaining it relevance experiment found that the present approach outperforms in grammaticality and in relevance a dependency centric approach oguro et al morooka et al yamagata et al fukutomi et al the only line of work in prior literature on japanese compression we are aware of that allows replication and permit a direct comparison 
in this paper we develop a story generator that leverage knowledge inherent in corpus without requiring extensive manual involvement a key feature in our approach is the reliance on a story planner which we acquire automatically by recording event their participant and their precedence relationship in a training corpus contrary to previous work our system doe not follow a generate and rank architecture instead we employ evolutionary search technique to explore the space of possible story which we argue are well suited to the story generation task experiment on generating simple child s story show that our system outperforms previous data driven approach 
for chinese po tagging word segmentation is a preliminary step to avoid error propagation and improve segmentation by utilizing po information segmentation and tagging can be performed simultaneously a challenge for this joint approach is the large combined search space which make efficient decoding very hard recent research ha explored the integration of segmentation and po tagging by decoding under restricted version of the full combined search space in this paper we propose a joint segmentation and po tagging model that doe not impose any hard constraint on the interaction between word and po information fast decoding is achieved by using a novel multiple beam search algorithm the system us a discriminative statistical model trained using the generalized perceptron algorithm the joint model give an error reduction in segmentation accuracy of and an error reduction in tagging accuracy of compared to the traditional pipeline approach 
we demonstrate the effectiveness of multilingual learning for unsupervised part of speech tagging the key hypothesis of multilingual learning is that by combining cue from multiple language the structure of each becomes more apparent we formulate a hierarchical bayesian model for jointly predicting bilingual stream of part of speech tag the model learns language specific feature while capturing cross lingual pattern in tag distribution for aligned word once the parameter of our model have been learned on bilingual parallel data we evaluate it performance on a held out monolingual test set our evaluation on six pair of language show consistent and significant performance gain over a state of the art monolingual baseline for one language pair we observe a relative reduction in error of 
for century the deep connection between language ha brought about major discovery about human communication in this paper we investigate how this powerful source of information can be exploited for unsupervised language learning in particular we study the task of morphological segmentation of multiple language we present a nonparametric bayesian model that jointly induces morpheme segmentation of each language under consideration and at the same time identifies cross lingual morpheme pattern or abstract morpheme we apply our model to three semitic language arabic hebrew aramaic a well a to english our result demonstrate that learning morphological model in tandem reduces error by up to relative to monolingual model furthermore we provide evidence that our joint model achieves better performance when applied to language from the same family 
minimum error rate training is the algorithm for log linear model parameter training most used in state of the art statistical machine translation system in it original formulation the algorithm us n best list output by the decoder to grow the translation pool that shape the surface on which the actual optimization is performed recent work ha been done to extend the algorithm to use the entire translation lattice built by the decoder instead of n best list we propose here a third intermediate way consisting in growing the translation pool using sample randomly drawn from the translation lattice we empirically measure a systematic improvement in the bleu score compared to training using n best list without suffering the increase in computational complexity associated with operating with the whole lattice 
we present a novel semi supervised training algorithm for learning dependency parser by combining a supervised large margin loss with an unsupervised least square loss a discriminative convex semi supervised learning algorithm can be obtained that is applicable to large scale problem to demonstrate the benefit of this approach we apply the technique to learning dependency parser from combined labeled and unlabeled corpus using a stochastic gradient descent algorithm a parsing model can be efficiently learned from semi supervised data that significantly outperforms corresponding supervised method 
we present the first evaluation of the utility of automatic evaluation metric on surface realization of penn treebank data using output of the openccg and xle realizers along with ranked wordnet synonym substitution we collected a corpus of generated surface realization these output were then rated and post edited by human annotator we evaluated the realization using seven automatic metric and analyzed correlation obtained between the human judgment and the automatic score in contrast to previous nlg meta evaluation we find that several of the metric correlate moderately well with human judgment of both adequacy and fluency with the ter family performing best overall we also find that all of the metric correctly predict more than half of the significant systemlevel difference though none are correct in all case we conclude with a discussion of the implication for the utility of such metric in evaluating generation in the presence of variation a further result of our research is a corpus of post edited realization which will be made available to the research community 
many factor are thought to increase the chance of misrecognizing a word in asr including low frequency nearby disfluency short duration and being at the start of a turn however few of these factor have been formally examined this paper analyzes a variety of lexical prosodic and disfluency factor to determine which are likely to increase asr error rate finding include the following for disfluency effect depend on the type of disfluency error increase by up to absolute for word near fragment but decrease by up to absolute for word near repetition this decrease seems to be due to longer word duration for prosodic feature there are more error for word with extreme value than word with typical value although our result are based on output from a system with speaker adaptation speaker difference are a major factor influencing error rate and the effect of feature such a frequency pitch and intensity may vary between speaker 
one major bottleneck in conversational system is their incapability in interpreting unexpected user language input such a out of vocabulary word to overcome this problem conversational system must be able to learn new word automatically during human machine conversation motivated by psycholinguistic finding on eye gaze and human language processing we are developing technique to incorporate human eye gaze for automatic word acquisition in multimodal conversational system this paper investigates the use of temporal alignment between speech and eye gaze and the use of domain knowledge in word acquisition our experiment result indicate that eye gaze provides a potential channel for automatically acquiring new word the use of extra temporal and domain knowledge can significantly improve acquisition performance 
this paper improves the use of pseudo word a an evaluation framework for selectional preference while pseudo word originally evaluated word sense disambiguation they are now commonly used to evaluate selectional preference a selectional preference model rank a set of possible argument for a verb by their semantic fit to the verb pseudo word serve a a proxy evaluation for these decision the evaluation take an argument of a verb like drive e g car pair it with an alternative word e g car rock and asks a model to identify the original this paper study two main aspect of pseudoword creation that affect performance result pseudo word evaluation often evaluate only a subset of the word we show that selectional preference should instead be evaluated on the data in it entirety different approach to selecting partner word can produce overly optimistic evaluation we offer suggestion to address these factor and present a simple baseline that outperforms the state of the art by absolute on a newspaper domain 
the research focus of computational coreference resolution ha exhibited a shift from heuristic approach to machine learning approach in the past decade this paper survey the major milestone in supervised coreference research since it inception fifteen year ago 
the viterbi algorithm is the conventional decoding algorithm most widely adopted for sequence labeling viterbi decoding is however prohibitively slow when the label set is large because it time complexity is quadratic in the number of label this paper proposes an exact decoding algorithm that overcomes this problem a novel property of our algorithm is that it efficiently reduces the label to be decoded while still allowing u to check the optimality of the solution experiment on three task po tagging joint po tagging and chunking and supertagging show that the new algorithm is several order of magnitude faster than the basic viterbi and a state of the art algorithm carpediem esposito and radicioni 
we propose an automatic machine translation mt evaluation metric that calculates a similarity score based on precision and recall of a pair of sentence unlike most metric we compute a similarity score between item across the two sentence we then find a maximum weight matching between the item such that each item in one sentence is mapped to at most one item in the other sentence this general framework allows u to use arbitrary similarity function between item and to incorporate different information in our comparison such a n gram dependency relation etc when evaluated on data from the acl mt workshop our proposed metric achieves higher correlation with human judgement than all automatic mt evaluation metric that were evaluated during the workshop 
we present a method for extracting social network from literature namely nineteenth century british novel and serial we derive the network from dialogue interaction and thus our method depends on the ability to determine when two character are in conversation our approach involves character name chunking quoted speech attribution and conversation detection given the set of quote we extract feature from the social network and examine their correlation with one another a well a with metadata such a the novel s setting our result provide evidence that the majority of novel in this time period do not fit two characterization provided by literacy scholar instead our result suggest an alternative explanation for difference in social network 
this paper proposes a convolution forest kernel to effectively explore rich structured feature embedded in a packed parse forest a opposed to the convolution tree kernel the proposed forest kernel doe not have to commit to a single best parse tree is thus able to explore very large object space and much more structured feature embedded in a forest this make the proposed kernel more robust against parsing error and data sparseness issue than the convolution tree kernel the paper present the formal definition of convolution forest kernel and also illustrates the computing algorithm to fast compute the proposed convolution forest kernel experimental result on two nlp application relation extraction and semantic role labeling show that the proposed forest kernel significantly outperforms the baseline of the convolution tree kernel 
stochastic inversion transduction grammar constitute a powerful formalism in machine translation for which an efficient dynamic programming parsing algorithm exists in this work we review this parsing algorithm and propose important modification that enlarge the search space these modification allow the parsing algorithm to search for more and better solution 
in this paper we propose a novel approach to automatic generation of summary template from given collection of summary article this kind of summary template can be useful in various application we first develop an entity aspect lda model to simultaneously cluster both sentence and word into aspect we then apply frequent subtree pattern mining on the dependency parse tree of the clustered and labeled sentence to discover sentence pattern that well represent the aspect key feature of our method include automatic grouping of semantically related sentence pattern and automatic identification of template slot that need to be filled in we apply our method on five wikipedia entity category and compare our method with two baseline method both quantitative evaluation based on human judgment and qualitative comparison demonstrate the effectiveness and advantage of our method 
we propose a new approach to language modeling which utilizes discriminative learning method our approach is an iterative one starting with an initial language model in each iteration we generate false sentence from the current model and then train a classifier to discriminate between them and sentence from the training corpus to the extent that this succeeds the classifier is incorporated into the model by lowering the probability of sentence classified a false and the process is repeated we demonstrate the effectiveness of this approach on a natural language corpus and show it provides an improvement in perplexity over a modified kneser ney smoothed trigram 
entropy guided transformation learning etl is a new machine learning strategy that combine the advantage of decision tree dt and transformation based learning tbl in this work we apply the etl framework to four phrase chunking task portuguese noun phrase chunking english base noun phrase chunking english text chunking and hindi text chunking in all four task etl show better result than decision tree and also than tbl with hand crafted template etl provides a new training strategy that accelerates transformation learning for the english text chunking task this corresponds to a factor of five speedup for portuguese noun phrase chunking etl show the best reported result for the task for the other three linguistic task etl show state of theart competitive result and maintains the advantage of using a rule based system 
this paper proposes a novel method that exploit multiple resource to improve statistical machine translation smt based paraphrasing in detail a phrasal paraphrase table and a feature function are derived from each resource which are then combined in a log linear smt model for sentence level paraphrase generation experimental result show that the smt based paraphrasing model can be enhanced using multiple resource the phrase level and sentence level precision of the generated paraphrase are above and respectively in addition the contribution of each resource is evaluated which indicates that all the exploited resource are useful for generating paraphrase of high quality 
computing the pairwise semantic similarity between all word on the web is a computationally challenging task parallelization and optimization are necessary we propose a highly scalable implementation based on distributional similarity implemented in the mapreduce framework and deployed over a billion word crawl of the web the pairwise similarity between million term is computed in hour using quad core node we apply the learned similarity matrix to the task of automatic set expansion and present a large empirical study to quantify the effect on expansion performance of corpus size corpus quality seed composition and seed size we make public an experimental testbed for set expansion analysis that includes a large collection of diverse entity set extracted from wikipedia 
in this paper we present a supervised method for back of the book index construction we introduce a novel set of feature that go beyond the typical frequency based analysis including feature based on discourse comprehension syntactic pattern and information drawn from an online encyclopedia in experiment carried out on a book collection the method wa found to lead to an improvement of roughly a compared to an existing state of the art supervised method 
we present a novel approach to weakly supervised semantic class learning from the web using a single powerful hyponym pattern combined with graph structure which capture two property associated with pattern based extraction popularity and productivity intuitively a candidate is popular if it wa discovered many time by other instance in the hyponym pattern a candidate is productive if it frequently lead to the discovery of other instance together these two measure capture not only frequency of occurrence but also cross checking that the candidate occurs both near the class name and near other class member we developed two algorithm that begin with just a class name and one seed instance and then automatically generate a ranked list of new class instance we conducted experiment on four semantic class and consistently achieved high accuracy 
many statistical translation model can be regarded a weighted logical deduction under this paradigm we use weight from the expectation semiring eisner to compute first order statistic e g the expected hypothesis length or feature count over packed forest of translation lattice or hypergraphs we then introduce a novel second order expectation semiring which computes second order statistic e g the variance of the hypothesis length or the gradient of entropy this second order semiring is essential for many interesting training paradigm such a minimum risk deterministic annealing active learning and semi supervised learning where gradient descent optimization requires computing the gradient of entropy or risk we use these semirings in an open source machine translation toolkit joshua enabling minimum risk training for a benefit of up to bleu point 
english noun verb n v pair contract cement have undergone complex pattern of change between stress pattern for several century we describe a longitudinal dataset of n v pair pronunciation leading to a set of property to be accounted for by any computational model we analyze the dynamic of dynamical system model of linguistic population each derived from a model of learning by individual we compare each model s dynamic to a set of property observed in the n v data and reason about how assumption about individual learning affect population level dynamic 
hierarchical hmm hhmm parser make promising cognitive model while they use a bounded model of working memory and pursue incremental hypothesis in parallel they still achieve parsing accuracy competitive with chart based technique this paper aim to validate that a right corner hhmm parser is also able to produce complexity metric which quantify a reader s incremental difficulty in understanding a sentence besides defining standard metric in the hhmm framework a new metric embedding difference is also proposed which test the hypothesis that hhmm store element represents syntactic working memory result show that hhmm surprisal outperforms all other evaluated metric in predicting reading time and that embedding difference make a significant independent contribution 
chinese abbreviation are widely used in modern chinese text compared with english abbreviation which are mostly acronym and truncation the formation of chinese abbreviation is much more complex due to the richness of chinese abbreviation many of them may not appear in available parallel corpus in which case current machine translation system simply treat them a unknown word and leave them untranslated in this paper we present a novel unsupervised method that automatically extractsthe relation between a full form phrase and it abbreviation from monolingual corpus and induces translation entry for the abbreviation by using it full form a a bridge our method doe not requireany additionalannotateddata other than the data that a regular translation system us we integrate our method into a state ofthe art baseline translation system and show that it consistently improves the performance of the baseline system on various nist mt test set 
in state of the art approach to information extraction ie dependency graph constitute the fundamental data structure for syntactic structuring and subsequent knowledge elicitation from natural language document the top performing system in the bionlp shared task on event extraction all shared the idea to use dependency structure generated by a variety of parser either directly or in some converted manner and optionally modified their output to fit the special need of ie a there are systematic difference between various dependency representation being used in this competition we scrutinize on different encoding style for dependency information and their possible impact on solving several ie task after assessing more or le established dependency representation such a the stanford and conll x dependency we will then focus on trimming operation that pave the way to more effective ie our evaluation study cover data from a number of constituencyand dependency based parser and provides experimental evidence which dependency representation are particularly beneficial for the event extraction task based on empirical finding from our study we were able to achieve the performance of f score on the development data set of the bionlp shared task 
this paper introduces dual decomposition a a framework for deriving inference algorithm for nlp problem the approach relies on standard dynamic programming algorithm a oracle solver for sub problem together with a simple method for forcing agreement between the different oracle the approach provably solves a linear programming lp relaxation of the global inference problem it lead to algorithm that are simple in that they use existing decoding algorithm efficient that they avoid exact algorithm for the full model and often exact in that empirically they often recover the correct solution in spite of using an lp relaxation we give experimental result on two problem the combination of two lexicalized parsing model and the combination of a lexicalized parsing model and a trigram part of speech tagger 
we describe the semi automatic adaptation of a timeml annotated corpus from english to portuguese a language for which timeml annotated data wa not available yet in order to validate this adaptation we use the obtained data to replicate some result in the literature that used the original english data the fact that comparable result are obtained indicates that our approach can be used successfully to rapidly create semantically annotated resource for new language 
the analysis of reading time can provide insight into the process that underlie language comprehension with longer reading time indicating greater cognitive load there is evidence that the language processor is highly predictive such that prior context allows upcoming linguistic material to be anticipated previous work ha investigated the contribution of semantic and syntactic context in isolation essentially treating them a independent factor in this paper we analyze reading time in term of a single predictive measure which integrates a model of semantic composition with an incremental parser and a language model 
this work introduces a model free approach to sentence compression which grew out of idea from nomoto and examines how it compare to a state of art model intensive approach known a tree to tree transducer or t cohn and lapata it is found that a model free approach significantly outperforms t on the particular data we created from the internet we also discus what might have caused t s poor performance 
current statistical machine translation smt system are trained on sentence aligned and word aligned parallel text collected from various source translation model parameter are estimated from the word alignment and the quality of the translation on a given test set depends on the parameter estimate there are at least two factor affecting the parameter estimation domain match and training data quality this paper describes a novel approach for automatically detecting and down weighing certain part of the training corpus by assigning a weight to each sentence in the training bitext so a to optimize a discriminative objective function on a designated tuning set this way the proposed method can limit the negative effect of low quality training data and can adapt the translation model to the domain of interest it is shown that such discriminative corpus weight can provide significant improvement in arabic english translation on various condition using a state of the art smt system 
statistical machine learning method are employed to train a named entity recognizer from annotated data method like maximum entropy and conditional random field make use of feature for the training purpose these method tend to overfit when the available training corpus is limited especially if the number of feature is large or the number of value for a feature is large to overcome this we proposed two technique for feature reduction based on word clustering and selection a number of word similarity measure are proposed for clustering word for the named entity recognition task a few corpus based statistical measure are used for important word selection the feature reduction technique lead to a substantial performance improvement over baseline maximum entropy technique 
we present minimum bayes risk mbr decoding over translation lattice that compactly encode a huge number of translation hypothesis we describe condition on the loss function that will enable efficient implementation of mbr decoder on lattice we introduce an approximation to the bleu score papineni et al that satisfies these condition the mbr decoding under this approximate bleu is realized using weighted finite state automaton our experiment show that the lattice mbr decoder yield moderate consistent gain in translation performance over n best mbr decoding on arabic to english chinese to english and english to chinese translation task we conduct a range of experiment to understand why lattice mbr improves upon n best mbr and study the impact of various parameter on mbr performance 
many approach to unsupervised morphology acquisition incorporate the frequency of character sequence with respect to each other to identify word stem and affix this typically involves heuristic search procedure and calibrating multiple arbitrary threshold we present a simple approach that us no threshold other than those involved in standard application of x significance testing a key part of our approach is using document boundary to constrain generation of candidate stem and affix and clustering morphological variant of a given word stem we evaluate our model on english and the mayan language uspanteko it compare favorably to two benchmark system which use considerably more complex strategy and rely more on experimentally chosen threshold value 
in this paper we develop multilingual supervised latent dirichlet allocation mlslda a probabilistic generative model that allows insight gleaned from one language s data to inform how the model capture property of other language mlslda accomplishes this by jointly modeling two aspect of text how multilingual concept are clustered into thematically coherent topic and how topic associated with text connect to an observed regression variable such a rating on a sentiment scale concept are represented in a general hierarchical framework that is flexible enough to express semantic ontology dictionary clustering constraint and a a special degenerate case conventional topic model both the topic and the regression are discovered via posterior inference from corpus we show mlslda can build topic that are consistent across language discover sensible bilingual lexical correspondence and leverage multilingual corpus to better predict sentiment 
paraphrase pattern are useful in paraphrase recognition and generation in this paper we present a pivot approach for extracting paraphrase pattern from bilingual parallel corpus whereby the english paraphrase pattern are extracted using the sentence in a foreign language a pivot we propose a loglinear model to compute the paraphrase likelihood of two pattern and exploit feature function based on maximum likelihood estimation mle and lexical weighting lw using the presented method we extract over pair of paraphrase pattern from m bilingual sentence pair the precision of which exceeds the evaluation result show that the pivot approach is effective in extracting paraphrase pattern which significantly outperforms the conventional method dirt especially the log linear model with the proposed feature function achieves high performance the coverage of the extracted paraphrase pattern is high which is above the extracted paraphrase pattern can be classified into type which are useful in various application 
this paper examines how a new class of nonparametric bayesian model can be effectively applied to an open domain event coreference task designed with the purpose of clustering complex linguistic object these model consider a potentially infinite number of feature and categorical outcome the evaluation performed for solving both withinand cross document event coreference show significant improvement of the model when compared against two baseline for this task 
statistical translation model that try to capture the recursive structure of language have been widely adopted over the last few year these model make use of varying amount of information from linguistic theory some use none at all some use information about the grammar of the target language some use information about the grammar of the source language but progress ha been slower on translation model that are able to learn the relationship between the grammar of both the source and target language we discus the reason why this ha been a challenge review existing attempt to meet this challenge and show how some old and new idea can be combined into a simple approach that us both source and target syntax for significant improvement in translation accuracy 
this paper employ morphological structure and relation between sentence segment for opinion analysis on word and sentence chinese word are classified into eight morphological type by two proposed classifier crf classifier and svm classifier experiment show that the injection of morphological information improves the performance of the word polarity detection to utilize syntactic structure we annotate structural trio to represent relation between sentence segment experiment show that considering structural trio is useful for sentence opinion analysis the best f score achieves for opinion word extraction for opinion word polarity detection for opinion sentence extraction and for opinion sentence polarity detection 
using multi layer neural network to estimate the probability of word sequence is a promising research area in statistical language modeling with application in speech recognition and statistical machine translation however training such model for large vocabulary task is computationally challenging which doe not scale easily to the huge corpus that are nowadays available in this work we study the performance and behavior of two neural statistical language model so a to highlight some important caveat of the classical training algorithm the induced word embeddings for extreme case are also analysed thus providing insight into the convergence issue a new initialization scheme and new training technique are then introduced these method are shown to greatly reduce the training time and to significantly improve performance both in term of perplexity and on a large scale translation task 
we describe a model for the lexical analysis of arabic text using the list of alternative supplied by a broad coverage morphological analyzer sama which include stable lemma id that correspond to combination of broad word sense category and po tag we break down each of the hundred of thousand of possible lexical label into it constituent element including lemma id and part of speech feature are computed for each lexical token based on it local and document level context and used in a novel simple and highly efficient two stage supervised machine learning algorithm that overcomes the extreme sparsity of label distribution in the training data the resulting system achieves accuracy of for it first choice and for it top two choice in selecting among the alternative provided by the sama lexical analyzer we have successfully used this system in application such a an online reading helper for intermediate learner of the arabic language and a tool for improving the productivity of arabic treebank annotator 
coreference system are driven by syntactic semantic and discourse constraint we present a simple approach which completely modularizes these three aspect in contrast to much current work which focus on learning and on the discourse component our system is deterministic and is driven entirely by syntactic and semantic compatibility a learned from a large unlabeled corpus despite it simplicity and discourse naivete our system substantially outperforms all unsupervised system and most supervised one primary contribution include the presentation of a simple to reproduce high performing baseline and the demonstration that most remaining error can be attributed to syntactic and semantic factor external to the coreference phenomenon and perhaps best addressed by non coreference system 
finding allowable place in word to insert hyphen is an important practical problem the algorithm that is used most often nowadays ha remained essentially unchanged for year this method is the tex hyphenation algorithm of knuth and liang we present here a hyphenation method that is clearly more accurate the new method is an application of conditional random field we create new training set for english and dutch from the celex european lexical resource and achieve error rate for english of le than for correctly allowed hyphen and le than for dutch experiment show that both the knuth liang method and a leading current commercial alternative have error rate several time higher for both language 
this paper describes a lexical trigger model for statistical machine translation we present various method using triplet incorporating long distance dependency that can go beyond the local context of phrase or n gram based language model we evaluate the presented method on two translation task in a reranking framework and compare it to the related ibm model we show slightly improved translation quality in term of bleu and ter and address various constraint to speed up the training based on expectation maximization and to lower the overall number of triplet without loss in translation performance 
the computation of selectional preference the admissible argument value for a relation is a well known nlp task with broad applicability we present lda sp which utilizes linklda erosheva et al to model selectional preference by simultaneously inferring latent topic and topic distribution over relation lda sp combine the benefit of previous approach like traditional class based approach it produce human interpretable class describing each relation s preference but it is competitive with non class based method in predictive power we compare lda sp to several state of the art method achieving an increase in recall at precision over mutual information erk we also evaluate lda sp s effectiveness at filtering improper application of inference rule where we show substantial improvement over pantel et al s system pantel et al 
in contrast with the booming increase of internet data state of art qa question answering system otherwise concerned data from specific domain or resource such a search engine snippet online forum and wikipedia in a somewhat isolated way user may welcome a more general qa system for it capability to answer question of various source integrated from existed specialized sub qa engine in this framework question classification is the primary task however the current paradigm of question classification were focused on some specified type of question i e factoid question which are inappropriate for the general qa in this paper we propose a new question classification paradigm which includes a question taxonomy suitable to the general qa and a question classifier based on mln markov logic network where rule based method and statistical method are unified into a single framework in a fuzzy discriminative learning approach experiment show that our method outperforms traditional question classification approach 
in this paper we investigate structured model for document level sentiment classification when predicting the sentiment of a subjective document e g a positive or negative it is well known that not all sentence are equally discriminative or informative but identifying the useful sentence automatically is itself a difficult learning problem this paper proposes a joint two level approach for document level sentiment classification that simultaneously extract useful i e subjective sentence and predicts document level sentiment based on the extracted sentence unlike previous joint learning method for the task our approach doe not rely on gold standard sentence level subjectivity annotation which may be expensive to obtain and optimizes directly for document level performance empirical evaluation on movie review and u s congressional floor debate show improved performance over previous approach 
recurrent event query req constitute a special class of search query occurring at regular predictable time interval the freshness of document ranked for such query is generally of critical importance req form a significant volume a much a of query traffic received by search engine in this work we develop an improved req classifier that could provide significant improvement in addressing this problem we analyze req query and develop novel feature from multiple source and evaluate them using machine learning technique from historical query log we develop feature utilizing query frequency click information and user intent dynamic within a search session we also develop temporal feature by time series analysis from query frequency other generated feature include word matching with recurrent event seed word and time sensitivity of search result set we use naive bayes svm and decision tree based logistic regression model to train req classifier the result on test data show that our model outperformed baseline approach significantly experiment on a commercial web search engine also show significant gain in overall relevance and thus overall user experience 
multi category bootstrapping algorithm were developed to reduce semantic drift by extracting multiple semantic lexicon simultaneously a category s search space may be restricted the best result have been achieved through reliance on manually crafted negative category unfortunately identifying these category is non trivial and their use shift the unsupervised bootstrapping paradigm towards a supervised framework we present neg finder the first approach for discovering negative category automatically neg finder exploit unsupervised term clustering to generate multiple negative category during bootstrapping our algorithm effectively remove the necessity of manual intervention and formulation of negative category with performance closely approaching that obtained using negative category defined by a domain expert 
chinese is a language that doe not have morphological tense marker that provide explicit grammaticalization of the temporal location of situation event or state however in many nlp application such a machine translation information extraction and question answering it is desirable to make the temporal location of the situation explicit we describe a machine learning framework where different source of information can be combined to predict the temporal location of situation in chinese text our experiment show that this approach significantly outperforms the most frequent tense baseline more importantly the high training accuracy show promise that this challenging problem is solvable to a level where it can be used in practical nlp application with more training data better modeling technique and more informative and generalizable feature 
we present a new approach to cross language text classification that build on structural correspondence learning a recently proposed theory for domain adaptation the approach us unlabeled document along with a simple word translation oracle in order to induce task specific cross lingual word correspondence we report on analysis that reveal quantitative insight about the use of unlabeled data and the complexity of inter language correspondence modeling we conduct experiment in the field of cross language sentiment classification employing english a source language and german french and japanese a target language the result are convincing they demonstrate both the robustness and the competitiveness of the presented idea 
event anaphora resolution is an important task for cascaded event template extraction and other nlp study previous study only touched on event pronoun resolution in this paper we provide the first systematic study to resolve event noun phrase to their verbal mention crossing long distance our study show various lexical syntactic and positional feature are needed for event noun phrase resolution and most of them such a morphology relation synonym and etc are different from those feature used for conventional noun phrase resolution syntactic structural information in the parse tree modeled with tree kernel is combined with the above diverse flat feature using a composite kernel which show more than f score improvement over the flat feature baseline in addition we employed a twin candidate based model to capture the pair wise candidate preference knowledge which further demonstrates a statistically significant improvement all the above contributes to an encouraging performance of f score on ontonotes corpus 
word sense disambiguation remains one of the most complex problem facing computational linguist to date in this paper we present a system that combine evidence from a monolingual wsd system together with that from a multilingual wsd system to yield state of the art performance on standard all word data set the monolingual system is based on a modification of the graph based state of the art algorithm in degree the multilingual system is an improvement over an all word unsupervised approach salaam salaam exploit multilingual evidence a a mean of disambiguation in this paper we present modification to both of the original approach and then their combination we finally report the highest result obtained to date on the senseval standard data set using an unsupervised method we achieve an overall f measure of using a voting scheme 
the peco framework is a knowledge representation for formulating clinical question query are decomposed into four aspect which are patient problem p exposure e comparison c and outcome o however no test collection is available to evaluate such framework in information retrieval in this work we first present the construction of a large test collection extracted from systematic literature review we then describe an analysis of the distribution of peco element throughout the relevant document and propose a language modeling approach that us these distribution a a weighting strategy in our experiment carried out on a collection of million document and query our method wa found to lead to an improvement of in map and in p a compared to the state of the art method 
a word in one language can be translated to zero one or several word in other language using word fertility feature ha been shown to be useful in building word alignment model for statistical machine translation we built a fertility hidden markov model by adding fertility to the hidden markov model this model not only achieves lower alignment error rate than the hidden markov model but also run faster it is similar in some way to ibm model but is much easier to understand we use gibbs sampling for parameter estimation which is more principled than the neighborhood method used in ibm model 
this paper proposes a novel method to refine the grammar in parsing by utilizing semantic knowledge from hownet based on the hierarchical state split approach which can refine grammar automatically in a data driven manner this study introduces semantic knowledge into the splitting process at two step firstly each part of speech node will be annotated with a semantic tag of it terminal word these new tag generated in this step are semantic related which can provide a good start for splitting secondly a knowledge based criterion is used to supervise the hierarchical splitting of these semantic related tag which can alleviate overfitting the experiment are carried out on both chinese and english penn treebank show that the refined grammar with semantic knowledge can improve parsing performance significantly especially with respect to chinese our parser achieves an f score of which is the best published result we are aware of 
in vehicle dialogue system often contain more than one application e g a navigation and a telephone application this mean that the user might for example interrupt the interaction with the telephone application to ask for direction from the navigation application and then resume the dialogue with the telephone application in this paper we present an analysis of interruption and resumption behaviour in human human in vehicle dialogue and also propose some implication for resumption strategy in an in vehicle dialogue system 
morphological process in semitic language deliver space delimited word which introduce multiple distinct syntactic unit into the structure of the input sentence these word are in turn highly ambiguous breaking the assumption underlying most parser that the yield of a tree for a given sentence is known in advance here we propose a single joint model for performing both morphological segmentation and syntactic disambiguation which bypass the associated circularity using a treebank grammar a data driven lexicon and a linguistically motivated unknown token handling technique our model outperforms previous pipelined integrated or factorized system for hebrew morphological and syntactic processing yielding an error reduction of over the best published result so far 
hierarchical phrase based hpb translation provides a powerful mechanism to capture both short and long distance phrase reordering however the phrase reordering lack of contextual information in conventional hpb system this paper proposes a context dependent phrase reordering approach that us the maximum entropy maxent model to help the hpb decoder select appropriate reordering pattern we classify translation rule into several reordering pattern and build a maxent model for each pattern based on various contextual feature we integrate the maxent model into the hpb model experimental result show that our approach achieves significant improvement over a standard hpb system on large scale translation task on chinese to english translation the absolute improvement in bleu case insensitive range from to 
this paper is concerned with the problem of question search in question search given a question a query we are to return question semantically equivalent or close to the queried question in this paper we propose to conduct question search by identifying question topic and question focus more specifically we first summarize question in a data structure consisting of question topic and question focus then we model question topic and question focus in a language modeling framework for search we also propose to use the mdlbased tree cut model for identifying question topic and question focus automatically experimental result indicate that our approach of identifying question topic and question focus for search significantly outperforms the baseline method such a vector space model vsm and language model for information retrieval lmir 
we propose a novel language independent approach for improving statistical machine translation for resource poor language by exploiting their similarity to resource rich one more precisely we improve the translation from a resource poor source language x into a resource rich language y given a bi text containing a limited number of parallel sentence for x y and a larger bi text for x y for some resource rich language x that is closely related to x the evaluation for indonesian english using malay and spanish english using portuguese and pretending spanish is resource poor show an absolute gain of up to and bleu point respectively which is an improvement over the rivaling approach while using much le additional data 
we define a new formalism based on sikkel s parsing schema for constituency parser that can be used to describe analyze and compare dependency parsing algorithm this abstraction allows u to establish clear relation between several existing projective dependency parser and prove their correctness 
this paper introduces a new kernel which computes similarity between two natural language sentence a the number of path shared by their dependency tree the paper give a very efficient algorithm to compute it this kernel is also an improvement over the word subsequence kernel because it only count linguistically meaningful word subsequence which are based on word dependency it overcomes some of the difficulty encountered by syntactic tree kernel a well experimental result demonstrate the advantage of this kernel over word subsequence and syntactic tree kernel 
this paper present a semi supervised training method for linear chain conditional random field that make use of labeled feature rather than labeled instance this is accomplished by using generalized expectation criterion to express a preference for parameter setting in which the model s distribution on unlabeled data match a target distribution we induce target conditional probability distribution of label given feature from both annotated feature occurrence in context and adhoc feature majority label assignment the use of generalized expectation criterion allows for a dramatic reduction in annotation time by shifting from traditional instance labeling to feature labeling and the method presented outperform traditional crf training and other semi supervised method when limited human effort is available 
automatic processing of medical dictation pose a significant challenge we approach the problem by introducing a statistical framework capable of identifying type and boundary of section list and other structure occurring in a dictation thereby gaining explicit knowledge about the function of such element training data is created semi automatically by aligning a parallel corpus of corrected medical report and corresponding transcript generated via automatic speech recognition we highlight the property of our statistical framework which is based on conditional random field crfs and implemented a an efficient publicly available toolkit finally we show that our approach is effective both under ideal condition and for real life dictation involving speech recognition error and speech related phenomenon such a hesitation and repetition 
in this paper we develop an approach to tackle the problem of verb selection for learner of english a a second language esl by using feature from the output of semantic role labeling srl unlike existing approach to verb selection that use local feature such a n gram our approach exploit semantic feature which explicitly model the usage context of the verb the verb choice highly depends on it usage context which is not consistently captured by local feature we then combine these semantic feature with other local feature under the generalized perceptron learning framework experiment on both indomain and out of domain corpus show that our approach outperforms the baseline and achieves state of the art performance 
we observe that how a given named entity ne is translated i e either semantically or phonetically depends greatly on it associated entity type and entity within an aligned pair should share the same type also those initially detected ne are anchor whose information should be used to give certainty score when selecting candidate from this basis an integrated model is thus proposed in this paper to jointly identify and align bilingual named entity between chinese and english it adopts a new mapping type ratio feature which is the proportion of ne internal token that are semantically translated enforces an entity type consistency constraint and utilizes additional monolingual candidate certainty factor based on those ne anchor the experiment show that this novel approach ha substantially raised the type sensitive f score of identified ne pair from to f score imperfection reduction in our chinese english ne alignment task 
previous study of data driven dependency parsing have shown that the distribution of parsing error are correlated with theoretical property of the model used for learning and inference in this paper we show how these result can be exploited to improve parsing accuracy by integrating a graph based and a transition based model by letting one model generate feature for the other we consistently improve accuracy for both model resulting in a significant improvement of the state of the art when evaluated on data set from the conll x shared task 
this paper present a method for the automatic discovery of manner relation from text an extended definition of manner is proposed including restriction on the sort of concept that can be part of it domain and range the connection with other relation and the lexico syntactic pattern that encode manner are analyzed a new feature set specialized on manner detection is depicted and justified experimental result show improvement over previous attempt to extract manner combination of manner with other semantic relation are also discussed 
we propose a novel lexicon acquirer that work in concert with the morphological analyzer and ha the ability to run in online mode every time a sentence is analyzed it detects unknown morpheme enumerates candidate and selects the best candidate by comparing multiple example kept in the storage when a morpheme is unambiguously selected the lexicon acquirer update the dictionary of the analyzer and it will be used in subsequent analysis we use the constraint of japanese morphology and effectively reduce the number of example required to acquire a morpheme experiment show that unknown morpheme were acquired with high accuracy and improved the quality of morphological analysis 
this paper present a two stage approach to summarizing multiple contrastive viewpoint in opinionated text in the first stage we use an unsupervised probabilistic approach to model and extract multiple viewpoint in text we experiment with a variety of lexical and syntactic feature yielding significant performance gain over bag of word feature set in the second stage we introduce comparative lexrank a novel random walk formulation to score sentence and pair of sentence from opposite viewpoint based on both their representativeness of the collection a well a their contrastiveness with each other experimental result show that the proposed approach can generate informative summary of viewpoint in opinionated text 
incremental parsing technique such a shift reduce have gained popularity thanks to their efficiency but there remains a major problem the search is greedy and only explores a tiny fraction of the whole space even with beam search a opposed to dynamic programming we show that surprisingly dynamic programming is in fact possible for many shift reduce parser by merging equivalent stack based on feature value empirically our algorithm yield up to a five fold speedup over a state of the art shift reduce dependency parser with no loss in accuracy better search also lead to better learning and our final parser outperforms all previously reported dependency parser for english and chinese yet is much faster 
in this paper we propose a novel statistical language model to capture long range semantic dependency specifically we apply the concept of semantic composition to the problem of constructing predictive history representation for upcoming word we also examine the influence of the underlying semantic space on the composition task by comparing spatial semantic representation against topic based one the composition model yield reduction in perplexity when combined with a standard n gram language model over the n gram model alone we also obtain perplexity reduction when integrating our model with a structured language model 
we propose the use of regular tree grammar rtgs a a formalism for the underspecified processing of scope ambiguity by applying standard result on rtgs we obtain a novel algorithm for eliminating equivalent reading and the first efficient algorithm for computing the best reading of a scope ambiguity we also show how to derive rtgs from more traditional underspecified description 
semantic role labeling srl ha proved to be a valuable tool for performing automatic analysis of natural language text currently however most system rely on a large training set which is manually annotated an effort that need to be repeated whenever different language or a different set of semantic role is used in a certain application a possible solution for this problem is semi supervised learning where a small set of training example is automatically expanded using unlabeled text we present the latent word language model which is a language model that learns word similarity from unlabeled text we use these similarity for different semi supervised srl method a additional feature or to automatically expand a small training set we evaluate the method on the propbank dataset and find that for small training size our best performing system achieves an error reduction of f measure compared to a state of the art supervised baseline 
there have been increasing need for task specific ranking in web search such a ranking for specific query segment like long query time sensitive query navigational query etc or ranking for specific domain content like answer blog news etc in the spirit of divide and conquer task specific ranking may have potential advantage over generic ranking since different task have task specific feature data distribution a well a feature grade correlation a critical problem for the task specific ranking is training data insufficiency which may be solved by using the data extracted from click log this paper empirically study how to appropriately exploit click data to improve rank function learning in task specific ranking the main contribution are the exploration on the utility of two promising approach for click pair extraction the analysis of the role played by the noise information which inevitably appears in click data extraction the appropriate strategy for combining training data and click data the comparison of click data which are consistent and inconsistent with baseline function 
discovering and summarizing opinion from online review is an important and challenging task a commonly adopted framework generates structured review summary with aspect and opinion recently topic model have been used to identify meaningful review aspect but existing topic model do not identify aspect specific opinion word in this paper we propose a maxent lda hybrid model to jointly discover both aspect and aspect specific opinion word we show that with a relatively small amount of training data our model can effectively identify aspect and opinion word simultaneously we also demonstrate the domain adaptability of our model 
question classification play an important role in question answering feature are the key to obtain an accurate question classifier in contrast to li and roth s approach which make use of very rich feature space we propose a compact yet effective feature set in particular we propose head word feature and present two approach to augment semantic feature of such head word using wordnet in addition lesk s word sense disambiguation wsd algorithm is adapted and the depth of hypernym feature is optimized with further augment of other standard feature such a unigrams our linear svm and maximum entropy me model reach the accuracy of and respectively over a standard benchmark dataset which outperform the best previously reported accuracy of 
part of speech po induction is one of the most popular task in research on unsupervised nlp many different method have been proposed yet comparison are difficult to make since there is little consensus on evaluation framework and many paper evaluate against only one or two competitor system here we evaluate seven different po induction system spanning nearly year of work using a variety of measure we show that some of the oldest and simplest system stand up surprisingly well against more recent approach since most of these system were developed and tested using data from the wsj corpus we compare their generalization ability by testing on both wsj and the multilingual multext east corpus finally we introduce the idea of evaluating system based on their ability to produce cluster prototype that are useful a input to a prototype driven learner in most case the prototype driven learner outperforms the unsupervised system used to initialize it yielding state of the art result on wsj and improvement on non english corpus 
the accuracy of current word sense disambiguation wsd system is affected by the fine grained sense inventory of wordnet a well a a lack of training example using the wsd example provided through ontonotes we conduct the first large scale wsd evaluation involving hundred of word type and ten of thousand of sense tagged example while adopting a coarse grained sense inventory we show that though wsd system trained with a large number of example can obtain a high level of accuracy they nevertheless suffer a substantial drop in accuracy when applied to a different domain to address this issue we propose combining a domain adaptation technique using feature augmentation with active learning our result show that this approach is effective in reducing the annotation effort required to adapt a wsd system to a new domain finally we propose that one can maximize the dual benefit of reducing the annotation effort while ensuring an increase in wsd accuracy by only performing active learning on the set of most frequently occurring word type 
one of the main obstacle to producing high quality joint model is the lack of jointly annotated data joint modeling of multiple natural language processing task outperforms single task model learned from the same data but still under performs compared to single task model learned on the more abundant quantity of available single task annotated data in this paper we present a novel model which make use of additional single task annotated data to improve the performance of a joint model our model utilizes a hierarchical prior to link the feature weight for shared feature in several single task model and the joint model experiment on joint parsing and named entity recognition using the ontonotes corpus show that our hierarchical joint model can produce substantial gain over a joint model trained on only the jointly annotated data 
semantic similarity is a central concept that extends across numerous field such a artificial intelligence natural language processing cognitive science and psychology accurate measurement of semantic similarity between word is essential for various task such a document clustering information retrieval and synonym extraction we propose a novel model of semantic similarity using the semantic relation that exist among word given two word first we represent the semantic relation that hold between those word using automatically extracted lexical pattern cluster next the semantic similarity between the two word is computed using a mahalanobis distance measure we compare the proposed similarity measure against previously proposed semantic similarity measure on miller charles benchmark dataset and wordsimilarity collection the proposed method outperforms all existing web based semantic similarity measure achieving a pearson correlation coefficient of on the millet charles dataset 
if we take an existing supervised nlp system a simple and general way to improve accuracy is to use unsupervised word representation a extra word feature we evaluate brown cluster collobert and weston embeddings and hlbl mnih hinton embeddings of word on both ner and chunking we use near state of the art supervised baseline and find that each of the three word representation improves the accuracy of these baseline we find further improvement by combining different word representation you can download our word feature for off the shelf use in existing nlp system a well a our code here http metaoptimize com project wordreprs 
bootstrapping is the process of improving the performance of a trained classifier by iteratively adding data that is labeled by the classifier itself to the training set and retraining the classifier it is often used in situation where labeled training data is scarce but unlabeled data is abundant in this paper we consider the problem of domain adaptation the situation where training data may not be scarce but belongs to a different domain from the target application domain a the distribution of unlabeled data is different from the training data standard bootstrapping often ha difficulty selecting informative data to add to the training set we propose an effective domain adaptive bootstrapping algorithm that selects unlabeled target domain data that are informative about the target domain and easy to automatically label correctly we call these instance bridge a they are used to bridge the source domain to the target domain we show that the method outperforms supervised transductive and bootstrapping algorithm on the named entity recognition task 
we present a simple robust generation system which performs content selection and surface realization in a unified domain independent framework in our approach we break up the end to end generation process into a sequence of local decision arranged hierarchically and each trained discriminatively we deployed our system in three different domain robocup sportscasting technical weather forecast and common weather forecast obtaining result comparable to state of the art domain specific system both in term of bleu score and human evaluation 
synchronous tree adjoining grammar stag is a promising formalism for syntaxaware machine translation and simultaneous computation of natural language syntax and semantics current research in both of these area is actively pursuing it incorporation however stag parsing is known to be np hard due to the potential for intertwined correspondence between the linked nonterminal symbol in the elementary structure given a particular grammar the polynomial degree of efficient stag parsing algorithm depends directly on the rank of the grammar the maximum number of correspondence that appear within a single elementary structure in this paper we present a compile time algorithm for transforming a stag into a strongly equivalent stag that optimally minimizes the rank k across the grammar the algorithm performs ino g y l g time where lg is the maximum number of link in any single synchronous tree pair in the grammar and y is the set of synchronous tree pair ofg 
polysemy is a major characteristic of natural language like word syntactic form can have several meaning understanding the correct meaning of a syntactic form is of great importance to many nlp application in this paper we address an important type of syntactic polysemy the multiple possible sens of tense syntactic form we make our discussion concrete by introducing the task of tense sense disambiguation tsd given a concrete tense syntactic form present in a sentence select it appropriate sense among a set of possible sens using english grammar textbook we compiled a syntactic sense dictionary comprising common tense syntactic form and semantic sens for each we annotated thousand of bnc sentence using the defined sens we describe a supervised tsd algorithm trained on these annotation which outperforms a strong baseline for the task 
this paper proposes a method for automatically inserting comma into japanese text in japanese sentence comma play an important role in explicitly separating the constituent such a word and phrase of a sentence the method can be used a an elemental technology for natural language generation such a speech recognition and machine translation or in writing support tool for non native speaker we categorized the usage of comma and investigated the appearance tendency of each category in this method the position where comma should be inserted are decided based on a machine learning approach we conducted a comma insertion experiment using a text corpus and confirmed the effectiveness of our method 
in the early day of email widely used convention for indicating quoted reply content and email signature made it easy to segment email message into their functional part today the explosion of different email format and style coupled with the ad hoc way in which people vary the structure and layout of their message mean that simple technique for identifying quoted reply that used to yield accuracy now find le than of such content in this paper we describe zebra an svm based system for segmenting the body text of email message into nine zone type based on graphic orthographic and lexical cue zebra performs this task with an accuracy of when the number of zone is abstracted to two or three zone class this increase to and respectively 
the ambiguity of person name in the web ha become a new area of interest for nlp researcher this challenging problem ha been formulated a the task of clustering web search result returned in response to a person name query according to the individual they mention in this paper we compare the coverage reliability and independence of a number of feature that are potential information source for this clustering task paying special attention to the role of named entity in the text to be clustered although named entity are used in most approach our result show that independently of the machine learning or clustering algorithm used named entity recognition and classification per se only make a small contribution to solve the problem 
bootstrapping ha a tendency called semantic drift to select instance unrelated to the seed instance a the iteration proceeds we demonstrate the semantic drift of bootstrapping ha the same root a the topic drift of kleinberg s hit using a simplified graph based reformulation of bootstrapping we confirm that two graph based algorithm the von neumann kernel and the regularized laplacian can reduce semantic drift in the task of word sense disambiguation wsd on senseval english lexical sample task proposed algorithm achieve superior performance to espresso and previous graph based wsd method even though the proposed algorithm have le parameter and are easy to calibrate 
this paper describes an empirical study of high performance dependency parser based on a semi supervised learning approach we describe an extension of semi supervised structured conditional model s scms to the dependency parsing problem whose framework is originally proposed in suzuki and isozaki moreover we introduce two extension related to dependency parsing the first extension is to combine s scms with another semi supervised approach described in koo et al the second extension is to apply the approach to second order parsing model such a those described in carreras using a two stage semi supervised learning approach we demonstrate the effectiveness of our proposed method on dependency parsing experiment using two widely used test collection the penn treebank for english and the prague dependency tree bank for czech our best result on test data in the above datasets achieve parent prediction accuracy for english and for czech 
in this paper we study the problem of summarizing email conversation we first build a sentence quotation graph that capture the conversation structure among email we adopt three cohesion measure clue word semantic similarity and cosine similarity a the weight of the edge second we use two graph based summarization approach generalized cluewordsummarizer and pagerank to extract sentence a summary third we propose a summarization approach based on subjective opinion and integrate it with the graph based one the empirical evaluation show that the basic clue word have the highest accuracy among the three cohesion measure moreover subjective word can significantly improve accuracy 
in this paper we address the task of mapping high level instruction to sequence of command in an external environment processing these instruction is challenging they posit goal to be achieved without specifying the step required to complete them we describe a method that fill in missing information using an automatically derived environment model that encodes state transition and command that cause these transition to happen we present an efficient approximate approach for learning this environment model a part of a policy gradient reinforcement learning algorithm for text interpretation this design enables learning for mapping high level instruction which previous statistical method cannot handle 
we propose bilingual tree kernel btks to capture the structural similarity across a pair of syntactic translational equivalence and apply btks to sub tree alignment along with some plain feature our study reveals that the structural feature embedded in a bilingual parse tree pair are very effective for sub tree alignment and the bilingual tree kernel can well capture such feature the experimental result show that our approach achieves a significant improvement on both gold standard tree bank and automatically parsed tree pair against a heuristic similarity based method we further apply the sub tree alignment in machine translation with two method it is suggested that the subtree alignment benefit both phrase and syntax based system by relaxing the constraint of the word alignment 
we present a graph based semi supervised label propagation algorithm for acquiring open domain labeled class and their instance from a combination of unstructured and structured text source this acquisition method significantly improves coverage compared to a previous set of labeled class and instance derived from free text while achieving comparable precision 
although research in other language is increasing much of the work in subjectivity analysis ha been applied to english data mainly due to the large body of electronic resource and tool that are available for this language in this paper we propose and evaluate method that can be employed to transfer a repository of subjectivity resource across language specifically we attempt to leverage on the resource available for english and by employing machine translation generate resource for subjectivity analysis in other language through comparative evaluation on two different language romanian and spanish we show that automatic translation is a viable alternative for the construction of resource and tool for subjectivity analysis in a new target language 
we investigate the combination of several source of information for the purpose of subjectivity recognition and polarity classification in meeting we focus on feature from two modality transcribed word and acoustic and we compare the performance of three different textual representation word character and phoneme our experiment show that character level feature outperform wordlevel feature for these task and that a careful fusion of all feature yield the best performance 
in this work we present a novel technique to rescore fragment in the data oriented translation model based on their contribution to translation accuracy we describe three new rescoring method and present the initial result of a pilot experiment on a small subset of the europarl corpus this work is a proof of concept and is the first step in directly optimizing translation decision solely on the hypothesized accuracy of potential translation resulting from those decision 
this paper explores the use of clickthrough data for query spelling correction first large amount of query correction pair are derived by analyzing user query reformulation behavior encoded in the clickthrough data then a phrase based error model that account for the transformation probability between multi term phrase is trained and integrated into a query speller system experiment are carried out on a human labeled data set result show that the system using the phrase based error model outperforms significantly it baseline system 
for language with semi free word order such a german labelling grammatical function on top of phrase structural constituent analysis is crucial for making them interpretable unfortunately most statistical classifier consider only local information for function labelling and fail to capture important restriction on the distribution of core argument function such a subject object etc namely that there is at most one subject etc per clause we augment a statistical classifier with an integer linear program imposing hard linguistic constraint on the solution space output by the classifier capturing global distributional restriction we show that this improves labelling quality in particular for argument grammatical function in an intrinsic evaluation and importantly grammar coverage for treebank based lexical functional grammar acquisition and parsing in an extrinsic evaluation 
identifying background context information in scientific article can help scholar understand major contribution in their research area more easily in this paper we propose a general framework based on probabilistic inference to extract such context information from scientific paper we model the sentence in an article and their lexical similarity a a markov random field tuned to detect the pattern that context data create and employ a belief propagation mechanism to detect likely context sentence we also address the problem of generating survey of scientific paper our experiment show greater pyramid score for survey generated using such context information rather than citation sentence alone 
in this paper we start to explore two part collocation extraction association measure that do not estimate expected probability on the basis of the independence assumption we propose two new measure based upon the well known measure of mutual information and pointwise mutual information expected probability are derived from automatically trained aggregate markov model on three collocation gold standard we find the new association measure vary in their effectiveness 
previously topic model such a plsi probabilistic latent semantic indexing and lda latent dirichlet allocation were developed for modeling the content of plain text recently topic model for processing hypertext such a web page were also proposed the proposed hypertext model are generative model giving rise to both word and hyperlink this paper point out that to better represent the content of hypertext it is more essential to assume that the hyperlink are fixed and to define the topic model a that of generating word only the paper then proposes a new topic model for hypertext processing referred to a hypertext topic model htm htm defines the distribution of word in a document i e the content of the document a a mixture over latent topic in the document itself and latent topic in the document which the document cite the topic are further characterized a distribution of word a in the conventional topic model this paper further proposes a method for learning the htm model experimental result show that htm outperforms the baseline on topic discovery and document classification in three datasets 
this paper present an investigation of the relation between word and their gender in two gendered language german and romanian gender is an issue that ha long preoccupied linguist and baffled language learner we verify the hypothesis that gender is dictated by the general sound pattern of a language and that it go beyond suffix or word ending experimental result on german and romanian noun show strong support for this hypothesis a gender prediction can be done with high accuracy based on the form of the word 
code switching is an interesting linguistic phenomenon commonly observed in highly bilingual community it consists of mixing language in the same conversational event this paper present result on part of speech tagging spanish english code switched discourse we explore different approach to exploit existing resource for both language that range from simple heuristic to language identification to machine learning the best result are achieved by training a machine learning algorithm with feature that combine the output of an english and a spanish part of speech tagger 
foreign name translation typically include multiple spelling variant these variant cause data sparseness problem increase out of vocabulary oov rate and present challenge for machine translation information extraction and other nlp task this paper aim to identify name spelling variant in the target language using the source name a an anchor based on word to word translation and transliteration probability a well a the string edit distance metric target name translation with similar spelling are clustered with this approach ten of thousand of high precision name translation spelling variant are extracted from sentence aligned bilingual corpus when these name spelling variant are applied to machine translation and information extraction task improvement over strong baseline system are observed in both case 
we improve the quality of paraphrase extracted from parallel corpus by requiring that phrase and their paraphrase be the same syntactic type this is achieved by parsing the english side of a parallel corpus and altering the phrase extraction algorithm to extract phrase label alongside bilingual phrase pair in order to retain broad coverage of non constituent phrase complex syntactic label are introduced a manual evaluation indicates a absolute improvement in paraphrase quality over the baseline method 
structured syntactic knowledge is important for phrase reordering this paper proposes using convolution tree kernel over source parse tree to model structured syntactic knowledge for btg based phrase reordering in the context of statistical machine translation our study reveals that the structured syntactic feature over the source phrase are very effective for btg constraint based phrase reordering and those feature can be well captured by the tree kernel we further combine the structured feature and other commonly used linear feature into a composite kernel experimental result on the nist mt chinese english translation task show that our proposed phrase reordering model statistically significantly outperforms the baseline method 
we address the modeling parameter estimation and search challenge that arise from the introduction of reordering model that capture non local reordering in alignment modeling in particular we introduce several reordering model that utilize pair of function word a context for alignment reordering to address the parameter estimation challenge we propose to estimate these reordering model from a relatively small amount of manually aligned corpus to address the search challenge we devise an iterative local search algorithm that stochastically explores reordering possibility by capturing non local reordering phenomenon our proposed alignment model bear a closer resemblance to state of the art translation model empirical result show significant improvement in alignment quality a well a in translation performance over baseline in a large scale chinese english translation task 
in this paper we present a novel approach for mining opinion from product review where it convert opinion mining task to identify product feature expression of opinion and relation between them by taking advantage of the observation that a lot of product feature are phrase a concept of phrase dependency parsing is introduced which extends traditional dependency parsing to phrase level this concept is then implemented for extracting relation between product feature and expression of opinion experimental evaluation show that the mining task can benefit from phrase dependency parsing 
sentiment analysis often relies on a semantic orientation lexicon of positive and negative word a number of approach have been proposed for creating such lexicon but they tend to be computationally expensive and usually rely on significant manual annotation and large corpus most of these method use wordnet in contrast we propose a simple approach to generate a high coverage semantic orientation lexicon which includes both individual word and multi word expression using only a roget like thesaurus and a handful of affix further the lexicon ha property that support the polyanna hypothesis using the general inquirer a gold standard we show that our lexicon ha percentage point more correct entry than the leading wordnet based high coverage lexicon sentiwordnet in an extrinsic evaluation we obtain significantly higher performance in determining phrase polarity using our thesaurus based lexicon than with any other additionally we explore the use of visualization technique to gain insight into the our algorithm beyond the evaluation mentioned above 
definition extraction is the task of automatically identifying definitional sentence within text the task ha proven useful in many research area including ontology learning relation extraction and question answering however current approach mostly focused on lexicosyntactic pattern suffer from both low recall and precision a definitional sentence occur in highly variable syntactic structure in this paper we propose word class lattice wcls a generalization of word lattice that we use to model textual definition lattice are learned from a dataset of definition from wikipedia our method is applied to the task of definition and hypernym extraction and compare favorably to other pattern generalization method proposed in the literature 
in this paper we adopt two view personal and impersonal view and systematically employ them in both supervised and semi supervised sentiment classification here personal view consist of those sentence which directly express speaker s feeling and preference towards a target object while impersonal view focus on statement towards a target object for evaluation to obtain them an unsupervised mining approach is proposed on this basis an ensemble method and a co training algorithm are explored to employ the two view in supervised and semi supervised sentiment classification respectively experimental result across eight domain demonstrate the effectiveness of our proposed approach 
in this paper we conducted a systematic comparative analysis of language in different context of bursty topic including web search news medium blogging and social bookmarking we analyze the content similarity and predictability between context the coverage of search content by each context and the intrinsic coherence of information in each context our experiment show that social bookmarking is a better predictor to the bursty search query but news medium and social blogging medium have a much more compelling coverage this comparison provides insight on how the search behavior and social information sharing behavior of user are correlated to the professional news medium in the context of bursty event 
we present an implicit discourse relation classifier in the penn discourse treebank pdtb our classifier considers the context of the two argument word pair information a well a the argument internal constituent and dependency par our result on the pdtb yield a significant improvement over the baseline in our error analysis we discus four challenge in recognizing implicit relation in the pdtb 
this paper approach the scope learning problem via simplified shallow semantic parsing this is done by regarding the cue a the predicate and mapping it scope into several constituent a the argument of the cue evaluation on the bioscope corpus show that the structural information play a critical role in capturing the relationship between a cue and it dominated argument it also show that our parsing approach significantly outperforms the state of the art chunking one although our parsing approach is only evaluated on negation and speculation scope learning here it is portable to other kind of scope learning 
we present a machine learning approach for the task of ranking previously answered question in a question repository with respect to their relevance to a new unanswered reference question the ranking model is trained on a collection of question group manually annotated with a partial order relation reflecting the relative utility of question inside each group based on a set of meaning and structure aware feature the new ranking model is able to substantially outperform more straightforward unsupervised similarity measure 
we propose an approach to adjective noun composition an for corpus based distributional semantics that building on insight from theoretical linguistics represents noun a vector and adjective a data induced linear function encoded a matrix over nominal vector our model significantly outperforms the rival on the task of reconstructing an vector not seen in training a small post hoc analysis further suggests that when the model generated an vector is not similar to the corpus observed an vector this is due to anomaly in the latter we show moreover that our approach provides two novel way to represent adjective meaning alternative to it representation via corpus based co occurrence vector both outperforming the latter in an adjective clustering task 
inspired by the success of english grapheme to phoneme research in speech synthesis many researcher have proposed phoneme based english to chinese transliteration model however such approach have severely suffered from the error in chinese phoneme to grapheme conversion to address this issue we propose a new english to chinese transliteration model and make systematic comparison with the conventional model our proposed model relies on the joint use of chinese phoneme and their corresponding english grapheme and phoneme experiment showed that chinese phoneme in our proposed model can contribute to the performance improvement in english to chinese transliteration 
we examine the problem of overcoming noisy word level alignment when learning tree to string translation rule our approach introduces new rule and re estimate rule probability using em the major obstacle to this approach are the very reason that word alignment are used for rule extraction the huge space of possible rule a well a controlling overfitting by carefully controlling which portion of the original alignment are reanalyzed and by using bayesian inference during re analysis we show significant improvement over the baseline rule extracted from word level alignment 
this paper present a framework for automatically processing information coming from community question answering cqa portal with the purpose of generating a trustful complete relevant and succinct summary in response to a question we exploit the metadata intrinsically present in user generated content ugc to bias automatic multi document summarization technique toward high quality information we adopt a representation of concept alternative to n gram and propose two concept scoring function based on semantic overlap experimental result on data drawn from yahoo answer demonstrate the effectiveness of our method in term of rouge score we show that the information contained in the best answer voted by user of cqa portal can be successfully complemented by our method 
in a linguistically motivated syntax based translation system the entire translation process is normally carried out in two step translation rule matching and target sentence decoding using the matched rule both step are very time consuming due to the tremendous number of translation rule the exhaustive search in translation rule matching and the complex nature of the translation task itself in this paper we propose a hyper tree based fast algorithm for translation rule matching experimental result on the nist mt chinese english translation task show that our algorithm is at least time faster in rule matching and is able to help to save of overall translation time over previous method when using large fragment translation rule 
we describe the first tractable gibbs sampling procedure for estimating phrase pair frequency under a probabilistic model of phrase alignment we propose and evaluate two nonparametric prior that successfully avoid the degenerate behavior noted in previous work where overly large phrase memorize the training data phrase table weight learned under our model yield an increase in bleu score over the word alignment based heuristic estimate used regularly in phrase based translation system 
in this paper we introduce a method that automatically build text classifier in a new language by training on already labeled data in another language our method transfer the classification knowledge across language by translating the model feature and by using an expectation maximization em algorithm that naturally take into account the ambiguity associated with the translation of a word we further exploit the readily available unlabeled data in the target language via semi supervised learning and adapt the translated model to better fit the data distribution of the target language 
once released treebanks tend to remain unchanged despite any shortcoming in their depth of linguistic analysis or coverage of specific phenomenon instead separate resource are created to address such problem in this paper we show how to improve the quality of a treebank by integrating resource and implementing improved analysis for specific construction we demonstrate this rebanking process by creating an updated version of ccg bank that includes the predicate argument structure of both verb and noun base np bracket verb particle construction and restrictive and non restrictive nominal modifier and evaluate the impact of these change on a statistical parser 
substantial research effort ha been invested in recent decade into the computational study and automatic processing of multi party conversation while most aspect of conversational speech have benefited from a wide availability of analytic computationally tractable technique only qualitative assessment are available for characterizing multi party turn taking the current paper attempt to address this deficiency by first proposing a framework for computing turn taking model perplexity and then by evaluating several multi participant modeling approach experiment show that direct multi participant model do not generalize to held out data and likely never will for practical reason in contrast the extended degree of overlap model represents a suitable candidate for future work in this area and is shown to successfully predict the distribution of speech in time and across participant in previously unseen conversation 
obtaining labeled data is a significant obstacle for many nlp task recently online game have been proposed a a new way of obtaining labeled data game attract user by being fun to play in this paper we consider the application of this idea to collecting semantic relation between word such a hypernym hyponym relationship we built three online game inspired by the real life game of scattergories and taboo a of june player have entered nearly data instance in two category the first type of data consists of category answer pair type of vehicle car while the second is essentially free association data submarine underwater we analyze both type of data in detail and discus potential us of the data we show that we can extract from our data set a significant number of new hypernym hyponym pair not already found in wordnet 
because of the importance of protein protein interaction ppi extraction from text many corpus have been proposed with slightly differing definition of protein and ppi since no single corpus is large enough to saturate a machine learning system it is necessary to learn from multiple different corpus in this paper we propose a solution to this challenge we designed a rich feature vector and we applied a support vector machine modified for corpus weighting svm cw to complete the task of multiple corpus ppi extraction the rich feature vector made from multiple useful kernel is used to express the important information for ppi extraction and the system with our feature vector wa shown to be both faster and more accurate than the original kernel based system even when using just a single corpus svm cw learns from one corpus while using other corpus for support svm cw is simple but it is more effective than other method that have been successfully applied to other nlp task earlier with the feature vector and svm cw our system achieved the best performance among all state of the art ppi extraction system reported so far 
while significant effort ha been put into annotating linguistic resource for several language there are still many left that have only small amount of such resource this paper investigates a method of propagating information specifically mention detection information into such low resource language from richer one experiment run on three language pair arabic english chinese english and spanish english show that one can achieve relatively decent performance by propagating information from a language with richer resource such a english into a foreign language alone no resource or model in the foreign language furthermore while examining the performance using various degree of linguistic information in a statistical framework result show that propagated feature from english help improve the source language system performance even when used in conjunction with all feature type built from the source language the experiment also show that using propagated feature in conjunction with lexically derived feature only a can be obtained directly from a mention annotated corpus yield similar performance to using feature type derived from many linguistic resource 
we present a method to transliterate name in the framework of end to end statistical machine translation the system is trained to learn when to transliterate for arabic to english mt we developed and trained a transliterator on a bitext of million sentence and google s english terabyte ngrams and achieved better name translation accuracy than out of professional translator the paper also includes a discussion of challenge in name translation evaluation 
we outline different method to detect error in automatically parsed dependency corpus by comparing so called dependency rule to their representation in the training data and flagging anomalous one by comparing each new rule to every relevant rule from training we can identify part of parse tree which are likely erroneous even the relatively simple method of comparison we propose show promise for speeding up the annotation process 
string transformation which map a source string s into it desirable form t is related to various application including stemming lemmatization and spelling correction the essential and important step for string transformation is to generate candidate to which the given string s is likely to be transformed this paper present a discriminative approach for generating candidate string we use substring substitution rule a feature and score them using an l regularized logistic regression model we also propose a procedure to generate negative instance that affect the decision boundary of the model the advantage of this approach is that candidate string can be enumerated by an efficient algorithm because the process of string transformation are tractable in the model we demonstrate the remarkable performance of the proposed method in normalizing inflected word and spelling variation 
minimum error rate training mert involves choosing parameter value for a machine translation mt system that maximize performance on a tuning set a measured by an automatic evaluation metric such a bleu the method is best when the system will eventually be evaluated using the same metric but in reality most mt evaluation have a human based component although performing mert with a human based metric seems like a daunting task we describe a new metric rypt which take human judgment into account but only requires human input to build a database that can be reused over and over again hence eliminating the need for human input at tuning time in this investigative study we analyze the diversity or lack thereof of the candidate produced during mert we describe how this redundancy can be used to our advantage and show that rypt is a better predictor of translation quality than bleu 
current method of using lexical feature in machine translation have difficulty in scaling up to realistic mt task due to a prohibitively large number of parameter involved in this paper we propose method of using new linguistic and contextual feature that do not suffer from this problem and apply them in a state of the art hierarchical mt system the feature used in this work are non terminal label non terminal length distribution source string context and source dependency lm score the effectiveness of our technique is demonstrated by significant improvement over a strong base line on arabic to english translation improvement in lower cased bleu are on nist mt and on mt newswire data on decoding output on chinese to english translation the improvement are on mt and on mt newswire data 
we propose a succinct randomized language model which employ a perfect hash function to encode fingerprint of n gram and their associated probability backoff weight or other parameter the scheme can represent any standard n gram model and is easily combined with existing model reduction technique such a entropy pruning we demonstrate the space saving of the scheme via machine translation experiment within a distributed language modeling framework 
in modern machine translation practice a statistical phrasal or hierarchical translation system usually relies on a huge set of translation rule extracted from bi lingual training data this approach not only result in space and efficiency issue but also suffers from the sparse data problem in this paper we propose to use factorized grammar an idea widely accepted in the field of linguistic grammar construction to generalize translation rule so a to solve these two problem we designed a method to take advantage of the xtag english grammar to facilitate the extraction of factorized rule we experimented on various setup of low resource language translation and showed consistent significant improvement in bleu over state of the art string to dependency baseline system with k word of bi lingual training data 
most existing information retrieval ir system do not take much advantage of natural language processing nlp technique due to the complexity and limited observed effectiveness of applying nlp to ir in this paper we demonstrate that substantial gain can be obtained over a strong baseline using nlp technique if properly handled we propose a framework for deriving semantic text matching feature from named entity identified in web query we then utilize these feature in a supervised machine learned ranking approach applying a set of emerging machine learning technique our approach is especially useful for query that contain multiple type of concept comparing to a major commercial web search engine we observe a substantial dcg gain over the affected query 
knowledge of noun phrase anaphoricity might be profitably exploited in coreference resolution to bypass the resolution of non anaphoric noun phrase however it is surprising to notice that recent attempt to incorporate automatically acquired anaphoricity information into coreference resolution have been somewhat disappointing this paper employ a global learning method in determining the anaphoricity of noun phrase via a label propagation algorithm to improve learning based coreference resolution in particular two kind of kernel i e the feature based rbf kernel and the convolution tree kernel are employed to compute the anaphoricity similarity between two noun phrase experiment on the ace corpus demonstrate the effectiveness of our method in anaphoricity determination of noun phrase and it application in learning based coreference resolution 
we present a discriminative model that directly predicts which set of phrasal translation rule should be extracted from a sentence pair our model score extraction set nested collection of all the overlapping phrase pair consistent with an underlying word alignment extraction set model provide two principle advantage over word factored alignment model first we can incorporate feature on phrase pair in addition to word link second we can optimize for an extraction based loss function that relates directly to the end task of generating translation our model give improvement in alignment quality relative to state of the art unsupervised and supervised baseline a well a providing up to a improvement in bleu score in chinese to english translation experiment 
this paper establishes a connection between two apparently very different kind of probabilistic model latent dirichlet allocation lda model are used a topic model to produce a low dimensional representation of document while probabilistic context free grammar pcfgs define distribution over tree the paper begin by showing that lda topic model can be viewed a a special kind of pcfg so bayesian inference for pcfgs can be used to infer topic model a well adaptor grammar ag are a hierarchical non parameteric bayesian extension of pcfgs exploiting the close relationship between lda and pcfgs just described we propose two novel probabilistic model that combine insight from lda and ag model the first replaces the unigram component of lda topic model with multi word sequence or collocation generated by an ag the second extension build on the first one to learn aspect of the internal structure of proper name 
music recommendation system often recommend individual song a opposed to entire album the challenge is to generate review for each song since only full album review are available on line we developed a summarizer that combine information extraction and generation technique to produce summary of review of individual song we present an intrinsic evaluation of the extraction component and of the informativeness of the summary and a user study of the impact of the song review summary on user decision making process user were able to make quicker and more informed decision when presented with the summary a compared to the full album review 
despite it substantial coverage nombank doe not account for all within sentence argument and ignores extra sentential argument altogether these argument which we call implicit are important to semantic processing and their recovery could potentially benefit many nlp application we present a study of implicit argument for a select group of frequent nominal predicate we show that implicit argument are pervasive for these predicate adding to the coverage of nombank we demonstrate the feasibility of recovering implicit argument with a supervised classification model our result and analysis provide a baseline for future work on this emerging task 
in this paper we formulate extractive summarization a a risk minimization problem and propose a unified probabilistic framework that naturally combine supervised and unsupervised summarization model to inherit their individual merit a well a to overcome their inherent limitation in addition the introduction of various loss function also provides the summarization framework with a flexible but systematic way to render the redundancy and coherence relationship among sentence and between sentence and the whole document respectively experiment on speech summarization show that the method deduced from our framework are very competitive with existing summarization approach 
regular expression have served a the dominant workhorse of practical information extraction for several year however there ha been little work on reducing the manual effort involved in building high quality complex regular expression for information extraction task in this paper we propose relie a novel transformation based algorithm for learning such complex regular expression we evaluate the performance of our algorithm on multiple datasets and compare it against the crf algorithm we show that relie in addition to being an order of magnitude faster outperforms crf under condition of limited training data and cross domain data finally we show how the accuracy of crf can be improved by using feature extracted by relie 
we show that the standard beam search algorithm can be used a an efficient decoder for the global linear model of zhang and clark for joint word segmentation and po tagging achieving a significant speed improvement such decoding is enabled by separating full word feature from partial word feature so that feature template can be instantiated incrementally according to whether the current character is separated or appended deciding the po tag of a potential word when it first character is processed early update is used with perceptron training so that the linear model give a high score to a correct partial candidate a well a a full output effective scoring of partial structure allows the decoder to give high accuracy with a small beam size of in our fold cross validation experiment with the chinese tree bank our system performed over time a fast a zhang and clark with little accuracy loss the accuracy of our system on the standard ctb test wa competitive with the best in the literature 
topical blog post retrieval is the task of ranking blog post with respect to their relevance for a given topic to improve topical blog post retrieval we incorporate textual credibility indicator in the retrieval process we consider two group of indicator post level determined using information about individual blog post only and blog level determined using information from the underlying blog we describe how to estimate these indicator and how to integrate them into a retrieval approach based on language model experiment on the trec blog track test set show that both group of credibility indicator significantly improve retrieval effectiveness the best performance is achieved when combining them 
in this paper we propose a method for the automatic decipherment of lost language given a non parallel corpus in a known related language our model produce both alphabetic mapping and translation of word into their corresponding cognate we employ a non parametric bayesian framework to simultaneously capture both low level character mapping and high level morphemic correspondence this formulation enables u to encode some of the linguistic intuition that have guided human decipherer when applied to the ancient semitic language ugaritic the model correctly map of letter to their hebrew counterpart and deduces the correct hebrew cognate for of the ugaritic word which have cognate in hebrew 
existing work on sentiment analysis on product review suffer from the following limitation the knowledge of hierarchical relationship of product attribute is not fully utilized review or sentence mentioning several attribute associated with complicated sentiment are not dealt with very well in this paper we propose a novel hl sot approach to labeling a product s attribute and their associated sentiment in product review by a hierarchical learning hl process with a defined sentiment ontology tree sot the empirical analysis against a human labeled data set demonstrates promising and reasonable performance of the proposed hl sot approach while this paper is mainly on sentiment analysis on review of one product our proposed hl sot approach is easily generalized to labeling a mix of review of more than one product 
while speaking spontaneously speaker often make error such a self correction or false start which interfere with the successful application of natural language processing technique like summarization and machine translation to this data there is active work on reconstructing this errorful data into a clean and fluent transcript by identifying and removing these simple error previous research ha approximated the potential benefit of conducting word level reconstruction of simple error only on those sentence known to have error in this work we explore new approach for automatically identifying speaker construction error on the utterance level and quantify the impact that this initial step ha on wordand sentence level reconstruction accuracy 
the third pascal recognizing textual entailment challenge rte contained an optional task that extended the main entailment task by requiring a system to make three way entailment decision entail contradicts neither and to justify it response contradiction wa rare in the rte test set occurring in only about of the case and system found accurately detecting it difficult subsequent analysis of the result show a test set must contain many more entailment pair for the three way decision task than the traditional two way task to have equal confidence in system comparison each of six human judge representing eventual end user rated the quality of a justification by assigning understandability and correctness score rating of the same justification across judge differed significantly signaling the need for a better characterization of the justification task 
we propose a translation recommendation framework to integrate statistical machine translation smt output with translation memory tm system the framework recommends smt output to a tm user when it predicts that smt output are more suitable for post editing than the hit provided by the tm we describe an implementation of this framework using an svm binary classifier we exploit method to fine tune the classifier and investigate a variety of feature of different type we rely on automatic mt evaluation metric to approximate human judgement in our experiment experimental result show that our system can achieve precision at recall excluding exact match furthermore it is possible for the end user to achieve a desired balance between precision and recall by adjusting confidence level 
syntactic knowledge is important for discourse relation recognition yet only heuristically selected flat path and level production rule have been used to incorporate such information so far in this paper we propose using tree kernel based approach to automatically mine the syntactic information from the parse tree for discourse analysis applying kernel function to the tree structure directly these structural syntactic feature together with other normal flat feature are incorporated into our composite kernel to capture diverse knowledge for simultaneous discourse identification and classification for both explicit and implicit relation the experiment show tree kernel approach is able to give statistical significant improvement over flat syntactic path feature we also illustrate that tree kernel approach cover more structure information than the production rule which allows tree kernel to further incorporate information from a higher dimension space for possible better discrimination besides we further propose to leverage on temporal ordering information to constrain the interpretation of discourse relation which also demonstrate statistical significant improvement for discourse relation recognition on pdtb for both explicit and implicit a well 
problem stemming from domain adaptation continue to plague the statistical natural language processing community there ha been continuing work trying to find general purpose algorithm to alleviate this problem in this paper we argue that existing general purpose approach usually only focus on one of two issue related to the difficulty faced by adaptation difference in base feature statistic or task difference that can be detected with labeled data we argue that it is necessary to combine these two class of adaptation algorithm using evidence collected through theoretical analysis and simulated and real world data experiment we find that the combined approach often outperforms the individual adaptation approach by combining simple approach from each class of adaptation algorithm we achieve state of the art result for both named entity recognition adaptation task and the preposition sense disambiguation adaptation task second we also show that applying an adaptation algorithm that find shared representation between domain often impact the choice in adaptation algorithm that make use of target labeled data 
current system combination method usually use confusion network to find consensus translation among different system requiring one to one mapping between the word in candidate translation confusion network have difficulty in handling more general situation in which several word are connected to another several word instead we propose a lattice based system combination model that allows for such phrase alignment and us lattice to encode all candidate translation experiment show that our approach achieves significant improvement over the state of the art baseline system on chinese to english translation test set 
we investigate the problem of binary text classification in the domain of legal docket entry this work present an illustrative instance of a domain specific problem where the stateof the art machine learning ml classifier such a svms are inadequate our investigation into the reason for the failure of these classifier revealed two type of prominent error which we call conjunctive and disjunctive error we developed simple heuristic to address one of these error type and improve the performance of the svms based on the intuition gained from our experiment we also developed a simple propositional logic based classifier using hand labeled feature that address both type of error simultaneously we show that this new but simple approach outperforms all existing state of the art ml model with statistically significant gain we hope this work serf a a motivating example of the need to build more expressive classifier beyond the standard model class and to address text classification problem in such nontraditional domain 
name ambiguity problem ha raised urgent demand for efficient high quality named entity disambiguation method in recent year the increasing availability of large scale rich semantic knowledge source such a wikipedia and wordnet creates new opportunity to enhance the named entity disambiguation by developing algorithm which can exploit these knowledge source at best the problem is that these knowledge source are heterogeneous and most of the semantic knowledge within them is embedded in complex structure such a graph and network this paper proposes a knowledge based method called structural semantic relatedness ssr which can enhance the named entity disambiguation by capturing and leveraging the structural semantic knowledge in multiple knowledge source empirical result show that in comparison with the classical bow based method and social network based method our method can significantly improve the disambiguation performance by respectively and 
people rarely articulate explicitly what a native speaker of a language is already assumed to know so to acquire the stereotypical knowledge that underpins much of what is said in a given culture one must look to what is implied by language rather than what is overtly stated simile are a convenient vehicle for this kind of knowledge insofar a they mark out the most salient aspect of the most frequently evoked concept in this paper we perform a multilingual exploration of the space of common place simile by mining a large body of chinese simile from the web and comparing these to the english simile harvested by veale and hao we demonstrate that while the simile frame is inherently leaky in both language a multilingual analysis allows u to filter much of the noise that otherwise hinders the knowledge extraction process in doing so we can also identify a core set of stereotypical description that exist in both language and accurately map these description onto a multilingual lexical ontology like hownet finally we demonstrate that conceptual description that are derived from common place simile are extremely compact and predictive of ontological structure 
we describe a novel approach for syntax based statistical mt which build on a variant of tree adjoining grammar tag inspired by work in discriminative dependency parsing the key idea in our approach is to allow highly flexible reordering operation during parsing in combination with a discriminative model that can condition on rich feature of the source language string experiment on translation from german to english show improvement over phrase based system both in term of bleu score and in human evaluation 
in this paper we tackle the problem of automatic caption generation for news image our approach leverage the vast resource of picture available on the web and the fact that many of them are captioned inspired by recent work in summarization we propose extractive and abstractive caption generation model they both operate over the output of a probabilistic image annotation model that pre process the picture and suggests keywords to describe their content experimental result show that an abstractive model defined over phrase is superior to extractive method 
this work describes an answer ranking engine for non factoid question built using a large online community generated question answer collection yahoo answer we show how such collection may be used to effectively set up large supervised learning experiment furthermore we investigate a wide range of feature type some exploiting nlp processor and demonstrate that using them in combination lead to considerable improvement in accuracy 
current semantic role labeling technology are based on inductive algorithm trained over large scale repository of annotated example frame based system currently make use of the framenet database but fail to show suitable generalization capability in out of domain scenario in this paper a state of art system for frame based srl is extended through the encapsulation of a distributional model of semantic similarity the resulting argument classification model promotes a simpler feature space that limit the potential overfitting effect the large scale empirical study here discussed confirms that state of art accuracy can be obtained for out of domain evaluation 
we argue that group of unannotated text with overlapping and non contradictory semantics represent a valuable source of information for learning semantic representation a simple and efficient inference method recursively induces joint semantic representation for each group and discovers correspondence between lexical entry and latent semantic concept we consider the generative semantics text correspondence model liang et al and demonstrate that exploiting the noncontradiction relation between text lead to substantial improvement over natural baseline on a problem of analyzing human written weather forecast 
string to string transduction is a central problem in computational linguistics and natural language processing it occurs in task a diverse a name transliteration spelling correction pronunciation modeling and inflectional morphology we present a conditional loglinear model for string to string transduction which employ overlapping feature over latent alignment sequence and which learns latent class and latent string pair region from incomplete training data we evaluate our approach on morphological task and demonstrate that latent variable can dramatically improve result even when trained on small data set on the task of generating morphological form we outperform a baseline method reducing the error rate by up to on a lemmatization task we reduce the error rate in wicentowski by 
we introduce a novel mechanism for incorporating articulatory dynamic into speech recognition with the theory of task dynamic this system reranks sentence level hypothesis by the likelihood of their hypothetical articulatory realization which are derived from relationship learned with aligned acoustic articulatory data experiment compare this with two baseline system namely an acoustic hidden markov model and a dynamic bayes network augmented with discretized representation of the vocal tract our system based on task dynamic reduces word error rate significantly by relative to the best baseline model 
large scale discriminative machine translation promise to further the state of the art but ha failed to deliver convincing gain over current heuristic frequency count system we argue that a principle reason for this failure is not dealing with multiple equivalent translation we present a translation model which model derivation a a latent variable in both training and decoding and is fully discriminative and globally optimised result show that accounting for multiple derivation doe indeed improve performance additionally we show that regularisation is essential for maximum conditional likelihood model in order to avoid degenerate solution 
this paper explores the challenge of scaling up language processing algorithm to increasingly large datasets while cluster computing ha been available in commercial environment for several year academic researcher have fallen behind in their ability to work on large datasets i discus two barrier contributing to this problem lack of a suitable programming model for managing concurrency and difficulty in obtaining access to hardware hadoop an open source implementation of google s mapreduce framework provides a compelling solution to both issue it simple programming model hide system level detail from the developer and it ability to run on commodity hardware put cluster computing within the reach of many academic research group this paper illustrates these point with a case study in building word cooccurrence matrix from large corpus i conclude with an analysis of an alternative computing model based on renting instead of buying computer cluster 
system combination ha emerged a a powerful method for machine translation mt this paper pursues a joint optimization strategy for combining output from multiple mt system where word alignment ordering and lexical selection decision are made jointly according to a set of feature function combined in a single log linear model the decoding algorithm is described in detail and a set of new feature that support this joint decoding approach is proposed the approach is evaluated in comparison to state of the art confusion network based system combination method using equivalent feature and shown to outperform them significantly 
this paper introduces algorithm for non projective parsing based on dual decomposition we focus on parsing algorithm for non projective head automaton a generalization of head automaton model to non projective structure the dual decomposition algorithm are simple and efficient relying on standard dynamic programming and minimum spanning tree algorithm they provably solve an lp relaxation of the non projective parsing problem empirically the lp relaxation is very often tight for many language exact solution are achieved on over of test sentence the accuracy of our model is higher than previous work on a broad range of datasets 
this paper focus on the task of inserting punctuation symbol into transcribed conversational speech text without relying on prosodic cue we investigate limitation associated with previous method and propose a novel approach based on dynamic conditional random field different from previous work our proposed approach is designed to jointly perform both sentence boundary and sentence type prediction and punctuation prediction on speech utterance we performed evaluation on a transcribed conversational speech domain consisting of both english and chinese text empirical result show that our method outperforms an approach based on linear chain conditional random field and other previous approach 
we present a novel approach to distributionalonly fully unsupervised po tagging based on an adaptation of the em algorithm for the estimation of a gaussian mixture in this approach which we call latent descriptor clustering ldc word type are clustered using a series of progressively more informative descriptor vector these descriptor which are computed from the immediate left and right context of each word in the corpus are updated based on the previous state of the cluster assignment the ldc algorithm is simple and intuitive using standard evaluation criterion for unsupervised po tagging ldc show a substantial improvement in performance over state of the art method along with a several fold reduction in computational cost 
there is growing interest in applying bayesian technique to nlp problem there are a number of different estimator for bayesian model and it is useful to know what kind of task each doe well on this paper compare a variety of different bayesian estimator for hidden markov model po tagger with various number of hidden state on data set of different size recent paper have given contradictory result when comparing bayesian estimator to expectation maximization em for unsupervised hmm po tagging and we show that the difference in reported result is largely due to difference in the size of the training data and the number of state in the hmm we invesigate a variety of sampler for hmms including some that these earlier paper did not study we find that all of gibbs sampler do well with small data set and few state and that variational bayes doe well on large data set and is competitive with the gibbs sampler in term of time of convergence we find that variational bayes wa the fastest of all the estimator especially on large data set and that explicit gibbs sampler both pointwise and sentence blocked were generally faster than their collapsed counterpart on large data set 
in recent year there ha been substantial work on the important problem of coreference resolution most of which ha concentrated on the development of new model and algorithmic technique these work often show that complex model improve over a weak pairwise baseline however le attention ha been given to the importance of selecting strong feature to support learning a coreference model this paper describes a rather simple pairwise classification model for coreference resolution developed with a well designed set of feature we show that this produce a state of the art system that outperforms system built with complex model we suggest that our system can be used a a baseline for the development of more complex model which may have le impact when a more robust set of feature is used the paper also present an ablation study and discus the relative contribution of various feature 
in this paper we present a study of a novel summarization problem i e summarizing the impact of a scientic publication given a paper and it citation context we study how to extract sentence that can represent the most inuential content of the paper we propose language modeling method for solving this problem and study how to incorporate feature such a authority and proximity to accurately estimate the impact language model experiment result on a sigir publication collection show that the proposed method are effective for generating impact based summary 
we present a novel fully unsupervised algorithm for po induction from plain text motivated by the cognitive notion of prototype the algorithm first identifies landmark cluster of word serving a the core of the induced po category the rest of the word are subsequently mapped to these cluster we utilize morphological and distributional representation computed in a fully unsupervised manner we evaluate our algorithm on english and german achieving the best reported result for this task 
this paper present an empirical study on the robustness and generalization of two alternative role set for semantic role labeling propbank numbered role and verbnet thematic role by testing a state of the art srl system with the two alternative role annotation we show that the propbank role set is more robust to the lack of verb specific semantic information and generalizes better to infrequent and unseen predicate keeping in mind that thematic role are better for application need we also tested the best way to generate verbnet annotation we conclude that tagging first propbank role and mapping into verbnet role is a effective a training and tagging directly on verbnet and more robust for domain shift 
much nlp research on multi word expression mwes focus on the discovery of new expression a opposed to the identification in text of known expression however mwe identification is not trivial because many expression allow variation in form and differ in the range of variation they allow we show that simple rule based baseline do not perform identification satisfactorily and present a supervised learning method for identification that us sentence surface feature based on expression canonical form to evaluate the method we have annotated sentence from the british national corpus containing potential us of verbal mwes the method achieves an f score of compared with for the leading rule based baseline our method is easily applicable to any expression type experiment in previous research have been limited to the compositional non compositional distinction while we also test on sentence in which the word comprising the mwe appear but not a an expression 
web search engine today typically show result a a list of title and short snippet that summarize how the retrieved document are related to the query however recent research suggests that longer summary can be preferable for certain type of query this paper present empirical evidence that judge can predict appropriate search result summary length and that perception of search result quality can be affected by varying these result length these finding have important implication for search result presentation especially for natural language query 
this paper describes a language independent scalable system for both challenge of cross document co reference name variation and entity disambiguation we provide system result from the ace evaluation in both english and arabic our english system s accuracy is relative better than an exact match baseline and relative better over entity mentioned in more than one document unlike previous evaluation ace evaluated both name variation and entity disambiguation over naturally occurring named mention an information extraction engine find document entity in text we describe how our architecture designed for the k document ace task is scalable to an even larger corpus our cross document approach us the name of entity to find an initial set of document entity that could refer to the same real world entity and then us an agglomerative clustering algorithm to disambiguate the potentially co referent document entity we analyze how different aspect of our system affect performance using ablation study over the english evaluation set in addition to evaluating cross document co reference performance we used the result of the cross document system to improve the accuracy of within document extraction and measured the impact in the ace within document evaluation 
we describe a process for automatically detecting decision making sub dialogue in multi party human human meeting in real time our basic approach to decision detection involves distinguishing between different utterance type based on the role that they play in the formulation of a decision in this paper we describe how this approach can be implemented in real time and show that the resulting system s performance compare well with other detector including an off line version 
we propose a novel self training method for a parser which us a lexicalised grammar and supertagger focusing on increasing the speed of the parser rather than it accuracy the idea is to train the supertagger on large amount of parser output so that the supertagger can learn to supply the supertags that the parser will eventually choose a part of the highest scoring derivation since the supertagger supply fewer supertags overall the parsing speed is increased we demonstrate the effectiveness of the method using a ccg supertagger and parser obtaining significant speed increase on newspaper text with no loss in accuracy we also show that the method can be used to adapt the ccg parser to new domain obtaining accuracy and speed improvement for wikipedia and biomedical text 
we study graphical modeling in the case of stringvalued random variable whereas a weighted finite state transducer can model the probabilistic relationship between two string we are interested in building up joint model of three or more string this is needed for inflectional paradigm in morphology cognate modeling or language reconstruction and multiple string alignment we propose a markov random field in which each factor potential function is a weighted finite state machine typically a transducer that evaluates the relationship between just two of the string the full joint distribution is then a product of these factor though decoding is actually undecidable in general we can still do efficient joint inference using approximate belief propagation the necessary computation and message are all finitestate we demonstrate the method by jointly predicting morphological form overview 
in this paper we present a textual dialogue system that us word association retrieved from the web to create proposition we also show experiment result for the role of modality generation the proposed system automatically extract set of word related to a conversation topic set freely by a user after the extraction process it generates an utterance add a modality and verifies the semantic reliability of the proposed sentence we evaluate word association extracted form the web and the result of adding modality over of the extracted word association were evaluated a correct adding modality improved the system significantly for all evaluation criterion we also show how our system can be used a a simple and expandable platform for almost any kind of experiment with human computer textual conversation in japanese two example with affect analysis and humor generation are given 
this paper explores the relationship between discourse segmentation and coverbal gesture introducing the idea of gestural cohesion we show that coherent topic segment are characterized by homogeneous gestural form and that change in the distribution of gestural feature predict segment boundary gestural feature are extracted automatically from video and are combined with lexical feature in a bayesian generative model the resulting multimodal system outperforms text only segmentation on both manual and automaticallyrecognized speech transcript 
we propose using large scale clustering of dependency relation between verb and multiword noun mn to construct a gazetteer for named entity recognition ner since dependency relation capture the semantics of mn well the mn cluster constructed by using dependency relation should serve a a good gazetteer however the high level of computational cost ha prevented the use of clustering for constructing gazetteer we parallelized a clustering algorithm based on expectationmaximization em and thus enabled the construction of large scale mn cluster we demonstrated with the irex dataset for the japanese ner that using the constructed cluster a a gazetteer cluster gazetteer is a effective way of improving the accuracy of ner moreover we demonstrate that the combination of the cluster gazetteer and a gazetteer extracted from wikipedia which is also useful for ner can further improve the accuracy in several case 
constructing an encoding of a concept lattice using short bit vector allows for efficient computation of join operation on the lattice join is the central operation any unification based parser must support we extend the traditional bit vector encoding which represents join failure using the zero vector to count any vector with le than a fixed number of one bit a failure this allows non joinable element to share bit resulting in a smaller vector size a constraint solver is used to construct the encoding and a variety of technique are employed to find near optimal solution and handle timeouts an evaluation is provided comparing the extended representation of failure with traditional bit vector technique 
this paper present an effective method for generating natural language sentence from their underlying meaning representation the method is built on top of a hybrid tree representation that jointly encodes both the meaning representation a well a the natural language in a tree structure by using a tree conditional random field on top of the hybrid tree representation we are able to explicitly model phrase level dependency amongst neighboring natural language phrase and meaning representation component in a simple and natural way we show that the additional dependency captured by the tree conditional random field allows it to perform better than directly inverting a previously developed hybrid tree semantic parser furthermore we demonstrate that the model performs better than a previous state of the art natural language generation model experiment are performed on two benchmark corpus with standard automatic evaluation metric 
we present algorithm for higher order dependency parsing that are third order in the sense that they can evaluate substructure containing three dependency and efficient in the sense that they require only o n time importantly our new parser can utilize both sibling style and grandchild style interaction we evaluate our parser on the penn treebank and prague dependency treebank achieving unlabeled attachment score of and respectively 
classical information extraction ie system fill slot in domain specific frame this paper report on seq a novel open ie system that leverage a domain independent frame to extract ordered sequence such a president of the united state or the most common cause of death in the u s seq leverage regularity about sequence to extract a coherent set of sequence from web text seq nearly double the area under the precision recall curve compared to an extractor that doe not exploit these regularity 
this paper describes a time series model for parsing transcribed speech containing disfluency this model differs from previous parser in it explicit modeling of a buffer of recent word which allows it to recognize repair more easily due to the frequent overlap in word between error and their repair the parser implementing this model is evaluated on the standard switchboard transcribed speech parsing task for overall parsing accuracy and edited word detection 
the variety of engaging interaction among user in social medial distinguishes it from traditional web medium such a feature should be utilized while attempting to provide intelligent service to social medium participant in this article we present a framework to recommend relevant information in internet forum and blog using user comment one of the most representative of user behavior in online discussion when incorporating user comment we consider structural semantic and authority information carried by them one of the most important observation from this work is that semantic content of user comment can play a fairly different role in a different form of social medium when designing a recommendation system for this purpose such a difference must be considered with caution 
this paper describes how external resource can be used to improve parser performance for heavily lexicalised grammar looking at both robustness and efficiency in term of robustness we try using different type of external data to increase lexical coverage and find that simple po tag have the most effect increasing coverage on unseen data by up to we also show that filtering lexical item in a supertagging manner is very effective in increasing efficiency even using vanilla po tag we achieve some efficiency gain but when using detailed lexical type a supertags we manage to halve parsing time with minimal loss of coverage or precision 
this paper introduces mncd a method for automatic evaluation of machine translation the measure is based on normalized compression distance ncd a general information theoretic measure of string similarity and flexible word matching provided by stemming and synonym the mncd measure outperforms ncd in system level correlation to human judgment in english 
we introduce a novel training algorithm for unsupervised grammar induction called zoomed learning given a training set t and a test set s the goal of our algorithm is to identify subset pair ti si of t and s such that when the unsupervised parser is trained on a training subset ti it result on it paired test subset si are better than when it is trained on the entire training set t a successful application of zoomed learning improves overall performance on the full test set s we study our algorithm s effect on the leading algorithm for the task of fully unsupervised parsing seginer in three different english domain wsj brown and genia and show that it improves the parser f score by up to 
we investigate the effectiveness of self training pcfg grammar with latent annotation pcfg la for parsing language with different amount of labeled training data compared to charniak s lexicalized parser the pcfg la parser wa more effectively adapted to a language for which parsing ha been le well developed i e chinese and benefited more from self training we show for the first time that self training is able to significantly improve the performance of the pcfg la parser a single generative parser on both small and large amount of labeled training data our approach achieves state of the art parsing accuracy for a single parser on both english and chinese 
question answering research ha only recently started to spread from short factoid question to more complex one one significant challenge is the evaluation manual evaluation is a difficult time consuming process and not applicable within efficient development of system automatic evaluation requires a corpus of question and answer a definition of what is a correct answer and a way to compare the correct answer to automatic answer produced by a system for this purpose we present a wikipedia based corpus of whyquestions and corresponding answer and article the corpus wa built by a novel method paid participant were contacted through a web interface a procedure which allowed dynamic fast and inexpensive development of data collection method each question in the corpus ha several corresponding partly overlapping answer which is an asset when estimating the correctness of answer in addition the corpus contains information related to the corpus collection process we believe this additional information can be used to post process the data and to develop an automatic approval system for further data collection project conducted in a similar manner 
a central problem in historical linguistics is the identification of historically related cognate word we present a generative phylogenetic model for automatically inducing cognate group structure from unaligned word list our model represents the process of transformation and transmission from ancestor word to daughter word a well a the alignment between the word list of the observed language we also present a novel method for simplifying complex weighted automaton created during inference to counteract the otherwise exponential growth of message size on the task of identifying cognate in a dataset of romance word our model significantly outperforms a baseline approach increasing accuracy by a much a finally we demonstrate that our automatically induced group can be used to successfully reconstruct ancestral word 
translation rule extraction is a fundamental problem in machine translation especially for linguistically syntax based system that need parse tree from either or both side of the bi text the current dominant practice only us best tree which adversely affect the rule set quality due to parsing error so we propose a novel approach which extract rule from a packed forest that compactly encodes exponentially many par experiment show that this method improves translation quality by over bleu point on a state of the art tree to string system and is point better than and twice a fast a extracting on best par when combined with our previous work on forest based decoding it achieves a bleu point improvement over the base line and even outperforms the hierarchical system of hiero by point 
this paper present a novel approach to automatic captioning of geo tagged image by summarizing multiple web document that contain information related to an image s location the summarizer is biased by dependency pattern model towards sentence which contain feature typically provided for different scene type such a those of church bridge etc our result show that summary biased by dependency pattern model lead to significantly higher rouge score than both n gram language model reported in previous work and also wikipedia baseline summary summary generated using dependency pattern also lead to more readable summary than those generated without dependency pattern 
the alignment problem establishing link between corresponding phrase in two related sentence is a important in natural language inference nli a it is in machine translation mt but the tool and technique of mt alignment do not readily transfer to nli where one cannot assume semantic equivalence and for which large volume of bitext are lacking we present a new nli aligner the manli system designed to address these challenge it us a phrase based alignment representation exploit external lexical resource and capitalizes on a new set of supervised training data we compare the performance of manli to existing nli and mt aligners on an nli alignment task over the well known recognizing textual entailment data we show that manli significantly outperforms existing aligners achieving gain of in f over a representative nli aligner and over giza 
traditionally statistical machine translation system have relied on parallel bi lingual data to train a translation model while bi lingual parallel data are expensive to generate monolingual data are relatively common yet monolingual data have been under utilized having been used primarily for training a language model in the target language this paper describes a novel method for utilizing monolingual target data to improve the performance of a statistical machine translation system on news story the method exploit the existence of comparable text multiple text in the target language that discus the same or similar story a found in the source language document for every source document that is to be translated a large monolingual data set in the target language is searched for document that might be comparable to the source document these document are then used to adapt the mt system to increase the probability of generating text that resemble the comparable document experimental result obtained by adapting both the language and translation model show substantial gain over the baseline system 
in this paper we present a machine learning system that find the scope of negation in biomedical text the system consists of two memory based engine one that decides if the token in a sentence are negation signal and another that find the full scope of these negation signal our approach to negation detection differs in two main aspect from existing research on negation first we focus on finding the scope of negation signal instead of determining whether a term is negated or not second we apply supervised machine learning technique whereas most existing system apply rule based algorithm a far a we know this way of approaching the negation scope finding task is novel 
in this paper we employ the centering theory in pronoun resolution from the semantic perspective first diverse semantic role feature with regard to different predicate in a sentence are explored moreover given a pronominal anaphor it relative ranking among all the pronoun in a sentence according to relevant semantic role information and it surface position is incorporated in particular the use of both the semantic role feature and the relative pronominal ranking feature in pronoun resolution is guided by extending the centering theory from the grammatical level to the semantic level in tracking the local discourse focus finally detailed pronominal subcategory feature are incorporated to enhance the discriminative power of both the semantic role feature and the relative pronominal ranking feature experimental result on the ace corpus show that the centering motivated feature contribute much to pronoun resolution 
user click on a url in response to a query are extremely useful predictor of the url s relevance to that query exact match click feature tend to suffer from severe data sparsity issue in web ranking such sparsity is particularly pronounced for new url or long query where each distinct query url pair will rarely occur to remedy this we present a set of straightforward yet informative query url n gram feature that allows for generalization of limited user click data to large amount of unseen query url pair the method is motivated by technique leveraged in the nlp community for dealing with unseen word we find that there are interesting regularity across query and their preferred destination url for example query containing form tend to lead to click on url containing pdf we evaluate our set of new query url feature on a web search ranking task and obtain improvement that are statistically significant at a p value 
adaptor grammar johnson et al b are a non parametric bayesian extension of probabilistic context free grammar pcfgs which in effect learn the probability of entire subtrees in practice this mean that an adaptor grammar learns the structure useful for generating the training data a well a their probability we present several different adaptor grammar that learn to segment phonemic input into word by modeling different linguistic property of the input one of the advantage of a grammar based framework is that it is easy to combine grammar and we use this ability to compare model that capture different kind of linguistic structure we show that incorporating both unsupervised syllabification and collocation finding into the adaptor grammar significantly improves unsupervised word segmentation accuracy over that achieved by adaptor grammar that model only one of these linguistic phenomenon 
we present a framework to extract the most important feature tree fragment from a tree kernel tk space according to their importance in the target kernel based machine e g support vector machine svms in particular our mining algorithm selects the most relevant feature based on svm estimated weight and us this information to automatically infer an explicit representation of the input data the explicit feature a improve our knowledge on the target problem domain and b make large scale learning practical improving training and test time while yielding accuracy in line with traditional tk classifier experiment on semantic role labeling and question classification illustrate the above claim 
we consider a parsed text corpus a an instance of a labelled directed graph where node represent word and weighted directed edge represent the syntactic relation between them we show that graph walk combined with existing technique of supervised learning can be used to derive a task specific word similarity measure in this graph we also propose a new path constrained graph walk method in which the graph walk process is guided by high level knowledge about meaningful edge sequence path empirical evaluation on the task of named entity coordinate term extraction show that this framework is preferable to vector based model for small sized corpus it is also shown that the path constrained graph walk algorithm yield both performance and scalability gain 
we present three novel method of compactly storing very large n gram language model these method use substantially le space than all known approach and allow n gram probability or count to be retrieved in constant time at speed comparable to modern language modeling toolkits our basic approach generates an explicit minimal perfect hash function that map all n gram in a model to distinct integer to enable storage of associated value extension of this approach exploit distributional characteristic of n gram data to reduce storage cost including variable length coding of value and the use of tiered structure that partition the data for more efficient storage we apply our approach to storing the full google web t n gram set and all to gram of the gigaword newswire corpus for the billion n gram of gigaword for example we can store full count information at a cost of byte per n gram around of the cost when using the current state of the art approach or quantized count for byte per n gram for application that are tolerant of a certain class of relatively innocuous error where unseen n gram may be accepted a rare n gram we can reduce the latter cost to below byte per n gram 
grounded language model represent the relationship between word and the non linguistic context in which they are said this paper describes how they are learned from large corpus of unlabeled video and are applied to the task of automatic speech recognition of sport video result show that grounded language model improve perplexity and word error rate over text based language model and further support video information retrieval better than human generated speech transcription 
targeted paraphrasing is a new approach to the problem of obtaining cost effective reasonable quality translation that make use of simple and inexpensive human computation by monolingual speaker in combination with machine translation the key insight behind the process is that it is possible to spot likely translation error with only monolingual knowledge of the target language and it is possible to generate alternative way to say the same thing i e paraphrase with only monolingual knowledge of the source language evaluation demonstrate that this approach can yield substantial improvement in translation quality 
we introduce a method for solving substitution cipher using low order letter n gram model this method enforces global constraint using integer programming and it guarantee that no decipherment key is overlooked we carry out extensive empirical experiment showing how decipherment accuracy varies a a function of cipher length and n gram order we also make an empirical investigation of shannon s theory of uncertainty in decipherment 
to improve the mandarin large vocabulary continuous speech recognition lvcsr a unified framework based approach is introduced to exploit multi level linguistic knowledge in this framework each knowledge source is represented by a weighted finite state transducer wfst and then they are combined to obtain a so called analyzer for integrating multi level knowledge source due to the uniform transducer representation any knowledge source can be easily integrated into the analyzer a long a it can be encoded into wfsts moreover a the knowledge in each level is modeled independently and the combination is processed in the model level the information inherently in each knowledge source ha a chance to be thoroughly exploited by simulation the effectiveness of the analyzer is investigated and then a lvcsr system embedding the presented analyzer is evaluated experimental result reveal that this unified framework is an effective approach which significantly improves the performance of speech recognition with a relative reduction of character error rate on the hub test set a widely used mandarin speech recognition task 
in this paper we introduce the new task of social event extraction from text we distinguish two broad type of social event depending on whether only one or both party are aware of the social contact we annotate part of automatic content extraction ace data and perform experiment using support vector machine with kernel method we use a combination of structure derived from phrase structure tree and dependency tree a characteristic of our event which distinguishes them from ace event is that the participating entity can be spread far across the parse tree we use syntactic and semantic insight to devise a new structure derived from dependency tree and show that this play a role in achieving the best performing system for both social event detection and classification task we also use three data sampling approach to solve the problem of data skewness sampling method improve the f measure for the task of relation detection by over absolute over the baseline 
minimum error rate training mert is an effective mean to estimate the feature function weight of a linear model such that an automated evaluation criterion for measuring system performance can directly be optimized in training to accomplish this the training procedure determines for each feature function it exact error surface on a given set of candidate translation the feature function weight are then adjusted by traversing the error surface combined over all sentence and picking those value for which the resulting error count reach a minimum typically candidate in mert are represented a n best list which contain the n most probable translation hypothesis produced by a decoder in this paper we present a novel algorithm that allows for efficiently constructing and representing the exact error surface of all translation that are encoded in a phrase lattice compared to n best mert the number of candidate translation thus taken into account increase by several order of magnitude the proposed method is used to train the feature function weight of a phrase based statistical machine translation system experiment conducted on the nist translation task show significant runtime improvement and moderate bleu score gain over n best mert 
in adding syntax to statistical mt there is a tradeoff between taking advantage of linguistic analysis versus allowing the model to exploit linguistically unmotivated mapping learned from parallel training data a number of previous effort have tackled this tradeoff by starting with a commitment to linguistically motivated analysis and then nding appropriate way to soften that commitment we present an approach that explores the tradeoff from the other direction starting with a context free translation model learned directly from aligned parallel text and then adding soft constituent level constraint based on par of the source language we obtain substantial improvement in performance for translation from chinese and arabic to english 
discourse reference notably coreference and bridging play an important role in many text understanding application but their impact on textual entailment is yet to be systematically understood on the basis of an in depth analysis of entailment instance we argue that discourse reference have the potential of substantially improving textual entailment recognition and identify a number of research direction towards this goal 
scoring sentence in document given abstract summary created by human is important in extractive multi document summarization in this paper we formulate extractive summarization a a two step learning problem building a generative model for pattern discovery and a regression model for inference we calculate score for sentence in document cluster based on their latent characteristic using a hierarchical topic model then using these score we train a regression model based on the lexical and structural characteristic of the sentence and use the model to score sentence of new document to form a summary our system advance current state of the art improving rouge score by generated summary are le redundant and more coherent based upon manual quality evaluation 
in this paper we investigate how an accurate question classifier contributes to a question answering system we first present a maximum entropy me based question classifier which make use of head word feature and their wordnet hypernym we show that our question classifier can achieve the state of the art performance in the standard uiuc question dataset we then investigate quantitatively the contribution of this question classifier to a feature driven question answering system with our accurate question classifier and some standard question answer feature our question answering system performs close to the state of the art using trec corpus 
the traditional mention pair model for coreference resolution cannot capture information beyond mention pair for both learning and testing to deal with this problem we present an expressive entity mention model that performs coreference resolution at an entity level the model adopts the inductive logic programming ilp algorithm which provides a relational way to organize different knowledge of entity and mention the solution can explicitly express relation between an entity and the contained mention and automatically learn first order rule important for coreference decision the evaluation on the ace data set show that the ilp based entity mention model is effective for the coreference resolution task 
we present a simple yet powerful hierarchical search algorithm for automatic word alignment our algorithm induces a forest of alignment from which we can efficiently extract a ranked k best list we score a given alignment within the forest with a flexible linear discriminative model incorporating hundred of feature and trained on a relatively small amount of annotated data we report result on arabic english word alignment and translation task our model outperforms a giza model baseline by point in f measure yielding a bleu score increase over a state of the art syntax based machine translation system 
this paper proposes a fast and simple unsupervised word segmentation algorithm that utilizes the local predictability of adjacent character sequence while searching for a least effort representation of the data the model us branching entropy a a mean of constraining the hypothesis space in order to efficiently obtain a solution that minimizes the length of a two part mdl code an evaluation with corpus in japanese thai english and the childes corpus for research in language development reveals that the algorithm achieves an accuracy comparable to that of the state of the art method in unsupervised word segmentation in a significantly reduced computational time 
although machine translation mt is a very active research field which is receiving an increasing amount of attention from the research community the result that current mt system are capable of producing are still quite far away from perfection because of this and in order to build system that yield correct translation human knowledge must be integrated into the translation process which will be carried out in our case in an interactive predictive ip framework in this paper we show that considering mouse action a a significant information source for the underlying system improves the productivity of the human translator involved in addition we also show that the initial translation that the mt system provides can be quickly improved by an expert by only performing additional mouse action in this work we will be using word graph a an efficient interface between a phrase based mt system and the ip engine 
this paper explores the use of innovative kernel based on syntactic and semantic structure for a target relation extraction task syntax is derived from constituent and dependency parse tree whereas semantics concern to entity type and lexical sequence we investigate the effectiveness of such representation in the automated relation extraction from text we process the above data by mean of support vector machine along with the syntactic tree the partial tree and the word sequence kernel our study on the ace corpus illustrates that the combination of the above kernel achieves high effectiveness and significantly improves the current state of the art 
automatic word alignment is a key step in training statistical machine translation system despite much recent work on word alignment method alignment accuracy increase often produce little or no improvement in machine translation quality in this work we analyze a recently proposed agreement constrained em algorithm for unsupervised alignment model we attempt to tease apart the effect that this simple but effective modification ha on alignment precision and recall trade offs and how rare and common word are affected across several language pair we propose and extensively evaluate a simple method for using alignment model to produce alignment better suited for phrase based mt system and show significant gain a measured by bleu score in end to end translation system for six language pair used in recent mt competition 
we show that jointly parsing a bitext can substantially improve parse quality on both side in a maximum entropy bitext parsing model we define a distribution over source tree target tree and node to node alignment between them feature include monolingual parse score and various measure of syntactic divergence using the translated portion of the chinese treebank our model is trained iteratively to maximize the marginal likelihood of training tree pair with alignment treated a latent variable the resulting bitext parser outperforms state of the art monolingual parser baseline by f at predicting english side tree and f at predicting chinese side tree the highest published number on these corpus moreover these improved tree yield a bleu increase when used in a downstream mt evaluation 
different summarization requirement could make the writing of a good summary more difficult or easier summary length and the characteristic of the input are such constraint influencing the quality of a potential summary in this paper we report the result of a quantitative analysis on data from large scale evaluation of multi document summarization empirically confirming this hypothesis we further show that feature measuring the cohesiveness of the input are highly correlated with eventual summary quality and that it is possible to use these a feature to predict the difficulty of new unseen summarization input 
this paper explores the use of set expansion se to improve question answering qa when the expected answer is a list of entity belonging to a certain class given a small set of seed se algorithm mine textual resource to produce an extended list including additional member of the class represented by the seed we explore the hypothesis that a noise resistant se algorithm can be used to extend candidate answer produced by a qa system and generate a new list of answer that is better than the original list produced by the qa system we further introduce a hybrid approach which combine the original answer from the qa system with the output from the se algorithm experimental result for several state of the art qa system show that the hybrid system performs better than the qa system alone when tested on list question data from past trec evaluation 
we present a novel framework for automated extraction and approximation of numerical object attribute such a height and weight from the web given an object attribute pair we discover and analyze attribute information for a set of comparable object in order to infer the desired value this allows u to approximate the desired numerical value even when no exact value can be found in the text our framework make use of relation defining pattern and wordnet similarity information first we obtain from the web and wordnet a list of term similar to the given object then we retrieve attribute value for each term in this list and information that allows u to compare different object in the list and to infer the attribute value range finally we combine the retrieved data for all term from the list to select or approximate the requested value we evaluate our method using automated question answering wordnet enrichment and comparison with answer given in wikipedia and by leading search engine in all of these our framework provides a significant improvement 
in recent year research in natural language processing ha increasingly focused on normalizing sm message different well defined approach have been proposed but the problem remains far from being solved best system achieve a word error rate this paper present a method that share similarity with both spell checking and machine translation approach the normalization part of the system is entirely based on model trained from a corpus evaluated in french by fold cross validation the system achieves a word error rate and a bleu score 
we develop a general method to match unstructured text review to a structured list of object for this we propose a language model for generating review that incorporates a description of object and a generic review language model this mixture model give u a principled method to find given a review the object most likely to be the topic of the review extensive experiment and analysis on review from yelp show that our language model based method vastly outperforms traditional tf idf based method 
in this paper we present a joint content selection and compression model for single document summarization the model operates over a phrase based representation of the source document which we obtain by merging information from pcfg parse tree and dependency graph using an integer linear programming formulation the model learns to select and combine phrase subject to length coverage and grammar constraint we evaluate the approach on the task of generating story highlight a small number of brief self contained sentence that allow reader to quickly gather information on news story experimental result show that the model s output is comparable to human written highlight in term of both grammaticality and content 
set expansion refers to expanding a partial set of seed object into a more complete set one system that doe set expansion is seal set expander for any language which expands entity automatically by utilizing resource from the web in a language independent fashion in this paper we illustrated in detail the construction of character level wrapper for set expansion implemented in seal we also evaluated several kind of wrapper for set expansion and showed that character based wrapper perform better than html based wrapper in addition we demonstrated a technique that extends seal to learn binary relational concept e g x is the mayor of the city y from only two seed we also show that the extended seal ha good performance on our evaluation datasets which includes english and chinese thus demonstrating language independence 
statistical parser have become increasingly accurate to the point where they are useful in many natural language application however estimating parsing accuracy on a wide variety of domain and genre is still a challenge in the absence of gold standard parse tree in this paper we propose a technique that automatically take into account certain characteristic of the domain of interest and accurately predicts parser performance on data from these new domain a a result we have a cheap no annotation involved and effective recipe for measuring the performance of a statistical parser on any given domain 
method that learn from prior information about input feature such a generalized expectation ge have been used to train accurate model with very little effort in this paper we propose an active learning approach in which the machine solicits label on feature rather than instance in both simulated and real user experiment on two sequence labeling task we show that our active learning method outperforms passive learning with feature a well a traditional active learning with instance preliminary experiment suggest that novel interface which intelligently solicit label on multiple feature facilitate more efficient annotation 
we examine the problem of content selection in statistical novel sentence generation our approach model the process performed by professional editor when incorporating material from additional sentence to support some initially chosen key summary sentence a process we refer to a sentence augmentation we propose and evaluate a method called seed and grow for selecting such auxiliary information additionally we argue that this can be performed using schema a represented by word pair co occurrence and demonstrate it use in statistical summary sentence generation evaluation result are supportive indicating that a schema model significantly improves over the baseline 
information extraction ie research typically focus on clean text input however an ie engine serving real application yield many false alarm due to le well formed input for example ie in a multilingual broadcast processing system ha to deal with inaccurate automatic transcription and translation the resulting presence of non target language text in this case and non language material interspersed in data from other application raise the research problem of making ie robust to such noisy input text we address one such ie task entity mention detection we describe augmenting a statistical mention detection system in order to reduce false alarm from spurious passage the diverse nature of input noise lead u to pursue a multi faceted approach to robustness for our english language system at various miss rate we eliminate of false alarm on input from other latin alphabet language in another experiment representing scenario in which genre specific training is infeasible we process real financial transaction text containing mixed language and data set code on these data because we do not train on data like it we achieve a smaller but significant improvement these gain come with virtually no loss in accuracy on clean english text 
almost all chinese language processing task involve word segmentation of the language input a their first step thus robust and reliable segmentation technique are always required to make sure those task well performed in recent year machine learning and sequence labeling model such a conditional random field crfs are often used in segmenting chinese text compared with traditional lexicon driven model machine learned model achieve higher f measure score but machine learned model heavily depend on training material although they can effectively process text from the same domain a the training text they perform relatively poorly when text from new domain are to be processed in this paper we propose to use x statistic when training an svm hmm based segmentation model to improve it ability to recall oov word and then use bootstrapping strategy to maintain it ability to recall iv word experiment show the approach proposed in this paper enhances the domain portability of the chinese word segmentation model and prevents drastic decline in performance when processing text across domain 
linear context free rewriting system lcfrss are a grammar formalism capable of modeling discontinuous phrase many parsing application use lcfrss where the fan out a measure of the discontinuity of phrase doe not exceed we present an efficient algorithm for optimal reduction of the length of production right hand side in lcfrss with fan out at most this result in asymptotical running time improvement for known parsing algorithm for this class 
this paper describes a lattice based decoder for hierarchical phrase based translation the decoder is implemented with standard wfst operation a an alternative to the well known cube pruning procedure we find that the use of wfsts rather than k best list requires le pruning in translation search resulting in fewer search error direct generation of translation lattice in the target language better parameter optimization and improved translation performance when rescoring with long span language model and mbr decoding we report translation experiment for the arabic to english and chinese to english nist translation task and contrast the wfstbased hierarchical decoder with hierarchical translation under cube pruning 
we formulate dependency parsing a a graphical model with the novel ingredient of global constraint we show how to apply loopy belief propagation bp a simple and effective tool for approximate learning and inference a a parsing algorithm bp is both asymptotically and empirically efficient even with second order feature or latent variable which would make exact parsing considerably slower or np hard bp need only o n time with a small constant factor furthermore such feature significantly improve parse accuracy over exact first order method incorporating additional feature would increase the runtime additively rather than multiplicatively 
mining sentiment from user generated content is a very important task in natural language processing an example of such content is threaded discussion which act a a very important tool for communication and collaboration in the web threaded discussion include e mail e mail list bulletin board newsgroups and internet forum most of the work on sentiment analysis ha been centered around finding the sentiment toward product or topic in this work we present a method to identify the attitude of participant in an online discussion toward one another this would enable u to build a signed network representation of participant interaction where every edge ha a sign that indicates whether the interaction is positive or negative this is different from most of the research on social network that ha focused almost exclusively on positive link the method is experimentally tested using a manually labeled set of discussion post the result show that the proposed method is capable of identifying attitudinal sentence and their sign with high accuracy and that it outperforms several other baseline 
this paper present a supervised method for resolving metonymy we enhance a commonly used feature set with feature extracted based on collocation information from corpus generalized using lexical and encyclopedic knowledge to determine the preferred sense of the potentially metonymic word using method from unsupervised word sense disambiguation the methodology developed address one issue related to metonymy resolution the influence of local context the method developed is applied to the metonymy resolution task from semeval the result obtained higher for the country subtask on a par for the company subtask compared to participating system confirm that lexical encyclopedic and collocation information can be successfully combined for metonymy resolution 
we present a discriminative substring decoder for transliteration this decoder extends recent approach for discriminative character transduction by allowing for a list of known target language word an important resource for transliteration our approach improves upon sherif and kondrak s b state of the art decoder creating a relative improvement in transliteration accuracy on a japanese katakana to english task we also conduct a controlled comparison of two feature paradigm for discriminative training indicator and hybrid generative feature surprisingly the generative hybrid outperforms it purely discriminative counterpart despite losing access to rich source context feature finally we show that machine transliteration have a positive impact on machine translation quality improving human judgment by on a point scale 
semi supervised word alignment aim to improve the accuracy of automatic word alignment by incorporating full or partial manual alignment motivated by standard active learning query sampling framework like uncertainty marginand query by committee sampling we propose multiple query strategy for the alignment link selection task our experiment show that by active selection of uncertain and informative link we reduce the overall manual effort involved in elicitation of alignment link data for training a semi supervised word aligner 
in domain with insufficient matched training data language model are often constructed by interpolating component model trained from partially matched corpus since the n gram from such corpus may not be of equal relevance to the target domain we propose an n gram weighting technique to adjust the component n gram probability based on feature derived from readily available segmentation and metadata information for each corpus using a log linear combination of such feature the resulting model achieves up to a absolute word error rate reduction over a linearly interpolated baseline language model on a lecture transcription task 
this article address the issue of extracting context and answer of question from post of online discussion forum in previous work general purpose graphical model have been employed without any customization to this specific extraction problem instead in this article we propose a unified approach to context and answer extraction by customizing the structural support vector machine method the customization enables our proposal to explore various relation among sentence of post and complex structure of thread we design new inference algorithm to find or approximate the most violated constraint by utilizing the specific structure of forum thread which enables u to efficiently find the global optimum of the customized optimizing problem we also optimize practical performance measure by varying loss function experimental result show that our method are both promising and flexible 
there have been considerable attempt to incorporate semantic knowledge into coreference resolution system different knowledge source such a wordnet and wikipedia have been used to boost the performance in this paper we propose new way to extract wordnet feature this feature along with other feature such a named entity feature can be used to build an accurate semantic class sc classifier in addition we analyze the sc classification error and propose to use relaxed sc agreement feature the proposed accurate sc classifier and the relaxation of sc agreement feature on ace coreference evaluation can boost our baseline system by and using muc score and anaphor accuracy respectively 
combining information extraction system yield significantly higher quality resource than each system in isolation in this paper we generalize such a mixing of source and feature in a framework called ensemble semantics we show very large gain in entity extraction by combining state of the art distributional and pattern based system with a large set of feature from a webcrawl query log and wikipedia experimental result on a web scale extraction of actor athlete and musician show significantly higher mean average precision score gain compared with the current state of the art 
this paper describes a new automatic method for japanese predicate argument structure analysis the method learns relevant feature to assign case role to the argument of the target predicate using the feature of the word located closest to the target predicate under various constraint such a dependency type word semantic category part of speech functional word and predicate voice we constructed decision list in which these feature were sorted by their learned weight using our method we integrated the task of semantic role labeling and zero pronoun identification and achieved a improvement compared with a baseline method in a sentence level performance analysis 
determining whether two term in text have an ancestor relation e g toyota and car or a sibling relation e g toyota and honda is an essential component of textual inference in nlp application such a question answering summarization and recognizing textual entailment significant work ha been done on developing stationary knowledge source that could potentially support these task but these resource often suffer from low coverage noise and are inflexible when needed to support term that are not identical to those placed in them making their use a general purpose background knowledge resource difficult in this paper rather than building a stationary hierarchical structure of term and relation we describe a system that given two term determines the taxonomic relation between them using a machine learning based approach that make use of existing resource moreover we develop a global constraint optimization inference process and use it to leverage an existing knowledge base also to enforce relational constraint among term and thus improve the classifier prediction our experimental evaluation show that our approach significantly outperforms other system built upon existing well known knowledge source 
we propose a novel unsupervised approach for distinguishing literal and non literal use of idiomatic expression our model combine an unsupervised and a supervised classifier the former base it decision on the cohesive structure of the context and label training data for the latter which can then take a larger feature space into account we show that a combination of both classifier lead to significant improvement over using the unsupervised classifier alone 
we describe the strategy currently pursued for verbalising owl ontology by sentence in controlled natural language i e combining generic rule for realising logical pattern with ontology specific lexicon for realising atomic term for individual class and property and argue that it success depends on assumption about the complexity of term and axiom in the ontology we then show through analysis of a corpus of ontology that although these assumption could in principle be violated they are overwhelmingly respected in practice by ontology developer 
even the entire web corpus doe not explicitly answer all question yet inference can uncover many implicit answer but where do inference rule come from this paper investigates the problem of learning inference rule from web text in an unsupervised domain independent manner the sherlock system described herein is a first order learner that acquires over horn clause from web text sherlock embodies several innovation including a novel rule scoring function based on statistical relevance salmon et al which is effective on ambiguous noisy and incomplete web extraction our experiment show that inference over the learned rule discovers three time a many fact at precision a the textrunner system which merely extract fact explicitly stated in web text 
minimum error rate training mert is a bottleneck for current development in statistical machine translation because it is limited in the number of weight it can reliably optimize building on the work of watanabe et al we explore the use of the mira algorithm of crammer et al a an alternative to mert we first show that by parallel processing and exploiting more of the parse forest we can obtain result using mira that match or surpass mert in term of both translation quality and computational cost we then test the method on two class of feature that address deficiency in the hiero hierarchical phrase based model first we simultaneously train a large number of marton and resnik s soft syntactic constraint and second we introduce a novel structural distortion model in both case we obtain significant improvement in translation performance optimizing them in combination for a total of feature weight we improve performance by bleu on a subset of the nist arabic english evaluation data 
there is plenty of evidence that emotion analysis ha many valuable application in this study a blog emotion corpus is constructed for chinese emotional expression analysis this corpus contains manual annotation of eight emotional category expect joy love surprise anxiety sorrow angry and hate emotion intensity emotion holder target emotional word phrase degree word negative word conjunction rhetoric punctuation and other linguistic expression that indicate emotion annotation agreement analysis for emotion class and emotional word and phrase are described then using this corpus we explore emotion expression in chinese and present the analysis on them 
probabilistic latent topic model have recently enjoyed much success in extracting and analyzing latent topic in text in an unsupervised way one common deficiency of existing topic model though is that they would not work well for extracting cross lingual latent topic simply because word in different language generally do not co occur with each other in this paper we propose a way to incorporate a bilingual dictionary into a probabilistic topic model so that we can apply topic model to extract shared latent topic in text data of different language specifically we propose a new topic model called probabilistic cross lingual latent semantic analysis pclsa which extends the probabilistic latent semantic analysis plsa model by regularizing it likelihood function with soft constraint defined based on a bilingual dictionary both qualitative and quantitative experimental result show that the pclsa model can effectively extract cross lingual latent topic from multilingual text data 
cube pruning is a fast inexact method for generating the item of a beam decoder in this paper we show that cube pruning is essentially equivalent to a search on a specific search space with specific heuristic we use this insight to develop faster and exact variant of cube pruning 
this paper proposes a method to correct english verb form error made by non native speaker a basic approach is template matching on parse tree the proposed method improves on this approach in two way to improve recall irregularity in parse tree caused by verb form error are taken into account to improve precision n gram count are utilized to filter proposed correction evaluation on non native corpus representing two genre and mother tongue show promising result 
the columbia arabic treebank catib is a database of syntactic analysis of arabic sentence catib contrast with previous approach to arabic treebanking in it emphasis on speed with some constraint on linguistic richness two basic idea inspire the catib approach no annotation of redundant information and using representation and terminology inspired by traditional arabic syntax we describe catib s representation and annotation procedure and report on interannotator agreement and speed 
traditional information extraction ie take a relation name and hand tagged example of that relation a input open ie is a relationindependent extraction paradigm that is tailored to massive and heterogeneous corpus such a the web an open ie system extract a diverse set of relational tuples from text without any relation specific input how is open ie possible we analyze a sample of english sentence to demonstrate that numerous relationship are expressed using a compact set of relation independent lexico syntactic pattern which can be learned by an open ie system what are the tradeoff between open ie and traditional ie we consider this question in the context of two task first when the number of relation is massive and the relation themselves are not pre specified we argue that open ie is necessary we then present a new model for open ie called o crf and show that it achieves increased precision and nearly double the recall than the model employed by textrunner the previous stateof the art open ie system second when the number of target relation is small and their name are known in advance we show that o crf is able to match the precision of a traditional extraction system though at substantially lower recall finally we show how to combine the two type of system into a hybrid that achieves higher precision than a traditional extractor with comparable recall 
query expansion by word alteration alternative form of a word is often used in web search to replace word stemming this allows user to specify particular word form in a query however if many alteration are added query traffic will be greatly increased in this paper we propose method to select only a few useful word alteration for query expansion the selection is made according to the appropriateness of the alteration to the query context using a bigram language model or according to it expected impact on the retrieval effectiveness using a regression model our experiment on two trec collection will show that both method only select a few expansion term but the retrieval effectiveness can be improved significantly 
lexical gap between query and question document have been a major issue in question retrieval on large online question and answer q a collection previous study address the issue by implicitly expanding query with the help of translation model pre constructed using statistical technique however since it is possible for unimportant word e g non topical word common word to be included in the translation model a lack of noise control on the model can cause degradation of retrieval performance this paper investigates a number of empirical method for eliminating unimportant word in order to construct compact translation model for retrieval purpose experiment conducted on a real world q a collection show that substantial improvement in retrieval performance can be achieved by using compact translation model 
current turn taking approach for spoken dialogue system rely on the speaker releasing the turn before the other can take it this reliance result in restricted interaction that can lead to inefficient dialogue in this paper we present a model we refer to a importance driven turn bidding that treat turn taking a a negotiative process each conversant bid for the turn based on the importance of the intended utterance and reinforcement learning is used to indirectly learn this parameter we find that importance driven turn bidding performs better than two current turn taking approach in an artificial collaborative slot filling domain the negotiative nature of this model creates efficient dialogue and support the improvement of mixed initiative interaction 
seed sampling is critical in semi supervised learning this paper proposes a clustering based stratified seed sampling approach to semi supervised learning first various clustering algorithm are explored to partition the unlabeled instance into different stratum with each stratum represented by a center then diversity motivated intra stratum sampling is adopted to choose the center and additional instance from each stratum to form the unlabeled seed set for an oracle to annotate finally the labeled seed set is fed into a bootstrapping procedure a the initial labeled data we systematically evaluate our stratified bootstrapping approach in the semantic relation classification subtask of the ace rdc relation detection and classification task in particular we compare various clustering algorithm on the stratified bootstrapping performance experimental result on the ace rdc corpus show that our clustering based stratified bootstrapping approach achieves the best f score of on the sub task of semantic relation classification approaching the one with golden clustering 
this paper proposes a method that speed up a classifier trained with many conjunctive feature combination of primitive feature the key idea is to precompute a partial result the weight of primitive feature vector that appear frequently in the target nlp task a trie compactly store the primitive feature vector with their weight and it enables the classifier to find for a given feature vector it longest prefix feature vector whose weight ha already been computed experimental result for a japanese dependency parsing task show that our method speeded up the svm and llm classifier of the parser which achieved accuracy of by a factor of 
in this paper we first introduce a new architecture for parsing bidirectional incremental parsing we propose a novel algorithm for incremental construction which can be applied to many structure learning problem in nlp we apply this algorithm to ltag dependency parsing and achieve significant improvement on accuracy over the previous best result on the same data set 
measure word in chinese are used to indicate the count of noun conventional statistical machine translation smt system do not perform well on measure word generation due to data sparseness and the potential long distance dependency between measure word and their corresponding head word in this paper we propose a statistical model to generate appropriate measure word of noun for an english to chinese smt system we model the probability of measure word generation by utilizing lexical and syntactic knowledge from both source and target sentence our model work a a post processing procedure over output of statistical machine translation system and can work with any smt system experimental result show our method can achieve high precision and recall in measure word generation 
we address the task of unsupervised po tagging we demonstrate that good result can be obtained using the robust em hmm learner when provided with good initial condition even with incomplete dictionary we present a family of algorithm to compute effective initial estimation p t w we test the method on the task of full morphological disambiguation in hebrew achieving an error reduction of over a strong uniform distribution baseline we also test the same method on the standard wsj unsupervised po tagging task and obtain result competitive with recent state ofthe art method while using simple and efficient learning method 
linguistic steganography is concerned with hiding information in natural language text one of the major transformation used in linguistic steganography is synonym substitution however few existing study have studied the practical application of this approach in this paper we propose two improvement to the use of synonym substitution for encoding hidden bit of information first we use the web t google n gram corpus for checking the applicability of a synonym in context and we evaluate this method using data from the semeval lexical substitution task second we address the problem that arises from word with more than one sense which creates a potential ambiguity in term of which bit are encoded by a particular word we develop a novel method in which word are the vertex in a graph synonym are linked by edge and the bit assigned to a word are determined by a vertex colouring algorithm this method ensures that each word encodes a unique sequence of bit without cutting out large number of synonym and thus maintaining a reasonable embedding capacity 
we present pem the first fully automatic metric to evaluate the quality of paraphrase and consequently that of paraphrase generation system our metric is based on three criterion adequacy fluency and lexical dissimilarity the key component in our metric is a robust and shallow semantic similarity measure based on pivot language n gram that allows u to approximate adequacy independently of lexical similarity human evaluation show that pem achieves high correlation with human judgment 
we propose the playcoref game whose purpose is to obtain substantial amount of text data with the coreference annotation we provide a description of the game design that cover the strategy the instruction for the player the input text selection and preparation and the score evaluation 
machine learning method have been extensively employed in developing mt evaluation metric and several study show that it can help to achieve a better correlation with human assessment adopting the regression svm framework this paper discus the linguistic motivated feature formulation strategy we argue that blind combination of available feature doe not yield a general metric with high correlation rate with human assessment instead certain simple intuitive feature serve better in establishing the regression svm evaluation model with six feature selected we show evidence to support our view through a few experiment in this paper 
the performance of machine translation system varies greatly depending on the source and target language involved determining the contribution of different characteristic of language pair on system performance is key to knowing what aspect of machine translation to improve and which are irrelevant this paper investigates the effect of different explanatory variable on the performance of a phrase based system for european language pair we show that three factor are strong predictor of performance in isolation the amount of reordering the morphological complexity of the target language and the historical relatedness of the two language together these factor contribute to the variability of the performance of the system 
in this research we aim to detect subjective sentence in multimodal conversation we introduce a novel technique wherein subjective pattern are learned from both labeled and unlabeled data using n gram word sequence with varying level of lexical instantiation applying this technique to meeting speech and email conversation we gain significant improvement over state of the art approach furthermore we show that coupling the pattern based approach with feature that capture characteristic of general conversation structure yield additional improvement 
determining whether a textual phrase denotes a functional relation i e a relation that map each domain element to a unique range element is useful for numerous nlp task such a synonym resolution and contradiction detection previous work on this problem ha relied on either counting method or lexico syntactic pattern however determining whether a relation is functional by analyzing mention of the relation in a corpus is challenging due to ambiguity synonymy anaphora and other linguistic phenomenon we present the leibniz system that overcomes these challenge by exploiting the synergy between the web corpus and freely available knowledge resource such a free base it first computes multiple typed functionality score representing functionality of the relation phrase when it argument are constrained to specific type it then aggregate these score to predict the global functionality for the phrase leibniz outperforms previous work increasing area under the precision recall curve from to we utilize leibniz to generate the first public repository of automatically identified functional relation 
conditional random field crfs are a widely used approach for supervised sequence labelling notably due to their ability to handle large description space and to integrate structural dependency between label even for the simple linear chain model taking structure into account implies a number of parameter and a computational effort that grows quadratically with the cardinality of the label set in this paper we address the issue of training very large crfs containing up to hundred output label and several billion feature efficiency stem here from the sparsity induced by the use of a l penalty term based on our own implementation we compare three recent proposal for implementing this regularization strategy our experiment demonstrate that very large crfs can be trained efficiently and that very large model are able to improve the accuracy while delivering compact parameter set 
we introduce an extension to ccg that allows form and function to be represented simultaneously reducing the proliferation of modifier category seen in standard ccg analysis we can then remove the non combinatory rule ccgbank us to address this problem producing a grammar that is fully lexicalised and far le ambiguous there are intrinsic benefit to full lexicalisation such a semantic transparency and simpler domain adaptation the clearest advantage is a improvement in parse speed which come with only a small reduction in accuracy 
one of the main obstacle to high performance word sense disambiguation wsd is the knowledge acquisition bottleneck in this paper we present a methodology to automatically extend wordnet with large amount of semantic relation from an encyclopedic resource namely wikipedia we show that when provided with a vast amount of high quality semantic relation simple knowledge lean disambiguation algorithm compete with state of the art supervised wsd system in a coarse grained all word setting and outperform them on gold standard domain specific datasets 
current statistical machine translation system usually extract rule from bilingual corpus annotated with best alignment they are prone to learn noisy rule due to alignment mistake we propose a new structure called weighted alignment matrix to encode all possible alignment for a parallel text compactly the key idea is to assign a probability to each word pair to indicate how well they are aligned we design new algorithm for extracting phrase pair from weighted alignment matrix and estimating their probability our experiment on multiple language pair show that using weighted matrix achieves consistent improvement over using n best list in significant le extraction time 
we describe an approach for acquiring the domain specific dialog knowledge required to configure a task oriented dialog system that us human human interaction data the key aspect of this problem are the design of a dialog information representation and a learning approach that support capture of domain information from in domain dialog to represent a dialog for a learning purpose we based our representation the form based dialog structure representation on an observable structure we show that this representation is sufficient for modeling phenomenon that occur regularly in several dissimilar task oriented domain including information access and problem solving with the goal of ultimately reducing human annotation effort we examine the use of unsupervised learning technique in acquiring the component of the form based representation i e task subtask and concept these technique include statistical word clustering based on mutual information and kullback liebler distance texttiling hmm based segmentation and bisecting k mean document clustering with some modification to make these algorithm more suitable for inferring the structure of a spoken dialog the unsupervised learning algorithm show promise 
in this paper we consider the problem of generating candidate correction for the task of correcting error in text we focus on the task of correcting error in preposition usage made by non native english speaker using discriminative classifier the standard approach to the problem assumes that the set of candidate correction for a preposition consists of all preposition choice participating in the task we determine likely preposition confusion using an annotated corpus of non native text and use this knowledge to produce smaller set of candidate we propose several method of restricting candidate set these method exclude candidate preposition that are not observed a valid correction in the annotated corpus and take into account the likelihood of each preposition confusion in the non native text we find that restricting candidate to those that are observed in the non native data improves both the precision and the recall compared to the approach that view all preposition a possible candidate furthermore the approach that take into account the likelihood of each preposition confusion is shown to be the most effective 
in this paper we propose a novel string todependency algorithm for statistical machine translation with this new framework we employ a target dependency language model during decoding to exploit long distance word relation which are unavailable with a traditional n gram language model our experiment show that the string to dependency decoder achieves point improvement in bleu and point improvement in ter compared to a standard hierarchical string tostring system on the nist chinese english evaluation set 
this paper investigates the effect of direction in phrase based statistial machine translation decoding we compare a typical phrase based machine translation decoder using a left to right decoding strategy to a right to left decoder we also investigate the effectiveness of a bidirectional decoding strategy that integrates both mono directional approach with the aim of reducing the effect due to language specificity our experimental evaluation wa extensive based on different language pair and gave the surprising result that for most of the language pair it wa better decode from right to left than from left to right a expected the relative performance of left to right and right to left strategy proved to be highly language dependent the bidirectional approach outperformed the both the left to right strategy and the right to left strategy showing consistent improvement that appeared to be unrelated to the specific language used for translation bidirectional decoding gave rise to an improvement in performance over a left to right decoding strategy in term of the bleu score in of our experiment 
quantifying the semantic relevance between question and their candidate answer is essential to answer detection in social medium corpus in this paper a deep belief network is proposed to model the semantic relevance for question answer pair observing the textual similarity between the community driven question answering cqa dataset and the forum dataset we present a novel learning strategy to promote the performance of our method on the social community datasets without hand annotating work the experimental result show that our method outperforms the traditional approach on both the cqa and the forum corpus 
a large body of recent research ha been investigating the acquisition and application of applied inference knowledge such knowledge may be typically captured a entailment rule applied over syntactic representation efficient inference with such knowledge then becomes a fundamental problem starting out from a formalism for entailment rule application we present a novel packed data structure and a corresponding algorithm for it scalable implementation we proved the validity of the new algorithm and established it efficiency analytically and empirically 
there is a growing research interest in opinion retrieval a on line user opinion are becoming more and more popular in business social network etc practically speaking the goal of opinion retrieval is to retrieve document which entail opinion or comment relevant to a target subject specified by the user s query a fundamental challenge in opinion retrieval is information representation existing research focus on document based approach and document are represented by bag of word however due to loss of contextual information this representation fails to capture the associative information between an opinion and it corresponding target it cannot distinguish different degree of a sentiment word when associated with different target this in turn seriously affect opinion retrieval performance in this paper we propose a sentence based approach based on a new information representation namely topic sentiment word pair to capture intra sentence contextual information between an opinion and it target additionally we consider inter sentence information to capture the relationship among the opinion on the same topic finally the two type of information are combined in a unified graph based model which can effectively rank the document compared with existing approach experimental result on the coae dataset showed that our graph based model achieved significant improvement 
this paper study the effect of training data on binary text classification and postulate that negative training data is not needed and may even be harmful for the task traditional binary classification involves building a classifier using labeled positive and negative training example the classifier is then applied to classify test instance into positive and negative class a fundamental assumption is that the training and test data are identically distributed however this assumption may not hold in practice in this paper we study a particular problem where the positive data is identically distributed but the negative data may or may not be so many practical text classification and retrieval application fit this model we argue that in this setting negative training data should not be used and that pu learning can be employed to solve the problem empirical evaluation ha been conducted to support our claim this result is important a it may fundamentally change the current binary classification paradigm 
this paper examines tagging model for spontaneous english speech transcript we analyze the performance of state of the art tagging model either generative or discriminative left to right or bidirectional with or without latent annotation together with the use of tobi break index and several method for segmenting the speech transcript i e conversation side speaker turn or human annotated sentence based on these study we observe that bidirectional model tend to achieve better accuracy level than left to right model generative model seem to perform somewhat better than discriminative model on this task and prosody improves tagging performance of model on conversation side but ha much le impact on smaller segment we conclude that although the use of break index can indeed significantly improve performance over baseline model without them on conversation side tagging accuracy improves more by using smaller segment for which the impact of the break index is marginal 
information extraction ie system seek to distill semantic relation from natural language text but most system use supervised learning of relation specific example and are thus limited by the availability of training data open ie system such a textrunner on the other hand aim to handle the unbounded number of relation found on the web but how well can these open system perform this paper present woe an open ie system which improves dramatically on textrunner s precision and recall the key to woe s performance is a novel form of self supervised learning for open extractor using heuristic match between wikipedia infobox attribute value and corresponding sentence to construct training data like textrunner woe s extractor eschews lexicalized feature and handle an unbounded set of semantic relation woe can operate in two mode when restricted to po tag feature it run a quickly a textrunner but when set to use dependency parse feature it precision and recall rise even higher 
the standard set of rule defined in combinatory categorial grammar ccg fails to provide satisfactory analysis for a number of syntactic structure found in natural language these structure can be analyzed elegantly by augmenting ccg with a class of rule based on the combinator d curry and feys we show two way to derive the d rule one based on unary composition and the other based on a logical characterization of ccg s rule base baldridge we also show how eisner s normal form constraint follow from this logic ensuring that the d rule do not lead to spurious ambiguity 
automatically detecting human social intention from spoken conversation is an important task for dialogue understanding since the social intention of the speaker may differ from what is perceived by the hearer system that analyze human conversation need to be able to extract both the perceived and the intended social meaning we investigate this difference between intention and perception by using a spoken corpus of speed date in which both the speaker and the listener rated the speaker on flirtatiousness our flirtation detection system us prosodic dialogue and lexical feature to detect a speaker s intent to flirt with up to accuracy significantly outperforming the baseline but also outperforming the human inter locuters our system address lexical feature sparsity given the small amount of training data by using an autoencoder network to map sparse lexical feature vector into compressed feature our analysis show that human are very poor perceiver of intended flirtatiousness instead often projecting their own intended behavior onto their interlocutor 
model of latent document semantics such a the mixture of multinomial model and latent dirichlet allocation have received substantial attention for their ability to discover topical semantics in large collection of text in an effort to apply such model to noisy optical character recognition ocr text output we endeavor to understand the effect that character level noise can have on unsupervised topic modeling we show the effect both with document level topic analysis document clustering and with word level topic analysis lda on both synthetic and real world ocr data a expected experimental result show that performance decline a word error rate increase common technique for alleviating these problem such a filtering low frequency word are successful in enhancing model quality but exhibit failure trend similar to model trained on unprocessed ocr output in the case of lda to our knowledge this study is the first of it kind 
virtual evidence ve first introduced by pearl provides a convenient way of incorporating prior knowledge into bayesian network this work generalizes the use of ve to undirected graphical model and in particular to conditional random field crfs we show that ve can be naturally encoded into a crf model a potential function more importantly we propose a novel semi supervised machine learning objective for estimating a crf model integrated with ve the objective can be optimized using the expectation maximization algorithm while maintaining the discriminative nature of crfs when evaluated on the classified data our approach significantly outperforms the best known solution reported on this task 
while phrase based statistical machine translation system currently deliver state of the art performance they remain weak on word order change current phrase reordering model can properly handle swap between adjacent phrase but they typically lack the ability to perform the kind of long distance re ordering possible with syntax based system in this paper we present a novel hierarchical phrase reordering model aimed at improving non local reordering which seamlessly integrates with a standard phrase based system with little loss of computational efficiency we show that this model can successfully handle the key example often used to motivate syntax based system such a the rotation of a prepositional phrase around a noun phrase we contrast our model with reordering model commonly used in phrase based system and show that our approach provides statistically significant bleu point gain for two language pair chinese english on mt and on mt and arabic english on mt 
we present a novel approach to integrate transliteration into hindi to urdu statistical machine translation we propose two probabilistic model based on conditional and joint probability formulation that are novel solution to the problem our model consider both transliteration and translation when translating a particular hindi word given the context whereas in previous work transliteration is only used for translating oov out of vocabulary word we use transliteration a a tool for disambiguation of hindi homonym which can be both translated or transliterated or transliterated differently based on different context we obtain final bleu score of conditional probability model and joint probability model a compared to for a baseline phrase based system and for a system which transliterates oov word in the baseline system this indicates that transliteration is useful for more than only translating oov word for language pair like hindi urdu 
we improve the quality of statistical machine translation smt by applying model that predict word form from their stem using extensive morphological and syntactic information from both the source and target language our inflection generation model are trained independently of the smt system we investigate different way of combining the inflection prediction component with the smt system by training the base mt system on fully inflected form or on word stem we applied our inflection generation model in translating english into two morphologically complex language russian and arabic and show that our model improves the quality of smt over both phrasal and syntax based smt system according to bleu and human judgement 
we advance the state of the art for discriminatively trained machine translation system by presenting novel probabilistic inference and search method for synchronous grammar by approximating the intractable space of all candidate translation produced by intersecting an ngram language model with a synchronous grammar we are able to train and decode model incorporating million of sparse heterogeneous feature further we demonstrate the power of the discriminative training paradigm by extracting structured syntactic feature and achieving increase in translation performance 
in the s plot unit were proposed a a conceptual knowledge structure for representing and summarizing narrative story our research explores whether current nlp technology can be used to automatically produce plot unit representation for narrative text we create a system called aesop that exploit a variety of existing resource to identify affect state and applies projection rule to map the affect state onto the character in a story we also use corpus based technique to generate a new type of affect knowledge base verb that impart positive or negative state onto their patient e g being eaten is an undesirable state but being fed is a desirable state we harvest these patient polarity verb from a web corpus using two technique co occurrence with evil kind agent pattern and bootstrapping over conjunction of verb we evaluate the plot unit representation produced by our system on a small collection of aesop s fable 
a new approach to large scale information extraction exploit both web document and query log to acquire thousand of opendomain class of instance along with relevant set of open domain class attribute at precision level previously obtained only on small scale manually assembled class 
this paper show that discriminative reranking with an averaged perceptron model yield substantial improvement in realization quality with ccg the paper confirms the utility of including language model log probability a feature in the model which prior work on discriminative training with log linear model for hpsg realization had called into question the perceptron model allows the combination of multiple n gram model to be optimized and then augmented with both syntactic feature and discriminative n gram feature the full model yield a state of the art bleu score of on section of the ccgbank to our knowledge the best score reported to date using a reversible corpus engineered grammar 
binarization of synchronous context free grammar scfg is essential for achieving polynomial time complexity of decoding for scfg parsing based machine translation system in this paper we first investigate the excess edge competition issue caused by a left heavy binary scfg derived with the method of zhang et al then we propose a new binarization method to mitigate the problem by exploring other alternative equivalent binary scfgs we present an algorithm that iteratively improves the resulting binary scfg and empirically show that our method can improve a string to tree statistical machine translation system based on the synchronous binarization method in zhang et al on the nist machine translation evaluation task 
we propose a cascaded linear model for joint chinese word segmentation and partof speech tagging with a character based perceptron a the core combined with realvalued feature such a language model the cascaded model is able to efficiently utilize knowledge source that are inconvenient to incorporate into the perceptron directly experiment show that the cascaded model achieves improved accuracy on both segmentation only and joint segmentation and part of speech tagging on the penn chinese treebank we obtain an error reduction of on segmentation and on joint segmentation and part of speech tagging over the perceptron only baseline 
syllable play an important role in speech synthesis and recognition we present several different approach to the syllabification of phoneme we investigate approach based on linguistic theory of syllabification a well a a discriminative learning technique that combine support vector machine and hidden markov model technology our experiment on english dutch and german demonstrate that our transparent implementation of the sonority sequencing principle is more accurate than previous implementation and that our language independent svm based approach advance the current state of the art achieving word accuracy of over in english and in german and dutch 
existing word similarity measure are not robust to data sparseness since they rely only on the point estimation of word context profile obtained from a limited amount of data this paper proposes a bayesian method for robust distributional word similarity the method us a distribution of context profile obtained by bayesian estimation and take the expectation of a base similarity measure under that distribution when the context profile are multinomial distribution the prior are dirichlet and the base measure is the bhattacharyya coefficient we can derive an analytical form that allows efficient calculation for the task of word similarity estimation using a large amount of web data in japanese we show that the proposed measure give better accuracy than other well known similarity measure 
the recently introduced online confidence weighted cw learning algorithm for binary classification performs well on many binary nlp task however for multi class problem cw learning update and inference cannot be computed analytically or solved a convex optimization problem a they are in the binary case we derive learning algorithm for the multi class cw setting and provide extensive evaluation using nine nlp datasets including three derived from the recently released new york time corpus our best algorithm out performs state of the art online and batch method on eight of the nine task we also show that the confidence information maintained during learning yield useful probabilistic information at test time 
the computation of meaning similarity a operationalized by vector based model ha found widespread use in many task ranging from the acquisition of synonym and paraphrase to word sense disambiguation and textual entailment vector based model are typically directed at representing word in isolation and thus best suited for measuring similarity out of context in his paper we propose a probabilistic framework for measuring similarity in context central to our approach is the intuition that word meaning is represented a a probability distribution over a set of latent sens and is modulated by context experimental result on lexical substitution and word similarity show that our algorithm outperforms previously proposed model 
this paper address the problem of learning to map sentence to logical form given training data consisting of natural language sentence paired with logical representation of their meaning previous approach have been designed for particular natural language or specific meaning representation here we present a more general method the approach induces a probabilistic ccg grammar that represents the meaning of individual word and defines how these meaning can be combined to analyze complete sentence we use higher order unification to define a hypothesis space containing all grammar consistent with the training data and develop an online learning algorithm that efficiently search this space while simultaneously estimating the parameter of a log linear parsing model experiment demonstrate high accuracy on benchmark data set in four language with two different meaning representation 
the availability of database of image labeled with keywords is necessary for developing and evaluating image annotation model dataset collection is however a costly and time consuming task in this paper we exploit the vast resource of image available on the web we create a database of picture that are naturally embedded into news article and propose to use their caption a a proxy for annotation keywords experimental result show that an image annotation model can be developed on this dataset alone without the overhead of manual annotation we also demonstrate that the news article associated with the picture can be used to boost image annotation performance 
the aim of this research is to provide a principled account of the generation of embedded construction called parenthetical and to implement the result in a natural language generation system parenthetical construction are frequently used in text written in a good writing style and have an important role in text understanding we propose a framework to model the rhetorical property of parenthetical based on a corpus study and develop a unified natural language generation architecture which integrates syntax semantics rhetorical and document structure into a complex representation which can be easily extended to handle parenthetical 
relationship discovery is the task of identifying salient relationship between named entity in text we propose novel approach for two sub task of the problem identifying the entity of interest and partitioning and describing the relation based on their semantics in particular we show that term frequency pattern can be used effectively instead of supervised ner and that the p median clustering objective function naturally uncovers relation exemplar appropriate for describing the partitioning furthermore we introduce a novel application of relationship discovery the unsupervised identification of protein protein interaction phrase 
we present a system that learns to follow navigational natural language direction where traditional model learn from linguistic annotation or word distribution our approach is grounded in the world learning by apprenticeship from route through a map paired with english description lacking an explicit alignment between the text and the reference path make it difficult to determine what portion of the language describe which aspect of the route we learn this correspondence with a reinforcement learning algorithm using the deviation of the route we follow from the intended path a a reward signal we demonstrate that our system successfully ground the meaning of spatial term like above and south into geometric property of path 
letter phoneme alignment is usually generated by a straightforward application of the em algorithm we explore several alternative alignment method that employ phonetics integer programming and set of constraint and propose a novel approach of refining the em alignment by aggregation of best alignment we perform both intrinsic and extrinsic evaluation of the assortment of method we show that our proposed em aggregation algorithm lead to the improvement of the state of the art in letter to phoneme conversion on several different data set 
we use the technique of svm anchoring to demonstrate that lexical feature extracted from a training corpus are not necessary to obtain state of the art result on task such a named entity recognition and chunking while standard model require a many a k distinct feature we derive model with a little a k feature that perform a well or better on different domain these robust reduced model indicate that the way rare lexical feature contribute to classification in nlp is not fully understood contrastive error analysis with and without lexical feature indicates that lexical feature do contribute to resolving some semantic and complex syntactic ambiguity but we find this contribution doe not generalize outside the training corpus a a general strategy we believe lexical feature should not be directly derived from a training corpus but instead carefully inferred and selected from other source 
the intersection of tree transducer based translation model with n gram language model result in huge dynamic program for machine translation decoding we propose a multipass coarse to fine approach in which the language model complexity is incrementally introduced in contrast to previous order based bigram to trigram approach we focus on encoding based method which use a clustered encoding of the target language across various encoding scheme and for multiple language pair we show speed ups of up to time over single pas decoding while improving bleu score moreover our entire decoding cascade for trigram language model is faster than the corresponding bigram pas alone of a bigram to trigram decoder 
distance based windowless word assocation measure have only very recently appeared in the nlp literature and their performance compared to existing windowed or frequency based measure is largely unknown we conduct a large scale empirical comparison of a variety of distance based and frequency based measure for the reproduction of syntagmatic human assocation norm overall our result show an improvement in the predictive power of windowless over windowed measure this provides support to some of the previously published theoretical advantage and make windowless approach a promising avenue to explore further this study also serf a a first comparison of windowed method across numerous human association datasets during this comparison we also introduce some novel variation of window based measure which perform a well a or better in the human association norm task than established measure 
there is a widely held belief in the natural language and computational linguistics community that semantic role labeling srl is a significant step toward improving important application e g question answering and information extraction in this paper we present an srl system for modern standard arabic that exploit many aspect of the rich morphological feature of the language the experiment on the pilot arabic propbank data show that our system based on support vector machine and kernel method yield a global srl f score of which improves the current state of the art in arabic srl 
this paper study two issue non isomorphic structure translation and target syntactic structure usage for statistical machine translation in the context of forest based tree to tree sequence translation for the first issue we propose a novel non isomorphic translation framework to capture more non isomorphic structure mapping than traditional tree based and tree sequence based translation method for the second issue we propose a parallel space searching method to generate hypothesis using tree to string model and evaluate it syntactic goodness using tree to tree tree sequence model this not only reduces the search complexity by merging spurious ambiguity translation path and solves the data sparseness issue in training but also serf a a syntax based target language model for better grammatical generation experiment result on the benchmark data show our proposed two solution are very effective achieving significant performance improvement over baseline when applying to different translation model 
bleu is the de facto standard for evaluation and development of statistical machine translation system we describe three real world situation involving comparison between different version of the same system where one can obtain improvement in bleu score that are questionable or even absurd these situation arise because bleu lack the property of decomposability a property which is also computationally convenient for various application we propose a very conservative modification to bleu and a cross between bleu and word error rate that address these issue while improving correlation with human judgment 
in a previous work of ours chinnakotla et al we introduced a novel framework for pseudo relevance feedback prf called multiprf given a query in one language called source we used english a the assisting language to improve the performance of prf for the source language mulitiprf showed remarkable improvement over plain model based feedback mbf uniformly for language viz french german hungarian and finnish with english a the assisting language this fact inspired u to study the effect of any source assistant pair on multiprf performance from out of a set of language with widely different characteristic viz dutch english finnish french german and spanish carrying this further we looked into the effect of using two assisting language together on prf the present paper is a report of these investigation their result and conclusion drawn therefrom while performance improvement on multiprf is observed whatever the assisting language and whatever the source observation are mixed when two assisting language are used simultaneously interestingly the performance improvement is more pronounced when the source and assisting language are closely related e g french and spanish 
recent syntactic extension of statistical translation model work with a synchronous context free or tree substitution grammar extracted from an automatically parsed parallel corpus the decoder accompanying these extension typically exceed quadratic time complexity this paper extends the direct translation model dtm with syntax while maintaining linear time decoding we employ a linear time parsing algorithm based on an eager incremental interpretation of combinatory categorial grammar ccg a every input word is processed the local parsing decision resolve ambiguity eagerly by selecting a single supertag operator pair for extending the dependency parse incrementally alongside translation feature extracted from the derived parse tree we explore syntactic feature extracted from the incremental derivation process our empirical experiment show that our model significantly outperforms the state of the art dtm system 
we present a simple linguistically motivated method for characterizing the semantic relation that hold between two noun the approach leverage the vast size of the web in order to build lexically specific feature the main idea is to look for verb preposition and coordinating conjunction that can help make explicit the hidden relation between the target noun using these feature in instance based classifier we demonstrate state of the art result on various relational similarity problem including mapping noun modifier pair to abstract relation like time location and container characterizing noun noun compound in term of abstract linguistic predicate like cause use and from classifying the relation between nominal in context and solving sat verbal analogy problem in essence the approach put together some existing idea showing that they apply generally to various semantic task finding that verb are especially useful feature 
this paper present a parse and paraphrase paradigm to ass the degree of sentiment for product review sentiment identification ha been well studied however most previous work provides binary polarity only positive and negative and the polarity of sentiment is simply reversed when a negation is detected the extraction of lexical feature such a unigram bigram also complicates the sentiment classification task a linguistic structure such a implicit long distance dependency is often disregarded in this paper we propose an approach to extracting adverb adjective noun phrase based on clause structure obtained by parsing sentence into a hierarchical representation we also propose a robust general solution for modeling the contribution of adverbial and negation to the score for degree of sentiment in an application involving extracting aspect based pro and con from restaurant review we obtained a relative improvement in recall through the use of parsing method while also improving precision 
automated essay scoring is one of the most important educational application of natural language processing recently researcher have begun exploring method of scoring essay with respect to particular dimension of quality such a coherence technical error and relevance to prompt but there is relatively little work on modeling organization we present a new annotated corpus and propose heuristic based and learning based approach to scoring essay along the organization dimension utilizing technique that involve sequence alignment alignment kernel and string kernel 
automatic processing of metaphor can be clearly divided into two subtasks metaphor recognition distinguishing between literal and metaphorical language in a text and metaphor interpretation identifying the intended literal meaning of a metaphorical expression both of them have been repeatedly addressed in nlp this paper is the first comprehensive and systematic review of the existing computational model of metaphor the issue of metaphor annotation in corpus and the available resource 
strong indication of perspective can often come from collocation of arbitrary length for example someone writing get the government out of my x is typically expressing a conservative rather than progressive viewpoint however going beyond unigram or bigram feature in perspective classification give rise to problem of data sparsity we address this problem using nonparametric bayesian modeling specifically adaptor grammar johnson et al we demonstrate that an adaptive na ve bayes model capture multiword lexical usage associated with perspective and establishes a new state of the art for perspective classification result using the bitter lemon corpus a collection of essay about mid east issue from israeli and palestinian point of view 
we combine two complementary idea for learning supertaggers from highly ambiguous lexicon grammar informed tag transition and model minimized via integer programming each strategy on it own greatly improves performance over basic expectation maximization training with a bitag hidden markov model which we show on the ccgbank and ccg tut corpus the strategy provide further error reduction when combined we describe a new two stage integer programming strategy that efficiently deal with the high degree of ambiguity on these datasets while obtaining the full effect of model minimization 
we present a syntactically enriched vector model that support the computation of contextualized semantic representation in a quasi compositional fashion it employ a systematic combination of firstand second order context vector we apply our model to two different task and show that i it substantially outperforms previous work on a paraphrase ranking task and ii achieves promising result on a wordsense similarity task to our knowledge it is the first time that an unsupervised method ha been applied to this task 
this paper proposes new algorithm to compute the sense similarity between two unit word phrase rule etc from parallel corpus the sense similarity score are computed by using the vector space model we then apply the algorithm to statistical machine translation by computing the sense similarity between the source and target side of translation rule pair similarity score are used a additional feature of the translation model to improve translation performance significant improvement are obtained over a state of the art hierarchical phrase based machine translation system 
a characterization of the expressive power of synchronous tree adjoining grammar stag in term of tree transducer or equivalently synchronous tree substitution grammar is developed essentially a stag corresponds to an extended tree transducer that us explicit substitution in both the input and output this characterization allows the easy integration of stag into toolkits for extended tree transducer moreover the applicability of the characterization to several representational and algorithmic problem is demonstrated 
we describe a new approach to smt adaptation that weight out of domain phrase pair according to their relevance to the target domain determined by both how similar to it they appear to be and whether they belong to general language or not this extends previous work on discriminative weighting by using a finer granularity focusing on the property of instance rather than corpus component and using a simpler training procedure we incorporate instance weighting into a mixture model framework and find that it yield consistent improvement over a wide range of baseline 
morphological disambiguation proceeds in stage an analyzer provides all possible analysis for a given token and a stochastic disambiguation module pick the most likely analysis in context when the analyzer doe not recognize a given token we hit the problem of unknown in large scale corpus unknown appear at a rate of to depending on the genre and the maturity of the lexi 
we present a propbank semantic role labeling system for english that is integrated with a dependency parser to tackle the problem of joint syntactic semantic analysis the system relies on a syntactic and a semantic subcomponent the syntactic model is a projective parser using pseudo projective transformation and the semantic model us global inference mechanism on top of a pipeline of classifier the complete syntactic semantic output is selected from a candidate pool generated by the subsystem we evaluate the system on the conll test set using segment based and dependency based metric using the segment based conll metric our system achieves a near state of the art f figure of on the wsj brown test set or if punctuation is treated consistently using a dependency based metric the f figure of our system is on the test set from conll our system is the first dependency based semantic role labeler for propbank that rival constituent based system in term of performance 
parallel web page are important source of training data for statistical machine translation in this paper we present a new approach to sentence alignment on parallel web page parallel web page tend to have parallel structure and the structural correspondence can be indicative information for identifying parallel sentence in our approach the web page is represented a a tree and a stochastic tree alignment model is used to exploit the structural correspondence for sentence alignment experiment show that this method significantly enhances alignment accuracy and robustness for parallel web page which are much more diverse and noisy than standard parallel corpus such a hansard with improved sentence alignment performance web mining system are able to acquire parallel sentence of higher quality from the web 
we present a method for detecting and correcting multiple real word spelling error using the google web t gram data set and a normalized and modified version of the longest common subsequence lcs string matching algorithm our method is focused mainly on how to improve the detection recall the fraction of error correctly detected and the correction recall the fraction of error correctly amended while keeping the respective precision the fraction of detection or amendment that are correct a high a possible evaluation result on a standard data set show that our method outperforms two other method on the same task 
letter substitution cipher encode a document from a known or hypothesized language into an unknown writing system or an unknown encoding of a known writing system it is a problem that can occur in a number of practical application such a in the problem of determining the encoding of electronic document in which the language is known but the encoding standard is not it ha also been used in relation to ocr application in this paper we introduce an exact method for deciphering message using a generalization of the viterbi algorithm we test this model on a set of cipher developed from various web site and find that our algorithm ha the potential to be a viable practical method for efficiently solving decipherment problem 
this paper present a new approach to selecting the initial seed set using stratified sampling strategy in bootstrapping based semi supervised learning for semantic relation classification first the training data is partitioned into several stratum according to relation type subtypes then relation instance are randomly sampled from each stratum to form the initial seed set we also investigate different augmentation strategy in iteratively adding reliable instance to the labeled set and find that the bootstrapping procedure may stop at a reasonable point to significantly decrease the training time without degrading too much in performance experiment on the ace rdc and corpus show the stratified sampling strategy contributes more than the bootstrapping procedure itself this suggests that a proper sampling strategy is critical in semi supervised learning 
previous study evaluate simulated dialog corpus using evaluation measure which can be automatically extracted from the dialog system log however the validity of these automatic measure ha not been fully proven in this study we first recruit human judge to ass the quality of three simulated dialog corpus and then use human judgment a the gold standard to validate the conclusion drawn from the automatic measure we observe that it is hard for the human judge to reach good agreement when asked to rate the quality of the dialog from given perspective however the human rating give consistent ranking of the quality of simulated corpus generated by different simulation model when building prediction model of human judgment using previously proposed automatic measure we find that we cannot reliably predict human rating using a regression model but we can predict human ranking by a ranking model 
automatic evaluation of machine translation mt quality is essential to developing high quality mt system various evaluation metric have been proposed and bleu is now used a the de facto standard metric however when we consider translation between distant language pair such a japanese and english most popular metric e g bleu nist per and ter do not work well it is well known that japanese and english have completely different word order and special care must be paid to word order in translation otherwise translation with wrong word order often lead to misunderstanding and incomprehensibility for instance smt based japanese to english translator tend to translate a because b a b because a thus word order is the most important problem for distant language translation however conventional evaluation metric do not significantly penalize such word order mistake therefore locally optimizing these metric lead to inadequate translation in this paper we propose an automatic evaluation metric based on rank correlation coefficient modified with precision our meta evaluation of the ntcir patmt je task data show that this metric outperforms conventional metric 
in this paper we present a semi supervised method for automatic speech act recognition in email and forum the major challenge of this task is due to lack of labeled data in these two genre our method leverage labeled data in the switchboard damsl and the meeting recorder dialog act database and applies simple domain adaptation technique over a large amount of unlabeled email and forum data to address this problem our method us automatically extracted feature such a phrase and dependency tree called subtree feature for semi supervised learning empirical result demonstrate that our model is effective in email and forum speech act recognition 
paraphrase have proved to be useful in many application including machine translation question answering summarization and information retrieval paraphrase acquisition method that use a single monolingual corpus often produce only syntactic paraphrase we present a method for obtaining surface paraphrase using a gb billion word monolingual corpus our method achieves an accuracy of around on the paraphrase acquisition task we further show that we can use these paraphrase to generate surface pattern for relation extraction our pattern are much more precise than those obtained by using a state of the art baseline and can extract relation with more than precision for each of the test relation 
domain adaptation the problem of adapting a natural language processing system trained in one domain to perform well in a different domain ha received significant attention this paper address an important problem for deployed system that ha received little attention detecting when such adaptation is needed by a system operating in the wild i e performing classification over a stream of unlabeled example our method us a distance a metric for detecting shift in data stream combined with classification margin to detect domain shift we empirically show effective domain shift detection on a variety of data set and shift condition 
we address the task of computing vector space representation for the meaning of word occurrence which can vary widely according to context this task is a crucial step towards a robust vector based compositional account of sentence meaning we argue that existing model for this task do not take syntactic structure sufficiently into account we present a novel structured vector space model that address these issue by incorporating the selectional preference for word argument position this make it possible to integrate syntax into the computation of word meaning in context in addition the model performs at and above the state of the art for modeling the contextual adequacy of paraphrase 
we describe our experiment with training algorithm for tree to tree synchronous tree substitution grammar stsg for monolingual translation task such a sentence compression and paraphrasing these translation task are characterized by the relative ability to commit to parallel parse tree and availability of word alignment yet the unavailability of large scale data calling for a bayesian tree to tree formalism we formalize nonparametric bayesian stsg with epsilon alignment in full generality and provide a gibbs sampling algorithm for posterior inference tailored to the task of extractive sentence compression we achieve improvement against a number of baseline including expectation maximization and variational bayes training illustrating the merit of nonparametric inference over the space of grammar a opposed to sparse parametric inference with a fixed grammar 
in this paper we present a novel approach to enhance hierarchical phrase based machine translation system with linguistically motivated syntactic feature rather than directly using treebank category a in previous study we learn a set of linguistically guided latent syntactic category automatically from a source side parsed word aligned parallel corpus based on the hierarchical structure among phrase pair a well a the syntactic structure of the source side in our model each x nonterminal in a scfg rule is decorated with a real valued feature vector computed based on it distribution of latent syntactic category these feature vector are utilized at decoding time to measure the similarity between the syntactic analysis of the source side and the syntax of the scfg rule that are applied to derive translation our approach maintains the advantage of hierarchical phrase based translation system while at the same time naturally incorporates soft syntactic constraint 
this paper describes a computational approach to resolving the true referent of a named mention of a person in the body of an email a generative model of mention generation is used to guide mention resolution result on three relatively small collection indicate that the accuracy of this approach compare favorably to the best known technique and result on the full cmu enron collection indicate that it scale well to larger collection 
rapid and inexpensive technique for automatic transcription of speech have the potential to dramatically expand the type of content to which information retrieval technique can be productively applied but limitation in accuracy and robustness must be overcome before that promise can be fully realized combining retrieval result from system built on various errorful representation of the same collection offer some potential to address these challenge this paper explores that potential by applying generalized additive model to optimize the combination of ranked retrieval result obtained using transcript produced automatically for the same spoken content by substantially different recognition system topic averaged retrieval effectiveness better than any previously reported for the same collection wa obtained and even larger gain are apparent when using an alternative measure emphasizing result on the most difficult topic 
an important relation in information extraction is the part whole relation ontological study mention several type of this relation in this paper we show that the traditional practice of initializing minimally supervised algorithm with a single set that mix seed of different type fails to capture the wide variety of part whole pattern and tuples the result obtained with mixed seed ultimately converge to one of the part whole relation type we also demonstrate that all the different type of part whole relation can still be discovered regardless of the type characterized by the initializing seed we performed our experiment with a state of the art information extraction algorithm 
active learning is well suited to many problem in natural language processing where unlabeled data may be abundant but annotation is slow and expensive this paper aim to shed light on the best active learning approach for sequence labeling task such a information extraction and document segmentation we survey previously used query selection strategy for sequence model and propose several novel algorithm to address their shortcoming we also conduct a large scale empirical comparison using multiple corpus which demonstrates that our proposed method advance the state of the art 
previous work on ordering event in text ha typically focused on local pairwise decision ignoring globally inconsistent label however temporal ordering is the type of domain in which global constraint should be relatively easy to represent and reason over this paper present a framework that informs local decision with two type of implicit global constraint transitivity a before b and b before c implies a before c and time expression normalization e g last month is before yesterday we show how these constraint can be used to create a more densely connected network of event and how global consistency can be enforced by incorporating these constraint into an integer linear programming framework we present result on two event ordering task showing a absolute increase in the accuracy of before after classification over a pairwise model 
discriminative feature based method are widely used in natural language processing but sentence parsing is still dominated by generative method while prior feature based dynamic programming parser have restricted training and evaluation to artificially short sentence we present the first general featurerich discriminative parser based on a conditional random field model which ha been successfully scaled to the full wsj parsing data our efficiency is primarily due to the use of stochastic optimization technique a well a parallelization and chart prefiltering on wsj we attain a state of the art f score of a relative reduction in error over previous model while being two order of magnitude faster on sentence of length our system achieves an f score of a relative reduction in error over a generative baseline 
in this work we develop and evaluate a wide range of feature space for deriving levinstyle verb classification levin we perform the classification experiment using bayesian multinomial regression an efficient log linear modeling framework which we found to outperform svms for this task with the proposed feature space our experiment suggest that subcategorization frame are not the most effective feature for automatic verb classification a mixture of syntactic information and lexical information work best for this task 
this work concern automatic topic segmentation of email conversation we present a corpus of email thread manually annotated with topic and evaluate annotator reliability to our knowledge this is the first such email corpus we show how the existing topic segmentation model i e lexical chain segmenter lcseg and latent dirichlet allocation lda which are solely based on lexical information can be applied to email by pointing out where these method fail and what any desired model should consider we propose two novel extension of the model that not only use lexical information but also exploit finer level conversation structure in a principled way empirical evaluation show that lcseg is a better model than lda for segmenting an email thread into topical cluster and incorporating conversation structure into these model improves the performance significantly 
the sense of a preposition is related to the semantics of it dominating prepositional phrase knowing the sense of a preposition could help to correctly classify the semantic role of the dominating prepositional phrase and vice versa in this paper we propose a joint probabilistic model for word sense disambiguation of preposition and semantic role labeling of prepositional phrase our experiment on the propbank corpus show that jointly learning the word sense and the semantic role lead to an improvement over state of the art individual classifier model on the two task 
we propose a top down algorithm for extracting k best list from a parser our algorithm tka is a variant of the kbest a ka algorithm of paul and klein in contrast to ka which performs an inside and outside pas before performing k best extraction bottom up tka performs only the inside pas before extracting k best list top down tka maintains the same optimality and efficiency guarantee of ka but is simpler to both specify and implement 
cross language document summarization is a task of producing a summary in one language for a document set in a different language existing method simply use machine translation for document translation or summary translation however current machine translation service are far from satisfactory which result in that the quality of the cross language summary is usually very poor both in readability and content in this paper we propose to consider the translation quality of each sentence in the english to chinese cross language summarization process first the translation quality of each english sentence in the document set is predicted with the svm regression method and then the quality score of each sentence is incorporated into the summarization process finally the english sentence with high translation quality and high informative ness are selected and translated to form the chinese summary experimental result demonstrate the effectiveness and usefulness of the proposed approach 
the core adjunct argument distinction is a basic one in the theory of argument structure the task of distinguishing between the two ha strong relation to various basic nlp task such a syntactic parsing semantic role labeling and subcategorization acquisition this paper present a novel unsupervised algorithm for the task that us no supervised model utilizing instead state of the art syntactic induction algorithm this is the first work to tackle this task in a fully unsupervised scenario 
three method are proposed to classify query by intent cqi e g navigational informational commercial etc following mixed initiative dialog system search engine should distinguish navigational query where the user is taking the initiative from other query where there are more opportunity for system initiative e g suggestion ad the query intent problem ha a number of useful application for search engine affecting how many if any advertisement to display which result to return and how to arrange the result page click log are used a a substitute for annotation click on ad are evidence for commercial intent other type of click are evidence for other intent we start with a simple na ve bayes baseline that work well when there is plenty of training data when training data is le plentiful we back off to nearby url in a click graph using a method similar to word sense disambiguation thus we can infer that designer trench is commercial because it is close to www saksfifthavenue com which is known to be commercial the baseline method wa designed for precision and the backoff method wa designed for recall both method are fast and do not require crawling webpage we recommend a third method a hybrid of the two that doe no harm when there is plenty of training data and generalizes better when there isn t a a strong baseline for the cqi task 
we present a new syntactic parser that work left to right and top down thus maintaining a fully connected parse tree for a few alternative parse hypothesis all of the commonly used statistical parser use context free dynamic programming algorithm and a such work bottom up on the entire sentence thus they only find a complete fully connected parse at the very end in contrast both subjective and experimental evidence show that people understand a sentence word to word a they go along or close to it the constraint that the parser keep one or more fully connected syntactic tree is intended to operationalize this cognitive fact our parser achieves a new best result for top down parser of a error reduction over the previous single parser best result for parser of this type of roark the improved performance is due to embracing the very large feature set available in exchange for giving up dynamic programming 
online forum discussion often contain vast amount of question that are the focus of discussion extracting context and answer together with the question will yield not only a coherent forum summary but also a valuable qa knowledge base in this paper we propose a general framework based on conditional random field crfs to detect the context and answer of question from forum thread we improve the basic framework by skip chain crfs and d crfs to better accommodate the feature of forum for better performance experimental result show that our technique are very promising 
we define the crouching dirichlet hidden markov model cdhmm an hmm for part of speech tagging which draw state prior distribution for each local document context this simple modification of the hmm take advantage of the dichotomy in natural language between content and function word in contrast a standard hmm draw all prior distribution once over all state and it is known to perform poorly in unsupervised and semi supervised po tagging this modification significantly improves unsupervised po tagging performance across several measure on five data set for four language we also show that simply using different hyperparameter value for content and function word state in a standard hmm which we call hmm is surprisingly effective 
representing document by vector that are independent of language enhances machine translation and multilingual text categorization we use discriminative training to create a projection of document from multiple language into a single translingual vector space we explore two variant to create these projection oriented principal component analysis opca and coupled probabilistic latent semantic analysis cplsa both of these variant start with a basic model of document pca and plsa each model is then made discriminative by encouraging comparable document pair to have similar vector representation we evaluate these algorithm on two task parallel document retrieval for wikipedia and europarl document and cross lingual text classification on reuters the two discriminative variant opca and cplsa significantly outperform their corresponding baseline the largest difference in performance are observed on the task of retrieval when the document are only comparable and not parallel the opca method is shown to perform best 
most sentiment analysis approach use a baseline a support vector machine svm classifier with binary unigram weight in this paper we explore whether more sophisticated feature weighting scheme from information retrieval can enhance classification accuracy we show that variant of the classic tf idf scheme adapted to sentiment analysis provide significant increase in accuracy especially when using a sublinear function for term frequency weight and document frequency smoothing the technique are tested on a wide selection of data set and produce the best accuracy to our knowledge 
this paper evaluates the lingo grammar matrix a cross linguistic resource for the development of precision broad coverage grammar by applying it to the australian language wambaya despite large typological difference between wambaya and the language on which the development of the resource wa based the grammar matrix is found to provide a significant jump start in the creation of the grammar for wambaya with le than person week of development the wambaya grammar wa able to assign correct semantic representation to of the sentence in a naturally occurring text while the work on wambaya identified some area of refinement for the grammar matrix of the matrix provided type were invoked in the final wambaya grammar and only of the matrix provided type required modification 
a significant portion of the world s text is tagged by reader on social bookmarking website credit attribution is an inherent problem in these corpus because most page have multiple tag but the tag do not always apply with equal specificity across the whole document solving the credit attribution problem requires associating each word in a document with the most appropriate tag and vice versa this paper introduces labeled lda a topic model that constrains latent dirichlet allocation by defining a one to one correspondence between lda s latent topic and user tag this allows labeled lda to directly learn word tag correspondence we demonstrate labeled lda s improved expressiveness over traditional lda with visualization of a corpus of tagged web page from del icio u labeled lda outperforms svms by more than to when extracting tag specific document snippet a a multi label text classifier our model is competitive with a discriminative baseline on a variety of datasets 
machine learning approach to coreference resolution are typically supervised and require expensive labeled data some unsupervised approach have been proposed e g haghighi and klein but they are le accurate in this paper we present the first unsupervised approach that is competitive with supervised one this is made possible by performing joint inference across mention in contrast to the pairwise classification typically used in supervised method and by using markov logic a a representation language which enables u to easily express relation like apposition and predicate nominal on muc and ace datasets our model outperforms haghigi and klein s one using only a fraction of the training data and often match or exceeds the accuracy of state of the art supervised model 
we combine the strength of bayesian modeling and synchronous grammar in unsupervised learning of basic translation phrase pair the structured space of a synchronous grammar is a natural fit for phrase pair probability estimation though the search space can be prohibitively large therefore we explore efficient algorithm for pruning this space that lead to empirically effective result incorporating a sparse prior using variational bayes bias the model toward generalizable parsimonious parameter set leading to significant improvement in word alignment this preference for sparse solution together with effective pruning method form a phrase alignment regimen that produce better end to end translation than standard word alignment approach 
probabilistic model of sentence comprehension are increasingly relevant to question concerning human language processing however such model are often limited to syntactic factor this paper introduces a novel sentence processing model that consists of a parser augmented with a probabilistic logic based model of coreference resolution which allows u to simulate how context interacts with syntax in a reading task our simulation show that a weakly interactive cognitive architecture can explain data which had been provided a evidence for the strongly interactive hypothesis 
we describe an unsupervised approach to multi document sentence extraction based summarization for the task of producing biography we utilize wikipedia to automatically construct a corpus of biographical sentence and tdt to construct a corpus of non biographical sentence we build a biographical sentence classifier from these corpus and an svm regression model for sentence ordering from the wikipedia corpus we evaluate our work on the duc evaluation data and with human judge overall our system significantly outperforms all system that participated in duc according to the rouge l metric and is preferred by human subject 
most previous work on trainable language generation ha focused on two paradigm a using a statistical model to rank a set of generated utterance or b using statistic to inform the generation decision process both approach rely on the existence of a handcrafted generator which limit their scalability to new domain this paper present bagel a statistical language generator which us dynamic bayesian network to learn from semantically aligned data produced by untrained annotator a human evaluation show that bagel can generate natural and informative utterance from unseen input in the information presentation domain additionally generation performance on sparse datasets is improved significantly by using certainty based active learning yielding rating close to the human gold standard with a fraction of the data 
we present a novel approach to information presentation ip in spoken dialogue system sd using a data driven statistical optimisation framework for content planning and attribute selection first we collect data in a wizard of oz woz experiment and use it to build a supervised model of human behaviour this form a baseline for measuring the performance of optimised policy developed from this data using reinforcement learning rl method we show that the optimised policy significantly outperform the baseline in a variety of generation scenario while the supervised model is able to attain up to of the possible reward on this task the rl policy are significantly better in out of scenario gaining up to of the total possible reward the rl policy perform especially well in more complex scenario we are also the first to show that adding predictive lower level feature e g from the nlg realiser is important for optimising ip strategy according to user preference this provides new insight into the nature of the ip problem for sd 
this paper present a novel ranking style word segmentation approach called rsvm seg which is well tailored to chinese information retrieval cir this strategy make segmentation decision based on the ranking of the internal associative strength between each pair of adjacent character of the sentence on the training corpus composed of query item a ranking model is learned by a widely used tool ranking svm with some useful statistical feature such a mutual information difference of t test frequency and dictionary information experimental result show that this method is able to eliminate overlapping ambiguity much more effectively compared to the current word segmentation method furthermore a this strategy naturally generates segmentation result with different granularity the performance of cir system is improved and achieves the state of the art 
this paper present a novel ranking style word segmentation approach called rsvmseg which is well tailored to chinese information retrieval cir this strategy make segmentation decision based on the ranking of the internal associative strength between each pair of adjacent character of the sentence on the training corpus composed of query item a ranking model is learned by a widely used tool ranking svm with some useful statistical feature such a mutual information difference of t test frequency and dictionary information experimental result show that this method is able to eliminate overlapping ambiguity much more effectively compared to the current word segmentation method furthermore a this strategy naturally generates segmentation result with different granularity the performance of cir system is improved and achieves the state of the art 
we present a discriminative structureprediction model for the letter to phoneme task a crucial step in text to speech processing our method encompasses three task that have been previously handled separately input segmentation phoneme prediction and sequence modeling the key idea is online discriminative training which update parameter according to a comparison of the current system output to the desired output allowing u to train all of our component together by folding the three step of a pipeline approach into a unified dynamic programming framework we are able to achieve substantial performance gain our result surpass the current state of the art on six publicly available data set representing four different language 
we show how web mark up can be used to improve unsupervised dependency parsing starting from raw bracketings of four common html tag anchor bold italic and underline we refine approximate partial phrase boundary to yield accurate parsing constraint conversion procedure fall out of our linguistic analysis of a newly available million word hyper text corpus we demonstrate that derived constraint aid grammar induction by training klein and manning s dependency model with valence dmv on this data set parsing accuracy on section all sentence of the wall street journal corpus jump to beating previous state of the art by more than web scale experiment show that the dmv perhaps because it is unlexicalized doe not benefit from order of magnitude more annotated but noisier data our model trained on a single blog generalizes to accuracy out of domain against the brown corpus nearly higher than the previous published best the fact that web mark up strongly correlate with syntactic structure may have broad applicability in nlp 
while inversion transduction grammar itg ha regained more and more attention in recent year it still suffers from the major obstacle of speed we propose a discriminative itg pruning framework using minimum error rate training and various feature from previous work on itg alignment experiment result show that it is superior to all existing heuristic in itg pruning on top of the pruning framework we also propose a discriminative itg alignment model using hierarchical phrase pair which improves both f score and bleu score over the baseline alignment system of giza 
in this paper we propose a linear model based general framework to combine k best parse output from multiple parser the proposed framework leverage on the strength of previous system combination and re ranking technique in parsing by integrating them into a linear model a a result it is able to fully utilize both the logarithm of the probability of each k best parse tree from each individual parser and any additional useful feature for feature weight tuning we compare the simulated annealing algorithm and the perceptron algorithm our experiment are carried out on both the chinese and english penn treebank syntactic parsing task by combining two state of the art parsing model a head driven lexicalized model and a latent annotation based un lexicalized model experimental result show that our f score of on chinese and on english outperform the previously best reported system by and respectively 
in this paper we describe a system by which the multilingual characteristic of wikipedia can be utilized to annotate a large corpus of text with named entity recognition ner tag requiring minimal human intervention and no linguistic expertise this process though of value in language for which resource exist is particularly useful for le commonly taught language we show how the wikipedia format can be used to identify possible named entity and discus in detail the process by which we use the category structure inherent to wikipedia to determine the named entity type of a proposed entity we further describe the method by which english language data can be used to bootstrap the ner process in other language we demonstrate the system by using the generated corpus a training set for a variant of bbn s identifinder in french ukrainian spanish polish russian and portuguese achieving overall f score a high a on independent human annotated corpus comparable to a system trained on up to word of human annotated newswire 
word alignment play a central role in statistical mt smt since almost all smt system extract translation rule from word aligned parallel training data while most smt system use unsupervised algorithm e g giza for training word alignment supervised method which exploit a small amount of human aligned data have become increasingly popular recently this work empirically study the performance of these two class of alignment algorithm and explores strategy to combine them to improve overall system performance we used two unsupervised aligners giza and hmm and one supervised aligner itg in this study to avoid language and genre specific conclusion we ran experiment on test set consisting of two language pair chinese to english and arabic to english and two genre newswire and weblog result show that the two class of algorithm achieve the same level of mt performance modest improvement were achieved by taking the union of the translation grammar extracted from different alignment significant improvement around in bleu were achieved by combining output of different system trained with different alignment the improvement are consistent across language and genre 
in previous research in automatic verb classification syntactic feature have proved the most useful feature although manual classification rely heavily on semantic feature we show in contrast with previous work that considerable additional improvement can be obtained by using semantic feature in automatic classification verb selectional preference acquired from corpus data using a fully unsupervised method we report these promising result using a new framework for verb clustering which incorporates a recent subcategorization acquisition system rich syntactic semantic feature set and a variation of spectral clustering which performs particularly well in high dimensional feature space 
untranslated word still constitute a major problem for statistical machine translation smt and current smt system are limited by the quantity of parallel training text augmenting the training data with paraphrase generated by pivoting through other language alleviates this problem especially for the so called low density language but pivoting requires additional parallel text we address this problem by deriving paraphrase monolingually using distributional semantic similarity measure thus providing access to larger training resource such a comparable and unrelated monolingual corpus we present what is to our knowledge the first successful integration of a collocational approach to untranslated word with an end to end state of the art smt system demonstrating significant translation improvement in a low resource setting 
determining the semantic intent of web query not only involves identifying their semantic class which is a primary focus of previous work but also understanding their semantic structure in this work we formally define the semantic structure of noun phrase query a comprised of intent head and intent modifier we present method that automatically identify these constituent a well a their semantic role based on markov and semi markov conditional random field we show that the use of semantic feature and syntactic feature significantly contribute to improving the understanding performance 
we examine effect that empty category have on machine translation empty category are element in parse tree that lack corresponding overt surface form word such a dropped pronoun and marker for control construction we start by training machine translation system with manually inserted empty element we find that inclusion of some empty category in training data improves the translation result we expand the experiment by automatically inserting these element into a larger data set using various method and training on the modified corpus we show that even when automatic prediction of null element is not highly accurate it nevertheless improves the end translation result 
subjectivity analysis is a rapidly growing field of study along with it application to various nlp task much work have put effort into multilingual subjectivity learning from existing resource multilingual subjectivity analysis requires language independent criterion for comparable outcome across language this paper proposes to measure the multilanguage comparability of subjectivity analysis tool and provides meaningful comparison of multilingual subjectivity analysis from various point of view 
in many nlp system there is a unidirectional flow of information in which a parser supply input to a semantic role labeler in this paper we build a system that allows information to flow in both direction we make use of semantic role prediction in choosing a single best parse this process relies on an averaged perceptron model to distinguish likely semantic role from erroneous one our system penalizes par that give rise to low scoring semantic role to explore the consequence of this we perform two experiment first we use a baseline generative model to produce n best par which are then re ordered by our semantic model second we use a modified version of our semantic role labeler to predict semantic role at parse time the performance of this modified labeler is weaker than that of our best full srl because it is restricted to feature that can be computed directly from the parser s packed chart for both experiment the resulting semantic prediction are then used to select par finally we feed the selected par produced by each experiment to the full version of our semantic role labeler we find that srl performance can be improved over this baseline by selecting par with likely semantic role 
contradiction detection cd in text is a difficult nlp task we investigate cd over function e g bornin person place and present a domain independent algorithm that automatically discovers phrase denoting function with high precision previous work on cd ha investigated hand chosen sentence pair in contrast we automatically harvested from the web pair of sentence that appear contradictory but were surprised to find that most pair are in fact consistent for example mozart wa born in salzburg doe not contradict mozart wa born in austria despite the functional nature of the phrase wa born in we show that background knowledge about meronym e g salzburg is in austria synonym function and more is essential for success in the cd task 
while a significant amount of research ha been devoted to textual entailment automated entailment from conversational script ha received le attention to address this limitation this paper investigates the problem of conversation entailment automated inference of hypothesis from conversation script we examine two level of semantic representation a basic representation based on syntactic parsing from conversation utterance and an augmented representation taking into consideration of conversation structure for each of these level we further explore two way of capturing long distance relation between language constituent implicit modeling based on the length of distance and explicit modeling based on actual pattern of relation our empirical finding have shown that the augmented representation with conversation structure is important which achieves the best performance when combined with explicit modeling of long distance relation 
a information extraction ie becomes more central to enterprise application rule based ie engine have become increasingly important in this paper we describe systemt a rule based ie system whose basic design remove the expressivity and performance limitation of current system based on cascading grammar systemt us a declarative rule language aql and an optimizer that generates high performance algebraic execution plan for aql rule we compare systemt s approach against cascading grammar both theoretically and with a thorough experimental evaluation our result show that systemt can deliver result quality comparable to the state of the art and an order of magnitude higher annotation throughput 
weighted tree transducer have been proposed a useful formal model for representing syntactic natural language processing application but there ha been little description of inference algorithm for these automaton beyond formal foundation we give a detailed description of algorithm for application of cascade of weighted tree transducer to weighted tree acceptor connecting formal theory with actual practice additionally we present novel on the fly variant of these algorithm and compare their performance on a syntax machine translation cascade based on yamada and knight 
having seen a news title alba denies wedding report how do we infer that it is primarily about jessica alba rather than about wedding or report we probably realize that in a randomly driven sentence the word alba is le anticipated than wedding or report which add value to the word alba if used such anticipation can be modeled a a ratio between an empirical probability of the word in a given corpus and it estimated probability in general english aggregated over all word in a document this ratio may be used a a measure of the document s topicality assuming that the corpus consists of on topic and off topic document we call them the core and the noise our goal is to determine which document belong to the core we propose two unsupervised method for doing this first we assume that word are sampled i i d and propose an information theoretic framework for determining the core second we relax the independence assumption and use a simple graphical model to rank document according to their likelihood of belonging to the core we discus theoretical guarantee of the proposed method and show their usefulness for web mining and topic detection and tracking tdt 
most coreference resolution model determine if two mention are coreferent using a single function over a set of constraint or feature this approach can lead to incorrect decision a lower precision feature often overwhelm the smaller number of high precision one to overcome this problem we propose a simple coreference architecture based on a sieve that applies tier of deterministic coreference model one at a time from highest to lowest precision each tier build on the previous tier s entity cluster output further our model propagates global information by sharing attribute e g gender and number across mention in the same cluster this cautious sieve guarantee that stronger feature are given precedence over weaker one and that each decision is made using all of the information available at the time the framework is highly modular new coreference module can be plugged in without any change to the other module in spite of it simplicity our approach outperforms many state of the art supervised and unsupervised model on several standard corpus this suggests that sieve based approach could be applied to other nlp task 
this paper explores two class of model adaptation method for web search ranking model interpolation and error driven learning approach based on a boosting algorithm the result show that model interpolation though simple achieves the best result on all the open test set where the test data is very different from the training data the tree based boosting algorithm achieves the best performance on most of the closed test set where the test data and the training data are similar but it performance drop significantly on the open test set due to the instability of tree several method are explored to improve the robustness of the algorithm with limited success 
training a statistical machine translation start with tokenizing a parallel corpus some language such a chinese do not incorporate spacing in their writing system which creates a challenge for tokenization moreover morphologically rich language such a korean present an even bigger challenge since optimal token boundary for machine translation in these language are often unclear both rule based solution and statistical solution are currently used in this paper we present unsupervised method to solve tokenization problem our method incorporate information available from parallel corpus to determine a good tokenization for machine translation 
we present an automatic approach to determining whether a pronoun in text refers to a preceding noun phrase or is instead nonreferential we extract the surrounding textual context of the pronoun and gather from a large corpus the distribution of word that occur within that context we learn to reliably classify these distribution a representing either referential or non referential pronoun instance despite it simplicity experimental result on classifying the english pronoun it show the system achieves the highest performance yet attained on this important task 
we present a discriminative method for learning selectional preference from unlabeled text positive example are taken from observed predicate argument pair while negative are constructed from unobserved combination we train a support vector machine classifier to distinguish the positive from the negative instance we show how to partition the example for efficient training with thousand feature and million training instance the model outperforms other recent approach achieving excellent correlation with human plausibility judgment compared to mutual information it identifies more verb object pair in unseen text and resolve more pronoun correctly in a pronoun resolution experiment 
parser disambiguation with precision grammar generally take place via statistical ranking of the parse yield of the grammar using a supervised parse selection model in the standard process the parse selection model is trained over a hand disambiguated treebank meaning that without a significant investment of effort to produce the treebank parse selection is not possible furthermore a treebanking is generally streamlined with parse selection model creating the initial treebank without a model requires more resource than subsequent treebanks in this work we show that by taking advantage of the constrained nature of these hpsg grammar we can learn a discriminative parse selection model from raw text in a purely unsupervised fashion this allows u to bootstrap the treebanking process and provide better parser faster and with le resource 
extracting knowledge from unstructured text is a long standing goal of nlp although learning approach to many of it subtasks have been developed e g parsing taxonomy induction information extraction all end to end solution to date require heavy supervision and or manual engineering limiting their scope and scalability we present ontousp a system that induces and populates a probabilistic ontology using only dependency parsed text a input ontousp build on the usp unsupervised semantic parser by jointly forming isa and is part hierarchy of lambda form cluster the isa hierarchy allows more general knowledge to be learned and the use of smoothing for parameter estimation we evaluate ontousp by using it to extract a knowledge base from biomedical abstract and answer question ontousp improves on the recall of usp by and greatly outperforms previous state of the art approach 
we present a generative model for unsupervised coreference resolution that view coreference a an em clustering process for comparison purpose we revisit haghighi and klein s fully generative bayesian model for unsupervised coreference resolution discus it potential weakness and consequently propose three modification to their model experimental result on the ace data set show that our model outperforms their original model by a large margin and compare favorably to the modified model 
we propose the first unsupervised approach to the problem of modeling dialogue act in an open domain trained on a corpus of noisy twitter conversation our method discovers dialogue act by clustering raw utterance because it account for the sequential behaviour of these act the learned model can provide insight into the shape of communication in a new medium we address the challenge of evaluating the emergent model with a qualitative visualization and an intrinsic conversation ordering task this work is inspired by a corpus of million twitter conversation which will be made publicly available this huge amount of data available only because twitter blur the line between chatting and publishing highlight the need to be able to adapt quickly to a new medium 
datasets annotated with semantic role are an important prerequisite to developing high performance role labeling system unfortunately the reliance on manual annotation which are both difficult and highly expensive to produce present a major obstacle to the widespread application of these system across different language and text genre in this paper we describe a method for inducing the semantic role of verbal argument directly from unannotated text we formulate the role induction problem a one of detecting alternation and finding a canonical syntactic form for them both step are implemented in a novel probabilistic model a latent variable variant of the logistic classifier our method increase the purity of the induced role cluster by a wide margin over a strong baseline 
confusion network are a simple representation of multiple speech recognition or translation hypothesis in a machine translation system a typical operation on a confusion network is to find the path which minimizes or maximizes a certain evaluation metric in this article we show that this problem is generally np hard for the popular bleu metric a well a for smaller variant of bleu this also hold for more complex representation like generic word graph in addition we give an efficient polynomial time algorithm to calculate unigram bleu on confusion network but show that even small generalization of this data structure render the problem to be np hard again since finding the optimal solution is thus not always feasible we introduce an approximating algorithm based on a multi stack decoder which find a not necessarily optimal solution for n gram bleu in polynomial time 
unknown lexical item present a major obstacle to the development of broad coverage semantic role labeling system we address this problem with a semi supervised learning approach which acquires training instance for unseen verb from an unlabeled corpus our method relies on the hypothesis that unknown lexical item will be structurally and semantically similar to known item for which annotation are available accordingly we represent known and unknown sentence a graph formalize the search for the most similar verb a a graph alignment problem and solve the optimization using integer linear programming experimental result show that role labeling performance for unknown lexical item improves with training data produced automatically by our method 
several attempt have been made to learn phrase translation probability for phrase based statistical machine translation that go beyond pure counting of phrase in word aligned training data most approach report problem with over fitting we describe a novel leaving one out approach to prevent over fitting that allows u to train phrase model that show improved translation performance on the wmt europarl german english task in contrast to most previous work where phrase model were trained separately from other model used in translation we include all component such a single word lexica and reordering model in training using this consistent training of phrase model we are able to achieve improvement of up to point in bleu a a side effect the phrase table size is reduced by more than 
this paper present a new hypothesis alignment method for combining output of multiple machine translation mt system an indirect hidden markov model ihmm is proposed to address the synonym matching and word ordering issue in hypothesis alignment unlike traditional hmms whose parameter are trained via maximum likelihood estimation mle the parameter of the ihmm are estimated indirectly from a variety of source including word semantic similarity word surface similarity and a distance based distortion penalty the ihmm based method significantly outperforms the state of the art ter based alignment model in our experiment on nist benchmark datasets our combined smt system using the proposed method achieved the best chinese to english translation result in the constrained training track of the nist open mt evaluation 
one of the most desired information type when planning a trip to some place is the knowledge of transport road and geographical connectedness of prominent site in this place while some transport company or repository make some of this information accessible it is not easy to find and the majority of information about uncommon place can only be found in web free text such a blog and forum in this paper we present an algorithmic framework which allows an automated acquisition of map like information from the web based on surface pattern like from x to y given a set of location a initial seed we retrieve from the web an extended set of location and produce a map like network which connects these location using transport type edge we evaluate our framework in several setting producing meaningful and precise connection set 
this work present an agenda based approach to improve the robustness of the dialog manager by using dialog example and n best recognition hypothesis this approach support n best hypothesis in the dialog manager and keep track of the dialog state using a discourse interpretation algorithm with the agenda graph and focus stack given the agenda graph and n best hypothesis the system can predict the next system action to maximize multi level score function to evaluate the proposed method a spoken dialog system for a building guidance robot wa developed preliminary evaluation show this approach would be effective to improve the robustness of example based dialog modeling 
we have designed implemented and evaluated an end to end system spellchecking and autocorrection system that doe not require any manually annotated training data the world wide web is used a a large noisy corpus from which we infer knowledge about misspelling and word usage this is used to build an error model and an n gram language model a small secondary set of news text with artificially inserted misspelling are used to tune confidence classifier because no manual annotation is required our system can easily be instantiated for new language when evaluated on human typed data with real misspelling in english and german our web based system outperform baseline which use candidate correction based on hand curated dictionary our system achieves total error rate in english we show similar improvement in preliminary result on artificial data for russian and arabic 
tree based translation model are a compelling mean of integrating linguistic information into machine translation syntax can inform lexical selection and reordering choice and thereby improve translation quality research to date ha focussed primarily on decoding with such model but le on the difficult problem of inducing the bilingual grammar from data we propose a generative bayesian model of tree to string translation which induces grammar that are both smaller and produce better translation than the previous heuristic two stage approach which employ a separate word alignment step 
this paper explores the effect that different corpus configuration have on the performance of a coreference resolution system a measured by muc b and ceaf by varying separately three parameter language annotation scheme and preprocessing information and applying the same coreference resolution system the strong bond between system and corpus are demonstrated the experiment reveal problem in coreference resolution evaluation relating to task definition coding scheme and feature they also expose systematic bias in the coreference evaluation metric we show that system comparison is only possible when corpus parameter are in exact agreement 
in this paper we present a novel approach to web search result clustering based on the automatic discovery of word sens from raw text a task referred to a word sense induction wsi we first acquire the sens i e meaning of a query by mean of a graph based clustering algorithm that exploit cycle triangle and square in the co occurrence graph of the query then we cluster the search result based on their semantic similarity to the induced word sens our experiment conducted on datasets of ambiguous query show that our approach improves search result clustering in term of both clustering quality and degree of diversification 
statistical bilingual word alignment ha been well studied in the context of machine translation this paper adapts the bilingual word alignment algorithm to monolingual scenario to extract collocation from monolingual corpus the monolingual corpus is first replicated to generate a parallel corpus where each sentence pair consists of two identical sentence in the same language then the monolingual word alignment algorithm is employed to align the potentially collocated word in the monolingual sentence finally the aligned word pair are ranked according to refined alignment probability and those with higher score are extracted a collocation we conducted experiment using chinese and english corpus individually compared with previous approach which use association measure to extract collocation from the co occurring word pair within a given window our method achieves higher precision and recall according to human evaluation in term of precision our method achieves absolute improvement of on the chinese corpus and on the english corpus respectively especially we can extract collocation with longer span achieving a high precision of on the long span chinese collocation 
a challenging problem in open information extraction and text mining is the learning of the selectional restriction of semantic relation we propose a minimally supervised bootstrapping algorithm that us a single seed and a recursive lexico syntactic pattern to learn the argument and the supertypes of a diverse set of semantic relation from the web we evaluate the performance of our algorithm on multiple semantic relation expressed using verb noun and verb prep lexico syntactic pattern human based evaluation show that the accuracy of the harvested information is about we also compare our result with existing knowledge base to outline the similarity and difference of the granularity and diversity of the harvested knowledge 
randomised technique allow very big language model to be represented succinctly however being batch based they are unsuitable for modelling an unbounded stream of language whilst maintaining a constant error rate we present a novel randomised language model which us an online perfect hash function to efficiently deal with unbounded text stream translation experiment over a text stream show that our online randomised model match the performance of batch based lm without incurring the computational overhead associated with full retraining this open up the possibility of randomised language model which continuously adapt to the massive volume of text published on the web each day 
in this paper we address the issue of automatic extending lexical resource by exploiting existing knowledge repository in particular we deal with the new task of linking framenet and wikipedia using a word sense disambiguation system that for a given pair frame lexical unit f l find the wikipage that best express the the meaning of l the mapping can be exploited to straightforwardly acquire new example sentence and new lexical unit both for english and for all language available in wikipedia in this way it is possible to easily acquire good quality data a a starting point for the creation of framenet in new language the evaluation reported both for the monolingual and the multilingual expansion of framenet show that the approach is promising 
recently system combination ha been shown to be an effective way to improve translation quality over single machine translation system in this paper we present a simple and effective method to systematically derive an ensemble of smt system from one baseline linear smt model for use in system combination each system in the resulting ensemble is based on a feature set derived from the feature of the baseline model typically a subset of it we will discus the principle to determine the feature set for derived system and present in detail the system combination model used in our work evaluation is performed on the data set for nist and nist chinese to english machine translation task experimental result show that our method can bring significant improvement to baseline system with state of the art performance 
compared to the telephone email based customer care is increasingly becoming the preferred channel of communication for corporation and customer most email based customer care management system provide a method to include template text in order to reduce the handling time for a customer s email the text in a template is suitably modified into a response by a customer care agent in this paper we present two technique to improve the effectiveness of a template by providing tool for the template author first we present a tool to track and visualize the edits made by agent to a template which serf a a vital feedback to the template author second we present a novel method that automatically extract potential template from response authored by agent these method are investigated in the context of an email customer care analysis tool that handle over a million email a year 
a strong inductive bias is essential in unsupervised grammar induction we explore a particular sparsity bias in dependency grammar that encourages a small number of unique dependency type specifically we investigate sparsity inducing penalty on the posterior distribution of parent child po tag pair in the posterior regularization pr framework of gra a et al in experiment with language we achieve substantial gain over the standard expectation maximization em baseline with average improvement in attachment accuracy of further our method outperforms model based on a standard bayesian sparsity inducing prior by an average of on english in particular we show that our approach improves on several other state of the art technique 
traditional learning based coreference resolvers operate by training a mentionpair classifier for determining whether two mention are coreferent or not two independent line of recent research have attempted to improve these mention pair classifier one by learning a mentionranking model to rank preceding mention for a given anaphor and the other by training an entity mention classifier to determine whether a preceding cluster is coreferent with a given mention we propose a cluster ranking approach to coreference resolution that combine the strength of mention ranker and entitymention model we additionally show how our cluster ranking framework naturally allows discourse new entity detection to be learned jointly with coreference resolution experimental result on the ace data set demonstrate it superior performance to competing approach 
argumentative zoning az is an analysis of the argumentative and rhetorical structure of a scientific paper it ha been shown to be reliably used by independent human coder and ha proven useful for various information access task annotation experiment have however so far been restricted to one discipline computational linguistics cl here we present a more informative az scheme with category in place of the original and show that it can be applied to the life science a well a to cl we use a domain expert to encode basic knowledge about the subject such a terminology and domain specific rule for individual category a part of the annotation guideline our result show that non expert human coder can then use these guideline to reliably annotate this scheme in two domain chemistry and computational linguistics 
we present a novel scheme to apply factored phrase based smt to a language pair with very disparate morphological structure our approach relies on syntactic analysis on the source side english and then encodes a wide variety of local and non local syntactic structure a complex structural tag which appear a additional factor in the training data on the target side turkish we only perform morphological analysis and disambiguation but treat the complete complex morphological tag a a factor instead of separating morpheme we incrementally explore capturing various syntactic substructure a complex tag on the english side and evaluate how our translation improve in bleu score our maximal set of source and target side transformation coupled with some additional technique provide an relative improvement from a baseline to bleu all averaged over training and test set now that the syntactic analysis on the english side is available we also experiment with more long distance constituent reordering to bring the english constituent order close to turkish but find that these transformation do not provide any additional consistent tangible gain when averaged over the set 
this paper study sentiment analysis of conditional sentence the aim is to determine whether opinion expressed on different topic in a conditional sentence are positive negative or neutral conditional sentence are one of the commonly used language construct in text in a typical document there are around of such sentence due to the condition clause sentiment expressed in a conditional sentence can be hard to determine for example in the sentence if your nokia phone is not good buy this great samsung phone the author is positive about samsung phone but doe not express an opinion on nokia phone although the owner of the nokia phone may be negative about it however if the sentence doe not have if the first clause is clearly negative although if commonly signifies a conditional sentence there are many other word and construct that can express condition this paper first present a linguistic analysis of such sentence and then build some supervised learning model to determine if sentiment expressed on different topic in a conditional sentence are positive negative or neutral experimental result on conditional sentence from diverse domain are given to demonstrate the effectiveness of the proposed approach 
state of the art set expansion algorithm produce varying quality expansion for different entity type even for the highest quality expansion error still occur and manual refinement are necessary for most practical us in this paper we propose algorithm to aide this refinement process greatly reducing the amount of manual labor required the method rely on the fact that most expansion error are systematic often stemming from the fact that some seed element are ambiguous using our method empirical evidence show that average r precision over random entity set improves by to when given from to manually tagged error both proposed refinement model have linear time complexity in set size allowing for practical online use in set expansion system 
is it possible to use sense inventory to improve web search result diversity for one word query to answer this question we focus on two broad coverage lexical resource of a different nature wordnet a a de facto standard used in word sense disambiguation experiment and wikipedia a a large coverage updated encyclopaedic resource which may have a better coverage of relevant sens in web page our result indicate that i wikipedia ha a much better coverage of search result ii the distribution of sens in search result can be estimated using the internal graph structure of the wikipedia and the relative number of visit received by each sense in wikipedia and iii associating web page to wikipedia sens with simple and efficient algorithm we can produce modified ranking that cover more wikipedia sens than the original search engine ranking 
we present a discriminative latent variable approach to syntactic parsing in which rule exist at multiple scale of refinement the model is formally a latent variable crf grammar over tree learned by iteratively splitting grammar production not category different region of the grammar are refined to different degree yielding grammar which are three order of magnitude smaller than the single scale baseline and time smaller than the split and merge grammar of petrov et al in addition our discriminative approach integrally admits feature beyond local tree configuration we present a multiscale training method along with an efficient cky style dynamic program on a variety of domain and language this method produce the best published parsing accuracy with the smallest reported grammar 
confidence weighted linear classifier cw and it successor were shown to perform well on binary and multiclass nlp problem in this paper we extend the cw approach for sequence learning and show that it achieves state of the art performance on four noun phrase chucking and named entity recognition task we then derive few algorithmic approach to estimate the prediction s correctness of each label in the output sequence we show that our approach provides a reliable relative correctness information a it outperforms other alternative in ranking label prediction according to their error we also show empirically that our method output close to absolute estimation of error finally we show how to use this information to improve active learning 
in this paper we present a simple and effective method to address the issue of how to generate diversified translation system from a single statistical machine translation smt engine for system combination our method is based on the framework of boosting first a sequence of weak translation system is generated from a baseline system in an iterative manner then a strong translation system is built from the ensemble of these weak translation system to adapt boosting to smt system combination several key component of the original boosting algorithm are redesigned in this work we evaluate our method on chinese to english machine translation mt task in three baseline system including a phrase based system a hierarchical phrase based system and a syntax based system the experimental result on three nist evaluation test set show that our method lead to significant improvement in translation accuracy over the baseline system 
we report in this paper a way of doing word sense disambiguation wsd that ha it origin in multilingual mt and that is cognizant of the fact that parallel corpus wordnet and sense annotated corpus are scarce resource with respect to these resource language show different level of readiness however a more resource fortunate language can help a le resource fortunate language our wsd method can be applied to a language even when no sense tagged corpus for that language is available this is achieved by projecting wordnet and corpus parameter from another language to the language in question the approach is centered around a novel synset based multilingual dictionary and the empirical observation that within a domain the distribution of sens remains more or le invariant across language the effectiveness of our approach is verified by doing parameter projection and then running two different wsd algorithm the accuracy value of approximately f score for three language in two different domain establish the fact that within a domain it is possible to circumvent the problem of scarcity of resource by projecting parameter like sense distribution corpus co occurrence conceptual distance etc from one language to another 
this paper present a graph theoretic model of the acquisition of lexical syntactic representation the representation the model learns are non categorical or graded we propose a new evaluation methodology of syntactic acquisition in the framework of exemplar theory when applied to the childes corpus the evaluation show that the model s graded syntactic representation perform better than previously proposed categorical representation 
tree adjoining grammar have well known advantage but are typically considered too difficult for practical system we demonstrate that when done right adjoining improves translation quality without becoming computationally intractable using adjoining to model optionality allows general translation pattern to be learned without the clutter of endless variation of optional material the appropriate modifier can later be spliced in a needed in this paper we describe a novel method for learning a type of synchronous tree adjoining grammar and associated probability from aligned tree string training data we introduce a method of converting these grammar to a weakly equivalent tree transducer for decoding finally we show that adjoining result in an end to end improvement of bleu over a baseline statistical syntax based mt modelona large scalearabic englishmt task 
we present a simple and effective semisupervised method for training dependency parser we focus on the problem of lexical representation introducing feature that incorporate word cluster derived from a large unannotated corpus we demonstrate the effectiveness of the approach in a series of dependency parsing experiment on the penn treebank and prague dependency treebank and we show that the cluster based feature yield substantial gain in performance across a wide range of condition for example in the case of english unlabeled second order parsing we improve from a baseline accuracy of to and in the case of czech unlabeled second order parsing we improve from a baseline accuracy of to in addition we demonstrate that our method also improves performance when small amount of training data are available and can roughly halve the amount of supervised data required to reach a desired level of performance 
the speech transcription analysis tool stat is an open source tool for aligning and comparing two phonetically transcribed text of human speech the output analysis is a parameterized set of phonological difference these difference are based upon a selectable set of binary phonetic feature such a voice continuant high etc stat wa initially designed to provide set of phonological speech pattern in the comparison of various english accent found in the speech accent archive http accent gmu edu but it scope and utility expand to matter of language assessment phonetic training forensic linguistics and speech recognition 
in this paper we describe research on summarizing conversation in the meeting and email domain we introduce a conversation summarization system that work in multiple domain utilizing general conversational feature and compare our result with domain dependent system for meeting and email data we find that by treating meeting and email a conversation with general conversational feature in common we can achieve competitive result with state of the art system that rely on more domain specific feature 
in this paper we present babelnet a very large wide coverage multilingual semantic network the resource is automatically constructed by mean of a methodology that integrates lexicographic and encyclopedic knowledge from wordnet and wikipedia in addition machine translation is also applied to enrich the resource with lexical information for all language we conduct experiment on new and existing gold standard datasets to show the high quality and coverage of the resource 
named entity disambiguation concern linking a potentially ambiguous mention of named entity in text to an unambiguous identifier in a standard database one approach to this task is supervised classification however the availability of training data is often limited and the available data set tend to be imbalanced and in some case heterogeneous we propose a new method that distinguishes a named entity by finding the informative keywords in it surrounding context and then train a model to predict whether each keyword indicates the semantic class of the entity while maintaining a comparable performance to supervised classification this method avoids using expensive manually annotated data for each new domain and thus achieves better portability 
machine learning offer a range of tool for training system from data but these method are only a good a the underlying representation this paper proposes to acquire representation for machine learning by reading text written to accommodate human learning we propose a novel form of semantic analysis called reading to learn where the goal is to obtain a high level semantic abstract of multiple document in a representation that facilitates learning we obtain this abstract through a generative model that requires no labeled data instead leveraging repetition across multiple document the semantic abstract is converted into a transformed feature space for learning resulting in improved generalization on a relational learning task 
in spite of decade of research on word sense disambiguation wsd all word general purpose wsd ha remained a distant goal many supervised wsd system have been built but the effort of creating the training corpus annotated sense marked corpus ha always been a matter of concern therefore attempt have been made to develop unsupervised and knowledge based technique for wsd which do not need sense marked corpus however such approach have not proved effective since they typically do not better wordnet first sense baseline accuracy our research reported here proposes to stick to the supervised approach but with far le demand on annotation we show that if we have any sense marked corpus be it from mixed domain or a specific domain a small amount of annotation in any other domain can deliver the good almost a if exhaustive sense marking were available in that domain we have tested our approach across tourism and health domain corpus using also the well known mixed domain semcor corpus accuracy figure close to self domain training lend credence to the viability of our approach our contribution thus lie in finding a convenient middle ground between pure supervised and pure unsupervised wsd finally our approach is not restricted to any specific set of target word a departure from a commonly observed practice in domain specific wsd 
we present a method to align word in a bitext that combine element of a traditional statistical approach with linguistic knowledge we demonstrate this approach for arabic english using an alignment lexicon produced by a statistical word aligner a well a linguistic resource ranging from an english parser to heuristic alignment rule for function word these linguistic heuristic have been generalized from a development corpus of parallel sentence our aligner ualign outperforms both the commonly used giza aligner and the state of the art leaf aligner on f measure and produce superior score in end to end statistical machine translation bleu point over giza and over leaf 
the task of selecting information and rendering it appropriately appears in multiple context in summarization in this paper we present a model that simultaneously optimizes selection and rendering preference the model operates over a phrase based representation of the source document which we obtain by merging pcfg parse tree and dependency graph selection preference for individual phrase are learned discriminatively while a quasi synchronous grammar smith and eisner capture rendering preference such a paraphrase and compression based on an integer linear programming formulation the model learns to generate summary that satisfy both type of preference while ensuring that length topic coverage and grammar constraint are met experiment on headline and image caption generation show that our method obtains state of the art performance using essentially the same model for both task without any major modification 
we propose a language independent approach for improving statistical machine translation for morphologically rich language using a hybrid morpheme word representation where the basic unit of translation is the morpheme but word boundary are respected at all stage of the translation process our model extends the classic phrase based model by mean of word boundary aware morpheme level phrase extraction minimum error rate training for a morpheme level translation model using word level bleu and joint scoring with morphemeand word level language model further improvement are achieved by combining our model with the classic one the evaluation on english to finnish using europarl k sentence pair m english word show statistically significant improvement over the classic model based on bleu and human judgment 
we present a novel hierarchical prior structure for supervised transfer learning in named entity recognition motivated by the common structure of feature space for this task across natural language data set the problem of transfer learning where information gained in one learning task is used to improve performance in another related task is an important new area of research in the subproblem of domain adaptation a model trained over a source domain is generalized to perform well on a related target domain where the two domain data are distributed similarly but not identically we introduce the concept of group of closely related domain called genre and show how inter genre adaptation is related to domain adaptation we also examine multitask learning where two domain may be related but where the concept to be learned in each case is distinct we show that our prior conveys useful information across domain genre and task while remaining robust to spurious signal not related to the target domain and concept we further show that our model generalizes a class of similar hierarchical prior smoothed to varying degree and lay the groundwork for future exploration in this area 
we take a multi pas approach to machine translation decoding when using synchronous context free grammar a the translation model and n gram language model the first pas us a bigram language model and the resulting parse forest is used in the second pas to guide search with a trigram language model the trigram pas close most of the performance gap between a bigram decoder and a much slower trigram decoder but take time that is insignificant in comparison to the bigram pas an additional fast decoding pas maximizing the expected count of correct translation hypothesis increase the bleu score significantly 
online review are often accompanied with numerical rating provided by user for a set of service or product aspect we propose a statistical model which is able to discover corresponding topic in text and extract textual evidence from review supporting each of these aspect rating a fundamental problem in aspect based sentiment summarization hu and liu a our model achieves high accuracy without any explicitly labeled data except the user provided opinion rating the proposed approach is general and can be used for segmentation in other application where sequential data is accompanied with correlated signal 
many sequence labeling task in nlp require solving a cascade of segmentation and tagging subtasks such a chinese po tagging named entity recognition and so on traditional pipeline approach usually suffer from error propagation joint training decoding in the cross product state space could cause too many parameter and high inference complexity in this paper we present a novel method which integrates graph structure of two sub task into one using virtual node and performs joint training and decoding in the factorized state space experimental evaluation on conll shallow parsing data set and fourth sighan bakeoff ctb po tagging data set demonstrate the superiority of our method over cross product pipeline and candidate reranking approach 
recent time have seen a tremendous growth in mobile based data service that allow people to use short message service sm to access these data service in a multilingual society it is essential that data service that were developed for a specific language be made accessible through other local language also in this paper we present a service that allows a user to query a frequently asked question faq database built in a local language hindi using noisy sm english query the inherent noise in the sm query along with the language mismatch make this a challenging problem we handle these two problem by formulating the query similarity over faq question a a combinatorial search problem where the search space consists of combination of dictionary variation of the noisy query and it top n translation we demonstrate the effectiveness of our approach on a real life dataset 
we show that jointly performing semantic role labeling srl on bitext can improve srl result on both side in our approach we use monolingual srl system to produce argument candidate for predicate in bitext at first then we simultaneously generate srl result for two side of bitext using our joint inference model our model prefers the bilingual srl result that is not only reasonable on each side of bitext but also ha more consistent argument structure between two side to evaluate the consistency between two argument structure we also formulate a log linear model to compute the probability of aligning two argument we have experimented with our model on chinese english parallel prop bank data using our joint inference model f score of srl result on chinese and english text achieve and respectively which are and point higher than the result of baseline monolingual srl combination system respectively 
wikipedia article in different language are connected by interwiki link that are increasingly being recognized a a valuable source of cross lingual information unfortunately large number of link are imprecise or simply wrong in this paper technique to detect such problem are identified we formalize their removal a an optimization task based on graph repair operation we then present an algorithm with provable property that us linear programming and a region growing technique to tackle this challenge this allows u to transform wikipedia into a much more consistent multilingual register of the world s entity and concept 
we address two problem in the field of automatic optimization of dialogue strategy learning effective dialogue strategy when no initial data or system exists and evaluating the result with real user we use reinforcement learning rl to learn multimodal dialogue strategy by interaction with a simulated environment which is bootstrapped from small amount of wizard of oz woz data this use of woz data allows development of optimal strategy for domain where no working prototype is available we compare the rl based strategy against a supervised strategy which mimic the wizard policy this comparison allows u to measure relative improvement over the training data our result show that rl significantly outperforms supervised learning when interacting in simulation a well a for interaction with real user the rl based policy gain on average time more reward when tested in simulation and almost time more reward when interacting with real user user also subjectively rate the rl based policy on average higher 
textual entailment recognition play a fundamental role in task that require indepth natural language understanding in order to use entailment recognition technology for real world application a large scale entailment knowledge base is indispensable this paper proposes a conditional probability based directional similarity measure to acquire verb entailment pair on a large scale we targeted verb type that were derived from japanese web document without regard for whether they were used in daily life or only in specific field in an evaluation of the top verb entailment pair acquired by previous method and ours we found that our similarity measure outperformed the previous one our method also worked well for the top result 
production of parallel training corpus for the development of statistical machine translation smt system for resource poor language usually requires extensive manual effort active sample selection aim to reduce the labor time and expense incurred in producing such resource attaining a given performance benchmark with the smallest possible training corpus by choosing informative nonredundant source sentence from an available candidate pool for manual translation we present a novel discriminative sample selection strategy that preferentially selects batch of candidate sentence with construct that lead to erroneous translation on a held out development set the proposed strategy support a built in diversity mechanism that reduces redundancy in the selected batch simulation experiment on english to pashto and spanish to english translation task demonstrate the superiority of the proposed approach to a number of competing technique such a random selection dissimilarity based selection a well a a recently proposed semi supervised active learning strategy 
we extend previous work on fully unsupervised part of speech tagging using a non parametric version of the hmm called the infinite hmm ihmm we address the problem of choosing the number of hidden state in unsupervised markov model for po tagging we experiment with two non parametric prior the dirichlet and pitman yor process on the wall street journal dataset using a parallelized implementation of an ihmm inference algorithm we evaluate the result with a variety of clustering evaluation metric and achieve equivalent or better performance than previously reported building on this promising result we evaluate the output of the unsupervised po tagger a a direct replacement for the output of a fully supervised po tagger for the task of shallow parsing and compare the two evaluation 
the work presented in this paper explores a supervised method for learning a probabilistic model of a lexicon of verbnet class we intend for the probabilistic model to provide a probability distribution of verb class association over known and unknown verb including polysemous word in our approach training instance are obtained from an existing lexicon and or from an annotated corpus while the feature which represent syntactic frame semantic similarity and selectional preference are extracted from unannotated corpus our model is evaluated in type level verb classification task we measure the prediction accuracy of verbnet class for unknown verb and also measure the dissimilarity between the learned and observed probability distribution we empirically compare several setting for model learning while we vary the use of feature source corpus for feature extraction and disam biguated corpus in the task of verb classification into all verbnet class our best model achieved a error reduction in the classification accuracy over the previously proposed model 
in this paper we focus on the opinion target extraction a part of the opinion mining task we model the problem a an information extraction task which we address based on conditional random field crf a a baseline we employ the supervised algorithm by zhuang et al which represents the state of the art on the employed data we evaluate the algorithm comprehensively on datasets from four different domain annotated with individual opinion target instance on a sentence level furthermore we investigate the performance of our crf based approach and the baseline in a singleand cross domain opinion target extraction setting our crf based approach improves the performance by and regarding f measure in the single domain extraction in the four domain in the cross domain setting our approach improves the performance by and regarding f measure over the baseline 
in recent year with the development of chinese semantically annotated corpus such a chinese proposition bank and normalization bank the chinese semantic role labeling srl task ha been boosted similar to english the chinese srl can be divided into two task semantic role identification sri and classification src many feature were introduced into these task and promising result were achieved in this paper we mainly focus on the second task src after exploiting the linguistic discrepancy between numbered argument and argms we built a semantic role classifier based on a hierarchical feature selection strategy different from the previous src system we divided src into three sub task in sequence and trained model for each sub task under the hierarchical architecture each argument should first be determined whether it is a numbered argument or an argm and then be classified into fine gained category finally we integrated the idea of exploiting argument interdependence into our system and further improved the performance with the novel method the classification precision of our system is which outperforms the strong baseline significantly it is also the state of the art on chinese src 
event extraction is a particularly challenging type of information extraction ie most current event extraction system rely on local information at the phrase or sentence level however this local context may be insufficient to resolve ambiguity in identifying particular type of event information from a wider scope can serve to resolve some of these ambiguity in this paper we use document level information to improve the performance of ace event extraction in contrast to previous work we do not limit ourselves to information about event of the same type but rather use information about other type of event to make prediction or resolve ambiguity regarding a given event we learn such relationship from the training corpus and use them to help predict the occurrence of event and event argument in a text experiment show that we can get absolute gain in trigger event classification and more than gain for argument role classification in ace event extraction 
we present an approach to grammar induction that utilizes syntactic universal to improve dependency parsing across a range of language our method us a single set of manually specified language independent rule that identify syntactic dependency between pair of syntactic category that commonly occur across language during inference of the probabilistic model we use posterior expectation constraint to require that a minimum proportion of the dependency we infer be instance of these rule we also automatically refine the syntactic category given in our coarsely tagged input across six language our approach outperforms state of the art unsupervised method by a significant margin 
prior use of machine learning in genre classification used a list of label a classification category however genre class are often organised into hierarchy e g covering the subgenres of fiction in this paper we present a method of using the hierarchy of label to improve the classification accuracy a a testbed for this approach we use the brown corpus a well a a range of other corpus including the bnc hgc and syracuse the result are not encouraging apart from the brown corpus the improvement of our structural classifier over the flat one are not statistically significant we discus the relation between structural learning performance and the visual and distributional balance of the label hierarchy suggesting that only balanced hierarchy might profit from structural learning 
automatically identifying the polarity of word is a very important task in natural language processing it ha application in text classification text filtering analysis of product review analysis of response to survey and mining online discussion we propose a method for identifying the polarity of word we apply a markov random walk model to a large word related ness graph producing a polarity estimate for any given word a key advantage of the model is it ability to accurately and quickly assign a polarity sign and magnitude to any word the method could be used both in a semi supervised setting where a training set of labeled word is used and in an unsupervised setting where a handful of seed is used to define the two polarity class the method is experimentally tested using a manually labeled set of positive and negative word it outperforms the state of the art method in the semi supervised setting the result in the unsupervised setting is comparable to the best reported value however the proposed method is faster and doe not need a large corpus 
the pipeline of most phrase based statistical machine translation pb smt system start from automatically word aligned parallel corpus but word appears to be too fine grained in some case such a non compositional phrasal equivalence where no clear word alignment exist using word a input to pb smt pipeline ha inborn deficiency this paper proposes pseudo word a a new start point for pb smt pipeline pseudo word is a kind of basic multi word expression that characterizes minimal sequence of consecutive word in sense of translation by casting pseudo word searching problem into a parsing framework we search for pseudo word in a monolingual way and a bilingual synchronous way experiment show that pseudo word significantly outperforms word for pb smt model in both travel translation domain and news translation domain 
this paper proposes an approach to reference resolution in situated dialogue by exploiting extra linguistic information recently investigation of referential behaviour involved in situation in the real world have received increasing attention by researcher di eugenio et al byron van deemter spanger et al in order to create an accurate reference resolution model we need to handle extra linguistic information a well a textual information examined by existing approach soon et al ng and cardie etc in this paper we incorporate extra linguistic information into an existing corpus based reference resolution model and investigate it effect on reference resolution problem within a corpus of japanese dialogue the result demonstrate that our proposed model achieves an accuracy of for this task 
in this work we propose two extension of standard word lexicon in statistical machine translation a discriminative word lexicon that us sentence level source information to predict the target word and a trigger based lexicon model that extends ibm model with a second trigger allowing for a more fine grained lexical choice of target word the model capture dependency that go beyond the scope of conventional smt model such a phrase and language model we show that the model improve translation quality by in bleu over a competitive baseline on a large scale task 
we combine lexical syntactic and discourse feature to produce a highly predictive model of human reader judgment of text readability this is the first study to take into account such a variety of linguistic factor and the first to empirically demonstrate that discourse relation are strongly associated with the perceived quality of text we show that various surface metric generally expected to be related to readability are not very good predictor of readability judgment in our wall street journal corpus we also establish that readability predictor behave differently depending on the task predicting text readability or ranking the readability our experiment indicate that discourse relation are the one class of feature that exhibit robustness across these two task 
the graph based ranking algorithm ha been recently exploited for multi document summarization by making only use of the sentence to sentence relationship in the document under the assumption that all the sentence are indistinguishable however given a document set to be summarized different document are usually not equally important and moreover different sentence in a specific document are usually differently important this paper aim to explore document impact on summarization performance we propose a document based graph model to incorporate the document level information and the sentence to document relationship into the graph based ranking process various method are employed to evaluate the two factor experimental result on the duc and duc datasets demonstrate that the good effectiveness of the proposed model moreover the result show the robustness of the proposed model 
we present a novel unsupervised sentence fusion method which we apply to a corpus of biography in german given a group of related sentence we align their dependency tree and build a dependency graph using integer linear programming we compress this graph to a new tree which we then linearize we use germanet and wikipedia for checking semantic compatibility of co argument in an evaluation with human judge our method outperforms the fusion approach of barzilay mckeown with respect to readability 
we reveal a previously unnoticed connection between dependency parsing and statistical machine translation smt by formulating the dependency parsing task a a problem of word alignment furthermore we show that two well known model for these respective task dmv and the ibm model share common modeling assumption this motivates u to develop an alignment based framework for unsupervised dependency parsing the framework which will be made publicly available is flexible modular and easy to extend using this framework we implement several algorithm based on the ibm alignment model which prove surprisingly effective on the dependency parsing task and demonstrate the potential of the alignment based approach 
information of interest to user is often distributed over a set of document user can specify their request for information a a query topic a set of one or more sentence or question producing a good summary of the relevant information relies on understanding the query and linking it with the associated set of document to understand the query we expand it using encyclopedic knowledge in wikipedia the expanded query is linked with it associated document through spreading activation in a graph that represents word and their grammatical connection in these document the topic expanded word and activated node in the graph are used to produce an extractive summary the method proposed is tested on the duc summarization data the system implemented rank high compared to the participating system in the duc competition confirming our hypothesis that encyclopedic knowledge is a useful addition to a summarization system 
how can the development of idea in a scientific field be studied over time we apply unsupervised topic modeling to the acl anthology to analyze historical trend in the field of computational linguistics from to we induce topic cluster using latent dirichlet allocation and examine the strength of each topic over time our method find trend in the field including the rise of probabilistic method starting in a steady increase in application and a sharp decline of research in semantics and understanding between and possibly rising again after we also introduce a model of the diversity of idea topic entropy using it to show that coling is a more diverse conference than acl but that both conference a well a emnlp are becoming broader over time finally we apply jensen shannon divergence of topic distribution to show that all three conference are converging in the topic they cover 
the named entity recognition ner task ha been garnering significant attention in nlp a it help improve the performance of many natural language processing application in this paper we investigate the impact of using different set of feature in two discriminative machine learning framework namely support vector machine and conditional random field using arabic data we explore lexical contextual and morphological feature on eight standardized data set of different genre we measure the impact of the different feature in isolation rank them according to their impact for each named entity class and incrementally combine them in order to infer the optimal machine learning approach and feature set our system yield a performance of f measure on ace broadcast news data 
web search query are known to be short but little else is known about their structure in this paper we investigate the applicability of part of speech tagging to typical english language web search engine query and the potential value of these tag for improving search result we begin by identifying a set of part of speech tag suitable for search query and quantifying their occurrence we find that proper noun constitute of query term and proper noun and noun together constitute over of query term we also show that the majority of query are noun phrase not unstructured collection of term we then use a set of query manually labeled with these tag to train a brill tagger and evaluate it performance in addition we investigate classification of search query into grammatical class based on the syntax of part of speech tag sequence we also conduct preliminary investigative experiment into the practical applicability of leveraging query trained part of speech tagger for information retrieval task in particular we show that part of speech information can be a significant feature in machine learned search result relevance these experiment also include the potential use of the tagger in selecting word for omission or substitution in query reformulation action which can improve recall we conclude that training a part of speech tagger on labeled corpus of query significantly outperforms tagger based on traditional corpus and leveraging the unique linguistic structure of web search query can improve search experience 
with the proliferation of user generated article over the web it becomes imperative to develop automated method that are aware of the ideological bias implicit in a document collection while there exist method that can classify the ideological bias of a given document little ha been done toward understanding the nature of this bias on a topical level in this paper we address the problem of modeling ideological perspective on a topical level using a factored topic model we develop efficient inference algorithm using collapsed gibbs sampling for posterior inference and give various evaluation and illustration of the utility of our model on various document collection with promising result finally we give a metropolis hasting inference algorithm for a semi supervised extension with decent result 
this paper proposes a unified framework for zero anaphora resolution which can be divided into three sub task zero anaphor detection anaphoricity determination and antecedent identification in particular all the three sub task are addressed using tree kernel based method with appropriate syntactic parse tree structure experimental result on a chinese zero anaphora corpus show that the proposed tree kernel based method significantly outperform the feature based one this indicates the critical role of the structural information in zero anaphora resolution and the necessity of tree kernel based method in modeling such structural information to our best knowledge this is the first systematic work dealing with all the three sub task in chinese zero anaphora resolution via a unified framework moreover we release a chinese zero anaphora corpus of document which add a layer of annotation to the manually parsed sentence in the chinese treebank ctb 
graph based semi supervised learning ssl algorithm have been successfully used to extract class instance pair from large unstructured and structured text collection however a careful comparison of different graph based ssl algorithm on that task ha been lacking we compare three graph based ssl algorithm for class instance acquisition on a variety of graph constructed from different domain we find that the recently proposed mad algorithm is the most effective we also show that class instance extraction can be significantly improved by adding semantic information in the form of instance attribute edge derived from an independently developed knowledge base all of our code and data will be made publicly available to encourage reproducible research in this area 
this paper present a new method for inferring the semantic property of document by leveraging free text keyphrase annotation such annotation are becoming increasingly abundant due to the recent dramatic growth in semi structured user generated online content one especially relevant domain is product review which are often annotated by their author with pro con keyphrases such a a real bargain or good value these annotation are representative of the underlying semantic property however unlike expert annotation they are noisy lay author may use different label to denote the same property and some label may be missing to learn using such noisy annotation we find a hidden paraphrase structure which cluster the keyphrases the paraphrase structure is linked with a latent topic model of the review text enabling the system to predict the property of unannotated document and to effectively aggregate the semantic property of multiple review our approach is implemented a a hierarchical bayesian model with joint inference we find that joint inference increase the robustness of the keyphrase clustering and encourages the latent topic to correlate with semantically meaningful property multiple evaluation demonstrate that our model substantially outperforms alternative approach for summarizing single and multiple document into a set of semantically salient keyphrases 
in this paper we investigate how modeling content structure can benefit text analysis application such a extractive summarization and sentiment analysis this follows the linguistic intuition that rich contextual information should be useful in these task we present a framework which combine a supervised text analysis application with the induction of latent content structure both of these element are learned jointly using the em algorithm the induced content structure is learned from a large unannotated corpus and biased by the underlying text analysis task we demonstrate that exploiting content structure yield significant improvement over approach that rely only on local context 
this paper present a simple and effective approach to improve dependency parsing by using subtrees from auto parsed data first we use a baseline parser to parse large scale unannotated data then we extract subtrees from dependency parse tree in the auto parsed data finally we construct new subtree based feature for parsing algorithm to demonstrate the effectiveness of our proposed approach we present the experimental result on the english penn treebank and the chinese penn treebank these result show that our approach significantly outperforms baseline system and it achieves the best accuracy for the chinese data and an accuracy which is competitive with the best known system for the english data 
in this work the problem of extracting phrase translation is formulated a an information retrieval process implemented with a log linear model aiming for a balanced precision and recall we present a generic phrase training algorithm which is parameterized with feature function and can be optimized jointly with the translation engine to directly maximize the end to end system performance multiple data driven feature function are proposed to capture the quality and confidence of phrase and phrase pair experimental result demonstrate consistent and significant improvement over the widely used method that is based on word alignment matrix only 
we propose a new model for unsupervised po tagging based on linguistic distinction between open and closed class item exploiting notion from current linguistic theory the system us far le information than previous system far simpler computational method and far sparser description in learning context by applying simple language acquisition technique based on counting the system is given the closed class lexicon acquires a large open class lexicon and then acquires disambiguation rule for both this system achieves a error reduction for po tagging over state of the art unsupervised system tested under the same condition and achieves comparable accuracy when trained with much le prior information 
we present a fully automatic method for content selection evaluation in summarization that doe not require the creation of human model summary our work capitalizes on the assumption that the distribution of word in the input and an informative summary of that input should be similar to each other result on a large scale evaluation from the text analysis conference show that input summary comparison are very effective for the evaluation of content selection our automatic method rank participating system similarly to manual model based pyramid evaluation and to manual human judgment of responsiveness the best feature jensen shannon divergence lead to a correlation a high a with manual pyramid and with responsiveness evaluation 
we present a method for learning bilingual translation lexicon from monolingual corpus word type in each language are characterized by purely monolingual feature such a context count and orthographic substring translation are induced using a generative model based on canonical correlation analysis which explains the monolingual lexicon in term of latent matchings we show that high precision lexicon can be learned in a variety of language pair and from a range of corpus type 
we present a study on how grammar binarization empirically affect the efficiency of the cky parsing we argue that binarizations affect parsing efficiency primarily by affecting the number of incomplete constituent generated and the effectiveness of binarization also depends on the nature of the input we propose a novel binarization method utilizing rich information learnt from training corpus experimental result not only show that different binarizations have great impact on parsing efficiency but also confirm that our learnt binarization outperforms other existing method furthermore we show that it is feasible to combine existing parsing speed up technique with our binarization to achieve even better performance 
we present a novel approach for written dialect identification based on the discriminative potential of entire word we generate swiss german dialect word from a standard german lexicon with the help of hand crafted phonetic graphemic rule that are associated with occurrence map extracted from a linguistic atlas created through extensive empirical fieldwork in comparison with a character n gram approach to dialect identification our model is more robust to individual spelling difference which are frequently encountered in non standardized dialect writing moreover it cover the whole swiss german dialect continuum which trained model struggle to achieve due to sparsity of training data 
this paper provides evidence that the use of more unlabeled data in semi supervised learning can improve the performance of natural language processing nlp task such a part of speech tagging syntactic chunking and named entity recognition we first propose a simple yet powerful semi supervised discriminative model appropriate for handling large scale unlabeled data then we describe experiment performed on widely used test collection namely ptb iii data conll and shared task data for the above three nlp task respectively we incorporate up to g word one billion token of unlabeled data which is the largest amount of unlabeled data ever used for these task to investigate the performance improvement in addition our result are superior to the best reported result for all of the above test collection 
this paper present a probabilistic model for sense disambiguation which chooses the best sense based on the conditional probability of sense paraphrase given a context we use a topic model to decompose this conditional probability into two conditional probability with latent variable we propose three different instantiation of the model for solving sense disambiguation problem with different degree of resource availability the proposed model are tested on three different task coarse grained word sense disambiguation fine grained word sense disambiguation and detection of literal v non literal usage of potentially idiomatic expression in all three case we outperform state of the art system either quantitatively or statistically significantly 
text and dialogue often express information indirectly for instance speaker answer to yes no question do not always straightforwardly convey a yes or no answer the intended reply is clear in some case wa it good it wa great but uncertain in others wa it acceptable it wa unprecedented in this paper we present method for interpreting the answer to question like these which involve scalar modifier we show how to ground scalar modifier meaning based on data collected from the web we learn scale between modifier and infer the extent to which a given answer conveys yes or no to evaluate the method we collected example of question answer pair involving scalar modifier from cnn transcript and the dialog act corpus and use response distribution from mechanical turk worker to ass the degree to which each answer conveys yes or no our experimental result closely match the turkers response data demonstrating that meaning can be learned from web data and that such meaning can drive pragmatic inference 
we apply machine learning to the linear ordering problem in order to learn sentence specific reordering model for machine translation we demonstrate that even when these model are used a a mere preprocessing step for german english translation they significantly outperform moses integrated lexicalized reordering model our model are trained on automatically aligned bitext their form is simple but novel they ass based on feature of the input sentence how strongly each pair of input word token wi wj would like to reverse their relative order combining all these pairwise preference to find the best global reordering is np hard however we present a non trivial o n algorithm based on chart parsing that at least find the best reordering within a certain exponentially large neighborhood we show how to iterate this reordering process within a local search algorithm which we use in training 
polarity lexicon have been a valuable resource for sentiment analysis and opinion mining there are a number of such lexical resource available but it is often suboptimal to use them a is because general purpose lexical resource do not reflect domain specific lexical usage in this paper we propose a novel method based on integer linear programming that can adapt an existing lexicon into a new one to reflect the characteristic of the data more directly in particular our method collectively considers the relation among word and opinion expression to derive the most likely polarity of each lexical item positive neutral negative or negator for the given domain experimental result show that our lexicon adaptation technique improves the performance of fine grained polarity classification 
previous work on statistical language generation ha primarily focused on grammaticality and naturalness scoring generation possibility according to a language model or user feedback more recent work ha investigated data driven technique for controlling linguistic style without overgeneration by reproducing variation dimension extracted from corpus another line of work ha produced handcrafted rule based system to control specific stylistic dimension such a politeness and personality this paper describes a novel approach that automatically learns to produce recognisable variation along a meaningful stylistic dimension personality without the computational cost incurred by overgeneration technique we present the first evaluation of a data driven generation method that project multiple personality trait simultaneously and on a continuous scale we compare our performance to a rule based generator in the same domain 
we present a grand challenge to build a corpus that will include all of the world s language in a consistent structure that permit large scale cross linguistic processing enabling the study of universal linguistics the focal data type bilingual text and lexicon relate each language to one of a set of reference language we propose that the ability to train system to translate into and out of a given language be the yardstick for determining when we have successfully captured a language we call on the computational linguistics community to begin work on this universal corpus pursuing the many strand of activity described here a their contribution to the global effort to document the world s linguistic heritage before more language fall silent 
although many algorithm have been developed to harvest lexical resource few organize the mined term into taxonomy we propose a semi supervised algorithm that us a root concept a basic level concept and recursive surface pattern to learn automatically from the web hyponym hypernym pair subordinated to the root a web based concept positioning procedure to validate the learned pair is a relation and a graph algorithm that derives from scratch the integrated taxonomy structure of all the term comparing result with wordnet we find that the algorithm miss some concept and link but also that it discovers many additional one lacking in wordnet we evaluate the taxonomization power of our method on reconstructing part of the wordnet taxonomy experiment show that starting from scratch the algorithm can reconstruct of the wordnet taxonomy for the region tested 
we present a machine translation framework that can incorporate arbitrary feature of both input and output sentence the core of the approach is a novel decoder based on lattice parsing with quasi synchronous grammar smith and eisner a syntactic formalism that doe not require source and target tree to be isomorphic using generic approximate dynamic programming technique this decoder can handle non local feature similar approximate inference technique support efficient parameter estimation with hidden variable we use the decoder to conduct controlled experiment on a german to english translation task to compare lexical phrase syntax and combined model and to measure effect of various restriction on non isomorphism 
this paper explores joint syntactic and semantic parsing of chinese to further improve the performance of both syntactic and semantic parsing in particular the performance of semantic parsing in this paper semantic role labeling this is done from two level firstly an integrated parsing approach is proposed to integrate semantic parsing into the syntactic parsing process secondly semantic information generated by semantic parsing is incorporated into the syntactic parsing model to better capture semantic information in syntactic parsing evaluation on chinese treebank chinese propbank and chinese nombank show that our integrated parsing approach outperforms the pipeline parsing approach on n best parse tree a natural extension of the widely used pipeline parsing approach on the top best parse tree moreover it show that incorporating semantic role related information into the syntactic parsing model significantly improves the performance of both syntactic parsing and semantic parsing to our best knowledge this is the first research on exploring syntactic parsing and semantic role labeling for both verbal and nominal predicate in an integrated way 
traditionally machine learning approach for information extraction require human annotated data that can be costly and time consuming to produce however in many case there already exists a database db with schema related to the desired output and record related to the expected input text we present a conditional random field crf that aligns token of a given db record and it realization in text the crf model is trained using only the available db and unlabeled text with generalized expectation criterion an annotation of the text induced from inferred alignment is used to train an information extractor we evaluate our method on a citation extraction task in which alignment between dblp database record and citation text are used to train an extractor experimental result demonstrate an error reduction of over a previous state of the art method that us heuristic alignment 
in lexicalized grammatical formalism it is possible to separate lexical category assignment from the combinatory process that make use of such category such a parsing and realization we adapt technique from supertagging a relatively recent technique that performs complex lexical tagging before full parsing bangalore and joshi clark for chart realization in openccg an open source nlp toolkit for ccg we call this approach hypertagging a it operates at a level above the syntax tagging semantic representation with syntactic lexical category our result demonstrate that a hypertagger informed chart realizer can achieve substantial improvement in realization speed being approximately twice a fast with superior realization quality 
a human annotator can provide hint to a machine learner by highlighting contextual rationale for each of his or her annotation zaidan et al how can one exploit this side information to better learn the desired parameter we present a generative model of how a given annotator knowing the true stochastically chooses rationale thus observing the rationale help u infer the true we collect substring rationale for a sentiment classification task pang and lee and use them to obtain significant accuracy improvement for each annotator our new generative approach exploit the rationale more effectively than our previous masking svm approach it is also more principled and could be adapted to help learn other kind of probabilistic classifier for quite different task 
we propose two hashing based solution to the problem of fast and effective personal name spelling correction in people search application the key idea behind our method is to learn hash function that map similar name to similar and compact binary codewords the two method differ in the data they use for learning the hash function the first method us a set of name in a given language script whereas the second us a set of bilingual name we show that both method give excellent retrieval performance in comparison to several baseline on two list of misspelled personal name more over the method that us bilingual data for learning hash function give the best performance 
in this paper we present an algorithm for learning a generative model of natural language sentence together with their formal meaning representation with hierarchical structure the model is applied to the task of mapping sentence to hierarchical representation of their underlying meaning we introduce dynamic programming technique for efficient training and decoding in experiment we demonstrate that the model when coupled with a discriminative reranking technique achieves state of the art performance when tested on two publicly available corpus the generative model degrades robustly when presented with instance that are different from those seen in training this allows a notable improvement in recall compared to previous model 
inducing a grammar directly from text is one of the oldest and most challenging task in computational linguistics significant progress ha been made for inducing dependency grammar however the model employed are overly simplistic particularly in comparison to supervised parsing model in this paper we present an approach to dependency grammar induction using tree substitution grammar which is capable of learning large dependency fragment and thereby better modelling the text we define a hierarchical non parametric pitman yor process prior which bias towards a small grammar with simple production this approach significantly improves the state of the art when measured by head attachment accuracy 
a fundamental step in sentence comprehension involves assigning semantic role to sentence constituent to accomplish this the listener must parse the sentence find constituent that are candidate argument and assign semantic role to those constituent each step depends on prior lexical and syntactic knowledge where do child learning their first language begin in solving this problem in this paper we focus on the parsing and argument identification step that precede semantic role labeling srl training we combine a simplified srl with an un supervised hmm part of speech tagger and experiment with psycholinguistically motivated way to label cluster resulting from the hmm so that they can be used to parse input for the srl system the result show that proposed shallow representation of sentence structure are robust to reduction in parsing accuracy and that the contribution of alternative representation of sentence structure to successful semantic role labeling varies with the integrity of the parsing and argument identification stage 
jointly parsing two language ha been shown to improve accuracy on either or both side however it search space is much bigger than the monolingual case forcing existing approach to employ complicated modeling and crude approximation here we propose a much simpler alternative bilingually constrained monolingual parsing where a source language parser learns to exploit reordering a additional observation but not bothering to build the target side tree a well we show specifically how to enhance a shift reduce dependency parser with alignment feature to resolve shift reduce conflict experiment on the bilingual portion of chinese treebank show that with just bilingual feature we can improve parsing accuracy by absolute for both english and chinese over a state of the art baseline with negligible efficiency overhead thus much faster than biparsing 
to date parser have made limited use of semantic information but there is evidence to suggest that semantic feature can enhance parse disambiguation this paper show that semantic class help to obtain significant improvement in both parsing and pp attachment task we devise a gold standard senseand parse tree annotated dataset based on the intersection of the penn treebank and semcor and experiment with different approach to both semantic representation and disambiguation for the bikel parser we achieved a maximal error reduction rate over the baseline parser of and for parsing and pp attachment respectively using an unsupervised wsd strategy this demonstrates that word sense information can indeed enhance the performance of syntactic disambiguation 
applying statistical parser developed for english to language with freer word order ha turned out to be harder than expected this paper investigates the adequacy of different statistical parsing model for dealing with a relatively free word order language we show that the recently proposed relational realizational rr model consistently outperforms state of the art head driven hd model on the hebrew treebank our analysis reveals a weakness of hd model their intrinsic focus on configurational information we conclude that the form function separation ingrained in rr model make them better suited for parsing nonconfigurational phenomenon 
the person cross document coreference system depend on the context for making decision on the possible coreference between person name mention the amount of context required is a parameter that varies from corpus to corpus which make it difficult for usual disambiguation method in this paper we show that the amount of context required can be dynamically controlled on the basis of the prior probability of coreference and we present a new statistical model for the computation of these probability the experiment we carried on a news corpus prof that the prior probability of coreference are an important factor for maintaining a good balance between precision and recall for cross document coreference system 
a number of result in the study of realtime sentence comprehension have been explained by computational model a resulting from the rational use of probabilistic linguistic information many time these hypothesis have been tested in reading by linking prediction about relative word difficulty to word aggregated eye tracking measure such a go past time in this paper we extend these result by asking to what extent reading is well modeled a rational behavior at a finer level of analysis predicting not aggregate measure but the duration and location of each fixation we present a new rational model of eye movement control in reading the central assumption of which is that eye movement decision are made to obtain noisy visual information a the reader performs bayesian inference on the identity of the word in the sentence a a case study we present two simulation demonstrating that the model give a rational explanation for between word regression 
one goal of natural language generation is to produce coherent text that present information in a logical order in this paper we show that topological field which model high level clausal structure are an important component of local coherence in german first we show in a sentence ordering experiment that topological field information improves the entity grid model of barzilay and lapata more than grammatical role and simple clausal order information do particularly when manual annotation of this information are not available then we incorporate the model enhanced with topological field into a natural language generation system that generates constituent order for german text and show that the added coherence component improves performance slightly though not statistically significantly 
the design of practical language application by mean of statistical approach requires annotated data which is one of the most critical constraint this is particularly true for spoken dialog system since considerably domain specific conceptual annotation is needed to obtain accurate language understanding model since data annotation is usually costly method to reduce the amount of data are needed in this paper we show that better feature representation serve the above purpose and that structure kernel provide the needed improved representation given the relatively high computational cost of kernel method we apply them to just re rank the list of hypothesis provided by a fast generative model experiment with support vector machine and different kernel on two different dialog corpus show that our re ranking model can achieve better result than state of the art approach when small data is available 
this paper proposes a framework for representing the meaning of phrase and sentence in vector space central to our approach is vector composition which we operationalize in term of additive and multiplicative function under this framework we introduce a wide range of composition model which we evaluate empirically on a sentence similarity task experimental result demonstrate that the multiplicative model are superior to the additive alternative when compared against human judgment 
we identify four type of error that unsupervised induction system make and study each one in turn our contribution include using a meta model to analyze the incorrect bias of a model in a systematic way providing an efficient and robust method of measuring distance between two parameter setting of a model and showing that local optimum issue which typically plague em can be somewhat alleviated by increasing the number of training example we conduct our analysis on three model the hmm the pcfg and a simple dependency model 
we present a novel approach for multilingual document clustering using only comparable corpus to achieve cross lingual semantic interoperability the method model document collection a weighted graph and supervisory information is given a set of must linked constraint for document in different language recursive k nearest neighbor similarity propagation is used to exploit the prior knowledge and merge two language space spectral method is applied to find the best cut of the graph experimental result show that using limited supervisory information our method achieves promising clustering result furthermore since the method doe not need any language dependent information in the process our algorithm can be applied to language in various alphabetical system 
determining the polarity of a sentiment bearing expression requires more than a simple bag of word approach in particular word or constituent within the expression can interact with each other to yield a particular overall polarity in this paper we view such subsentential interaction in light of compositional semantics and present a novel learning based approach that incorporates structural inference motivated by compositional semantics into the learning procedure our experiment show that simple heuristic based on compositional semantics can perform better than learning based method that do not incorporate compositional semantics accuracy of v but a method that integrates compositional semantics into learning performs better than all other alternative we also find that content word negators not widely employed in previous work play an important role in determining expression level polarity finally in contrast to conventional wisdom we find that expression level classification accuracy uniformly decrease a additional potentially disambiguating context is considered 
language is sensitive to both semantic and pragmatic effect to capture both effect we model language use a a cooperative game between two player a speaker who generates an utterance and a listener who responds with an action specifically we consider the task of generating spatial reference to object wherein the listener must accurately identify an object described by the speaker we show that a speaker model that act optimally with respect to an explicit embedded listener model substantially outperforms one that is trained to directly generate spatial description 
in this paper we introduce a corpus of consumer review from the rateitall and the eopinions website annotated with opinion related information we present a two level annotation scheme in the first stage the review are analyzed at the sentence level for i relevancy to a given topic and ii expressing an evaluation about the topic in the second stage on topic sentence containing evaluation about the topic are further investigated at the expression level for pinpointing the property semantic orientation intensity and the functional component of the evaluation opinion term target and holder we discus the annotation scheme the inter annotator agreement for different subtasks and our observation 
phrase based decoding produce state of theart translation with no regard for syntax we add syntax to this process with a cohesion constraint based on a dependency tree for the source sentence the constraint allows the decoder to employ arbitrary non syntactic phrase but ensures that those phrase are translated in an order that respect the source tree s structure in this way we target the phrasal decoder s weakness in order modeling without affecting it strength to further increase flexibility we incorporate cohesion a a decoder feature creating a soft constraint the resulting cohesive phrase based decoder is shown to produce translation that are preferred over non cohesive output in both automatic and human evaluation 
this paper proposes a novel maximum entropy based rule selection mers model for syntax based statistical machine translation smt the mers model combine local contextual information around rule and information of sub tree covered by variable in rule therefore our model allows the decoder to perform context dependent rule selection during decoding we incorporate the mers model into a state of the art linguistically syntax based smt model the tree to string alignment template model experiment show that our approach achieves significant improvement over the baseline system 
we present a new phrase based conditional exponential family translation model for statistical machine translation the model operates on a feature representation in which sentence level translation are represented by enumerating all the known phrase level translation that occur inside them this make the model a good match with the commonly used phrase extraction heuristic the model s prediction are properly normalized probability in addition the model automatically take into account information provided by phrase overlap and doe not suffer from reference translation reachability problem we have implemented an open source translation system sinuhe based on the proposed translation model our experiment on europarl and gigafren corpus demonstrate that finding the unique map parameter for the model on large scale data is feasible with simple stochastic gradient method sinuhe is fast and memory efficient and the bleu score obtained by it are only slightly inferior to those of moses 
this paper connects two research area automatic tagging on the web and statistical keyphrase extraction first we analyze the quality of tag in a collaboratively created folksonomy using traditional evaluation technique next we demonstrate how document can be tagged automatically with a state of the art keyphrase extraction algorithm and further improve performance in this new domain using a new algorithm maui that utilizes semantic information extracted from wikipedia maui outperforms existing approach and extract tag that are competitive with those assigned by the best performing human tagger 
we present a data driven approach to learn user adaptive referring expression generation reg policy for spoken dialogue system referring expression can be difficult to understand in technical domain where user may not know the technical jargon name of the domain entity in such case dialogue system must be able to model the user s lexical domain knowledge and use appropriate referring expression we present a reinforcement learning rl framework in which the system learns reg policy which can adapt to unknown user online furthermore unlike supervised learning method which require a large corpus of expert adaptive behaviour to train on we show that effective adaptive policy can be learned from a small dialogue corpus of non adaptive human machine interaction by using a rl framework and a statistical user simulation we show that in comparison to adaptive hand coded baseline policy the learned policy performs significantly better with an average increase in adaptation accuracy the best learned policy also take le dialogue time average min le than the best hand coded policy this is because the learned policy can adapt online to changing evidence about the user s domain expertise 
the use of lexical semantic knowledge in information retrieval ha been a field of active study for a long time collaborative knowledge base like wikipedia and wiktionary which have been applied in computational method only recently offer new possibility to enhance information retrieval in order to find the most beneficial way to employ these resource we analyze the lexical semantic relation that hold among query and document term and compare how these relation are represented by a measure for semantic relatedness we explore the potential of different indicator of document relevance that are based on semantic relatedness and compare the characteristic and performance of the knowledge base wikipedia wiktionary and wordnet 
tree to string translation rule are widely used in linguistically syntax based statistical machine translation system in this paper we propose to use deep syntactic information for obtaining fine grained translation rule a head driven phrase structure grammar hpsg parser is used to obtain the deep syntactic information which includes a fine grained description of the syntactic property and a semantic representation of a sentence we extract fine grained rule from aligned hpsg tree forest string pair and use them in our tree to string and string to tree system extensive experiment on large scale bidirectional japanese english translation testified the effectiveness of our approach 
we describe a new scalable algorithm for semi supervised training of conditional random field crf and it application to part of speech po tagging the algorithm us a similarity graph to encourage similar n gram to have similar po tag we demonstrate the efficacy of our approach on a domain adaptation task where we assume that we have access to large amount of unlabeled data from the target domain but no additional labeled data the similarity graph is used during training to smooth the state posterior on the target domain standard inference can be used at test time our approach is able to scale to very large problem and yield significantly improved target domain accuracy 
this paper present a new method of developing a large scale hyponymy relation database by combining wikipedia and other web document we attach new word to the hyponymy database extracted from wikipedia by using distributional similarity calculated from document on the web for a given target word our algorithm first find k similar word from the wikipedia database then the hypernym of these k similar word are assigned score by considering the distributional similarity and hierarchical distance in the wikipedia database finally new hyponymy relation are output according to the score in this paper we tested two distributional similarity one is based on raw verb noun dependency which we call rvd and the other is based on a large scale clustering of verb noun dependency called cvd our method achieved an attachment accuracy of for the top relation and an attachment accuracy of for the top relation when using cvd this wa a far better outcome compared to the other baseline approach excluding the region that had very high score cvd wa found to be more effective than rvd we also confirmed that most relation extracted by our method cannot be extracted merely by applying the well known lexico syntactic pattern to web document 
for resource limited language pair coverage of the test set by the parallel corpus is an important factor that affect translation quality in two respect out of vocabulary word the same information in an input sentence can be expressed in different way while current phrase based smt system cannot automatically select an alternative way to transfer the same information therefore given limited data in order to facilitate translation from the input side this paper proposes a novel method to reduce the translation difficulty using source side lattice based paraphrase we utilise the original phrase from the input sentence and the corresponding paraphrase to build a lattice with estimated weight for each edge to improve translation quality compared to the baseline system our method achieves relative improvement of and in term of bleu score on small medium and large scale english to chinese translation task respectively the result show that the proposed method is effective not only for resource limited language pair but also for resource sufficient pair to some extent 
this research explores the idea of inducing domain specific semantic class tagger using only a domain specific text collection and seed word the learning process begin by inducing a classifier that only ha access to contextual feature forcing it to generalize beyond the seed the contextual classifier then label new instance to expand and diversify the training set next a cross category bootstrapping process simultaneously train a suite of classifier for multiple semantic class the positive instance for one class are used a negative instance for the others in an iterative bootstrapping cycle we also explore a one semantic class per discourse heuristic and use the classifier to dynamically create semantic feature we evaluate our approach by inducing six semantic tagger from a collection of veterinary medicine message board post 
it is well known that pragmatic knowledge is useful and necessary in many difficult language processing task but because this knowledge is difficult to acquire and process automatically it is rarely used we present an open information extraction technique for automatically extracting a particular kind of pragmatic knowledge from text and we show how to integrate the knowledge into a markov logic network model for quantifier scope disambiguation our model improves quantifier scope judgment in experiment 
the conditional phrase translation probability constitute the principal component of phrase based machine translation system these probability are estimated using a heuristic method that doe not seem to optimize any reasonable objective function of the word aligned parallel training corpus earlier effort on devising a better understood estimator either do not scale to reasonably sized training data or lead to deteriorating performance in this paper we explore a new approach based on three ingredient a generative model with a prior over latent segmentation derived from inversion transduction grammar itg a phrase table containing all phrase pair without length limit and smoothing a learning objective using a novel maximum a posteriori version of deleted estimation working with expectation maximization where others conclude that latent segmentation lead to overfitting and deteriorating performance we show here that these three ingredient give performance equivalent to the heuristic method on reasonably sized training data 
combinatory categorial grammar ccg is generally construed a a fully lexicalized formalism where all grammar use one and the same universal set of rule and cross linguistic variation is isolated in the lexicon in this paper we show that the weak generative capacity of this pure form of ccg is strictly smaller than that of ccg with grammar specific rule and of other mildly context sensitive grammar formalism including tree adjoining grammar tag our result also carry over to a multi modal extension of ccg 
the reliable extraction of knowledge from text requires an appropriate treatment of the time at which reported event take place unfortunately there are very few annotated data set that support the development of technique for event time stamping and tracking the progression of time through a narrative in this paper we present a new corpus of temporally rich document sourced from english wikipedia which we have annotated with timex tag the corpus contains around token and timex expression thus comparing favourably in size to other existing corpus used in these area we describe the preparation of the corpus and compare the profile of the data with other existing temporally annotated corpus we also report the result obtained when we use dante our temporal expression tagger to process this corpus and point to where further work is required the corpus is publicly available for research purpose 
in this paper we address the problem of identifying a broad range of term variation in japanese web search query where these variation pose a particularly thorny problem due to the multiple character type employed in it writing system our method extends the technique proposed for english spelling correction of web query to handle a wider range of term variant including spelling mistake valid alternative spelling using multiple character type transliteration and abbreviation the core of our method is a statistical model built on the mart algorithm friedman we show that both string and semantic similarity feature contribute to identifying term variation in web search query specifically the semantic similarity feature used in our system are learned by mining user session and click through log and are useful not only a model feature but also in generating term variation candidate efficiently the proposed method achieves precision on the term variation identification task with the recall slightly higher than reducing the error rate of a na ve baseline by 
most state of the art wide coverage parser are trained on newspaper text and suffer a loss of accuracy in other domain making parser adaptation a pressing issue in this paper we demonstrate that a ccg parser can be adapted to two new domain biomedical text and question for a qa system by using manually annotated training data at the po and lexical category level only this approach achieves parser accuracy comparable to that on newspaper data without the need for annotated parse tree in the new domain we find that retraining at the lexical category level yield a larger performance increase for question than for biomedical text and analyze the two datasets to investigate why different domain might behave differently for parser adaptation 
factored statistical machine translation extends the phrase based smt model by allowing each word to be a vector of factor experiment have shown effectiveness of many factor including the part of speech tag in improving the grammaticality of the output however high quality part of speech tagger are not available in open domain for many language in this paper we used fixed length word suffix a a new factor in the factored smt and were able to achieve significant improvement in three set of experiment large nist arabic to english system medium wmt spanish to english system and small transtac english to iraqi system 
we present a method for automatically generating focused and accurate topic specific subjectivity lexicon from a general purpose polarity lexicon that allow user to pin point subjective on topic information in a set of relevant document we motivate the need for such lexicon in the field of medium analysis describe a bootstrapping method for generating a topic specific lexicon from a general purpose polarity lexicon and evaluate the quality of the generated lexicon both manually and using a trec blog track test set for opinionated blog post retrieval although the generated lexicon can be an order of magnitude more selective than the general purpose lexicon they maintain or even improve the performance of an opinion retrieval system 
there is considerable interest in interdisciplinary combination of automatic speech recognition asr machine learning natural language processing text classification and information retrieval many of these box especially asr are often based on considerable linguistic resource we would like to be able to process spoken document with few if any resource moreover connecting black box in series tends to multiply error especially when the key term are out of vocabulary oov the proposed alternative applies text processing directly to the speech without a dependency on asr the method find long sec repetition in speech and cluster them into pseudo term roughly phrase document clustering and classification work surprisingly well on pseudo term performance on a switchboard task approach a baseline using gold standard manual transcription 
in situated dialogue human often utter linguistic expression that refer to extralinguistic entity in the environment correctly resolving these reference is critical yet challenging for artificial agent partly due to their limited speech recognition and language understanding capability motivated by psycholinguistic study demonstrating a tight link between language production and human eye gaze we have developed approach that integrate naturally occurring human eye gaze with speech recognition hypothesis to resolve exophoric reference in situated dialogue in a virtual world in addition to incorporating eye gaze with the best recognized spoken hypothesis we developed an algorithm to also handle multiple hypothesis modeled a word confusion network our empirical result demonstrate that incorporating eye gaze with recognition hypothesis consistently outperforms the result obtained from processing recognition hypothesis alone incorporating eye gaze with word confusion network further improves performance 
we report on an experiment to track complex decision point in linguistic metadata annotation where the decision behavior of annotator is observed with an eye tracking device a experimental condition we investigate different form of textual context and linguistic complexity class relative to syntax and semantics our data render evidence that annotation performance depends on the semantic and syntactic complexity of the decision point and more interestingly indicates that full scale context is mostly negligible with the exception of semantic high complexity case we then induce from this observational data a cognitively grounded cost model of linguistic meta data annotation and compare it with existing non cognitive model our data reveals that the cognitively founded model explains annotation cost expressed in annotation time more adequately than non cognitive one 
we investigate active learning method for japanese dependency parsing we propose active learning method of using partial dependency relation in a given sentence for parsing and evaluate their effectiveness empirically furthermore we utilize syntactic constraint of japanese to obtain more labeled example from precious labeled one that annotator give experimental result show that our proposed method improve considerably the learning curve of japanese dependency parsing in order to achieve an accuracy of over one of our method requires only of labeled example a compared to passive learning 
unknown word are a hindrance to the performance of hand crafted computational grammar of natural language however word with incomplete and incorrect lexical entry pose an even bigger problem because they can be the cause of a parsing failure despite being listed in the lexicon of the grammar such lexical entry are hard to detect and even harder to correct we employ an error miner to pinpoint word with problematic lexical entry an automated lexical acquisition technique is then used to learn new entry for those word which allows the grammar to parse previously uncovered sentence successfully we test our method on a large scale grammar of dutch and a set of sentence for which this grammar fails to produce a parse the application of the method enables the grammar to cover of those sentence with an accuracy of 
in this paper we describe an intuitionistic method for dependency parsing where a classifier is used to determine whether a pair of word form a dependency edge and we also propose an effective strategy for dependency projection where the dependency relationship of the word pair in the source language are projected to the word pair of the target language leading to a set of classification instance rather than a complete tree experiment show that the classifier trained on the projected classification instance significantly outperforms previous projected dependency parser more importantly when this classifier is integrated into a maximum spanning tree mst dependency parser obvious improvement is obtained over the mst baseline 
there are many possible different semantic relationship between nominal classification of such relationship is an important and difficult task for example the well known noun compound classification task is a special case of this problem we propose a novel pattern cluster method for nominal relationship nr classification pattern cluster are discovered in a large corpus independently of any particular training set in an unsupervised manner each of the extracted cluster corresponds to some unspecified semantic relationship the pattern cluster are then used to construct feature for training and classification of specific inter nominal relationship our nr classification evaluation strictly follows the acl semeval task datasets and protocol obtaining an f score of a opposed to of the best previous work that did not use the manually provided wordnet sense disambiguation tag 
we present a scalable joint language model designed to utilize fine grain syntactic tag we discus challenge such a design face and describe our solution that scale well to large tagsets and corpus we advocate the use of relatively simple tag that do not require deep linguistic knowledge of the language but provide more structural information than po tag and can be derived from automatically generated parse tree a combination of property that allows easy adoption of this model for new language we propose two fine grain tagsets and evaluate our model using these tag a well a po tag and superarv tag in a speech recognition task and discus future direction 
we present a novel approach to relation extraction that integrates information across document performs global inference and requires no labelled text in particular we tackle relation extraction and entity identification jointly we use distant supervision to train a factor graph model for relation extraction based on an existing knowledge base freebase derived in part from wikipedia for inference we run an efficient gibbs sampler that lead to linear time joint inference we evaluate our approach both for an indomain wikipedia and a more realistic out of domain new york time corpus setting for the in domain setting our joint model lead to higher precision than an isolated local approach but ha no advantage over a pipeline for the out of domain data we benefit strongly from joint modelling and observe improvement in precision of over the pipeline and over the isolated baseline 
distinguishing speculative statement from factual one is important for most biomedical text mining application we introduce an approach which is based on solving two sub problem to identify speculative sentence fragment the first sub problem is identifying the speculation keywords in the sentence and the second one is resolving their linguistic scope we formulate the first sub problem a a supervised classification task where we classify the potential keywords a real speculation keywords or not by using a diverse set of linguistic feature that represent the context of the keywords after detecting the actual speculation keywords we use the syntactic structure of the sentence to determine their scope 
the rapid growth of geotagged social medium raise new computational possibility for investigating geographic linguistic variation in this paper we present a multi level generative model that reason jointly about latent topic and geographical region high level topic such a sport or entertainment are rendered differently in each geographic region revealing topic specific regional distinction applied to a new dataset of geotagged microblogs our model recovers coherent topic and their regional variant while identifying geographic area of linguistic consistency the model also enables prediction of an author s geographic location from raw text outperforming both text regression and supervised topic model 
in many application replacing a complex word form by it stem can reduce sparsity revealing connection in the data that would not otherwise be apparent in this paper we focus on prefix verb verb formed by adding a prefix to an existing verb stem a prefix verb is considered compositional if it can be decomposed into a semantically equivalent expression involving it stem we develop a classifier to predict compositionality via a range of lexical and distributional feature including novel feature derived from web scale n gram data result on a new annotated corpus show that prefix verb compositionality can be predicted with high accuracy our system also performs well when trained and tested on conventional morphological segmentation of prefix verb 
query expansion is an effective technique to improve the performance of information retrieval system although hand crafted lexical resource such a wordnet could provide more reliable related term previous study showed that query expansion using only wordnet lead to very limited performance improvement one of the main challenge is how to assign appropriate weight to expanded term in this paper we re examine this problem using recently proposed axiomatic approach and find that with appropriate term weighting strategy we are able to exploit the information from lexical resource to significantly improve the retrieval performance our empirical result on six trec collection show that query expansion using only hand crafted lexical resource lead to significant performance improvement the performance can be further improved if the proposed method is combined with query expansion using co occurrence based resource 
this paper proposes to use monolingual collocation to improve statistical machine translation smt we make use of the collocation probability which are estimated from monolingual corpus in two aspect namely improving word alignment for various kind of smt system and improving phrase table for phrase based smt the experimental result show that our method improves the performance of both word alignment and translation quality significantly a compared to baseline system we achieve absolute improvement of bleu score on a phrase based smt system and bleu score on a parsing based smt system 
we consider the search for a maximum likelihood assignment of hidden derivation and grammar weight for a probabilistic context free grammar the problem approximately solved by viterbi training we show that solving and even approximating viterbi training for pcfgs is np hard we motivate the use of uniformat random initialization for viterbi em a an optimal initializer in absence of further information about the correct model parameter providing an approximate bound on the log likelihood 
a described in this paper we propose a new automatic evaluation method for machine translation using noun phrase chunking our method correctly determines the matching word between two sentence using corresponding noun phrase moreover our method determines the similarity between two sentence in term of the noun phrase order of appearance evaluation experiment were conducted to calculate the correlation among human judgment along with the score produced using automatic evaluation method for mt output obtained from the machine translation system in ntcir experimental result show that our method obtained the highest correlation among the method in both sentence level adequacy and fluency 
a number of recent publication have made use of the incremental output of stochastic parser to derive measure of high utility for psycholinguistic modeling following the work of hale in this paper we present novel method for calculating separate lexical and syntactic surprisal measure from a single incremental parser using a lexicalized pcfg we also present an approximation to entropy measure that would otherwise be intractable to calculate for a grammar of that size empirical result demonstrate the utility of our method in predicting human reading time 
this paper proposes a dependency parsing method that us bilingual constraint to improve the accuracy of parsing bilingual text bitexts in our method a target side tree fragment that corresponds to a source side tree fragment is identified via word alignment and mapping rule that are automatically learned then it is verified by checking the subtree list that is collected from large scale automatically parsed data on the target side our method thus requires gold standard tree only on the source side of a bilingual corpus in the training phase unlike the joint parsing model which requires gold standard tree on the both side compared to the reordering constraint model which requires the same training data a ours our method achieved higher accuracy because of richer bilingual constraint experiment on the translated portion of the chinese treebank show that our system outperforms monolingual parser by point for chinese and point for english 
when multiple conversation occur simultaneously a listener must decide which conversation each utterance is part of in order to interpret and respond to it appropriately we refer to this task a disentanglement we present a corpus of internet relay chat irc dialogue in which the various conversation have been manually disentangled and evaluate annotator reliability this is to our knowledge the first such corpus for internet chat we propose a graph theoretic model for disentanglement using discourse based feature which have not been previously applied to this task the model s predicted disentanglement are highly correlated with manual annotation 
we propose a global algorithm for learning entailment relation between predicate we define a graph structure over predicate that represents entailment relation a directed edge and use a global transitivity constraint on the graph to learn the optimal set of edge by formulating the optimization problem a an integer linear program we motivate this graph with an application that provides a hierarchical summary for a set of proposition that focus on a target concept and show that our global algorithm improves performance by more than over baseline algorithm 
in this paper we describe an approach based on off the shelf parser and semantic resource for the recognizing textual entailment rte challenge that can be generally applied to any domain syntax is exploited by mean of tree kernel whereas lexical semantics is derived from heterogeneous resource e g wordnet or distributional semantics through wikipedia the joint syntactic semantic model is realized by mean of tree kernel which can exploit lexical relatedness to match syntactically similar structure i e whose lexical compound are related the comparative experiment across different rte challenge and traditional system show that our approach consistently and meaningfully achieves high accuracy without requiring any adaptation or tuning 
an increasingly popular method for finding information online is via the community question answering cqa portal such a yahoo answer naver and baidu know searching the cqa archive and ranking filtering and evaluating the submitted answer requires intelligent processing of the question and answer posed by the user one important task is automatically detecting the question s subjectivity orientation namely whether a user is searching for subjective or objective information unfortunately real user question are often vague ill posed poorly stated furthermore there ha been little labeled training data available for real user question to address these problem we present cocqa a co training system that exploit the association between the question and contributed answer for question analysis task the co training approach allows cocqa to use the effectively unlimited amount of unlabeled data readily available in cqa archive in this paper we study the effectiveness of cocqa for the question subjectivity classification task by experimenting over thousand of real user question 
for million of people in le resourced region of the world text message sm provide the only regular contact with their doctor classifying message by medical label support rapid response to emergency the early identification of epidemic and everyday administration but challenge include textbrevity rich morphology phonological variation and limited training data we present a novel system that address these working with a clinic in rural malawi and text in the chichewa language we show that modeling morphological and phonological variation lead to a substantial average gain of f and an error reduction of up to for specific label relative to a baseline system optimized over word sequence by comparison there is no significant gain when applying the same system to the english translation of the same text label emphasizing the need for subword modeling in many language language independent morphological model perform a accurately a language specific model indicating a broad deployment potential 
automatic error detection is desired in the post processing to improve machine translation quality the previous work is largely based on confidence estimation using system based feature such a word posterior probability calculated from n best list or word lattice we propose to incorporate two group of linguistic feature which convey information from outside machine translation system into error detection lexical and syntactic feature we use a maximum entropy classifier to predict translation error by integrating word posterior probability feature and linguistic feature the experimental result show that linguistic feature alone outperform word posterior probability based confidence estimation in error detection and linguistic feature can further provide complementary information when combined with word confidence score which collectively reduce the classification error rate by and improve the f measure by 
this paper investigates two strategy for improving coreference resolution training separate model that specialize in particular type of mention e g pronoun versus proper noun and using a ranking loss function rather than a classification function in addition to being conceptually simple these modification of the standard single model classification based approach also deliver significant performance improvement specifically we show that on the ace corpus both strategy produce f score gain of more than across the three coreference evaluation metric muc b and ceaf 
in this paper we provide descriptive and empirical approach to effectively extracting underlying dependency among parsing error in the descriptive approach we define some combination of error pattern and extract them from given error in the empirical approach on the other hand we re parse a sentence with a target error corrected and observe error corrected together experiment on an hpsg parser show that each of these approach can clarify the dependency among individual error from each point of view moreover the comparison between the result of the two approach show that combining these approach can achieve a more detailed error analysis 
manual evaluation of translation quality is generally thought to be excessively time consuming and expensive we explore a fast and inexpensive way of doing it using amazon s mechanical turk to pay small sum to a large number of non expert annotator for we redundantly recreate judgment from a wmt translation task we find that when combined non expert judgment have a high level of agreement with the existing gold standard judgment of machine translation quality and correlate more strongly with expert judgment than bleu doe we go on to show that mechanical turk can be used to calculate human mediated translation edit rate hter to conduct reading comprehension experiment with machine translation and to create high quality reference translation 
we address the problem of translating from morphologically poor to morphologically rich language by adding per word linguistic information to the source language we use the syntax of the source sentence to extract information for noun case and verb person and annotate the corresponding word accordingly in experiment we show improved performance for translating from english into greek and czech for english greek we reduce the error on the verb conjugation from to and noun case agreement from to 
most web based q a system work by finding page that contain an explicit answer to a question these system are helpless if the answer ha to be inferred from multiple sentence possibly on different page to solve this problem we introduce the holmes system which utilizes textual inference ti over tuples extracted from text whereas previous work on ti e g the literature on textual entailment ha been applied to paragraph sized text holmes utilizes knowledge based model construction to scale ti to a corpus of million web page given only a few minute holmes double recall for example query in three disparate domain geography business and nutrition importantly holmes s runtime is linear in the size of it input corpus due to a surprising property of many textual relation in the web corpus they are approximately functional in a well defined sense 
part of speech po tag distribution are known to exhibit sparsity a word is likely to take a single predominant tag in a corpus recent research ha demonstrated that incorporating this sparsity constraint improves tagging accuracy however in existing system this expansion come with a steep increase in model complexity this paper proposes a simple and effective tagging method that directly model tag sparsity and other distributional property of valid po tag assignment in addition this formulation result in a dramatic reduction in the number of model parameter thereby enabling unusually rapid training our experiment consistently demonstrate that this model architecture yield substantial performance gain over more complex tagging counterpart on several language we report performance exceeding that of more complex state of the art system 
some phrase can be interpreted either idiomatically figuratively or literally in context and the precise identification of idiom is indispensable for full fledged natural language processing nlp to this end we have constructed an idiom corpus for japanese this paper report on the corpus and the result of an idiom identification experiment using the corpus the corpus target ambiguous idiom and consists of sentence each of which is annotated with a literal idiom label for idiom identification we targeted out of the idiom and adopted a word sense disambiguation wsd method using both common wsd feature and idiom specific feature the corpus and the experiment are the largest of their kind a far a we know a a result we found that a standard supervised wsd method work well for the idiom identification and achieved an accuracy of and with without idiom specific feature and that the most effective idiom specific feature is the one involving the adjacency of idiom constituent 
we present a novel framework for the discovery and representation of general semantic relationship that hold between lexical item we propose that each such relationship can be identified with a cluster of pattern that capture this relationship we give a fully unsupervised algorithm for pattern cluster discovery which search cluster and merges highfrequency word based pattern around randomly selected hook word pattern cluster can be used to extract instance of the corresponding relationship to ass the quality of discovered relationship we use the pattern cluster to automatically generate sat analogy question we also compare to a set of known relationship achieving very good result in both method the evaluation done in both english and russian substantiates the premise that our pattern cluster indeed reflect relationship perceived by human 
in this paper we first compare several strategy to handle the newly proposed three way recognizing textual entailment rte task then we define a new measurement for a pair of text called textual relatedness which is a weaker concept than semantic similarity or paraphrase we show that an alignment model based on the predicate argument structure using this measurement can help an rte system to recognize the unknown case at the first stage and contribute to the improvement of the overall performance in the rte task in addition several heterogeneous lexical resource are tested and different contribution from them are observed 
set of lexical item sharing a significant aspect of their meaning concept are fundamental in linguistics and nlp manual concept compilation is labor intensive error prone and subjective we present a web based concept extension algorithm given a set of term specifying a concept in some language we translate them to a wide range of intermediate language disambiguate the translation using web count and discover additional concept term using symmetric pattern we then translate the discovered term back into the original language score them and extend the original concept by adding back translation having high score we evaluate our method in source language and intermediate language using both human judgment and wordnet in all case our cross lingual algorithm significantly improves high quality concept extension 
we present a shared logistic normal distribution a a bayesian prior over probabilistic grammar weight this approach generalizes the similar use of logistic normal distribution enabling soft parameter tying during inference across dierent multinomial comprising the probabilistic grammar we show that this model outperforms previous approach on an unsupervised dependency grammar induction task 
syntax based translation model should in principle be efficient with polynomially sized search space but in practice they are often embarassingly slow partly due to the cost of language model integration in this paper we borrow from phrase based decoding the idea to generate a translation incrementally left to right and show that for tree to string model with a clever encoding of derivation history this method run in average case polynomial time in theory and linear time with beam search in practice whereas phrase based decoding is exponential time in theory and quadratic time in practice experiment show that with comparable translation quality our tree to string system in python can run more than time faster than the phrase based system moses in c 
the problem of automatically classifying the gender of a blog author ha important application in many commercial domain existing system mainly use feature such a word word class and po part of speech n gram for classification learning in this paper we propose two new technique to improve the current result the first technique introduces a new class of feature which are variable length po sequence pattern mined from the training data using a sequence pattern mining algorithm the second technique is a new feature selection method which is based on an ensemble of several feature selection criterion and approach empirical evaluation using a real life blog data set show that these two technique improve the classification accuracy of the current state of the art method significantly 
we propose a new graph based semi supervised learning ssl algorithm and demonstrate it application to document categorization each document is represented by a vertex within a weighted undirected graph and our proposed framework minimizes the weighted kullback leibler divergence between distribution that encode the class membership probability of each vertex the proposed objective is convex with guaranteed convergence using an alternating minimization procedure further it generalizes in a straightforward manner to multi class problem we present result on two standard task namely reuters and webkb showing that the proposed algorithm significantly outperforms the state of the art 
the recent availability of large corpus for training n gram language model ha shown the utility of model of higher order than just trigram in this paper we investigate method to control the increase in model size resulting from applying standard method at higher order we introduce significance based n gram selection which not only reduces model size but also improves perplexity for several smoothing method including katz backoff and absolute discounting we also show that when combined with a new smoothing method and a novel variant of weighted difference pruning our selection method performs better in the trade off between model size and perplexity than the best pruning method we found for modified kneser ney smoothing 
parse tree path are commonly used to incorporate information from syntactic par into nlp system these system typically treat the pathsas atomic or nearly atomic feature these feature are quite sparse due to the immense variety of syntactic expression in this paper we propose a general method for learning how to iteratively simplify a sentence thus decomposing complicated syntax into small easy to process piece our method applies a series of hand written transformation rule corresponding to basic syntactic pattern for example one rule depassivizes a sentence the model is parameterized by learned weight specifying preference for some rule over others after applying all possible transformation to a sentence we are left with a set of candidate simplified sentence we apply our simplification system to semantic role labeling srl a we do not have labeled example of correct simplification we use labeled training data for the srl task to jointly learn both the weight of the simplification model and of an srl model treating the simplification a a hidden variable by extracting and labeling simplified sentence this combined simplification srl system better generalizes across syntactic variation it achieves a statistically significant f measure increase over a strong baseline on the conll srl task attaining near state of the art performance 
complex question that require inferencing and synthesizing information from multiple document can be seen a a kind of topic oriented informative multi document summarization in this paper we have experimented with one empirical and two unsupervised statistical machine learning technique k mean and expectation maximization em for computing relative importance of the sentence however the performance of these approach depends entirely on the feature set used and the weighting of these feature we extracted different kind of feature i e lexical lexical semantic cosine similarity basic element tree kernel based syntactic and shallow semantic for each of the document sentence in order to measure it importance and relevancy to the user query we used a local search technique to learn the weight of the feature for all our method of generating summary we have shown the effect of syntactic and shallow semantic feature over the bag of word bow feature 
most supervised language processing system show a significant drop off in performance when they are tested on text that come from a domain significantly different from the domain of the training data semantic role labeling technique are typically trained on newswire text and in test their performance on fiction is a much a worse than their performance on newswire text we investigate technique for building open domain semantic role labeling system that approach the ideal of a train once use anywhere system we leverage recently developed technique for learning representation of text using latent variable language model and extend these technique to one that provide the kind of feature that are useful for semantic role labeling in experiment our novel system reduces error by relative to the previous state of the art on out of domain text 
we connect two scenario in structured learning adapting a parser trained on one corpus to another annotation style and projecting syntactic annotation from one language to another we propose quasi synchronous grammar qg feature for these structured learning task that is we score a aligned pair of source and target tree based on local feature of the tree and the alignment our quasi synchronous model assigns positive probability to any alignment of any tree in contrast to a synchronous grammar which would insist on some form of structural parallelism in monolingual dependency parser adaptation we achieve high accuracy in translating among multiple annotation style for the same sentence on the more difficult problem of cross lingual parser projection we learn a dependency parser for a target language by using bilingual text an english parser and automatic word alignment our experiment show that unsupervised qg projection improves on par trained using only high precision projected annotation and far outperforms by more than absolute dependency accuracy learning an unsupervised parser from raw target language text alone when a few target language parse tree are available projection give a boost equivalent to doubling the number of target language tree 
named entity recognition ner is an important task required in a wide variety of application while rule based system are appealing due to their well known explainability most if not all state of the art result for ner task are based on machine learning technique motivated by these result we explore the following natural question in this paper are rule based system still a viable approach to named entity recognition specifically we have designed and implemented a high level language nerl on top of systemt a general purpose algebraic information extraction system nerl is tuned to the need of ner task and simplifies the process of building understanding and customizing complex rule based named entity annotator we show that these customized annotator match or outperform the best published result achieved with machine learning technique these result confirm that we can reap the benefit of rule based extractor explainability without sacrificing accuracy we conclude by discussing lesson learned while building and customizing complex rule based annotator and outlining several research direction towards facilitating rule development 
