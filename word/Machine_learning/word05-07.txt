recent research ha studied the role of sparsity in high dimensional regression and signal reconstruction establishing theoretical limit f or recovering sparse model from sparse data this line of work show that regularized least square regression can accurately estimate a sparse linear model from n noisy example in p dimension even if p is much larger than n in this paper we study a variant of this problem where the original n input variable are compressed by a random linear transformation to m n example in p dimension and establish condition under which a sparse linear model can be successfully recovered from the compressed data a primary motivation for this compression procedure is to anonymize the data and preserve privacy by revealing little information about the original data we characte rize the number of random projection that are required for regularized compressed regression to identify the nonzero coefficient in the true model with probability appr oaching one a property called sparsistence in addition we show that regularized compressed regression asymptotically predicts a well a an oracle linear model a property called persistence finally we characterize the privacy property of the compression procedure in information theoretic term establishing upper bound on the mutual information between the compressed and uncompressed data that decay to zero 
we propose consensus propagation an asynchronous distributed protocol for averaging number across a network we establish convergence characterize the convergence rate for regular graph and demonstrate that the protocol exhibit better scaling property than pairwise averaging an alternative that ha received much recent attention consensus propagation can be viewed a a special case of belief propagation and our result contribute to the belief propagation literature in particular beyond singly connected graph there are very few class of relevant problem for which belief propagation is known to converge 
many classification task such a spam filtering intrusion detection and terrorism detection are complicated by an adversary who wish to avoid detection previous work on adversarial classification ha made the unrealistic assumption that the attacker ha perfect knowledge of the classifier in this paper we introduce the adversarial classifier reverse engineering acre learning problem the task of learning sufficient information about a classifier to construct adversarial attack we present efficient algorithm for reverse engineering linear classifier with either continuous or boolean feature and demonstrate their effectiveness using real data from the domain of spam filtering 
a cost sensitive extension of boosting denoted a asymmetric boosting is presented unlike previous proposal the new algorithm is derived from sound decision theoretic principle which exploit the statistical interpretation of boosting to determine a principled extension of the boosting loss similarly to adaboost the cost sensitive extension minimizes this loss by gradient descent on the functional space of convex combination of weak learner and produce large margin detector it is shown that asymmetric boosting is fully compatible with adaboost in the sense that it becomes the latter when error are weighted equally experimental evidence is provided to demonstrate the claim of cost sensitivity and large margin the algorithm is also applied to the computer vision problem of face detection where it is shown to outperform a number of previous heuristic proposal for cost sensitive boosting adacost csb csb csb asymmetricadaboost adac adac and adac 
in this age of globalization organization need to publish their micro data owing to legal directive or share it with business associate in order to remain competitive this put personal privacy at risk to surmount this risk attribute that clearly identify individual such a name social security number driving license number are generally removed or replaced by random value but this may not be enough because such de identified database can sometimes be joined with other public database on attribute such a gender date of birth and zipcode to re identify individual who were supposed to remain anonymous inliterature suchanidentity leakingattributecombinationiscalled a a quasi identifier it is always critical to be able to recognize quasi identifier and to apply to them appropriate protective measure to mitigate the identity disclosure risk posed by join attack in this paper we start out by providing the first formal characterization and a practical technique to identify quasi identifier we show an interesting connection between whether a set of column form a quasi identifier and the number of distinct value assumed by the combination of the column we then use this characterization to come up with a probabilistic notion of anonymity again we show an interesting connection between the number of distinct value taken by a combination of column and the anonymity it can offer this allows u to find an ideal amount of generalization or suppression to apply to different column in order to achieve probabilistic anonymity we work through many example and show that our analysis can be used to make a published database conform to privacy act like hipaa in order to achieve the probabilistic anonymity we observe that one need to solve multiple dimensional k anonymity problem we propose many efficient and scalable algorithm for achieving dimensional anonymity supported in part by nsf grant itr this work wa also supported in part by trust the team for research in ubiquitous secure technology which receives support from the national science foundation nsf award number ccf and the following organization cisco escher hp ibm intel microsoft ornl qualcomm pirelli sun and symantec this work wa initiated when the second author wa a summer intern at tata research development and design center pune india our algorithm are optimal in a sense that they minimally distort data and retain much of it utility 
often the best performing supervised learning model are ensemble of hundred or thousand of base level classiers unfortunately the space required to store this many classiers and the time required to execute them at run time prohibits their use in application where test set are large e g google where storage space is at a premium e g pda and where computational power is limited e g hearing aid we present a method for compressing large complex ensemble into smaller faster model usually without signican t loss in performance 
we consider the problem of denoising a noisily sampled submanifold m in rd where the submanifoldm is a priori unknown and we are only given a noisy point sample the presented denoising algorithm is based on a graph based diffusion process of the point sample we analyze this diffusion process using recent result about the convergence of graph laplacians in the experiment we show that our method is capable of dealing with non trivial high dimensional noise moreover using the denoising algorithm a pre processing method we can improve the result of a semi supervised learning algorithm 
we introduce the boasting problem wherein useful trend in historical ordinal data ranking are discovered claim of the form our object wa ranked r or better in x of the last t time unit are formalized and maximal claim boast of this form are defined under two natural partial order for the first partial order we give an efficient and optimal algorithm for finding all such maximal claim for the second we apply a classical result from computational geometry to achieve an algorithm whose running time is significantly more efficient than that of a na ve one finally we connect this boasting problem to a novel variation of the problem of finding optimized confidence association rule a originally posed by fukuda et al and give an efficient algorithm for solving a simplification of the new problem 
this talk is about the next frontier in knowledge discovery and data mining 
hierarchical penalization is a generic framework for incorporating prior information inthefittingofstatisticalmodels whentheexplicativevariablesareorganized in a hierarchical structure the penalizer is a convex functional that performs soft selection at the group level and shrink variable within each group this favor solution with few leading term in the final combination the framework originally derived for taking prior knowledge into account is shown to be useful in linear regression when several parameter are used to model the influence of one feature or in kernel regression for learning multiple kernel 
we generalize the winnow algorithm for learning disjunction to learning subspace of low rank subspace are represented by symmetric projection matrix the online algorithm maintains it uncertainty about the hidden low rank projection matrix a a symmetric positive definite matrix this matrix is updated using a version of the matrix exponentiated gradient algorithm that is based on matrix exponential and matrix logarithm a in the case of the winnow algorithm the bound are logarithmic in the dimension n of the problem but linear in the rank r of the hidden subspace we show that the algorithm can be adapted to handle arbitrary matrix of any dimension via a reduction 
independent component analysis ica is a popular method for extracting independent feature from visual data however a a fundamentally linear technique there is always nonlinear residual redundancy that is not captured by ica hence there have been many attempt to try to create a hierarchical version of ica but so far none of the approach have a natural way to apply them more than once here we show that there is a relatively simple technique that transforms the absolute value of the output of a previous application of ica into a normal distribution to which ica maybe applied again this result in a recursive ica algorithm that may be applied any number of time in order to extract higher order structure from previous layer 
this paper explores several kernel in the context of text classification a novel view of how document might have been created is introduced and kernel are derived from this framework the relation between these kernel a well a to the gaussian kernel are discussed moreover the popular tf idf weighting scheme will be derived a a natural consequence finally the kernel have been evaluated on the reuters corpus volume i newswire database to ass their quality in a topic classification application 
bayesian reinforcement learning ha generated substantial interest recently a it provides an elegant solution to the exploration exploitation trade off in reinforcement learning however most investigation of bayesian reinforcement learning to date focus on the standard markov decision process mdps our goal is to extend these idea to the more general partially observable mdp pomdp framework where the state is a hidden variable to address this problem we introduce a new mathematical model the bayes adaptive pomdp this new model allows u to improve knowledge of the pomdp domain through interaction with the environment and plan optimal sequence of action which can tradeoff between improving the model identifying the state and gathering reward we show how the model can be fi nitely approximated while preserving the value function we describe approximation for belief tracking and planning in this model empirical result on two domain show that the model estimate and agent s return improve over time a the agent learns better model estimate in many real world system uncertainty can arise in both the prediction of the system s behavior and the observability of the system s state partially observable markov decision process pomdps take both kind of uncertainty into account and provide a powerful model for sequential decision making under these condition however most solving method for pomdps assume that the model is known a priori which is rarely the case in practice for instance in robotics the pomdp must refl ect exactly the uncertainty on the robot s sensor and actuator these parameter are rarely known exactly and therefore must often be approximated by a human designer such that even if this approximate pomdp could be solved exactly the resulting policy may not be optimal thus we seek a decision theoretic planner which can take into account the uncertainty over model parameter during the planning process a well a being able to learn from experience the value of these unknown parameter bayesian reinforcement learning ha investigated this problem in the context of fully observable mdps an extension to pomdp ha recently been proposed yet this method relies on heuristic to select action that will improve the model thus forgoing any theoretical guarantee on the quality of the approximation and on an oracle that can be queried to provide the current state in this paper we draw inspiration from the bayes adaptive mdp framework which is formulated to provide an optimal solution to the exploration exploitation trade off to extend these idea to pomdps we face two challenge how to update dirichlet parameter when the state is a hidden variable how to approximate the infi nite dimensional belief space to perform belief monitoring and compute the optimal policy this paper tackle both problem jointly the fi rst problem is solved by including the dirichlet parameter in the state space and maintaining belief state over these parameter we address the second by bounding the space of dirichlet parameter to a fi nite subspace necessary for optimal solution 
we propose a bayesian undirected graphical model for co training or more generally for semi supervised multi view learning this make explicit the previously unstated assumption of a large class of co training type algorithm and also clarifies the circumstance under which these assumption fail building upon new insight from this model we propose an improved method for co training which is a novel co training kernel for gaussian process classifier the resulting approach is convex and avoids local maximum problem unlike some previous multi view learning method furthermore it can automatically estimate how much each view should be trusted and thus accommodate noisy or unreliable view experiment on toy data and real world data set illustrate the benefit of this approach 
we develop a bayesian sum of tree model named bart where each tree is constrained by a prior to be a weak learner fitting and inference are accomplished via an iterative backfitting mcmc algorithm this model is motivated by ensemble method in general and boosting algorithm in particular like boosting each weak learner i e each weak tree contributes a small amount to the overall model however our procedure is defined by a statistical model a prior and a likelihood while boosting is defined by an algorithm this model based approach enables a full and accurate assessment of uncertainty in model prediction while remaining highly competitive in term of predictive accuracy 
graph matching is a fundamental problem in computer vision and machine learning we present two contribution first we give a new spectral relaxation technique for approximate solution to matching problem that naturally incorporates one to one or one to many constraint within the relaxation scheme the second is a normalization procedure for existing graph matching scoring function that can dramatically improve the matching accuracy it is based on a reinterpretation of the graph matching compatibility matrix a a bipartite graph on edge for which we seek a bistochastic normalization we evaluate our two contribution on a comprehensive test set of random graph matching problem a well a on image correspondence problem our normalization procedure can be used to improve the performance of many existing graph matching algorithm including spectral matching graduated assignment and semidefinite programmi ng because of it combinatorial nature graph matching is either solved exactly in a very restricted setting bipartite matching for example with the hungarian method or approximately most of the recent literature on graph matching ha followed this second path developing approximate relaxation to the graph matching problem in this paper we make two contribution the first contribution is a spectral relaxation for the graph matching problem that incorporates one to one or one to many mapping constraint represented a affine constraint a n ew mathematical tool is developed for that respect affinely constrained rayleigh quotient our meth od achieves comparable performance to state of the art algorithm while offering much better scal ability our second contribution relates to the graph matching scoring function itself which we argue is prone to systematic confusion error we show how a proper bistochastic normalization of the graph matching compatibility matrix is able to considerably reduce those error and improve the overall matching performance this improvement is demonstrated both for our spectral relaxation algorithm and for three state of the art graph matching algorithm spectral matching graduated assignment and semidefinite programming 
we present a novel algorithm for agglomerative hierarchical clustering based on evaluating marginal likelihood of a probabilistic model this algorithm ha several advantage over traditional distance based agglomerative clustering algorithm it defines a probabilistic model of the data which can be used to compute the predictive distribution of a test point and the probability of it belonging to any of the existing cluster in the tree it us a model based criterion to decide on merging cluster rather than an ad hoc distance metric bayesian hypothesis testing is used to decide which merges are advantageous and to output the recommended depth of the tree the algorithm can be interpreted a a novel fast bottom up approximate inference method for a dirichlet process i e countably infinite mixture model dpm it provides a new lower bound on the marginal likelihood of a dpm by summing over exponentially many clustering of the data in polynomial time we describe procedure for learning the model hyperpa rameters computing the predictive distribution and extension to the algorithm experimental result on synthetic and real world data set demonstrate useful property of the algorithm 
it is known that determinining whether a dec pomdp namely a cooperative partially observable stochastic game posg ha a cooperative strategy with positive expected reward is complete for nexp it wa not known until now how cooperation affected that complexity we show that for competitive posgs the complexity of determining whether one team ha a positive expected reward strategy is complete for nexpnp 
long distance language modeling is important not only in speech recognition and machine translation but also in high dimensional discrete sequence modeling in general however the problem of context length ha almost been neglected so far and a na ive bag of word history ha been employed in natural language processing in contrast in this paper we view topic shift within a text a a latent stochastic process to give an explicit probabilistic generative model that ha partial exchangeability we propose an online inference algorithm using particle filter to recognize topic shift to employ the most appropriate length of context automatically experiment on the bnc corpus showed consistent improvement over previous method involving no chronological order 
the risk or probability of error of the classifier produced by the adaboost algorithm is investigated in particular we consider the stopping strategy to be used in adaboost to achieve universal consistency we show that provided adaboost is stopped after n iteration for sample size n and the sequence of risk of the classifier it produce approach the bayes risk 
the learning of probabilistic model with many hidden variable and nondecomposable dependency is an important and challenging problem in contrast to traditional approach based on approximate inference in a single intractable model our approach is to train a set of tractable submodels by encouraging them to agree on the hidden variable this allows u to capture non decomposable aspect of the data while still maintaining tractability we propose an objective function for our approach derive em style algorithm for parameter estimation and demonstrate their effectiveness on three challenging real world learning task 
we state and analyze the rst active learning algorithm that nd an optimal hypothesis in any hypothesis class when the underlying distribution ha arbitrary form of noise the algorithm a for agnostic active relies only upon the assumption that it ha access to a stream of unlabeled example drawn i i d from a xed distribution we show that a achieves an exponential improvement i e requires only o ln sample to nd an optimal classier over the usual sample complexity of supervised learning for several setting considered before in the realizable case these include learning threshold classiers and learning homogeneous linear separator with respect to an input distribution which is uniform over the unit sphere 
we formulate a new data mining problem called storytelling a a generalization of redescription mining in traditional redescription mining we are given a set of object and a collection of subset defined over these object the goal is to view the set system a a vocabulary and identify two expression in this vocabulary that induce the same set of object storytelling on the other hand aim to explicitly relate object set that are disjoint and hence maximally dissimilar by finding a chain of approximate redescriptions between the set this problem find application in bioinformatics for instance where the biologist is trying to relate a set of gene expressed in one experiment to another set implicated in a dierent pathway we outline an ecient storytelling implementation that embeds the cartwheel redescription mining algorithm in an a search procedure using the former to supply next move operator on search branch to the latter this approach is practical and eective for mining large datasets and at the same time exploit the structure of partition imposed by the given vocabulary three application case study are presented a study of word overlap in large english dictionary exploring connection between genesets in a bioinformatics dataset and relating publication in the pubmed index of abstract 
we address the issue of the learnability of concept class under three classification noise model in the probably approximately correct framework after introducing the class conditional classification noise cccn model we investigate the problem of the learnability of concept class under this particular setting and we show that concept class that are learnable under the well known uniform classification noise cn setting are also cccn learnable which give cn cccn we then use this result to prove the equality between the set of concept class that are cn learnable and the set of concept class that are learnable in the constant partition classification noise cpcn setting or in other word we show that cn cpcn 
dimensionality reduction is the problem of finding a low dimensional representation of highdimensional input data this paper examines the case where additional information is known about the data in particular we assume the data are given in a sequence with action label associated with adjacent data point such a might come from a mobile robot the goal is a variation on dimensionality reduction where the output should be a representation of the input data that is both low dimensional and respect the action i e action correspond to simple transformation in the output representation we show how this variation can be solved with a semidefinite program we evaluate the technique in a synthetic robot inspired domain demonstrating qualitatively superior representation and quantitative improvement on a data prediction task 
an organization make a new release a new information become available release a tailored view for each data request release sensitive information and identifying information separately the availability of related release sharpens the identification of indi viduals by a global quasi identifier consisting of attribute from re lated release since it is not an option to anonymize previously released data the current release must be anonymized to ensure that a global quasi identifier is not effective for identification in this paper we study the sequential anonymization problem under this assumption a key question is how to anonymize the current release so that it cannot be linked to previous release yet remains useful for it own release purpose we introduce the lossy join a negative property in relational database design a a way to hide the join relationship among release and propose a scalable and practical solution 
partition of sequential data exist either per se or a a result of sequence segmentation algorithm it is often the case that the same timeline is partitioned in many different way for example different segmentation algorithm produce different partition of the same underlying data point in such case we are interested in producing an aggregate partition i e a segmentation that agrees a much a possible with the input segmentation each partition is defined a a set of continuous non overlapping segment of the timeline we show that this problem can be solved optimally in polynomial time using dynamic programming we also propose faster greedy heuristic that work well in practice we experiment with our algorithm and we demonstrate their utility in clustering the behavior of mobile phone user and combining the result of different segmentation algorithm on genomic sequence 
common strategy to liberate an organization s information asset for situational awareness frequently rely on infrastructure component such a data integration enterprise search federation data warehousing and so on and while these traditional platform enable analyst to get better and faster answer to their query the next big advance will change this paradigm user cannot be expected to formulate and ask every smart question every day and to escape this impractical and un scalable model the new paradigm will involve technology where the data find the data and relevance find the user perpetual analytics describes a class of application whereby enterprise context is assembled in real time on data stream a fast a operational system record observation context construction is a data find the data activity which enables event of interest to be streamed to subscriber in this talk i will talk at some depth about the dynamic of such system including scalability and sustainability 
we propose a simple information theoretic approach to soft clustering based on maximizing the mutual information i x y between the unknown cluster label y and the training pattern x with respect to parameter of specifically constrained encoding distribution the constraint are chosen such that pattern are likely to be clustered similarly if they lie close to specific unknown vector in the feature space the method may be conveniently applied to learning the optimal affinity matrix which corresponds to learning parameter of the kernelized encoder the procedure doe not require computation of eigenvalue of the gram matrix which make it potentially attractive for clustering large data set 
we describe a family of embedding algorithm that are based on nonparametric estimate of mutual information mi using parzen window estimate of the distribution in the joint input embedding space we derive a mi based objective function for dimensionality reduction that can be optimized directly with respect to a set of latent data representative various type of supervision signal can be introduced within the framework by replacing plain mi with several form of conditional mi example of the semi un supervised algorithm that we obtain this way are a new model for manifold alignment and a new type of embedding method that performs conditional dimensionality reduction 
in bioinformatics it is often desirable to combine data from various measurement source and thus structured feature vector are to be analyzed that posse different intrinsic blocking characteristic e g different patt ern of missing value observation noise level effective intrinsic dimensionalitie s we propose a new machine learning tool heterogeneous component analysis hca for feature extraction in order to better understand the factor that underlie such complex structured heterogeneous data hca is a linear block wise sparse bayesian pca based not only on a probabilistic model with block wise residual variance term but also on a bayesian treatment of a block wise sparse factor loading matrix we study various algorithm that implement our hca concept extracting sparse heterogeneous structure by obtaining common component for the block and specific component within each block simulation on toy and bioinformatics data underline the usefulness of the proposed structured matrix factorizatio n concept 
we introduce hidden process model hpms a class of probabilistic model for multivariate time series data the design of hpms ha been motivated by the challenge of modeling hidden cognitive process in the brain given functional magnetic resonance imaging fmri data fmri data is sparse high dimensional non markovian and often involves prior knowledge of the form hidden event a occurs n time within the interval t t hpms provide a generalization of the widely used general linear model approach to fmri analysis and hpms can also be viewed a a subclass of dynamic bayes network 
clustering is one of the most widely used statistical tool for data analysis among all existing clustering technique k mean is a very popular method because of it ease of programming and because it accomplishes a good trade o between achieved performance and computational complexity however kmeans is prone to local minimum problem and it doe not scale well with high dimensional data set a common approach to dealing with high dimensional data is to cluster in the space spanned by the principal component pc in this paper we show the benefit of clustering in a low dimensional discriminative space rather than in the pc space generative in particular we propose a new clustering algorithm called discriminative cluster analysis dca dca jointly performs dimensionality reduction and clustering several toy and real example show the benefit of dca versus traditional pca k mean clustering additionally a new matrix formulation is suggested and connection with related technique such a spectral graph method and linear discriminant analysis are provided 
topic model such a latent dirichlet allocation lda have been an effective tool for the statistical analysis of document collection and other discrete data the lda model assumes that the word of each document arise from a mixture of topic each of which is a distribution over the vocabulary a limitation of lda is the inability to model topic correlation even though for example a document about sport is more likely to also be about health than international finance this limitation stem from the use of the dirichlet distribution to model the variability among the topic proportion in this paper we develop the correlated topic model ctm where the topic proportion exhibit correlation via the logistic normal distribution we derive a mean field variational inference algorithm for approximate posterior inference in this model which is complicated by the fact that the logistic normal is not conjugate to the multinomial the ctm give a better fit than lda on a collection of ocred article from the journal science furthermore the ctm provides a natural way of visualizing and exploring this and other unstructured data set 
characterising the difference between two database is an often occurring problem in data mining detection of change over time is a prime example comparing database from two branch is another one the key problem is to discover the pattern that describe the difference emerging pattern provide only a partial answer to this question in previous work we showed that the data distribution can be captured in a pattern based model using compression here we extend this approach to define a generic dissimilarity measure on database moreover we show that this approach can identify those pattern that characterise the difference between two distribution experimental result show that our method provides a wellfounded way to independently measure database dissimilarity that allows for thorough inspection of the actual difference this illustrates the use of our approach in real world data mining 
despite all the attention paid to variational method based on sum product message passing loopy belief propagation tree reweighted sum product these method are still bound to inference on a small set of probabilistic model mean field approximation have been applied to a broader set of problem but the solution are often poor we propose a new class of conditionally specified variational approximation based on mean field theory while not usable on their own combined with sequential monte carlo they produce guaranteed improvement over conventional mean field moreover experiment on a well studied problem inferring the stable configuration of the ising spin glass show that the solution can be significantly better than those obtained using sum product based method 
convexity ha recently received a lot of attention in the machine learning community and the lack of convexity ha been seen a a major disadvantage of many learning algorithm such a multi layer artificial neural network we show that training multi layer neural network in which the number of hidden unit is learned can be viewed a a convex optimization problem this problem involves an infinite number of variable but can be solved by incrementally inserting a hidden unit at a time each time finding a linear classifier that minimizes a weighted sum of error 
ordinal regression ha become an eective way of learning user preference but most research focus on single regression problem in this paper we introduce collaborative ordinal regression where multiple ordinal regression task are handled simultaneously rather than modeling each task individually we explore the dependency between ranking function through a hierarchical bayesian model and assign a common gaussian process gp prior to all individual function empirical study show that our collaborative model outperforms the individual counterpart in preference learning application 
clustering method can be either data driven or need driven data driven method intend to discover the true structure of the underlying data while need driven method aim at organizing the true structure to meet certain application requirement thus need driven e g constrained clustering is able to find more useful and actionable cluster in application such a energy aware sensor network privacy preservation and market segmentation however the existing method of constrained clustering require user to provide the number of cluster which is often unknown in advance but ha a crucial impact on the clustering result in this paper we argue that a more natural way to generate actionable cluster is to let the application specific constraint decide the number of cluster for this purpose we introduce a novel cluster model constraint driven clustering cdc which find an a priori unspecified number of compact cluster that satisfy all user provided constraint two general type of constraint are considered i e minimum significance constraint and minimum variance constraint a well a combination of these two type we prove the np hardness of the cdc problem with different constraint we propose a novel dynamic data structure the cd tree which organizes data point in leaf node such that each leaf node approximately satisfies the cdc constraint and minimizes the objective function based on cd tree we develop an efficient algorithm to solve the new clustering problem our experimental evaluation on synthetic and real datasets demonstrates the quality of the generated cluster and the scalability of the algorithm 
addressed in this paper is the issue of email data cleaning for text mining many text mining application need take email a input email data is usually noisy and thus it is necessary to clean it before mining several product offer email cleaning feature however the type of noise that can be eliminated are restricted despite the importance of the problem email cleaning ha received little attention in the research community a thorough and systematic investigation on the issue is thus needed in this paper email cleaning is formalized a a problem of non text filtering and text normalization in this way email cleaning becomes independent from any specific text mining processing a cascaded approach is proposed which clean up an email in four pass including non text filtering paragraph normalization sentence normalization and word normalization a far a we know non text filtering and paragraph normalization have not been investigated previously method for performing the task on the basis of support vector machine svm have also been proposed in this paper feature in the model have been defined experimental result indicate that the proposed svm based method can significantly outperform the baseline method for email cleaning the proposed method ha been applied to term extraction a typical text mining processing experimental result show that the accuracy of term extraction can be significantly improved by using the data cleaning method 
in many application association rule will only be interesting if they represent non trivial correlation between all constituent item numerous technique have been developed that seek to avoid false discovery however while all provide useful solution to aspect of this problem none provides a generic solution that is both flexible enough to accommodate varying definition of true and false discovery and powerful enough to provide strict control over the risk of false discovery this paper present generic technique that allow definition of true and false discovery to be specified in term of arbitrary statistical hypothesis test and which provide strict control over the experimentwise risk of false discovery category and subject descriptor h database management database application data mining 
a family of probabilistic time series model is developed to analyze the time evolution of topic in large document collection the approach is to use state space model on the natural parameter of the multinomial distribution that represent the topic variational approximation based on kalman filter and nonparametric wavelet regression are developed to carry out approximate posterior inference over the latent topic in addition to giving quantitative predictive model of a sequential corpus dynamic topic model provide a qualitative window into the content of a large document collection the model are demonstrated by analyzing the ocr ed archive of the journal science from through 
it is commonly assumed that high dimensional datasets contain point most of which are located in low dimensional manifold detection of low dimensional cluster is an extremely useful task for performing operation such a clustering and classification however it is a challenging computational problem in this paper we study the problem of finding subset of point with low intrinsic dimensionality our main contribution is to extend the definition of fractal correlation dimension which measure average volume growth rate in order to estimate the intrinsic dimensionality of the data in local neighborhood we provide a careful analysis of several key example in order to demonstrate the property of our measure based on our proposed measure we introduce a novel approach to discover cluster with low dimensionality the resulting algorithm extend previous density based measure which have been successfully used for clustering we demonstrate the effectiveness of our algorithm for discovering low dimensional m flat embedded in high dimensional space and for detecting low rank sub matrix 
we derive a robust euclidean embedding procedure based on semidefinite programming that may be used in place of the popular classical multidimensional scaling cmds algorithm we motivate this algorithm by arguing that cmds is not particularly robust and ha several other deficiency generalpurpose semidefinite programming solver are too memory intensive for medium to large sized application so we also describe a fast subgradient based implementation of the robust algorithm additionally since cmds is often used for dimensionality reduction we provide an in depth look at reducing dimensionality with embedding procedure in particular we show that it is np hard to find optimal low dimensional embeddings under a variety of cost function 
clustering or factoring of a document collection attempt to explain each observed document in term of one or a small number of inferred prototype prior work demonstrated that when link exist between document in the corpus a is the case with a collection of web page or scientific paper building a joint model of document content and connection produce a better model than that built from content or connection alone many problem arise when trying to apply these joint model to corpus at the scale of the world wide web however one of these is that the sheer overhead of representing a feature space on the order of billion of dimension becomes impractical we address this problem with a simple representational shift inspired by probabilistic relational model instead of representing document linkage in term of the identity of linking document we represent it by the explicit and inferred attribute of the linking document several surprising result come with this shift in addition to being computationally more tractable the new model produce factor that more cleanly decompose the document collection we discus several variation on this model and show how some can be seen a exact generalization of the pagerank algorithm 
principal component and canonical correlation are at the root of many exploratory data mining technique and provide standard pre processing tool in machine learning lately probabilistic reformulations of these method have been proposed roweis tipping bishop b bach jordan they are based on a gaussian density model and are therefore like their non probabilistic counterpart very sensitive to atypical observation in this paper we introduce robust probabilistic principal component analysis and robust probabilistic canonical correlation analysis both are based on a student t density model the resulting probabilistic reformulations are more suitable in practice a they handle outlier in a natural way we compute maximum likelihood estimate of the parameter by mean of the em algorithm 
for large scale classification problem the training sample can be clustered beforehand a a downsampling pre process and then only the obtained cluster are used for training motivated by such assumption we proposed a classification algorithm support cluster machine scm within the learning framework introduced by vapnik for the scm a compatible kernel is adopted such that a similarity measure can be handled not only between cluster in the training phase but also between a cluster and a vector in the testing phase we also proved that the scm is a general extension of the svm with the rbf kernel the experimental result confirm that the scm is very effective for largescale classification problem due to significantly reduced computational cost for both training and testing and comparable classification accuracy a a by product it provides a promising approach to dealing with privacy preserving data mining problem 
we propose statistical predicate invention a a key problem for statistical relational learning spi is the problem of discovering new concept property and relation in structured data and generalizes hidden variable discovery in statistical model and predicate invention in ilp we propose an initial model for spi based on second order markov logic in which predicate a well a argument can be variable and the domain of discourse is not fully known in advance our approach iteratively refines cluster of symbol based on the cluster of symbol they appear in atom with e g it cluster relation by the cluster of the object they relate since dierent clustering are better for predicting different subset of the atom we allow multiple cross cutting clustering we show that this approach outperforms markov logic structure learning and the recently introduced infinite relational model on a number of relational datasets explicitly representing the uncertainty in the discovered predicate these can in turn be used a a basis for discovering new predicate which is potentially much more powerful than learning based on a fixed set of simple primitive essentially all the concept used by human can be viewed a invented predicate with many level of discovery between them and the sensory percept they are ultimately based on in statistical learning this problem is known a hidden or latent variable discovery and in relational learning a predicate invention both hidden variable discovery and predicate invention are considered quite important in their respective community but are also very difficult with limited progress to date one might question the need for spi arguing that structure learning is sucient such a question can 
the score returned by support vector machine are often used a a confidence measure in the classification of new example however there is no theoretical argument sustaining this practice thus when classification uncertainty ha to be assessed it is safer to resort to classifier estimating conditional probability of class label here we focus on the ambiguity in the vicinity of the boundary decision we propose an adaptation of maximum likelihood estimation instantiated on logistic regression the model output proper conditional probability into a user defined interval and is le precise elsewhere the model is also sparse in the sense that few example contribute to the solution the computational eciency is thus improved compared to logistic regression furthermore preliminary experiment show improvement over standard logistic regression and performance similar to support vector machine 
we introduce supervised latent dirichlet allocation slda a statistical model of labelled document the model accommodates a variety of response type we derive a maximum likelihood procedure for parameter estimation which relies on variational approximation to handle intractable posterior expectation prediction problemsmotivatethisresearch weusethefittedmodeltopredictresponsevalues for new document we test slda on two real world problem movie rating predicted from review and web page popularity predicted from text description we illustrate the benefit of slda versus modern regularized regression a well a versus an unsupervised lda analysis followed by a separate regression 
we extend support vector machine to input space that are set by ensuring that the classifier is invariant to permutation of subelements within each input such permutation include reordering of scalar in an input vector re ordering of tuples in an input matrix or re ordering of general object in hilbert space within a set a well this approach induces permutational invariance in the classifier which can then be directly applied to unusual set based representation of data the permutation invariant support vector machine alternate the hungarian method for maximum weight matching within the maximum margin learning procedure we e ectiv ely estimate and apply permutation to the input data point to maximize classification margin while minimizing data radius this procedure ha a strong theoretical justification via well established error probability bound experiment are shown on character recognition d object recognition and various uci datasets 
many existing approach to collaborative filtering can nei ther handle very large datasets nor easily deal with user who have very few rating in this paper we present the probabilistic matrix factorization pmf model which scale linearly with the number of observation and more importantly performs well on the large sparse and very imbalanced netflix dataset we furth er extend the pmf model to include an adaptive prior on the model parameter and show how the model capacity can be controlled automatically finally we introduce a constrained version of the pmf model that is based on the assumption that user who have rated similar set of movie are likely to have similar preference the resulting model is able to generalize considerably better for user s with very few rating when the prediction of multiple pmf model are linearly combined with the prediction of restricted boltzmann machine model we achieve an error rate of that is nearly better than the score of netflix s own system 
estimation of distribution algorithm edas are a popular approach to learn a probability distribution over the good solution to a combinatorial optimization problem here we consider the case where there is a collection of such optimization problem with learned distribution and where each problem can be characterized by some vector of feature now we can dene a machine learning problem to predict the distribution of good solution q sjx for a new problem with feature x where s denotes a solution this predictive distribution is then used to focus the search we demonstrate the utility of our method on a compiler optimization task where the goal is to nd a sequence of code transformation to make the code run fastest result on a set of dieren t benchmark on two distinct architecture show that our approach consistently lead to signican t improvement in performance 
in several organization it ha become increasingly popular to document and log the step that makeup a typical business process in some situation a normative workflow model of such process is developed and it becomes important to know if such a model is actually being followed by analyzing the available activity log in other scenario no model is available and with the purpose of evaluating case or creating new production policy one is interested in learning a workflow representation of such activity in either case machine learning tool that can mine workflow model are of great interest and still relatively unexplored we present here a probabilistic workflow model and a corresponding learning algorithm that run in polynomial time we illustrate the algorithm on example data derived from a real world workflow 
by the term quantization we refer to the process of using quantum mechanic in order to improve a classical algorithm usually by making it go faster in this paper we initiate the idea of quantizing clustering algorithm by using variation on a celebrated quantum algorithm due to grover after having introduced this novel approach to unsupervised learning we illustrate it with a quantized version of three standard algorithm divisive clustering k median and an algorithm for the construction of a neighbourhood graph we obtain a significant speedup compared to the classical approach 
when the transition probability and reward of a markov decision process are specied exactly the problem can be solved without any interaction with the environment when no such specication is available the agent s only recourse is a long and potentially dangerous exploration we present a framework which allows the expert to specify imprecise knowledge of transition probability in term of stochastic dominance constraint our algorithm can be used to nd optimal policy for qualitatively specied problem or when no such solution is available to decrease the required amount of exploration the algorithm s behavior is demonstrated on simulation of two classic problem mountain car ascent and cart pole balancing 
we describe a nonnegative variant of the sparse pca problem the goal is to create a low dimensional representation from a collection of point which on the one hand maximizes the variance of the projected point and on the other us only part of the original coordinate and thereby creating a sparse representation what distinguishes our problem from other sparse pca formulation is that the projection involves only nonnegative weight of the original coordinate a desired quality in various field including economics bioinformatics and computer vision adding nonnegativity contributes to sparseness where it enforces a partitioning of the original coordinate among the new ax we describe a simple yet efficient iterative coordinate descent type of scheme which converges to a local optimum of our optimization criterion giving good result on large real world datasets 
the power and popularity of kernel method stem in part from their ability to handle diverse form of structured input including vector graph and string recently several method have been proposed for combining kernel from heterogeneous data source however all of these method produce stationary combination i e the relative weight of the various kernel do not vary among input example this article proposes a method for combining multiple kernel in a nonstationary fashion the approach us a large margin latent variable generative model within the maximum entropy discrimination med framework latent parameter estimation is rendered tractable by variational bound and an iterative optimization procedure the classifier we use is a log ratio of gaussian mixture in which each component is implicitly mapped via a mercer kernel function we show that the support vector machine is a special case of this model in this approach discriminative parameter estimation is feasible via a fast sequential minimal optimization algorithm empirical result are presented on synthetic data several benchmark and on a protein function annotation task 
we show that several important bayesian bound studied in machine learning both in the batch a well a the online setting arise by an application of a simple compression lemma in particular we derive i pac bayesian bound in the batch setting ii bayesian log loss bound and iii bayesian bounded loss bound in the online setting using the compression lemma although every setting ha different semantics for prior posterior and loss we show that the core bound argument is the same the paper simplifies our understanding of several important and apparently disparate result a well a brings to light a powerful tool for developing similar argument for other method 
this paper study boosting algorithm that make a single pas over a set of base classifier we first analyze a one pas algorithm in the setting of boosti ng with diverse base classifier our guarantee is the same a the best proved for a ny boosting algorithm but our one pas algorithm is much faster than previous approach we next exhibit a random source of example for which a picky variant of adaboost that skip poor base classifier can outperform the st andard adaboost algorithm which us every base classifier by an exponential factor experiment with reuters and synthetic data show that one pas boosting can substantially improve on the accuracy of naive bayes and that picky boosting can sometimes lead to a further improvement in accuracy 
we introduce a mixture of probabilistic canonical correlation analyzer model for analyzing local correlation or more generally mutual statistical dependency in cooccurring data pair the model extends the traditional canonical correlation analysis and it probabilistic interpretation in three main way first a full bayesian treatment enables analysis of small sample large p small n a crucial problem in bioinformatics for instance and rigorous estimation of the degree of dependency and independency secondly the mixture formulation generalizes the method from global linearity to the more reasonable assumption of dierent kind of dependency for dierent kind of data a a third novel extension the method decomposes the variation in the data into shared and data set specific component 
consider alice who is interacting with bob alice and bob have some shared secret which help alice identify bob impersonator now consider eve who know alice and bob but doe not know their shared secret eve would like to impersonate bob and fool alice without knowing the secret if eve is computationally unbounded how long doe she need to observe alice and bob interacting before she can successfully impersonate bob what is a good strategy for eve in this setting if eve run in polynomial time and if there exists a one way function then it is not hard to see that alice and bob may be safe from impersonator but is the existence of one way function an essential condition namely if one way function do not exist can an e cient eve always impersonate bob in this work we consider these natural question from the point of view of ever who is trying to observe bob and learn to impersonate him we formalize this setting in a new computational learning model of learning adaptively changing distribution acds which we believe capture a wide variety of natural learning task and is of interest from both cryptographic and computational learning point of view we present a learning algorithm that eve can use to successfully learn to impersonate bob in the information theoretic setting we also show that in the computational setting an e cient eve can learn to impersonate any e cient bob if and only if one way function do not exist 
this paper present a local learning projection llp approach for linear dimensionality reduction we first point out that the well known principal component analysis pca essentially seek the projection that ha the minimal global estimation error then we propose a dimensionality reduction algorithm that lead to the projection with the minimal local estimation error and elucidate it advantage for classification task we also indicate that llp keep the local information in the sense that the projection value of each point can be well estimated based on it neighbor and their projection value experimental result are provided to validate the eectiveness of the proposed algorithm 
the importance of dominance and skyline analysis ha been well recognized in multi criterion decision making application most previous study assume a xed order on the attribute in practice dieren t customer may have different preference on nominal attribute in this paper we identify an interesting data mining problem nding favorable facet which ha not been studied before given a set of point in a multidimensional space for a specic target point p we want to discover with respect to which combination of order e g customer preference on the nominal attribute p is not dominated by any other point such combination are called the favorable facet of p we consider both the eectiv ene and the eciency of the mining a given point may have many favorable facet we propose the notion of minimal disqualifying condition mdc which is eectiv e in summarizing favorable facet we develop ecien t algorithm for favorable facet mining for dieren t application scenario the rst method computes favorable facet on the y the second method pre computes all minimal disqualifying condition so that the favorable facet can be looked up in constant time an extensive performance study using both synthetic and real data set is reported to verify their eectiv ene and eciency 
we present a probabilistic generative model of visual attri butes together with an efficient learning algorithm attribute are visual quality of obj ect such a red striped or spotted the model see attribute a pattern of image se gments repeatedly sharing some characteristic property these can be any combination of appearance shape or the layout of segment within the pattern moreover attribute with g eneral appearance are taken into account such a the pattern of alternation of any two color which is characteristic for stripe to enable learning from unsegmented training i mages the model is learnt discriminatively by optimizing a likelihood ratio a demonstrated in the experimental evaluation our model can learn in a weakly supervised setting and encompasses a broad range of attribute we show that attribute can be learnt starting from a text query to google image search and can then be used to recognize the attribute and determine it spatial extent in novel real wo rld image 
modeling the evolution of topic with time is of great value in automatic summarization and analysis of large document collection in this work we propose a new probabilistic graphical model to address this issue the new model which we call the multiscale topic tomography model mttm employ non homogeneous poisson process to model generation of word count the evolution of topic is modeled through a multi scale analysis using haar wavelet one of the new feature of the model is it modeling the evolution of topic at various time scale of resolution allowing the user to zoom in and out of the time scale our experiment on science data using the new model uncovers some interesting pattern in topic the new model is also comparable to lda in predicting unseen data a demonstrated by our perplexity experiment 
imitation learning of sequential goaldirected behavior by standard supervised technique is often dicult we frame learning such behavior a a maximum margin structured prediction problem over a space of policy in this approach we learn mapping from feature to cost so an optimal policy in an mdp with these cost mimic the expert s behavior further we demonstrate a simple provably ecient approach to structured maximum margin learning based on the subgradient method that leverage existing fast algorithm for inference although the technique is general it is particularly relevant in problem where a and dynamic programming approach make learning policy tractable in problem beyond the limitation of a qp formulation we demonstrate our approach applied to route planning for outdoor mobile robot where the behavior a designer wish a planner to execute is often clear while specifying cost function that engender this behavior is a much more dicult task 
we introduce a family of kernel on discrete data structure within the general class of decomposition kernel a weighted decomposition kernel wdk is computed by dividing object into substructure indexed by a selector two substructure are then matched if their selector satisfy an equality predicate while the importance of the match is determined by a probability kernel on local distribution fitted on the substructure under reasonable assumption a wdk can be computed efficiently and can avoid combinatorial explosion of the feature space we report experimental evidence that the proposed kernel is highly competitive with respect to more complex state of the art method on a set of problem in bioinformatics 
protecting data privacy is an important problem in microdata distribution anonymization algorithm typically aim to protect individual privacy with minimal impact on the quality of the resulting data while the bulk of previous work ha measured quality through one size flts all measure we argue that quality is best judged with respect to the workload for which the data will ultimately be used this paper provides a suite of anonymization algorithm that produce an anonymous view based on a target class of workload consisting of one or more data mining task a well a selection predicate an extensive experimental evaluation indicates that this approach is often more efiective than previous anonymization technique 
many biological proposition can be supported by a variety of different type of evidence it is often useful to collect together large number of such proposition together with the evidence supporting them into database to be used in other analysis method that automatically make preliminary choice about which proposition to include can be helpful if they are accurate enough this can involve weighing evidence of varying strength we describe a method for learning a scoring function to weigh evidence of different type the algorithm evaluates each source of evidence by the extent to which other source tend to support it the detail are guided by a probabilistic formulation of the problem building on previous theoretical work we evaluate our method by applying it to predict protein protein interaction in yeast and using synthetic data 
previous work ha demonstrated that the image variation of many object human face in particular under variable lighting c an be effectively modeled by low dimensional linear space the typical linear subspace learning algorithm include principal component analysis pca linear discriminant analysis lda and locality preserving projection lpp all of these method consider an n n image a a high dimensional vector in rn n while an image represented in the plane is intrinsically a matrix in this paper we propose a new alg orithm called tensor subspace analysis tsa tsa considers an image a the second order tensor in rn rn where rn and rn are two vector space the relationship between the column vector of the image matrix and that between the row vector can be naturally characterized by tsa tsa detects the intrinsic local geometrical structure of the tensor space by learning a lower dimensional tensor subspace we compare our proposed approach with pca lda and lpp method on two standard database experimental result demonstrate that tsa achieves better recognition rate while being much more efficient 
the increasing pervasiveness of location acquisition technology gps gsm network etc is leading to the collection of large spatio temporal datasets and to the opportunity of discovering usable knowledge about movement behaviour which foster novel application and service in this paper we move towards this direction and develop an extension of the sequential pattern mining paradigm that analyzes the trajectory of moving object we introduce trajectory pattern a concise description of frequent behaviour in term of both space i e the region of space visited during movement and time i e the duration of movement in this setting we provide a general formal statement of the novel mining problem and then study several different instantiation of different complexity the various approach are then empirically evaluated over real data and synthetic benchmark comparing their strength and weakness 
in this paper we have proposed a novel framework to enable hierarchical image classification via statistical learning by integrating the concept hierarchy for semantic image concept organization a hierarchical mixture model is proposed to enable multi level modeling of semantic image concept and hierarchical classifier combination thus learning the classifier for the semantic image concept at the high level of the concept hierarchy can be effectively achieved by detecting the presence of the relevant base level atomic image concept to effectively learn the base level classifier for the atomic image concept at the first level of the concept hierarchy we have proposed a novel adaptive em algorithm to achieve more effective model selection and parameter estimation in addition a novel penalty term is proposed to effectively eliminate the misleading effect of the outlying unlabeled image on semi supervised classifier training our experimental result in a specific image domain of outdoor photo are very attractive 
relational clustering ha attracted more and more attention due to it phenomenal impact in various important application which involve multi type interrelated data object such a web mining search marketing bioinformatics citation analysis and epidemiology in this paper we propose a probabilistic model for relational clustering which also provides a principal framework to unify various important clustering task including traditional attribute based clustering semi supervised clustering co clustering and graph clustering the proposed model seek to identify cluster structure for each type of data object and interaction pattern between different type of object under this model we propose parametric hard and soft relational clustering algorithm under a large number of exponential family distribution the algorithm are applicable to relational data of various structure and at the same time unifies a number of stat of the art clustering algorithm co clustering algorithm the k partite graph clustering bregman k mean and semi supervised clustering based on hidden markov random field 
capital one is a highly quantitatively driven diversified financial service firm a such we make broad and deep use of the entire repertory of highly quantitative technique this talk will present our top ten statistical problem indeed one of them ha a a sub point the data mining dimension but it will likely be useful for data miner to see how their research need to complement and fit into the entire range of hard statistical issue 
we describe a framework for learning an object classifier from a single example this goal is achieved by emphasizing the relevant dimension for classification using available example of related class learning to accurately classify object from a single training example is often unfeasible due to overfitting effect however if the instance representation provides that the distance between each two instance of the same class is smaller than the distance between any two instance from different class then a nearest neighbor classifier could achieve perfect performance with a single training example we therefore suggest a two stage strategy first learn a metric over the instance that achieves the distance criterion mentioned above from available example of other related class then using the single example define a nearest neighbor classifier where distance is evaluated by the learned class relevance metric finding a metric that emphasizes the relevant dimension for classification might not be possible when restricted to linear projection we therefore make use of a kernel based metric learning algorithm our setting encodes object instance a set of locality based descriptor and adopts an appropriate image kernel for the class relevance metric learning the proposed framework for learning from a single example is demonstrated in a synthetic setting and on a character classification task 
the problem of learning a mapping between input and structured interdependent output variable cover sequential spatial and relational learning a well a predicting recursive structure joint feature representation of the input and output variable have paved the way to leveraging discriminative learner such a svms to this class of problem we address the problem of semi supervised learning in joint input output space the co training approach is based on the principle of maximizing the consensus among multiple independent hypothesis we develop this principle into a semi supervised support vector learning algorithm for joint input output space and arbitrary loss function experiment investigate the benefit of semi supervised structured model in term of accuracy and f score 
many unsupervised algorithm for nonlinear dimensionality reduction such a locally linear embedding lle and laplacian eigenmaps are derived from the spectral decomposition of sparse matrix while these algorithm aim to preserve certain proximity relation on average their embeddings are not explicitly designed to preserve local feature such a distance or angle in this paper we show how to construct a low dimensional embedding that maximally preserve angle between nearby data point the embedding is derived from the bottom eigenvectors of lle and or laplacian eigenmaps by solving an additional but small problem in semidefinite programming whose size is independent of the number of data point the solution obtained by semidefinite programming also yield an estimate of the data s intrinsic dimensionality experimental result on several data set demonstrate the merit of our approach 
we study the problem of an apprentice learning to behave in an environment with an unknown reward function by observing the behavior of an expert we follow on the work of abbeel and ng who considered a framework in which the true reward function is assumed to be a linear combination of a set of known and observable feature we give a new algorithm that like theirs is guaranteed to learn a policy that is nearly a good a the expert s given enough e xamples however unlike their algorithm we show that ours may produce a policy that is substantially better than the expert s moreover our algorithm is comput ationally faster is easier to implement and can be applied even in the absence of an expert the method is based on a game theoretic view of the problem which lead naturally to a direct application of the multiplicative weight algorithm of fr eund and schapire for playing repeated matrix game in addition to our formal presentation and analysis of the new algorithm we sketch how the method can be applied when the transition function itself is unknown and we provide an experimental demonstration of the algorithm on a toy video game environment 
we present a new statistical framework called hidden markov dirichlet process hmdp to jointly model the genetic recombination among possibly infinite number of founder and the coalescence with mutation event in the resulting genealogy the hmdp posit that a haplotype of genetic marker is generated by a sequence of recombination event that select an ancestor for each locus from an unbounded set of founder according to a st order markov transition process conjoining this process with a mutation model our method accommodates both between lineage recombination and within lineage sequence variation and lead to a compact and natural interpretation of the population structure and inheritance process underlying haplotype data we have developed an efficient sampling algorithmfor hmdp basedona two levelnestedp olyaurn scheme onbothsimulated and real snp haplotype data our method performs competitively or significantly better than extant method in uncovering the recombination hotspot along chromosomal locus and in addition it also infers the ancestral genetic pattern and offer a highly accurate map of ancestral composition of modern population 
we consider a general form of transductive learning on graph with laplacian regularization and derive margin based generalization bound using appropriate geometric property of the graph we use this analysis to obtain a better understanding of the role of normalization of the graph laplacian matrix a well a the effect of dimension reduction the result suggest a limita tion of the standard degree based normalization we propose a remedy from our analysis and demonstrate empirically that the remedy lead to improved classification performance 
in this paper we derive an algorithm that computes the entire solution path of the support vector regression with essentially the same computational cost a fitting one svr model we also propose an unbiased estimate for the degree of freedom of the svr model which allows convenient selection of the regularization parameter 
we propose a gaussian process gp framework for robust inference in which a gp prior on the mixing weight of a two component noise model augments the standard process over latent function value this approach is a generalization of the mixture likelihood used in traditional robust gp regression and a specialization of the gp mixture model suggested by tresp and rasmussen and ghahramani the value of this restriction is in it tractable ex pectation propagation update which allow for faster inference and model selection and better convergence than the standard mixture an additional benefit over t he latter method lie in our ability to incorporate knowledge of the noise domain to influence prediction and to recover with the predictive distribution info rmation about the outlier distribution via the gating process the model ha asymptotic complexity equal to that of conventional robust method but yield more confi dent prediction on benchmark problem than classical heavy tailed model and exhibit improved stability for data with clustered corruption for which th ey fail altogether we show further how our approach can be used without adjustment for more smoothly heteroscedastic data and suggest how it could be extended to more general noise model we also address similarity with the work of goldberg et al 
we show how to use unlabeled data and a deep belief net dbn to learn a good covariance kernel for a gaussian process we first learn a dee p generative model of the unlabeled data using the fast greedy algorithm intro duced by if the data is high dimensional and highly structured a gaussian kernel applied to the top layer of feature in the dbn work much better than a similar kernel applied to the raw input performance at both regression and classifi cation can then be further improved by using backpropagation through the dbn to discriminatively fine tune the covariance kernel a mixture p yn p xn yn p yn and then infer p yn xn attempt to learn covariance kernel based on p x and assumes that the decision boundary should occur in region where the data density p x is low when faced with high dimensional highly structured data however none of the existing approach have proved to be particularly successful in this paper we exploit two property of dbn s first they can be learned efficiently from unlabeled data and the top level feature generally capture sig nificant high order correlation in the data second they can be discriminatively fine tuned using backp ropagation we first learn a dbn model of p x in an entirely unsupervised way using the fast greedy learning algorithm introduced by and further investigated in we then use this gener ative model to initialize a multi layer non linear mapping f x w parameterized by w with f x z mapping the input vector in x into a feature space z typically the mapping f x w will contain million of parameter the top level feature produced by this mapping allow fairly accurate reconstruction of the input so they must contain most of the information in the input vector but they express this information in a way that make explicit a lot of the higher order structure in th e input data 
we present a conditional temporal probabilistic framework for reconstructing d human motion in monocular video based on descriptor encoding image silhouette observation for computational efficiency we restrict visual inference to low dimensional kernel induced non linear state space our methodology kbme combine kernel pca based non linear dimensionality reduction kpca and conditional bayesian mixture of expert bme in order to learn complex multivalued predictor between observation and model hidden state this is necessary for accurate inverse visual perception inference where several probable distant d solution exist due to noise or the uncertainty of monocular perspective projection low dimensional model are appropriate because many visual process exhibit strong non linear correlation in both the image observation and the target hidden state variable the learned predictor are temporally combined within a conditional graphical model in order to allow a principled propagation of uncertainty we study several predictor and empirically show that the proposed algorithm positively compare with technique based on regression kernel dependency estimation kde or pca alone and give result competitive to those of high dimensional mixture predictor at a fraction of their computational cost we show that the method successfully reconstructs the complex d motion of human in real monocular video sequence 
unsupervised learning algorithm aim to discover the structure hidden in the data and to learn representation that are more suitable a input to a supervised machine than the raw input many unsupervised method are based on reconstructing the input from the representation while constraining the repr esentation to have certain desirable property e g low dimension sparsity e tc others are based on approximating density by stochastically reconstructing t he input from the representation we describe a novel and efficient algorithm to lea rn sparse representation and compare it theoretically and experimentally with a similar machine trained probabilistically namely a restricted boltzmann machine we propose a simple criterion to compare and select different unsupervised machine based on the trade off between the reconstruction error and the info rmation content of the representation we demonstrate this method by extracting feature from a dataset of handwritten numeral and from a dataset of natural image patch we show that by stacking multiple level of such machine and by training sequentially high order dependency between the input observed variable can be captured 
many collective labeling task require inference on graphical model where the clique potential depend only on the number of node that get a particular label we design efficient inference algorithm for various family of such potential our algorithm are exact for arbitrary cardinality based clique potential on binary label and for max like and majority like clique potential on multiple label moving towards more complex potential we show that inference becomes np hard even on clique with homogeneous potts potential we present a approximation algorithm with runtime sub quadratic in the clique size in contrast the best known previous guarantee for graph with potts potential is only we perform empirical comparison on real and synthetic data and show that our proposed method are an order of magnitude faster than the well known tree based re parameterization trw and graph cut algorithm 
we introduce a functional representation of time series which allows forecast to be performed over an unspecified horizon with progressively revealed information set by virtue of using gaussian process a complete covariance matrix between forecast at several time step is available this information is put to use in an application to actively trade price spread between commodity future contract the approach delivers impressive out of sample risk adjusted return after transaction cost on a portfolio of spread 
we study the problem of online prediction of a noisy labeling of a graph with the perceptron we address both label noise and concept noise graph learning is framed a an instance of prediction on afinite set to treat label noise we show that the hinge loss bound derived by gentile for online perceptron learning can be transformed to relative mistake bound with an optimal leading constant when applied to prediction on a finite set these bound depend crucially on the norm of the learned concept often the norm of a concept can vary dramatically with only small perturbation in a labeling we analyze a simple transformation that stabilizes the norm under perturbation we derive an upper bound that depends only on natural property of the graph the graph diameter and the cut size of a partitioning of the graph which are only indirectly dependent on the size of the graph the impossibility of such bound for the graph geodesic nearest neighbor algorithm will be demonstrated 
learning by imitation represents an important mechanism for rapid acquisition of new behavior in human and robot a critical requirement for learning by imitation is the ability to handle uncertainty arising from the observation process a well a the imitator s own dynamic and interaction with the environment in this paper we present a new probabilistic method for inferring imitative action that take into account both the observation of the teacher a well a the imitator s dynamic our key contribution is a nonparametric learning method which generalizes to system with very different dynamic rather than relying on a known forward model of the dynamic our approach learns a nonparametric forward model via exploration leveraging advance in approximate inference in graphical model we show how the learned forward model can be directly used to plan an imitating sequence we provide experimental result for two system a biomechanical model of the human arm and a degree of freedom humanoid robot we demonstrate that the proposed method can be used to learn appropriate motor input to the model arm which imitates the desired movement a second set of result demonstrates dynamically stable full body imitation of a human teacher by the humanoid robot in this paper we propose a new technique for imitation that explicitly handle uncertainty using a probabilistic model of action and their sensory consequence rather than relying on a physicsbased parametric model of system dynamic a in traditional method our approach learns a nonparametric model of the imitator s internal dynamic during a constrained exploration period the learned model is then used to infer appropriate action for imitation using probabilistic inference in a dynamic bayesian network dbn with teacher observation a evidence we demonstrate the viability of the approach using two system a biomechanical model of the human arm and a 
we present an efficient sparse sampling technique for approximating bayes optimal decision making in reinforcement learning addressing the well known exploration versus exploitation tradeoff our approach combine sparse sampling with bayesian exploration to achieve improved decision making while controlling computational cost the idea is to grow a sparse lookahead tree intelligently by exploiting information in a bayesian posterior rather than enumerate action branch standard sparse sampling or compensate myopically value of perfect information the outcome is a flexible practical technique for improving action selection in simple reinforcement learning scenario 
blind source separation i e the extraction of unknown source from a set of given signal is relevant for many application a special case of this problem is dimension reduction where the goal is to approximate a given set of signal by superposition of a minimal number of source since in this case the signal outnumber the source the problem is over determined most popular approach for addressing this problem are based on purely linear mixing model however many application like the modeling of acoustic signal emg signal or movement trajectory require temporal shift invariance of the extracted component this case ha only rarely been treated in the computational literature and specifically for the case of dimension reduction almost no algorithm have been proposed we present a new algorithm for the solution of this problem which is based on a timefrequency transformation wigner ville distribution of the generative model we show that this algorithm outperforms classical source separation algorithm for linear mixture and also a related method for mixture with delay in addition applying the new algorithm to trajectory of human gait we demonstrate that it is suitable for the extraction of spatio temporal component that are easier to interpret than component extracted with other classical algorithm 
a layer neuromorphic vision processor whose component communicate spike event asychronously using the address eventrepresentation aer is demonstrated the system includes a retina chip two convolution chip a d winner take all chip a delay line chip a learning classifier chip and a set of pcbs for computer interfacing and address space remappings the component use a mixture of analog and digital computation and will learn to classify trajectory of a moving object a complete experimental setup and measurement result are shown 
this paper investigates the problem of automatically learning how to restructure the reward function of a markov decision process so a to speed up reinforcement learning we begin by describing a method that learns a shaped reward function given a set of state and temporal abstraction next we consider decomposition of the per timestep reward in multieffector problem in which the overall agent can be decomposed into multiple unit that are concurrently carrying out various task we show by example that to find a good reward decomposition it is often necessary to first shape the reward appropriately we then give a function approximation algorithm for solving both problem together standard reinforcement learning algorithm can be augmented with our method and we show experimentally that in each case significantly faster learning result 
unsupervised learning method often involve summarizing the data using a small number of parameter in certain domain only a small subset of the available data is relevant for the problem one class classification or one class clustering attempt to find a useful subset by locating a dense region in the data in particular a recently proposed algorithm called one class information ball oc ib show the advantage of modeling a small set of highly coherent point a opposed to pruning outlier we present several modification to oc ib and integrate it with a global search that result in several improvement such a deterministic result optimality guarantee control over cluster size and extension to other cost function empirical study yield significantly better result on various real and artificial data 
we present a novel boosting algorithm called softboost designed for set of binary labeled example that are not necessarily separable by convex combination of base hypothesis our algorithm achieves robustness by capping the distribution on the example our update of the distribution is motivated by minimizing a relative entropy subject to the capping constraint and co nstraints on the edge of the obtained base hypothesis the capping constraint imply a soft margin in the dual optimization problem our algorithm produce a convex combination of hypothesis whose soft margin is within of it maximum we employ relative entropy projection method to prove an o ln n iteration bound for our algorithm where n is number of example we compare our algorithm with other approach including lpboost brownboost and smoothboost we show that there exist case where the number of iteration required by lpboost grows linearly in n instead of the logarithmic growth for softboost in simulation study we show that our algorithm converges about a fast a lpboost faster than brownboost and much faster than smoothboost in a benchmark comparison we illustrate the competitiveness of our approach 
this paper present a novel framework called proto reinforcement learning prl based on a mathematical model of a proto value function these are task independent basis function that form the building block of all value function on a given state space manifold proto value function are learned not from reward but instead from analyzing the topology of the state space formally proto value function are fourier eigenfunctions of the laplace beltrami diffusion operator on the state space manifold proto value function facilitate structural decomposition of large state space and form geodesically smooth orthonormal basis function for approximating any value function the theoretical basis for proto value function combine insight from spectral graph theory harmonic analysis and riemannian manifold proto value function enable a novel generation of algorithm called representation policy iteration unifying the learning of representation and behavior 
front are significant meteorological phenomenon of interest the extraction of frontal system from observation and model data can greatly benefit many kind of research and application in atmospheric science due to the huge amount of observational and model data available nowadays automated extraction of front system is necessary this paper present an automated method to detect frontal system from numerical model generated data in this method a frontal system is characterized by a vector of feature comprised of parameter derived from the model wind field k mean clustering is applied to the generated sample set of the feature vector to partition the feature space and to identify cluster representing the front the probability that a model grid belongs to a front is estimated based on it feature vector the probability image is generated corresponding to the model grid a hierarchical thresholding technique is applied to the probability image to identify the frontal system and a gaussian bayes classifier is trained to determine the proper threshold value this is followed by post processing to filter out false signature experiment result from this method are in good agreement with the one identified by the domain expert 
a foundational problem in semi supervised learning is the construction of a graph underlying the data we propose to use a method which optimally combine a number of differently constructed graph for each of these graph we associate a basic graph kernel we then compute an optimal combined kernel this kernel solves an extended regularization problem which requires a joint minimization over both the data and the set of graph kernel we present encouraging result on different ocr task where the optimal combined kernel is computed from graph constructed with a variety of distance function and the k in nearest neighbor 
many important application area of text classifier demand high precision andit is common to compare prospective solution to the performance of naive bayes this baseline is usually easy to improve upon but in this work we demonstrate that appropriate document representation can make out performing this classifier much more challenging most importantly we provide a link between naive bayes and the logarithmic opinion pooling of the mixture of expert framework which dictate a particular type of document length normalization motivated by document specific feature selection we propose monotonic constraint on document term weighting which is shown a an effective method of fine tuning document representation the discussion is supported by experiment using three large email corpus corresponding to the problem of spam detection where high precision is of particular importance 
permutation are ubiquitous in many real world problem such a voting ranking and data association representing uncertainty over permutation is challenging since there are n possibility and typical compact representation such a graphical model cannot efficiently capture the mutu al exclusivity constraint associated with permutation in this paper we u e the low frequency term of a fourier decomposition to represent such distribution compactly we present kronecker conditioning a general and efficient approach for maintaining these distribution directly in the fourier domain low order fourier based approximation can lead to function that do not correspond to valid distribution to address this problem we present an efficient quadratic pr ogram defined directly in the fourier domain to project the approximation onto a relaxed form of the marginal polytope we demonstrate the effectiveness of our approach on a real camera based multi people tracking setting 
the kernel function play a central role in kernel method in this paper we consider the automated learning of the kernel matrix over a convex combination of pre specified kernel matrix in regularized kernel discriminant analysis rkda which performs lineardiscriminant analysis in the feature space via the kernel trick previous study have shown that this kernel learning problem can be formulated a a semidefinite program sdp which is however computationally expensive even with the recent advance in interior point method based on the equivalence relationship between rkda and least square problem in the binary class case we propose a quadratically constrained quadratic programming qcqp formulation for the kernel learning problem which can be solved more efficiently than sdp while most existing work on kernel learning deal with binary class problem only we show that our qcqp formulation can be extended naturally to the multi class case experimental result on both binary class and multi class benchmarkdata set show the efficacy of the proposed qcqp formulation 
choosing the right internal representation of example and hypothesis is a key issue for many learning problem feature construction is an approach to find such a representation independently of the underlying learning algorithm unfortunately the construction of feature usually implies searching a very large space of possibility and is often computationally demanding in this work we propose an approach to feature construction that is based on meta learning learning task are stored together with a corresponding set of constructed feature in a case base this case base is then used to constraint and guide the feature construction for new task our proposed method consists essentially of a new representation model for learning task and a corresponding two step distance measure our approach is unique a it enables u to apply case based feature construction not only on a large scale but also in distributed learning scenario in which communication cost play an important role using the two step process the accuracy of recommendation can be increased while not loosing the benefit of efficiency the theoretical result are also confirmed by experiment on both synthetical data and data obtained from a distributed learning scenario on audio data 
in this paper we aim at analyzing the characteristic of neuronal population response to instantaneous or time dependent input and the role of synapsis in neural information processing we have derived an evolution equation of the membrane potential density function with synaptic depression and obtain the formula for analytic computing the response of instantaneousre rate through a technical analysis we arrive at several signicant conclusion the background input play an important role in information processing and act a a switch betwee temporal integration and coincidence detection the role of synapsis can be regarded a a spatio temporallter it is important in neural information processing for the spatial distribution of synapsis and the spatial and temporal relation of input the instantaneous input frequency can affect the response amplitude and phase delay 
we propose a simple yet potentially very effective way of visualizing trained support vector machine nomogram are an established model visualization technique that can graphically encode the complete model on a single page the dimensionality of the visualization doe not depend on the number of attribute but merely on the property of the kernel to represent the effect of each predictive feature on the log odds ratio scale a required for the nomogram we employ logistic regression to convert the distance from the separating hyperplane into a probability case study on selected data set show that for a technique thought to be a black box nomogram can clearly expose it internal structure by providing an easy to interpret visualization the analyst can gain insight and study the effect of predictive factor 
under the prediction model of learning a prediction strategy is presented with an i i d sample of n point inx and corresponding label from a concept f f and aim to minimize the worst case probability of erring on annth point by exploiting the structure off haussler et al achieved a vc f n bound for the natural one inclusion prediction strategy improving on bound implied by pac type result by a o logn factor the key data structure in their result is the natural subgraph of the hypercube the one inclusion graph the key step is a d vc f bound on one inclusion graph density the first main result of this paper is a density bound of n n d n d d which positively resolve a conjecture of kuzmin warmuth relating to their unlabeled peeling compression scheme and also lead to an improved mistake bound for the randomized deterministic one inclusion strategy for all d for d n the proof us a new form of vc invariant shifting and a group theoretic symmetrization our second main result is a k class analogue of the d n mistake bound replacing the vc dimension by the pollard pseudo dimension and the one inclusion strategy by it natural hypergraph generalization this bound on expected risk improves on known pac based result by a factor ofo logn and is shown to be optimal up to a o logk factor the combinatorial technique of shifting take a central role in understanding the one inclusion hyper graph and is a running theme throughout 
sequence segmentation is a flexible and highly accurate mechanism for modeling several application inference on segmentation model involves dynamic programming computation that in the worst case can be cubic in the length of a sequence in contrast typical sequence labeling model require linear time we remove this limitation of segmentation model vi a vi sequential model by designing a succinct representation of potential common across overlapping segment we exploit such potential to design efficient inference algorithm that are both analytically shown to have a lower complexity and empirically found to be comparable to sequential model for typical extraction task 
a situation where training and test sample follow different input distribution is called covariate shift under covariate shift standard learning method such a maximum likelihood estimation are no longer consistent weighted variant according to the ratio of test and training input density are consistent therefore accurately estimating the density ratio called the importance is one of the key issue in covariate shift adaptation a naive approach to this task is to first estimate training and test input density separately and then estimate the importance by taking the ratio of the estimated density however this naive approach tends to perform poorly since density estimation is a hard task particularly in high dimensional case in this paper we propose a direct importance estimation method that doe not involve density estimation our method is equipped with a natural cross validation procedure and hence tuning parameter such a the kernel width can be objectively optimized simulation illustrate the usefulness of our approach 
in an interactive classification application a user may find it more valuable to develop a diagnostic decision support method which can reveal significant classification behavior of exemplar record such an approach ha the additional advantage of being able to optimize the decision process for the individual record in order to design more effective classification method in this paper we propose the subspace decision path method which provides the user with the ability to interactively explore a small number of node of a hierarchical decision process so that the most significant classification characteristic for a given test instance are revealed in addition the sd path method can provide enormous interpretability by constructing view of the data in which the different class are clearly separated out even in case where the classification behavior of the test instance is ambiguous the sd path method provides a diagnostic understanding of the characteristic which result in this ambiguity therefore this method combine the ability of the human and the computer in creating an effective diagnostic tool for instance centered high dimensional classification 
the peak location in a population of phase tuned neuron ha been shown to be a more reliable estimator for disparity than the peak location in a population of position tuned neuron unfortunately the disparity range covered by a phasetuned population is limited by phase wraparound thus a single population cannot cover the large range of disparity encountered in natural scene unless the scale of the receptive field is chosen to be very large which result in very low resolution depth estimate here we describe a biologically plausible measure of the confidence that the stimulus disparity is inside the range covered by a population of phase tuned neuron based upon this confidence measure we propose an algorithm for disparity estimation that us many population of high resolution phase tuned neuron that are biased to different disparity range via position shift between the left and right eye receptive field the population with the highest confidence is used to estimate the stimulus disparity we show that this algorithm outperforms a previously proposed coarse to fine algorithm for disparity estimation which us disparity estimate from coarse scale to select the population used at finer scale and can effectively detect occlusion 
we propose an active learning algorithm that learns a continuous valuation model from discrete preference the algorithm automatically decides what item are best presented to an individual in order to find the item that they value highly in a few trial a possible and exploit quirk of human psychology to minimize time and cognitive burden to do this our algorithm maximizes the expected improvement at each query without accurately modelling the entire valuation surface which would be needlessly expensive the problem is particularly difficult because the space of choice is infinite we demonstrate the effectiveness of the new algorithm compared to related active learning method we also embed the algorithm within a decision making tool for assisting digital artist in rendering material the tool find the best parameter while minimizing the number of query 
we present a method for simultaneously performing bandwidth selection and variable selection in nonparametric regression the method start with a local linear estimator with large bandwidth and incrementally decrease the bandwidth in direction where the gradient of the estimator with respect to bandwidth is large when the unknown function satisfies a sparsity condition the approach avoids the curse of dimensionality the method called rodeo regularization of derivative expectation operator conduct a sequence of hypothesis test and is easy to implement a modified version that replaces testing with soft thresholding may be viewed a solving a sequence of lasso problem when applied in one dimension the rodeo yield a method for choosing the locally optimal bandwidth 
recent work in network analysis have revealed the existence of network motif in biological network such a the protein protein interaction ppi network however existing motif mining algorithm are not sufficiently scalable to find meso scale network motif also there ha been little or no work to systematically exploit the extracted network motif for dissecting the vast interactomes we describe an efficient network motif discovery algorithm nemofinder that can mine meso scale network motif that are repeated and unique in large ppi network using nemofinder we successfully discovered for the first time up to size network motif in a large whole genome s cerevisiae yeast ppi network we also show that such network motif can be systematically exploited for indexing the reliability of ppi data that were generated via highly erroneous high throughput experimental method 
this paper proposes a new approach to model based clustering under prior knowledge the proposed formulation can be interpreted from two different angle a penalized logistic regression where the class label are o nly indirectly observed via the probability density of each class a finite mixtur e learning under a grouping prior to estimate the parameter of the proposed model we derive a generalized em algorithm with a closed form e step in contrast with other recent approach to semi supervised probabilistic clustering which require gibbs sampling or suboptimal shortcut we show that our approach is ideally suited for image segmentation it avoids the combinatorial nature markov random field prior and open the door to more sophisticated spatial prior e g wavelet based in a simple and computationally efficient way finally we ex tend our formulation to work in unsupervised semi supervised or discriminative mode 
brain computer interface bcis a any other interaction modality based on physiological signal and body channel e g muscular activity speech and gesture are prone to error in the recognition of subject s intent an elegant approach to improve the accuracy of bcis consists in a verification procedure directly based on the presence of error related potential errp in the eeg recorded right after the occurrence of an error six healthy volunteer subject with no prior bci experience participated in a new human robot interaction experiment where they were asked to mentally move a cursor towards a target that can be reached within a few step using motor imagination this experiment confirms the previously reported presence of a new kind of errp these interaction errp exhibit a first sharp negative peak followed by a positive peak and a second broader negative peak and m after the feedback respectively but in order to exploit these errp we need to detect them in each single trial using a short window following the feedback associated to the response of the classifier embedded in the bci we have achieved an average recognition rate of correct and erroneous single trial of and respectively furthermore we have achieved an average recognition rate of the subject s intent while trying to mentally drive the cursor of these result show that it s possible to simultaneously extract useful information for mental control to operate a brain actuated device a well a cognitive state such a error potential to improve the quality of the braincomputer interaction finally using a well known inverse model sloreta we show that the main focus of activity at the occurrence of the errp are a expected in the pre supplementary motor area and in the anterior cingulate cortex 
this paper describes a bio surveillance system designed to detect anomalous pattern in pharmacy retail data the system monitor national level over the counter otc pharmacy sale on a daily basis fast space time scan statistic are used to detect disease outbreak and user feedback is incorporated to improve system utility and usability 
in this paper we investigate how deviation in evaluation activity may reveal bias on the part of reviewer and controversy on the part of evaluated object we focus on a data centric approach where the evaluation data is assumed to represent the ground truth the standard statistical approach take evaluation and deviation at face value we argue that attention should be paid to the subjectivity of evaluation judging the evaluation score not just on what is being said deviation but also on who say it reviewer a well a on whom it is said about object furthermore we observe that bias and controversy are mutually dependent a there is more bias if there is higher deviation on a le controversial object to address this mutual dependency we propose a reinforcement model to identify bias and controversy we test our model on real life data to verify it applicability 
classification ha been commonly used in many data mining project in the financial service industry for instance to predict collectability of account receivable a binary class label is created based on whether a payment is received within a certain period however optimization of the classifier doe not necessarily lead to maximization of return on investment roi since maximization of the true positive rate is often different from maximization of the collectable amount which determines the roi under a fixed budget constraint the typical cost sensitive learning doe not solve this problem either since it involves an unknown opportunity cost due to the budget constraint learning the rank of collectable amount would ultimately solve the problem but it try to tackle an unnecessarily difficult problem and often result in poorer result for our specific target we propose a new algorithm that us gradient descent to directly optimize the related monetary measure under the budget constraint and thus maximizes the roi by comparison with several classification regression and ranking algorithm we demonstrate the new algorithm s substantial improvement of the financial impact on our client in the financial service industry 
this paper address the issue of numerical computation in machine learning domain based on similarity metric such a kernel method spectral technique and gaussian process it present a general solution strategy based on krylov subspace iteration and fast n body learning method the experiment show significant gain in computation and storage on datasets arising in image segmentation object detection and dimensionality reduction the paper also present theoretical bound on the stability of these method 
shannon s noisy channel model which describes how a corrupted message might be reconstructed ha been the corner stone for much work in statistical language and speech processing the model factor into two component a language model to characterize the original message and a channel model to describe the channel s corruptive process the standard approach for estimating the parameter of the channel model is unsupervised maximum likelihood of the observation data usually approximated using the expectation maximization em algorithm in this paper we show that it is better to maximize the joint likelihood of the data at both end of the noisy channel we derive a corresponding bi directional em algorithm and show that it give better performance than standard em on two task translation using a probabilistic lexicon and adaptation of a part of speech tagger between related language 
the dirichlet compound multinomial dcm distribution also called the multivariate polya distribution is a model for text document that take into account burstiness the fact that if a word occurs once in a document it is likely to occur repeatedly we derive a new family of distribution that are approximation to dcm distribution and constitute an exponential family unlike dcm distribution we use these so called edcm distribution to obtain insight into the property of dcm distribution and then derive an algorithm for edcm maximum likelihood training that is many time faster than the corresponding method for dcm distribution next we investigate expectation maximization with edcm component and deterministic annealing a a new clustering algorithm for document experiment show that the new algorithm is competitive with the best method in the literature and superior from the point of view of finding model with low perplexity 
introduction a real world classification task can often be viewed a consisting of multiple subtasks in remote sensing for example one may have multiple set of radar image each collected at a particular geographical location with the aim of designing classifier for detecting object of interest in image at all location in this situation one can either learn a single classifier from simple pooling of image from different location or learn multiple classifier each for a particular location and based on using image from that location only unfortunately neither of the two are optimal because the first ignores the difference between different location and the second ignores the analogy between them the above example represents a typical instance of a general learning scenario called multitask learning mtl the mtl is distinct from standard learning in two major aspect the task are not identical thus simply pooling them and treating them a a single task is not proper the task are dependent on each other thus isolating them and treating them a independent task is not appropriate the fact that the task are dependent implies that what is learned from one task is transferable to another correlated task by learning the task in parallel under a unified representation the transferability of expertise between task is exploited to the benefit of all task this expertise transfer is particularly important in the situation for which the training data of each task are scarce by using the data of related task the training set of each task is strengthened and the generalization of the resulting classifier is improved a major challenge in multitask learning is to find a representation of the task that simultaneously characterizes the difference and similarity between them in the existing multitask representation each task typically ha it own parameter to capture it characteristic what is different is the approach to modelling the between task similarity i e the way in which the task are related to each other 
in many practical application one is interested in generating a ranked list of item using information mined from continuous stream of data for example in the context of computer network one might want to generate list of node ranked according to their susceptibility to attack in addition real world data stream often exhibit concept drift making the learning task even more challenging we present an online learning approach to ranking with concept drift using weighted majority technique by continuously modeling different snapshot of the data and tuning our measure of belief in these model over time we capture change in the underlying concept and adapt our prediction accordingly we measure the performance of our algorithm on real electricity data a well a asynthetic data stream and demonstrate that our approach to ranking from stream data outperforms previously known batch learning method and other online method that do not account for concept drift 
a key challenge in designing analog to digital converter for cortically implanted prosthesis is to sense and process high dimensional neural signal recorded by the micro electrode array in this paper we describe a novel architecture for analog to digital a d conversion that combine conversion with spatial de correlation within a single module the architecture called multiple input multiple output mimo is based on a min max gradient descent optimization of a regularized linear cost function that naturally le nd to an a d formulation using an online formulation the architecture can a dapt to slow variation in cross channel correlation observed due to relat ive motion of the microelectrodes with respect to the signal source experimental result with real recorded multi channel neural data demonstrate the effectiveness of the proposed algorithm in alleviating cross channel redundancy across electrode and performing data compression directly at the a d converter 
recent research ha studied the role of sparsity in high dimensional regression and signal reconstruction establishing theoretical limit f or recovering sparse model from sparse data this line of work show that regularized least square regression can accurately estimate a sparse linear model from n noisy example in p dimension even if p is much larger than n in this paper we study a variant of this problem where the original n input variable are compressed by a random linear transformation to m n example in p dimension and establish condition under which a sparse linear model can be successfully recovered from the compressed data a primary motivation for this compression procedure is to anonymize the data and preserve privacy by revealing little information about the original data we characte rize the number of random projection that are required for regularized compressed regression to identify the nonzero coefficient in the true model with probability appr oaching one a property called sparsistence in addition we show that regularized compressed regression asymptotically predicts a well a an oracle linear model a property called persistence finally we characterize the privacy property of the compression procedure in information theoretic term establishing upper bound on the mutual information between the compressed and uncompressed data that decay to zero 
we propose consensus propagation an asynchronous distributed protocol for averaging number across a network we establish convergence characterize the convergence rate for regular graph and demonstrate that the protocol exhibit better scaling property than pairwise averaging an alternative that ha received much recent attention consensus propagation can be viewed a a special case of belief propagation and our result contribute to the belief propagation literature in particular beyond singly connected graph there are very few class of relevant problem for which belief propagation is known to converge 
many classification task such a spam filtering intrusion detection and terrorism detection are complicated by an adversary who wish to avoid detection previous work on adversarial classification ha made the unrealistic assumption that the attacker ha perfect knowledge of the classifier in this paper we introduce the adversarial classifier reverse engineering acre learning problem the task of learning sufficient information about a classifier to construct adversarial attack we present efficient algorithm for reverse engineering linear classifier with either continuous or boolean feature and demonstrate their effectiveness using real data from the domain of spam filtering 
a cost sensitive extension of boosting denoted a asymmetric boosting is presented unlike previous proposal the new algorithm is derived from sound decision theoretic principle which exploit the statistical interpretation of boosting to determine a principled extension of the boosting loss similarly to adaboost the cost sensitive extension minimizes this loss by gradient descent on the functional space of convex combination of weak learner and produce large margin detector it is shown that asymmetric boosting is fully compatible with adaboost in the sense that it becomes the latter when error are weighted equally experimental evidence is provided to demonstrate the claim of cost sensitivity and large margin the algorithm is also applied to the computer vision problem of face detection where it is shown to outperform a number of previous heuristic proposal for cost sensitive boosting adacost csb csb csb asymmetricadaboost adac adac and adac 
in this age of globalization organization need to publish their micro data owing to legal directive or share it with business associate in order to remain competitive this put personal privacy at risk to surmount this risk attribute that clearly identify individual such a name social security number driving license number are generally removed or replaced by random value but this may not be enough because such de identified database can sometimes be joined with other public database on attribute such a gender date of birth and zipcode to re identify individual who were supposed to remain anonymous inliterature suchanidentity leakingattributecombinationiscalled a a quasi identifier it is always critical to be able to recognize quasi identifier and to apply to them appropriate protective measure to mitigate the identity disclosure risk posed by join attack in this paper we start out by providing the first formal characterization and a practical technique to identify quasi identifier we show an interesting connection between whether a set of column form a quasi identifier and the number of distinct value assumed by the combination of the column we then use this characterization to come up with a probabilistic notion of anonymity again we show an interesting connection between the number of distinct value taken by a combination of column and the anonymity it can offer this allows u to find an ideal amount of generalization or suppression to apply to different column in order to achieve probabilistic anonymity we work through many example and show that our analysis can be used to make a published database conform to privacy act like hipaa in order to achieve the probabilistic anonymity we observe that one need to solve multiple dimensional k anonymity problem we propose many efficient and scalable algorithm for achieving dimensional anonymity supported in part by nsf grant itr this work wa also supported in part by trust the team for research in ubiquitous secure technology which receives support from the national science foundation nsf award number ccf and the following organization cisco escher hp ibm intel microsoft ornl qualcomm pirelli sun and symantec this work wa initiated when the second author wa a summer intern at tata research development and design center pune india our algorithm are optimal in a sense that they minimally distort data and retain much of it utility 
often the best performing supervised learning model are ensemble of hundred or thousand of base level classiers unfortunately the space required to store this many classiers and the time required to execute them at run time prohibits their use in application where test set are large e g google where storage space is at a premium e g pda and where computational power is limited e g hearing aid we present a method for compressing large complex ensemble into smaller faster model usually without signican t loss in performance 
we consider the problem of denoising a noisily sampled submanifold m in rd where the submanifoldm is a priori unknown and we are only given a noisy point sample the presented denoising algorithm is based on a graph based diffusion process of the point sample we analyze this diffusion process using recent result about the convergence of graph laplacians in the experiment we show that our method is capable of dealing with non trivial high dimensional noise moreover using the denoising algorithm a pre processing method we can improve the result of a semi supervised learning algorithm 
we introduce the boasting problem wherein useful trend in historical ordinal data ranking are discovered claim of the form our object wa ranked r or better in x of the last t time unit are formalized and maximal claim boast of this form are defined under two natural partial order for the first partial order we give an efficient and optimal algorithm for finding all such maximal claim for the second we apply a classical result from computational geometry to achieve an algorithm whose running time is significantly more efficient than that of a na ve one finally we connect this boasting problem to a novel variation of the problem of finding optimized confidence association rule a originally posed by fukuda et al and give an efficient algorithm for solving a simplification of the new problem 
this talk is about the next frontier in knowledge discovery and data mining 
hierarchical penalization is a generic framework for incorporating prior information inthefittingofstatisticalmodels whentheexplicativevariablesareorganized in a hierarchical structure the penalizer is a convex functional that performs soft selection at the group level and shrink variable within each group this favor solution with few leading term in the final combination the framework originally derived for taking prior knowledge into account is shown to be useful in linear regression when several parameter are used to model the influence of one feature or in kernel regression for learning multiple kernel 
we generalize the winnow algorithm for learning disjunction to learning subspace of low rank subspace are represented by symmetric projection matrix the online algorithm maintains it uncertainty about the hidden low rank projection matrix a a symmetric positive definite matrix this matrix is updated using a version of the matrix exponentiated gradient algorithm that is based on matrix exponential and matrix logarithm a in the case of the winnow algorithm the bound are logarithmic in the dimension n of the problem but linear in the rank r of the hidden subspace we show that the algorithm can be adapted to handle arbitrary matrix of any dimension via a reduction 
independent component analysis ica is a popular method for extracting independent feature from visual data however a a fundamentally linear technique there is always nonlinear residual redundancy that is not captured by ica hence there have been many attempt to try to create a hierarchical version of ica but so far none of the approach have a natural way to apply them more than once here we show that there is a relatively simple technique that transforms the absolute value of the output of a previous application of ica into a normal distribution to which ica maybe applied again this result in a recursive ica algorithm that may be applied any number of time in order to extract higher order structure from previous layer 
this paper explores several kernel in the context of text classification a novel view of how document might have been created is introduced and kernel are derived from this framework the relation between these kernel a well a to the gaussian kernel are discussed moreover the popular tf idf weighting scheme will be derived a a natural consequence finally the kernel have been evaluated on the reuters corpus volume i newswire database to ass their quality in a topic classification application 
bayesian reinforcement learning ha generated substantial interest recently a it provides an elegant solution to the exploration exploitation trade off in reinforcement learning however most investigation of bayesian reinforcement learning to date focus on the standard markov decision process mdps our goal is to extend these idea to the more general partially observable mdp pomdp framework where the state is a hidden variable to address this problem we introduce a new mathematical model the bayes adaptive pomdp this new model allows u to improve knowledge of the pomdp domain through interaction with the environment and plan optimal sequence of action which can tradeoff between improving the model identifying the state and gathering reward we show how the model can be fi nitely approximated while preserving the value function we describe approximation for belief tracking and planning in this model empirical result on two domain show that the model estimate and agent s return improve over time a the agent learns better model estimate in many real world system uncertainty can arise in both the prediction of the system s behavior and the observability of the system s state partially observable markov decision process pomdps take both kind of uncertainty into account and provide a powerful model for sequential decision making under these condition however most solving method for pomdps assume that the model is known a priori which is rarely the case in practice for instance in robotics the pomdp must refl ect exactly the uncertainty on the robot s sensor and actuator these parameter are rarely known exactly and therefore must often be approximated by a human designer such that even if this approximate pomdp could be solved exactly the resulting policy may not be optimal thus we seek a decision theoretic planner which can take into account the uncertainty over model parameter during the planning process a well a being able to learn from experience the value of these unknown parameter bayesian reinforcement learning ha investigated this problem in the context of fully observable mdps an extension to pomdp ha recently been proposed yet this method relies on heuristic to select action that will improve the model thus forgoing any theoretical guarantee on the quality of the approximation and on an oracle that can be queried to provide the current state in this paper we draw inspiration from the bayes adaptive mdp framework which is formulated to provide an optimal solution to the exploration exploitation trade off to extend these idea to pomdps we face two challenge how to update dirichlet parameter when the state is a hidden variable how to approximate the infi nite dimensional belief space to perform belief monitoring and compute the optimal policy this paper tackle both problem jointly the fi rst problem is solved by including the dirichlet parameter in the state space and maintaining belief state over these parameter we address the second by bounding the space of dirichlet parameter to a fi nite subspace necessary for optimal solution 
we propose a bayesian undirected graphical model for co training or more generally for semi supervised multi view learning this make explicit the previously unstated assumption of a large class of co training type algorithm and also clarifies the circumstance under which these assumption fail building upon new insight from this model we propose an improved method for co training which is a novel co training kernel for gaussian process classifier the resulting approach is convex and avoids local maximum problem unlike some previous multi view learning method furthermore it can automatically estimate how much each view should be trusted and thus accommodate noisy or unreliable view experiment on toy data and real world data set illustrate the benefit of this approach 
we develop a bayesian sum of tree model named bart where each tree is constrained by a prior to be a weak learner fitting and inference are accomplished via an iterative backfitting mcmc algorithm this model is motivated by ensemble method in general and boosting algorithm in particular like boosting each weak learner i e each weak tree contributes a small amount to the overall model however our procedure is defined by a statistical model a prior and a likelihood while boosting is defined by an algorithm this model based approach enables a full and accurate assessment of uncertainty in model prediction while remaining highly competitive in term of predictive accuracy 
graph matching is a fundamental problem in computer vision and machine learning we present two contribution first we give a new spectral relaxation technique for approximate solution to matching problem that naturally incorporates one to one or one to many constraint within the relaxation scheme the second is a normalization procedure for existing graph matching scoring function that can dramatically improve the matching accuracy it is based on a reinterpretation of the graph matching compatibility matrix a a bipartite graph on edge for which we seek a bistochastic normalization we evaluate our two contribution on a comprehensive test set of random graph matching problem a well a on image correspondence problem our normalization procedure can be used to improve the performance of many existing graph matching algorithm including spectral matching graduated assignment and semidefinite programmi ng because of it combinatorial nature graph matching is either solved exactly in a very restricted setting bipartite matching for example with the hungarian method or approximately most of the recent literature on graph matching ha followed this second path developing approximate relaxation to the graph matching problem in this paper we make two contribution the first contribution is a spectral relaxation for the graph matching problem that incorporates one to one or one to many mapping constraint represented a affine constraint a n ew mathematical tool is developed for that respect affinely constrained rayleigh quotient our meth od achieves comparable performance to state of the art algorithm while offering much better scal ability our second contribution relates to the graph matching scoring function itself which we argue is prone to systematic confusion error we show how a proper bistochastic normalization of the graph matching compatibility matrix is able to considerably reduce those error and improve the overall matching performance this improvement is demonstrated both for our spectral relaxation algorithm and for three state of the art graph matching algorithm spectral matching graduated assignment and semidefinite programming 
we present a novel algorithm for agglomerative hierarchical clustering based on evaluating marginal likelihood of a probabilistic model this algorithm ha several advantage over traditional distance based agglomerative clustering algorithm it defines a probabilistic model of the data which can be used to compute the predictive distribution of a test point and the probability of it belonging to any of the existing cluster in the tree it us a model based criterion to decide on merging cluster rather than an ad hoc distance metric bayesian hypothesis testing is used to decide which merges are advantageous and to output the recommended depth of the tree the algorithm can be interpreted a a novel fast bottom up approximate inference method for a dirichlet process i e countably infinite mixture model dpm it provides a new lower bound on the marginal likelihood of a dpm by summing over exponentially many clustering of the data in polynomial time we describe procedure for learning the model hyperpa rameters computing the predictive distribution and extension to the algorithm experimental result on synthetic and real world data set demonstrate useful property of the algorithm 
it is known that determinining whether a dec pomdp namely a cooperative partially observable stochastic game posg ha a cooperative strategy with positive expected reward is complete for nexp it wa not known until now how cooperation affected that complexity we show that for competitive posgs the complexity of determining whether one team ha a positive expected reward strategy is complete for nexpnp 
long distance language modeling is important not only in speech recognition and machine translation but also in high dimensional discrete sequence modeling in general however the problem of context length ha almost been neglected so far and a na ive bag of word history ha been employed in natural language processing in contrast in this paper we view topic shift within a text a a latent stochastic process to give an explicit probabilistic generative model that ha partial exchangeability we propose an online inference algorithm using particle filter to recognize topic shift to employ the most appropriate length of context automatically experiment on the bnc corpus showed consistent improvement over previous method involving no chronological order 
the risk or probability of error of the classifier produced by the adaboost algorithm is investigated in particular we consider the stopping strategy to be used in adaboost to achieve universal consistency we show that provided adaboost is stopped after n iteration for sample size n and the sequence of risk of the classifier it produce approach the bayes risk 
the learning of probabilistic model with many hidden variable and nondecomposable dependency is an important and challenging problem in contrast to traditional approach based on approximate inference in a single intractable model our approach is to train a set of tractable submodels by encouraging them to agree on the hidden variable this allows u to capture non decomposable aspect of the data while still maintaining tractability we propose an objective function for our approach derive em style algorithm for parameter estimation and demonstrate their effectiveness on three challenging real world learning task 
we state and analyze the rst active learning algorithm that nd an optimal hypothesis in any hypothesis class when the underlying distribution ha arbitrary form of noise the algorithm a for agnostic active relies only upon the assumption that it ha access to a stream of unlabeled example drawn i i d from a xed distribution we show that a achieves an exponential improvement i e requires only o ln sample to nd an optimal classier over the usual sample complexity of supervised learning for several setting considered before in the realizable case these include learning threshold classiers and learning homogeneous linear separator with respect to an input distribution which is uniform over the unit sphere 
we formulate a new data mining problem called storytelling a a generalization of redescription mining in traditional redescription mining we are given a set of object and a collection of subset defined over these object the goal is to view the set system a a vocabulary and identify two expression in this vocabulary that induce the same set of object storytelling on the other hand aim to explicitly relate object set that are disjoint and hence maximally dissimilar by finding a chain of approximate redescriptions between the set this problem find application in bioinformatics for instance where the biologist is trying to relate a set of gene expressed in one experiment to another set implicated in a dierent pathway we outline an ecient storytelling implementation that embeds the cartwheel redescription mining algorithm in an a search procedure using the former to supply next move operator on search branch to the latter this approach is practical and eective for mining large datasets and at the same time exploit the structure of partition imposed by the given vocabulary three application case study are presented a study of word overlap in large english dictionary exploring connection between genesets in a bioinformatics dataset and relating publication in the pubmed index of abstract 
we address the issue of the learnability of concept class under three classification noise model in the probably approximately correct framework after introducing the class conditional classification noise cccn model we investigate the problem of the learnability of concept class under this particular setting and we show that concept class that are learnable under the well known uniform classification noise cn setting are also cccn learnable which give cn cccn we then use this result to prove the equality between the set of concept class that are cn learnable and the set of concept class that are learnable in the constant partition classification noise cpcn setting or in other word we show that cn cpcn 
dimensionality reduction is the problem of finding a low dimensional representation of highdimensional input data this paper examines the case where additional information is known about the data in particular we assume the data are given in a sequence with action label associated with adjacent data point such a might come from a mobile robot the goal is a variation on dimensionality reduction where the output should be a representation of the input data that is both low dimensional and respect the action i e action correspond to simple transformation in the output representation we show how this variation can be solved with a semidefinite program we evaluate the technique in a synthetic robot inspired domain demonstrating qualitatively superior representation and quantitative improvement on a data prediction task 
an organization make a new release a new information become available release a tailored view for each data request release sensitive information and identifying information separately the availability of related release sharpens the identification of indi viduals by a global quasi identifier consisting of attribute from re lated release since it is not an option to anonymize previously released data the current release must be anonymized to ensure that a global quasi identifier is not effective for identification in this paper we study the sequential anonymization problem under this assumption a key question is how to anonymize the current release so that it cannot be linked to previous release yet remains useful for it own release purpose we introduce the lossy join a negative property in relational database design a a way to hide the join relationship among release and propose a scalable and practical solution 
partition of sequential data exist either per se or a a result of sequence segmentation algorithm it is often the case that the same timeline is partitioned in many different way for example different segmentation algorithm produce different partition of the same underlying data point in such case we are interested in producing an aggregate partition i e a segmentation that agrees a much a possible with the input segmentation each partition is defined a a set of continuous non overlapping segment of the timeline we show that this problem can be solved optimally in polynomial time using dynamic programming we also propose faster greedy heuristic that work well in practice we experiment with our algorithm and we demonstrate their utility in clustering the behavior of mobile phone user and combining the result of different segmentation algorithm on genomic sequence 
common strategy to liberate an organization s information asset for situational awareness frequently rely on infrastructure component such a data integration enterprise search federation data warehousing and so on and while these traditional platform enable analyst to get better and faster answer to their query the next big advance will change this paradigm user cannot be expected to formulate and ask every smart question every day and to escape this impractical and un scalable model the new paradigm will involve technology where the data find the data and relevance find the user perpetual analytics describes a class of application whereby enterprise context is assembled in real time on data stream a fast a operational system record observation context construction is a data find the data activity which enables event of interest to be streamed to subscriber in this talk i will talk at some depth about the dynamic of such system including scalability and sustainability 
we propose a simple information theoretic approach to soft clustering based on maximizing the mutual information i x y between the unknown cluster label y and the training pattern x with respect to parameter of specifically constrained encoding distribution the constraint are chosen such that pattern are likely to be clustered similarly if they lie close to specific unknown vector in the feature space the method may be conveniently applied to learning the optimal affinity matrix which corresponds to learning parameter of the kernelized encoder the procedure doe not require computation of eigenvalue of the gram matrix which make it potentially attractive for clustering large data set 
we describe a family of embedding algorithm that are based on nonparametric estimate of mutual information mi using parzen window estimate of the distribution in the joint input embedding space we derive a mi based objective function for dimensionality reduction that can be optimized directly with respect to a set of latent data representative various type of supervision signal can be introduced within the framework by replacing plain mi with several form of conditional mi example of the semi un supervised algorithm that we obtain this way are a new model for manifold alignment and a new type of embedding method that performs conditional dimensionality reduction 
in bioinformatics it is often desirable to combine data from various measurement source and thus structured feature vector are to be analyzed that posse different intrinsic blocking characteristic e g different patt ern of missing value observation noise level effective intrinsic dimensionalitie s we propose a new machine learning tool heterogeneous component analysis hca for feature extraction in order to better understand the factor that underlie such complex structured heterogeneous data hca is a linear block wise sparse bayesian pca based not only on a probabilistic model with block wise residual variance term but also on a bayesian treatment of a block wise sparse factor loading matrix we study various algorithm that implement our hca concept extracting sparse heterogeneous structure by obtaining common component for the block and specific component within each block simulation on toy and bioinformatics data underline the usefulness of the proposed structured matrix factorizatio n concept 
we introduce hidden process model hpms a class of probabilistic model for multivariate time series data the design of hpms ha been motivated by the challenge of modeling hidden cognitive process in the brain given functional magnetic resonance imaging fmri data fmri data is sparse high dimensional non markovian and often involves prior knowledge of the form hidden event a occurs n time within the interval t t hpms provide a generalization of the widely used general linear model approach to fmri analysis and hpms can also be viewed a a subclass of dynamic bayes network 
clustering is one of the most widely used statistical tool for data analysis among all existing clustering technique k mean is a very popular method because of it ease of programming and because it accomplishes a good trade o between achieved performance and computational complexity however kmeans is prone to local minimum problem and it doe not scale well with high dimensional data set a common approach to dealing with high dimensional data is to cluster in the space spanned by the principal component pc in this paper we show the benefit of clustering in a low dimensional discriminative space rather than in the pc space generative in particular we propose a new clustering algorithm called discriminative cluster analysis dca dca jointly performs dimensionality reduction and clustering several toy and real example show the benefit of dca versus traditional pca k mean clustering additionally a new matrix formulation is suggested and connection with related technique such a spectral graph method and linear discriminant analysis are provided 
topic model such a latent dirichlet allocation lda have been an effective tool for the statistical analysis of document collection and other discrete data the lda model assumes that the word of each document arise from a mixture of topic each of which is a distribution over the vocabulary a limitation of lda is the inability to model topic correlation even though for example a document about sport is more likely to also be about health than international finance this limitation stem from the use of the dirichlet distribution to model the variability among the topic proportion in this paper we develop the correlated topic model ctm where the topic proportion exhibit correlation via the logistic normal distribution we derive a mean field variational inference algorithm for approximate posterior inference in this model which is complicated by the fact that the logistic normal is not conjugate to the multinomial the ctm give a better fit than lda on a collection of ocred article from the journal science furthermore the ctm provides a natural way of visualizing and exploring this and other unstructured data set 
characterising the difference between two database is an often occurring problem in data mining detection of change over time is a prime example comparing database from two branch is another one the key problem is to discover the pattern that describe the difference emerging pattern provide only a partial answer to this question in previous work we showed that the data distribution can be captured in a pattern based model using compression here we extend this approach to define a generic dissimilarity measure on database moreover we show that this approach can identify those pattern that characterise the difference between two distribution experimental result show that our method provides a wellfounded way to independently measure database dissimilarity that allows for thorough inspection of the actual difference this illustrates the use of our approach in real world data mining 
despite all the attention paid to variational method based on sum product message passing loopy belief propagation tree reweighted sum product these method are still bound to inference on a small set of probabilistic model mean field approximation have been applied to a broader set of problem but the solution are often poor we propose a new class of conditionally specified variational approximation based on mean field theory while not usable on their own combined with sequential monte carlo they produce guaranteed improvement over conventional mean field moreover experiment on a well studied problem inferring the stable configuration of the ising spin glass show that the solution can be significantly better than those obtained using sum product based method 
convexity ha recently received a lot of attention in the machine learning community and the lack of convexity ha been seen a a major disadvantage of many learning algorithm such a multi layer artificial neural network we show that training multi layer neural network in which the number of hidden unit is learned can be viewed a a convex optimization problem this problem involves an infinite number of variable but can be solved by incrementally inserting a hidden unit at a time each time finding a linear classifier that minimizes a weighted sum of error 
ordinal regression ha become an eective way of learning user preference but most research focus on single regression problem in this paper we introduce collaborative ordinal regression where multiple ordinal regression task are handled simultaneously rather than modeling each task individually we explore the dependency between ranking function through a hierarchical bayesian model and assign a common gaussian process gp prior to all individual function empirical study show that our collaborative model outperforms the individual counterpart in preference learning application 
clustering method can be either data driven or need driven data driven method intend to discover the true structure of the underlying data while need driven method aim at organizing the true structure to meet certain application requirement thus need driven e g constrained clustering is able to find more useful and actionable cluster in application such a energy aware sensor network privacy preservation and market segmentation however the existing method of constrained clustering require user to provide the number of cluster which is often unknown in advance but ha a crucial impact on the clustering result in this paper we argue that a more natural way to generate actionable cluster is to let the application specific constraint decide the number of cluster for this purpose we introduce a novel cluster model constraint driven clustering cdc which find an a priori unspecified number of compact cluster that satisfy all user provided constraint two general type of constraint are considered i e minimum significance constraint and minimum variance constraint a well a combination of these two type we prove the np hardness of the cdc problem with different constraint we propose a novel dynamic data structure the cd tree which organizes data point in leaf node such that each leaf node approximately satisfies the cdc constraint and minimizes the objective function based on cd tree we develop an efficient algorithm to solve the new clustering problem our experimental evaluation on synthetic and real datasets demonstrates the quality of the generated cluster and the scalability of the algorithm 
addressed in this paper is the issue of email data cleaning for text mining many text mining application need take email a input email data is usually noisy and thus it is necessary to clean it before mining several product offer email cleaning feature however the type of noise that can be eliminated are restricted despite the importance of the problem email cleaning ha received little attention in the research community a thorough and systematic investigation on the issue is thus needed in this paper email cleaning is formalized a a problem of non text filtering and text normalization in this way email cleaning becomes independent from any specific text mining processing a cascaded approach is proposed which clean up an email in four pass including non text filtering paragraph normalization sentence normalization and word normalization a far a we know non text filtering and paragraph normalization have not been investigated previously method for performing the task on the basis of support vector machine svm have also been proposed in this paper feature in the model have been defined experimental result indicate that the proposed svm based method can significantly outperform the baseline method for email cleaning the proposed method ha been applied to term extraction a typical text mining processing experimental result show that the accuracy of term extraction can be significantly improved by using the data cleaning method 
in many application association rule will only be interesting if they represent non trivial correlation between all constituent item numerous technique have been developed that seek to avoid false discovery however while all provide useful solution to aspect of this problem none provides a generic solution that is both flexible enough to accommodate varying definition of true and false discovery and powerful enough to provide strict control over the risk of false discovery this paper present generic technique that allow definition of true and false discovery to be specified in term of arbitrary statistical hypothesis test and which provide strict control over the experimentwise risk of false discovery category and subject descriptor h database management database application data mining 
a family of probabilistic time series model is developed to analyze the time evolution of topic in large document collection the approach is to use state space model on the natural parameter of the multinomial distribution that represent the topic variational approximation based on kalman filter and nonparametric wavelet regression are developed to carry out approximate posterior inference over the latent topic in addition to giving quantitative predictive model of a sequential corpus dynamic topic model provide a qualitative window into the content of a large document collection the model are demonstrated by analyzing the ocr ed archive of the journal science from through 
it is commonly assumed that high dimensional datasets contain point most of which are located in low dimensional manifold detection of low dimensional cluster is an extremely useful task for performing operation such a clustering and classification however it is a challenging computational problem in this paper we study the problem of finding subset of point with low intrinsic dimensionality our main contribution is to extend the definition of fractal correlation dimension which measure average volume growth rate in order to estimate the intrinsic dimensionality of the data in local neighborhood we provide a careful analysis of several key example in order to demonstrate the property of our measure based on our proposed measure we introduce a novel approach to discover cluster with low dimensionality the resulting algorithm extend previous density based measure which have been successfully used for clustering we demonstrate the effectiveness of our algorithm for discovering low dimensional m flat embedded in high dimensional space and for detecting low rank sub matrix 
we derive a robust euclidean embedding procedure based on semidefinite programming that may be used in place of the popular classical multidimensional scaling cmds algorithm we motivate this algorithm by arguing that cmds is not particularly robust and ha several other deficiency generalpurpose semidefinite programming solver are too memory intensive for medium to large sized application so we also describe a fast subgradient based implementation of the robust algorithm additionally since cmds is often used for dimensionality reduction we provide an in depth look at reducing dimensionality with embedding procedure in particular we show that it is np hard to find optimal low dimensional embeddings under a variety of cost function 
clustering or factoring of a document collection attempt to explain each observed document in term of one or a small number of inferred prototype prior work demonstrated that when link exist between document in the corpus a is the case with a collection of web page or scientific paper building a joint model of document content and connection produce a better model than that built from content or connection alone many problem arise when trying to apply these joint model to corpus at the scale of the world wide web however one of these is that the sheer overhead of representing a feature space on the order of billion of dimension becomes impractical we address this problem with a simple representational shift inspired by probabilistic relational model instead of representing document linkage in term of the identity of linking document we represent it by the explicit and inferred attribute of the linking document several surprising result come with this shift in addition to being computationally more tractable the new model produce factor that more cleanly decompose the document collection we discus several variation on this model and show how some can be seen a exact generalization of the pagerank algorithm 
principal component and canonical correlation are at the root of many exploratory data mining technique and provide standard pre processing tool in machine learning lately probabilistic reformulations of these method have been proposed roweis tipping bishop b bach jordan they are based on a gaussian density model and are therefore like their non probabilistic counterpart very sensitive to atypical observation in this paper we introduce robust probabilistic principal component analysis and robust probabilistic canonical correlation analysis both are based on a student t density model the resulting probabilistic reformulations are more suitable in practice a they handle outlier in a natural way we compute maximum likelihood estimate of the parameter by mean of the em algorithm 
for large scale classification problem the training sample can be clustered beforehand a a downsampling pre process and then only the obtained cluster are used for training motivated by such assumption we proposed a classification algorithm support cluster machine scm within the learning framework introduced by vapnik for the scm a compatible kernel is adopted such that a similarity measure can be handled not only between cluster in the training phase but also between a cluster and a vector in the testing phase we also proved that the scm is a general extension of the svm with the rbf kernel the experimental result confirm that the scm is very effective for largescale classification problem due to significantly reduced computational cost for both training and testing and comparable classification accuracy a a by product it provides a promising approach to dealing with privacy preserving data mining problem 
we propose statistical predicate invention a a key problem for statistical relational learning spi is the problem of discovering new concept property and relation in structured data and generalizes hidden variable discovery in statistical model and predicate invention in ilp we propose an initial model for spi based on second order markov logic in which predicate a well a argument can be variable and the domain of discourse is not fully known in advance our approach iteratively refines cluster of symbol based on the cluster of symbol they appear in atom with e g it cluster relation by the cluster of the object they relate since dierent clustering are better for predicting different subset of the atom we allow multiple cross cutting clustering we show that this approach outperforms markov logic structure learning and the recently introduced infinite relational model on a number of relational datasets explicitly representing the uncertainty in the discovered predicate these can in turn be used a a basis for discovering new predicate which is potentially much more powerful than learning based on a fixed set of simple primitive essentially all the concept used by human can be viewed a invented predicate with many level of discovery between them and the sensory percept they are ultimately based on in statistical learning this problem is known a hidden or latent variable discovery and in relational learning a predicate invention both hidden variable discovery and predicate invention are considered quite important in their respective community but are also very difficult with limited progress to date one might question the need for spi arguing that structure learning is sucient such a question can 
the score returned by support vector machine are often used a a confidence measure in the classification of new example however there is no theoretical argument sustaining this practice thus when classification uncertainty ha to be assessed it is safer to resort to classifier estimating conditional probability of class label here we focus on the ambiguity in the vicinity of the boundary decision we propose an adaptation of maximum likelihood estimation instantiated on logistic regression the model output proper conditional probability into a user defined interval and is le precise elsewhere the model is also sparse in the sense that few example contribute to the solution the computational eciency is thus improved compared to logistic regression furthermore preliminary experiment show improvement over standard logistic regression and performance similar to support vector machine 
we introduce supervised latent dirichlet allocation slda a statistical model of labelled document the model accommodates a variety of response type we derive a maximum likelihood procedure for parameter estimation which relies on variational approximation to handle intractable posterior expectation prediction problemsmotivatethisresearch weusethefittedmodeltopredictresponsevalues for new document we test slda on two real world problem movie rating predicted from review and web page popularity predicted from text description we illustrate the benefit of slda versus modern regularized regression a well a versus an unsupervised lda analysis followed by a separate regression 
we extend support vector machine to input space that are set by ensuring that the classifier is invariant to permutation of subelements within each input such permutation include reordering of scalar in an input vector re ordering of tuples in an input matrix or re ordering of general object in hilbert space within a set a well this approach induces permutational invariance in the classifier which can then be directly applied to unusual set based representation of data the permutation invariant support vector machine alternate the hungarian method for maximum weight matching within the maximum margin learning procedure we e ectiv ely estimate and apply permutation to the input data point to maximize classification margin while minimizing data radius this procedure ha a strong theoretical justification via well established error probability bound experiment are shown on character recognition d object recognition and various uci datasets 
many existing approach to collaborative filtering can nei ther handle very large datasets nor easily deal with user who have very few rating in this paper we present the probabilistic matrix factorization pmf model which scale linearly with the number of observation and more importantly performs well on the large sparse and very imbalanced netflix dataset we furth er extend the pmf model to include an adaptive prior on the model parameter and show how the model capacity can be controlled automatically finally we introduce a constrained version of the pmf model that is based on the assumption that user who have rated similar set of movie are likely to have similar preference the resulting model is able to generalize considerably better for user s with very few rating when the prediction of multiple pmf model are linearly combined with the prediction of restricted boltzmann machine model we achieve an error rate of that is nearly better than the score of netflix s own system 
estimation of distribution algorithm edas are a popular approach to learn a probability distribution over the good solution to a combinatorial optimization problem here we consider the case where there is a collection of such optimization problem with learned distribution and where each problem can be characterized by some vector of feature now we can dene a machine learning problem to predict the distribution of good solution q sjx for a new problem with feature x where s denotes a solution this predictive distribution is then used to focus the search we demonstrate the utility of our method on a compiler optimization task where the goal is to nd a sequence of code transformation to make the code run fastest result on a set of dieren t benchmark on two distinct architecture show that our approach consistently lead to signican t improvement in performance 
in several organization it ha become increasingly popular to document and log the step that makeup a typical business process in some situation a normative workflow model of such process is developed and it becomes important to know if such a model is actually being followed by analyzing the available activity log in other scenario no model is available and with the purpose of evaluating case or creating new production policy one is interested in learning a workflow representation of such activity in either case machine learning tool that can mine workflow model are of great interest and still relatively unexplored we present here a probabilistic workflow model and a corresponding learning algorithm that run in polynomial time we illustrate the algorithm on example data derived from a real world workflow 
by the term quantization we refer to the process of using quantum mechanic in order to improve a classical algorithm usually by making it go faster in this paper we initiate the idea of quantizing clustering algorithm by using variation on a celebrated quantum algorithm due to grover after having introduced this novel approach to unsupervised learning we illustrate it with a quantized version of three standard algorithm divisive clustering k median and an algorithm for the construction of a neighbourhood graph we obtain a significant speedup compared to the classical approach 
when the transition probability and reward of a markov decision process are specied exactly the problem can be solved without any interaction with the environment when no such specication is available the agent s only recourse is a long and potentially dangerous exploration we present a framework which allows the expert to specify imprecise knowledge of transition probability in term of stochastic dominance constraint our algorithm can be used to nd optimal policy for qualitatively specied problem or when no such solution is available to decrease the required amount of exploration the algorithm s behavior is demonstrated on simulation of two classic problem mountain car ascent and cart pole balancing 
we describe a nonnegative variant of the sparse pca problem the goal is to create a low dimensional representation from a collection of point which on the one hand maximizes the variance of the projected point and on the other us only part of the original coordinate and thereby creating a sparse representation what distinguishes our problem from other sparse pca formulation is that the projection involves only nonnegative weight of the original coordinate a desired quality in various field including economics bioinformatics and computer vision adding nonnegativity contributes to sparseness where it enforces a partitioning of the original coordinate among the new ax we describe a simple yet efficient iterative coordinate descent type of scheme which converges to a local optimum of our optimization criterion giving good result on large real world datasets 
the power and popularity of kernel method stem in part from their ability to handle diverse form of structured input including vector graph and string recently several method have been proposed for combining kernel from heterogeneous data source however all of these method produce stationary combination i e the relative weight of the various kernel do not vary among input example this article proposes a method for combining multiple kernel in a nonstationary fashion the approach us a large margin latent variable generative model within the maximum entropy discrimination med framework latent parameter estimation is rendered tractable by variational bound and an iterative optimization procedure the classifier we use is a log ratio of gaussian mixture in which each component is implicitly mapped via a mercer kernel function we show that the support vector machine is a special case of this model in this approach discriminative parameter estimation is feasible via a fast sequential minimal optimization algorithm empirical result are presented on synthetic data several benchmark and on a protein function annotation task 
we show that several important bayesian bound studied in machine learning both in the batch a well a the online setting arise by an application of a simple compression lemma in particular we derive i pac bayesian bound in the batch setting ii bayesian log loss bound and iii bayesian bounded loss bound in the online setting using the compression lemma although every setting ha different semantics for prior posterior and loss we show that the core bound argument is the same the paper simplifies our understanding of several important and apparently disparate result a well a brings to light a powerful tool for developing similar argument for other method 
this paper study boosting algorithm that make a single pas over a set of base classifier we first analyze a one pas algorithm in the setting of boosti ng with diverse base classifier our guarantee is the same a the best proved for a ny boosting algorithm but our one pas algorithm is much faster than previous approach we next exhibit a random source of example for which a picky variant of adaboost that skip poor base classifier can outperform the st andard adaboost algorithm which us every base classifier by an exponential factor experiment with reuters and synthetic data show that one pas boosting can substantially improve on the accuracy of naive bayes and that picky boosting can sometimes lead to a further improvement in accuracy 
we introduce a mixture of probabilistic canonical correlation analyzer model for analyzing local correlation or more generally mutual statistical dependency in cooccurring data pair the model extends the traditional canonical correlation analysis and it probabilistic interpretation in three main way first a full bayesian treatment enables analysis of small sample large p small n a crucial problem in bioinformatics for instance and rigorous estimation of the degree of dependency and independency secondly the mixture formulation generalizes the method from global linearity to the more reasonable assumption of dierent kind of dependency for dierent kind of data a a third novel extension the method decomposes the variation in the data into shared and data set specific component 
consider alice who is interacting with bob alice and bob have some shared secret which help alice identify bob impersonator now consider eve who know alice and bob but doe not know their shared secret eve would like to impersonate bob and fool alice without knowing the secret if eve is computationally unbounded how long doe she need to observe alice and bob interacting before she can successfully impersonate bob what is a good strategy for eve in this setting if eve run in polynomial time and if there exists a one way function then it is not hard to see that alice and bob may be safe from impersonator but is the existence of one way function an essential condition namely if one way function do not exist can an e cient eve always impersonate bob in this work we consider these natural question from the point of view of ever who is trying to observe bob and learn to impersonate him we formalize this setting in a new computational learning model of learning adaptively changing distribution acds which we believe capture a wide variety of natural learning task and is of interest from both cryptographic and computational learning point of view we present a learning algorithm that eve can use to successfully learn to impersonate bob in the information theoretic setting we also show that in the computational setting an e cient eve can learn to impersonate any e cient bob if and only if one way function do not exist 
this paper present a local learning projection llp approach for linear dimensionality reduction we first point out that the well known principal component analysis pca essentially seek the projection that ha the minimal global estimation error then we propose a dimensionality reduction algorithm that lead to the projection with the minimal local estimation error and elucidate it advantage for classification task we also indicate that llp keep the local information in the sense that the projection value of each point can be well estimated based on it neighbor and their projection value experimental result are provided to validate the eectiveness of the proposed algorithm 
the importance of dominance and skyline analysis ha been well recognized in multi criterion decision making application most previous study assume a xed order on the attribute in practice dieren t customer may have different preference on nominal attribute in this paper we identify an interesting data mining problem nding favorable facet which ha not been studied before given a set of point in a multidimensional space for a specic target point p we want to discover with respect to which combination of order e g customer preference on the nominal attribute p is not dominated by any other point such combination are called the favorable facet of p we consider both the eectiv ene and the eciency of the mining a given point may have many favorable facet we propose the notion of minimal disqualifying condition mdc which is eectiv e in summarizing favorable facet we develop ecien t algorithm for favorable facet mining for dieren t application scenario the rst method computes favorable facet on the y the second method pre computes all minimal disqualifying condition so that the favorable facet can be looked up in constant time an extensive performance study using both synthetic and real data set is reported to verify their eectiv ene and eciency 
we present a probabilistic generative model of visual attri butes together with an efficient learning algorithm attribute are visual quality of obj ect such a red striped or spotted the model see attribute a pattern of image se gments repeatedly sharing some characteristic property these can be any combination of appearance shape or the layout of segment within the pattern moreover attribute with g eneral appearance are taken into account such a the pattern of alternation of any two color which is characteristic for stripe to enable learning from unsegmented training i mages the model is learnt discriminatively by optimizing a likelihood ratio a demonstrated in the experimental evaluation our model can learn in a weakly supervised setting and encompasses a broad range of attribute we show that attribute can be learnt starting from a text query to google image search and can then be used to recognize the attribute and determine it spatial extent in novel real wo rld image 
modeling the evolution of topic with time is of great value in automatic summarization and analysis of large document collection in this work we propose a new probabilistic graphical model to address this issue the new model which we call the multiscale topic tomography model mttm employ non homogeneous poisson process to model generation of word count the evolution of topic is modeled through a multi scale analysis using haar wavelet one of the new feature of the model is it modeling the evolution of topic at various time scale of resolution allowing the user to zoom in and out of the time scale our experiment on science data using the new model uncovers some interesting pattern in topic the new model is also comparable to lda in predicting unseen data a demonstrated by our perplexity experiment 
imitation learning of sequential goaldirected behavior by standard supervised technique is often dicult we frame learning such behavior a a maximum margin structured prediction problem over a space of policy in this approach we learn mapping from feature to cost so an optimal policy in an mdp with these cost mimic the expert s behavior further we demonstrate a simple provably ecient approach to structured maximum margin learning based on the subgradient method that leverage existing fast algorithm for inference although the technique is general it is particularly relevant in problem where a and dynamic programming approach make learning policy tractable in problem beyond the limitation of a qp formulation we demonstrate our approach applied to route planning for outdoor mobile robot where the behavior a designer wish a planner to execute is often clear while specifying cost function that engender this behavior is a much more dicult task 
we introduce a family of kernel on discrete data structure within the general class of decomposition kernel a weighted decomposition kernel wdk is computed by dividing object into substructure indexed by a selector two substructure are then matched if their selector satisfy an equality predicate while the importance of the match is determined by a probability kernel on local distribution fitted on the substructure under reasonable assumption a wdk can be computed efficiently and can avoid combinatorial explosion of the feature space we report experimental evidence that the proposed kernel is highly competitive with respect to more complex state of the art method on a set of problem in bioinformatics 
protecting data privacy is an important problem in microdata distribution anonymization algorithm typically aim to protect individual privacy with minimal impact on the quality of the resulting data while the bulk of previous work ha measured quality through one size flts all measure we argue that quality is best judged with respect to the workload for which the data will ultimately be used this paper provides a suite of anonymization algorithm that produce an anonymous view based on a target class of workload consisting of one or more data mining task a well a selection predicate an extensive experimental evaluation indicates that this approach is often more efiective than previous anonymization technique 
many biological proposition can be supported by a variety of different type of evidence it is often useful to collect together large number of such proposition together with the evidence supporting them into database to be used in other analysis method that automatically make preliminary choice about which proposition to include can be helpful if they are accurate enough this can involve weighing evidence of varying strength we describe a method for learning a scoring function to weigh evidence of different type the algorithm evaluates each source of evidence by the extent to which other source tend to support it the detail are guided by a probabilistic formulation of the problem building on previous theoretical work we evaluate our method by applying it to predict protein protein interaction in yeast and using synthetic data 
previous work ha demonstrated that the image variation of many object human face in particular under variable lighting c an be effectively modeled by low dimensional linear space the typical linear subspace learning algorithm include principal component analysis pca linear discriminant analysis lda and locality preserving projection lpp all of these method consider an n n image a a high dimensional vector in rn n while an image represented in the plane is intrinsically a matrix in this paper we propose a new alg orithm called tensor subspace analysis tsa tsa considers an image a the second order tensor in rn rn where rn and rn are two vector space the relationship between the column vector of the image matrix and that between the row vector can be naturally characterized by tsa tsa detects the intrinsic local geometrical structure of the tensor space by learning a lower dimensional tensor subspace we compare our proposed approach with pca lda and lpp method on two standard database experimental result demonstrate that tsa achieves better recognition rate while being much more efficient 
the increasing pervasiveness of location acquisition technology gps gsm network etc is leading to the collection of large spatio temporal datasets and to the opportunity of discovering usable knowledge about movement behaviour which foster novel application and service in this paper we move towards this direction and develop an extension of the sequential pattern mining paradigm that analyzes the trajectory of moving object we introduce trajectory pattern a concise description of frequent behaviour in term of both space i e the region of space visited during movement and time i e the duration of movement in this setting we provide a general formal statement of the novel mining problem and then study several different instantiation of different complexity the various approach are then empirically evaluated over real data and synthetic benchmark comparing their strength and weakness 
in this paper we have proposed a novel framework to enable hierarchical image classification via statistical learning by integrating the concept hierarchy for semantic image concept organization a hierarchical mixture model is proposed to enable multi level modeling of semantic image concept and hierarchical classifier combination thus learning the classifier for the semantic image concept at the high level of the concept hierarchy can be effectively achieved by detecting the presence of the relevant base level atomic image concept to effectively learn the base level classifier for the atomic image concept at the first level of the concept hierarchy we have proposed a novel adaptive em algorithm to achieve more effective model selection and parameter estimation in addition a novel penalty term is proposed to effectively eliminate the misleading effect of the outlying unlabeled image on semi supervised classifier training our experimental result in a specific image domain of outdoor photo are very attractive 
relational clustering ha attracted more and more attention due to it phenomenal impact in various important application which involve multi type interrelated data object such a web mining search marketing bioinformatics citation analysis and epidemiology in this paper we propose a probabilistic model for relational clustering which also provides a principal framework to unify various important clustering task including traditional attribute based clustering semi supervised clustering co clustering and graph clustering the proposed model seek to identify cluster structure for each type of data object and interaction pattern between different type of object under this model we propose parametric hard and soft relational clustering algorithm under a large number of exponential family distribution the algorithm are applicable to relational data of various structure and at the same time unifies a number of stat of the art clustering algorithm co clustering algorithm the k partite graph clustering bregman k mean and semi supervised clustering based on hidden markov random field 
capital one is a highly quantitatively driven diversified financial service firm a such we make broad and deep use of the entire repertory of highly quantitative technique this talk will present our top ten statistical problem indeed one of them ha a a sub point the data mining dimension but it will likely be useful for data miner to see how their research need to complement and fit into the entire range of hard statistical issue 
we describe a framework for learning an object classifier from a single example this goal is achieved by emphasizing the relevant dimension for classification using available example of related class learning to accurately classify object from a single training example is often unfeasible due to overfitting effect however if the instance representation provides that the distance between each two instance of the same class is smaller than the distance between any two instance from different class then a nearest neighbor classifier could achieve perfect performance with a single training example we therefore suggest a two stage strategy first learn a metric over the instance that achieves the distance criterion mentioned above from available example of other related class then using the single example define a nearest neighbor classifier where distance is evaluated by the learned class relevance metric finding a metric that emphasizes the relevant dimension for classification might not be possible when restricted to linear projection we therefore make use of a kernel based metric learning algorithm our setting encodes object instance a set of locality based descriptor and adopts an appropriate image kernel for the class relevance metric learning the proposed framework for learning from a single example is demonstrated in a synthetic setting and on a character classification task 
the problem of learning a mapping between input and structured interdependent output variable cover sequential spatial and relational learning a well a predicting recursive structure joint feature representation of the input and output variable have paved the way to leveraging discriminative learner such a svms to this class of problem we address the problem of semi supervised learning in joint input output space the co training approach is based on the principle of maximizing the consensus among multiple independent hypothesis we develop this principle into a semi supervised support vector learning algorithm for joint input output space and arbitrary loss function experiment investigate the benefit of semi supervised structured model in term of accuracy and f score 
many unsupervised algorithm for nonlinear dimensionality reduction such a locally linear embedding lle and laplacian eigenmaps are derived from the spectral decomposition of sparse matrix while these algorithm aim to preserve certain proximity relation on average their embeddings are not explicitly designed to preserve local feature such a distance or angle in this paper we show how to construct a low dimensional embedding that maximally preserve angle between nearby data point the embedding is derived from the bottom eigenvectors of lle and or laplacian eigenmaps by solving an additional but small problem in semidefinite programming whose size is independent of the number of data point the solution obtained by semidefinite programming also yield an estimate of the data s intrinsic dimensionality experimental result on several data set demonstrate the merit of our approach 
we study the problem of an apprentice learning to behave in an environment with an unknown reward function by observing the behavior of an expert we follow on the work of abbeel and ng who considered a framework in which the true reward function is assumed to be a linear combination of a set of known and observable feature we give a new algorithm that like theirs is guaranteed to learn a policy that is nearly a good a the expert s given enough e xamples however unlike their algorithm we show that ours may produce a policy that is substantially better than the expert s moreover our algorithm is comput ationally faster is easier to implement and can be applied even in the absence of an expert the method is based on a game theoretic view of the problem which lead naturally to a direct application of the multiplicative weight algorithm of fr eund and schapire for playing repeated matrix game in addition to our formal presentation and analysis of the new algorithm we sketch how the method can be applied when the transition function itself is unknown and we provide an experimental demonstration of the algorithm on a toy video game environment 
we present a new statistical framework called hidden markov dirichlet process hmdp to jointly model the genetic recombination among possibly infinite number of founder and the coalescence with mutation event in the resulting genealogy the hmdp posit that a haplotype of genetic marker is generated by a sequence of recombination event that select an ancestor for each locus from an unbounded set of founder according to a st order markov transition process conjoining this process with a mutation model our method accommodates both between lineage recombination and within lineage sequence variation and lead to a compact and natural interpretation of the population structure and inheritance process underlying haplotype data we have developed an efficient sampling algorithmfor hmdp basedona two levelnestedp olyaurn scheme onbothsimulated and real snp haplotype data our method performs competitively or significantly better than extant method in uncovering the recombination hotspot along chromosomal locus and in addition it also infers the ancestral genetic pattern and offer a highly accurate map of ancestral composition of modern population 
we consider a general form of transductive learning on graph with laplacian regularization and derive margin based generalization bound using appropriate geometric property of the graph we use this analysis to obtain a better understanding of the role of normalization of the graph laplacian matrix a well a the effect of dimension reduction the result suggest a limita tion of the standard degree based normalization we propose a remedy from our analysis and demonstrate empirically that the remedy lead to improved classification performance 
in this paper we derive an algorithm that computes the entire solution path of the support vector regression with essentially the same computational cost a fitting one svr model we also propose an unbiased estimate for the degree of freedom of the svr model which allows convenient selection of the regularization parameter 
we propose a gaussian process gp framework for robust inference in which a gp prior on the mixing weight of a two component noise model augments the standard process over latent function value this approach is a generalization of the mixture likelihood used in traditional robust gp regression and a specialization of the gp mixture model suggested by tresp and rasmussen and ghahramani the value of this restriction is in it tractable ex pectation propagation update which allow for faster inference and model selection and better convergence than the standard mixture an additional benefit over t he latter method lie in our ability to incorporate knowledge of the noise domain to influence prediction and to recover with the predictive distribution info rmation about the outlier distribution via the gating process the model ha asymptotic complexity equal to that of conventional robust method but yield more confi dent prediction on benchmark problem than classical heavy tailed model and exhibit improved stability for data with clustered corruption for which th ey fail altogether we show further how our approach can be used without adjustment for more smoothly heteroscedastic data and suggest how it could be extended to more general noise model we also address similarity with the work of goldberg et al 
we show how to use unlabeled data and a deep belief net dbn to learn a good covariance kernel for a gaussian process we first learn a dee p generative model of the unlabeled data using the fast greedy algorithm intro duced by if the data is high dimensional and highly structured a gaussian kernel applied to the top layer of feature in the dbn work much better than a similar kernel applied to the raw input performance at both regression and classifi cation can then be further improved by using backpropagation through the dbn to discriminatively fine tune the covariance kernel a mixture p yn p xn yn p yn and then infer p yn xn attempt to learn covariance kernel based on p x and assumes that the decision boundary should occur in region where the data density p x is low when faced with high dimensional highly structured data however none of the existing approach have proved to be particularly successful in this paper we exploit two property of dbn s first they can be learned efficiently from unlabeled data and the top level feature generally capture sig nificant high order correlation in the data second they can be discriminatively fine tuned using backp ropagation we first learn a dbn model of p x in an entirely unsupervised way using the fast greedy learning algorithm introduced by and further investigated in we then use this gener ative model to initialize a multi layer non linear mapping f x w parameterized by w with f x z mapping the input vector in x into a feature space z typically the mapping f x w will contain million of parameter the top level feature produced by this mapping allow fairly accurate reconstruction of the input so they must contain most of the information in the input vector but they express this information in a way that make explicit a lot of the higher order structure in th e input data 
we present a conditional temporal probabilistic framework for reconstructing d human motion in monocular video based on descriptor encoding image silhouette observation for computational efficiency we restrict visual inference to low dimensional kernel induced non linear state space our methodology kbme combine kernel pca based non linear dimensionality reduction kpca and conditional bayesian mixture of expert bme in order to learn complex multivalued predictor between observation and model hidden state this is necessary for accurate inverse visual perception inference where several probable distant d solution exist due to noise or the uncertainty of monocular perspective projection low dimensional model are appropriate because many visual process exhibit strong non linear correlation in both the image observation and the target hidden state variable the learned predictor are temporally combined within a conditional graphical model in order to allow a principled propagation of uncertainty we study several predictor and empirically show that the proposed algorithm positively compare with technique based on regression kernel dependency estimation kde or pca alone and give result competitive to those of high dimensional mixture predictor at a fraction of their computational cost we show that the method successfully reconstructs the complex d motion of human in real monocular video sequence 
unsupervised learning algorithm aim to discover the structure hidden in the data and to learn representation that are more suitable a input to a supervised machine than the raw input many unsupervised method are based on reconstructing the input from the representation while constraining the repr esentation to have certain desirable property e g low dimension sparsity e tc others are based on approximating density by stochastically reconstructing t he input from the representation we describe a novel and efficient algorithm to lea rn sparse representation and compare it theoretically and experimentally with a similar machine trained probabilistically namely a restricted boltzmann machine we propose a simple criterion to compare and select different unsupervised machine based on the trade off between the reconstruction error and the info rmation content of the representation we demonstrate this method by extracting feature from a dataset of handwritten numeral and from a dataset of natural image patch we show that by stacking multiple level of such machine and by training sequentially high order dependency between the input observed variable can be captured 
many collective labeling task require inference on graphical model where the clique potential depend only on the number of node that get a particular label we design efficient inference algorithm for various family of such potential our algorithm are exact for arbitrary cardinality based clique potential on binary label and for max like and majority like clique potential on multiple label moving towards more complex potential we show that inference becomes np hard even on clique with homogeneous potts potential we present a approximation algorithm with runtime sub quadratic in the clique size in contrast the best known previous guarantee for graph with potts potential is only we perform empirical comparison on real and synthetic data and show that our proposed method are an order of magnitude faster than the well known tree based re parameterization trw and graph cut algorithm 
we introduce a functional representation of time series which allows forecast to be performed over an unspecified horizon with progressively revealed information set by virtue of using gaussian process a complete covariance matrix between forecast at several time step is available this information is put to use in an application to actively trade price spread between commodity future contract the approach delivers impressive out of sample risk adjusted return after transaction cost on a portfolio of spread 
we study the problem of online prediction of a noisy labeling of a graph with the perceptron we address both label noise and concept noise graph learning is framed a an instance of prediction on afinite set to treat label noise we show that the hinge loss bound derived by gentile for online perceptron learning can be transformed to relative mistake bound with an optimal leading constant when applied to prediction on a finite set these bound depend crucially on the norm of the learned concept often the norm of a concept can vary dramatically with only small perturbation in a labeling we analyze a simple transformation that stabilizes the norm under perturbation we derive an upper bound that depends only on natural property of the graph the graph diameter and the cut size of a partitioning of the graph which are only indirectly dependent on the size of the graph the impossibility of such bound for the graph geodesic nearest neighbor algorithm will be demonstrated 
learning by imitation represents an important mechanism for rapid acquisition of new behavior in human and robot a critical requirement for learning by imitation is the ability to handle uncertainty arising from the observation process a well a the imitator s own dynamic and interaction with the environment in this paper we present a new probabilistic method for inferring imitative action that take into account both the observation of the teacher a well a the imitator s dynamic our key contribution is a nonparametric learning method which generalizes to system with very different dynamic rather than relying on a known forward model of the dynamic our approach learns a nonparametric forward model via exploration leveraging advance in approximate inference in graphical model we show how the learned forward model can be directly used to plan an imitating sequence we provide experimental result for two system a biomechanical model of the human arm and a degree of freedom humanoid robot we demonstrate that the proposed method can be used to learn appropriate motor input to the model arm which imitates the desired movement a second set of result demonstrates dynamically stable full body imitation of a human teacher by the humanoid robot in this paper we propose a new technique for imitation that explicitly handle uncertainty using a probabilistic model of action and their sensory consequence rather than relying on a physicsbased parametric model of system dynamic a in traditional method our approach learns a nonparametric model of the imitator s internal dynamic during a constrained exploration period the learned model is then used to infer appropriate action for imitation using probabilistic inference in a dynamic bayesian network dbn with teacher observation a evidence we demonstrate the viability of the approach using two system a biomechanical model of the human arm and a 
we present an efficient sparse sampling technique for approximating bayes optimal decision making in reinforcement learning addressing the well known exploration versus exploitation tradeoff our approach combine sparse sampling with bayesian exploration to achieve improved decision making while controlling computational cost the idea is to grow a sparse lookahead tree intelligently by exploiting information in a bayesian posterior rather than enumerate action branch standard sparse sampling or compensate myopically value of perfect information the outcome is a flexible practical technique for improving action selection in simple reinforcement learning scenario 
blind source separation i e the extraction of unknown source from a set of given signal is relevant for many application a special case of this problem is dimension reduction where the goal is to approximate a given set of signal by superposition of a minimal number of source since in this case the signal outnumber the source the problem is over determined most popular approach for addressing this problem are based on purely linear mixing model however many application like the modeling of acoustic signal emg signal or movement trajectory require temporal shift invariance of the extracted component this case ha only rarely been treated in the computational literature and specifically for the case of dimension reduction almost no algorithm have been proposed we present a new algorithm for the solution of this problem which is based on a timefrequency transformation wigner ville distribution of the generative model we show that this algorithm outperforms classical source separation algorithm for linear mixture and also a related method for mixture with delay in addition applying the new algorithm to trajectory of human gait we demonstrate that it is suitable for the extraction of spatio temporal component that are easier to interpret than component extracted with other classical algorithm 
a layer neuromorphic vision processor whose component communicate spike event asychronously using the address eventrepresentation aer is demonstrated the system includes a retina chip two convolution chip a d winner take all chip a delay line chip a learning classifier chip and a set of pcbs for computer interfacing and address space remappings the component use a mixture of analog and digital computation and will learn to classify trajectory of a moving object a complete experimental setup and measurement result are shown 
this paper investigates the problem of automatically learning how to restructure the reward function of a markov decision process so a to speed up reinforcement learning we begin by describing a method that learns a shaped reward function given a set of state and temporal abstraction next we consider decomposition of the per timestep reward in multieffector problem in which the overall agent can be decomposed into multiple unit that are concurrently carrying out various task we show by example that to find a good reward decomposition it is often necessary to first shape the reward appropriately we then give a function approximation algorithm for solving both problem together standard reinforcement learning algorithm can be augmented with our method and we show experimentally that in each case significantly faster learning result 
unsupervised learning method often involve summarizing the data using a small number of parameter in certain domain only a small subset of the available data is relevant for the problem one class classification or one class clustering attempt to find a useful subset by locating a dense region in the data in particular a recently proposed algorithm called one class information ball oc ib show the advantage of modeling a small set of highly coherent point a opposed to pruning outlier we present several modification to oc ib and integrate it with a global search that result in several improvement such a deterministic result optimality guarantee control over cluster size and extension to other cost function empirical study yield significantly better result on various real and artificial data 
we present a novel boosting algorithm called softboost designed for set of binary labeled example that are not necessarily separable by convex combination of base hypothesis our algorithm achieves robustness by capping the distribution on the example our update of the distribution is motivated by minimizing a relative entropy subject to the capping constraint and co nstraints on the edge of the obtained base hypothesis the capping constraint imply a soft margin in the dual optimization problem our algorithm produce a convex combination of hypothesis whose soft margin is within of it maximum we employ relative entropy projection method to prove an o ln n iteration bound for our algorithm where n is number of example we compare our algorithm with other approach including lpboost brownboost and smoothboost we show that there exist case where the number of iteration required by lpboost grows linearly in n instead of the logarithmic growth for softboost in simulation study we show that our algorithm converges about a fast a lpboost faster than brownboost and much faster than smoothboost in a benchmark comparison we illustrate the competitiveness of our approach 
this paper present a novel framework called proto reinforcement learning prl based on a mathematical model of a proto value function these are task independent basis function that form the building block of all value function on a given state space manifold proto value function are learned not from reward but instead from analyzing the topology of the state space formally proto value function are fourier eigenfunctions of the laplace beltrami diffusion operator on the state space manifold proto value function facilitate structural decomposition of large state space and form geodesically smooth orthonormal basis function for approximating any value function the theoretical basis for proto value function combine insight from spectral graph theory harmonic analysis and riemannian manifold proto value function enable a novel generation of algorithm called representation policy iteration unifying the learning of representation and behavior 
front are significant meteorological phenomenon of interest the extraction of frontal system from observation and model data can greatly benefit many kind of research and application in atmospheric science due to the huge amount of observational and model data available nowadays automated extraction of front system is necessary this paper present an automated method to detect frontal system from numerical model generated data in this method a frontal system is characterized by a vector of feature comprised of parameter derived from the model wind field k mean clustering is applied to the generated sample set of the feature vector to partition the feature space and to identify cluster representing the front the probability that a model grid belongs to a front is estimated based on it feature vector the probability image is generated corresponding to the model grid a hierarchical thresholding technique is applied to the probability image to identify the frontal system and a gaussian bayes classifier is trained to determine the proper threshold value this is followed by post processing to filter out false signature experiment result from this method are in good agreement with the one identified by the domain expert 
a foundational problem in semi supervised learning is the construction of a graph underlying the data we propose to use a method which optimally combine a number of differently constructed graph for each of these graph we associate a basic graph kernel we then compute an optimal combined kernel this kernel solves an extended regularization problem which requires a joint minimization over both the data and the set of graph kernel we present encouraging result on different ocr task where the optimal combined kernel is computed from graph constructed with a variety of distance function and the k in nearest neighbor 
many important application area of text classifier demand high precision andit is common to compare prospective solution to the performance of naive bayes this baseline is usually easy to improve upon but in this work we demonstrate that appropriate document representation can make out performing this classifier much more challenging most importantly we provide a link between naive bayes and the logarithmic opinion pooling of the mixture of expert framework which dictate a particular type of document length normalization motivated by document specific feature selection we propose monotonic constraint on document term weighting which is shown a an effective method of fine tuning document representation the discussion is supported by experiment using three large email corpus corresponding to the problem of spam detection where high precision is of particular importance 
permutation are ubiquitous in many real world problem such a voting ranking and data association representing uncertainty over permutation is challenging since there are n possibility and typical compact representation such a graphical model cannot efficiently capture the mutu al exclusivity constraint associated with permutation in this paper we u e the low frequency term of a fourier decomposition to represent such distribution compactly we present kronecker conditioning a general and efficient approach for maintaining these distribution directly in the fourier domain low order fourier based approximation can lead to function that do not correspond to valid distribution to address this problem we present an efficient quadratic pr ogram defined directly in the fourier domain to project the approximation onto a relaxed form of the marginal polytope we demonstrate the effectiveness of our approach on a real camera based multi people tracking setting 
the kernel function play a central role in kernel method in this paper we consider the automated learning of the kernel matrix over a convex combination of pre specified kernel matrix in regularized kernel discriminant analysis rkda which performs lineardiscriminant analysis in the feature space via the kernel trick previous study have shown that this kernel learning problem can be formulated a a semidefinite program sdp which is however computationally expensive even with the recent advance in interior point method based on the equivalence relationship between rkda and least square problem in the binary class case we propose a quadratically constrained quadratic programming qcqp formulation for the kernel learning problem which can be solved more efficiently than sdp while most existing work on kernel learning deal with binary class problem only we show that our qcqp formulation can be extended naturally to the multi class case experimental result on both binary class and multi class benchmarkdata set show the efficacy of the proposed qcqp formulation 
choosing the right internal representation of example and hypothesis is a key issue for many learning problem feature construction is an approach to find such a representation independently of the underlying learning algorithm unfortunately the construction of feature usually implies searching a very large space of possibility and is often computationally demanding in this work we propose an approach to feature construction that is based on meta learning learning task are stored together with a corresponding set of constructed feature in a case base this case base is then used to constraint and guide the feature construction for new task our proposed method consists essentially of a new representation model for learning task and a corresponding two step distance measure our approach is unique a it enables u to apply case based feature construction not only on a large scale but also in distributed learning scenario in which communication cost play an important role using the two step process the accuracy of recommendation can be increased while not loosing the benefit of efficiency the theoretical result are also confirmed by experiment on both synthetical data and data obtained from a distributed learning scenario on audio data 
in this paper we aim at analyzing the characteristic of neuronal population response to instantaneous or time dependent input and the role of synapsis in neural information processing we have derived an evolution equation of the membrane potential density function with synaptic depression and obtain the formula for analytic computing the response of instantaneousre rate through a technical analysis we arrive at several signicant conclusion the background input play an important role in information processing and act a a switch betwee temporal integration and coincidence detection the role of synapsis can be regarded a a spatio temporallter it is important in neural information processing for the spatial distribution of synapsis and the spatial and temporal relation of input the instantaneous input frequency can affect the response amplitude and phase delay 
we propose a simple yet potentially very effective way of visualizing trained support vector machine nomogram are an established model visualization technique that can graphically encode the complete model on a single page the dimensionality of the visualization doe not depend on the number of attribute but merely on the property of the kernel to represent the effect of each predictive feature on the log odds ratio scale a required for the nomogram we employ logistic regression to convert the distance from the separating hyperplane into a probability case study on selected data set show that for a technique thought to be a black box nomogram can clearly expose it internal structure by providing an easy to interpret visualization the analyst can gain insight and study the effect of predictive factor 
under the prediction model of learning a prediction strategy is presented with an i i d sample of n point inx and corresponding label from a concept f f and aim to minimize the worst case probability of erring on annth point by exploiting the structure off haussler et al achieved a vc f n bound for the natural one inclusion prediction strategy improving on bound implied by pac type result by a o logn factor the key data structure in their result is the natural subgraph of the hypercube the one inclusion graph the key step is a d vc f bound on one inclusion graph density the first main result of this paper is a density bound of n n d n d d which positively resolve a conjecture of kuzmin warmuth relating to their unlabeled peeling compression scheme and also lead to an improved mistake bound for the randomized deterministic one inclusion strategy for all d for d n the proof us a new form of vc invariant shifting and a group theoretic symmetrization our second main result is a k class analogue of the d n mistake bound replacing the vc dimension by the pollard pseudo dimension and the one inclusion strategy by it natural hypergraph generalization this bound on expected risk improves on known pac based result by a factor ofo logn and is shown to be optimal up to a o logk factor the combinatorial technique of shifting take a central role in understanding the one inclusion hyper graph and is a running theme throughout 
sequence segmentation is a flexible and highly accurate mechanism for modeling several application inference on segmentation model involves dynamic programming computation that in the worst case can be cubic in the length of a sequence in contrast typical sequence labeling model require linear time we remove this limitation of segmentation model vi a vi sequential model by designing a succinct representation of potential common across overlapping segment we exploit such potential to design efficient inference algorithm that are both analytically shown to have a lower complexity and empirically found to be comparable to sequential model for typical extraction task 
a situation where training and test sample follow different input distribution is called covariate shift under covariate shift standard learning method such a maximum likelihood estimation are no longer consistent weighted variant according to the ratio of test and training input density are consistent therefore accurately estimating the density ratio called the importance is one of the key issue in covariate shift adaptation a naive approach to this task is to first estimate training and test input density separately and then estimate the importance by taking the ratio of the estimated density however this naive approach tends to perform poorly since density estimation is a hard task particularly in high dimensional case in this paper we propose a direct importance estimation method that doe not involve density estimation our method is equipped with a natural cross validation procedure and hence tuning parameter such a the kernel width can be objectively optimized simulation illustrate the usefulness of our approach 
in an interactive classification application a user may find it more valuable to develop a diagnostic decision support method which can reveal significant classification behavior of exemplar record such an approach ha the additional advantage of being able to optimize the decision process for the individual record in order to design more effective classification method in this paper we propose the subspace decision path method which provides the user with the ability to interactively explore a small number of node of a hierarchical decision process so that the most significant classification characteristic for a given test instance are revealed in addition the sd path method can provide enormous interpretability by constructing view of the data in which the different class are clearly separated out even in case where the classification behavior of the test instance is ambiguous the sd path method provides a diagnostic understanding of the characteristic which result in this ambiguity therefore this method combine the ability of the human and the computer in creating an effective diagnostic tool for instance centered high dimensional classification 
the peak location in a population of phase tuned neuron ha been shown to be a more reliable estimator for disparity than the peak location in a population of position tuned neuron unfortunately the disparity range covered by a phasetuned population is limited by phase wraparound thus a single population cannot cover the large range of disparity encountered in natural scene unless the scale of the receptive field is chosen to be very large which result in very low resolution depth estimate here we describe a biologically plausible measure of the confidence that the stimulus disparity is inside the range covered by a population of phase tuned neuron based upon this confidence measure we propose an algorithm for disparity estimation that us many population of high resolution phase tuned neuron that are biased to different disparity range via position shift between the left and right eye receptive field the population with the highest confidence is used to estimate the stimulus disparity we show that this algorithm outperforms a previously proposed coarse to fine algorithm for disparity estimation which us disparity estimate from coarse scale to select the population used at finer scale and can effectively detect occlusion 
we propose an active learning algorithm that learns a continuous valuation model from discrete preference the algorithm automatically decides what item are best presented to an individual in order to find the item that they value highly in a few trial a possible and exploit quirk of human psychology to minimize time and cognitive burden to do this our algorithm maximizes the expected improvement at each query without accurately modelling the entire valuation surface which would be needlessly expensive the problem is particularly difficult because the space of choice is infinite we demonstrate the effectiveness of the new algorithm compared to related active learning method we also embed the algorithm within a decision making tool for assisting digital artist in rendering material the tool find the best parameter while minimizing the number of query 
we present a method for simultaneously performing bandwidth selection and variable selection in nonparametric regression the method start with a local linear estimator with large bandwidth and incrementally decrease the bandwidth in direction where the gradient of the estimator with respect to bandwidth is large when the unknown function satisfies a sparsity condition the approach avoids the curse of dimensionality the method called rodeo regularization of derivative expectation operator conduct a sequence of hypothesis test and is easy to implement a modified version that replaces testing with soft thresholding may be viewed a solving a sequence of lasso problem when applied in one dimension the rodeo yield a method for choosing the locally optimal bandwidth 
recent work in network analysis have revealed the existence of network motif in biological network such a the protein protein interaction ppi network however existing motif mining algorithm are not sufficiently scalable to find meso scale network motif also there ha been little or no work to systematically exploit the extracted network motif for dissecting the vast interactomes we describe an efficient network motif discovery algorithm nemofinder that can mine meso scale network motif that are repeated and unique in large ppi network using nemofinder we successfully discovered for the first time up to size network motif in a large whole genome s cerevisiae yeast ppi network we also show that such network motif can be systematically exploited for indexing the reliability of ppi data that were generated via highly erroneous high throughput experimental method 
this paper proposes a new approach to model based clustering under prior knowledge the proposed formulation can be interpreted from two different angle a penalized logistic regression where the class label are o nly indirectly observed via the probability density of each class a finite mixtur e learning under a grouping prior to estimate the parameter of the proposed model we derive a generalized em algorithm with a closed form e step in contrast with other recent approach to semi supervised probabilistic clustering which require gibbs sampling or suboptimal shortcut we show that our approach is ideally suited for image segmentation it avoids the combinatorial nature markov random field prior and open the door to more sophisticated spatial prior e g wavelet based in a simple and computationally efficient way finally we ex tend our formulation to work in unsupervised semi supervised or discriminative mode 
brain computer interface bcis a any other interaction modality based on physiological signal and body channel e g muscular activity speech and gesture are prone to error in the recognition of subject s intent an elegant approach to improve the accuracy of bcis consists in a verification procedure directly based on the presence of error related potential errp in the eeg recorded right after the occurrence of an error six healthy volunteer subject with no prior bci experience participated in a new human robot interaction experiment where they were asked to mentally move a cursor towards a target that can be reached within a few step using motor imagination this experiment confirms the previously reported presence of a new kind of errp these interaction errp exhibit a first sharp negative peak followed by a positive peak and a second broader negative peak and m after the feedback respectively but in order to exploit these errp we need to detect them in each single trial using a short window following the feedback associated to the response of the classifier embedded in the bci we have achieved an average recognition rate of correct and erroneous single trial of and respectively furthermore we have achieved an average recognition rate of the subject s intent while trying to mentally drive the cursor of these result show that it s possible to simultaneously extract useful information for mental control to operate a brain actuated device a well a cognitive state such a error potential to improve the quality of the braincomputer interaction finally using a well known inverse model sloreta we show that the main focus of activity at the occurrence of the errp are a expected in the pre supplementary motor area and in the anterior cingulate cortex 
this paper describes a bio surveillance system designed to detect anomalous pattern in pharmacy retail data the system monitor national level over the counter otc pharmacy sale on a daily basis fast space time scan statistic are used to detect disease outbreak and user feedback is incorporated to improve system utility and usability 
in this paper we investigate how deviation in evaluation activity may reveal bias on the part of reviewer and controversy on the part of evaluated object we focus on a data centric approach where the evaluation data is assumed to represent the ground truth the standard statistical approach take evaluation and deviation at face value we argue that attention should be paid to the subjectivity of evaluation judging the evaluation score not just on what is being said deviation but also on who say it reviewer a well a on whom it is said about object furthermore we observe that bias and controversy are mutually dependent a there is more bias if there is higher deviation on a le controversial object to address this mutual dependency we propose a reinforcement model to identify bias and controversy we test our model on real life data to verify it applicability 
classification ha been commonly used in many data mining project in the financial service industry for instance to predict collectability of account receivable a binary class label is created based on whether a payment is received within a certain period however optimization of the classifier doe not necessarily lead to maximization of return on investment roi since maximization of the true positive rate is often different from maximization of the collectable amount which determines the roi under a fixed budget constraint the typical cost sensitive learning doe not solve this problem either since it involves an unknown opportunity cost due to the budget constraint learning the rank of collectable amount would ultimately solve the problem but it try to tackle an unnecessarily difficult problem and often result in poorer result for our specific target we propose a new algorithm that us gradient descent to directly optimize the related monetary measure under the budget constraint and thus maximizes the roi by comparison with several classification regression and ranking algorithm we demonstrate the new algorithm s substantial improvement of the financial impact on our client in the financial service industry 
this paper address the issue of numerical computation in machine learning domain based on similarity metric such a kernel method spectral technique and gaussian process it present a general solution strategy based on krylov subspace iteration and fast n body learning method the experiment show significant gain in computation and storage on datasets arising in image segmentation object detection and dimensionality reduction the paper also present theoretical bound on the stability of these method 
shannon s noisy channel model which describes how a corrupted message might be reconstructed ha been the corner stone for much work in statistical language and speech processing the model factor into two component a language model to characterize the original message and a channel model to describe the channel s corruptive process the standard approach for estimating the parameter of the channel model is unsupervised maximum likelihood of the observation data usually approximated using the expectation maximization em algorithm in this paper we show that it is better to maximize the joint likelihood of the data at both end of the noisy channel we derive a corresponding bi directional em algorithm and show that it give better performance than standard em on two task translation using a probabilistic lexicon and adaptation of a part of speech tagger between related language 
the dirichlet compound multinomial dcm distribution also called the multivariate polya distribution is a model for text document that take into account burstiness the fact that if a word occurs once in a document it is likely to occur repeatedly we derive a new family of distribution that are approximation to dcm distribution and constitute an exponential family unlike dcm distribution we use these so called edcm distribution to obtain insight into the property of dcm distribution and then derive an algorithm for edcm maximum likelihood training that is many time faster than the corresponding method for dcm distribution next we investigate expectation maximization with edcm component and deterministic annealing a a new clustering algorithm for document experiment show that the new algorithm is competitive with the best method in the literature and superior from the point of view of finding model with low perplexity 
introduction a real world classification task can often be viewed a consisting of multiple subtasks in remote sensing for example one may have multiple set of radar image each collected at a particular geographical location with the aim of designing classifier for detecting object of interest in image at all location in this situation one can either learn a single classifier from simple pooling of image from different location or learn multiple classifier each for a particular location and based on using image from that location only unfortunately neither of the two are optimal because the first ignores the difference between different location and the second ignores the analogy between them the above example represents a typical instance of a general learning scenario called multitask learning mtl the mtl is distinct from standard learning in two major aspect the task are not identical thus simply pooling them and treating them a a single task is not proper the task are dependent on each other thus isolating them and treating them a independent task is not appropriate the fact that the task are dependent implies that what is learned from one task is transferable to another correlated task by learning the task in parallel under a unified representation the transferability of expertise between task is exploited to the benefit of all task this expertise transfer is particularly important in the situation for which the training data of each task are scarce by using the data of related task the training set of each task is strengthened and the generalization of the resulting classifier is improved a major challenge in multitask learning is to find a representation of the task that simultaneously characterizes the difference and similarity between them in the existing multitask representation each task typically ha it own parameter to capture it characteristic what is different is the approach to modelling the between task similarity i e the way in which the task are related to each other 
in many practical application one is interested in generating a ranked list of item using information mined from continuous stream of data for example in the context of computer network one might want to generate list of node ranked according to their susceptibility to attack in addition real world data stream often exhibit concept drift making the learning task even more challenging we present an online learning approach to ranking with concept drift using weighted majority technique by continuously modeling different snapshot of the data and tuning our measure of belief in these model over time we capture change in the underlying concept and adapt our prediction accordingly we measure the performance of our algorithm on real electricity data a well a asynthetic data stream and demonstrate that our approach to ranking from stream data outperforms previously known batch learning method and other online method that do not account for concept drift 
a key challenge in designing analog to digital converter for cortically implanted prosthesis is to sense and process high dimensional neural signal recorded by the micro electrode array in this paper we describe a novel architecture for analog to digital a d conversion that combine conversion with spatial de correlation within a single module the architecture called multiple input multiple output mimo is based on a min max gradient descent optimization of a regularized linear cost function that naturally le nd to an a d formulation using an online formulation the architecture can a dapt to slow variation in cross channel correlation observed due to relat ive motion of the microelectrodes with respect to the signal source experimental result with real recorded multi channel neural data demonstrate the effectiveness of the proposed algorithm in alleviating cross channel redundancy across electrode and performing data compression directly at the a d converter 
we construct a biologically motivated stochastic differential model of the neural and hemodynamic activity underlying the observed blood oxygen level dependent bold signal in functional magnetic resonance imaging fmri the model pose a difficult parameter estimation problem both theoretically due to the nonlinearity and divergence of the differential system and computationally due to it time and space complexity we adapt a particle filter and smoother to the task and discus some of the practical approach used to tackle the difficulty including use of sparse matrix and parallelisation result demonstrate the tractability of the approach in it application to an effective connectivity study 
adaptation to other initially unknown agent often requires computing an effective counter strategy in the bayesian paradigm one must find a good counterstrategy to the inferred posterior of the other agent behavior in the expert paradigm one may want to choose expert that are good counter strategy to the other agent expected behavior in this paper we introduce a technique for computing robust counter strategy for adaptation in multiagent scenario under a variety of paradigm the strategy can take advantage of a suspected tendency in the decision of the other agent while bounding the worst case performance when the tendency is not observed the technique involves solving a modified game and therefore can make use of recently developed algorithm for solving very large extensive game we demonstrate the effectiveness of the technique in two player texas hold em we show that the computed poker strategy are substantially more robust than best response counter strategy while still exploiting a suspected tendency we also compose the generated strategy in an expert algorithm showing a dramatic improvement in performance over using simple best response 
incorporating invariance into a learning algorithm is a common problem in machine learning we provide a convex formulation which can deal with arbitrary loss function and arbitrary loss in addition it is a drop in replacement for most optimization algorithm for kernel including solver of the svmstruct family the advantage of our setting is that it relies on column generation instead of modifying the underlying optimization problem directly 
geometrically based method for various task of machine learning have attracted considerable attention over the last few year in this paper we show convergence of eigenvectors of the point cloud laplacian to the eigenfunctions of the laplace beltrami operator on the underlying manifold thus establishing the first convergence result for a spectral dimensionality reduction algorithm in the manifold setting in the manifold learning setting the underlying manifold is usually unknown therefore functional map from the manifold need to be estimated using point cloud data the common approximation strategy in these method is to construct an adjacency graph associated to a point cloud the underlying intuition is that since the graph is a proxy for the manifold inference based on the structure of the graph corresponds to the desired inference based on the geometric structure of the manifold theoretical result to justify this intuition have been developed over the last few year building on recent result on functional convergence of approximation for the laplace beltrami operator using heat kernel and result on consistency of eigenfunctions for empirical approximation of such operator we show convergence of the laplacian eigenmaps algorithm we note that in order to prove convergence of a 
maximum variance unfolding mvu is an effective heuristic for dimensionality reduction it produce a low dimensional representation of the data by maximizing the variance of their embeddings while preserving the local distance of the original data we show that mvu also optimizes a statistical dependence measure which aim to retain the identity of individual observation under the distancepreserving constraint this general view allows u to design colored variant of mvu which produce low dimensional representation for a given task e g subject to class label or other side information 
we apply nonparametric hierarchical bayesian modelling to relational learning in a hierarchical bayesian approach model parameter can be personalized i e owned by entity or relationship and are coupled via a common prior distribution flexibility is added in a nonparametric hierarchical bayesian approach such that the learned knowledge can be truthfully represented we apply our approach to a medical domain where we form a nonparametric hierarchical bayesian model for relation involving hospital patient procedure and diagnosis the experiment show that the additional flexibility in a nonparametric hierarchical bayes approach result in a more accurate model of the dependency between procedure and diagnosis and give significantly improved estimate of the probability of future procedure 
policy gradient method are reinforcement learning algorithm that adapt a parameterized policy by following a performance gradient estimate conventional policy gradient method use monte carlo technique to estimate this gradient since monte carlo method tend to have high variance a large number of sample is required resulting in slow convergence in this paper we propose a bayesian framework that model the policy gradient a a gaussian process this reduces the number of sample needed to obtain accurate gradient estimate moreover estimate of the natural gradient a well a a measure of the uncertainty in the gradient estimate are provided at little extra cost 
traditional machine learning make a basic assumption the training and test data should be under the same distribution however in many case this identicaldistribution assumption doe not hold the assumption might be violated when a task from one new domain come while there are only labeled data from a similar old domain labeling the new data can be costly and it would also be a waste to throw away all the old data in this paper we present a novel transfer learning framework called tradaboost which extends boosting based learning algorithm freund schapire tradaboost allows user to utilize a small amount of newly labeled data to leverage the old data to construct a high quality classification model for the new data we show that this method can allow u to learn an accurate model using only a tiny amount of new data and a large amount of old data even when the new data are not sufficient to train a model alone we show that tradaboost allows knowledge to be effectively transferred from the old data to the new the effectiveness of our algorithm is analyzed theoretically and empirically to show that our iterative algorithm can converge well to an accurate model 
summary the integrated likelihood also called the marginal likelihood or the normalizing constant is a central quantity in bayesian model selection and model averaging it is defined a the integral over the parameter space of the likelihood time the prior density the bayes factor for model comparison and bayesian testing is a ratio of integrated likelihood and the model weight in bayesian model averaging are proportional to the integrated likelihood we consider the estimation of the integrated likelihood from posterior simulation output aiming at a generic method that us only the likelihood from the posterior simulation iteration the key is the harmonic mean identity which say that the reciprocal of the integrated likelihood is equal to the posterior harmonic mean of the likelihood the simplest estimator based on the identity is thus the harmonic mean of the likelihood while this is an unbiased and simulation consistent estimator it reciprocal can have infinite variance and so it is unstable in general we describe two method for stabilizing the harmonic mean estimator in the first one the parameter space is reduced in such a way that the modified estimator involves a harmonic mean of heavier tailed density thus resulting in a finite variance estimator the resulting estimator is stable it is also self monitoring since it obeys the central limit theorem and so confidence interval are available we discus general condition under which this reduction is applicable 
we present a new actor critic learning model in which a bayesian class of non parametric critic using gaussian process temporal difference learning is used such critic model the state action value function a a gaussian process allowing bayes rule to be used in computing the posterior distribution over state action value function conditioned on the observed data appropriate choice of the prior covariance kernel between stateaction value and of the parametrization of the policy allow u to obtain closed form expression for the posterior distribution of the gradient of the average discounted return with respect to the policy parameter the posterior mean which serf a our estimate of the policy gradient is used to update the policy while the posterior covariance allows u to gauge the reliability of the update 
sparse coding provides a class of algorithm for finding succinct representation of stimulus given only unlabeled input data it discovers basis function that capture higher level feature in the data however finding sparse code remains a very difficult computational problem in this paper we present efficient sparse coding algorithm that are based on iteratively solving two convex optimization problem an l regularized least square problem and an l constrained least square problem we propose novel algorithm to solve both of these optimization problem our algorithm result in a significant speedup for sparse coding allowing u to learn larger sparse code than possible with previously described algorithm we apply these algorithm to natural image and demonstrate that the inferred sparse code exhibit end stopping and non classical receptive field surround suppression and therefore may provide a partial explanation for these two phenomenon in v neuron 
clustering with constraint is an emerging area of data mining research however most work assumes that the constraint are given a one large batch in this paper we explore the situation where the constraint are incrementally given in this way the user after seeing a clustering can provide positive and negative feedback via constraint to critique a clustering solution we consider the problem of efficientlyupdating a clustering to satisfy the new and old constraint rather than re clustering the entire data set we show that the problem of incremental clustering under constraint is np hard in general but identify several sufficient condition which lead to efficiently solvable version these translate into a set of rule on the type of constraint that can be added and constraint set property that must be maintained we demonstrate that this approach is more efficient than re clustering the entire data set and ha several other advantage 
we propose a novel ensemble learning algorithm called triskel which ha two interesting feature first triskel learns an ensemble of classifier each biased to have high precision on instance from a single class a opposed to for example boosting where the ensemble member are biased to maximise accuracy over a subset of instance from all class second the ensemble member voting weight are assigned so that certain pair of biased classifier outweigh the rest of the ensemble if their prediction agree our experiment demonstrate that triskel often outperforms boosting in term of both accuracy and training time we also present an roc analysis which show that triskel s iterative structure corresponds to a sequence of nested roc space the analysis predicts that triskel work best when there are concavity in the roc curve this prediction agrees with our empirical result 
we introduce two method to improve convergence of the kernel hebbian algorithm kha for iterative kernel pca kha ha a scalar gain parameter which is either held constant or decreased a t leading to slow convergence our kha et algorithm accelerates kha by incorporating the reciprocal of the current estimated eigenvalue a a gain vector we then derive and apply stochastic metadescent smd to kha et this further speed convergence by performing gain adaptation in rkhs experimental result for kernel pca and spectral clustering of usps digit a well a motion capture and image de noising problem confirm that our method converge substantially faster than conventional kha 
the structure of a bayesian network bn encodes variable independence learning the structure of a bn however is typically of high computational complexity in this paper we explore and represent variable independence in learning conditional probability table cpts instead of in learning structure a full bayesian network is used a the structure and a decision tree is learned for each cpt the resulting model is called full bayesian network classifier fbcs in learning an fbc learning the decision tree for cpts capture essentially both variable independence and context specific independence we present a novel efficient decision tree learning which is also effective in the context of fbc learning in our experiment the fbc learning algorithm demonstrates better performance in both classification and ranking compared with other state of the art learning algorithm in addition it reduced effort on structure learning make it time complexity quite low a well 
in this paper we investigate the regularization property of kernel principal component analysis kpca by studying it application a a preprocessing step to supervised learning problem we show that performing kpca and then ordinary least square on the projected data a procedure known a kernel principal component regression kpcr is equivalent to spectral cut o regularization the regularization parameter being exactly the number of principal component to keep using probabilistic estimate for integral operator we can prove error estimate for kpcr and propose a parameter choice procedure allowing to prove consistency of the algorithm 
a standard method to obtain stochastic model for symbolic time series is to train state emitting hidden markov model se hmms with the baum welch algorithm based on observable operator model ooms in the last few month a number of novel learning algorithm for similar purpose have been developed two version of an efficiency sharpening e algorithm which iteratively improves the statistical efficiency of a sequence of oom estimator a constrained gradient descent ml estimator for transition emitting hmms te hmms we give an overview on these algorithm and compare them with se hmm em learning on synthetic and real life data 
there is a range of potential application of machine learning where it would be more useful to predict the probability distribution for a variable rather than simply the most likely value for that variable in meteorology and in finance it is often important to know the probability of a variable falling within or outside different range in this paper we consider the prediction of surf height with the objective of predicting if it will fall within a given surfable range prediction problem such a this are considerably more difficult if the distribution of the phenomenon is significantly different from a normal distribution this is the case with the surf data we have studied to address this we use an ensemble of mixture density network to predict the probability density function our evaluation show that this is an effective solution we also describe a web based application that present these prediction in a usable manner 
a reliable motion estimation algorithm must function under a wide range of condition one regime which we consider here is the case of moving object with contour but no visible texture tracking distinctive feature such a corner can disambiguate the motion of contour but spurious feature such a t junction can be badly misleading it is difficult to determine the reliability of motion from local measurement since a full rank covariance matrix can result from both real and spurious feature we propose a novel approach that avoids these point altogether and derives global motion estimate by utilizing information from three level of contour analysis edgelets boundary fragment and contour boundary fragment are chain of orientated edgelets for which we derive motion estimate from local evidence the uncertainty of the local estimate are disambiguated after the boundary fragment are properly grouped into contour the grouping is done by constructing a graphical model and marginalizing it using importance sampling we propose two equivalent representation in this graphical model reversible switch variable attached to the end of fragment and fragment chain to capture both local and global statistic of boundary our system is successfully applied to both synthetic and real video sequence containing high contrast boundary and textureless region the system produce good motion estimate along with properly grouped and completed contour 
we study the rate of growth of the regret in online convex optimization first we show that a simple extension of the algorithm of hazan et al eliminates the need for a priori knowledge of the lower bound on the second derivative of the observed function we then provide an algorithm adaptive online gradient descent which interpolates between the result of zinkevich for linear function and of hazan et al for strongly convex function achieving intermediate rate between p t and logt furthermore we show strong optimality of the algorithm finally we provide an extension of our result to general norm 
protection of privacy ha become an important problem in data mining in particular individual have become increasingly unwilling to share their data frequently resulting in individual either refusing to share their data or providing incorrect data in turn such problem in data collection can affect the success of data mining which relies on sufficient amount of accurate data in order to produce meaningful result random perturbation and randomized response technique can provide some level of privacy in data collection but they have an associated cost in accuracy cryptographic privacy preserving data mining method provide good privacy and accuracy property however in order to be efficient those solution must be tailored to specific mining task thereby losing generality in this paper we propose efficient cryptographic technique for online data collection in which data from a large number of respondent is collected anonymously without the help of a trusted third party that is our solution allows the miner to collect the original data from each respondent but in such a way that the miner cannot link a respondent s data to the respondent an advantage of such a solution is that because it doe not change the actual data it success doe not depend on the underlying data mining problem we provide proof of the correctness and privacy of our solution a well a experimental data that demonstrates it efficiency we also extend our solution to tolerate certain kind of malicious behavior of the participant 
we consider the problem of localizing a set of microphone together with a set of external acoustic event e g hand clap emitted at unknown time and unknown location we propose a solution that approximates this problem under a far field approximation defined in the calculus of affine geometry and that relies on singular value decomposition svd to recover the affine structure of the problem we then define low dimensional optimization technique for embedding the solution into euclidean geometry and further technique for recovering the location and emission time of the acoustic event the approach is useful for the calibration of ad hoc microphone array and sensor network 
web usage model and profile capture significant interest and trend from past access they are used to improve user experience say through recommendation of page pre fetching of page etc while browsing behavior change dynamically over time many web usage modeling technique are static due to prohibitive model compilation time and also lack of fast incremental update mechanism however profile have to be maintained so that they dynamically adapt to new interest and trend since otherwise their use can lead to poor irrelevant and mi targeted recommendation in personalization system we present a new profile maintenance scheme which extends the relational fuzzy subtractive clustering rfsc technique and enables efficient incremental update of usage profile an impact factor is defined whose value can be used to decide the need for recompilation the result from extensive experiment on a large real dataset of web log show that the proposed maintenance technique with considerably reduced computational cost is almost a good a complete remodeling 
we present a method for learning a low dimensional representation which is shared across a set of multiple related task the method build upon the wellknown norm regularization problem using a new regularizer which control the number of learned feature common for all the task we show that this problem is equivalent to a convex optimization problem and develop an iterative algorithm for solving it the algorithm ha a simple interpretation it alternately performs a supervised and an unsupervised step where in the latter step we learn commonacross task representation and in the former step we learn task specific function using these representation we report experiment on a simulated and a real data set which demonstrate that the proposed method dramatically improves the performance relative to learning each task independently our algorithm can also be used a a special case to simply select not learn a few common feature across the task 
experimental data indicate that norepinephrine is critica lly involved in aspect of vigilance and attention previously we considered the function of this neuromodulatory system on a time scale of minute and longer and suggested that it signal global uncertainty ar ising from gross change in environmental contingency however norepinephrine is also known to be activated phasically by familiar stimulus in welllearned task here we extend our uncertainty based treatment of norepinephrine to this phasic mode proposing that it is involve d in the detection and reaction to state uncertainty within a task this role of norepinephrine can be understood through the metaphor of neural interrupt 
consider each row of a dataset a the subset of the column for which the row ha an then a dataset is nested if for all pair of row one row is either a superset or subset of the other the concept of nestedness ha it origin in ecology where approximate version of it ha been used to model the specie distribution in different location we argue that nestedness and it extension are interesting property of datasets and that they can be applied also to domain other than ecology we first define natural measure of nestedness and study their property we then define the concept of k nestedness a dataset is almost k nested if the set of column can be partitioned to k part so that each part is almost nested we consider the algorithmic problem of computing how far a dataset is from being k nested and for finding a good partition of the column into k part the algorithm are based on spectral partitioning and scale to moderately large datasets we apply the method to real data from ecology and from other application and demonstrate the usefulness of the concept 
we present a generalization of dynamic bayesian network to concisely describe complex probability distribution such a in problem with multiple interacting variable length stream of random variable our framework incorporates recent graphical model construct to account for existence uncert ainty value specific independence aggregation relationship and local and gl obal constraint while still retaining a bayesian network interpretation and effic ient inference and learning technique we introduce one such general technique which is an extension of value elimination a backtracking search inference algorithm multi dynamic bayesian network are motivated by our work on statistical machine translation mt we present result on mt word alignment in support of our claim that mdbns are a promising framework for the rapid prototyping of new mt system 
intuitively learning should be easier when the data point lie on a low dimensional submanifold of the input space recently there ha been a growing interest in algorithm that aim to exploit such geometrical property of the data oftentimes these algorithm require estimating the dimension of the manifold first in this paper we propose an algorithm for dimension estimation and study it finite sample behaviour the algorithm estimate the dimension locally around the data point using nearest neighbor technique and then combine these local estimate we show that the rate of convergence of the resulting estimate is independent of the dimension of the input space and hence the algorithm is manifold adaptive thus when the manifold supporting the data is low dimensional the algorithm can be exponentially more efficient than it counterpart that are not exploiting this property our computer experiment confirm the obtained theoretical result 
even though several technique have been proposed in the literature for achieving multiclass classication using support vector machine svm the scalability aspect of these approach to handle large data set still need much of exploration core vector machine cvm is a technique for scaling up a two class svm to handle large data set in this paper we propose a multiclass core vector machine mcvm here we formulate the multiclass svm problem a a quadratic programming qp problem dening an svm with vector valued output this qp problem is then solved using the cvm technique to achieve scalability to handle large data set experiment done with several large synthetic and real world data set show that the proposed mcvm technique give good generalization performance a that of svm at a much lesser computational expense further it is observed that mcvm scale well with the size of the data set 
a multiclass classification problem can be reduced to a collection of binary problem with the aid of a coding matrix the quality of the final solution which is an ensemble of base classifier learned on the binary problem is affected by both the performance of the base learner and the error correcting ability of the coding matrix a coding matrix with strong error correcting ability may not be overall optimal if the binary problem are too hard for the base learner thus a trade off between error correcting and base learning should be sought in this paper we propose a new multiclass boosting algorithm that modifies the coding matrix according to the learning ability of the base learner we show experimentally that our algorithm is very efficient in optimizing the multiclass margin cost and outperforms existing multiclass algorithm such a adaboost ecc and one v one the improvement is especially significant when the base learner is not very powerful 
while the vast majority of clustering algorithm are partitional many real world datasets have inherently overlapping cluster several approach to finding overlapping cluster have come from work on analysis of biological datasets in this paper we interpret an overlapping clustering model proposed by segal et al a a generalization of gaussian mixture model and we extend it to an overlapping clustering model based on mixture of any regular exponential family distribution and the corresponding bregman divergence we provide the necessary algorithm modification for this extension and present result on synthetic data a well a subset of newsgroups and eachmovie datasets 
in this work we take a novel view of nonlinear manifold learning usually manifold learning is formulated in term of finding an embedding or unrolling of a manifold into a lower dimensional space instead we treat it a the problem of learning a representation of a nonlinear possibly non isometric manifold that allows for the manipulation of novel point central to this view of manifold learning is the concept of generalization beyond the training data drawing on concept from supervised learning we establish a framework for studying the problem of model assessment model complexity and model selection for manifold learning we present an extension of a recent algorithm locally smooth manifold learning lsml and show it ha good generalization property lsml learns a representation of a manifold or family of related manifold and can be used for computing geodesic distance finding the projection of a point onto a manifold recovering a manifold from point corrupted by noise generating novel point on a manifold and more 
in many application it is desirable to learn from several kernel multiple kernel learning mkl allows the practitioner to optimize over linear combination of kernel by enforcing sparse coecien t it also generalizes feature selection to kernel selection we propose mkl for joint feature map this provides a convenient and principled way for mkl with multiclass problem in addition we can exploit the joint feature map to learn kernel on output space we show the equivalence of several dieren t primal formulation including dieren t regularizers we present several optimization method and compare a convex quadratically constrained quadratic program qcqp and two semi innite linear program silps on toy data showing that the silps are faster than the qcqp we then demonstrate the utility of our method by applying the silp to three real world datasets 
this paper study the learning problem of ranking when one wish not just to accurately predict pairwise ordering but also preserve the magnitude of the preference or the dierence between rating a problem motivated by it key importance in the design of search engine movie recommendation and other similar ranking system we describe and analyze several algorithm for this problem and give stability bound for their generalization error extending previously known stability result to non bipartite ranking and magnitude of preference preserving algorithm we also report the result of experiment comparing these algorithm on several datasets and compare these result with those obtained using an algorithm minimizing the pairwise misranking error and standard regression 
many real world object have state that change over time by tracking the state sequence of these object we can study their behavior and take preventive measure before they reach some undesirable state in this paper we propose a new kind of pattern called progressive confident rule to describe sequence of state with an increasing confidence that lead to a particular end state we give a formal definition of progressive confident rule and their concise set we devise pruning strategy to reduce the enormous search space experiment result show that the proposed algorithm is efficient and scalable we also demonstrate the application of progressive confident rule in classification 
linear discriminant analysis lda ha been an active topic of research during the last century however the existing algorithm have several limitation when applied to visual data lda is only optimal for gaussian distributed class with equal covariance matrix and only class feature can be extracted on the other hand lda doe not scale well to high dimensional data overfitting and it cannot handle optimally multimodal distribution in this paper we introduce multimodal oriented discriminant analysis moda a lda extension which can overcome these drawback a new formulation and several novelty are proposed an optimal dimensionality reduction for multimodal gaussian class with different covariance is derived the new criterion allows for extracting more than class feature a covariance approximation is introduced to improve generalization and avoid over fitting when dealing with high dimensional data a linear time iterative majorization method is suggested in order to find a local optimum several synthetic and real experiment on face recognition show that moda outperform existing linear technique 
we introduce a novel algorithm for decision tree learning in the multi instance setting a originally defined by dietterich et al it differs from existing multi instance tree learner in a few crucial well motivated detail experiment on synthetic and real life datasets confirm the beneficial effect of these difference and show that the resulting system outperforms the existing multi instance decision tree learner 
we present a framework for active learning in the multiple instance mi setting in an mi learning problem instance are naturally organized into bag and it is the bag instead of individual instance that are labeled for training mi learner assume that every instance in a bag labeled negative is actually negative whereas at least one instance in a bag labeled positive is actually positive we consider the particular case in which an mi learner is allowed to selectively query unlabeled instance from positive bag this approach is well motivated in domain in which it is inexpensive to acquire bag label and possible but expensive to acquire instance label we describe a method for learning from label at mixed level of granularity and introduce two active query selection strategy motivated by the mi setting our experiment show that learning from instance label can significantly improve performance of a basic mi learning algorithm in two multiple instance domain content based image retrieval and text classification 
abstract survival in the natural world demand the selection of relevant visual cue to rapidly and reliably guide attention towards prey an d predator in cluttered environment we investigate whether our visual system selects cue that guide search in an optimal manner we formally obtain the optimal cue selection strategy by maximizing the signal to noise ratio snr between a search target and surrounding distractors this optimal strategy successfully account for several phenomenon in visual search behavior including the effect of target distracto r discriminability uncertainty in target s feature distractor heterogenei ty and linear separability furthermore the theory generates a new predict ion which we verify through psychophysical experiment with human subject our result provide direct experimental evidence that human select visual cue so a to maximize snr between the target and surrounding clutter 
abstract in this paper we are presenting a novel multivariate analysis method our scheme is based on a novel kernel orthonormalized partial least square pls variant for feature extraction imposing sparsity constrains in the so lution to improve scalability the algorithm is tested on a benchmark of uci data set and on the analysis of integrated short time music feature for genre predicti on the upshot is that the method ha strong expressive power even with rather few feature is clearly outperforming the ordinary kernel pls and therefore is an appealing method for feature extraction of labelled data 
this paper present a rigorous statistical analysis characterizing regime in which active learning significantly outperforms classical passive learning active learning algorithm are able to make query or select sample location in an online fashion depending on the result of the previous query in some regime this extra flexibility lead to significantly faster rate of error decay than those possible in classical passive learning setting the nature of these regime is explored by studying fundamental performance limit of active and passive learning in two illustrative nonparametric function class in addition to examining the theoretical potential of active learning this paper describes a practical algorithm capable of exploiting the extra flexibility of the active setting and provably improving upon the classical passive technique our active learning theory and method show promise in a number of application including field estimation using wireless sensor network and fault line detection 
we present a method for performing transductive inference on very large datasets our algorithm is based on multiclass gaussian process and is effective whenever the multiplication of the kernel matrix or it inverse with a vector can be performed sufficiently fast this hold for instance for certain graph and string kernel transduction is achieved by variational inference over the unlabeled data subject to a balancing constraint 
we propose a local generative model for similarity based classiflcation the method is applicable to the case that only pairwise similarity between sample are available the classifler model the local class conditional distribution using a maximum entropy estimate and empirical moment constraint the resulting exponential class conditionaldistributions are combined with class prior probability and misclassiflcation cost to form the local similarity discriminant analysis local sda classifler we compare the performance of local sda to a non local version to the local nearest centroid classifler the nearest centroid classifler k nn and to the recently developed potential support vector machine psvm result show that local sda is competitive with k nn and the computationally demanding psvm while offering the advantage of a generative classifler 
information discovery and analysis system ida are designed to correlate multiple source of data and use data mining technique to identify potential significant event application domain for ida are numerous and include the emerging area of homeland security developing test case for an ida requires background data set into which hypothetical future scenario can be overlaid the ida can then be measured in term of false positive and false negative error rate obtaining the test data set can be an obstacle due to both privacy issue and also the time and cost associated with collecting a diverse set of data source in this paper we give an overview of the design and architecture of an ida data set generator idsg that enables a fast and comprehensive test of an ida the idsg generates data using statistical and rule based algorithm and also semantic graph that represent interdependency between attribute a credit card transaction application is used to illustrate the approach 
establishing correspondence between distinct object is an important and nontrivial task correctness of the correspondence hinge on property which are difficult to capture in an a priori criterion while previous work ha used a priori criterion which in some case led to very good result the present paper explores whether it is possible to learn a combination of feature that for a given training set of aligned human head characterizes the notion of correct correspondence by optimizing this criterion we are then able to compute correspondence and morphs for novel head 
learning pattern of human behavior from sensor data is extremely important for high level activity inference we show how to extract and label a person s activity and significant place from trace of gps data in contrast to existing technique our approach simultaneously detects and classifies the significant location of a person and take the high level context into account our system us relational markov network to represent the hierarchical activity model that encodes the complex relation among gps reading activity and significant place we apply fft based message passing to perform efficient summation over large number of node in the network we present experiment that show significant improvement over existing technique 
metric learning ha been shown to significantly improve the a ccuracy of k nearest neighbor knn classification in problem involving thou ands of feature distance learning algorithm cannot be used due to overfitting a nd high computational complexity in such case previous work ha relied on a two step solution first apply dimensionality reduction method to the data an d then learn a metric in the resulting low dimensional subspace in this paper we show that better classification performance can be achieved by unifying the o bjectives of dimensionality reduction and metric learning we propose a method that solves for the low dimensional projection of the input which minimizes a metric objective aimed at separating point in different class by a large margin this projection is defined by a significantly smaller number of parameter tha n metric learned in input space and thus our optimization reduces the risk of overfitting theory and result are presented for both a linear a well a a kernel ized version of the algorithm overall we achieve classification rate simila r and in several case superior to those of support vector machine 
given a probability measure p and a reference measure one is often interested in the minimum measure set with p measure at least minimum volume set of this type summarize the region of greatest probability mass of p and are useful for detecting anomaly and constructing confidence region this paper address the problem of estimating minimum volume set based on independent sample distributed according to p other than these sample no other information is available regarding p but the reference measure is assumed to be known we introduce rule for estimating minimum volume set that parallel the empirical risk minimization and structural risk minimization principle in classification a in classification we show that the performance of our estimator are controlled by the rate of uniform convergence of empirical to true probability over the class from which the estimator is drawn thus we obtain finite sample size performance bound in term of vc dimension and related quantity we also demonstrate strong universal consistency an oracle inequality and rate of convergence the proposed estimator are illustrated with histogram and decision tree set estimation rule 
in this paper we present an information theoretic approach to learning a mahalanobis distance function we formulate the problem a that of minimizing the differential relative entropy between two multivariate gaussians under constraint on the distance function we express this problem a a particular bregman optimization problem that of minimizing the logdet divergence subject to linear constraint our resulting algorithm ha several advantage over existing method first our method can handle a wide variety of constraint and can optionally incorporate a prior on the distance function second it is fast and scalable unlike most existing method no eigenvalue computation or semi definite programming are required we also present an online version and derive regret bound for the resulting algorithm finally we evaluate our method on a recent error reporting system for software called clarify in the context of metric learning for nearest neighbor classification a well a on standard data set 
finite mixture of tree structured distribution have been shown to be ecient and eective in modeling multivariate distribution using dirichlet process we extend this approach to allow countably many treestructured mixture component the resulting bayesian framework allows u to deal with the problem of selecting the number of mixture component by computing the posterior distribution over the number of component and integrating out the component by bayesian model averaging we apply the proposed framework to identify the number and the property of predominant precipitation pattern in historical archive of climate data 
we consider regularized least square rls with a gaussian kernel we prove that if we let the gaussian bandwidth while letting the regularization parameter the rls solution tends to a polynomial whose order is controlled by the relative rate of decay of and if k then a the rls solution tends to the kth order polynomial with minimal empirical error we illustrate the result with an example 
we describe and analyze an algorithmic framework for online classification where each online trial consists of multiple prediction task that are tied together we tackle the problem by defining an instantaneous projection p roblem in which all the prediction task are tied through a single slack paramet er we then introduce a general method for approximately solving the problem by projecting simultaneously and independently each constraint which corresponds to a prediction subproblem and then average the individual solution we show that this approach constitutes a feasible albeit not necessarily optimal so lution for the original projection problem we derive concrete simultaneous projection variant and analyze them in the mistake bound model we demonstrate the power of the proposed algorithm in experiment with online multiclass text categorization our experiment indicate that a combination of class dependent feature with the simultaneous projection method outperforms previous algorithm for this task 
in this paper we study a new framework introduced by vapnik and vapnik that is an alternative capacity concept to the large margin approach in the particular case of binary classification we are given a set of labeled example and a collection of non example that do not belong to either class of interest this collection called the universum allows one to encode prior knowledge by representing meaningful concept in the same domain a the problem at hand we describe an algorithm to leverage the universum by maximizing the number of observed contradiction and show experimentally that this approach delivers accuracy improvement over using labeled data alone 
we propose a new method for constructing hyperkenels and dene two promising special case that can be computed in closed form these we call the gaussian and wishart hyperkernels the former is especially attractive in that it ha an interpretable regularization scheme reminiscent of that of the gaussian rbf kernel we discus how kernel learning can be used not just for improving the performance of classication and regression method but also a a stand alone algorithm for dimensionality reduction and relational or metric learning 
a new bottom up visual saliency model graph based visual saliency gbvs is proposed it consists of two step rst forming activation map on certain feature channel and then normalizing them in a way which highlight conspicuity and admits combination with other map the model is simple and biologically plausible insofar a it is naturally parallelized this model powerfully predicts human xations on variation of natural image achieving of the roc area of a human based control whereas the classical algorithm of itti koch achieve only 
this paper present an algorithm to estimate simultaneously both mean and variance of a non parametric regression problem the key point is that we are able to estimate variance locally unlike standard gaussian process regression or svms this mean that our estimator adapts to the local noise the problem is cast in the setting of maximum a posteriori estimation in exponential family unlike previous work we obtain a convex optimization problem which can be solved via newton s method 
this paper introduces gaussian process dynamical model gpdm for nonlinear time series analysis a gpdm comprises a low dimensional latent space with associated dynamic and a map from the latent space to an observation space we marginalize out the model parameter in closed form which amount to using gaussian process gp prior for both the dynamic and the observation mapping this result in a nonparametric model for dynamical system that account for uncertainty in the model we demonstrate the approach on human motion capture data in which each pose is dimensional despite the use of small data set the gpdm learns an effective representation of the nonlinear dynamic in these space webpage http www dgp toronto edu jmwang gpdm 
we analyze classification error on unseen case i e case t hat are different from those in the training set unlike standard generali zation error this off training set error may differ significantly from the empirical error with high probability even with large sample size we derive a datadependent bound on the difference between off training set and standard generalization error our result is based on a new bound on th e missing mass which for small sample is stronger than existing bound based on good turing estimator a we demonstrate on uci data set our bound give nontrivial generalization guarantee in many practical case in light of these result we show that certain claim made in the no free lunch literature are overly pessimistic 
we present a variational bayesian framework for performing inference density estimation and model selection in a special class of graphical model hidden markov random field hmrfs hmrfs are particularly well suited to image modelling and in this paper we apply them to the problem of image segmentation unfortunately hmrfs are notoriously hard to train and use because the exact inference problem they create are intractable our main contribution is to introduce an efficient variational approach for performing approximate inference of the bayesian formulation of hmrfs which we can then apply to the density estimation and model selection problem that arise when learning image model from data with this variational approach we can conveniently tackle the problem of image segmentation we present experimental result which show that our technique outperforms recent hmrf based segmentation method on real world image 
the application of kernel method to link analysis is explored in particular kandola et al s neumann kernel are shown to subsume not only the co citation and bibliographic coupling relatedness but also kleinberg s hit importance these popular measure of relatedness and importance correspond to the neumann kernel at the extreme of their parameter range and hence these kernel can be interpreted a defining a spectrum of link analysis measure intermediate between co citation bibliographic coupling and hit we also show that the kernel based on the graph laplacian including the regularized laplacian and diffusion kernel provide relatedness measure that overcome some limitation of co citation relatedness the property of these kernel based link analysis measure is examined with a network of bibliographic citation practical issue in applying these method to real data are discussed and possible solution are proposed 
latent semantic indexing is a classical method to produce optimal low rank approximation of a term document matrix however in the context of a particular query distribution the approximation thus produced need not be optimal we propose vlsi a new query dependent or variable low rank approximation that minimizes approximation error for any specified query distribution with this tool it is possible to tailor the lsi technique to particular setting often resulting in vastly improved approximation at much lower dimensionality we validate this method via a series of experiment on classical corpus showing that vlsi typically performs similarly to lsi with an order of magnitude fewer dimension 
there ha been considerable interest in random projection an approximate algorithm for estimating distance between pair of point in a high dimensional vector space let a in rn x d be our n point in d dimension the method multiplies a by a random matrix r in rd x k reducing the d dimension down to just k for speeding up the computation r typically consists of entry of standard normal n it is well known that random projection preserve pairwise distance in the expectation achlioptas proposed sparse random projection by replacing the n entry in r with entry in with probability achieving a threefold speedup in processing time we recommend using r of entry in with probability d d d for achieving a significant d fold speedup with little loss in accuracy 
in the paper we show that diagnostic class in cancer gene expression data set which most often include thousand of feature gene may be effectively separated with simple two dimensional plot such a scatterplot and radviz graph the principal innovation proposed in the paper is a method called vizrank which is able to score and identify the best among possibly million of candidate projection for visualization compared to recently much applied technique in the field of cancer genomics that include neural network support vector machine and various ensemble based approach vizrank is fast and find visualization model that can be easily examined and interpreted by domain expert our experiment on a number of gene expression data set show that vizrank wa always able to find data visualization with a small number of two to seven gene and excellent class separation in addition to providing ground for gene expression cancer diagnosis vizrank and it visualization also identify small set of relevant gene uncover interesting gene interaction and point to outlier and potential misclassifications in cancer data set 
ingcreasingly data mining algorithm must deal with database that continuously grow over time these algorithm must avoid repeatedly scanning their database when database attribute are symbolic adtrees have already shown to be ecien t structure to store sucien t statistic in main memory and to accelerate the mining process in batch environment here we present an ecien t method to sequentially update adtrees that is suitable for incremental environment 
a semi supervised multitask learning mtl framework is presented in which m parameterized semi supervised classifier each associated with one of m partially labeled data manifold are learned jointly under the constraint of a softsharing prior imposed over the parameter of the classifier the unlabeled data are utilized by basing classifier learning on neighborhood induced by a markov random walk over a graph representation of each manifold experimental result on real data set demonstrate that semi supervised mtl yield significant improvement in generalization performance over either semi supervised single task learning stl or supervised mtl in this paper we attempt to integrate the benefit offered by semi supervised learning and mtl by proposing semi supervised multitask learning the semi supervised mtl framework consists of m semi supervised classifier coupled by a joint prior distribution over the parameter of all classifier each classifier provides the solution for a partially labeled data classification task the solution for the m task are obtained simultaneously under the unified framework existing semi supervised algorithm are often not directly amenable to mtl extension transductive algorithm directly operate on label since the label is a local property of the associated data point information sharing must be performed at the level of data location instead of at the task level the inductive algorithm in employ a data dependent prior to encode manifold information since the information transferred from related task is also often represented by a prior the two prior will compete and need be balanced moreover this precludes a dirichlet process or it variant to represent the sharing prior across task because the base distribution of a dirichlet process cannot be dependent on any particular manifold we develop a new semi supervised formulation which enjoys several nice property that make the formulation immediately amenable to an mtl extension first the formulation ha a parametric classifier built for each task thus multitask learning can be performed efficiently at the task level using the parameter of the classifier second the formulation encodes the manifold information 
we present a new class of model for high dimensional nonparametric regression and classification called sparse additive model spam ou r method combine idea from sparse linear modeling and additive nonparametric regression we derive a method for fitting the model that is effective even whe n the number of covariates is larger than the sample size a statistical ana lysis of the property of spam is given together with empirical result on synthetic and real data showing that spam can be effective in fitting sparse nonparametri c model in high dimensional data 
recent experimental result suggest that dendritic and back propagating spike can influence synaptic plasticity in different way in this study we investigate how these signal could temporally interact at dendrite leading to changing plasticity property at local synapse cluster similar to a previous study we employ a differential hebbian plasticity rule to emulate spike timing dependent plasticity we use dendritic d and back propagating bp spike a post synaptic signal in the learning rule and investigate how their interaction will influence plasticity we will analyze a situation where synapse plasticity characteristic change in the course of time depending on the type of post synaptic activity momentarily elicited starting with weak synapsis which only elicit local d spike a slow unspecific growth process is induced a soon a the soma begin to spike this process is replaced by fast synaptic change a the consequence of the much stronger and sharper bp spike which now dominates the plasticity rule this way a winner take all mechanism emerges in a two stage process enhancing the best correlated input these result suggest that synaptic plasticity is a temporal changing process by which the computational property of dendrite or complete neuron can be substantially augmented 
convex learning algorithm such a support vector machine svms are often seen a highly desirable because they ofier strong practical property and are amenable to theoretical analysis however in this work we show how non convexity can provide scalability advantage over convexity we show how concave convex programming can be applied to produce i faster svms where training error are no longer support vector and ii much faster transductive svms 
we present an efficient generalization of the sparse pseudoinput gaussian process spgp model developed by snelson and ghahramani applying it to binary classification problem by taking advantage of the s pgp prior covariance structure we derive a numerically stable algorithm with o nm training complexity asymptotically the same a related sparse method such a the informative vector machine but which more faithfully represents the posterior we present experimental result for several benchmark problem showing that in many case this allows an exceptional degree of sparsity without compromising accuracy following we locate pseudo input by gradient ascent on the marginal likelihood but exhibit occasion when this is lik ely to fail for which we suggest alternative solution 
we present a nonparametric bayesian method of estimating variable order markov process up to a theoretically infinite order by extending a stick breaking prior which is usually defined on a unit interval vertically to t he tree of infinite depth associated with a hierarchical chinese restaurant process our model directly infers the hidden order of markov dependency from which each symbol originated experiment on character and word sequence in natural language showed that the model ha a comparative performance with an exponentially large full order model while computationally much efficient in both time and space we expect that this basic model will also extend to the variable order h ierarchical clustering of general data 
this paper proposes a pac bayes bound to measure the performance of support vector machine svm classifier the bound is based on learning a prior over the distribution of classifier with a part of the training sample experimental work show that this bound is tighter than the original pac bayes resulting in an enhancement of the predictive capability of the pac bayes bound in addition it is shown that the use of this bound a a mean to estimate the hyperparameters of the classifier compare favourably with cross validation in term of accuracy of the model while saving a lot of computational burden 
in ranking one is given example of order relationship among object and the goal is to learn from these example a real valued ranking function that induces a ranking or ordering over the object space we consider the problem of learning such a ranking function when the data is represented a a graph in which vertex correspond to object and edge encode similarity between object building on recent development in regularization theory for graph and corresponding laplacian based method for classification we develop an algorithmic framework for learning ranking function on graph data we provide generalization guarantee for our algorithm via recent result based on the notion of algorithmic stability and give experimental evidence of the potential benefit of our framework 
how do we find a natural clustering of a real world point set which contains an unknown number of cluster with dierent shape and which may be contaminated by noise most clustering algorithm were designed with certain assumption gaussianity they often require the user to give input parameter and they are sensitive to noise in this paper we propose a robust framework for determining a natural clustering of a given data set based on the minimum description length mdl principle the proposed framework robust information theoretic clustering ric is orthogonal to any known clustering algorithm given a preliminary clustering ric purifies these cluster from noise and adjusts the clustering such that it simultaneously determines the most natural amount and shape subspace of the cluster our ric method can be combined with any clustering technique ranging from k mean and k medoids to advanced method such a spectral clustering in fact ric is even able to purify and improve an initial coarse clustering even if we start with very simple method such a grid based space partitioning moreover ric scale well with the data set size extensive experiment on synthetic and real world data set validate the proposed ric framework 
fisher linear discriminant analysis lda can be sensitive to the problem data robust fisher lda can systematically alleviate the sensitivity problem by explicitly incorporating a model of data uncertainty in a classification problem and optimizing for the worst case scenar io under this model the main contribution of this paper is show that with general convex uncertainty model on the problem data robust fisher lda can be carried out using convex optimization for a certain type of product form uncertainty model robust fisher lda can be carried out at a cost comparable to standard fisher lda the method is demonstrated with some numerical example finally we show how to extend these result to robust kernel fisher discriminant analysis i e robust fisher lda in a high dimensional feature space 
this paper present a generalization of regression error characteristic rec curve rec curve describe the cumulative distribution function of the prediction error of model and can be seen a a generalization of roc curve to regression problem rec curve provide useful information for analyzing the performance of model particularly when compared to error statistic like for instance the mean squared error in this paper we present regression error characteristic rec surface that introduce a further degree of detail by plotting the cumulative distribution function of the error across the distribution of the target variable i e the joint cumulative distribution function of the error and the target variable this provides a more detailed analysis of the performance of model when compared to rec curve this extra detail is particularly relevant in application with non uniform error cost where it is important to study the performance of model for specific range of the target variable in this paper we present the notion of rec surface describe how to use them to compare the performance of model and illustrate their use with an important practical class of application the prediction of rare extreme value 
we introduce relational temporal difference learning a an effective approach to solving multi agent markov decision problem with large state space our algorithm us temporal difference reinforcement to learn a distributed value function represented over a conceptual hierarchy of relational predicate we present experiment using two domain from the general game playing repository in which we observe that our system achieves higher learning rate than non relational method we also discus related work and direction for future research 
in recent year the language model latent dirichlet allocation lda which cluster co occurring word into topic ha been widely applied in the computer vision field however many of these application have difficulty with modeling the spatial and temporal structure among visual word since lda assumes that a document is a bag of word it is also critical to properly design word and document when using a language model to solve vision problem in this paper we propose a topic model spatial latent dirichlet allocation slda which better encodes spatial structure among visual word that are essential for solving many vision problem the spatial information is not encoded in the value of visual word but in the design of document instead of knowing the partition of word into document a priori the word document assignment becomes a random hidden variable in slda there is a generative procedure where knowledge of spatial structure can be flexibly added a a prior grouping visual word which are close in space into the same document we use slda to discover object from a collection of image and show it achieves better performance than lda 
we introduce a method to automatically improve character model for a handwritten script without the use of transcription and u ing a minimum of document specific training data we show that we can use sea rches for the word in a dictionary to identify portion of the document whose transcription are unambiguous using template extracted from those region we retrain our character prediction model to drastically improve our search retrieval performance for word in the document 
the primary purpose of news article is to convey information about who what when and where but learning and summarizing these relationship for collection of thousand to million of article is dicult while statistical topic model have been highly successful at topically summarizing huge collection of text document they do not explicitly address the textual interaction between who where i e named entity person organization location and what i e the topic we present new graphical model that directly learn the relationship between topic discussed in news article and entity mentioned in each article we show how these entity topic model through a better understanding of the entity topic relationship are better at making prediction about entity 
given a huge real graph how can we derive a representative sample there are many known algorithm to compute interesting measure shortest path centrality betweenness etc but several of them become impractical for large graph thus graph sampling is essential the natural question to ask are a which sampling method to use b how small can the sample size be and c how to scale up the measurement of the sample e g the diameter to get estimate for the large graph the deeper underlying question is subtle how do we measure success we answer the above question and test our answer by thorough experiment on several diverse datasets spanning thousand node and edge we consider several sampling method propose novel method to check the goodness of sampling and develop a set of scaling law that describe relation between the property of the original and the sample in addition to the theoretical contribution the practical conclusion from our work are sampling strategy based on edge selection do not perform well simple uniform random node selection performs surprisingly well overall best performing method are the one based on random walk and forest fire they match very accurately both static a well a evolutionary graph pattern with sample size down to about of the original graph 
we propose a simple clustering framework on graph encoding pairwise data similarity unlike usual similarity based method the approach softly assigns data to cluster in a probabilistic way more importantly a hierarchical clustering is naturally derived in this framework to gradually merge lower level cluster into higher level one a random walk analysis indicates that the algorithm expose clustering structure in various resolution i e a higher level statistically model a longer term diffusion on graph and thus discovers a more global clustering structure finally we provide very encouraging experimental result 
recently we have introduced a novel approach to dynamic programming and reinforcement learning that is based on maintaining explicit representation of stationary distribution instead of value function in this paper we investigate the convergence property of these dual algorithm both theoretically and empirically and show how they can be scaled up by incorporating function approximation 
we apply classic online learning technique similar to the perceptron algorithm to the problem of learning a function defined on a graph the benefit of our approach includes simple algorithm and performance guarantee that we naturally interpret in term of structural property of the graph such a the algebraic connectivity or the diameter of the graph we also discus how these method can be modified to allow active learning on a graph we present preliminary experiment with encouraging result 
we measure the ability of human observer to predict the next datum in a sequence that is generated by a simple statistical process undergoing change at random point in time accurate performance in this task requires the identification of changepoints we ass individual difference between observer both empirically and using two kind of model a bayesian approach for change detection and a family of cognitively plausible fast and frugal model some individual detect too many change and hence perform sub optimally due to excess variability other individual do not detect enough change and perform sub optimally because they fail to notice short term temporal trend 
discriminative learning method for classification perfor m well when training and test data are drawn from the same distribution in many situation though we have labeled training data for a source domain and we wish to learn a classifier which performs well on a target domain with a different distribution under what condition can we adapt a classifier trained on the source dom ain for use in the target domain intuitively a good feature representation is a crucial factor in the success of domain adaptation we formalize this intuition theoretically with a generalization bound for domain adaption our theory illustrates the tradeoff inherent in designing a representation for domain adaptation and give a new justification for a recently proposed model it also point toward a promising new model for domain adaptation one which explicitly minimizes the difference between the source and target domain while at the same time maximizing the margin of the training set 
lstd is numerically instable for some ergodic markov chain with preferred visit among some state over the remaining one because the matrix that lstd accumulates ha large condition number in this paper we propose a variant of temporal difference learning with high data efficiency a class of preconditioned temporal difference learning algorithm are also proposed to speed up the new method it includes lspe and several new data efficient algorithm the data efficiency of these algorithm is validated by learning an absorbing markov chain also the asymptotic property of the new algorithm are analyzed 
predictive state representation psrs have shown a great deal of promise a an alternative to markov model however learning a psr from a single stream of data generated from an environment remains a challenge in this work we present a formalism of psrs and the domain they model this formalization suggests an algorithm for learning psrs that will almost surely converge to a globally optimal model given sufficient training data 
we consider the problem of choosing a linear classier that minimizes misclassication probability in two class classication which is a bi criterion problem involving a trade o between two objective we assume that the class conditional distribution are gaussian this assumption make it computationally tractable to nd pareto optimal linear classiers whose classication capability are inferior to no other linear one the main purpose of this paper is to establish several robustness property of those classiers with respect to variation and uncertainty in the distribution we also extend the result to kernel based classication finally we show how to carry out trade o analysis empirically with a nite number of given labeled data 
image represent an important and abundant source of data understanding their statistical structure ha important application such a image compression and restoration in this paper we propose a particular kind of probabilistic model dubbed the product of edge perts model to describe the structure of wavelet transformed image we develop a practical denoising algorithm based on a single edge pert and show state ofthe art denoising performance on benchmark image 
given observed data and a collection of parameterized candidate model a confidence region in parameter space provides useful insight a to those model which are a good fit to the data all while keeping the probability of incorrect exclusion below with complex model optimally precise procedure those with small expected size are in practice difficult to derive one solution is the minimax expected size me confidence procedure the key computational problem of me is computing a minimax equilibrium to a certain zero sum game we show that this game is convex with bilinear payoff allowing u to apply any convex game solver including linear programming exploiting the sparsity of the matrix along with using fast linear programming software allows u to compute approximate minimax expected size confidence region order of magnitude faster than previously published method we test these approach by estimating parameter for a cosmological model 
we present a new method to estimate the intrinsic dimensionality of a submanifold m in rd from random sample the method is based on the convergence rate of a certain u statistic on the manifold we solve at least partially the question of the choice of the scale of the data moreover the proposed method is easy to implement can handle large data set and performs very well even for small sample size we compare the proposed method to two standard estimator on several artificial a well a real data set 
our motor system change due to cause that span multiple timescales for example muscle response can change because of fatigue a condition where the disturbance ha a fast timescale or because of disease where the disturbance is much slower here we hypothesize that the nervous system adapts in a way that reflects the temporal property of such potential disturbance according to a bayesian formulation of this idea movement error result in a credit assignment problem what timescale is responsible for this disturbance the adaptation schedule influence the behavior of the optimal learner changing estimate at different timescales a well a the uncertainty a system that adapts in this way predicts many property observed in saccadic gain adaptation it well predicts the timecourses of motor adaptation in case of partial sensory deprivation and reversal of the adaptation direction 
outlier detection can uncover malicious behavior in field like intrusion detection and fraud analysis although there ha been a significant amount of work in outlier detection most of the algorithm proposed in the literature are based on a particular definition of outlier e g density based and use ad hoc threshold to detect them in this paper we present a novel technique to detect outlier with respect to an existing clustering model however the test can also be successfully utilized to recognize outlier when the clustering information is not available our method is based on transductive confidence machine which have been previously proposed a a mechanism to provide individual confidence measure on classification decision the test us hypothesis testing to prove or disprove whether a point is fit to be in each of the cluster of the model we experimentally demonstrate that the test is highly robust and produce very few misdiagnosed point even when no clustering information is available furthermore our experiment demonstrate the robustness of our method under the circumstance of data contaminated by outlier we finally show that our technique can be successfully applied to identify outlier in a noisy data set for which no information is available e g ground truth clustering structure etc a such our proposed methodology is capable of bootstrapping from a noisy data set a clean one that can be used to identify future outlier 
we outline a structured speech model a a special and perhaps extreme form of probabilistic generative modeling the model is equipped with long contextual span capability that are missing in the hmm approach compact and physically meaningful parameterization of the model is made possible by the continuity constraint in the hidden vocal tract resonance vtr domain the target directed vtr dynamic jointly characterize coarticulation and incomplete articulation reduction preliminary evaluation result are presented on the standard timit phonetic recognition task showing the best result in this task reported in the literature without using many heterogeneous classifier combination the pro and con of our structured generative modeling approach in comparison with the structured discriminative classification approach are discussed 
we describe an algorithm for converting linear support vector machine and any other arbitrary hyperplane based linear classifier into a set of non overlapping rule that unlike the original classifier can be easily interpreted by human each iteration of the rule extraction algorithm is formulated a a constrained optimization problem that is computationally inexpensive to solve we discus various property of the algorithm and provide proof of convergence for two different optimization criterion we demonstrate the performance and the speed of the algorithm on linear classifier learned from real world datasets including a medical dataset on detection of lung cancer from medical image the ability to convert svm s and other black box classifier into a set of human understandable rule is critical not only for physician acceptance but also to reducing the regulatory barrier for medical decision support system based on such classifier 
abstract we present a probabilistic approach to language change in which word form are represented by phoneme sequence that undergo stochastic edits along the branch of a phylogenetic tree this framework combine the advantage of the classical comparative method with the robustness of corpus based probabilistic model we use this framework to explore the consequence of two different scheme for defining probabilistic model of phonological change evaluating these scheme by reconstructing ancient word form of romance language the result is an efficient inference procedure for automatically inferring ancient word form from modern language which can be generalized to support inference about linguistic phylogeny 
in supervised kernel method it ha been observed that the performance of the svm classifier is poor in case where the diagonal entry of the gram matrix are large relative to the off diagonal entry this problem referred to a diagonal dominance often occurs when certain kernel function are applied to sparse high dimensional data such a text corpus in this paper we investigate the implication of diagonal dominance for unsupervised kernel method specifically in the task of document clustering we propose a selection of strategy for addressing this issue and evaluate their effectiveness in producing more accurate and stable clustering 
this paper promotes a new task for supervised machine learning research quantification the pursuit of learning method for accurately estimating the class distribution of a test set with no concern for prediction on individual case a variant for cost quantification address the need to total up cost according to category predicted by imperfect classifier these task cover a large and important family of application that measure trend over time the paper establishes a research methodology and us it to evaluate several proposed method that involve selecting the classification threshold in a way that would spoil the accuracy of individual classification in empirical test median sweep method show outstanding ability to estimate the class distribution despite wide disparity in testing and training condition the paper address shifting class prior and cost but not concept drift in general 
document such a those seen on wikipedia and folksonomy have tended to be assigned with multiple topic a a meta data therefore it is more and more important to analyze a relationship between a document and topic assigned to the document in this paper we proposed a novel probabilistic generative model of document with multiple topic a a meta data by focusing on modeling the generation process of a document with multiple topic we can extract specific property of document with multiple topic proposed model is an expansion of an existing probabilistic generative model parametric mixture model pmm pmm model document with multiple topic by mixing model parameter of each single topic since however pmm assigns the same mixture ratio to each single topic pmm cannot take into account the bias of each topic within a document to deal with this problem we propose a model that considers dirichlet distribution a a prior distribution of the mixture ratio we adopt variational bayes method to infer the bias of each topic within a document we evaluate the proposed model and pmm using medline corpus the result of f measure precision and recall show that the proposed model is more effective than pmm on multiple topic classification moreover we indicate the potential of the proposed model that extract topic and document specific keywords using information about the assigned topic 
we consider the sparse grid combination technique for regression which we regard a a problem of function reconstruction in some given function space we use a regularised least square approach discretised by sparse grid and solved using the so called combination technique where a certain sequence of conventional grid is employed the sparse grid solution is then obtained by addition of the partial solution with combination co efficients dependent on the involved grid this approach show instability in certain situation and is not guaranteed to converge with higher discretisation level in this article we apply the recently introduced optimised combination technique which repair these instability now the combination coefficient also depend on the function to be reconstructed resulting in a non linear approximation method which achieves very competitive result we show that the computational complexity of the improved method still scale only linear in regard to the number of data 
human make optimal perceptual decision in noisy and ambiguous condition computation underlying such optimal behavior have been shown to rely on bayesian probabilistic inference a key element of bayesian computation is the generative model that determines the statistical property of sensory experience the goal of perceptual learning can thus be framed a estimating the generative model from available data in previous study the generative model that subject had to infer wa relatively simple it structure wa also assumed to be known a priori so that only a few model parameter had to be estimated we investigated whether human are capable of inferring more complex generative model from experience in a completely unsupervised perceptual task subject learnt subtle statistical property of visual scene consisting of object that could only be identified by their statistical contingency not by lowlevel feature we show that human performance in this task can be accounted for by bayesian learning of model structure and parameter within a class of model that seek to explain observed variable by a minimum number of independent hidden cause 
conditional random field crfs are graphical model for modeling the probability of label given the observation they have traditionally been trained with using a set of observation and label pair underlying all crfs is the assumption that conditioned on the training data the label are independent and identically distributed iid in this paper we explore the use of crfs in a class of temporal learning algorithm namely policy gradient reinforcement learning rl now the label are no longer iid they are action that update the environment and affect the next observation from an rl point of view crfs provide a natural way to model joint action in a decentralized markov decision process they define how agent can communicate with each other to choose the optimal joint action our experiment include a synthetic network alignment problem a distributed sensor network and road traffic control clearly outperforming rl method which do not model the proper joint policy 
since the workshop on knowledge discovery in database the field ha seen sustained growth and interest and ha attained significant maturity the main objective of this panel will be to reflect on the success and failure in the field of data mining over the last eighteen year and to examine what insight we can take with u a we move forward 
we present a discrete spectral framework for the sparse or cardinality constrained solution of a generalized rayleigh quotient this np hard combinatorial optimization problem is central to supervised learning task such a sparse lda feature selection and relevance ranking for classification we derive a new generalized form of the inclusion principle for variational eigenvalue bound leading to exact and optimal sparse linear discriminants using branch and bound search an efficient greedy approximate technique is also presented the generalization performance of our sparse lda algorithm is demonstrated with real world uci ml benchmark and compared to a leading svm based gene selection algorithm for cancer classification 
bayesian estimator are defined in term of the posterior distribution typically this is written a the product of the likelihood function and a prior probability density both of which are assumed to be known but in many situation the prior density is not known and is difficult to learn from data since one doe not have access to uncorrupted sample of the variable being estimated we show that for a wide variety of observation model the bayes least square bls estimator may be formulated without explicit reference to the prior specifically we derive a direct expression for the estimator and a related expression for the mean squared estimation error both in term of the density of the observed measurement each of these prior free formulation allows u to approximate the estimator given a sufficient amount of observed data we use the first form to develop practical nonparametric approximation of bls estimator for several different observation process and the second form to develop a parametric family of estimator for use in the additive gaussian noise case we examine the empirical performance of these estimator a a function of the amount of observed data 
we present an intuitive scheme for lossy color image compression use the color information from a few representative pixel to learn a model which predicts color on the rest of the pixel now storing the representative pixel and the image in grayscale suffice to recover the original image a similar scheme is also applicable for compressing video where a single model can be used to predict color on many consecutive frame leading to better compression existing algorithm for colorization the process of adding color to a grayscale image or video sequence are tedious and require intensive human intervention we bypass these limitation by using a graph based inductive semi supervised learning module for colorization and a simple active learning strategy to choose the representative pixel experiment on a wide variety of image and video sequence demonstrate the efficacy of our algorithm 
we consider the scenario where training and test data are drawn from different distribution commonly referred to a sample selection bias most algorithm for this setting try to first recover sampling distribution and then make appropriate correction based on the distribution estimate we p resent a nonparametric method which directly produce resampling weight without distribution estimation our method work by matching distribution between training and testing set in feature space experimental result demonstrate th at our method work well in practice 
calculation of object similarity for example through a distance function is a common part of data mining and machine learning algorithm this calculation is crucial for efficiency since distance are usually evaluated a large number of time the classical example being query by example find object that are similar to a given query object moreover the performance of these algorithm depends critically on choosing a good distance function however it is often the case that the correct distance is unknown or chosen by hand and it calculation is computationally expensive e g such a for large dimensional object in this paper we propose a method for constructing relative distance preserving low dimensional mapping sparse mapping this method allows learning unknown distance function or approximating known function with the additional property of reducing distance computation time we present an algorithm that given example of proximity comparison among triple of object object i is more like object j than object k learns a distance function in a few dimension a possible that preserve these distance relationship the formulation is based on solving a linear programming optimization problem that find an optimal mapping for the given dataset and distance relationship unlike other popular embedding algorithm this method can easily generalize to new point doe not have local minimum and explicitly model computational efficiency by finding a mapping that is sparse i e one that depends on a small subset of feature or dimension experimental evaluation show that the proposed formulation compare favorably with a state of the art method in several publicly available datasets 
this paper considers the problem of selecting the most informative experiment x to get measurement y for learning a regression model y f x we propose a novel and simple concept for active learning transductive experimental design that explores available unmeasured experiment i e unlabeled data and ha a better scalability in comparison with classic experimental design method our in depth analysis show that the new method tends to favor experiment that are on the one side hard to predict and on the other side representative for the rest of the experiment efficient optimization of the new design problem is achieved through alternating optimization and sequential greedy search extensive experimental result on synthetic problem and three real world task including questionnaire design for preference learning active learning for text categorization and spatial sensor placement highlight the advantage of the proposed approach 
the need for mining causality beyond mere statistical correlation for real world problem ha been recognized widely many of these application naturally involve temporal data which raise the challenge of how best to leverage the temporal information for causal modeling recently graphical modeling with the concept of granger causality based on the intuition that a cause help predict it effect in the future ha gained attention in many domain involving time series data analysis with the surge of interest in model selection methodology for regression such a the lasso a practical alternative to solving structural learning of graphical model the question arises whether and how to combine these two notion into a practically viable approach for temporal causal modeling in this paper we examine a host of related algorithm that loosely speaking fall under the category of graphical granger method and characterize their relative performance from multiple viewpoint our experiment show for instance that the lasso algorithm exhibit consistent gain over the canonical pairwise graphical granger method we also characterize condition under which these variant of graphical granger method perform well in comparison to other benchmark method finally we apply these method to a real world data set involving key performance indicator of corporation and present some concrete result 
we present a unified duality view of several recently emerged spectral method for nonlinear dimensionality reduction including isomap locally linear embedding laplacian eigenmaps and maximum variance unfolding we discus the duality theory for the maximum variance unfolding problem and show that other method are directly related to either it primal formulation or it dual formulation or can be interpreted from the optimality condition this duality framework reveals close connection between these seemingly quite different algorithm in particular it resolve the myth about these method in using either the top eigenvectors of a dense matrix or the bottom eigenvectors of a sparse matrix these two eigenspaces are exactly aligned at primal dual optimality 
we describe hidden semi markov support vector machine shm svms an extension of hm svms to semi markov chain this allows u to predict segmentation of sequence based on segment based feature measuring property such a the length of the segment we propose a novel technique to partition the problem into sub problem the independently obtained partial solution can then be recombined in an efficient way which allows u to solve label sequence learning problem with several thousand of labeled sequence we have tested our algorithm for predicting gene structure an important problem in computational biology result on a well known model organism illustrate the great potential of shm svms in computational biology 
in genomic sequence analysis task like splice site recognition or promoter identification large amount of training sequence are available and indeed needed to achieve sufficiently high classification performance in this work we study two recently proposed and successfully used kernel namely the spectrum kernel and the weighted degree kernel wd in particular we suggest several extension using suffix tree and modification of an smo like svm training algorithm in order to accelerate the training of the svms and their evaluation on test sequence our simulation show that for the spectrum kernel and wd kernel large scale svm training can be accelerated by factor of and time respectively while using much le memory e g no kernel caching the evaluation on new sequence is often several thousand time faster using the new technique depending on the number of support vector our method allows u to train on set a large a one million sequence 
multinomial distribution over word are frequently used to model topic in text collection a common major challenge in applying all such topic model to any text mining problem is to label a multinomial topic model accurately so that a user can interpret the discovered topic so far such label have been generated manually in a subjective way in this paper we propose probabilistic approach to automatically labeling multinomial topic model in an objective way we cast this labeling problem a an optimization problem involving minimizing kullback leibler divergence between word distribution and maximizing mutual information between a label and a topic model experiment with user study have been done on two text data set with different genre the result show that the proposed labeling method are quite effective to generate label that are meaningful and useful for interpreting the discovered topic model our method are general and can be applied to labeling topic learned through all kind of topic model such a plsa lda and their variation 
user attempt to express their search goal through web search query when a search goal ha multiple component or aspect document that represent all the aspect are likely to be more relevant than those that only represent some aspect current web search engine often produce result set whose top ranking document represent only a subset of the query aspect by expanding the query using the right keywords the search engine can find document that represent more query aspect and performance improves this paper describes abraq an approach for automatically finding the right keywords to expand the query abraq identifies the aspect in the query identifies which aspect are underrepresented in the result set of the original query and finally for any particularly underrepresented aspect identifies keywords that would enhance that aspect s representation and automatically expands the query using the best one the paper present experiment that show abraq significantly increase the precision of hard query whereas traditional automatic query expansion technique have not improved precision abraq also compared favourably against a range of interactive query expansion technique that require user involvement including clustering web log analysis relevance feedback and pseudo relevance feedback 
we present a local learning approach for clustering the basic idea is that a good clustering result should have the property that the cluster label of each data point can be well predicted based on it neighboring data and their cluster label using current supervised learning method an optimization problem is formulated such that it solution ha the above property relaxation and eigen decomposition are applied to solve this optimization problem we also briefly investigate the parameter selection issue and provide a simple parameter selection method for the proposed algorithm experimental result are provided to validate the effectiveness of the proposed approach 
in many application unlabelled example are inexpensive and easy to obtain semi supervised approach try to utilise such example to reduce the predictive error in this paper we investigate a semi supervised least square regression algorithm based on the co learning approach similar to other semi supervised algorithm our base algorithm ha cubic runtime complexity in the number of unlabelled example to be able to handle larger set of unlabelled example we devise a semi parametric variant that scale linearly in the number of unlabelled example experiment show a significant error reduction by co regularisation and a large runtime improvement for the semi parametric approximation last but not least we propose a distributed procedure that can be applied without collecting all data at a single site 
the advance in kernel based learning necessitate the study on solving a large scale non sparse positive definite linear system to provide a deterministic approach recent research focus on designing fast matrix vector multiplication technique coupled with a conjugate gradient method instead of using the conjugate gradient method our paper proposes to use a domain decomposition approach in solving such a linear system it convergence property and speed can be understood within von neumann s alternating projection framework we will report signi ficant and consistent improvement in convergence speed over the conjugate gradient method when the approach is applied to recent machine learning problem 
it is becoming increasingly common to construct database from information automatically culled from many heterogeneous source for example a research publication database can be constructed by automatically extracting title author and conference information from online paper a common difficulty in consolidating data from multiple source is that record are referenced in a variety of way e g abbreviation alias and misspelling therefore it can be difficult to construct a single standard representation to present to the user we refer to the task of constructing this representation a canonicalization despite it importance there is little existing work on canonicalization in this paper we explore the use of edit distance measure to construct a canonical representation that is central in the sense that it is most similar to each of the disparate record this approach reduces the impact of noisy record on the canonical representation furthermore because the user may prefer different style of canonicalization we show how different edit distance cost can result in different form of canonicalization for example reducing the cost of character deletion can result in representation that favor abbreviated form over expanded form e g kdd versus conference on knowledge discovery and data mining we describe how to learn these cost from a small amount of manually annotated data using stochastic hill climbing additionally we investigate feature based method to learn ranking preference over canonicalizations these approach can incorporate arbitrary textual evidence to select a canonical record we evaluate our approach on a real world publication database and show that our learning method result in a canonicalization solution that is robust to error and easily customizable to user preference 
we present a simple new criterion for classification based on principle from lossy data compression the criterion assigns a test sample to the class that us the minimum number of additional bit to code the test sample subject to an allowable distortion we prove asymptotic optimality of this criterion for gaussian data and analyze it relationship to classical classifier theoretical result provide new insight into relationship among popular classifier such a map and rda a well a unsupervised clustering method based on lossy compression minimizing the lossy coding length induces a regularization effect which stabilizes the implicit density estimate in a small sample setting compression also provides a uniform mean of handling class of varying dimension this simple classification criterion and it kernel and local version perform competitively against existing classifier on both synthetic example and real imagery data such a handwritten digit and human face without requiring domain specific information 
we propose efficient algorithm for learning ranking functi ons from order constraint between set i e class of training sample our algorithm may be used for maximizing the generalized wilcoxon mann whitney statistic that account for the partial ordering of the class special case include maximizing the area under the roc curve for binary classification and it generalization for ordinal regressi on experiment on public benchmark indicate that a the proposed algorithm is at least a accurate a the current state of the art b computati onally it is several order of magnitude faster and unlike current method it is easily able to handle even large datasets with over sample 
active learning refers to algorithmic framework aimed at selecting training data point in order to reduce the number of required training data point and or improve the generalization performance of a learning method in this paper we present an asymptotic analysis of active learning for generalized linear model our analysis hold under the common practical situation of model misspecification and is based on realistic assumption regarding the nature of the sampling distribution which are usually neither independent nor i dentical we derive unbiased estimator of generalization performance a well a estimator of expected reduction in generalization error after adding a new traini ng data point that allow u to optimize it sampling distribution through a convex optimization problem our analysis naturally lead to an algorithm for sequential active learning which is applicable for all task supported by generalized linear model e g binary classification multi class classification regression and ca n be applied in non linear setting through the use of mercer kernel 
an analytical framework for using powerlaw theory to estimate market size for niche product and consumer group 
a significant challenge in developing planning system for practical application is the difficulty of acquiring the domain knowledge needed by such system one method for acquiring this knowledge is to learn it from plan trace but this method typically requires a huge number of plan trace to converge in this paper we show that the problem with slow convergence can be circumvented by having the learner generate solution plan even before the planning domain is completely learned our empirical result show that these improvement reduce the size of the training set that is needed to find correct answer to a large percentage of planning problem in the test set 
this talk is about recent work on new way to exploit preprocessed view of data table for tractably solving big statistical query we ll describe deployment of these new algorithm in the realm of detecting killer asteroid and unnatural disease outbreak in recent year several group have looked at method for pre storing general sufficient statistic of the data in spatial data structure such a kd tree and ball tree so that both frequentist and bayesian statistical operation become fast for large datasets in this talk we will look at two other class of optimization required in important statistical query the first involves iterating over all spatial region big and small the second involves detection of track from noisy intermittent observation separated far apart in time and space we will also discus the implication that have arisen from making these operation tractable we will focus particularly ondetecting all asteroid in the solar system larger than pittsburgh s cathedral of learning data to be collected over early detection of emerging disease based on national monitoring of health related transaction 
structural equation model can be seen a an extension of gaussian belief network to cyclic graph and we show they can be understood generatively a the model for the joint distribution of long term average equilibrium activity of gaussian dynamic belief network most use of structural equation model in fmri involves postulating a particular structure and comparing learnt parameter across different group in this paper it is argued that there are situation where prior about structure are not firm or exhaustive and given sufficient data it is worth investigating learning network structure a part of the approach to connectivity analysis first we demonstrate structure learning on a toy problem we then show that for particular fmri data the simple model usually assumed are not supported we show that is is possible to learn sensible structural equation model that can provide modelling benefit but that are not necessarily going to be the same a a true causal model and suggest the combination of prior model and learning or the use of temporal information from dynamic model may provide more benefit than learning structural equation alone 
kernel machine have been shown a the state of the art learning technique for classification in this paper we propose a novel general framework of learning the unified kernel machine ukm from both labeled and unlabeled data our proposed framework integrates supervised learning semi supervised kernel learning and active learning in a unified solution in the suggested framework we particularly focus our attention on designing a new semi supervised kernel learning method i e spectral kernel learning skl which is built on the principle of kernel target alignment and unsupervised kernel design our algorithm is related to an equivalent quadratic programming problem that can be efficiently solved empirical result have shown that our method is more effective and robust to learn the semi supervised kernel than traditional approach based on the framework we present a specific paradigm of unified kernel machine with respect to kernel logistic regresions klr i e unified kernel logistic regression uklr we evaluate our proposed uklr classification scheme in comparison with traditional solution the promising result show that our proposed uklr paradigm is more effective than the traditional classification approach 
the locally linear embedding lle is improved by introducing multiple linearly independent local weight vector for each neighborhood we characterize the reconstruction weight and show the existence of the linearly independent weight vector at each neighborhood the modified locally linear embedding mlle proposed in this paper is much stable it can retrieve the ideal embedding if mlle is applied on data point sampled from an isometric manifold mlle is also compared with the local tangent space alignment ltsa numerical example are given that show the improvement and efficiency of mlle 
auditory scene analysis is extremely challenging one approach perhaps that adopted by the brain is to shape useful representation of sound on prior knowledge about their statistical structure for example sound with harmonic section are common and so time frequency representation are efficient most current representation concentrate on the shorter component here we propose representation for structure on longer time scale like the phoneme and sentence of speech we decompose a sound into a product of process each with it own characteristic time scale this demodulation cascade relates to classical amplitude demodulation but traditional algorithm fail to realise the representation fully a new approach probabilistic amplitude demodulation is shown to out perform the established method and to easily extend to representation of a full demodulation cascade 
green s function for the laplace operator represents the propagation of influence of point source and is the foundation for solving many physic problem on a graph of pairwise similarity the green s function is the inverse of the combinatorial laplacian we resolve the zero mode difficulty by showing it physical origin a the consequence of the von neumann boundary condition we propose to use green s function to propagate label information for both semi supervised and unsupervised learning we also derive this learning framework from the kernel regularization using reproducing kernel hilbert space theory at strong regularization limit green s function provides a well defined distance metric on a generic weighted graph either a the effective distance on the network of electric resistor or the average commute time in random walk we show that for unsupervised learning this approach is identical to ratio cut and normalized cut spectral clustering algorithm experiment on newsgroups and six uci datasets illustrate the effectiveness of this approach finally we propose a novel item based recommender system using green s function and show it effectiveness 
variational method are frequently used to approximate or bound the partition or likelihood function of a markov random field method base d on mean field theory are guaranteed to provide lower bound whereas certain type of convex relaxation provide upper bound in general loopy belief propagation bp provides often accurate approximation but not bound we prove that for a class of attractive binary model the so called bethe approximation associated with any fixed point of loopy bp always lower bound the true likelihoo d empirically this bound is much tighter than the naive mean field bound and requires no further work than running bp we establish these lower bound using a loop series expansion due to chertkov and chernyak which we show can be derived a a consequence of the tree reparameterization characterization of bp fixed point 
we describe a general framework for online multiclass learning based on the notion of hypothesis sharing in our framework set of class are associated with hypothesis thus all class within a given set share the same hypothesis this framework includes a special case commonly used construction for multiclass categorization such a allocating a unique hypothesis for each class and allocating a single common hypothesis for all class we generalize the multiclass perceptron to our framework and derive a unifying mistake bound analysis our construction naturally extends to setting where the number of class is not known in advance but rather is revealed along the online learning process we demonstrate the merit of our approach by comparing it to previous method on both synthetic and natural datasets 
pattern of contrast are a very important way of comparing multi dimensional datasets such pattern are able to capture region of high difference between two class of data and are useful for human expert and the construction of classifier however mining such pattern is particularly challenging when the number of dimension is large this paper describes a new technique for mining several variety of contrast pattern based on the use of zero suppressed binary decision diagram zbdds a powerful data structure for manipulating sparse data we study the mining of both simple contrast pattern such a emerging pattern and more novel and complex contrast which we call disjunctive emerging pattern a performance study demonstrates our zbdd technique is highly scalable substantially improves on state of the art mining for emerging pattern and can be effective for discovering complex contrast from datasets with thousand of attribute 
network with complex topology describe system a diverse a the cell the world wide web or the society the emergence of most network is driven by self organizing process that are governed by simple but generic law the analysis of the cellular network of various organism show that cell and complex man made network such a the internet or the world wide web and many social and collaboration network share the same large scale topology i will show that the scale free topology of these complex web have important consequence on their robustness against failure and attack with implication on drug design the internet s ability to survive attack and failure and the ability of idea and innovation to spread on the network 
structural alignment are the most widely used tool for comparing protein with low sequence similarity the main contribution of this paper is to derive various kernel on protein from structural alignment which do not use sequence information central to the kernel is a novel alignment algorithm which match substructure of fixed size using spectral graph matching technique we derive positive semi definite kernel which capture the notion of similarity between substructure using these a base more sophisticated kernel on protein structure are proposed to empirically evaluate the kernel we used a sequence non redundant structure from different scop superfamily the kernel when used with svms show competitive performance with ce a state of the art structure comparison program 
we present a kernel based algorithm for hierarchical text classification where the document are allowed to belong to more than one category at a time the classification model is a variant of the maximum margin markov network framework where the classification hierarchy is represented a a markov tree equipped with an exponential family defined on the edge we present an efficient optimization algorithm based on incremental conditional gradient ascent in single example subspace spanned by the marginal dual variable experiment show that the algorithm can feasibly optimize training set of thousand of example and classification hierarchy consisting of hundred of node the algorithm s predictive accuracy is competitive with other recently introduced hierarchical multi category or multilabel classification learning algorithm 
algorithm ucb for multi armed bandit problem ha already been extended to algorithm uct which work for minimax tree search we have developed a monte carlo program mogo which is the first computer go program using uct we explain our modification of uct for go application among which ecient memory management parametrization ordering of non visited node and parallelization mogo is now a top level computer go program on go board 
the problem of obtaining the maximum a posteriori estimate of a general discrete random field i e a random field defined using a finite and discrete set of label is known to be np hard however due to it central importance in many application several approximate algorithm have been proposed in the literature in this paper we present an analysis of three such algo rithms based on convex relaxation i lp s the linear programming lp relaxation proposed by schlesinger for a special case and independently in for the general case ii qp rl the quadratic programming qp relaxation by ravikumar and lafferty and iii socp m the second order cone programming socp relaxation first proposed by muramatsu and suzuki for two l abel problem and later extended in for a general label set we show that the socp m and the qp rl relaxation are equivalent furthermore we prove that despite the flexibility in the form of the c onstraints objective function offered by qp and socp the lp s relaxation strictly dominates i e provides a better approximation than qp rl and socp m we generalize these result by defining a large class of socp and equivalent qp relaxation which is dominated by the lp s relaxation based on these result we propose some novel socp relaxation which strictly dominate the previous approach 
can we leverage learning technique to build a fast nearest neighbor nn retrieval data structure we present a general learning framework for the nn problem in which sample query are used to learn the parameter of a data structure that minimize the retrieval time and or the miss rate we explore the potential of this novel framework through two popular nn data structure kd tree and the rectilinear structure employed by locality sensitive hashing we derive a generalization theory for these data structure class and present simple learning algorithm for both experimental result reveal that learning often improves on the already strong performance of these data structure 
we consider reinforcement learning in system with unknown dynamic algorithm such a e kearns and singh learn near optimal policy by using exploration policy to drive the system towards poorly modeled state so a to encourage exploration but this make these algorithm impractical for many system for example on an autonomous helicopter overly aggressive exploration may well result in a crash in this paper we consider the apprenticeship learning setting in which a teacher demonstration of the task is available we show that given the initial demonstration no explicit exploration is necessary and we can attain near optimal performance compared to the teacher simply by repeatedly executing exploitation policy that try to maximize reward in finite state mdps our algorithm scale polynomially in the number of state in continuous state linear dynamical system it scale polynomially in the dimension of the state these result are proved using a martingale construction over relative loss 
inference task in markov random field mrfs are closely related to the constraint satisfaction problem csp and it soft generalization in particular map inference in mrf is equivalent to the weighted maxsum csp a well known tool to tackle csps are arc consistency algorithm a k a relaxation labeling a promising approach to map inference in mrfs is linear programming relaxation solved by sequential treereweighted message passing trw s there is a not widely known algorithm equivalent to trw s max sum diffusion which is slower but very simple we give two theoretical result first we show that arc consistency algorithm and max sum diffusion become the same thing if formulated in an abstractalgebraic way thus we argue that max sum arc consistency algorithm or max sum relaxation labeling is a more suitable name for max sum diffusion second we give a criterion that strictly decrease during these algorithm it turn out that every class of equivalent problem contains a unique problem that is minimal w r t this criterion 
a number of exact and approximate method are available for inference calculation in graphical model many recent approximate method for graph with cycle are based on tractable algorithm for tree structured graph here we base the approximation on a different tractable model planar graph with binary variable and pure interaction potential no external field the partition function for such model can be calculated exactly using an algorithm introduced by fisher and kasteleyn in the s we show how such tractable planar model can be used in a decomposition to derive upper bound on the partition function of non planar model the resulting algorithm also allows for the estimation of marginals we compare our planar decomposition to the tree decomposition method of wainwright et al showing that it result in a much tighter bound on the partition function improved pairwise marginals and comparable singleton marginals 
although there ha been substantial progress in understanding the neurophysiological mechanism of stereopsis how neuron interact in a network during stereo computation remains unclear computational model on stereopsis suggest local competition and long range cooperation are important for resolving ambiguity during stereo matching to test these prediction we simultaneously recorded from multiple neuron in v of awake behaving macaque while presenting surface of different depth rendered in dynamic random dot stereograms we found that the interaction between pair of neuron wa a function of similarity in receptive field a well a of the input stimulus neuron coding the same depth experienced common inhibition early in their response for stimulus presented at their nonpreferred disparity they experienced mutual facilitation later in their response for stimulation at their preferred disparity these finding are consistent with a local competition mechanism that first remove gross mismatch and a global cooperative mechanism that further refines depth estimate 
ordering and ranking item of different type are important task in various application such a query processing and scientific data mining a total order for the item can be misleading since there are group of item that have practically equal rank we consider bucket order i e total order with tie they can be used to capture the essential order information without overfitting the data they form a useful concept class between total order and arbitrary partial order we address the question of finding a bucket order for a set of item given pairwise precedence information between the item we also discus method for computing the pairwise precedence data we describe simple and efficient algorithm for finding good bucket order several of the algorithm have a provable approximation guarantee and they scale well to large datasets we provide experimental result on artificial and a real data that show the usefulness of bucket order and demonstrate the accuracy and efficiency of the algorithm 
machine learning technique are increasingly being used to produce a wide range of classifier for complex real world application that inv olve nonuniform testing cost and misclassification cost a the complexity of thes e application grows the management of resource during the learning and classifi cation process becomes a challenging task in this work we introduce act anytime cost sensitive tree a novel framework for operating in such environment act is an anytime algorithm that allows trading computation time for lower classification cost it build a tree top down and exploit additional time resource to obtain better estimation for the utility of the different candidate split u sing sampling technique act approximates for each candidate split the cost of the subtree under it and favor the one with a minimal cost due to it stochastic nature act is expected to be able to escape local minimum into which greedy method may be trapped experiment with a variety of datasets were conducted to compare the performance of act to that of the state of the art cost sensitive tree lear ners the result show that for most domain act produce tree of significantly low er cost act is also shown to exhibit good anytime behavior with diminishing return 
there is a long standing controversy on the site of the cerebellar motor learning different theory and experimental result suggest that either the cerebellar flocculus or the brainstem learns the task and store the memory with a dynamical system approach we clarify the mechanism of transferring the memory generated in the flocculus to the brainstem and that of so called saving phenomenon the brainstem learning must comply with a sort of hebbian rule depending on purkinje cell activity in contrast to earlier numerical model our model is simple but it accommodates explanation and prediction of experimental situation a qualitative feature of trajectory in the phase space of synaptic weight without fine parameter tuning 
we present a probabilistic model applied to the fmri video rating prediction task of the pittsburgh brain activity interpretation competiti on pbaic our goal is to predict a time series of subjective semantic rati ng of a movie given functional mri data acquired during viewing by three subject our method us conditionally trained gaussian markov random field which model both the relationship between the subject fmri voxel measurement an d the rating a well a the dependency of the rating across time step and between subject we also employed non traditional method for feature selection and regularization that exploit the spatial structure of voxel activity in the brain t he model displayed good performance in predicting the scored rating for the three s ubjects in test data set and a variant of this model wa the third place entrant to the pbaic 
in order to represent state in controlled partially observ able stochastic dynamical system some sort of sufficient statistic for history is nec essary predictive representation of state psrs capture state a statistic of th e future we introduce a new model of such system called the exponential family psr which defines a state the time varying parameter of an exponential family distribution which model n sequential observation in the future this choice of state representation explicitly connects psrs to state of the art probabilist ic modeling which allows u to take advantage of current effort in high dimensional density estimation and in particular graphical model and maximum entropy model we present a parameter learning algorithm based on maximum likelihood and we show how a variety of current approximate inference method apply we evaluate the quality of our model with reinforcement learning by directly evaluating the control performance of the model 
basis function derived from an undirected graph connecting nearby sample from a markov decision process mdp have proven useful for approximating value function the success of this technique is attributed to the smoothness of the basis function with respect to the state space geometry this paper explores the property of base created from directed graph which are a more natural fit for expressing state connectivity digraph capture the effect of non reversible mdps whose value function may not be smooth across adjacent state we provide an analysis using the dirichlet sum of the directed graph laplacian to show how the smoothness of the basis function is affected by the graph s invariant distribution experiment in discrete and continuous mdps with non reversible action demonstrate a significant improvement in the policy learned using directed graph base 
we address classification problem for which the training instance are governed by a distribution that is allowed to differ arbitrarily from the test distribution problem also referred to a classification under covariate shift we derive a solution that is purely discriminative neither training nor test distribution are modeled explicitly we formulate the general problem of learning under covariate shift a an integrated optimization problem we derive a kernel logistic regression classifier for differing training and test distribution 
to unravel the concept structure and dynamic of the bioinformatics field we analyze a set of publication from the web of science and medline database publication year for delineating this complex interdisciplinary field a novel bibliometric retrieval strategy is used given that the performance of unsupervised clustering and classification of scientific publication is significantly improved by deeply merging textual content with the structure of the citation graph we proceed with a hybrid clustering method based on fisher s inverse chi square the optimal number of cluster is determined by a compound semiautomatic strategy comprising a combination of distance based and stability based method we also investigate the relationship between number of latent semantic indexing factor number of cluster and clustering performance the hit and pagerank algorithm are used to determine representative publication in each cluster next we develop a methodology for dynamic hybrid clustering of evolving bibliographic data set the same clustering methodology is applied to consecutive period defined by time window on the set and in a subsequent phase chain are formed by matching and tracking cluster through time term network for the eleven resulting cluster chain present the cognitive structure of the field finally we provide a view on how much attention the bioinformatics community ha devoted to the different subfields through time 
this paper proposes a new approach to feature selection based on a statistical feature mining technique for sequence and tree kernel since natural language data take discrete structure convolution kernel such a sequence and tree kernel are advantageous for both the concept and accuracy of many natural language processingtasks however experiment have shown that the best result can only be achieved when limited small sub structure are dealt with by these kernel this paper discus this issue of convolution kernel and then proposes a statistical feature selection that enable u to use larger sub structure effectively the proposed method in order to execute efficiently can be embedded into an original kernel calculation process by using sub structure mining algorithm experiment on real nlp task confirm the problem in the conventional method and compare the performance of a conventional method to that of the proposed method 
this paper proposes a new map building framework for mobile robot named localization free mapping by dimensionality reduction lfmdr in this framework the robot map building is interpreted a a problem of reconstructing the d coordinate of object so that they maximally preserve the local proximity of the object in the space of robot s observation history not only traditional linear pca but also recent manifold learning technique can be used for solving this problem in contrast to the slam framework lfmdr framework doe not require localization procedure nor explicit measurement and motion model in the latter part of this paper we will demonstrate visibility only and bearing only localization free mapping which are derived by applying lfmdr framework to the visibility and bearing measurement respectively 
this paper present a support vector method for optimizing multivariate nonlinear performance measure like the f score taking a multivariate prediction approach we give an algorithm with which such multivariate svms can be trained in polynomial time for large class of potentially non linear performance measure in particular rocarea and all measure that can be computed from the contingency table the conventional classification svm arises a a special case of our method 
consider the problem of joint parameter estimation and prediction in a markov random field i e the model parameter are estimated on the basis of an initial set of data and then the fitted model is used to perform prediction e g smoothing denoising interpolation on a new noisy observation working in the computation limited setting we analyze a joint method in which the same convex variational relaxationis used to construct an m estimator for fitting parameter and to perform approximate marginalization for the prediction step the key result of this paper is that in the computation limited setting using an inconsistent parameter estimator i e an estimator that return the wrong model even in the infinite data limit is provably beneficial since the resulting error can par tially compensate for error made by using an approximate prediction technique en route to this result we analyze the asymptotic property of m estimator based on convex variational relaxation and establish a lipschitz stability p roperty that hold for a broad class of variational method we show that joint estimation prediction based on the reweighted sum product algorithm substantially outperforms a commonly used heuristic based on ordinary sum product 
the co training algorithm us unlabeled example in multiple view to bootstrap classifier in each view typically in a greedy manner and operating under assumption of view independence and compatibility in this paper we propose a co regularization framework where classifier are learnt in each view through form of multi view regularization we propose algorithm within this framework that are based on optimizing measure of agreement and smoothness over labeled and unlabeled example these algorithm naturally extend standard regularization method like support vector machine svm and regularized least square rls for multi view semi supervised learning and inherit their benefit and applicability to high dimensional classification problem an empirical investigation is presented that confirms the promise of this approach 
cascade detector have been shown to operate extremely rapidly with high accuracy and have important application such a face detection driven by this success cascade learning ha been an area of active research in recent year nevertheless there are still challenging technical problem during the training process of cascade detector in particular determining the optimal target detection rate for each stage of the cascade remains an unsolved issue in this paper we propose the multiple instance pruning mip algorithm for soft cascade this algorithm computes a set of threshold which aggressively terminate computation with no reduction in detection rate or increase in false positive rate on the training dataset the algorithm is based on two key insight i example that are destined to be rejected by the complete classifier can be safely pruned early ii face detection is a multiple instance learning problem the mip process is fully automatic and requires no assumption of probability distribution statistical independence or ad hoc intermediate rejection target experimental result on the mit cmu dataset demonstrate significant performance advantage 
we describe a causal learning method which employ measuring the strength of statistical dependence in term of the hilbert schmidt norm of kernel based cross covariance operator following the line of the common faithfulness assumption of constraint based causal learning our approach assumes that a variable z is likely to be a common effect of x and y if conditioning on z increase the dependence between x and y based on this assumption we collect vote for hypothetical causal direction and orient the edge by the majority principle in most experiment with known causal structure our method provided plausible result and outperformed the conventional constraint based pc algorithm 
maximum margin clustering wa proposed lately and ha shown promising performance in recent study it extends the theory of support vector machine to unsupervised learning despite it good performance there are three major problem with maximum margin clustering that question it e ciency for real world application first it is computationally expensive and di cult to scale to large scale datasets because the number of parameter in maximum margin clustering is quadratic in the number of example second it requires data preprocessing to ensure that any clustering boundary will pas through the origin which make it unsuitable for clustering unbalanced dataset third it is sensitive to the choice of kernel function and requires external procedure to determine the appropriate value for the parameter of kernel function in this paper we propose generalized maximum margin clustering framework that address the above three problem simultaneously the new framework generalizes the maximum margin clustering algorithm by allowing any clustering boundary including those not passing through the origin it signiflcantly improves the computational e ciency by reducing the number of parameter furthermore the new framework is able to automatically determine the appropriate kernel matrix without any labeled data finally we show a formal connection between maximum margin clustering and spectral clustering we demonstrate the e ciency of the generalized maximum margin clustering algorithm using both synthetic datasets and real datasets from the uci repository 
markov logic network mlns combine logic and probability by attaching weight to first order clause and viewing these a template for feature of markov network in this paper we develop an algorithm for learning the structure of mlns from relational database combining idea from inductive logic programming ilp and feature induction in markov network the algorithm performs a beam or shortest first search of the space of clause guided by a weighted pseudo likelihood measure this requires computing the optimal weight for each candidate structure but we show how this can be done efficiently the algorithm can be used to learn an mln from scratch or to refine an existing knowledge base we have applied it in two real world domain and found that it outperforms using off the shelf ilp system to learn the mln structure a well a pure ilp purely probabilistic and purely knowledge based approach 
density based distance metric have application in semi supervised learning nonlinear interpolation and clustering we consider density based metric induced by riemannian manifold structure and estimate them using kernel density estimator for the underlying data distribution we lower bound the rate of convergence of these plug in path length estimate and hence of the metric a the sample size increase we present an upper bound on the rate of convergence of all estimator of the metric we also show that the metric can be consistently computed using the shortest path algorithm on a suitably constructed graph on the data sample and lower bound the convergence rate of the computation error we present experiment illustrating the use of the metric for semi supervised classification and non linear interpolation 
abstract we address the task of learning ranking of document from search engine log of user behavior previous work on this problem ha relied on passively collected clickthrough data in contrast we show that an active exploration strategy can provide data that lead to much faster learning specifically we develop a bayesian approach for selecting ranking to present user so that interaction result in more informative training data our result using the trec web corpus a well a synthetic data demonstrate that a directed exploration strategy quickly lead to user being presented improved ranking in an online learning setting we find that active exploration substantially outperforms passive observation and random exploration category and subject descriptor 
ranking node in graph is of much recent interest edge via the graph laplacian are used to encourage local smoothness of node score in svm like formulation with generalization guarantee in contrast page rank variant are based on markovian random walk for directed graph there is no simple known correspondence between these view of scoring ranking recent scalable algorithm for learning the pagerank transition probability do not have generalization guarantee in this paper we show some correspondence result between the laplacian and the pagerank approach and give new generalization guarantee for the latter we enhance the pagerank learning approach to use an additive margin we also propose a general framework for rank sensitive score learning and apply it to laplacian smoothing experimental result are promising 
this paper describes a successful but challenging application of data mining in the railway industry the objective is to optimize maintenance and operation of train through prognostic of wheel failure in addition to reducing maintenance cost the proposed technology will help improve railway safety and augment throughput building on established technique from data mining and machine learning we present a methodology to learn model to predict train wheel failure from readily available operational and maintenance data this methodology address various data mining task such a automatic labeling feature extraction model building model fusion and evaluation after a detailed description of the methodology we report result from large scale experiment these result clearly show the great potential of this innovative application of data mining in the railway industry 
we describe a novel unsupervised method for learning sparse overcomplete feature the model us a linear encoder and a linear decoder preceded by a sparsifying non linearity that turn a code vector into a quasibinary sparse code vector given an input the optimal code minimizes the distance between the output of the decoder and the input patch while being a similar a possible to the encoder output learning proceeds in a two phase em like fashion compute the minimum energy code vector adjust the parameter of the encoder and decoder so a to decrease the energy the model produce stroke detector when trained on handwritten numeral and gabor like filter whe n trained on natural image patch inference and learning are very fast requiring no preprocessing and no expensive sampling using the proposed unsupervised method to initialize the first layer of a convolutional network we achieved an err or rate slightly lower than the best reported result on the mnist dataset finally an extension of the method is described to learn topographical filter map 
point process encoding model provide powerful statistical method for understanding the response of neuron to sensory stimulus although these model have been successfully applied to neuron in the early sensory pathway they have fared le well capturing the response property of neuron in deeper brain area owing in part to the fact that they do not take into account multiple stage of processing here we introduce a new twist on the point process modeling approach we include unobserved a well a observed spiking neuron in a joint encoding model the resulting model exhibit richer dynamic and more highly nonlinear response property making it more powerful and more flexible for fitting neural data more importantly it allows u to estimate connectivity pattern among neuron both observed and unobserved and may provide insight into how network process sensory input we formulate the estimation procedure using variational em and the wake sleep algorithm and illustrate the model s performance using a simulated example network consisting of two coupled neuron 
although kernel measure of independence have been widely applied in machine learning notably in kernel ica there is a yet no method to determine whether they have detected statistically significant dependence w e provide a novel test of the independence hypothesis for one particular kernel independence measure the hilbert schmidt independence criterion hsic the resulting test cost o m wherem is the sample size we demonstrate that this test outperforms established contingency table and functional correlation based test and that this advantage is greater for multivariate data finally we show the hsic test also applies to text and to structured data more generally for which no other independence test presently exists 
we investigate using gradient descent method for learning ranking function we propose a simple probabilistic cost function and we introduce ranknet an implementation of these idea using a neural network to model the underlying ranking function we present test result on toy data and on data from a commercial internet search engine 
loopy belief propagation ha been employed in a wide variety of application with great empirical success but it come with few theoretical guarantee in this paper we investigate the use of the max product form of belief propagation for weighted matching problem on general graph we show that max product converges to the correct answer if the linear programming lp relaxation of the weighted matching problem is tight and doe not converge if the lp relaxation is loose this provides an exact characterization of max product performance and reveals connection to the widely used optimization technique of lp relaxation in addition we demonstrate that max product is effective in solving practical w eighted matching problem in a distributed fashion by applying it to the problem of self organization in sensor network 
kernelizing partial least square pls an algorithm which ha been particularly popular in chemometrics lead to kernel pls which ha several interesting property including a sub cubic runtime for learning and an iterative construction of direction which are relevant for predicting the output we show that the kernelization of pls introduces interesting property not found in ordinary pls giving novel insight into the working of kernel pls and the connection to kernel ridge regression and conjugate gradient descent method furthermore we show how to correctly define the degree of freedom for kernel pls and how to efficiently compute an unbiased estimate finally we address the practical problem of model selection we demonstrate how to use the degree of freedom estimate to perform effective model selection and discus how to implement crossvalidation scheme efficiently 
privacy becomes a more and more serious concern in application involving microdata recently efficient anonymization ha attracted much research work most of the previous method use global recoding which map the domain of the quasi identifier attribute to generalized or changed value however global recoding may not always achieve effective anonymization in term of discernability and query answering accuracy using the anonymized data moreover anonymized data is often for analysis a well accepted in many analytical application different attribute in a data set may have different utility in the analysis the utility of attribute ha not been considered in the previous method in this paper we study the problem of utility based anonymization first we propose a simple framework to specify utility of attribute the framework cover both numeric and categorical data second we develop two simple yet efficient heuristic local recoding method for utility based anonymization our extensive performance study using both real data set and synthetic data set show that our method outperform the state of the art multidimensional global recoding method in both discernability and query answering accuracy furthermore our utility based method can boost the quality of analysis using the anonymized data 
autonomous off road navigation of robotic ground vehicle ha important application on earth and in space exploration progress in this domain ha been retarded by the limited lookahead range of d sensor and by the difficulty of preprogramming system to understand the traversability of the wide variety of terrain they can encounter enabling robot to learn from experience may alleviate both of these problem we define two paradigm for this learning from d geometry and learning from proprioception and describe initial instantiation of them we have developed under darpa and nasa program field test result show promise for learning traversability of vegetated terrain learning to extend the lookahead range of the vision system and learning how slip varies with slope 
we introduce coherent point drift cpd a novel probabilistic method for nonrigid registration of point set the registration is treated a a maximum likelihood ml estimation problem with motion coherence constraint over the velocity eld such that one point set move coherently to align with the second set we formulate the motion coherence constraint and derive a solution of regularized ml estimation through the variational approach which lead to an elegant kernel form we also derive the em algorithm for the penalized ml optimization with deterministic annealing the cpd method simultaneously nd both the non rigid transformation and the correspondence between two point set without making any prior assumption of the transformation model except that of motion coherence this method can estimate complex non linear non rigid transformation and is shown to be accurate on d and d example and robust in the presence of outlier and missing point 
several algorithm for learning near optimal policy in markov decision process have been analyzed and proven efficient empirical result have suggested that model based interval estimation mbie learns efficiently in practice effectively balancing exploration and exploitation this paper present the first theoretical analysis of mbie proving it efficiency even under worst case condition the paper also introduces a new performance metric average loss and relates it to it le online cousin from the literature 
we usually endow the investigated object with pairwise relationship which can be illustrated a graph in many real world problem however relationship among the object of our interest are more complex than pair wise naively squeezing the complex relationship into pairwise one will inevitably lead to loss of information which can be expected valuable for our learning task however therefore we consider using hypergraphs in stead to completely represent complex relationship among the object of our interest and thus the problem of learning with hypergraphs arises our main contribution in this paper is to generalize the powerful methodology of spectral clustering which originally operates on undirected graph to hy pergraphs and further develop algorithm for hypergraph embedding and transductive classi cation on the basis of the spectral hypergraph cluster ing approach our experiment on a number of benchmark showed the advantage of hypergraphs over usual graph 
we consider boosting algorithm that maintain a distribution over a set of example at each iteration a weak hypothesis is received and the distribution is updated we motivate these update a minimizing the relative entropy subject to linear constraint for example adaboost constrains the edge of the last hypothesis w r t the updated distribution to be at most in some sense adaboost is corrective w r t the last hypothesis a cleaner boosting method is to be totally corrective the edge of all past hypothesis are constrained to be at most where is suitably adapted using new technique we prove the same iteration bound for the totally corrective algorithm a for their corrective version moreover with adaptive the algorithm provably maximizes the margin experimentally the totally corrective version return smaller convex combination of weak hypothesis than the corrective one and are competitive with lpboost a totally corrective boosting algorithm with no regularization for which there is no iteration bound known 
we examine the problem of predicting local sentiment flow in d ocuments and it application to several area of text analysis formally th e problem is stated a predicting an ordinal sequence based on a sequence of word set in the spirit of isotonic regression we develop a variant of conditional ra ndom field that is wellsuited to handle this problem using the mobius transform we express the model a a simple convex optimization problem experiment demonstrate the model and it application to sentiment prediction style analysis and text summarization 
we propose a method for the classification of matrix we use a linear classifier with a novel regularization scheme based on the spectral l norm of it coefficient matrix the spectral regularization not only provides a principled way of complexity control but also enables automatic determination of the rank of the coefficient matrix using the linear matrix inequality technique we formulate the inference task a a single convex optimization problem we apply our method to the motor imagery eeg classification problem the method not only improves upon conventional method in the classification performance but also determines a subspace in the signal that concentrate discriminative information without any additional feature extraction step the method can be easily generalized to regression problem by changing the loss function connection to other method are also discussed 
discovery of sequential pattern is an essential data mining task with broad application among several variation of sequential pattern closed sequential pattern is the most useful one since it retains all the information of the complete pattern set but is often much more compact than it unfortunately there is no parallel closed sequential pattern mining method proposed yet in this paper we develop an algorithm called par csp parallel closed sequential pattern mining to conduct parallel mining of closed sequential pattern on a distributed memory system par csp partition the work among the processor by exploiting the divide and conquer property so that the overhead of interprocessor communication is minimized par csp applies dynamic scheduling to avoid processor idling moreover it employ a technique called selective sampling to address the load imbalance problem we implement par csp using mpi on a node linux cluster our experimental result show that par csp attains good parallelization efficiency on various input datasets 
we initiate the study of learning from multiple source of limited data each of which may be corrupted at a different rate we develop a complete theory of which data source should be used for two fundamental problem estimating the bias of a coin and learning a classifier in the presence of label noise in both case efficient algorithm are provided for computing the optimal subset of data 
the study of point cloud data sampled from a stratification a collection of manifold with possible different dimension is pursued in this paper we present a technique for simultaneously soft clustering and estimating the mixed dimensionality and density of such structure the framework is based on a maximum likelihood estimation of a poisson mixture model the presentation of the approach is completed with artificial and real example demonstrating the importance of extending manifold learning to stratification learning 
support vector machine svms suffer from a widely recognized scalability problem in both memory use and computational time to improve scalability we have developed a parallel svm algorithm psvm which reduces memory use through performing a row based approximate matrix factorization and which load only essential data to each machine to perform parallel computation let n denote the number of training instance p the reduced matrix dimension after factorization p is significantly smaller than n and m the number of machine psvm reduces the memory requirement from o n to o np m and improves computation time to o np m empirical study show psvm to be effective psvm open source is available for download at http code google com p psvm 
dimensionality reduction is an important pre processing step for many application linear discriminant analysis lda is one of the well known method for supervised dimensionality reduction however the classical lda formulation requires the nonsingularity of scatter matrix involved for undersampled problem where the data dimension is much larger than the sample size all scatter matrix are singular and classical lda fails many extension including null space based lda nlda orthogonal lda olda etc have been proposed in the past to overcome this problem in this paper we present a computational and theoretical analysis of nlda and olda our main result show that under a mild condition which hold in many application involving high dimensional data nlda is equivalent to olda we have performed extensive experiment on various type of data and result are consistent with our theoretical analysis the presented analysis and experimental result provide further insight into several lda based algorithm 
we consider the machine vision task of pose estimation from static image specifically for the case of articulated object this problem is hard because of the large number of degree of freedom to be estimated following a established line of research pose estimation is framed a inference in a probabilistic model in our experience however the success of many approach often lie in the power of the feature our primary contribution is a novel casting of visual inference a an iterative parsing process where one sequentially learns better and better feature tuned to a particular image we show quantitative result for human pose estimation on a database of over image that suggest our algorithm is competitive with or surpasses the state of the art since our procedure is quite general it doe not rely on face or skin detection we also use it to estimate the pose of horse in the weizmann database 
reinforcement learning by direct policy gradient estimation is attractive in theory but in practice lead to notoriously ill behaved optimization problem we improve it robustness and speed of convergence with stochastic meta descent a gain vector adaptation method that employ fast hessian vector product in our experiment the resulting algorithm outperform previously employed online stochastic offline conjugate and natural policy gradient method 
low rank matrix decomposition are essential tool in the application of kernel method to large scale learning problem these decomposition have generally been treated a black box the decomposition of the kernel matrix that they deliver is independent of the specific learning task at hand and this is a potentially significant source of inefficiency in this paper we present an algorithm that can exploit side information e g classification label regression response in the computation of low rank decomposition for kernel matrix our algorithm ha the same favorable scaling a state of the art method such a incomplete cholesky decomposition it is linear in the number of data point and quadratic in the rank of the approximation we present simulation result that show that our algorithm yield decomposition of significantly smaller rank than those found by incomplete cholesky decomposition 
clustering is often formulated a a discrete optimization problem the objective is to find among all partition of the data set the best one according to some quality measure however in the statistical setting where we assume that the finite data set ha been sampled from some underlying space the goal is not to find the best partition of the given sample but to approximate the true partition of the underlying space we argue that the discrete optimization approach usually doe not achieve this goal a an alternative we suggest the paradigm of nearest neighbor clustering instead of selecting the best out of all partition of the sample it only considers partition in some restricted function class using tool from statistical learning theory we prove that nearest neighbor clustering is statistically consistent moreover it worst case complexity is polynomial by construction and it can be implemented with small average case complexity using branch and bound 
we propose a method for reconstruction of human brain state directly from functional neuroimaging data the method extends the traditional multivariate regression analysis of discretized fmri data to the domain of stochastic functional measurement facilitating evaluation of brain response to complex stimulus and boosting the power of functional imaging the method search for set of voxel time course that optimize a multivariate functional linea r model in term of r statistic population based incremental learning is used t o identify spatially distributed brain response to complex stimulus without attempting to localize function first variation in hemodynamic lag across brain area a nd among subject is taken into account by voxel wise non linear registratio n of stimulus pattern to fmri data application of the method on an international test benchmark for prediction of naturalistic stimulus from new and unknown fmri data show that the method successfully uncovers spatially distributed part of the brain that are highly predictive of a given stimulus 
semi supervised support vector machine s vms are an appealing method for using unlabeled data in classiflcation their objective function favor decision boundary which do not cut cluster however their main problem is that the optimization problem is non convex and ha many local minimum which often result in suboptimal performance in this paper we propose to use a global optimization technique known a continuation to alleviate this problem compared to other algorithm minimizing the same objective function our continuation method often lead to lower test error 
we present a new connectionist model for constructive intuitionistic modal reasoning we use ensemble of neural network to represent intuitionistic modal theory and show that for each intuiti onistic modal program there exists a corresponding neural network ensemble that computes the program this provides a massively parallel model for intuitionistic modal reasoning and set the scene for integrat ed reasoning knowledge representation and learning of intuitionistic theory in neural network since the network in the ensemble can be trained by example using standard neural learning algorithm 
in this paper we consider the evolution of structure within large online social network we present a series of measurement of two such network together comprising in excess of five million people and ten million friendship link annotated with metadata capturing the time of every event in the life of the network our measurement expose a surprising segmentation of these network into three region singleton who do not participate in the network isolated community which overwhelmingly display star structure and a giant component anchored by a well connected core region which persists even in the absence of star we present a simple model of network growth which capture these aspect of component structure the model follows our experimental result characterizing user a either passive member of the network inviters who encourage offline friend and acquaintance to migrate online and linkers who fully participate in the social evolution of the network 
we consider the learning task consisting in predicting a well a the best function in a finite reference set g up to the smallest possible additive term if r g denotes the generalization error of a prediction function g under reasonable assumption on the loss function typically satisfied by the least square loss when the output is bounded it is known that the progressive mixture ruleg satisfies er g ming g r g cst log g n where n denotes the size of the training set and e denotes the expectation w r t the training set distribution this work show that surpri singly for appropriate reference set g the deviation convergence rate of the progressive mixture rule is no better than cst p n it fails to achieve the expected cst n we also provide an algorithm which doe not suffer from this drawback and which is optimal in both deviation and expectation convergence rate 
various data mining application involve data object of multiple type that are related to each other which can be naturally formulated a a k partite graph however the research on mining the hidden structure from a k partite graph is still limited and preliminary in this paper we propose a general model the relation summary network to find the hidden structure the local cluster structure and the global community structure from a k partite graph the model provides a principal framework for unsupervised learning on k partite graph of various structure under this model we derive a novel algorithm to identify the hidden structure of a k partite graph by constructing a relation summary network to approximate the original k partite graph under a broad range of distortion measure experiment on both synthetic and real datasets demonstrate the promise and effectiveness of the proposed model and algorithm we also establish the connection between existing clustering approach and the proposed model to provide a unified view to the clustering approach 
several author have suggested viewing boosting a a gradient descent search for a good fit in function space at each iteration observation are re weighted using the gradient of the underlying loss function we present an approach of weight decay for observation weight which is equivalent to robustifying the underlying loss function at the extreme end of decay this approach converges to bagging which can be viewed a boosting with a linear underlying loss function we illustrate the practical usefulness of weight decay for improving prediction performance and present an equivalence between one form of weight decay and huberizing a statistical method for making loss function more robust 
previous work on text mining ha almost exclusively focused on a single stream however we often have available multiple text stream indexed by the same set of time point called coordinated text stream which offer new opportunity for text mining for example when a major event happens all the news article published by different agency in different language tend to cover the same event for a certain period exhibiting a correlated bursty topic pattern in all the news article stream in general mining correlated bursty topic pattern from coordinated text stream can reveal interesting latent association or event behind these stream in this paper we define and study this novel text mining problem we propose a general probabilistic algorithm which can effectively discover correlated bursty pattern and their bursty period across text stream even if the stream have completely different vocabulary e g english v chinese evaluation of the proposed method on a news data set and a literature data set show that it can effectively discover quite meaningful topic pattern from both data set the pattern discovered from the news data set accurately reveal the major common event covered in the two stream of news article in english and chinese respectively while the pattern discovered from two database publication stream match well with the major research paradigm shift in database research since the proposed method is general and doe not require the stream to share vocabulary it can be applied to any coordinated text stream to discover correlated topic pattern that burst in multiple stream in the same period 
we present a directed markov random field mrf model that combine n gram model probabilistic context free grammar pcfgs and probabilistic latent semantic analysis plsa for the purpose of statistical language modeling even though the composite directed mrf model potentially ha an exponential number of loop and becomes a context sensitive grammar we are nevertheless able to estimate it parameter in cubic time using an efficient modified em method the generalized inside outside algorithm which extends the inside outside algorithm to incorporate the effect of the n gram and plsa language model we generalize various smoothing technique to alleviate the sparseness of n gram count in case where there are hidden variable we also derive an analogous algorithm to calculate the probability of initial subsequence of a sentence generated by the composite language model our experimental result on the wall street journal corpus show that we obtain significant reduction in perplexity compared to the state of the art baseline trigram model with good turing and kneser ney smoothings 
observed in many application there is a potential need of extracting a small set of frequent pattern having not only high significance but also low redundancy the significance is usually defined by the context of application previous study have been concentrating on how to compute top k significant pattern or how to remove redundancy among pattern separately there is limited work on finding those top k pattern which demonstrate high significance and low redundancy simultaneously in this paper we study the problem of extracting redundancy aware top k pattern from a large collection of frequent pattern we first examine the evaluation function for measuring the combined significance of a pattern set and propose the mm maximal marginal significance a the problem formulation the problem is known a np hard we further present a greedy algorithm which approximates the optimal solution with performance bound o log k with condition on redundancy where k is the number of reported pattern the direct usage of redundancy aware top k pattern is illustrated through two real application disk block prefetch and document theme extraction our method can also be applied to processing redundancy aware top k query in traditional database 
we present an improvement to the dp slam algorithm for simultaneous localization and mapping slam that maintains multiple hypothesis about densely populated map one full map per particle in a particle filter in time that is linear in all significant algorit hm parameter and take constant amortized time per iteration this mean that the asymptotic complexity of the algorithm is no greater than that of a pure localization algorithm using a single map and the same number of particle we also present a hierarchical extension of dp slam that us a two level particle filter which model drift in the particle fi ltering process itself the hierarchical approach enables recovery from the inevitable drift that result from using a finite number of particle in a particle filter and permit the use of dp slam in more challenging domain while maintaining linear time asymptotic complexity 
a good distance metric is crucial for many data mining task to learn a metric in the unsupervised setting most metric learning algorithm project observed data to a low dimensional manifold where geometric relationship such a pairwise distance are preserved it can be extended to the nonlinear case by applying the kernel trick which embeds the data into a feature space by specifying the kernel function that computes the dot product between data point in the feature space in this paper we propose a novel unsupervised nonlinear adaptive metric learning algorithm called naml which performs clustering and distance metric learning simultaneously naml firstmaps the data to a high dimensional space through a kernel function then applies a linear projection to find a low dimensional manifold where the separability of the data is maximized and finally performs clustering in the low dimensional space the performance of naml depends on the selection of the kernel function and the projection we show that the joint kernel learning dimensionality reduction and clustering can be formulated a a trace maximization problem which can be solved via an iterative procedure in the em framework experimental result demonstrated the efficacy of the proposed algorithm 
we present a novel bayesian model for semi supervised part of speech tagging our model extends the latent dirichlet allocation model and incorporates the intuition that word distribution over tag p t w are sparse in addition we introduce a model for determining the set of possible tag of a word which capture important dependency in the ambiguity class of word our model outperforms the best previously proposed model for this task on a standard dataset 
we cast the ranking problem a multiple classification mc multiple ordinal classification which lead to computationally tracta ble learning algorithm for relevance ranking in web search we consider the dcg criterion discounted cumulative gain a standard quality measure in information retrieval our approach is motivated by the fact that perfect classification result in perfect dcg score and the dcg error are bounded by classification error s we propose using the expected relevanceto convert class probability into ranking score the class probability are learned using a gradient boosting t ree algorithm evaluation on large scale datasets show that our approach can improve lambdarank and the regression based ranker in term of the normalized dcg score an efficient implementation of the boosting tree algorithm is a lso presented introduction the general ranking problem ha widespread application including commercial search engine and recommender system we develop mcrank a computationally tractable learning algorithm for the general ranking problem and we present our approach in the context of ranking in web search for a given user input query a commercial search engine return many page of url in an order determined by the underlying proprietary ranking algorithm the quality of the returned result are largely evaluated on the url displayed in the very first page the type of ranking problem in this study is sometimes referred to a dynamic ranking or simply just ranking because the url are dynamically ranked in real time according to the specific user input query this is different from the query independent static ranking based on for example page rank or authority and hub which may at least conceptually serve a an important feature for dynamic ranking or to guide the generation of a list of url fed to the dynamic ranker there are two main category of ranking algorithm a popular scheme is based on learning pairwise preference including ranknet lambdarank ranksvm rankboost gbrank and frank both lambdarankand ranknet used neural net ranknet used a cross entropy type of loss function and lambdarankused a gradient based on ndcg smoothed by the ranknet loss another scheme is based on regression considered the dcg measure discounted cumulative gain and showed that the dcg error are bounded by regression error in this study we also consider the dcg measure from the defin ition of dcg it appears more direct to cast the ranking problem a multiple classification mc a opposed to regression in order to convert classification result into ranking score we prop ose a simple and stable mechanism by using the expected relevance our evaluation on large scale datasets demonstrate the superiority of the classification based ranker mcrank over both the regression based and pair based scheme 
we present a computational bayesian approach for wiener diffusion model which are prominent account of response time distribution in decision making we first develop a general closed form analytic approximation to the response time distribution for one dimensional diffusion process and derive the required wiener diffusion a a special case we use this result to undertake bayesian modeling of benchmark data using posterior sampling to draw inference about the interesting psychological parameter with the aid of the benchmark data we show the bayesian account ha several advantage including dealing naturally with the parameter variation needed to account for some key feature of the data and providing quantitative measure to guide decision about model construction 
in many data mining application online labeling feedback is only available for example which were predicted to belong to the positive class such application includespam filtering in the case where user never checkemails marked spam document retrieval where user cannotgive relevance feedback on unretrieved document and online advertising where user behavior cannot beobserved for unshown advertisement one sided feedback can cripple the performance of classical mistake driven online learner such a perceptron previous work under the apple tasting framework showed how to transform standard online learner into successful learner from one sided feedback however we find in practice that this transformation may request more label than necessary to achieve strong performance in this paper we employ two active learning method which reduce the number of label requested in practice one method is the use of label efficient active learning the other method somewhat surprisingly is the use of margin based learner without modification which we show combine implicit active learning and a greedy strategy to managing the exploration exploitation tradeoff experimental result show that these method can be significantly more effective in practice than those using the apple tasting transformation even on minority class problem 
we present a method for parameter learning in relational bayesian network rbns our approach consists of compiling the rbn model into a computation graph for the likelihood function and to use this likelihood graph to perform the necessary computation for a gradient ascent likelihood optimization procedure the method can be applied to all rbn model that only contain differentiable combining rule this includes model with non decomposable combining rule a well a model with weighted combination or nested occurrence of combining rule experimental result on artificial random graph data explores the feasibility of the approach both for complete and incomplete data 
correlation is one of the most widely used similarity measure in machine learning like euclidean and mahalanobis distance however compared with proposed numerous discriminant learning algorithm in distance metric space only a very little work ha been conducted on this topic using correlation similarity measure in this paper we propose a novel discriminant learning algorithm in correlation measure space correlation discriminant analysis cda in this framework based on the definition of within class correlation and between class correlation the optimum transformation can be sought for to maximize the difference between them which is in accordance with good classification performance empirically under different case of the transformation different implementation of the algorithm are given extensive empirical evaluation of cda demonstrate it advantage over alternative method 
this paper present an algorithm for synthesis of human motion in specified style we use a theory of movement observation laban movement analysis to describe movement style a point in a multi dimensional perceptual space we cast the task of learning to synthesize desired movement style a a regression problem sequence generated via space time interpolation of motion capture data are used to learn a nonlinear mapping between animation parameter and movement style in perceptual space we demonstrate that the learned model can apply a variety of motion style to pre recorded motion sequence and it can extrapolate style not originally included in the training data 
regardless of the frequent pattern to discover either the full frequent pattern or the condensed one either closed or maximal the strategy always includes the traversal of the lattice of candidate pattern we study the existing depth versus breadth traversal approach for generating candidate pattern and propose in this paper a new traversal approach that jump in the search space among only promising node our leaping approach avoids node that would not participate in the answer set and reduce drastically the number of candidate pattern we use this approach to efficiently pinpoint maximal pattern at the border of the frequent pattern in the lattice and collect enough information in the process to generate all subsequent pattern 
this paper present a search algorithm for finding function that are highly correlated with an arbitrary set of data the function found by the search can be used to approximate the unknown function that generated the data a special case of this approach is a method for learning fourier representation empirical result demonstrate that on typical real world problem the most highly correlated function can be found very quickly while combination of these function provide good approximation of the unknown function 
distance metric learning and nonlinear dimensionality reduction are two interesting and active topic in recent year however the connection between them is not thoroughly studied yet in this paper a transductive framework of distance metric learning is proposed and it close connection with many nonlinear spectral dimensionality reduction method is elaborated furthermore we prove a representer theorem for our framework linking it with function estimation in an rkhs and making it possible for generalization to unseen test sample in our framework it suffices to solve a sparse eigenvalue problem thus datasets with sample can be handled finally experiment result on synthetic data several uci database and the mnist handwritten digit database are shown 
graph data such a chemical compound and xml document are getting more common in many application domain a main difficulty of graph data processing lie in the intrinsic high dimensionality of graph namely when a graph is represented a a binary feature vector of indicator of all possible subgraph pattern the dimensionality get too large for usual statistical method we propose an efficient method to select a small number of salient pattern by regularization path tracking the generation of useless pattern is minimized by progressive extension of the search space in experiment it is shown that our technique is considerably more efficient than a simpler approach based on frequent substructure mining 
the peristimulus time histogram psth and it more continuous cousin the spike density function sdf are staple in the analytic toolkit of neurophysiologists the former is usually obtained by binning spike train whereas the standard method for the latter is smoothing with a gaussian kernel selection of a bin width or a kernel size is often done in an relatively arbitrary fashion even though there have been recent attempt to remedy this situation we develop an exact bayesian generative model approach to estimating psths and demonstate it superiority to competing method further advantage of our scheme include automatic complexity control and error bar on it prediction 
metasearch engine comparison shopping and deep web crawling application need to extract search result record enwrapped in result page returned from search engine in response to user query the search result record from a given search engine are usually formatted based on a template precisely identifying this template can greatly help extract and annotate the data unit within each record correctly in this paper we propose a graph model to represent record template and develop a domain independent statistical method to automatically mine the record template for any search engine using sample search result record our approach can identify both template tag html tag and template text non tag text and it also explicitly address the mismatch between the tag structure and the data structure of search result record our experimental result indicate that this approach is very effective 
in this paper we compare both discriminative and generative parameter learning on both discriminatively and generatively structured bayesian network classifier we use either maximum likelihood ml or conditional maximum likelihood cl to optimize network parameter for structure learning we use either conditional mutual information cmi the explaining away residual ear or the classification rate cr a objective function experiment with the naive bayes classifier nb the tree augmented naive bayes classifier tan and the bayesian multinet have been performed on data set from the uci repository merz et al and from kohavi john our empirical study suggests that discriminative structure learnt using cr produce the most accurate classifier on almost half the data set this approach is feasible however only for rather small problem since it is computationally expensive discriminative parameter learning produce on average a better classifier than ml parameter learning 
reinforcement learning algorithm can become unstable when combined with linear function approximation algorithm that minimize the mean square bellman error are guaranteed to converge but often do so slowly or are computationally expensive in this paper we propose to improve the convergence speed of piecewise linear function approximation by tracking the dynamic of the value function with the kalman filter using a random walk model we cast this a a general framework in which we implement the td q learning and maxq algorithm for different domain and report empirical result demonstrating improved learning speed over previous method 
we present micropower mixed signal vlsi hardware for real time blind separation and localization of acoustic source gradient flow representation of the traveling wave signal acquired over a miniature cm diameter array of four microphone yield linearly mixed instantaneous observation of the time differentiated source separated and localized by independent component analysis ica the gradient flow and ica processor each measure mm mm in m cmos and consume w and w power respectively from a v supply at k s sampling rate experiment demonstrate perceptually clear db separation and precise localization of two speech source presented through speaker positioned at m from the array on a conference room table analysis of the multipath residual show that they are spectrally diffuse and void of the direct path 
neighborhood graph construction is usually the first step in algorithm for isometric data embedding and manifold learning that cope with the problem of projecting high dimensional data to a low space this paper begin by explaining the algorithmic fundamental of technique for isometric data embedding and derives a general classification of these technique we will see that the nearest neighbor approach commonly used to construct neighborhood graph do not guarantee connectedness of the constructed neighborhood graph and consequently may cause an algorithm fail to project data to a single low dimensional coordinate system in this paper we review three existing method to construct k edge connected neighborhood graph and propose a new method to construct k connected neighborhood graph these method are applicable to a wide range of data including data distributed among cluster their feature are discussed and compared through experiment 
in this paper we examine the problem of efficiently finding set of observation that conform to a given underlying motion model while this problem is often phrased a a tracking problem where it is called track initiation it is useful in a variety of task where we want to find correspondence or pattern in spatial temporal data unfortunately this problem often suffers from a combinatorial explosion in the number of potential set that must be evaluated we consider the problem with respect to large scale asteroid observation data where the goal is to find association among the observation that correspond to the same underlying asteroid in this domain it is vital that we can efficiently extract the underlying association we introduce a new methodology for track initiation that exhaustively considers all possible linkage we then introduce an exact tree based algorithm for tractably finding all compatible set of point further we extend this approach to use multiple tree exploiting structure from several time step at once we compare this approach to a standard sequential approach and show how the use of multiple tree can provide a significant benefit 
sensorimotor data from many interesting physical interaction comprises discontinuity while existing locally weighted learning approach aim at learning smooth function we propose a model that learns how to switch discontinuously between local model the local responsibility usually represented by gaussian kernel are learned by a product of local sigmoidal classifier that can represent complex shaped and sharply bounded region local model are incrementally added a locality prior constrains them to learn only local data which is the key ingredient for incremental learning with local model 
the problem of multimodal data mining in a multimedia database can be addressed a a structured prediction problem where we learn the mapping from an input to the structured and interdependent output variable in this paper built upon the existing literature on the max margin based learning we develop a new max margin learning approach called enhanced max margin learning emml framework in addition we apply emml framework to developing an effective and efficient solution to the multimodal data mining problem in a multimedia database the main contribution include we have developed a new max margin learning approach the enhanced max margin learning framework that is much more efficient in learning with a much faster convergence rate which is verified in empirical evaluation we have applied this emml approach to developing an effective and efficient solution to the multimodal data mining problem that is highly scalable in the sense that the query response time is independent of the database scale allowing facilitating a multimodal data mining querying to a very large scale multimedia database and excelling many existing multimodal data mining method in the literature that do not scale up at all this advantage is also supported through the complexity analysis a well a empirical evaluation against a state of the art multimodal data mining method from the literature while emml is a general framework for the evaluation purpose we apply it to the berkeley drosophila embryo image database and report the performance comparison with a state of the art multimodal data mining method 
scientific model typically depend on parameter preserving the parameter dependence of model in the pattern mining context open up several application within association rule mining arm the choice of parameter can be studied with more flexibly then in traditional model building studying support confidence and other rule metric a a function of model parameter allows conclusion on assumption underlying the model we present efficient technique to handle multiple model output data set at little more than the cost of one we integrate output from hidden markov model into the association rule mining framework demonstrating the potential for frequent pattern mining in the field of scientific modeling and experimentation 
we show that linear generalization of rescorla wagner can perform maximum likelihood estimation of the parameter of all generative model for causal reasoning our approach involves augmenting variable to deal with conjunction of cause similar to the agumented model of rescorla our result involve genericity assumption on the distribution of cause if these assumption are violated for example for the cheng causal power theory then we show that a linear rescorla wagner can estimate the parameter of the model up to a nonlinear transformtion moreover a nonlinear rescorla wagner is able to estimate the parameter directly to within arbitrary accuracy previous resul t can be used to determine convergence and to estimate convergence rate 
we propose a general framework for learning from labeled and unlabeled data on a directed graph in which the structure of the graph including the directionality of the edge is considered the time complexity of the algorithm derived from this framework is nearly linear due to recently developed numerical technique in the absence of labeled instance this framework can be utilized a a spectral clustering method for directed graph which generalizes the spectral clustering approach for undirected graph we have applied our framework to real world web classification problem and obtained encouraging result 
we present a class of richly structured undirected hidden variable model suitable for simultaneously modeling text along with other attribute encoded in different modality our model generalizes technique such a principal component analysis to heterogeneous data type in contrast to other approach this framework allows modality such a word author and timestamps to be captured in their natural probabilistic encoding a latent space representation for a previously unseen document can be obtained through a fast matrix multiplication using our method we demonstrate the effectiveness of our framework on the task of author prediction from year of the nip conference proceeding and for a recipient prediction task using a month academic email archive of a researcher our approach should be more broadly applicable to many real world application where one wish to efficiently make prediction for a large number of potential output using dimensionality reduction in a well defined probabilistic framework 
machine learning contains many computational bottleneck in the form of nested summation over datasets computation of these summation is typically o n or higher which severely limit application to large datasets we present a multistage stratified monte carlo method for approximating such summation with probabilistic relative error control the essential idea is fast approximation by sampling in tree this method differs from many previous scalability technique such a multi tree method in that it error is stochastic but we derive condition for error control and demonstrate that they work further we give a theoretical sample complexity for the method that is independent of dataset size and show that this appears to hold in experiment where speedup reach a high a many order of magnitude beyond the previous state of the art 
image congealing ic is a non parametric method for the joint alignment of a collection of image affected by systematic and unwanted deformation the method attempt to undo the deformation by minimizing a measure of complexity of the image ensemble such a the averaged per pixel entropy this enables alignment without an explicit model of the aligned dataset a required by other method e g transformed component analysis while ic is simple and general it may introduce degenerate solution when the transformation allow minimizing the complexity of the data by collapsing them to a constant such solution need to be explicitly removed by regularization in this paper we propose an alternative formulation which solves this regularization issue on a more principled ground we make the simple observation that alignment should simplify the data while preserving the useful information carried by them therefore we trade off fidelity and complexity of the aligned ensemble rather than minimizing the complexity alone this eliminates the need for an explicit regularization of the transformation and ha a number of other useful property such a noise suppression we show the modeling and computational benefit of the approach to the some of the problem on which ic ha been demonstrated 
this work present a method for estimating human facial attractiveness based on supervised learning technique numerous facial feature that describe facial geometry color and texture combined with an average human attractiveness score for each facial image are used to train various predictor facial attractiveness rating produced by the final predictor are found to be highly correlated with human rating markedly improving previous machine learning achievement simulated psychophysical experiment with virtually manipulated image reveal preference in the machine s judgment which are remarkably similar to those of human these experiment shed new light on existing theory of facial attractiveness such a the averageness smoothness and symmetry hypothesis it is intriguing to find that a machine trained explicitly to capture an operational performance criterion such a attractiveness rating implicitly capture basic human psychophysical bias characterizing the perception of facial attractiveness in general 
biological movement is built up of sub block or motion primitive such primitive provide a compact representation of movement which is also desirable in robotic control application we analyse handwriting data to gain a better understanding of primitive and their timing in biological movement inference of the shape and the timing of primitive can be done using a factorial hmm based model allowing the handwriting to be represented in primitive timing space this representation provides a distribution of spike corresponding to the primitive activation which can also be modelled using hmm architecture we show how the coupling of the low level primitive model and the higher level timing model during inference can produce good reconstruction of handwriting with shared primitive for all character modelled this coupled model also capture the variance profile of the dataset which is accounted for by spike timing jitter the timing code provides a compact representation of the movement while generating a movement without an explicit timing model produce a scribbling style of output 
the l bfgs limited memory quasi newton method is the algorithm of choice for optimizing the parameter of large scale log linear model with l regularization but it cannot be used for an l regularized loss due to it non differentiability whenever some parameter is zero efficient algorithm have been proposed for this task but they are impractical when the number of parameter is very large we present an algorithm orthant wise limited memory quasi newton owl qn based on l bfgs that can efficiently optimize the l regularized log likelihood of log linear model with million of parameter in our experiment on a parse reranking task our algorithm wa several order of magnitude faster than an alternative algorithm and substantially faster than l bfgs on the analogous l regularized problem we also present a proof that owl qn is guaranteed to converge to a globally optimal parameter vector 
data set that characterize human activity over time through collection of timestamped event or count are of increasing interest in application area a humancomputer interaction video surveillance and web data analysis we propose a non parametric bayesian framework for modeling collection of such data in particular we use a dirichlet process framework for learning a set of intensity function corresponding to different category which form a basis set for representing individual time period e g several day depending on which category the time period are assigned to this allows the model to learn in a data driven fashion what factor are generating the observation on a particular day including for example weekday versus weekend effect or day specific effect corresponding to unique single day occurrence of unusual behavior sharing information where appropriate to obtain improved estimate of the behavior associated with each category application to real world data set of count data involving both vehicle and people are used to illustrate the technique 
scoring structure of undirected graphical model by mean of evaluating the marginal likelihood is very hard the main reason is the presence of the partition function which is intractable to evaluate let alone integrate over we propose to approximate the marginal likelihood by employing two level of approximation we assume normality of the posterior the laplace approximation and approximate all remaining intractable quantity using belief propagation and the linear response approximation this result in a fast procedure for model scoring empirically we find that our procedure ha about two order of magnitude better accuracy than standard bic method for small datasets but deteriorates when the size of the dataset grows 
in this paper we study a large query log of more than twenty million query with the goal of extracting the semantic relation that are implicitly captured in the action of user submitting query and clicking answer previous query log analysis were mostly done with just the query and not the action that followed after them we first propose a novel way to represent query in a vector space based on a graph derived from the query click bipartite graph we then analyze the graph produced by our query log showing that it is le sparse than previous result suggested and that almost all the measure of these graph follow power law shedding some light on the searching user behavior a well a on the distribution of topic that people want in the web the representation we introduce allows to infer interesting semantic relationship between query second we provide an experimental analysis on the quality of these relation showing that most of them are relevant finally we sketch an application that detects multitopical url 
in this paper we address the problem of detecting topic in large scale linked document collection recently topic detection ha become a very active area of research due to it utility for information navigation trend analysis and high level description of data we present a unique approach that us the correlation between the distribution of a term that represents a topic and the link distribution in the citation graph where the node are limited to the document containing the term this tight coupling between term and graph analysis is distinguished from other approach such a those that focus on language model we develop a topic score measure for each term using the likelihood ratio of binary hypothesis based on a probabilistic description of graph connectivity our approach is based on the intuition that if a term is relevant to a topic the document containing the term have denser connectivity than a random selection of document we extend our algorithm to detect a topic represented by a set of term using the intuition that if the co occurrence of term represents a new topic the citation pattern should exhibit the synergistic effect we test our algorithm on two electronic research literature collection arxiv and citeseer our evaluation show that the approach is effective and reveals some novel aspect of topic detection 
we present a bayesian search algorithm for learning the structure of latent variable model of continuous variable we stress the importance of applying search operator designed especially for the parametric family used in our model this is performed by searching for subset of the observed variable whose covariance matrix can be represented a a sum of a matrix of low rank and a diagonal matrix of residual the resulting search procedure is relatively efficient since the main search operator ha a branch factor that grows linearly with the number of variable the resulting model are often simpler and give a better fit than model based on generalization of factor analysis or those derived from standard hill climbing method 
we extend a previously developed bayesian framework for perception to account for sensory adaptation we first note that the perceptual effect of adaptation seems inconsistent with an adjustment of the internally represented prior distribution instead we postulate that adaptation increase the signal to noise ratio of the measurement by adapting the operational range of the measurement stage to the input range we show that this change the likelihood function in such a way that the bayesian estimator model can account for reported perceptual behavior in particular we compare the model s prediction to human motion discrimination data and demonstrate that the model account for the commonly observed perceptual adaptation effect of repulsion and enhanced discriminability 
we provide provably privacy preserving version of belief propagation gibbs sampling and other local algorithm distributed multiparty protocol in which each party or vertex learns only it final local value and ab olutely nothing else 
we investigate the empirical applicability of several bound a number of which are new on the true error rate of learned classifier which hold whenever the example are chosen independently at random from a fixed distribution the collection of trick we use includes a technique using unlabeled data for a tight derandomization of randomized bound a tight form of the progressive validation bound the exact form of the test set bound the bound are implemented in the semibound package and are freely available 
gradient descent is a widely used paradigm for solving many optimization problem stochastic gradient descent performs a series of iteration to minimize a target function in order to reach a local minimum in machine learning or data mining this function corresponds to a decision model that is to be discovered the gradient descent paradigm underlies many commonly used technique in data mining and machine learning such a neural network bayesian network genetic algorithm and simulated annealing to the best of our knowledge there ha not been any work that extends the notion of privacy preservation or secure multi party computation to gradient descent based technique in this paper we propose a preliminary approach to enable privacy preservation in gradient descent method in general and demonstrate it feasibility in specific gradient descent method 
semi supervised clustering employ limited supervision in the form of labeled instance or pairwise instance constraint to aid unsupervised clustering and often significantly improves the clustering performance despite the vast amount of expert knowledge spent on this problem most existing work is not designed for handling high dimensional sparse data this paper thus fill this crucial void by developing a semi supervised clustering method based on spherical k mean via feature projection screen specifically we formulate the problem of constraint guided feature projection which can be nicely integrated with semi supervised clustering algorithm and ha the ability to effectively reduce data dimension indeed our experimental result on several real world data set show that the screen method can effectively deal with high dimensional data and provides an appealing clustering performance 
we utilize the ensemble of tree framework a tractable mixture over superexponential number of tree structured distribution to develop a new model for multivariate density estimation the model is based on a construction of treestructured copula multivariate distribution with uniform on marginals by averaging over all possible tree structure the new model can approximate distribution with complex variable dependency we propose an em algorithm to estimate the parameter for these tree averaged model for both the real valued and the categorical case based on the tree averaged framework we propose a new model for joint precipitation amount data on network of rain station 
this paper study the aggregation of prediction made by tree based model for several perturbed version of the attribute vector of a test case a closed form approximation of this scheme combined with cross validation to tune the level of perturbation is proposed this yield soft tree model in a parameter free way and preserve their interpretability empirical evaluation on classification and regression problem show that accuracy and bias variance tradeoff are improved significantly at the price of an acceptable computational overhead the method is further compared and combined with tree bagging 
many unsupervised learning problem can be expressed a a form of matrix factorization reconstructing an observed data matrix a the p roduct of two matrix of latent variable a standard challenge in solving these p roblems is determining the dimensionality of the latent matrix nonparametric bayesian matrix factorization is one way of dealing with this challenge yielding a posterior distribution over possible factorization of unbounded dimensionality a drawback to this approach is that posterior estimation is typically done using gibbs sampling which can be slow for large problem and when conjugate prior cannot be used a an alternative we present a particle filter for posterior esti mation in nonparametric bayesian matrix factorization model we illustrate this a pproach with two matrix factorization model and show favorable performance relative to gibbs sampling 
we compare two recently proposed framework for combining generative and discriminative probabilistic classifier and apply them to semi supervised classification in both case we explore the tradeoff between maximizing a discriminative likelihood of labeled data and a generative likelihood of labeled and unlabeled data while prominent semi supervised learning method assume low density region between class or are subject to generative modeling assumption we conjecture that hybrid generative discriminative method allow semi supervised learning in the presence of strongly overlapping class and reduce the risk of modeling structure in the unlabeled data that is irrelevant for the specific classification task of interest we apply both hybrid approach within naively structured markov random field model and provide a thorough empirical comparison with two well known semi supervised learning method on six text classification task a semi supervised hybrid generative discriminative method provides the best accuracy in of the experiment and the multi conditional learning hybrid approach achieves the highest overall mean accuracy across all task 
in this paper we present a novel semisupervised regression algorithm working on multiclass data that may lie on multiple manifold unlike conventional manifold regression algorithm that do not consider the class distinction of sample our method introduces the class information to the regression process and try to exploit the similar configuration shared by the label distribution of multi class data to utilize the correlation among data from different class we develop a cross manifold label propagation process and employ label from different class to enhance the regression performance the interclass relation are coded by a set of intermanifold graph and a regularization item is introduced to impose inter class smoothness on the possible solution in addition the algorithm is further extended with the kernel trick for predicting label of the out of sample data even without class information experiment on both synthesized data and real world problem validate the effectiveness of the proposed framework for semisupervised regression 
long term search history contains rich information about a user s search preference which can be used a search context to improve retrieval performance in this paper we study statistical language modeling based method to mine contextual information from long term search history and exploit it for a more accurate estimate of the query language model experiment on real web search data show that the algorithm are effective in improving search accuracy for both fresh and recurring query the best performance is achieved when using clickthrough data of past search that are related to the current query 
we introduce a method for approximate smoothed inference in a class of switching linear dynamical system based on a novel form of gaussian sum smoother this class includes the switching kalman filter and the more general case of switch transition dependent on the continuous latent state the method improves on the standard kim smoothing approach by dispensing with one of the key approximation thus making fuller use of the available future inform ation whilst the only central assumption required is projection to a mixture of gaussians we show that an additional conditional independence assumption result in a simpler but stable and accurate alternative unlike the alternative unstable expectation propagation procedure our method consists only of a single forward and backward pas and is reminiscent of the standard smoothing correction recurs ion in the simpler linear dynamical system the algorithm performs well on both toy experiment and in a large scale application to noise robust speech recognition t p vt ht st p ht ht st p st ht st 
knowledge based planning method offer benefit over classical technique but they are time consuming and costly to construct there ha been research on learning plan knowledge from search but this can take substantial computer time and may even fail to find solution on complex task here we describe another approach that observes sequence of operator taken from expert solution to problem and learns hierarchical task network from them the method ha similarity to previous algorithm for explanation based learning but differs in it ability to acquire hierarchical structure and in the generality of learned condition these increase the method s capability to transfer learned knowledge to other problem and support the acquisition of recursive procedure after presenting the learning algorithm we report experiment that compare it ability to other technique on two planning domain in closing we review related work and direction for future research 
in this paper we describe a new method to reduce the complexity of support vector machine by reducing the number of necessary support vector included in their solution the reduction process iteratively selects two nearest support vector belonging to the same class and replaces them by a newly constructed vector through the analysis of relation between vector in the input and feature space we present the construction of new vector that requires to find the unique maximum point of a one variable function on the interval not to minimize a function of many variable with local minimum in former reduced set method experimental result on real life datasets show that the proposed method is effective in reducing number of support vector and preserving machine s generalization performance 
temporal pattern composed of symbolic interval are commonly formulated with allen s interval relation originating in temporal reasoning this representation ha severe disadvantage for knowledge discovery the time series knowledge representation tskr is a new hierarchical language for interval pattern expressing the temporal concept of coincidence and partial order we present efiective and e cient mining algorithm for such pattern based on itemset technique a novel form of search space pruning efiectively reduces the size of the mining result to ease interpretation and speed up the algorithm on a real data set a concise set of tskr pattern can explain the underlying temporal phenomenon whereas the pattern found with allen s relation are far more numerous yet only explain fragment of the data category and subject descriptor i computing methodology pattern recognition general term algorithm keywords knowledge discovery time series interval pattern 
we propose a probabilistic model based on independent component analysis for learning multiple related task in our model t he task parameter are assumed to be generated from independent source which account for the relatedness of the task we use laplace distribution to model hidden source which make it possible to identify the hidden independent component instead of just modeling correlation furthermore our model enjoys a sparsity property which make it both parsimonious and robust we also propose efficient algorithm for bo th empirical bayes method and point estimation our experimental result on two multi label text classification data set show that the prop osed approach is promising 
in multi task learning our goal is to design regression or classification model for each of the task and appropriately share information between task a dirichlet process dp prior can be used to encourage task clustering however the dp prior doe not allow local clustering of task with respect to a subset of the feature vector without making independence assumption motivated by this problem we develop a new multitask learning prior termed the matrix stick breaking process msbp which encourages cross task sharing of data however the msbp allows separate clustering and borrowing of information for the different feature component this is important when task are more closely related for certain feature than for others bayesian inference proceeds by a gibbs sampling algorithm and the approach is illustrated using a simulated example and a multi national application 
this paper present approach to semi supervised learning when the labeled training data and test data are differently distributed specifically the sample selected for labeling are a biased subset of some general distribution and the test set consists of sample drawn from either that general distribution or the distribution of the unlabeled sample an example of the former appears in loan application approval where sample with repay default label exist only for approved applicant and the goal is to model the repay default behavior of all applicant an example of the latter appears in spam filtering in which the labeled sample can be out dated due to the cost of labeling email by hand but an unlabeled set of up to date email exists and the goal is to build a filter to sort new incoming email most approach to overcoming such bias in the literature rely on the assumption that sample are selected for labeling depending only on the feature not the label a case in which provably correct method exist the missing label are said to be missing at random mar in real application however the selection bias can be more severe when the mar conditional independence assumption is not satisfied and missing label are said to be missing not at random mnar and no learning method is provably always correct we present a generative classifier the shifted mixture model smm with separate representation of the distribution of the labeled sample and the unlabeled sample the smm make no conditional independence assumption and can model distribution of semi labeled data set with arbitrary bias in the labeling we present a learning method based on the expectation maximization em algorithm that while not always able to overcome arbitrary labeling bias learns smms with higher test set accuracy in real world data set with mnar bias than existing learning method that are proven to overcome mar bias 
clustering is the problem of identifying the distribution of pattern and intrinsic correlation in large data set by partitioning the data point into similarity class this paper study the problem of clustering binary data this is the case for market basket datasets where the transaction contain item and for document datasets where the document contain bag of word the contribution of the paper is three fold first a general binary data clustering model is presented the model treat the data and feature equally based on their symmetric association relation and explicitly describes the data assignment a well a feature assignment we characterize several variation with different optimization procedure for the general model second we also establish the connection between our clustering model with other existing clustering method third we also discus the problem for determining the number of cluster for binary clustering experimental result show the effectiveness of the proposed clustering model 
hidden markov model hmms model sequential data in many field such a text speech processing and biosignal analysis active learning algorithm learn faster and or better by closing the data gathering loop i e they choose the example most informative with respect to their learning objective we introduce a framework and objective function for active learning in three fundamental hmm problem model learning state estimation and path estimation in addition we describe a new set of algorithm for efficiently finding optimal greedy query using these objective function the algorithm are fast i e linear in the number of time step to select the optimal query and we present empirical result showing that these algorithm can significantly reduce the need for labelled training data 
an intuitive approach to utilizing unlabeled data in kernel based classification algorithm is to simply treat unknown label a additional optimization variable for margin based loss function one can view this approach a attempting to learn low density separator however this is a hard optimization problem to solve in typical semi supervised setting where unlabeled data is abundant the popular transductive svm algorithm is a label switching retraining procedure that is known to be susceptible to local minimum in this paper we present a global optimization framework for semi supervised kernel machine where an easier problem is parametrically deformed to the original hard problem and minimizers are smoothly tracked our approach is motivated from deterministic annealing technique and involves a sequence of convex optimization problem that are exactly and efficiently solved we present empirical result on several synthetic and real world datasets that demonstrate the effectiveness of our approach 
stability is a desirable characteristic for linear dynamical system but it is often ignored by algorithm that learn these system from data we propose a novel method for learning stable linear dynamical system we formulate an approximation of the problem a a convex program start with a solution to a relaxed version of the program and incrementally add constraint to improve stability rather than continuing to generate constraint until we reach a feasible solution we test stability at each step because the convex program is only an approximation of the desired problem this early stopping rule can yield a higher quality solution we apply our algorithm to the task of learning dynamic texture from image sequence a well a to modeling biosurveillance drug sale data the constraint generation approach lead to noticeable improvement in the quality of simulated sequence we compare our method to those of lacy and bernstein with positive result in term of accuracy quality of simulated sequence and efficiency 
we consider the estimation problem in gaussian graphical model with arbitrary structure we analyze the embedded tree algorithm which solves a sequence of problem on tractable subgraphs thereby leading to the solution of the estimation problem on an intractable graph our analysis is based on the recently developed walk sum interpretation of gaussian estimation we show that non stationary iteration of the embedded tree algorithm using any sequence of subgraphs converge in walk summable model based on walk sum calculation we develop adaptive method that optimize the choice of subgraphs used at each iteration with a view to achieving maximum reduction in error these adaptive procedure provide a significant speedup in convergence over stationary iterative method and also appear to converge in a larger class of model we describe a rich class of iterative algorithm for estimation in gaussian graphical model with arbitrary structure specifically we discus the embedded tree et iteration that solves a sequence of estimation problem on tree or more generally tractable subgraphs leading to the solution of the original problem on the intractable graph we analyze non stationary iteration of the et algorithm that perform inference calculation on an arbitrary sequence of subgraphs our analysis is based on the recently developed walk sum interpretation of inference in gaussian graphical model we show that in the broad class of so called walk summable model the et algorithm converges for any arbitrary sequence of subgraphs used the walk summability of a model is easily tested thus providing a simple sufficient condition for the convergence of such non stationary algorithm previous convergence result analyzed stationary or cyclo stationary iteration that use the same subgraph at each iteration or cycle through a fixed sequence of subgraphs the focus of this paper is on analyzing and developing algorithm based on arbitrary non stationary iteration that use any non cyclic sequence of subgraphs and the recently developed concept of walk sum appears to be critical to this analysis 
a standard approach to cross language information retrieval clir us latent semantic analysis lsa in conjunction with a multilingual parallel aligned corpus this approach ha been shown to be successful in identifying similar document across language or more precisely retrieving the most similar document in one language to a query in another language however the approach ha severe drawback when applied to a related task that of clustering document language independently so that document about similar topic end up closest to one another in the semantic space regardless of their language the problem is that document are generally more similar to other document in the same language than they are to document in a different language but on the same topic a a result when using multilingual lsa document will in practice cluster by language not by topic we propose a novel application of parafac which is a variant of parafac a multi way generalization of the singular value decomposition svd to overcome this problem instead of forming a single multilingual term by document matrix which under lsa is subjected to svd we form an irregular three way array each slice of which is a separate term by document matrix for a single language in the parallel corpus the goal is to compute an svd for each language such that v the matrix of right singular vector is the same across all language effectively parafac imposes the constraint not present in standard lsa that the concept in all document in the parallel corpus are the same regardless of language intuitively this constraint make sense since the whole purpose of using a parallel corpus is that exactly the same concept are expressed in the translation we tested this approach by comparing the performance of parafac with standard lsa in solving a particular clir problem from our result we conclude that parafac offer a very promising alternative to lsa not only for multilingual document clustering but also for solving other problem in cross language information retrieval 
many neural area notably the hippocampus show structured dynamical population behavior such a coordinated oscillation it ha long been observed that such oscillation provide a substrate for representing analog information in the firing phase of neuron relative to the underlying population rhythm however it ha become increasingly clear that it is essential for neural population to represent uncertainty about the information they capture and the substantial recent work on neural code for uncertainty ha omitted any analysis of oscillatory system here we observe that since neuron in an oscillatory network need not only fire once in each cycle or even at all uncertainty about the analog quantity each neuron represents by it firing phase might naturally be reported through the degree of concentration of the spike that it fire we apply this theory to memory in a model of oscillatory associative recall in hippocampal area ca although it is not well treated in the literature representing and manipulating uncertainty is fundamental to competent memory our theory enables u to view ca a an effective uncertainty aware retrieval system 
we develop conditional random sampling cr a technique particularly suitable for sparse data in large scale application the data are often highly sparse cr combine sketching and sampling in that it convert sketch of the data into conditional random samplesonline in the estimation stage with the sample size determined retrospectively this paper focus on approximating pairwise l and l distance and comparing cr with random projection for boolean data cr is provably better than random projection we show using real world data that cr often outperforms random projection this technique can be applied in learning data mining information retrieval and databas e query optimization 
previous effort on event detection from the web have focused primarily on web content and structure data ignoring the rich collection of web log data in this paper we propose the first approach to detect event from the click through data which is the log data of web search engine the intuition behind event detection from click through data is that such data is often event driven and each event can be represented a a set ofquery page pair that are not only semantically similar but also have similar evolution pattern over time given the click through data in our proposed approach we first segment it into a sequence of bipartite graph based on theuser defined time granularity next the sequence of bipartite graph is represented a a vector based graph which record the semantic and evolutionary relationship between query and page after that the vector based graph is transformed into it dual graph where each node is a query page pair that will be used to represent real world event then the problem of event detection is equivalent to the problem of clustering the dual graph of the vector based graph the clustering process is based on a two phase graph cut algorithm in the first phase query page pair are clustered based on thesemantic based similarity such that each cluster in the result corresponds to a specific topic in the second phase query page pair related to the same topic are further clustered based on the evolution pattern based similarity such that each cluster is expected to represent a specific event under the specific topic experiment with real click through data collected from a commercial web search engine show that the proposed approach produce high quality result 
event related potential erp are brain electrophysiological pattern created by averaging electroencephalographic eeg data time locking to event of interest e g stimulus or response onset in this paper we propose a generic framework for mining anddeveloping domain ontology and apply it to mine brainwave erp ontology the concept and relationship in erp ontology can be mined according to the following step pattern decomposition extraction of summary metric for concept candidate hierarchical clustering of pattern for class and class taxonomy and clustering based classification and association rule mining for relationship axiom of concept we have applied this process to several dense array channel erp datasets result suggest good correspondence between mined concept and rule on the one hand and pattern and rule that were independently formulated by domain expert on the other data mining result also suggest way in which expert defined rule might be refined to improve ontologyrepresentation and classification result the next goal of our erp ontology mining framework is to address some long standing challenge in conducting large scale comparison and integration of result across erp paradigm and laboratory in a more general context this work illustrates the promise of an interdisciplinary research program which combine data mining neuroinformatics andontology engineering to address real world problem 
graph based method for semi supervised learning have recently been shown to be promising for combining labeled and unlabeled data in classification problem however inference for graph based method often doe not scale well to very large data set since it requires inversion of a large matrix or solution of a large linear program moreover such approach are inherently transductive giving prediction for only those point in the unlabeled set and not for an arbitrary test point in this paper a new approach is presented that preserve the strength of graph based semi supervised learning while overcoming the limitation of scalability and non inductive inference through a combination of generative mixture model and discriminative regularization using the graph laplacian experimental result show that this approach preserve the accuracy of purely graph based transductive method when the data ha manifold structure and at the same time achieves inductive learning with significantly reduced computational cost 
current road traffic optimisation practice around the world is a combination of hand tuned policy with a small degree of automatic adaption even state ofthe art research controller need good model of the road traffic which cannot be obtained directly from existing sensor we use a policy gradient reinforcement learning approach to directly optimise the traffic signal mapping currently deployed sensor observation to control signal our trained controller are theoretically compatible with the traffic system used in sydney and many other city around the world we apply two policy gradient method the recent natural actor critic algorithm and a vanilla policy gradient algorithm for comparison along the way we extend natural actor critic approach to work for distributed and online infinite horizon problem 
the profileration of rich social medium on line community and collectively produced knowledge resource ha accelerated the convergence of technological and social network producing environment that reflect both the architecture of the underlying information system and the social structure on their member in studying the consequence of these development we are faced with the opportunity to analyze social network data at unprecedented level of scale and temporal resolution this ha led to a growing body of research at the intersection of the computing and social science we discus some of the current challenge in the analysis of large scale social network data focusing on two theme in particular the inference of social process from data and the problem of maintaining individual privacy in study of social network while early research on this type of data focused on structural question recent work ha extended this to consider the social process that unfold within the network particular line of investigation have focused on process in on line social system related to communication community formation information seeking and collective problem solving marketing the spread of news and the dynamic of popularity there are a number of fundamental issue however for which we have relatively little understanding including the extent to which the outcome of these type of social process are predictable from their early stage see e g the difference between property of individual and property of aggregate population in these type of data and the extent to which similar social phenomenon in different domain have uniform underlying explanation the second theme we pursue is concerned with the problem of privacy while much of the research on large scale social system ha been carried out on data that is public some of the richest emerging source of social interaction data come from setting such a e mail instant messaging or phone communication in which user have strong expectation of privacy how can such data be made available to researcher while protecting the privacy of the individual represented in the data many of the standard approach here are variation on the principle of anonymization the name of individual are replaced with meaningless unique identifier so that the network structure is maintained while private information ha been suppressed in recent joint work with lars backstrom and cynthia dwork we have identified some fundamental limitation on the power of network anonymization to ensure privacy in particular we describe a family of attack such that even from a single anonymized copy of a social network it is possible for an adversary to learn whether edge exist or not between specific targeted pair of node the attack are based on the uniqueness of small random subgraphs embedded in an arbitrary network using idea related to those found in argument from ramsey theory combined with other recent example of privacy breach in data containing rich textual or time series information these result suggest that anonymization contains pitfall even in very simple setting in this way our approach can be seen a a step toward understanding how technique of privacy preserving data mining see e g and the reference therein can inform how we think about the protection of eventhe most skeletal social network data 
the large number of gene and the relatively small number of sample are typical characteristic for microarray data these characteristic pose challenge for both sample classification and relevant gene selection the support vector machine svm is a widely used classification technique and previous study have demonstrated it superior classification performance in microarray analysis however a major limitation is that the svm can not perform automatic gene selection to overcome this limitation we propose the hybrid huberized support vector machine hhsvm the hhsvm us the huberized hinge loss function and the elastic net penalty it ha two major benefit automatic gene selection the grouping effect where highly correlated gene tend to be selected removed together we also develop an efficient algorithm that computes the entire regularized solution path for hhsvm we have applied our method to real microarray data and achieved promising result 
we consider the problem of multi task learning that is learning multiple related function our approach is based on a hierarchical bayesian framework that exploit the equivalence between parametric linear model and nonparametric gaussian process gps the resulting model can be learned easily via an em algorithm empirical study on multi label text categorization suggest that the presented model allow accurate solution of these multi task problem 
we describe a hierarchical compositional system for detecting deformable object in image object are represented by graphical model the algorithm us a hierarchical tree where the root of the tree corresponds to the full object and lower level element of the tree correspond to simpler feature the algorithm proceeds by passing simple message up and down the tree the method work rapidly in under a second on image we demonstrate the approach on detecting cat horse and hand the method work in the presence of background clutter and occlusion our approach is contrasted with more traditional method such a dynamic programming and belief propagation 
we introduce an expectation maximization type em algorithm for maximum likelihood optimization of conditional density it is applicable to hidden variable model where the distribution are from the exponential family the algorithm can alternatively be viewed a automatic step size selection for gradient ascent where the amount of computation is traded off to guarantee that each step increase the likelihood the tradeoff make the algorithm computationally more feasible than the earlier conditional em the method give a theoretical basis for extended baum welch algorithm used in discriminative hidden markov model in speech recognition and compare favourably with the current best method in the experiment 
we propose a new linear method for dimension reduction to identify nongaussian component in high dimensional data our method ngca non gaussian component analysis us a very general semi parametric framework in contrast to existing projection method we dene what is uninteresting gaussian by projecting out uninterestingness we can estimate the relevant non gaussian subspace we show that the estimation error of nding the non gaussian component tends to zero at a parametric rate once ngca component are identied and extracted various task can be applied in the data analysis process like data visualization clustering denoising or classication a numerical study demonstrates the usefulness of our method 
we describe an algorithmic framework for an abstract game which we term a convex repeated game we show that various online learning and boosting algorithm can be all derived a special case of our algorithmic framework this unified view explains the property of existing algorithm and also enables u to derive several new interesting algorithm our algorithmic framework stem from a connection that we build between the notion of regret in game theory and weak duality in convex optimization 
in this paper we study the problem of discovering interesting pattern through user s interactive feedback we assume a set of candidate pattern ie frequent pattern ha already been mined our goal is to help a particular user effectively discover interesting pattern according to his specific interest without requiring a user to explicitly construct a prior knowledge to measure the interestingness of pattern we learn the user s prior knowledge from his interactive feedback we propose two model to represent a user s prior the log linear model and biased belief model the former is designed for item set pattern whereas the latter is also applicable to sequential and structural pattern to learn these model we present a two stage approach progressive shrinking and clustering to select sample pattern for feedback the experimental result on real and synthetic data set demonstrate the effectiveness of our approach 
we introduce a reduction based model for analyzing supervised learning task we use this model to devise a new reduction from multi class cost sensitive classification to binary classification with the following guarantee if the learned binary classifier ha error rate at most then the cost sensitive classifier ha cost at most time the expected sum of cost of all possible lables since cost sensitive classification can embed any bounded loss finite choice supervised learning task this result show that any such task can be solved using a binary classification oracle finally we present experimental result showing that our new reduction outperforms existing algorithm for multi class cost sensitive learning 
we present a method for localizing and separating sound source in stereo recording that is robust to reverberation and doe not make any assumption about the source statistic the method consists of a probabilistic model of binaural multisource recording and an expectation maximization algorithm for finding the maximum likelihood parameter of that model these parameter include distribution over delay and assignment of time frequency region to source we evaluate this method against two comparable algorithm on simulation of simultaneous speech from two or three source our method outperforms the others in anechoic condition and performs a well a the better of the two in the presence of reverberation the method proposed in this paper take a probabilistic approach to localization using the psychoacoustic cue of interaural phase difference ipd unlike previous approach this em algorithm estimate true probability distribution over both the direction from which sound originate and the region of the time frequency plane associated with each sound source the basic assumption that make this possible are that a single source dominates each time frequency point and that a single delay and amplification cause the difference in the ear signal at a particular point by modelling the observed ipd in this way this method overcomes many of the limitation of other system it is able to localize more source than it ha observation even in reverberant environment it make no assumption about the statistic of the source signal making it well suited to localizing speech a highly non gaussian and non stationary signal it probabilistic nature also facilitates the incorporation of other probabilistic cue for source separation such a those obtained from single microphone computational auditory scene analysis many comparable method are also based on ipd but they first convert it into interaural time difference because of the inherent ambiguity in phase difference this mapping is one to one only up to a certain frequency our system however is able to use observation across the entire frequency 
to learn concept over massive data stream it is essential to design inference and learning method that operate in real time with limited memory online learning method such a perceptron or winnow are naturally suited to stream processing however in practice multiple pass over the same training data are required to achieve accuracy comparable to state of the art batch learner in the current work we address the problem of training an on line learner with a single passover the data we evaluate several existing method and also propose a new modification of margin balanced winnow which ha performance comparable to linear svm we also explore the effect of averaging a k a voting on online learning finally we describe how the new modified margin balanced winnow algorithm can be naturally adapted to perform feature selection this scheme performs comparably to widely used batch feature selection method like information gain or chi square with the advantage of being able to select feature on the fly taken together these technique allow single pas online learning to be competitive with batch technique and still maintain the advantage of on line learning 
we investigate a new convex relaxation of an expectation maximization em variant that approximates a standard objective while eliminating local minimum first a cautionary result is presented showing that any convex relaxation of em over hidden variable must give trivial result if any dependence on the missing value is retained although this appears to be a strong negative outcome we then demonstrate how the problem can be bypassed by using equivalence relation instead of value assignment over hidden variable in particular we develop new algorithm for estimating exponential conditional model that only require equivalence relation information over the variable value this reformulation lead to an exact expression for em variant in a wide range of problem we then develop a semidefinite relaxation that yield global training by eliminating local minimum 
we introduce model for density estimation with multiple hidden continuous factor in particular we propose a generalization of multilinear model using nonlinear basis function by marginalizing over the weight we obtain a multifactor form of the gaussian process latent variable model in this model each factor is kernelized independently allowing nonlinear mapping from any particular factor to the data we learn model for human locomotion data in which each pose is generated by factor representing the person s identity gait and the current state of motion we demonstrate our approach using time series prediction and by synthesizing novel animation from the model 
neuron can have rapidly changing spike train statistic dictated by the underlying network excitability or behavioural state of an animal to estimate the time course of such state dynamic from singleor multiple neuron recording we have developed an algorithm that maximizes the likelihood of observed spike train by optimizing the state lifetime and the state conditional interspike interval isi distribution our nonparametric algorithm is free of time binning and spike counting problem and ha the computational complexity of a mixed state markov model operating on a state sequence of length equal to the total number of recorded spike a an example we fit a two state model to paired recording of premotor neuron in the sleeping songbird we find that the two state conditional isi function are highly similar to the one measured during waking and singing respectively 
comparative evaluation of machine learning ml system used for information extraction ie ha suffered from various inconsistency in experimental procedure this paper report on the result of the pascal challenge on evaluating machine learning for information extraction which provides a standardised corpus set of task and evaluation methodology the challenge is described and the system submitted by the ten participant are briefly introduced and their performance is analysed 
we propose a generic algorithm for computation of similarity measure for sequential data the algorithm us generalized suffix tree f or efficient calculation of various kernel distance and non metric similarity func tions it worst case run time is linear in the length of sequence and independent of the underlying embedding language which can cover word k gram or all contained subsequence experiment with network intrusion detection dna analysis and text processing application demonstrate the utility of distan ce and similarity coefficient for sequence a alternative to classical kernel fu nctions 
semi supervised method use unlabeled data in addition to labeled data to construct predictor while existing semi supervised method have shown some promising empirical performance their development ha been based largely based on heuristic in this paper we study semi supervised learning from the viewpoint of minimax theory our first result show that some common met hod based on regularization using graph laplacians do not lead to faster minimax rate of convergence thus the estimator that use the unlabeled data do not have smaller risk than the estimator that use only labeled data we then develop several new approach that provably lead to improved performance the statistical tool of minimax analysis are thus used to offer some new perspective on the problem of semi supervised learning 
abstract experimental study have observed synaptic potentiation when a presynaptic neuron flres shortly before a postsynaptic neuron and synaptic depression when the presynaptic neuron flres shortly after the dependence of synaptic modulation on the precise timing of the two action potential is known a spike timing dependent plasticity or stdp we derive stdp from a simple computational principle synapsis adapt so a to minimize the postsynaptic neuron s variability to a given presynaptic input causing the neuron s output to become more reliable in the face of noise using an entropy minimization objective function and the biophysically realistic spike response model of gerstner we simulate neurophysiological experiment and obtain the characteristic stdp curve along with other phenomenon including the reduction in synaptic plasticity a synaptic e cacy increase we compare our account to other efiorts to derive stdp from computational principle and argue that our account provides the most comprehensive coverage of the phenomenon thus reliability of neural response in the face of noise may be a key goal of cortical adaptation 
the classical bayes rule computes the posterior model probability from the prior probability and the data likelihood we generalize this rule to the case when the prior is a density matrix symmetric positive definite and trace one and the data likelihood a covariance matrix the classical bayes rule is retained a the special case when the matrix are diagonal in the classical setting the calculation of the probability of the data is an expected likelihood where the expectation is over the prior distribution in the generalized setting this is replaced by an expected variance calculation where the variance is computed along the eigenvectors of the prior density matrix and the expectation is over the eigenvalue of the density matrix which form a probability vector the variance along any direction is determined by the covariance matrix curiously enough this expected variance calculation is a quantum measurement where the co variance matrix specifies the instrument and the prior density matrix the mixture state of the particle we motivate both the classical and the generalized bayes rule with a minimum relative entropy principle where the kullbach leibler version give the classical bayes rule and umegaki s quantum relative entropy the new bayes rule for density matrix 
attempting to model human categorization and similarity judgement is both a very interesting but also an exceedingly difficult challenge some of the difficulty arises because of conflicting evidence whether human categorization and similarity judgement should or should not be modelled a to operate on a mental representation that is essentially metric intuitively this ha a strong appeal a it would allow dis similarity to be represented geometrically a distance in some internal space here we show how a single stimulus carefully constructed in a psychophysical experiment introduces l violation in what used to be an internal similarity space that could be adequately modelled a euclidean we term this one influential data point a conflictual judgement we present an algorithm of how to analyse such data and how to identify the crucial point thus there may not be a strict dichotomy between either a metric or a non metric internal space but rather degree to which potentially large subset of stimulus are represented metrically with a small subset causing a global violation of metricity 
we use the k core decomposition to develop algorithm for the analysis of large scale complex network this decomposition based on a recursive pruning of the least connected vertex allows to disentangle the hierarchical structure of network by progressively focusing on their central core by using this strategy we develop a general visualization algorithm that can be used to compare the structural property of various network and highlight their hierarchical structure the low computational complexity of the algorithm o n e where n is the size of the network ande is the number of edge make it suitable for the visualization of very large sparse network we show how the proposed visualization tool allows to find specific structural fingerprint of network 
bayesian model of multisensory perception traditionally address the problem of estimating an underlying variable that is assumed to be the cause of the two sensory signal the brain however ha to solve a more general problem it also ha to establish which signal come from the same source and should be integrated and which one do not and should be segregated in the last couple of year a few model have been proposed to solve this problem in a bayesian fashion one of these ha the strength that it formalizes the causal structure of sensory signal we first compare these model on a formal level furthermore we conduct a psychophysics experiment to test human performance in an auditory visual spatial localization task in which integration is not mandatory we find that the causal bayesian inference model account for the data better than other model 
this paper investigates compression of d object in computer graphic using manifold learning spectral compression us the eigenvectors of the graph laplacian of an object s topology to adaptively compress d object d compression is a challenging application domain object model can have vertex and reliably computing the basis function on large graph is numerically challenging in this paper we introduce a novel multiscale manifold learning approach to d mesh compression using diffusion wavelet a general extension of wavelet to graph with arbitrary topology unlike the global nature of laplacian base diffusion wavelet base are compact and multiscale in nature we decompose large graph using a fast graph partitioning method and combine local multiscale wavelet base computed on each subgraph we present result showing that multiscale diffusion wavelet base are superior to the laplacian base for adaptive compression of large d object 
abstract some of the most effective recent method for content based image classification work by extracting dense or sparse local image descriptor quantizing them according to a coding rule such a k mean vector quantization accumulating histogram of the resulting visual word code over the image and classifying these with a conventional classifier such a an svm large number of descriptor and large codebooks are needed for good result and this becomes slow using k mean we introduce extremely randomized clustering forest ensemble of randomly created clustering tree and show that these provide more accurate result much faster training and testing and good resistance to background clutter in several state of the art image classification task 
we present a document routing and index partitioning scheme for scalable similarity based search of document in a large corpus we consider the case when similarity based search is performed by finding document that have feature in common with the query document while it is possible to store all the feature of all the document in one index this suffers from obvious scalability problem our approach is to partition the feature index into multiple smaller partition that can be hosted on separate server enabling scalable and parallel search execution when a document is ingested into the repository a small number of partition are chosen to store the feature of the document to perform similarity based search also only a small number of partition are queried our approach is stateless and incremental the decision a to which partition the feature of the document should be routed to for storing at ingestion time and for similarity based search at query time is solely based on the feature of the document our approach scale very well we show that executing similarity based search over such a partitioned search space ha minimal impact on the precision and recall of search result even though every search consults le than of the total number of partition 
this paper present representation and logic for labeling contrast edge and ridge in visual scene in term of both surface occlusion border ownership and thinline object in natural scene thinline object include stick and wire while in human graphical communication thinlines include connector divider and other abstract device our analysis is directed at both natural and graphical domain the basic problem is to formulate the logic of the interaction among local image event specifically contrast edge ridge junction and alignment relation such a to encode the natural constraint among these event in visual scene in a sparse heterogeneous markov random field framework we define a set of interpretation node and energy potential function among them the minimum energy configuration found by loopy belief propagation is shown to correspond to preferred human interpretation across a wide range of prototypical example including important illusory contour figure such a the kanizsa triangle a well a more difficult example in practical term the approach delivers correct interpretation of inherently ambiguous hand drawn box and connector diagram at low computational cost 
we present a model that learns the influence of interacting markov chain within a team the proposed model is a dynamic bayesian network dbn with a two level structure individual level and group level individual level model action of each player and the group level model action of the team a a whole experiment on synthetic multi player game and a multi party meeting corpus show the effectiveness of the proposed model 
traditional approach to system management have been largely based on domain expert through a knowledge acquisition process that translates domain knowledge into operating rule and policy this ha been well known and experienced a a cumbersome labor intensive and error prone process in addition this process is difficult to keep up with the rapidly changing environment in this paper we will describe our research effort on establishing an integrated framework for mining system log file for automatic management in particular we apply text mining technique to categorize message in log file into common situation improve categorization accuracy by considering the temporal characteristic of log message develop temporal mining technique to discover the relationship between different event and utilize visualization tool to evaluate and validate the interesting temporal pattern for system management 
we propose a family of clustering algorithm based on the maximization of dependence between the input variable and their cluster label a expressed by the hilbert schmidt independence criterion hsic under this framework we unify the geometric spectral and statistical dependence view of clustering and subsume many existing algorithm a special case e g k mean and spectral clustering distinctive to our framework is that kernel can also be applied on the label which can endow them with particular structure we also obtain a perturbation bound on the change in k mean clustering 
we focus on large graph where node have attribute such a a social network where the node are labelled with each person s job title in such a setting we want to find subgraphs that match a user query pattern for example a star query would be find a ceo who ha strong interaction with a manager a lawyer and an accountant or another structure a close to that a possible similarly a loop query could help spot a money laundering ring traditional sql based method a well a more recent graph indexing method will return no answer when an exact match doe not exist this is the first main feature of our method it can find exact a well a near match and it will present them to the user in our proposed goodness order for example our method tolerates indirect path between say the ceo and the accountant of the above sample query when direct path don t exist it second feature is scalability in general if the query ha nq node and the data graph ha n node the problem need polynomial time complexity o n n q which is prohibitive our g ray graph x ray method find high quality subgraphs in time linear on the size of the data graph experimental result on the dlbp author publication graph with k node and m edge illustrate both the effectiveness and scalability of our approach the result agree with our intuition and the speed is excellent it take second on average forum node query on the dblp graph 
we show that the relevant information about a classification problem in feature space is contained up to negligible error in a finite number of leading kernel pca component if the kernel match the underlying learning problem thus kernel not only transform data set such that good generalization can be achieved even by linear discriminant function but this transformation is also performed in a manner which make economic use of feature space dimension in the best case kernel provide efficient implicit representation of the data to perform classification practically we propose an algorithm which enables u to recover the subspace and dimensionality relevant for good classification our algorithm can therefore be applied to analyze the interplay of data set and kernel in a geometric fashion to help in model selection and to de noise in feature space in order to yield better classification result 
naive bayes model have been widely used for clustering and classification however they are seldom used for general probabilistic learning and inference i e for estimating and computing arbitrary joint conditional and marginal distribution in this paper we show that for a wide range of benchmark datasets naive bayes model learned using em have accuracy and learning time comparable to bayesian network with context specific independence most significantly naive bayes inference is order of magnitude faster than bayesian network inference using gibbs sampling and belief propagation this make naive bayes model a very attractive alternative to bayesian network for general probability estimation particularly in large or real time domain 
we propose a novel framework for the classification of single trial electroencephalography eeg based on regularized logistic regression framed in this robust statistical framework no prior feature extraction or outlier removal is required we present two variation of parameterizing the regression function a with a full rank symmetric matrix coecient and b a a dierence of two rank matrix in the first case the problem is convex and the logistic regression is optimal under a generative model the latter case is shown to be related to the common spatial pattern csp algorithm which is a popular technique in brain computer interfacing the regression coecients can also be topographically mapped onto the scalp similarly to csp projection which allows neuro physiological interpretation simulation on bci datasets demonstrate that classification accuracy and robustness compare favorably against conventional csp based classifier 
commercial relational database currently store vast amount of real world data the data within these relational repository are represented by multiple relation which are inter connected by mean of foreign key join the mining of such interrelated data pose a major challenge to the data mining community unfortunately traditional data mining algorithm usually only explore one relation the so called target relation thus excluding crucial knowledge embedded in the related so called background relation in this paper we propose a novel approach for classifying relational such domain this strategy employ multiple view to capture crucial information not only from the target relation but also from related relation this information is integrated into the relational mining process the framework presented here firstly explore the relational domain to partition it feature space into multiple subset subsequently these subset are used to construct multiple uncorrelated view based on a novel correlation based view validation method against the target concept finally the knowledge possessed by multiple view are incorporated into a meta learning mechanism to augment one another based on this framework a wide range of conventional data mining method can be applied to mine relational database our experiment on benchmark real world data set show that the proposed method achieves promising result both in term of overall accuracy obtained and run time when compared with two other relational data mining approach 
we describe a neuromorphic chip that us binary synapsis with spike timing dependent plasticity stdp to learn stimulated pattern of activity and to compensate for variability in excitability specifically stdp preferentially potentiates turn on synapsis that project from excitable neuron which spike early to lethargic neuron which spike late the additional excitatory synaptic current make lethargic neuron spike earlier thereby causing neuron that belong to the same pattern to spike in synchrony once learned an entire pattern can be recalled by stimulating a subset variability in neural system evidence suggests precise spike timing is important in neural coding specifically in the hippocampus the hippocampus us timing in the spike activity of place cell in addition to rate to encode location in space place cell employ a phase code the timing at which a neuron spike relative to the phase of the inhibitorytheta rhythm hz conveys information a an animal approach a place cell s preferred location the place cell not only increase it spike rate but also spike at earlier phase in the theta cycle to implement a phase code the theta rhythm is thought to prevent spiking until the input synaptic current exceeds the sum of the neuron threshold and the decreasing inhibition on the downward phase of the cycle however even with identical input and common theta inhibition neuron do not spike in synchrony variability in excitability spread the activity in phase lethargic neuron such a those with high threshold spike late in the theta cycle since their input exceeds the sum of the neuron threshold and theta inhibition only after the theta inhibition ha had time to decrease conversely excitable neuron such a those with low threshold spike early in the theta cycle consequently variability in excitability translates into variability in timing we hypothesize that the hippocampus achieves it precise spike timing about m throughplasticity enhanced phase coding pep the source of hippocampal timing precision in the presence of variability and noise remains unexplained synaptic plasticity can compensate for variability in excitability if it increase excitatory synaptic input to neuron in inverse proportionto theirexcitabilities recasting this in a phase coding framework we desire a learning rule that increase excitatory synaptic input to neuron directly related to their phase neuron that lag require additional synaptic input whereas neuron that lead 
we present a new approach to personalized handwriting recognition the problem also known a writer adaptation consists of converting a generic user independent recognizer into a personalized user dependent one which ha an improved recognition rate for a particular user the adaptation step usually involves user specific sample which lead to the fundamental question of how to fuse this new information with that captured by the generic recognizer we propose adapting the recognizer by minimizing a regularized risk functional a modified svm where the prior knowledge from the generic recognizer enters through a modified regularization term the result is a simple personalization framework with very good practical property experiment on a class real world data set show that the number of error can be reduced by over with a few a five user sample per character 
we propose a formulation of the decision tree learning algorithm in the compression setting and derive tight generalization error bound in particular we propose sample compression and occam s razor bound we show how such bound unlike the vc dimension or rademacher complexity based bound are more general and can also perform a margin sparsity trade off to obtain better classifers potentially these risk bound can also guide the model selection process and replace traditional pruning strategy 
high dimensionality of pomdp s belief state space is one major cause that make the underlying optimal policy computation intractable belief compression refers to the methodology that project the belief state space to a low dimensional one to alleviate the problem in this paper we propose a novel orthogonal non negative matrix factorization o nmf for the projection the proposed o nmf not only factor the belief state space by minimizing the reconstruction error but also allows the compressed pomdp formulation to be efficiently computed due to it orthogonality in a value directed manner so that the value function will take same value for corresponding belief state in the original and compressed state space we have tested the proposed approach using a number of benchmark problem and the empirical result confirms it effectiveness in achieving substantial computational cost saving in policy computation 
the computation required for gaussian process regression with n training example is about o n during training and o n for each prediction this make gaussian process regression too slow for large datasets in this paper we present a fast approximation method based on kd tree that signicantly reduces both the prediction and the training time of gaussian process regression 
we study the relation between notion of game theoretic equilibrium which are based on stability under a set of deviation and empirical equilibrium which are reached by rational player rational player are modeled by player using no regret algorithm which guarantee that their payoff in the long run is close to the maximum they could hope to achieve by consistently deviating from the algorithm s suggested action we show that for a given set of deviation over the strategy set of a player it is possible to efficiently approximate fixed point of a given deviation if and only if there exist efficient no regret algorithm resistant to the deviation further we show that if all player use a no regret algorithm then the empirical distribution of their play converges to an equilibrium 
a geometric and non parametric procedure for testing if two finite set of point are linearly separable is proposed the linear separability test is equivalent to a test that determines if a strictly positive point h exists in the range of a matrix a related to the point in the two finite set the algorithm proposed in the paper iteratively check if a strictly positive point exists in a subspace by projecting a strictly positive vector with equal co ordinate p on the subspace at the end of each iteration the subspace is reduced to a lower dimensional subspace the test is completed within r min n d step for both linearly separable and non separable problem r is the rank of a n is the number of point and d is the dimension of the space containing the point the worst case time complexity of the algorithm is o nr and space complexity of the algorithm is o nd a small review of some of the prominent algorithm and their time complexity is included the worst case computational complexity of our algorithm is lower than the worst case computational complexity of simplex perceptron support vector machine and convex hull algorithm if d 
many computer aided diagnosis cad problem can be best modelled a a multiple instance learning mil problem with unbalanced data i e the training data typically consists of a few positive bag and a very lar ge number of negative instance existing mil algorithm are much too computationally expensive for these datasets we describe ch a framework for learning a convex hull representation of multiple instance that is significantly faster than existing mil algorithm our ch framework applies to any standard hyperplane based learning algorithm and for some algorithm is guaranteed to find the global optimal solution experimental study on two different cad application further demonstrate that the proposed algorithm significantly improves diagnos tic accuracy when compared to both mil and traditional classifier although not d esigned for standard mil problem which have both positive and negative bag and relatively balanced datasets comparison against other mil method on benchmark problem also indicate that the proposed method is competitive with the state of the art 
we present a new approach to multiple instance learning mil that is particularly effective when the positive bag are sparse i e contain few positive instance unlike other svm based mil method our approach more directly enforces the desired constraint that at least one of the instance in a positive bag is positive using both artificial and real world data we experimentally demonstrate that our approach achieves greater accuracy than state of the art mil method when positive bag are sparse and performs competitively when they are not in particular our approach is the best performing method for image region classification 
we develop an approach for estimation with gaussian markov process that imposes a smoothness prior while allowing for discontinuity instead of propagating information laterally between neighboring node in a graph we study the posterior distribution of the hidden node a a whole how it is perturbed by invoking discontinuity or weakening the edge in the graph we show that the resulting computation amount to feed forward fan in operation reminiscent of v neuron moreover using suitable matrix preconditioners the incurred matrix inverse and determinant can be approximated without iteration in the same computational style simulation result illustrate the merit of this approach 
the core vector machine cvm is a recent approach for scaling up kernel method based on the notion of minimum enclosing ball meb though conceptually simple an efficient implementation still requires a sophisticated numerical solver in this paper we introduce the enclosing ball eb problem where the ball s radius is fixed and thus doe not have to be minimized we develop efficient e approximation algorithm that are simple to implement and do not require any numerical solver for the gaussian kernel in particular a suitable choice of this fixed radius is easy to determine and the center obtained from the e approximation of this eb problem is close to the center of the corresponding meb experimental result show that the proposed algorithm ha accuracy comparable to the other large scale svm implementation but can handle very large data set and is even faster than the cvm in general 
c by john von neumann institute for computing permission to make digital or hard copy of portion of this work for personal or classroom use is granted provided that the copy are not made or distributed for profit or commercial advantage and th at copy bear this notice and the full citation on the first page to cop y otherwise requires prior specific permission by the publisher mention ed above 
we study the statistical convergence and consistency of regularized boosting method where the sample are not independent and identically distributed i i d but come from empirical process of stationary fl mixing sequence utilizing a technique that construct a sequence of independent block close in distribution to the original sample we prove the consistency of the composite classifier resulting from a regularization achieved by restricting the norm of the base classifier weight when compared to the i i d case the nature of sampling manifest in the consistency result only through generalization of the original condition on the growth of the regularization parameter 
gaussian process are attractive model for probabilistic classification but unfortunately exact inference is analytically intractable we compare laplace s method and expectation propagation ep focusing on marginal likelihood estimate and predictive performance we explain theoretically and corroborate empirically that ep is superior to laplace we also compare to a sophisticated mcmc scheme and show that ep is surprisingly accurate in recent year model based on gaussian process gp prior have attracted much attention in the machine learning community whereas inference in the gp regression model with gaussian noise can be done analytically probabilistic classification using gps is analytically intractable several approach to approximate bayesian inference have been suggested including laplace s approximation expectation propagation ep variational approximation and markov chain monte carlo mcmc sampling some of these in conjunction with generalisation bound online learning scheme and sparse approximation despite the abundance of recent work on probabilistic gp classifier most experimental study provide only anecdotal evidence and no clear picture ha yet emerged a to when and why which algorithm should be preferred thus from a practitioner point of view probabilistic gp classification remains a jungle in this paper we set out to understand and compare two of the most wide spread approximation laplace s method and expectation propagation ep we also compare to a sophisticated but computationally demanding mcmc scheme to examine how close the approximation are to ground truth we examine two aspect of the approximation scheme firstly the accuracy of approximation to the marginal likelihood which is of central importance for model selection and model comparison in any practical application of gps in classification usually multiple parameter of the covariance function hyperparameters have to be handled bayesian model selection provides a consistent framework for setting such parameter therefore it is essential to evaluate the accuracy of the marginal likelihood approximation a a function of the hyperparameters in order to ass the practical usefulness of the approach secondly we need to ass the quality of the approximate probabilistic prediction in the past the probabilistic nature of the gp prediction have not received much attention the focus being mostly on classification error rate this unfortunate state of affair is caused primarily by typical benchmarking problem being considered outside of a realistic context the ability of a classifier to produce class probability or confidence have obvious 
conditional log linear model are a commonly used method for structured prediction efficient learning of parameter in these model is therefore an important problem this paper describes an exponentiated gradient eg algorithm for training such model eg is applied to the convex dual of the maximum likelihood objective this result in both sequential and parallel update algorithm where in the sequential algorithm parameter are updated in an online fashion we provide a convergence proof for both algorithm our analysis also simplifies previous result on eg for max margin model and lead to a tighter bound on convergence rate experiment on a large scale parsing task show that the proposed algorithm converges much faster than conjugate gradient and l bfgs approach both in term of optimization objective and test error 
there ha been a recent growing interest in classification and link prediction in structured domain method such a crfs lafferty et al and rmns taskar et al support flexible mechanism for modeling correlation due to the link structure in addition in many structured domain there is an interesting structure in the risk or cost function associated with different misclassifications there is a rich tradition of cost sensitive learning applied to unstructured iid data here we propose a general framework which can capture correlation in the link structure and handle structured cost function we present a novel cost sensitive structured classifier based on maximum entropy principle that directly determines the cost sensitive classification we contrast this with an approach which employ a standard loss structured classifier followed by minimization of the expected cost of misclassification we demonstrate the utility of our proposed classifier with experiment on both synthetic and real world data 
cooperative competitive network are believed to play a central role in cortical processing and have been shown to exhibit a wide set of useful computational property we propose a vlsi implementation of a spiking cooperative competitive network and show how it can perform context dependent computation both in the mean firing rate domain and in spike timing correlation space in the mean rate case the network amplifies the activity of neuron belonging to the selected stimulus and suppresses the activity of neuron receiving weaker stimulus in the event correlation case the recurrent network amplifies with a higher gain the correlation between neuron which receive highly correlated input while leaving the mean firing rate unaltered we describe the network architecture and present experimental data demonstrating it context dependent computation capability 
the problem of learning a transduction that is a string to string mapping is a common problem arising in natural language processing and computational biology previous method proposed for learning such mapping are based on classification technique this paper present a new and general regression technique for learning transduction and report the result of experiment showing it effectiveness our transduction learning consists of two phase the estimation of a set of regression coefficient and the computation of the pre image corresponding to this set of coefficient a novel and conceptually cleaner formulation of kernel dependency estimation provides a simple framework for estimating the regression coefficient and an efficient algorithm for computing the pre image from the regression coefficient extends the applicability of kernel dependency estimation to output sequence we report the result of a series of experiment illustrating the application of our regression technique for learning transduction 
learning algorithm often obtain relatively low average payoff in repeated general sum game between other learning agent due to a focus on myopic best response and one shot nash equilibrium ne strategy a le myopic approach place focus on ne of the repeated game which suggests that at the least a learning agent should posse two property first an agent should never learn to play a strategy that produce average payoff le than the minimax value of the game second an agent should learn to cooperate compromise when beneficial no learning algorithm from the literature is known to posse both of these property we present a reinforcement learning algorithm m qubed that provably satisfies the first property and empirically display in self play the second property in a wide range of game 
data clustering represents an important tool in exploratory data analysis the lack of objective criterion render model selection a well a the identification of robust solution particularly difficult the use of a stability assessment and the combination of multiple clustering solution represents an important ingredient to achieve the goal of finding useful partition in this work we propose a novel way of combining multiple clustering solution for both hard and soft partition the approach is based on modeling the probability that two object are grouped together an efficient em optimization strategy is employed in order to estimate the model parameter our proposal can also be extended in order to emphasize the signal more strongly by weighting individual base clustering solution according to their consistency with the prediction for previously unseen object in addition to that the probabilistic model support an out of sample extension that i make it possible to assign previously unseen object to class of the combined solution and ii render the efficient aggregation of solution possible in this work we also shed some light on the usefulness of such combination approach in the experimental result section we demonstrate the competitive performance of our proposal in comparison with other recently proposed method for combining multiple classification of a finite data set 
supervised clustering is the problem of training a clustering algorithm to produce desirable clustering given set of item and complete clustering over these set we learn how to cluster future set of item example application include noun phrase coreference clustering and clustering news article by whether they refer to the same topic in this paper we present an svm algorithm that train a clustering algorithm by adapting the item pair similarity measure the algorithm may optimize a variety of dierent clustering function to a variety of clustering performance measure we empirically evaluate the algorithm for noun phrase and news article clustering 
automatic relevance determination ard and the closely related sparse bayesian learning sbl framework are effective tool for pruning large number of irrelevant feature leading to a sparse explanatory subset however popular update rule used for ard are either difcult to extend to more general problem of interest or are characterized by non ideal convergence property moreover it remains unclear exactly how ard relates to more traditional map estimation based method for learning sparse representation e g the lasso this paper furnishes an alternative mean of expressing the ard cost function using auxiliary function that naturally address both of these issue first the proposed reformulation of ard can naturally be optimized by solving a series of re weighted problem the result is an efcient extensible algorithm that can be implemented using standard convex programming toolbox and is guaranteed to converge to a local minimum or saddle point secondly the analysis reveals that ard is exactly equivalent to performing standard map estimation in weight space using a particular featureand noise dependent non factorial weight prior we then demonstrate that this implicit prior maintains several desirable advantage over conventional prior with respect to feature selection overall these result suggest alternative cost function and update procedure for selecting feature and promoting sparse solution in a variety of general situation in particular the methodology readily extends to handle problem such a non negative sparse coding and covariance component estimation 
if we have found a good clustering c of a data set can we prove that c is not far from the unknown best clustering copt of these data perhaps surprisingly the answer to this question is sometimes yes when goodness is measured by the distortion of k mean clustering this paper prof spectral bound on the distance d c copt the bound exist in the case when the data admits a low distortion clustering 
stress and genetic background regulate different aspect of behavioral learning through the action of stress hormone and neuromodulators in reinforcement learning rl model meta parameter such a learning rate future reward discount factor and exploitation exploration factor contr ol learning dynamic and performance they are hypothesized to be related to neuromodulatory level in the brain we found that many aspect of animal learning and performance can be described by simple rl model using dynamic control of the meta parameter to study the effect of stress and genotype we carried out hole box light conditioning and morris water maze experiment with c bl and dba mouse strain the animal were exposed to different kind of stress to evaluate it effect on immediate performance a well a on long term memory then we used rl model to simulate their behavior for each experimental sessi on we estimated a set of model meta parameter that produced the best fit between t he model and the animal performance the dynamic of several estimated meta parameter were qualitatively similar for the two simulated experiment a nd with statistically significant difference between different genetic strain and stress condition 
in the model based policy search approach to reinforcement learning rl policy are found using a model or simulator of the markov decision process however for high dimensional continuous state task it can be extremely difficult to build an accurate model and thus often the algorithm return a policy that work in simulation but not in real life the other extreme model free rl tends to require infeasibly large number of real life trial in this paper we present a hybrid algorithm that requires only an approximate model and only a small number of real life trial the key idea is to successively ground the policy evaluation using real life trial but to rely on the approximate model to suggest local change our theoretical result show that this algorithm achieves near optimal performance in the real system even when the model is only approximate empirical result also demonstrate that when given only a crude model and a small number of real life trial our algorithm can obtain near optimal performance in the real system 
the uct algorithm learns a value function online using sample based search the td algorithm can learn a value function offline for the on policy distribution we consider three approach for combining offline and online value function in the uct algorithm first the offline value function is used a a default policy during monte carlo simulation second the uct value function is combined with a rapid online estimate of action value third the offline value function is used a prior knowledge in the uct search tree we evaluate these algorithm in x go against gnugo the first algorithm performs better than uct with a random simulation policy but surprisingly worse than uct with a weaker handcrafted simulation policy the second algorithm outperforms uct altogether the third algorithm outperforms uct with handcrafted prior knowledge we combine these algorithm in mogo the world s strongest x go program each technique significantly improves mogo s playing strength 
computation of a satisfactory control policy for a markov decision process when the parameter of the model are not exactly known is a problem encountered in many practical application the traditional robust appro ach is based on a worstcase analysis and may lead to an overly conservative policy in this paper we consider the tradeoff between nominal performance and the worst case performance over all possible model based on parametric linear programming we propose a method that computes the whole set of pareto efficient policy in the performancerobustness plane when only the reward parameter are subject to uncertainty in the more general case when the transition probability are also subject to error we show that the strategy with the optimal tradeoff might be non markovian and hence is in general not tractable 
the recent predictive linear gaussian model or plg improves upon traditional linear dynamical system model by using a predictive representation of state which make consistent parameter estimation possible without any loss of modeling power and while using fewer parameter in this paper we extend the plg to model stochastic nonlinear dynamical system by using kernel method with a gaussian kernel the model admits closed form solution to the state update equation due to conjugacy between the dynamic and the state representation we also explore an efficient sigma point approximation to the state update and show how all of the model parameter can be learned directly from data and can be learned on line with the kernel recursive least square algorithm we empirically compare the model and it approximation to the original plg and discus their relative advantage 
many real world sequence learning task require the prediction of sequence of label from noisy unsegmented input data in speech recognition for example an acoustic signal is transcribed into word or sub word unit recurrent neural network rnns are powerful sequence learner that would seem well suited to such task however because they require pre segmented training data and post processing to transform their output into label sequence their applicability ha so far been limited this paper present a novel method for training rnns to label unsegmented sequence directly thereby solving both problem an experiment on the timit speech corpus demonstrates it advantage over both a baseline hmm and a hybrid hmm rnn 
we argue that in many circumstance human observer evaluate sensory evidence simultaneously under multiple hypothesis regarding the physical process that ha generated the sensory information in such situation inference can be optimal if an observer combine the evaluation result under each hypothesis according to the probability that the associated hypothesis is correct however a number of experimental result reveal suboptimal behavior and may be explained by assuming that once an observer ha committed to a particular hypothesis subsequent evaluation is based on that hypothesis alone that is observer sacrifice optimality in order to ensure self consistency we formulate this behavior using a conditional bayesian observer model and demonstrate that it can account for psychophysical data from a recently reported perceptual experiment in which strong bias in perceptual estimate arise a a consequence of a preceding decision not only doe the model provide quantitative prediction of subjective response in variant of the original experiment but it also appears to be consistent with human response to cognitive dissonance 
this paper describes a gaussian process framework for inferring pixel wise disparity and bi layer segmentation of a scene given a stereo pair of image the gaussian process covariance is parameterized by a foreground backgroundocclusion segmentation label to model both smooth region and discontinuity a such we call our model a switched gaussian process we propose a greedy incremental algorithm for adding observation from the data and assigning segmentation label two observation schedule are proposed the first treat scanlines a independent the second us an active learning criterion to select a sparse subset of point to measure we show that this probabilistic framework ha comparable performance to the state of the art 
given a classification problem our goal is to find a low dimensional linear transformation of the feature vector which retains information needed to predict the class label we present a method based on maximum conditional likelihood estimation of mixture model use of mixture model allows u to approximate the distribution to any desired accuracy while use of conditional likelihood a the contrast function ensures that the selected subspace retains maximum possible mutual information between feature vector and class label classification experiment using gaussian mixture component show that this method compare favorably to related dimension reduction technique other distribution belonging to the exponential family can be used to reduce dimension when data is of a special type for example binary or integer valued data we provide an em like algorithm for model estimation and present visualization experiment using gaussian and bernoulli mixture model 
this paper explores the statistical relationship between n atural image and their underlying range depth image we look at how this relationship change over scale and how this information can be used to enhance low resolution range data using a full resolution intensity image based on our finding we propose an extension to an existing techni que known a shape recipe and the success of the two method are compared using image and laser scan of real scene our extension is shown to provide a two fold improvement over the current method furthermore we demonstrate that ideal linear shape from shading filter when learned from natural scene may derive even more strength from shadow cue than from the traditional linear lambertian shading cue 
we present a novel approach to the characterization of complex sensory neuron one of the main goal of characterizing sensory neuron is to characterize dimension in stimulus space to which the neuron are highly sensitive causing large gradient in the neural re pons or alternatively dimension in stimulus space to which the neuronal response are invariant defining iso response manifold we formul ate this problem a that of learning a geometry on stimulus space that is compatible with the neural response the distance between stimulus should be large when the response they evoke are very different and small when the response they evoke are similar here we show how to successfully train such distance function using rather limited amount of information the data consisted of the response of neuron in primary auditory cortex a of anesthetized cat to stimulus derived from natural sound for each neuron a subset of all pair of stimulus wa selected such that the response of the two stimulus in a pair were either very similar or very dissimilar the distance function wa trained to fit these co nstraints the resulting distance function generalized to predict the di stance between the response of a test stimulus and the trained stimulus 
we revisit recently proposed algorithm for probabilistic clustering with pair wise constraint between data point we evaluate and compare existing technique in term of robustness to misspecified constraint we show that the technique that strictly enforces the given constraint namely the chunklet model produce poor result even under a small number of misspecified constraint we further show that method that penalize constraint violation are more robust to misspecified constraint but have undesirable local behavior based on this evaluation we propose a new learning technique extending the chunklet model to allow soft constraint represented by an intuitive measure of confidence in the constraint 
the goal of a recommender system is to suggest item of interest to a user based on historical behavior of a community of user given detailed enough history item based collaborative filtering cf often performs a well or better than almost any other recommendation method however in cold start situation where a user an item or the entire system is new simple non personalized recommendation often fare better we improve the scalability and performance of a previous approach to handling cold start situation that us filterbots or surrogate user that rate item based only on user or item attribute we show that introducing a very small number of simple filterbots help make cf algorithm more robust in particular adding just seven global filterbots improves both user based and item based cf in cold start user cold start item and cold start system setting performance is better when data is scarce performance is no worse when data is plentiful and algorithm efficiency is negligibly affected we systematically compare a non personalized baseline user based cf item based cf and our bot augmented userand item based cf algorithm using three data set yahoo movie movielens and eachmovie with the normalized mae metric in three type of cold start situation the advantage of our na ve filterbot approach is most pronounced for the yahoo data the sparsest of the three data set 
in this paper we show that stylistic text feature can be exploited to determine an anonymous author s native language with high accuracy specifically we first use automatic tool to ascertain frequency of various stylistic idiosyncrasy in a text these frequency then serve a feature for support vector machine that learn to classify text according to author native language 
we present a novel message passing algorithm for approximating the map problem in graphical model the algorithm is similar in structure to max product but unlike max product it always converges and can be proven to find the exact map solution in various setting the algorithm is derived via b lock coordinate descent in a dual of the lp relaxation of map but doe not require any tunable parameter such a step size or tree weight we also describe a generalization of the method to cluster based potential the new method is tested on synthetic and real world problem and compare favorably with previous approach graphical model are an effective approach for modeling complex object via local interaction in such model a distribution over a set of variable is assumed to factor according to clique of a graph with potential assigned to each clique finding the assign ment with highest probability in these model is key to using them in practice and is often referred to a the map maximum aposteriori assignment problem in the general case the problem is np hard with complexity exponential in the tree width of the underlying graph linear programming lp relaxation have proven very useful in approximating the map problem and often yield satisfactory empirical result these appr oaches relax the constraint that the solution is integral and generally yield non integral solution h owever when the lp solution is integral it is guaranteed to be the exact map for some class of problem the lp relaxation is provably correct these include the minimum cut problem and maximum weight matching in bi partite graph although lp relaxation can be solved using standard lp solver this may be computationally intensive for large problem the key problem with generic lp solver is that they do not use the graph structure explicitly and thus may be sub optimal in term of computational efficiency the max product method is a message passing algorithm that is often used to approximate the map problem in contrast to generic lp solver it make direct use of the graph structure in constructing and passing message and is also very simple to implement the relation between max product and the lp relaxation ha remained largely elusive although there are some notable exception for tree structured graph max product and lp both yield the exact map a recent result showed that for maximum weight matching on bi partite graph max product and lp also yield the exact map finally tree reweighted max product trmp algorithm were shown to converge to the lp solution for binary xi variable a shown in in this work we propose the max product linear programming algorithm mplp a very simple variation on max product that is guaranteed to converge and ha several advantageous property mplp is derived from the dual of the lp relaxation and is equivalent to block coordinate descent in the dual although this result in monotone improvement of the dual objective global convergence is not always guaranteed since coordinate descent may get stuck in suboptimal point this can be remedied using various approach but in practice we have found mplp to converge to the lp 
while most software defect i e bug are corrected and tested a part of the lengthy software development cycle enterprise software vendor often have to release software product before all reported defect are corrected due to deadline and limited resource a small number of these defect will be escalated by customer and they must be resolved immediately by the software vendor at a very high cost in this paper we develop an escalation prediction ep system that mine historic defect report data and predict the escalation risk of the defect for maximum net profit more specifically we first describe a simple and general framework to convert the maximum net profit problem to cost sensitive learning we then apply and compare several well known cost sensitive learning approach for ep our experiment suggest that the cost sensitive decision tree is the best method for producing the highest positive net profit and comprehensible result the ep system ha been deployed successfully in the product group of an enterprise software vendor 
we present a method for binary on line classification of triggered but temporally blurred event that are embedded in noisy time series in the context of on line discrimination between left and right imaginary hand movement in particular the goal of the binary classification problem is to obtain the decision a fast and a reliably a possible from the recorded eeg single trial to provide a probabilistic decision at every time point t the presented method gather information from two distinct sequence of feature across time in order to incorporate decision from prior time point we suggest an appropriate weighting scheme that emphasizes time instance providing a higher discriminatory power between the instantaneous class distribution of each feature where the discriminatory power is quantified in term of the bayes error of misclassification the eectiveness of this procedure is verified by it successful application in the rd bci competition disclosure of the data after the competition revealed this approach to be superior with single trial error rate a low a and for the three dierent subject under study 
semi supervised inductive learning concern how to learn a decision rule from a data set containing both labeled and unlabeled data several boosting algorithm have been extended to semi supervised learning with various strategy to our knowledge however none of them take local smoothness constraint among data into account during ensemble learning in this paper we introduce a local smoothness regularizer to semi supervised boosting algorithm based on the universal optimization framework of margin cost functionals our regularizer is applicable to existing semi supervised boosting algorithm to improve their generalization and speed up their training comparative result on synthetic benchmark and real world task demonstrate the effectiveness of our local smoothness regularizer we discus relevant issue and relate our regularizer to previous work 
an increasing fraction of the global discourse is migrating online in the form of blog bulletin board web page wikis editorial and a dizzying array of new collaborative technology the migration ha now proceeded to the point that topic reflecting certain individual product are sufficiently popular to allow targeted online tracking of the ebb and flow of chatter around these topic based on an analysis of around half a million sale rank value for book over a period of four month and correlating posting in blog medium and web page we are able to draw several interesting conclusion first carefully hand crafted query produce matching posting whose volume predicts sale rank second these query can be automatically generated in many case and third even though sale rank motion might be difficult to predict in general algorithmic predictor can use online posting to successfully predict spike in sale rank 
this paper explores the issue of recognizing generalizing and reproducing arbitrary gesture we aim at extracting a representation that encapsulates only the key aspect of the gesture and discard the variability intrinsic to each person s motion we compare a decomposition into principal component pca and independent component ica a a first step of preprocessing in order to decorrelate and denoise the data a well a to reduce the dimensionality of the dataset to make this one tractable in a second stage of processing we explore the use of a probabilistic encoding through continuous hidden markov model hmms a a way to encapsulate the sequential nature and intrinsic variability of the motion in stochastic finite state automaton finally the method is validated in a humanoid robot to reproduce a variety of gesture performed by a human demonstrator 
we propose a fast manifold learning algorithm based on the methodology of domain decomposition starting with the set of sample point partitioned into two subdomains we develop the solution of the interface problem that can glue the embeddings on the two subdomains into an embedding on the whole domain we provide a detailed analysis to ass the error produced by the gluing process using matrix perturbation theory numerical example are given to illustrate the efficiency and effectiveness of the proposed method 
we propose a randomized algorithm for large scale svm learning which solves the problem by iterating over random subset of the data crucial to the algorithm for scalability is the size of the subset chosen in the context of text classication we show that by using idea from random projection a sample size of o log n can be used to obtain a solution which is close to the optimal with a high probability experiment done on synthetic and real life data set demonstrate that the algorithm scale up svm learner without loss in accuracy 
categorization is a central activity of human cognition when an individual is asked to categorize a sequence of item context effect arise categorization of one item influence category decision for subsequent item specifically when experimental subject are shown an exemplar of some target category the category prototype appears to be pulled toward the exemplar and the prototype of all nontarget category appear to be pushed away these push and pull effect diminish with experience and likely reflect long term learning of category boundary we propose and evaluate four principled probabilistic bayesian account of context effect in categorization in all four account the probability of an exemplar given a category is encoded a a gaussian density in feature space and categorization involves computing category posterior given an exemplar the model differ in how the uncertainty distribution of category prototype is represented localist or distributed and how it is updated following each experience using a maximum likelihood gradient ascent or a kalman filter update we find that the distributed maximum likelihood model can explain the key experimental phenomenon further the model predicts other phenomenon that were confirmed via reanalysis of the experimental data 
typical approach to the multi label classification problem require learning an independent classifier for every label from all the example and feature this can become a computational bottleneck for sizeable datasets with a large label space in this paper we propose an efficient and effective multi label learning algorithm called model shared subspace boosting mssboost a an attempt to reduce the information redundancy in the learning process this algorithm automatically find share and combine a number of base model across multiple label where each model is learned from random feature subspace and boot trap data sample the decision function for each label are jointly estimated and thus a small number of shared subspace model can support the entire label space our experimental result on both synthetic data and real multimedia collection have demonstrated that the proposed algorithm can achieve better classification performance than the non ensemble baselineclassifiers with a significant speedup in the learning and prediction process it can also use a smaller number of base model to achieve the same classification performance a it non model shared counterpart 
a general analysis of the limiting distribution of neural network function is performed with emphasis on non gaussian limit we show that with i i d symmetric stable output weight and more generally with weight distributed from the normal domain of attraction of a stable variable that the neural function converge in distribution to stable process condition are also investigated under which gaussian limit do occur when the weight are independent but not identically distributed some particularly tractable class of stable distribution are examined and the possibility of learning with such process 
collaborative filtering cf based recommender system are indispensable tool to find item of interest from the unmanageable number of available item moreover company who deploy a cf based recommender system may be able to increase revenue by drawing customer attention to item that they are likely to buy however the sheer number of customer and item typical in e commerce system demand specially designed cf algorithm that can gracefully cope with the vast size of the data many algorithm proposed thus far where the principal concern is recommendation quality may be too expensive to operate in a large scale system we propose clustknn a simple and intuitive algorithm that is well suited for large data set the method first compress data tremendously by building a straightforward but ecient clustering model recommendation are then generated quickly by using a simple nearest neighbor based approach we demonstrate the feasibility of clustknn both analytically and empirically we also show by comparing with a number of other popular cf algorithm that apart from being highly scalable and intuitive clustknn provides very good recommendation accuracy a well 
the gaussian process latent variable model gp lvm is a generative approach to nonlinear low dimensional embedding that provides a smooth probabilistic mapping from latent to data space it is also a non linear generalization of probabilistic pca ppca tipping bishop while most approach to non linear dimensionality method focus on preserving local distance in data space the gp lvm focus on exactly the opposite being a smooth mapping from latent to data space it focus on keeping thing apart in latent space that are far apart in data space in this paper we first provide an overview of dimensionality reduction technique placing the emphasis on the kind of distance relation preserved we then show how the gp lvm can be generalized through back constraint to additionally preserve local distance we give illustrative experiment on common data set 
we study a pattern classification algorithm which ha recently been proposed by vapnik and coworkers it build on a new inductive principle which assumes that in addition to positive and negative data a third class of data is available termed the universum we assay the behavior of the algorithm by establishing link with fisher discriminant analysis and oriented pca a well a with an svm in a projected subspace or equivalently with a data dependent reduced kernel we also provide experimental result 
the integration of diverse form of informative data by learning an optimal combination of base kernel in classification or regression problem can provide enhanced performance when compared to that obtained from any single data source we present a bayesian hierarchical model which enables kernel learning and present effective variational bayes estimator for regression and classification illustrative experiment demonstrate the utility of the proposed method matlab code replicating result reported is available at http www dc gla ac uk srogers kernel comb html 
independent component analysis ica is a powerful method to decouple signal most of the algorithm performing ica do not consider the temporal correlation of the signal but only higher moment of it amplitude distribution moreover they require some preprocessing of the data whitening so a to remove second order correlation in this paper we are interested in understanding the neural mechanism responsible for solving ica we present an online learning rule that exploit delayed correlation in the input this rule performs ica by detecting joint variation in the firing rate of preand postsynaptic neuron similar to a local rate based hebbian learning rule 
temporal difference td network have been introduced a a formalism for expressing and learning grounded world knowledge in a predictive form sutton tanner like conventional td method the learning algorithm for td network us step backup to train prediction unit about future event in conventional td learning the td algorithm is often used to do more general multi step backup of future prediction in our work we introduce a generalization of the step td network specification that is based on the td learning algorithm creating td network we present experimental result that show td network can learn solution in more complex environment than td network we also show that in problem that can be solved by td network td network generally learn solution much faster than their step counterpart finally we present an analysis of our algorithm that show that the computational cost of td network is only slightly more than that of td network 
we consider the problem of constructing a function whose zero set is to represent a surface given sample point with surface normal vector the contribution include a novel mean of regularising multi scale compactly supported basis function that lead to the desirable property previously only associated with fully supported base and show equivalence to a gaussian process with modified covariance function we also provide a regularisation framework for simpler and more direct treatment of surface normal along with a corresponding generalisation of the representer theorem we demonstrate the technique on d problem of up to million data point a well a d time series data 
we present a new machine learning approach for d qsar the task of predicting binding affinity of molecule to target protein based on d structure our approach predicts binding affinity by using regression on substructure discovered by relational learning we make two contribution to the state of the art first we use multiple instance mi regression which represents a molecule a a set of d conformation to model activity second the relational learning component employ the score a you use sayu method to select substructure for their ability to improve the regression model this is the first application of sayu to multiple instance real valued prediction we evaluate our approach on three task and demonstrate that i sayu outperforms standard coverage measure when selecting feature for regression ii the mi representation improves accuracy over standard single feature vector encoding and iii combining sayu with mi regression is more accurate for d qsar than either approach by itself 
redescription mining is a newly introduced data mining problem that seek to find subset of data that afford multiple definition it can be viewed a a generalization of association rule mining from finding implication to equivalence a a form of conceptual clustering where the goal is to identify cluster that afford dual characterization and a a form of constructive induction to build feature based on given descriptor that mutually reinforce each other in this paper we present the use of redescription mining a an important tool to reason about a collection of set especially their overlap similarity and difference we outline algorithm to mine all minimal non redundant redescriptions underlying a dataset using notion of minimal generator of closed itemsets we also show the use of these algorithm in an interactive context supporting constraint based exploration and querying specifically we showcase a bioinformatics application that empowers the biologist to define a vocabulary of set underlying a domain of gene and to reason about these set yielding significant biological insight 
measuring distance or some other form of proximity between object is a standard data mining tool connection subgraphs were recently proposed a a way to demonstrate proximity between node in network we propose a new way of measuring and extracting proximity in network called cycle free effective conductance cfec our proximity measure can handle more than two endpoint directed edge is statistically well behaved and produce an effectiveness score for the computed subgraphs we provide an efficien talgorithm also we report experimental result and show example for three large network data set a telecommunication calling graph the imdb actor graph and an academic co authorship network 
this paper examines high dimensional regression with noise contaminated input and output data goal of such learning problem include optimal prediction with noiseless query point and optimal system identification a a first step we focus on linear regression method since these can be easily cast into nonlinear learning problem with locally weighted learning approach standard linear regression algorithm generate biased regression estimate if input noise is present and suffer numerically when the data contains redundancy and irrelevancy inspired by factor analysis regression we develop a variational bayesian algorithm that is robust to ill conditioned data automatically detects relevant feature and identifies input and output noise all in a computationally efficient way we demonstrate the effectiveness of our technique on synthetic data and on a system identification task for a rigid body dynamic model of a robotic vision head our algorithm performs to better than previously suggested method 
we consider approximate inference in the important class of large gaussian distribution corresponding to multiply connected directed acylic network we show how directed belief propagation can be implemented in a numerically stable manner by associating backward message with an auxiliary variable enabling intermediate computation to be carried out in moment form we apply our method to the fast fourier transform network with missing data and show that the result are more accurate than those obtained using undirected belief propagation on the equivalent pairwise markov network 
we study hierarchical classification in the general case when an instance could belong to more than one class node in the underlying taxonomy experiment done in previous work showed that a simple hierarchy of support vector machine svm with a top down evaluation scheme ha a surprisingly good performance on this kind of task in this paper we introduce a refined evaluation scheme which turn the hierarchical svm classifier into an approximator of the bayes optimal classifier with respect to a simple stochastic model for the label experiment on synthetic datasets generated according to this stochastic model show that our refined algorithm outperforms the simple hierarchical svm on real world data however the advantage brought by our approach is a bit le clear we conjecture this is due to a higher noise rate for the training label in the low level of the taxonomy 
current computational model of bottom up and top down component of attention are predictive of eye movement across a range of stimul ia nd of simple fixed visual task such a visual search for a target among distractors however to date there exists no computational framework which can reliably mimic human gaze behavior in more complex environment and task such a driving av ehicle through tra c here we develop a hybrid computational behavioral framework combining simple model for bottom up salience and top down relevance and looking for change in the predictive power of these component at di erent critical event time during hour video f rames of observer playing car racing and flight combat video game this approach is motivated by our observation that the predictive strength of the salience and relevance model exhibit reliable temporal signature during critical event window in the task sequence for example when the game player directly engages an enemy plane in a flight combat game the predictive strength of the salience model increase significantly while that of the relevance model decrease significantly our new framework combine these temporal signature to implement several event detector critically we find that an event detector based on fuse db ehavioral and stimulus information in the form of the model s predictive strength is much stronger than detector based on behavioral information alone eye position or image information alone model prediction map this approach to event detection based on eye tracking combined with computational model applied to the visual input may have useful application a a le invasive alternativ et o other event detection approach based on neural signature derived from eeg or fmri recording 
this paper present a novel technique for analyzing electromagnetic imaging data obtained using the stimulus evoked experimental paradigm the technique is based on a probabilistic graphical model which describes the data in term of underlying evoked and interference source and explicitly model the stimulus evoked paradigm a variational bayesian em algorithm infers the model from data suppresses interference source and reconstructs the activity of separated individual brain source the new algorithm outperforms existing technique on two real datasets a well a on simulated data 
given a water distribution network where should we place sensor toquickly detect contaminant or which blog should we read to avoid missing important story these seemingly different problem share common structure outbreak detection can be modeled a selecting node sensor location blog in a network in order to detect the spreading of a virus or information asquickly a possible we present a general methodology for near optimal sensor placement in these and related problem we demonstrate that many realistic outbreak detection objective e g detection likelihood population affected exhibit the property of submodularity we exploit submodularity to develop an efficient algorithm that scale to large problem achieving near optimal placement while being time faster than a simple greedy algorithm we also derive online bound on the quality of the placement obtained by any algorithm our algorithm and bound also handle case where node sensor location blog have different cost we evaluate our approach on several large real world problem including a model of a water distribution network from the epa andreal blog data the obtained sensor placement are provably near optimal providing a constant fraction of the optimal solution we show that the approach scale achieving speedup and saving in storage of several order of magnitude we also show how the approach lead to deeper insight in both application answering multicriteria trade off cost sensitivity and generalization question 
we empirically study the relationship between supervised and multiple instance mi learning algorithm to learn various concept have been adapted to the mi representation however it is also known that concept that are pac learnable with one sided noise can be learned from mi data a relevant question then is how well do supervised learner do on mi data we attempt to answer this question by looking at a cross section of mi data set from various domain coupled with a number of learning algorithm including diverse density logistic regression nonlinear support vector machine and foil we consider a supervised and mi version of each learner several interesting conclusion emerge from our work no mi algorithm is superior across all tested domain some mi algorithm are consistently superior to their supervised counterpart using high false positive cost can improve a supervised learner s performance in mi domain and in several domain a supervised algorithm is superior to any mi algorithm we tested 
when we have several related task solving them simultaneously is shown to be more effective than solving them individually this approach is called multi task learning mtl and ha been studied extensively existing approach to mtl often treat all the task a uniformly related to each other and the relatedness of the task is controlled globally for this reason the existing method can lead to undesired solution when some task are not highly related to each other and some pair of related task can have significantly different solution in this paper we propose a novel mtl algorithm that can overcome these problem our method make use of a task network which describes the relation structure among task this allows u to deal with intricate relation structure in a systematic way furthermore we control the relatedness of the task locally so all pair of related task are guaranteed to have similar solution we apply the above idea to support vector machine svms and show that the optimization problem can be cast a a second order cone program which is convex and can be solved efficiently the usefulness of our approach is demonstrated through simulation with protein super family classification and ordinal regression problem 
we present an algorithm for mining tree shaped pattern in a large graph novel about our class of pattern is that they can contain constant and can contain existential node which are not counted when determining the number of occurrence of the pattern in the graph our algorithm ha a number of provable optimality property which are based on the theory of conjunctive database query we propose a database oriented implementation in sql and report upon some initial experimental result obtained with our implementation on graph data about food web about protein interaction and about citation analysis 
we address the problem of sub ordinate class recognition like the distinction between different type of motorcycle our approach is motivated by observation from cognitive psychology which identify part a the defining component of basic level category like motorcycle while sub ordinate category are more often defined by part property like jagged wheel accordingly we suggest a two stage algorithm first a relational part based object model is learnt using unsegmented object image from the inclusive class e g motorcycle in general the model is then used to build a class specific vector representation for image where each entry corresponds to a model s part in the second stage we train a standard discriminative classifier to classify subclass instance e g cross motorcycle based on the class specific vector representation we describe extensive experimental result with several subclass the proposed algorithm typically give better result than a competing one step algorithm or a two stage algorithm where classification is based on a model of the sub ordinate class the primary role of basic level category seems related to the structure of object in the world in tversky hemenway promote the hypothesis that the explanation lie in the notion of part their experiment show that basic level category like car and flower are often described a a combination of distinctive part e g stem and petal which are mostly unique higher superordinate and more inclusive level are more often described by their function e g used for transportation while lower sub ordinate and more specific level are often described by part property e g red petal and other fine detail these point are illustrated in fig this computational characterization of human categorization find parallel in computer vision and machine learning specifically traditional work in pattern recognition focused on discriminating vector of feature where the feature are shared by all object with different value if we make the analogy between feature and part this level of analysis is appropriate for sub ordinate category in this level different object share part but differ in the part value e g red petal v yellow petal this is called modified part in 
this paper considers the problem of modeling disease progression from historical clinical database with the ultimate objective of stratifying patient into group with clearly distinguishable prognosis or suitability for different treatment strategy to meet this objective we describe a procedure that first fit clinical variable measured over time to a disease progression model the resulting parameter estimate are then used a the basis for a stepwise clustering procedure to stratify patient into group with distinct survival characteristic a a practical illustration we apply this procedure to survival prediction using a liver transplant database from the national institute of diabetes and digestive and kidney disease niddk 
the ill posed nature of the meg eeg source localization problem requires the incorporation of prior assumption when choosing an appropriate solution out of an infinite set of candidate bayesian method are useful in this capacity because they allow these assumption to be explicitly quantified recently a number of empirical bayesian approach have been proposed that attempt a form of model selection by using the data to guide the search for an appropriate prior while seemingly quite different in many respect we apply a unifying framework based on automatic relevance determination ard that elucidates various attribute of these method and suggests direction for improvement we also derive theoretical property of this methodology related to convergence local minimum and localization bias and explore connection with established algorithm 
this paper introduces a new approach to action value function approximation by learning basis function from a spectral decomposition of the state action manifold this paper extends previous work on using laplacian base for value function approximation by using the action of the agent a part of the representation when creating basis function the approach result in a nonlinear learned representation particularly suited to approximating action value function without incurring the wasteful duplication of state base in previous work we discus two technique to create state action graph off policy and on policy we show that these graph have a greater expressive power and have better performance over state based laplacian basis function in domain modeled a semi markov decision process smdps we present a simple graph partitioning method to scale the approach to large discrete mdps 
a typical goal for transfer learning algorithm is to utilize knowledge gained in a source task to learn a target task faster recently introduced transfer method in reinforcement learning setting have shown considerable promise but they typically transfer between pair of very similar task this work introduces rule transfer a transfer algorithm that first learns rule to summarize a source task policy and then leverage those rule to learn faster in a target task this paper demonstrates that rule transfer can effectively speed up learning in keepaway a benchmark rl problem in the robot soccer domain based on experience from source task in the gridworld domain we empirically show through the use of three distinct transfer metric that rule transfer is effective across these domain 
in the on line analytical processing olap context exploration of huge and sparse data cube is a tedious task which doe not always lead to efficient result in this paper we couple olap with the multiple correspondence analysis mca in order to enhance visual representation of data cube and thus facilitate their interpretation and analysis we also provide a quality criterion to measure the relevance of obtained representation the criterion is based on a geometric neighborhood concept and a similarity metric between cell of a data cube experimental result on real data proved the interest and the efficiency of our approach 
we describe a neurobiologically plausible model to implement dynamic routing using the concept of neuronal communication through neuronal coherence the model ha a three tier architecture a raw input tier a rout ing control tier and an invariant output tier the correct mapping between input and output tier is realized by an appropriate alignment of the phase of their respective background oscillation by the routing control unit we present an example architecture implemented on a neuromorphic chip that is able to achieve circular shift invariance a simple extension to our model can accomplish circular shift dynamic routing with only o n connection compared to o n connection required by traditional model 
modelling the dynamic of transcriptional process in the cell requires the knowledge of a number of key biological quantity while some of them are relatively easy to measure such a mrna decay rate and mrna abundance level it is still very hard to measure the active concentration level of the transcription factor protein that drive the process and the sensitivity of target gene to these concentration in this paper we show how these quantity for a given transcription factor can be inferred from gene expression level of a set of known target gene we treat the protein concentration a a latent function with a gaussian process prior and include the sensitivity mrna decay rate and baseline expression level a hyperparameters we apply this procedure to a human leukemia dataset focusing on the tumour repressor p and obtaining result in good accordance with recent biological study 
semi naive bayesian classifier seek to retain the numerous strength of naive bayes while reducing error by relaxing the attribute independence assumption backwards sequential elimination bse is a wrapper technique for attribute elimination that ha proved effective at this task we explore a new technique lazy elimination le which eliminates highly related attribute value at classification time without the computational overhead inherent in wrapper technique we analyze the effect of le and bse on a state of the art semi naive bayesian algorithm averaged one dependence estimator aode our experiment show that le significantly reduces bias and error without undue computation while bse significantly reduces bias but not error with high training time complexity in the context of aode le ha a significant advantage over bse in both computational efficiency and error 
by adopting gaussian process prior a fully bayesian solution to the problem of integrating possibly heterogeneous data set within a cl assification setting is presented approximate inference scheme employing variational expectation propagation based method are developed and rigorously assessed we demonstrate our approach to integrating multiple data set on a la rge scale protein fold prediction problem where we infer the optimal combination of covariance function and achieve state of the art performance without resorting to any ad hoc parameter tuning and classifier combination 
this paper explores two aspect of social network modeling first we generalize a successful static model of relationship into a dynamic model that account for friendship drifting over time second we show how to make it tractable to learn such model from data even a the number of entity n get large the generalized model associate each entity with a point in p dimensional euclidean latent space the point can move a time progress but large move in latent space are improbable observed link between entity are more likely if the entity are close in latent space we show how to make such a model tractable sub quadratic in the number of entity by the use of appropriate kernel function for similarity in latent space the use of low dimensional kd tree a new ecien t dynamic adaptation of multidimensional scaling for a rst pas of approximate projection of entity into latent space and an ecien t conjugate gradient update rule for non linear local optimization in which amortized time per entity during an update is o logn we use both synthetic and real world data on up to entity which indicate near linear scaling in computation time and improved performance over four alternative approach we also illustrate the system operating on twelve year of nip co authorship data 
we consider method that try to find a good policy for a markov d ecision process by choosing one from a given class the policy is chosen based on it empirical performance in simulation we are interested in condition on the complexity of the policy class that ensure the success of such simulatio n based policy search method we show that under bound on the amount of computation involved in computing policy transition dynamic and reward uniform convergence of empirical estimate to true value function occurs previo usly such result were derived by assuming boundedness of pseudodimension and lipschitz continuity these assumption and ours are both stronger than the usual combinatorial complexity measure we show via minimax inequality that this is essential boundedness of pseudodimension or fat shattering dimension alone is not sufficient 
reinforcement learning rl wa originally proposed a a framework to allow agent to learn in an online fashion a they interact with their environment existing rl algorithm come short of achieving this goal because the amount of exploration required is often too costly and or too time consuming for online learning a a result rl is mostly used for offline learning in simulated environment we propose a new algorithm called beetle for effective online learning that is computationally efficient while minimizing the amount of exploration we take a bayesian model based approach framing rl a a partially observable markov decision process our two main contribution are the analytical derivation that the optimal value function is the upper envelope of a set of multivariate polynomial and an efficient point based value iteration algorithm that exploit this simple parameterization 
latent dirichlet allocation lda and other related topic model are increasingly popular tool for summarization and manifold discovery in discrete data however lda doe not capture correlation between topic in this paper we introduce the pachinko allocation model pam which capture arbitrary nested and possibly sparse correlation between topic using a directed acyclic graph dag the leaf of the dag represent individual word in the vocabulary while each interior node represents a correlation among it child which may be word or other interior node topic pam provides a flexible alternative to recent work by blei and lafferty which capture correlation only between pair of topic using text data from newsgroups historic nip proceeding and other research paper corpus we show improved performance of pam in document classification likelihood of held out data the ability to support finer grained topic and topical keyword coherence 
the current framework of reinforcement learning is based on maximizing the expected return based on scalar reward but in many real world situation tradeoff must be made among multiple objective moreover the agent s preference between different objective may vary with time in this paper we consider the problem of learning in the presence of time varying preference among multiple objective using numeric weight to represent their importance we propose a method that allows u to store a finite number of policy choose an appropriate policy for any weight vector and improve upon it the idea is that although there are infinitely many weight vector they may be well covered by a small number of optimal policy we show this empirically in two domain a version of the buridan s as problem and network routing 
many website have large collection of page generated dynamically from an underlying structured source like a database the data of a category are typically encoded into similar page by a common script or template in recent year some value added service such a comparison shopping and vertical search in a specific domain have motivated the research of extraction technology with high accuracy almost all previous work assume that input page of a wrapper induction system conform to a common template and they can be easily identified in term of a common schema of url however we observed that it is hard to distinguish different template using dynamic url today moreover since extraction accuracy heavily depends on how consistent input page are we argue that it is risky to determine whether page share a common template solely based on url instead we propose a new approach that utilizes similarity between page to detect template our approach separate page with notable inner difference and then generates wrapper respectively experimental result show that our proposed approach is feasible and effective for improving extraction accuracy 
order preserving submatrixes opsms have been accepted a a biologically meaningful subspace cluster model capturing the general tendency of gene expression across a subset of condition in an opsm the expression level of all gene induce the same linear ordering of the condition opsm mining is reducible to a special case of the sequential pattern mining problem in which a pattern and it supporting sequence uniquely specify an opsm cluster those small twig cluster specified by long pattern with naturally low support incur explosive computational cost and would be completely pruned off by most existing method for massive datasets containing thousand of condition and hundred of thousand of gene which are common in today s gene expression analysis however it is in particular interest of biologist to reveal such small group of gene that are tightly coregulated under many condition and some pathway or process might require only two gene to act in concert in this paper we introduce the kiwi mining framework for massive datasets that exploit two parameter k and w to provide a biased testing on a bounded number of candidate substantially reducing the search space and problem scale targeting on highly promising seed that lead to significant cluster and twig cluster extensive biological and computational evaluation on real datasets demonstrate that kiwi can effectively mine biologically meaningful opsm subspace cluster with good efficiency and scalability 
we consider the task of tuning hyperparameters in svm model based on minimizing a smooth performance validation function e g smoothed k fold crossvalidation error using non linear optimization technique the key computation in this approach is that of the gradient of the validation function with respect to hyperparameters we show that for large scale problem involving a wide choice of kernel based model and validation function this computation can be very efciently done often within just a fraction of the training time empirical result show that a near optimal set of hyperparameters can be identied by our approach with very few training round and gradient computation 
recently several manifold learning algorithm have been proposed such a isomap tenenbaum et al locally linear embedding roweis saul laplacian eigenmap belkin niyogi locality preserving projection lpp he niyogi etc all of them aim at discovering the meaningful low dimensional structure of the data space in this paper we present a statistical analysis of the lpp algorithm different from principal component analysis pca which obtains a subspace spanned by the largest eigenvectors of the global covariance matrix we show that lpp obtains a subspace spanned by the smallest eigenvectors of the local covariance matrix we applied pca and lpp to real world document clustering task experimental result show that the performance can be significantly improved in the subspace and especially lpp work much better than pca 
we derive a cost functional for estimating the inverse of the observation function in nonlinear dynamical system limiting our search to invertible observation function confers numerous benefit including a compact representation and no local minimum our approximation algorithm for optimizing this cost functional are fast and give diagnostic bound on the quality of their solution our method can be viewed a a manifold learning algorithm that utilizes a prior on the lowdimensional manifold coordinate the benefit of taking advantage of such prior in manifold learning and searching for the inverse observation function in system identification are demonstrated empirically by learning to track moving target from raw measurement in a sensor network setting and in an rfid tracking experiment 
how can we find community in dynamic network of socialinteractions such a who call whom who email whom or who sell to whom how can we spot discontinuity time point in such stream of graph in an on line any time fashion we propose graphscope that address both problem using information theoretic principle contrary to the majority of earlier method it need no user defined parameter moreover it is designed to operate on large graph in a streaming fashion we demonstrate the efficiency and effectiveness of our graphscope on real datasets from several diverse domain in all case it produce meaningful time evolving pattern that agree with human intuition 
clustering is often formulated a the maximum likelihood estimation of a mixture model that explains the data the em algorithm widely used to solve the resulting optimization problem is inherently a gradient descent method and is sensitive to initialization the resulting solution is a local optimum in the neighborhood of the initial guess this sensitivity to initialization present a significant challenge in clustering large data set into many cluster in this paper we present a different approach to approximate mixture fitting for clustering we introduce an exemplar based likelihood function that approximates the exact likelihood this formulation lead to a convex minimization problem and an efficient algorithm with guaranteed convergence to the globally optimal solution the resulting clustering can be thought of a a probabilistic mapping of the data point to the set of exemplar that minimizes the average distance and the information theoretic cost of mapping we present experimental result illustrating the performance of our algorithm and it comparison with the conventional approach to mixture model clustering 
learning agent can improve performance cooperating with other agent particularly learning agent forming a committee outperform individual agent this ensemble effect is well known for multi classifier system in machine learning however multi classifier system assume all data is known to all classifier while we focus on agent that learn from case example that are owned and stored individually in this article we focus on how individual agent can engage in bargaining activity that improve the performance of both individual agent and the committee the agent are capable of self evaluation and determining that some data used for learning is unnecessary this refuse data can then be exploited by other agent that might found some part of it profitable to improve their performance the experiment we performed show that this approach improves both individual and committee performance and we analyze how these result in term of the ensemble effect 
many algorithm have been recently developed for reducing dimensionality by projecting data onto an intrinsic non linear manifold unfortunately existing algorithm often lose significant precision in this transformation manifold sculpting is a new algorithm that iteratively reduces dimensionality by simulating surface tension in local neighborhood we present several experiment that show manifold sculpting yield more accurate result than existing algorithm with both generated and natural data set manifold sculpting is also able to benefit from both prior dimensionality reduction effort 
given a sample covariance matrix we examine the problem of maximizing the variance explained by a particular linear combination of the input variable while constraining the number of nonzero coefficient in this combination this is known a sparse principal component analysis and ha a wide array of application in machine learning and engineering we formulate a new semidefinite relaxation to this problem and derive a greedy algorithm that computes a full set of good solution for all number of non zero coefficient with complexity o n where n is the number of variable we then use the same relaxation to derive sufficient condition for global optimality of a solution which can be tested in o n we show on toy example and biological data that our algorithm doe provide globally optimal solution in many case 
many formal model of cognition implicitly use subjective probability distribution to capture the assumption of human learner most application of these model determine these distribution indirectly we propose a method for directly determining the assumption of human learner by sampling from subjective probability distribution using a correspondence between a model of human choice and markov chain monte carlo mcmc we describe a method for sampling from the distribution over object that people associate with different category in our task subject choose whether to accept or reject a proposed change to an object the task is constructed so that these decision follow an mcmc acceptance rule defining a markov chain for which the stationary distribution is the category distribution we test this procedure for both artificial category acquired in the laboratory and natural category acquired from experience 
we present a novel algorithm called pg mean which is able to learn the number of cluster in a classical gaussian mixture model our method is robust and efficient it us statistical hypothesis test on one dimensi onal projection of the data and model to determine if the example are well represented by the model in so doing we are applying a statistical test for the entire mode l at once not just on a per cluster basis we show that our method work well in diffi cult case such a non gaussian data overlapping cluster eccentric clust er high dimension and many true cluster further our new method provides a much more stable estimate of the number of cluster than existing method in this paper we present an algorithm called pg mean pg stand for projected gaussian which is able to discover an appropriate number of gaussian cluster and their location and orientation our method is a wrapper around the standard and widely used gaussian mixture model the paper s primary contribution is a novel method of determining if a whole mixture model fit it data well based on projection and statistical test we show that the new approach work well not only in simple case in which the cluster are well separated but al so in the situation where the cluster are overlapped eccentric in high dimension and even non gaussian we show that where some other method tend to severely overfit our method doe not a nd that our method is comparable to but much faster than a recent variational bayes based approach for learning k 
in application of data mining characterized by highly skewed misclassification cost certain type of error become virtually unacceptable this limit the utility of a classifier to a range in which such constraint can be met naive bayes which ha proven to be very useful in text mining application due to high scalability can be particularly affected although it loss tends to be small it misclassifications are often made with apparently high confidence aside from effort to better calibrate naive bayes score it ha been shown that it accuracy depends on document sparsity and feature selection can lead to marked improvement in classification performance traditionally sparsity is controlled globally and the result for any particular document may vary in this work we examine the merit of local sparsity control for naive bayes in the context of highly asymmetric misclassification cost in experiment with three benchmark document collection we demonstrate clear advantage of document level feature selection in the extreme cost setting multinomial naive bayes with local sparsity control is able to outperform even some of the recently proposed effective improvement to the naive bayes classifier there are also indication that local feature selection may be preferable in different cost setting 
hybrid cmol integrated circuit combining cmos subsystem with nanowire crossbar and simple two terminal nanodevices promise to extend the exponential moore law development of microelectronics into the sub nm range we are developing neuromorphic network crossnet architecture for this future technology in which neural cell body are implemented in cmos nanowires are used a axon and dendrite while nanodevices bistable latching switch are used a elementary synapsis we have shown how crossnets may be trained to perform pattern recovery and classification despite the limitation imposed by the cmol hardware preliminary estimate have shown that cmol crossnets may be extremely dense cell per cm and operate approximately a million time faster than biological neural network at manageable power consumption in conclusion we discus in brief possible short term and long term application of the emerging technology 
how should a reinforcement learning agent act if it sole purpose is to efficiently learn an optimal policy for later use in other word how should it explore to be able to exploit later we formulate this problem a a markov decision process by explicitly modeling the internal state of the agent and propose a principled heuristic for it solution we present experimental result in a number of domain also exploring the algorithm s use for learning a policy for a skill given it reward function an important but neglected component of skill discovery 
human listener have the extraordinary ability to hear and recognize speech even when more than one person is talking their machine counterpart have historically been unable to compete with this ability until now we present a modelbased system that performs on par with human in the task of separating speech of two talker from a single channel recording remarkably the system surpasses human recognition performance in many condition the model of speech use temporal dynamic to help infer the source speech signal given mixed speech signal the estimated source signal are then recognized using a conventional speech recognition system we demonstrate that the system achieves it best performance when the model of temporal dynamic closely capture the grammatical constraint of the task 
discriminative sequential learning model like conditional random field crfs have achieved significant success in several area such a natural language processing or information extraction their key advantage is the ability to capture various non independent and overlapping feature of input however several unexpected pitfall have a negative influence on the model s performance these mainly come from an imbalance among class label irregular phenomenon and potential ambiguity in the training data this paper present a data driven approach that can deal with such hard to predict data instance by discovering and emphasizing rare but important association of statistic hidden in the training data mined association are then incorporated into these model to deal with difficult example experimental result of english phrase chunking and named entity recognition using crfs show a significant improvement in accuracy in addition to the technical perspective our approach also highlight a potential connection between association mining and statistical learning by offering an alternative strategy to enhance learning performance with interesting and useful pattern discovered from large dataset 
we study the problem of parameter estimation in continuous density hidden markov model cd hmms for automatic speech recognition asr a in support vector machine we propose a learning algorithm based on the goal of margin maximization unlike earlier work on max margin markov network our approach is specifically geared to the modeling of real valued observation such a acoustic feature vector using gaussian mixture model unlike previous discriminative framework for asr such a maximum mutual information and minimum classification error our framework lead to a convex optimization without any spurious local minimum the objective function for large margin training of cd hmms is defined over a parameter space of positive semidefinite matrix it optimization can be performed efficiently with simple gradient based method that scale well to large problem we obtain competitive result for phonetic recognition on the timit speech corpus 
we present dl an exact algorithm for finding a decision tree that optimizes a ranking function under size depth accuracy and leaf constraint because the discovery of optimal tree ha high theoretical complexity until now few effort have been made to compute such tree for real world datasets an exact algorithm is of both scientific and practical interest from a scientific point of view it can be used a a gold standard to evaluate the performance of heuristic constraint based decision tree learner and to gain new insight in traditional decision tree learner from the application point of view it can be used to discover tree that cannot be found by heuristic decision tree learner the key idea behind our algorithm is that there is a relation between constraint on decision tree and constraint on itemsets we show that optimal decision tree can be extracted from lattice of itemsets in linear time we give several strategy to efficiently build these lattice experiment show that under the same constraint dl obtains better result than c which confirms that exhaustive search doe not always imply overfitting the result also show that dl is a useful and interesting tool to learn decision tree under constraint 
we consider the problem of clustering in it most basic form where only a local metric on the data space is given no parametric statistical model is assumed and the number of cluster is learned from the data we introduce analyze and demonstrate a novel approach to clustering where data point are viewed a node of a graph and pairwise similarity are used to derive a transition probability matrix p for a markov random walk between them the algorithm automatically reveals structure at increasing scale by varying the number of step taken by this random walk point are represented a row of pt which are the t step distribution of the walk starting at that point these distribution are then clustered using a kl minimizing iterative algorithm both the number of cluster and the number of step that best reveal it are found by optimizing spectral property of p 
the problem of resource allocation in sparse graph with real variable is studied using method of statistical physic an efficien t distributed algorithm is devised on the basis of insight gained from the analysis and is examined using numerical simulation showing excellent performance and full agreement with the theoretical result 
markov decision process are an effective tool in modeling decision making in uncertain dynamic environment since the parameter of these model are typically estimated from data learned from experience or designed by hand it is not surprising that the actual performance of a chosen strategy often significantly differs from the designer s initial expectation due to unavoidable model uncertainty in this paper we present a percentile criterion that capture the trade off between optimistic and pessimistic point of view on mdp with parameter uncertainty we describe tractable method that take parameter uncertainty into account in the process of decision making finally we propose a cost effective exploration strategy when it is possible to invest money time or computation effort in action that will reduce the uncertainty in the parameter 
dyadic data matrix such a co occurrence matrix rating matrix and proximity matrix arise frequently in various important application a fundamental problem in dyadic data analysis is to find the hidden block structure of the data matrix in this paper we present a new co clustering framework block value decomposition bvd for dyadic data which factorizes the dyadic data matrix into three component the row coefficient matrix r the block value matrix b and the column coefficient matrix c under this framework we focus on a special yet very popular case non negative dyadic data and propose a specific novel co clustering algorithm that iteratively computes the three decomposition matrix based on the multiplicative updating rule extensive experimental evaluation also demonstrate the effectiveness and potential of this framework a well a the specific algorithm for co clustering and in particular for discovering the hidden block structure in the dyadic data 
everyday inductive reasoning draw on many kind of knowledge including knowledge about relationship between property and knowledge about relationship between object previous account of inductive reasoning generally focus on just one kind of knowledge model of causal reasoning often focus on relationship between property and model of similarity based reasoning often focus on similarity relationship between object we present a bayesian model of inductive reasoning that incorporates both kind of knowledge and show that it account well for human inference about the property of biological specie 
we examine the relationship between the prediction made by different learning algorithm and true posterior probability we show that maximum margin method such a boosted tree and boosted stump push probability mass away from and yielding a characteristic sigmoid shaped distortion in the predicted probability model such a naive bayes which make unrealistic independence assumption push probability toward and other model such a neural net and bagged tree do not have these bias and predict well calibrated probability we experiment with two way of correcting the biased probability predicted by some learning method platt scaling and isotonic regression we qualitatively examine what kind of distortion these calibration method are suitable for and quantitatively examine how much data they need to be effective the empirical result show that after calibration boosted tree random forest and svms predict the best probability 
abstract a recently proposed formulation of the stochastic planning and control problem a one of parameter estimation for suitable artificial statistical model ha led to the adoption of inference algorithm for this notoriously hard problem at the algorithmic level the focus ha been on developing expectation maximization em algorithm in this paper we begin by making the crucial observation that the stochastic control problem can be reinterpreted a one of trans dimensional inference with this new understanding we are able to propose a novel reversible jump markov chain monte carlo mcmc algorithm that is more efficient than it em counterpart moreover it enables u to carry out full bayesian policy search without the need for gradient and with one single markov chain the new approach involves sampling directly from a distribution that is proportional to the reward and consequently performs better than classic simulation method in situation where the reward is a rare event 
this paper is about constructing confidence band around roc curve we first introduce to the machine learning community three band generating method from the medical field and evaluate how well they perform such confidence band represent the region where the true roc curve is expected to reside with the designated confidence level to ass the containment of the band we begin with a synthetic world where we know the true roc curve specifically where the class conditional model score are normally distributed the only method that attains reasonable containment out of the box produce non parametric fixed width band fwbs next we move to a context more appropriate for machine learning evaluation band that with a certain confidence level will bound the performance of the model on future data we introduce a correction to account for the larger uncertainty and the widened fwbs continue to have reasonable containment finally we ass the band on relatively large benchmark data set we conclude by recommending these fwbs noting that being non parametric they are especially attractive for machine learning study where the score distribution clearly are not normal and even for the same data set vary substantially from learning method to learning method 
relational data appear frequently in many machine learning application relational data consist of the pairwise relation similarity or dissimilarity between each pair of implicit object and are usually stored in relation matrix and typically no other knowledge is available although relational clustering can be formulated a graph partitioning in some application this formulation is not adequate for general relational data in this paper we propose a general model for relational clustering based on symmetric convex coding the model is applicable to all type of relational data and unifies the existing graph partitioning formulation under this model we derive two alternative bound optimization algorithm to solve the symmetric convex coding under two popular distance function euclidean distance and generalized i divergence experimental evaluation and theoretical analysis show the effectiveness and great potential of the proposed model and algorithm 
integrating information in multiple natural language is a challenging task that often requires manually created linguistic resource such a a bilingual dictionary or example of direct translation of text in this paper we propose a general cross lingual text mining method that doe not rely on any of these resource but can exploit comparable bilingual text corpus to discover mapping between word and document in different language comparable text corpus are collection of text document in different language that are about similar topic such text corpus are often naturally available e g news article in different language published in the same time period the main idea of our method is to exploit frequency correlation of word in different language in the comparable corpus and discover mapping between word in different language such mapping can then be used to further discover mapping between document in different language achieving cross lingual information integration evaluation of the proposed method on a mb chinese english comparable news collection show that the proposed method is effective for mapping word and document in english and chinese since our method only relies on naturally available comparable corpus it is generally applicable to any language pair a long a we have comparable corpus 
chord progression are the building block from which tonal music is constructed inferring chord progression is thus an essential step towards modeling long term dependency in music in this paper a distributed representation for chord is designed such that euclidean distance roughly correspond to psychoacoustic dissimilarity parameter in the graphical model are learnt with the em algorithm and the classical junction tree algorithm various model architecture are compared in term of conditional out of sample likelihood both perceptual and statistical evidence show that binary tree related to meter are well suited to capture chord dependency 
data set involving multiple group with shared characteristic frequently arise in practice in this paper we extend hierarchical dirichlet process to model such data each group is assumed to be generated from a template mixture model with group level variability in both the mixing proportion and the component parameter variability in mixing proportion across gr oups are handled using hierarchical dirichlet process also allowing for autom atic determination of the number of component in addition each group is allowed to have it own component parameter coming from a prior described by a template mixture model this group level variability in the component parameter is handled using a random effect model we present a markov chain monte carlo mcmc sampling algorithm to estimate model parameter and demonstrate the method by applying it to the problem of modeling spatial brain activation pattern across multiple image collected via functional magnetic resonance imaging fmri 
the proliferation of text document on the web a well a within institution necessitates their convenient organization to enable efficient retrieval of information although text corpus are frequently organized into concept hierarchy or taxonomy the classification of the document into the hierarchy is expensive in term human effort we present a novel and simple hierarchical dirichlet generative model for text corpus and derive an efficient algorithm for the estimation of model parameter and the unsupervised classification of text document into a given hierarchy the class conditional feature mean are assumed to be inter related due to the hierarchical bayesian structure of the model we show that the algorithm provides robust estimate of the classification parameter by performing smoothing or regularization we present experimental evidence on real web data that our algorithm achieves significant gain in accuracy over simpler model 
how do real graph evolve over time what are normal growth pattern in social technological and information network many study have discovered pattern in static graph identifying property in a single snapshot of a large network or in a very small number of snapshot these include heavy tail for inand out degree distribution community small world phenomenon and others however given the lack of information about network evolution over long period it ha been hard to convert these finding into statement about trend over time here we study a wide range of real graph and we observe some surprising phenomenon first most of these graph densify over time with the number of edge growing super linearly in the number of node second the average distance between node often shrink over time in contrast to the conventional wisdom that such distance parameter should increase slowly a a function of the number of node like o log n or o log log n existing graph generation model do not exhibit these type of behavior even at a qualitative level we provide a new graph generator based on a forest fire spreading process that ha a simple intuitive justification requires very few parameter like the flammability of node and produce graph exhibiting the full range of property observed both in prior work and in the present study 
most of the existing approach to collaborative filtering cannot handle very large data set in this paper we show how a class of two layer undirected graphical model called restricted boltzmann machine rbm s can be used to model tabular data such a user s rating of movie we present efficient learning and inference procedure for this class of model and demonstrate that rbm s can be successfully applied to the netflix data set containing over million user movie rating we also show that rbm s slightly outperform carefully tuned svd model when the prediction of multiple rbm model and multiple svd model are linearly combined we achieve an error rate that is well over better than the score of netflix s own system 
in general the problem of computing a maximum a posteriori map assignment in a markov random eld mrf is computationally intractable however in certain subclass of mrf an optimal or close to optimal assignment can be found very efciently using combinatorial optimization algorithm certain mrfs with mutual exclusion constraint can be solved using bipartite matching and mrfs with regular potential can be solved using minimum cut method however these solution do not apply to the many mrfs that contain such tractable component a sub network but also other non complying potential in this paper we present a new method called compose for exploiting combinatorial optimization for sub network within the context of a max product belief propagation algorithm compose us combinatorial optimization for computing exact maxmarginals for an entire sub network these can then be used for inference in the context of the network a a whole we describe highly efcient method for computing max marginals for subnetworks corresponding both to bipartite matchings and to regular network we present result on both synthetic and real network encoding correspondence problem between image which involve both matching constraint and pairwise geometric constraint we compare to a range of current method showing that the ability of compose to transmit information globally across the network lead to improved convergence decreased running time and higher scoring assignment 
this paper ha been accepted for presentation at the twentie th nip neural information processing conference december vancouver canada the fina l version of the paper to appear in the conference proceeding in will be slightly differ ent from the present one abstract bayesian model averaging model selection and their approximation such a bic are generally statistically consistent but sometimes ach ieve slower rate of convergence than other method such a aic and leave one out cross validation on the other hand these other method can be inconsistent we identify the catch up phenomenon a a novel explanation for the slow convergence of bayesian method based on this analysis we define the switch distributio n a modification of the bayesian marginal distribution we prove that in many situation model selection and prediction based on the switch distribution is both con sistent and achieves optimal convergence rate thereby resolving the aic bic dilemma the method is practical we give an efficient implementation 
the performance of eeg based brain computer interface bcis critically depends on the extraction of feature from the eeg carrying information relevant for the classification of different mental state for bcis e mploying imaginary movement of different limb the method of common spatial pattern csp ha been shown to achieve excellent classification result the csp algorithm however suffers from a lack of robustness requiring training d ata without artifact for good performance to overcome this lack of robustness we propose an adaptive spatial filter that replaces the training data in the csp appr oach by a priori information more specifically we design an adaptive spatial filt er that maximizes the ratio of the variance of the electric field originating in a pr edefined region of interest roi and the overall variance of the measured eeg since it is known that the component of the eeg used for discriminating imaginary movement originates in the motor cortex we design two adaptive spatial filter wi th the roi centered in the hand area of the left and right motor cortex we then use these to classify eeg data recorded during imaginary movement of the right and left hand of three subject and show that the adaptive spatial filter outperf orm the csp algorithm enabling classification rate of up to without artifac t rejection 
existing research on mining quantitative database mainly focus on mining association however mining association is too expensive to be practical in many case in this paper we study mining correlation from quantitative database and show that it is a more effective approach than mining association we propose a new notion of quantitative correlated pattern qcps which is founded on two formal concept mutual information and all confidence we first devise a normalization on mutual information and apply it to qcp mining to capture the dependency between the attribute we further adopt all confidence a a quality measure to control at a finer granularity the dependency between the attribute with specific quantitative interval we also propose a supervised method to combine the consecutive interval of the quantitative attribute based on mutual information such that the interval combining is guided by the dependency between the attribute we develop an algorithm qcomine to efficiently mine qcps by utilizing normalized mutual information and all confidence to perform a two level pruning our experiment verify the efficiency of qcomine and the quality of the qcps 
we establish a general oracle inequality for clipped approximate minimizers of regularized empirical risk and apply this inequality to support vector machine svm type algorithm we then show that for svms using gaussian rbf kernel for classification this oracle inequality lead to learning rate that are faster than the one established in finally we use our oracle inequality to show that a simple parameter selection approach based on a validation set can yield the same fast learning rate without knowing the noise exponent which were required to be known a priori in 
proportional data normalized histogram have been frequently occurring in various area and they could be mathematically abstracted a point residing in a geometric simplex a proper distance metric on this simplex is of importance in many application including classification and information retrieval in this paper we develop a novel framework to learn an optimal metric on the simplex major feature of our approach include it flexibility to handle correlation among bin dimension widespread applicability without being limited to ad hoc background and a real global solution in contrast to existing traditional local approach the technical essence of our approach is to fit a parametric distribution to the observed empirical data in the simplex the distribution is parameterized by affinity between simplex vertex which is learned via maximizing likelihood of observed data then these affinity induce a metric on the simplex defined a the earth mover s distance equipped with ground distance derived from simplex vertex affinity 
the k nearest neighbor algorithm can be easily adapted to classify complex object e g set graph a long a a proper dissimilarity function is given over an input space both the representation of the learning instance and the dissimilarity employed on that representation should be determined on the basis of domain knowledge however even in the presence of domain knowledge it can be far from obvious which complex representation should be used or which dissimilarity should be applied on the chosen representation in this paper we present a framework that allows to combine different complex representation of a given learning problem and or different dissimilarity defined on these representation we build on idea developed previously on metric learning for vectorial data we demonstrate the utility of our method in domain in which the learning instance are represented a set of vector by learning how to combine different set distance measure 
many application of supervised learning require good generalization from limited labeled data in the bayesian setting we can use an informative prior to try to achieve this goal by encoding useful domain knowledge we present an algorithm that construct such an informative prior for a given discrete supervised learning task the algorithm us other similar learning problem to discover property of optimal classiers expressed in term of covariance estimate for pair of feature parameter a semidenite program is then used to combine these estimate and learn a good prior for the current learning task we apply our method to binary text classication and demonstrate a to error reduction over a commonly used prior 
query log the pattern of activity left by million of user contain a wealth of information that can be mined to aid personalization we perform a large scale study of yahoo search engine log tracking million browser cooky over a period of month we define metric to address question such a how much history is available how do user topical interest vary a reflected by their query and what can we learn from user click we find that there is significantly more expected history for the user of a randomly picked query than for a randomly picked user we show that user exhibit consistent topical interest that vary between user we also see that user click indicate a variety of special interest our finding shed light on user activity and can inform future personalization effort 
an efficient and general multiple kernel learning mkl algorithm ha been recently proposed by sonnenburg et al this approach ha opened new perspective since it make the mkl approach tractable for large scale problem by iteratively using existing support vector machine code however it turn out that this iterative algorithm need several iteration before converging towards a reasonable solution in this paper we address the mkl problem through an adaptive norm regularization formulation weight on each kernel matrix are included in the standard svm empirical risk minimization problem with a l constraint to encourage sparsity we propose an algorithm for solving this problem and provide an new insight on mkl algorithm based on block norm regularization by showing that the two approach are equivalent experimental result show that the resulting algorithm converges rapidly and it efficiency compare favorably to other mkl algorithm 
the gaussian process latent variable model gp lvm is a powerful approach for probabilistic modelling of high dimensional data through dimensional reduction in this paper we extend the gp lvm through hierarchy a hierarchical model such a a tree allows u to express conditional independency in the data a well a the manifold structure we first introduce gaussian process hierarchy through a simple dynamical model we then extend the approach to a more complex hierarchy which is applied to the visualisation of human motion data set 
this paper present a novel second order cone programming socp formulation for large scale binary classification task assuming that the class conditional density are mixture distribution where each component of the mixture ha a spherical covariance the second order statistic of the component can be estimated efficiently using clustering algorithm like birch for each cluster the second order moment are used to derive a second order cone constraint via a chebyshev cantelli inequality this constraint ensures that any data point in the cluster is classified correctly with a high probability this lead to a large margin socp formulation whose size depends on the number of cluster rather than the number of training data point hence the proposed formulation scale well for large datasets when compared to the state of the art classifier support vector machine svms experiment on real world and synthetic datasets show that the proposed algorithm outperforms svm solver in term of training time and achieves similar accuracy 
abstract existing algorithm for discrete partially observable markov decision process can at best solve problem of a few thousand state due to two important source of intractability the curse of dimensionality and the policy space complexity this paper describes a new algorithm vdcbpi that mitigates both source of intractability by combining the value directed compression vdc technique with bounded policy iteration bpi the scalability of vdcbpi is demonstrated on synthetic network management problem with up to million state 
planning in partially observable domain is a notoriously difficult problem however in many real world scenario planning can be simplifi ed by decomposing the task into a hierarchy of smaller planning problem several approach have been proposed to optimize a policy that decomposes according to a hierarchy specified a priori in this paper we investigate the problem of automatically discovering the hierarchy more precisely we frame the optimization of a hierarchical policy a a non convex optimization problem that can be solved with general non linear solver a mixed integer non linear approximation or a form of bounded hierarchical policy iteration by encoding the hierarchical stru cture a variable of the optimization problem we can automatically discover a hierarchy our method is flexible enough to allow any part of the hierarchy to be speci fied based on prior knowledge while letting the optimization discover the unknown part it can also discover hierarchical policy including recursive policy that are more compact potentially infinitely fewer parameter and often easier to understand given the decomposition induced by the hierarchy 
linear support vector machine svms have become one of the most prominent machine learning technique for high dimensional sparse data commonly encountered in application like text classification word sense disambiguation and drug design these application involve a large number of example n a well a a large number of feature n while each example ha only s n non zero feature this paper present a cutting plane algorithm for training linear svms that provably ha training time s n for classification problem and o sn log n for ordinal regression problem the algorithm is based on an alternative but equivalent formulation of the svm optimization problem empirically the cutting plane algorithm is several order of magnitude faster than decomposition method like svm light for large datasets 
statistical model on full and partial ranking of n item are often of limited practical use for large n due to computational consideration we explore the use of non parametric model for partially ranked data and derive computationally efficient procedure for their use for large n the derivation are largely possible through combinatorial and algebraic manipulation based on the lattice of partial ranking a bias variance analysis and an experimental study demonstrate the applicability of the proposed method 
in this paper we use large neighborhood markov random field to learn rich prior model of color image our approach extends the monochromatic field of expert model roth black a to color image in the field of expert model the curse of dimensionality due to very large clique size is circumvented by parameterizing the potential function according to a product of expert we introduce simplification to the original approach by roth and black which allow u to cope with the increased clique size typically x x or x x pixel of color image experimental result are presented for image denoising which evidence improvement over state of the art monochromatic image prior 
go is an ancient board game that pose unique opportunity and challenge for ai and machine learning here we develop a machine learning approach to go and related board game focusing primarily on the problem of learning a good evaluation function in a scalable way scalability is essential at multiple level from the library of local tactical pattern to the integration o f pattern across the board to the size of the board itself the system we propose is capable of automatically learning the propensity of local pattern from a library of g ames propensity and other local tactical information are fed into a recursive ne ural network derived from a bayesian network architecture the network integrates local information across the board and produce local output that represent local territory ownership probability the aggregation of these probabilitie s provides an effective strategic evaluation function that is an estimate of the exp ected area at the end or at other stage of the game local area target for training can be derived from datasets of human game a system trained using only amateur game data performs surprisingly well on a test set derived from professional game data possible direction for further improvement are bri efly discussed 
a wide variety of machine learning problem can be described a minimizing a regularized risk functional with different algorithm using different notion of risk and different regularizers example include linear support vector machine svms logistic regression conditional random field crfs and lasso amongst others this paper describes the theory and implementation of a highly scalable and modular convex solver which solves all these estimation problem it can be parallelized on a cluster of workstation allows for data locality and can deal with regularizers such a l and l penalty at present our solver implement different estimation problem can be easily extended scale to million of observation and is up to time faster than specialized solver for many application the open source code is freely available a part of the elefant toolbox 
localized search engine are small scale system that index a particular community on the web they offer several benefit over their large scale counterpart in that they are relatively inexpensive to build and can provide more precise and complete search capability over their relevant domain one disadvantage such system have over large scale search engine is the lack of global pagerank value such information is needed to ass the value of page in the localized search domain within the context of the web a a whole in this paper we present well motivated algorithm to estimate the global pagerank value of a local domain the algorithm are all highly scalable in that given a local domain of size n they use o n resource that include computation time bandwidth and storage we test our method across a variety of localized domain including site specific domain and topic specific domain we demonstrate that by crawling a few a n or n additional page our method can give excellent global pagerank estimate 
when given a small sample we show that classification with svm can be considerably enhanced by using a kernel function learned from the training data prior to discrimination this kernel is also shown to enhance retrieval based on data similarity specifically we describe kernelboost a boosting algorithm which computes a kernel function a a combination of weak space partition the kernel learning method naturally incorporates domain knowledge in the form of unlabeled data i e in a semi supervised or transductive setting and also in the form of labeled sample from relevant related problem i e in a learning to learn scenario the latter goal is accomplished by learning a single kernel function for all class we show comparative evaluation of our method on datasets from the uci repository we demonstrate performance enhancement on two challenging task digit classification with kernel svm and facial image retrieval based on image similarity a measured by the learnt kernel 
this paper describes a highly successful application of mrfs to the problem of generating high resolution range image a new generation of range sensor combine the capture of low resolution range image with the acquisition of registered high resolution camera image the mrf in this paper exploit the fact that discontinuity in range and coloring tend to co align this enables it to generate high resolution low noise range image by integrating regular camera image into the range data we show that by using such an mrf we can substantially improve over existing range imaging technology 
many application of supervised learning require good generalization from limited labeled data in the bayesian setting we can try to achieve this goal by using an informative prior over the parameter one that encodes useful domain knowledge focusing on logistic regression we present an algorithm for automatically constructing a multivariate gaussian prior with a full covariance matrix for a given supervised learning task this prior relaxes a commonly used but overly simplistic independence assumption and allows parameter to be dependent the algorithm us other similar learning problem to estimate the covariance of pair of individual parameter we then use a semidefinite program to combine these estimate and learn a good prior for the current learning task we apply our method to binary text classification and demonstrate a to test error reduction over a commonly used prior 
although semi supervised learning ha been an active area of research it use in deployed application is still relatively rare because the method are often difficult to implement fragile in tuning or lacking in scalability this paper present expectation regularization a semi supervised learning method for exponential family parametric model that augments the traditional conditional label likelihood objective function with an additional term that encourages model prediction on unlabeled data to match certain expectation such a label prior the method is extremely easy to implement scale a well a logistic regression and can handle non independent feature we present experiment on five different data set showing accuracy improvement over other semi supervised method 
the present work aim to model the correspondence between facial motion and speech the face and sound are modelled separately with phoneme being the link between both we propose a sequential model and evaluate it suitability for the generation of the facial animation from a sequence of phoneme which we obtain from speech we evaluate the result both by computing the error between generated sequence and real video a well a with a rigorous double blind test with human subject experiment show that our model compare favourably to other existing method and that the sequence generated are comparable to real video sequence 
typically data collected by a spacecraft is downlinked to earth and preprocessed before any analysis is performed we have developed classifier that can be used onboard a spacecraft to identify high priority data for downlink to earth providing a method for maximizing the use of a potentially bandwidth limited downlink channel onboard analysis can also enable rapid reaction to dynamic event such a flooding volcanic eruption or sea ice break up four classifier were developed to identify cryosphere event using hyperspectral image these classifier include a manually constructed classifier a support vector machine svm a decision tree and a classifier derived by searching over combination of thresholded band ratio each of the classifier wa designed to run in the computationally constrained operating environment of the spacecraft a set of scene wa hand labeled to provide training and testing data performance result on the test data indicate that the svm and manual classifier outperformed the decision tree and band ratio classifier with the svm yielding slightly better classification than the manual classifier the manual and svm classifier have been uploaded to the eo spacecraft and have been running onboard the spacecraft for over a year result of the onboard analysis are used by the autonomous sciencecraft experiment ase of nasa s new millennium program onboard eo to automatically target the spacecraft to collect follow on imagery the software demonstrates the potential for future deep space mission to use onboard decision making to capture short lived science event 
we rigorously establish a close relationship between message passing algorithm and model of neurodynamics by showing that the equation of a continuous hopeld network can be derived from the equation of belief propagation on a binary markov random eld a hopeld network are equipped with a lyapunov function convergence is guaranteed a a consequence in the limit of many weak connection per neuron hopeld network exactly implement a continuous time variant of belief propagation starting from message initialisation that prevent from running into convergence problem our result lead to a better understanding of the role of message passing algorithm in real biological neural network 
network clustering or graph partitioning is an important task for the discovery of underlying structure in network many algorithm find cluster by maximizing the number of intra cluster edge while such algorithm find useful and interesting structure they tend to fail to identify and isolate two kind of vertex that play special role vertex that bridge cluster hub and vertex that are marginally connected to cluster outlier identifying hub is useful for application such a viral marketing and epidemiology since hub are responsible for spreading idea or disease in contrast outlier have little or no influence and may be isolated a noise in the data in this paper we proposed a novel algorithm called scan structural clustering algorithm for network which detects cluster hub and outlier in network it cluster vertex based on a structural similarity measure the algorithm is fast and efficient visiting each vertex only once an empirical evaluation of the method using both synthetic and real datasets demonstrates superior performance over other method such a the modularity based algorithm 
in biological neuron the timing of a spike depends on the timing of synaptic current in a way that is classically described by the phase response curve this ha implication for temporal coding an action potential t hat arrives on a synapse ha an implicit meaning that depends on the position of the postsynaptic neuron on the firing cycle here we show that this implicit code can be used to perform computation using theta neuron we derive a spike timing dependent learning rule from an error criterion we demonstrate how to train an a uto encoder neural network using this rule 
we present a competitive analysis of some non parametric bayesian algorithm in a worst case online learning setting where no probabilistic assumption about the generation of the data are made we consider model which use a gaussian process prior over the space of all function and provide bound on the regret under the log loss for commonly used non parametric bayesian algorithm including gaussian regression and logistic regression which show how these algorithm can perform favorably under rather general condition these bound explicitly handle the infinite dimensionality of these non parametric class in a natural way we also make formal connection to the minimax and minimum description length mdl framework here we show precisely how bayesian gaussian regression is a minimax strategy 
classification of multichannel eeg recording during motor imagination ha been exploited successfully for brain computer interface bci in this paper we consider eeg signal a the output of a networked dynamical system the cortex and exploit synchronization feature from the dynamical system for classification herein we also propose a new framework for learning optimal filter automatically from the data by employing a fisher ratio criterion experimental evaluation comparing the proposed dynamical system feature with the csp and the ar feature reveal their competitive performance during classification result also show the benefit of employing the spatial and the temporal filter optimized using the proposed learning approach 
we present a model of edge and region grouping using a conditional random eld built over a scale invariant representation of image to integrate multiple cue our model includes potential that capture low level similarity mid level curvilinear continuity and high level object shape maximum likelihood parameter for the model are learned from human labeled groundtruth on a large collection of horse image using belief propagation using held out test data we quantify the information gained by incorporating generic mid level cue and high level shape 
the four level pachinko allocation model pam li mccallum represents correlation among topic using a dag structure it doe not however represent a nested hierarchy of topic with some topical word distribution representing the vocabulary that is shared among several more specific topic this paper present hierarchical pam an enhancement that explicitly represents a topic hierarchy this model can be seen a combining the advantage of hlda s topical hierarchy representation with pam s ability to mix multiple leaf of the topic hierarchy experimental result show improvement in likelihood of held out document a well a mutual information between automatically discovered topic and humangenerated category such a journal 
in this paper we generalize the lars feature selection method to the linear svm model derive an efficient algorithm for it and empirically demonstrate it usefulness a a feature selection tool for text classification 
the problem of interestingness of discovered rule ha been investigated by many researcher the issue is that data mining algorithm often generate too many rule which make it very hard for the user to find the interesting one over the year many technique have been proposed however few have made it to real life application since august we have been working on a major application for motorola the objective is to find cause of cellular phone call failure from a large amount of usage log data class association rule have been shown to be suitable for this type of diagnostic data mining application we were also able to put several existing interestingness method to the test which revealed some major shortcoming one of the main problem is that most existing method treat rule individually however we discovered that user seldom regard a single rule to be interesting by itself a rule is only interesting in the context of some other rule furthermore in many case each individual rule may not be interesting but a group of them together can represent an important piece of knowledge this led u to discover a deficiency of the current rule mining paradigm using non zero minimum support and non zero minimum confidence eliminates a large amount of context information which make rule analysis difficult this paper proposes a novel approach to deal with all of these issue which cast rule analysis a olap operation and general impression mining this approach enables the user to explore the knowledge space to find useful knowledge easily and systematically it also provides a natural framework for visualization a an evidence of it effectiveness our system called opportunity map based on these idea ha been deployed and it is in daily use in motorola for finding actionable knowledge from it engineering and other type of data set 
latent dirichlet allocation lda is a bayesian network that ha recently gained much popularity in application ranging from document modeling to computer vision due to the large scale nature of these application current inference procedure like variational bayes and gibb sampling have been found lacking in this paper we propose the collapsed variational bayesian inference algorithm for lda and show that it is computationally efficient easy to implement and significantly more accurate than standard variational bayesian inference for lda 
we design a new learning algorithm for the set covering machine from a pac bayes perspective and propose a pac bayes risk bound which is minimized for classiflers achieving a non trivial margin sparsity trade ofi 
how do we find pattern in author keyword association evolving over time or in data cube with product branch customer sale information matrix decomposition like principal component analysis pca and variant are invaluable tool for mining dimensionality reduction feature selection rule identification in numerous setting like streaming data text graph social network and many more however they have only two order like author and keyword in the above example we propose to envision such higher order data a tensor and tap the vast literature on the topic however these method do not necessarily scale up let alone operate on semi infinite stream thus we introduce the dynamic tensor analysis dta method and it variant dta provides a compact summary for high order and high dimensional data and it also reveals the hidden correlation algorithmically we designed dta very carefully so that it is a scalable b space efficient it doe not need to store the past and c fully automatic with no need for user defined parameter moreover we propose sta a streaming tensor analysis method which provides a fast streaming approximation to dta we implemented all our method and applied them in two real setting namely anomaly detection and multi way latent semantic indexing we used two real large datasets one on network flow data gb over month and one from dblp mb over year our experiment show that our method are fast accurate and that they find interesting pattern and outlier on the real datasets 
we present a family of approximation technique for probabilistic graphical model based on the use of graphical preconditioners developed in the scientific computing literature our framework yield r igorous upper and lower bound on event probability and the log partition function of undirected graphical model using non iterative procedure that have low time complexity a in mean field approach the approxi mations are built upon tractable subgraphs however we recast the problem of optimizing the tractable distribution parameter and approx imate inference in term of the well studied linear system problem of obtaining a good matrix preconditioner experiment are presented that compare the new approximation scheme to variational method 
we introduce the use of learned shaping reward in reinforcement learning task where an agent us prior experience on a sequence of task to learn a portable predictor that estimate intermediate reward resulting in accelerated learning in later task that are related but distinct such agent can be trained on a sequence of relatively easy task in order to develop a more informative measure of reward that can be transferred to improve performance on more difficult task without requiring a hand coded shaping function we use a rod positioning task to show that this significantly improves performance even after a very brief training period 
we introduce binary matrix factorization a novel model for unsupervised matrix decomposition the decomposition is learned by fitting a non parametric bayesian probabilistic model with binary latent variable to a matrix of dyadic data unlike bi clustering model which assign each row or column to a single cluster based on a categorical hidden feature our binary feature model reflects the prior belief that item and attribute can be associated with more than one latent cluster at a time we provide simple learning and inference rule for this new model and show how to extend it to an infinite model in which the number of feature is not a priori fixed but is allowed to grow with the size of the data 
kernel based regularized learning seek a model in a hypothesis space by minimizing the empirical error and the model s complexity based on the representer theorem the solution consists of a linear combination of translates of a kernel this paper investigates a generalized form of representer theorem for kernel based learning after mapping predened feature and translates of a kernel simultaneously onto a hypothesis space by a specic way of constructing kernel we proposed a new algorithm by utilizing a generalized regularizer which leaf part of the space unregularized using a squared loss function in calculating the empirical error a simple convex solution is obtained which combine predened feature with translates of the kernel empirical evaluation have conrmed the effectiveness of the algorithm for supervised learning task 
we formalize the associative bandit problem framework introduced by kaelbling a a learning theory problem the learning environment is modeled a a k armed bandit where arm payoff are conditioned on an observable input selected on each trial we show that if the payoff function are constrained to a known hypothesis class learning can be performed efficiently with respect to the vc dimension of this class we formally reduce the problem of pac classification to the associative bandit problem producing an efficient algorithm for any hypothesis class for which efficient classification algorithm are known we demonstrate the approach empirically on a scalable concept class 
we describe an application of relational knowledge discovery to a key regulatory mission of the national association of security dealer nasd nasd is the world s largest private sector security regulator with responsibility for preventing and discovering misconduct among security broker our goal wa to help focus nasd s limited regulatory resource on the broker who are most likely to engage in security violation using statistical relational learning algorithm we developed model that rank broker with respect to the probability that they would commit a serious violation of security regulation in the near future our model incorporate organizational relationship among broker e g past coworker which domain expert consider important but have not been easily used before now the learned model were subjected to an extensive evaluation using more than month of data unseen by the model developer and comprising over two person week of effort by nasd staff model prediction were found to correlate highly with the subjective evaluation of experienced nasd examiner furthermore in all performance measure our model performed a well a or better than the handcrafted rule that are currently in use at nasd 
we investigate the problem of learning to predict move in the board game of go from game record of expert player in particular we obtain a probability distribution over legal move for professional play in a given position this distribution ha numerous application in computer go including serving a an efficient stand alone go player it would also be effective a a move selector and move sorter for game tree search and a a training tool for go player our method ha two major component a a pattern extraction scheme for efficiently harvesting pattern of given size and shape from expert game record and b a bayesian learning algorithm in two variant that learns a distribution over the value of a move given a board position based on the local pattern context the system is trained on expert game and show excellent prediction performance a indicated by it ability to perfectly predict the move made by professional go player in of test position 
we develop and analyze game theoretic algorithm for predicting coordinate binding of multiple dna binding regulator the allocation of protein to local neighborhood and to site is carried out with resource constraint while explicating competing and coordinate binding relation among protein with affinity to the site or region the focus of this paper is on mathematical foundation of the approach we also briefly demonstrate the approach in the context of the phage switch 
many time series experiment seek to estimate some signal a a continuous function of time in this paper we address the sampling problem for such experiment determining which time point ought to be sampled in order to minimize the cost of data collection we restrict our attention to a growing class of experiment which measure multiple signal at each time point and where raw material observation are archived initially and selectively analyzed later this analysis being the more expensive step we present an active learning algorithm for iteratively choosing time point to sample using the uncertainty in the quality of the currently estimated time dependent curve a the objective function using simulated data a well a gene expression data we show that our algorithm performs well and can significantly reduce experimental cost without loss of information 
graph data is getting increasingly popular in e g bioinformatics and text processing a main difficulty of graph data processing lie in the intrinsic high dimensionality of graph namely when a graph is represented a a binary feature vector of indicator of all possible subgraphs the dimensionality get too large for usual statistical method we propose an efficient method for learning a binomial mixture model in this feature space combining the l regularizer and the data structure called dfs code tree the map estimate of non zero parameter are computed efficiently by mean of the em algorithm our method is applied to the clustering of rna graph and is compared favorably with graph kernel and the spectral graph distance 
ontology represent data relationship a hierarchy of possibly overlapping class ontology are closely related to clustering hierarchy and in this article we explore this relationship in depth in particular we examine the space of ontology that can be generated by pairwise dissimilarity matrix we demonstrate that classical clustering algorithm which take dissimilarity matrix a input do not incorporate all available information in fact only special type of dissimilarity matrix can be exactly preserved by previous clustering method we model ontology a a partially ordered set poset over the subset relation in this paper we propose a new clustering algorithm that generates a partially ordered set of cluster from a dissimilarity matrix 
first order probabilistic model allow u to model situation in which a random variable in the first order model may have a large and varying number of parent variable in the ground unrolled model one approach to compactly describing such model is to independently specify the probability of a random variable conditioned on each individual parent or small set of parent and then combine these conditional distribution via a combining rule e g noisy or this paper present algorithm for learning with combining rule specifically algorithm based on gradient descent and expectation maximization are derived implemented and evaluated on synthetic data and on a real world task the result demonstrate that the algorithm are able to learn the parameter of both the individual parent target distribution and the combining rule 
some model of textual corpus employ text generation method involving n gram statistic while others use latent topic variable inferred using the bag of word assumption in which word order is ignored previously these method have not been combined in this work i explore a hierarchical generative probabilistic model that incorporates both n gram statistic and latent topic variable by extending a unigram topic model to include property of a hierarchical dirichlet bigram language model the model hyperparameters are inferred using a gibbs em algorithm on two data set each of document the new model exhibit better predictive accuracy than either a hierarchical dirichlet bigram language model or a unigram topic model additionally the inferred topic are le dominated by function word than are topic discovered using unigram statistic potentially making them more meaningful 
we characterize the sample complexity of active learning problem in term of a parameter which take into account the distribution over the input space the specific target hypothesis and the desired accuracy 
based on a large scale spiking neuron model of the input layer candof macaque we identify neural mechanism for the observed contrast dependent receptive field size of v cell we observe a rich variety of m echanisms for the phenomenon and analyze them based on the relative gain of excitatory and inhibitory synaptic input we observe an average growth in the spatial extent of excitation and inhibition for low contrast a predicted fr om phenomenological model however contrary to phenomenological model our simulation result suggest this is neither sufficient nor necessary to explain t he phenomenon 
in this work we focus on the problem of frequent itemset mining on large out of core data set after presenting a characterization of existing out of core frequent itemset mining algorithm and their drawback we introduce our efficient highly scalable solution presented in the context of the fpgrowth algorithm our technique involves several novel i o conscious optimization such a approximate hash based sorting and blocking and leverage recent architectural advancement in commodity computer such a bit processing we evaluate the proposed optimization on truly large data set up to gb and show they yield greater than a fold execution time improvement finally we discus the impact of this research in the context of other pattern mining challenge such a sequence mining and graph mining 
in a data streaming setting data point are observed one by one the concept to be learned from the data point may change infinitely often a the data is streaming in this paper we extend the idea of testing exchangeability online vovk et al to a martingale framework to detect concept change in time varying data stream two martingale test are developed to detect concept change using i martingale value a direct consequence of the doob s maximal inequality and ii the martingale difference justified using the hoeffding azuma inequality under some assumption the second test theoretically ha a lower probability than the first test of rejecting the null hypothesis no concept change in the data stream when it is in fact correct experiment show that both martingale test are effective in detecting concept change in time varying data stream simulated using two synthetic data set and three benchmark data set 
standard statistical model of language fail to capture one of the most striking property of natural language the power law distribution in the frequency of word token we present a framework for developing statistical model that generically produce power law augmenting standard generative model with an adaptor that produce the appropriate pattern of token frequency we show that taking a particular stochastic process the pitman yor process a an adaptor justies the appearance of type frequency in formal analysis of natural language and improves the performance of a model for unsupervised learning of morphology in this paper we introduce a framework for developing generative model for language that produce power law distribution our framework is based upon the idea of specifying language model in term of two component a generator an underlying generative model for word which need not and usually doe not produce a power law distribution and an adaptor which transforms the stream of word produced by the generator into one whose frequency obey a power law distribution this framework is extremely general any generative model for language can be used a a generator with the power law distribution being produced a the result of making an appropriate choice for the adaptor in our framework estimation of the parameter of the generator will be affected by assumption about the form of the adaptor we show that use of a particular adaptor the pitmanyor process shed light on a tension exhibited by formal approach to natural language whether explanation should be based upon the type of word that language exhibit or the frequency with which token of those word occur one place where this 
we considered a gamma distribution of interspike interval a a statistical model for neuronal spike generation the model parameter consist of a time dependent firing rate and a shape parameter that characterizes spiking irregularity of individual neuron because the environment change with time observed data are generated from the time dependent firing rate which is an unknown function a statistical model with an unknown function is called a semiparametric model which is one of the unsolved problem in statistic and is generally very difficult to solve we used a novel method of estimating function in information geometry to estimate the shape parameter without estimating the unknown function we analytically obtained an optimal estimating function for the shape parameter independent of the functional form of the firing rate this estimation is efficient without fisher information loss and better than maximum likelihood estimation 
this paper present a diffusion based probabilistic interpretation of spectral clustering and dimensionality reduction algorithm that use the eigenvectors of the normalized graph laplacian given the pairwise adjacency matrix of all point we define a diffusion distance between any two data point and show that the low dimensional representation of the data by the first few eigenvectors of the corresponding markov matrix is optimal under a certain mean squared error criterion furthermore assuming that data point are random sample from a density p x e u x we identify these eigenvectors a discrete approximation of eigenfunctions of a fokker planck operator in a potential u x with reflecting boundary condition finally applying known result regarding the eigenvalue and eigenfunctions of the continuous fokker planck operator we provide a mathematical justification for the success of spectral clustering and dimensional reduction algorithm based on these first few eigenvectors this analysis elucidates in term of the characteristic of diffusion process many empirical finding regarding spectral clustering algorithm 
the blogosphere ha unique structural and temporal property since blog are typically used a communication medium among human individual in this paper we propose a novel technique that capture the structure and temporal dynamic of blog community in our framework a community is a set of blog that communicate with each other triggered by some event such a a news article the community is represented by it structure and temporal dynamic a community graph indicates how often one blog communicates with another and a community intensity indicates the activity level of the community that varies over time our method community factorization extract such community from the blogosphere where the communication among blog is observed a a set of subgraphs i e thread of discussion this community extraction is formulated a a factorization problem in the framework of constrained optimization in which the objective is to best explain the observed interaction in the blogosphere over time we further provide a scalable algorithm for computing solution to the constrained optimization problem extensive experimental study on both synthetic and real blog data demonstrate that our technique is able to discover meaningful community that are not detectable by traditional method 
a good image object detection algorithm is accurate fast and doe not require exact location of object in a training set we can create such an object detector by taking the architecture of the viola jones detector cascade and training it with a new variant of boosting that we call milboost milboost us cost function from the multiple instance learning literature combined with the anyboost framework we adapt the feature selection criterion of milboost to optimize the performance of the viola jones cascade experiment show that the detection rate is up to time better using milboost this increased detection rate show the advantage of simultaneously learning the location and scale of the object in the training set along with the parameter of the classifier 
sound localization by barn owl is commonly modeled a a matching procedure where localization cue derived from auditory input are compared to stored template while the matching model can explain property of neural response no model explains how the owl resolve spatial ambiguity in the localization cue to produce accurate localization for source near the center o f gaze here i examine two model for the barn owl s sound localization behavio r first i consider a maximum likelihood estimator in order to further evaluate the cue matching model second i consider a maximum a posteriori estimator to test whether a bayesian model with a prior that emphasizes direction near the center of gaze can reproduce the owl s localization behavior i show that t he maximum likelihood estimator can not reproduce the owl s behavior while t he maximum a posteriori estimator is able to match the behavior this result su ggests that the standard cue matching model will not be sufficient to explain sound loc alization behavior in the barn owl the bayesian model provides a new framework for analyzing sound localization in the barn owl and lead to prediction about t he owl s localization behavior 
we consider the problem of fitting a large scale covariance matrix to multivariate gaussian data in such a way that the inverse is sparse thus providing model selection beginning with a dense empirical covariance matrix we solve a maximum likelihood problem with an l norm penalty term added to encourage sparsity in the inverse for model with ten of node the resulting problem can be solved using standard interior point algorithm for convex optimization but these method scale poorly with problem size we present two new algorithm aimed at solving problem with a thousand node the first based on nesterov s first order algorithm yield a rigorous complexity estimate for the problem with a much better dependence on problem size than interior point method our second algorithm us block coordinate descent updating row column of the covariance matrix sequentially experiment with genomic data show that our method is able to uncover biologically interpretable connection among gene 
a non linear dynamic system is called contracting if initial condition are forgotten exponentially fast so that all trajectory converge to a single trajectory we use contraction theory to derive an upper bound for the strength of recurrent connection that guarantee contraction for complex neural network specifically we apply this theory to a special class of recurrent network often called cooperative competitive network ccns which are an abstract representation of the cooperative competitive connectivity observed in cortex this specific type of network is believed to play a major role in shaping cortical response and selecting the relevant signal among distractors and noise in this paper we analyze contraction of combined ccns of linear threshold unit and verify the result of our analysis in a hybrid analog digital vlsi ccn comprising spiking neuron and dynamic synapsis 
spectral clustering method are common graph based approach to clustering of data spectral clustering algorithm typically start from local information encoded in a weighted graph on the data and cluster according to the global eigenvectors of the corresponding normalized similarity matrix one contribution of this paper is to present fundamental limitation of this general local to global approach we show that based only on local information the normalized cut functional is not a suitable measure for the quality of clustering further ev en with a suitable similarity measure we show that the first few eigenvectors of suc h adjacency matrix cannot successfully cluster datasets that contain structu re at different scale of size and density based on these finding a second contribut ion of this paper is a novel diffusion based measure to evaluate the coherence of individual cluster our measure can be used in conjunction with any bottom up graph based clustering method it is scale free and can determine coherent cluster at all scale we present both synthetic example and real image segmentation problem where various spectral clustering algorithm fail in contrast usi ng this coherence measure find the expected cluster at all scale 
recent research ha identified significant vulnerability in recommender system shilling attack in which attacker introduce biased rating in order to influence future recommendation have been shown to be effective against collaborative filtering algorithm we postulate that the distribution of item rating in time can reveal the presence of a wide range of shilling attack given reasonable assumption about their duration to construct a time series of rating for an item we use a window size of k to group consecutive rating for the item into disjoint window and compute the sample average and sample entropy in each window we derive a theoretically optimal window size to best detect an attack event if the number of attack profile is known for practical application where this number is unknown we propose a heuristic algorithm that adaptively change the window size our experimental result demonstrate that monitoring rating distribution in time series is an effective approach for detecting shilling attack 
in this paper we propose an information retrieval model called latent interest semantic map lism which feature retrieval composed of both collaborative filtering cf and probabilistic latent semantic analysis plsa the motivation behind this study is that the relation between user and document can be explained by the two different latent class where user belong probabilistically in one or more class with the same interest group while document also belong probabilistically in one or more class with the same topic group the novel aspect of lism is that it simultaneously provides a user model and latent semantic analysis in one map this benefit of lism is to enable collaborative filtering in term of user interest and document topic and thus solve the cold start problem 
semantic memory refers to our knowledge of fact and relationship between concept a successful semantic memory depends on inferring relationship between item that are not explicitly taught recent mathematical modeling of episodic memory argues that episodic recall relies on retrieval of a gradually changing representation of temporal context we show that retrieved context enables the development of a global memory space that reflects relationship between all item that have been previously learned when newly learned information is integrated into this structure it is placed in some relationship to all other item even if that relationship ha not been explicitly learned we demonstrate this effect for global semantic structure shaped topologically a a ring and a a two dimensional sheet we also examined the utility of this learning algorithm for learning a more realistic semantic space by training it on a large pool of synonym pair retrieved context enabled the model to infer relationship between synonym pair that had not yet been presented 
we address the problem of learning a symmetric positive definite matrix the central issue is to design parameter update that preserve positive definiteness our update are motivated with the von neumann divergence rather than treating the most general case we focus on two key application that exemplify our method on line learning with a simple square loss and finding a symmetric positive definite matrix subject to linear constraint the update generalize the exponentiated gradient eg update and adaboost respectively the parameter is now a symmetric positive definite matrix of trace one instead of a probability vector which in this context is a diagonal positive definite matrix with trace one the generalized update use matrix logarithm and exponential to preserve positive definiteness most importantly we show how the derivation and the analysis of the original eg update and adaboost generalize to the non diagonal case we apply the resulting matrix exponentiated gradient meg update and definiteboost to the problem of learning a kernel matrix from distance measurement 
we propose to investigate test statistic for testing homog eneity based on kernel fisher discriminant analysis asymptotic null distributi on under null hypothesis is derived and consistency against fixed alternative is a sessed finally experimental evidence of the performance of the proposed approach on artificial data and on a speaker verification task is provided 
in system management application an overwhelming amount of data are generated and collected in the form of temporal event while mining temporal event data to discover interesting and frequent pattern ha obtained rapidly increasing research effort user of the application are overwhelmed by the mining result the extracted pattern are generally of large volume and hard to interpret they may be of no emphasis intricate and meaningless to non expert even to domain expert while traditional research effort focus on finding interesting pattern in this paper we take a novel approach called event summarization towards the understanding of the seemingly chaotic temporal data event summarization aim at providing a concise interpretation of the seemingly chaotic data so that domain expert may take action upon the summarized model event summarization decomposes the temporal information into many independent subset and find well fitted model to describe each subset 
we show that it is possible to use data compression on independently obtained hypothesis from various task to algorithmically provide guarantee that the task are sufficiently related to benefit from multitask learning we give uniform bound in term of the empirical average error for the true average error of the n hypothesis provided by deterministic learning algorithm drawing independent sample from a set of n unknown computable task distribution over finite set 
we address the problem of detecting batch of email that have been created according to the same template this problem is motivated by the desire to filter spam more effectively by exploiting collective information about entire batch of jointly generated message the application match the problem setting of supervised clustering because example of correct clustering can be collected known decoding procedure for supervised clustering are cubic in the number of instance when decision cannot be reconsidered once they have been made owing to the streaming nature of the data then the decoding problem can be solved in linear time we devise a sequential decoding procedure and derive the corresponding optimization problem of supervised clustering we study the impact of collective attribute of email batch on the effectiveness of recognizing spam email 
we propose a new text mining system which extract characteristic content from given document we define key semantics a characteristic sub structure of syntactic dependency in the given document and consider the following three task in this paper key semantics extraction extracting characteristic syntactic dependency structure not only a ordered tree but also a unordered tree and free tree redundancy reduction from the result of extraction deleting redundant dependency structure such a sub structure or equivalent structure of the others and phrase sentence reconstruction generating a phrase or sentence in a natural language corresponding to the extracted structure our system is a combination of natural language processing technique and tree mining technique the system consists of the following five unit syntactic dependency analysis unit input filter characteristic ordered subtree extraction unit output filter and phrase sentence reconstruction unit although ordered tree are extracted in the third unit the overall behavior of the system can be switched into the extraction of ordered tree unordered tree or free tree depending on which of the input filter is are applied in the second step the output filter delete redundant tree from the extraction result for efficient knowledge discovery finally phrase or sentence corresponding to the extracted subtrees are reconstructed by utilizing the input document we demonstrate the validity of our system by showing experimental result using real data collected at a help desk and tdt pilot corpus 
we investigate under what condition a neuron can learn by experimentally supported rule for spike timing dependent plasticity stdp to predict the arrival time of strong teacher input to the same neuron it turn out that in contrast to the famous perceptron convergence theorem which predicts convergence of the perceptron learning rule for a simplified neuron model whenever a stable solution exists no equally strong convergence guarantee can be given for spiking neuron with stdp but we derive a criterion on the statistical dependency structure of input spike train which characterizes exactly when learning with stdp will converge on average for a simple model of a spiking neuron this criterion is reminiscent of the linear separability criterion of the perceptron convergence theorem but it applies here to the row of a correlation matrix related to the spike input in addition we show through computer simulation for more realistic neuron model that the resulting analytically predicted positive learning result not only hold for the common interpretation of stdp where stdp change the weight of synapsis but also for a more realistic interpretation suggested by experimental data where stdp modulates the initial release probability of dynamic synapsis 
the expectation maximization em algorithm is a widely used maximum likelihood estimation procedure for statistical model when the value of some of the variable in the model are not observed very often however our aim is primarily to find a model that assigns value to the latent variable that have intended meaning for our data and maximizing expected likelihood only sometimes accomplishes this unfortunately it is typically difficult to add even simple a priori information about latent variable in graphical model without making the model overly complex or intractable in this paper we present an efficient principled way to inject rich constraint on the posterior of latent variable into the em algorithm our method can be used to learn tractable graphical model that satisfy additional otherwise intractable constraint focusing on clustering and the alignment problem for statistical machine translation we show that simple intuitive posterior constraint can greatly improve the performance over standard baseline and be competitive with more complex intractable model 
we consider approximate value iteration with a parameterized approximator in which the state space is partitioned and the optimal cost to go function over each partition is approximated by a constant we establish performance loss bound for policy derived from approximation associated with fixed point these bound identify benefit to having projection weight equal to the invariant distribution of the resulting policy such projection weighting lead to the same fixed point a td our analysis also lead to the first performance loss bound for approximate value iteration with an average cost objective 
we present a probabilistic model based framework for distributed learning that take into account privacy restriction and is applicable to scenario where the different site have diverse possibly overlapping subset of feature our framework decouples data privacy issue from knowledge integration issue by requiring the individual site to share only privacy safe probabilistic model of the local data which are then integrated to obtain a global probabilistic model based on the union of the feature available at all the site we provide a mathematical formulation of the model integration problem using the maximum likelihood and maximum entropy principle and describe iterative algorithm that are guaranteed to converge to the optimal solution for certain commonly occurring special case involving hierarchically ordered feature set or conditional independence we obtain closed form solution and use these to propose an efficient alternative scheme by recursive decomposition of the model integration problem to address interpretability concern we also present a modified formulation where the global model is assumed to belong to a specified parametric family finally to highlight the generality of our framework we provide empirical result for various learning task such a clustering and classification on different kind of datasets consisting of continuous vector categorical and directional attribute the result show that high quality global model can be obtained without much loss of privacy 
we analyze skewing an approach that ha been empirically observed to enable greedy decision tree learner to learn difficult boolean function such a parity in the presence of irrelevant variable we prove tha in an idealized setting for any function and choice of skew parameter skewing find relevant variable with probability we present experiment exploring how different parameter choice affect the success of skewing in empirical setting finally we analyze a variant of skewing called sequential skewing 
abstract this paper compare a family of method for characterizing neural feature selectivity with natural stimulus in the framework of the linear nonlinear model in this model the neural firing rate is a nonlinear function of a small number of relevant stimulus component the relevant stimulus dimension can be found by maximizing one of the family of objective function r enyi divergence of different order we show that maximizing one of them r enyi divergence of order is equivalent to least square fitting of the linear nonlinear model to neural data next we derive reconstruction error in relevant dimension found by maximizing r enyi divergence of arbitrary order in the asymptotic limit of large spike number we find that the smallest error are obtained with r enyi divergence of order also known a kullback leibler divergence this corresponds to finding relevant dimension by maximizing mutual information finally we numerically test how these optimization scheme perform in the regime of low signal to noise ratio small number of spike and increasing neural noise for model visual neuron we find that optimization scheme based on either least square fitting or information maximization perform well even when number of spike is small information maximization provides slightly but significantly better reconstruction than least square fitting this make the problem of finding relevant dimension one of the example where information theoretic measure are no more data limited than those derived from least square 
we present a novel algorithm for computing a training set consistent subset for the nearest neighbor decision rule the algorithm called fcnn rule ha some desirable property indeed it is order independent and ha subquadratic worst case time complexity while it requires few iteration to converge and it is likely to select point very close to the decision boundary we compare the fcnn rule with state of the art competence preservation algorithm on large multidimensional training set showing that it outperforms existing method in term of learning speed and learning scaling behavior and in term of size of the model while it guarantee a comparable prediction accuracy 
using extension of linear algebra concept to reproducing kernel hilbert space rkhs we define a unifying framework for random walk kernel on graph reduction to a sylvester equation allows u to compute many of these kernel in o n worst case time this includes kernel whose previous worst case time complexity wa o n such a the geometric kernel of gartner et al and the marginal graph kernel of kashima et al our algebra in rkhs allow u to exploit sparsity in directed and undirected graph more effectively than previous method yielding sub cubic computational complexity when combined with conjugate gradient solver or fixed point iteration experiment on graph from bioinformatics and other application domain show that our algorithm are often more than time faster than existing approach 
grid system are proving increasingly useful for managing the batch computing job of organization one well known example is intel whose internally developed netbatch system manages ten of thousand of machine the size heterogeneity and complexity of grid system make them very difficult however to configure this often result in misconfigured machine which may adversely affect the entire system we investigate a distributed data mining approach for detection of misconfigured machine our grid monitoring system gm non intrusively collect data from all source log file system service etc available throughout the grid system it convert raw data to semantically meaningful data and store this data on the machine it wa obtained from limiting incurred overhead and allowing scalability afterwards when analysis is requested a distributed outlier detection algorithm is employed to identify misconfigured machine the algorithm itself is implemented a a recursive workflow of grid job it is especially suited to grid system in which the machine might be unavailable most of the time and often fail altogether 
the web provides an unprecedented opportunity to evaluate idea quickly using controlled experiment also called randomized experiment single factor or factorial design a b test and their generalization split test control treatment test and parallel flight controlled experiment embody the best scientific design for establishing a causal relationship between change and their influence on user observable behavior we provide a practical guide to conducting online experiment where end user can help guide the development of feature our experience indicates that significant learning and return on investment roi are seen when development team listen to their customer not to the highest paid person s opinion hippo we provide several example of controlled experiment with surprising result we review the important ingredient of running controlled experiment and discus their limitation both technical and organizational we focus on several area that are critical to experimentation including statistical power sample size and technique for variance reduction we describe common architecture for experimentation system and analyze their advantage and disadvantage we evaluate randomization and hashing technique which we show are not a simple in practice a is often assumed controlled experiment typically generate large amount of data which can be analyzed using data mining technique to gain deeper understanding of the factor influencing the outcome of interest leading to new hypothesis and creating a virtuous cycle of improvement organization that embrace controlled experiment with clear evaluation criterion can evolve their system with automated optimization and real time analysis based on our extensive practical experience with multiple system and organization we share key lesson that will help practitioner in running trustworthy controlled experiment 
in this paper we propose a novel exemplar based approach to extract dynamic foreground region from a changing background within a collection of image or a video sequence by using image segmentation a a pre processing step we convert this traditional pixel wise labeling problem into a lower dimensional supervised binary labeling procedure on image segment our approach consists of three step first a set of random image patch are spatial ly and adaptively sampled within each segment second these set of extracted sample are formed into two bag of patch to model the foreground background appearance respectively we perform a novel bidirectional consistency check between new patch from incoming frame and current bag of patch to reject outlier control model rigidity and make the model adaptive to new observation within each bag image patch are further partitioned and resampled to create an evolving appearance model finally the foreground background decision over segment in an image is formulated using an aggregation function defin ed on the similarity measurement of sampled patch relative to the foreground and background model the essence of the algorithm is conceptually simple and can be easily implemented within a few hundred line of matlab code we evaluate and validate the proposed approach by extensive real example of the object level image mapping and tracking within a variety of challenging environment we also show that it is straightforward to apply our problem formulation on non rigid object tracking with difficult surveillance video 
outlier detection ha recently become an important problem in many industrial and financial application in this paper a novel feature bagging approach for detecting outlier in very large high dimensional and noisy database is proposed it combine result from multiple outlier detection algorithm that are applied using different set of feature every outlier detection algorithm us a small subset of feature that are randomly selected from the original feature set a a result each outlier detector identifies different outlier and thus assigns to all data record outlier score that correspond to their probability of being outlier the outlier score computed by the individual outlier detection algorithm are then combined in order to find the better quality outlier experiment performed on several synthetic and real life data set show that the proposed method for combining output from multiple outlier detection algorithm provide non trivial improvement over the base algorithm 
we consider the problem of multi task reinforcement learning where the agent need to solve a sequence of markov decision process mdps chosen randomly from a fixed but unknown distribution we model the distribution over mdps using a hierarchical bayesian infinite mixture model for each novel mdp we use the previously learned distribution a an informed prior for modelbased bayesian reinforcement learning the hierarchical bayesian framework provides a strong prior that allows u to rapidly infer the characteristic of new environment based on previous environment while the use of a nonparametric model allows u to quickly adapt to environment we have not encountered before in addition the use of infinite mixture allows for the model to automatically learn the number of underlying mdp component we evaluate our approach and show that it lead to significant speedup in convergence to an optimal policy after observing only a small number of task 
abstract we propose a model by which dopamine da and norepinepherine ne combine to alternate behavior between relatively exploratory and exploitative mode the model is developed for a target detection task for which there is extant single neuron recording data available from locus coeruleus lc ne neuron an exploration exploitation trade off is elicited by regularly switching which of the two stimulus are rewarded da function within the model to change synaptic weight according to a reinforcement learning algorithm exploration is mediated by the state of lc firing with higher tonic and lower phasic activity producing greater response variability the opposite state of lc function with lower baseline firing rate and greater phasic response favor exploitative behavior change in lc firing mode result from combined measure of response conflict and reward rate where response conflict is monitored using model of anterior cingulate cortex acc increased long term response conflict and decreased reward rate which occurs following reward contingency switch favor the higher tonic state of lc function and ne release this increase exploration and facilitates discovery of the new target in t ro d u ct i on a central problem in reinforcement learning is determining how to adaptively move 
we consider the problem of learning classiers for structurally incomplete data where some object have a subset of feature inherently absent due to complex relationship between the feature the common approach for handling missing feature is to begin with a preprocessing phase that completes the missing feature and then use a standard classication procedure in this paper we show how incomplete data can be classied directly without any completion of the missing feature using a max margin learning framework we formulate this task using a geometrically inspired objective function and discus two optimization approach the linearly separable case is written a a set of convex feasibility problem and the non separable case ha a non convex objective that we optimize iteratively by avoiding the pre processing phase in which the data is completed these approach oer considerable computational saving more importantly we show that by elegantly handling complex pattern of missing value our approach is both competitive with other method when the value are missing at random and outperforms them when the missing value have non trivial structure we demonstrate our result on two real world problem edge prediction in metabolic pathway and automobile detection in natural image 
we consider the scaling of the number of example necessary to achieve good performance in distributed cooperative multi agent reinforcement learning a a function of the the number of agent n we prove a worstcase lower bound showing that algorithm that rely solely on a global reward signal to learn policy confront a fundamental limit they require a number of real world example that scale roughly linearly in the number of agent for setting of interest with a very large number of agent this is impractical we demonstrate however that there is a class of algorithm that by taking advantage of local reward signal in large distributed markov decision process are able to ensure good performance with a number of sample that scale a o log n this make them applicable even in setting with a very large number of agent n 
we present a simple variant of the k d tree which automatically adapts to intrinsic low dimensional structure in data 
we analyze a simple bellman error based approach to generating basis function for value function approximation we show that it generates orthogonal basis function that provably tighten approximation error bound we also illustrate the use of this approach in the presence of noise on some sample problem 
clustering with constraint is a developing area of machine learning various paper have used constraint to enforce particular clustering seed clustering algorithm and even learn distance function which are then used for clustering we present intractability result for some constraint combination and illustrate both formally and experimentally the implication of these result for using constraint with clustering 
we propose framework and algorithm for identifying community in social network that change over time community are intuitively characterized a unusually densely knit subset of a social network this notion becomes more problematic if the social interaction change over time aggregating social network over time can radically misrepresent the existing and changing community structure instead we propose an optimization based approach for modeling dynamic community structure we prove that finding the most explanatory community structure is np hard and apx hard and propose algorithm based on dynamic programming exhaustive search maximum matching and greedy heuristic we demonstrate empirically that the heuristic trace development of community structure accurately for several synthetic and real world example 
there is much recent work on detecting and tracking change in cluster often based on the study of the spatiotemporal property of a cluster for the many application where cluster change is relevant among them customer relationship management fraud detection and marketing it is also necessary to provide insight about the nature of cluster change is a cluster corresponding to a group of customer simply disappearing or are it member migrating to other cluster is a new emerging cluster reflecting a new target group of customer or doe it rather consist of existing customer whose preference shift to answer such question we propose the framework monic for modeling and tracking of cluster transition our cluster transition model encompasses change that involve more than one cluster thus allowing for insight on cluster change in the whole clustering our transition tracking mechanism is not based on the topological property of cluster which are only available for some type of clustering but on the content of the underlying data stream we present our first result on monitoring cluster transition over the acm digital library 
in this paper we consider the problem of identifying and segmenting topically cohesive region in the url tree of a large website each page of the website is assumed to have a topic label or a distribution on topic label generated using a standard classifier we develop a set of cost measure characterizing the benefit accrued by introducing a segmentation of the site based on the topic label we propose a general framework to use these measure for describing the quality of a segmentation we also provide an efficient algorithm to find the best segmentation in this framework extensive experiment on human labeled data confirm the soundness of our framework and suggest that a judicious choice of cost measure allows the algorithm to perform surprisingly accurate topical segmentation 
probabilistic modeling of correlated neural population firing activity is central to understanding the neural code and building practical decoding algorithm no parametric model currently exist for modeling multivariate correlated neural data and the high dimensional nature of the data make fully non parametric method impractical to address these problem we propose an energy based in which the joint probability of neural activity is represented a a gibbs model using learned function of the d marginal histogram of the data the parameter of the model are learned using contrastive divergence and an optimization procedure for finding appropriate marginal direction we evaluate the method using real data recorded from a population of motor cortical neuron in particular we model the joint probability of population spiking time and d hand position and show that the likelihood of test data under our model is significantly higher than under other model these result suggest that our model capture correlation in the firing activity our rich probabilistic model of neural population activity is a step towards both measurement of the importance of correlation in neural coding and improved decoding of population activity 
machine learning and data mining can be effectively used to model classify and discover interesting information for a wide variety of data including email the email mining toolkit emt ha been designed to provide a wide range of analysis for arbitrary email source depending upon the task one can usually achieve very high accuracy but with some amount of false positive tradeoff generally false positive are prohibitively expensive in the real world in the case of spam detection for example even if one email is misclassified this may be unacceptable if it is a very important email much work ha been done to improve specific algorithm for the task of detecting unwanted message but le work ha been report on leveraging multiple algorithm and correlating model in this particular domain of email analysis emt ha been updated with new correlation function allowing the analyst to integrate a number of emt s user behavior model available in the core technology we present result of combining classifier output for improving both accuracy and reducing false positive for the problem of spam detection we apply these method to a very large email data set and show result of different combination method on these corpus we introduce a new method to compare multiple and combined classifier and show how it differs from past work the method analyzes the relative gain and maximum possible accuracy that can be achieved for certain combination of classifier to automatically choose the best combination 
in this paper the proposed liped life profile based event detection employ the concept of life profile to predict the activeness of event for effective event detection a group of event with similar activeness pattern share a life profile modeled by a hidden markov model considering the burst and diverse property of event liped identifies the activeness status of event a a result liped balance the clustering precision and recall to achieve better f score than other well known approach evaluated on the official tdt corpus 
the category of visual stimulus ha been reliably decoded from pattern of neural activity in extrastriate visual cortex it ha yet to be seen whether object identity can be inferred from this activity we present fmri data measuring response in human extrastriate cortex to a set of distinct object image we use a simple winner take all classifier using half the data from each recording session a a training set to evaluate encoding of object identity across fmri voxels since this approach is sensitive to the inclusion of noisy voxels we describe two method for identifying subset of voxels in the data which optimally distinguish object identity one method characterizes the reliability of each voxel within subset of the data while another estimate the mutual information of each voxel with the stimulus set we find that both metric can identify subset of the data which reliably encode object identity even when noisy measurement are artificially added to the data the mutual information metric is le efficient at this task likely due to constraint in fmri data 
learning in real world domain often requires to deal with continuous state and action space although many solution have been proposed to apply reinforcement learning algorithm to continuous state problem the same technique can be hardly extended to continuous action space where besides the computation of a good approximation of the value function a fast method for the identifi cation of the highest valued action is needed in this paper we propose a novel actor critic approach in which the policy of the actor is estimated through sequential monte carlo method the importance sampling step is performed on the basis of the value learned by the critic while the resampling step modifi e the actor s policy the proposed approach ha been empirically compared to other learning algorithm into several domain in this paper we report result obtained in a control problem consisting of steering a boat across a river 
many task in speech processing involve classification of lo ng term characteristic of a speech segment such a language speaker dialect or topic a natural technique for determining these characteristic is to first conv ert the input speech into a sequence of token such a word phone etc from these token we can then look for distinctive sequence keywords that characterize the speech in many application a set of distinctive keywords may not be known a priori in this case an automatic method of building up keywords from short context unit such a phone is desirable we propose a method for the construction of keywords based upon support vector machine we cast the problem of keyword selection a a feature selection problem for n gram of phone we propose an alternating filter wrapper method that build successively longer k eywords application of this method to language recognition and topic recognition task show that the technique produce interesting and significant qualitativ e and quantitative result 
we introduce a bayesian model bayesanil that is capable of estimating uncertainty associated with the labeling process given a labeled or partially labeled training corpus of text document the model estimate the joint distribution of training document and class label by using a generalization of the expectation maximization algorithm the estimate can be used in standard classification model to reduce error rate since uncertainty in the labeling are taken into account the model provides an elegant mechanism to deal with noisy label we provide an intuitive modification to the em iteration by re estimating the empirical distribution in order to reinforce feature value in unlabeled data and to reduce the influence of noisily labeled example considerable improvement in the classification accuracy of two popular classification algorithm on standard labeled data set with and without artificially introduced noise a well a in the presence and absence of unlabeled data indicates that this may be a promising method to reduce the burden of manual labeling 
this paper present a new framework based on walk in a graph for analysis and inference in gaussian graphical model the key idea is to decompose correlation between variable a a sum over all walk between those variable in the graph the weight of each walk is given by a product of edgewise partial correlation we provide a walk sum interpretation of gaussian belief propagation in tree and of the approximate method of loopy belief propagation in graph with cycle this perspective lead to a better understanding of gaussian belief propagation and of it convergence in loopy graph 
kernel method make it relatively easy to define complex high dimensional feature space this raise the question of how we can identify the relevant subspace for a particular learning t ask when two view of the same phenomenon are available kernel canonical correlation analysis kcca ha been shown to be an effective preprocessing step that can improve the performance of classification algo rithms such a the support vector machine svm this paper take this observation to it logical conclusion and proposes a method that combine this two stage learning kcca followed by svm into a single optimisation termed svm k we present both experimental and theoretical analysis of the approach showing encouraging result and insight 
this paper aim to model relational data on edge of network we describe appropriate gaussian process gps for directed undirected and bipartite network the inter dependency of edge can be effectively modeled by adapting the gp hyper parameter the framework suggests an intimate connection between link prediction and transfer learning which were traditionally two separate research topic we develop an efficient learning algorithm that can handle a large number of observation the experimental result on several real world data set verify superior learning capacity the goal of this paper is to design a gaussian process gp framework to model the dependence structure of network and to contribute an efficient algorithm to learn and predict large scale relational data we explicitly construct a series of parametric model indexed by their dimensionality and show that in the limit we obtain nonparametric gp prior consistent with the dependence of edge wise measurement since the kernel matrix is on a quadratic number of edge and the computation cost is even cubic of the kernel size we develop an efficient algorithm to reduce the computational complexity we also demonstrate that transfer learning ha an intimate connection to link prediction our method generalizes several recent transfer learning algorithm by additionally learning a task specific kernel that directly express the dependence between task 
we propose a series of new feature weighting algorithm all stemming from a new interpretation of relief a an online algorithm that solves a convex optimization problem with a margin based objective function the new interpretation explains the simplicity and effectiveness of relief and enables u to identify some of it weakness we offer an analytic solution to mitigate these problem we extend the newly proposed algorithm to handle multiclass problem by using a new multiclass margin definition to reduce computational cost an online learning algorithm is also developed convergence theorem of the proposed algorithm are presented some experiment based on the uci and microarray datasets are performed to demonstrate the effectiveness of the proposed algorithm 
this paper present a novel gaussian process gp approach to regression with input dependent noise rate we follow goldberg et al s approach and model the noise variance using a second gp in addition to the gp governing the noise free output value in contrast to goldberg et al however we do not use a markov chain monte carlo method to approximate the posterior noise variance but a most likely noise approach the resulting model is easy to implement and can directly be used in combination with various existing extension of the standard gps such a sparse approximation extensive experiment on both synthetic and real world data including a challenging perception problem in robotics show the effectiveness of most likely heteroscedastic gp regression 
the proliferation of malware ha presented a serious threat to the security of computer system traditional signature based antivirus system fail to detect polymorphic and new previously unseen malicious executables in this paper resting on the analysis of window api execution sequence called by pe le we develop the intelligent malware detection system imds using objectiveoriented association ooa mining based classication imds is an integrated system consisting of three major module pe parser ooa rule generator and rule based classier an ooa fast fpgrowth algorithm is adapted to efciently generate ooa rule for classication a comprehensive experimental study on a large collection of pe le obtained from the anti virus laboratory of kingsoft corporation is performed to compare various malware detection approach promising experimental result demonstrate that the accuracy and efciency of our imds system outperform popular anti virus software such a norton antivirus and mcafee virusscan a well a previous data mining based detection system which employed naive bayes support vector machine svm and decision tree technique 
hewlett packard ha many million of technical support document in a variety of collection a part of content management such collection are periodically merged and groomed in the process it becomes important to identify and weed out support document that are largely duplicate of newer version doing so improves the quality of the collection eliminates chaff from search result and improves customer satisfaction the technical challenge is that through workflow and human process the knowledge of which document are related is often lost we required a method that could identify similar document based on their content alone without relying on metadata which may be corrupt or missing we present an approach for finding similar file that scale up to large document repository it is based on chunking the byte stream to find unique signature that may be shared in multiple file an analysis of the file chunk graph yield cluster of related file an optional bipartite graph partitioning algorithm can be applied to greatly increase scalability 
online store providing subscription service need to extend user subscription period a long a possible to increase their profit conventional recommendation method recommend item that best coincide with user s interest to maximize the purchase probability which doe not necessarily contribute to extend subscription period we present a novel recommendation method for subscription service that maximizes the probability of the subscription period being extended our method find frequent purchase pattern in the long subscription period user and recommends item for a new user to simulate the found pattern using survival analysis technique we efficiently extract information from the log data for finding the pattern furthermore we infer user s interest from purchase history based on maximum entropy model and use the interest to improve the recommendation since a longer subscription period is the result of greater user satisfaction our method benefit user a well a online store we evaluate our method using the real log data of an online cartoon distribution service for cell phone in japan 
we present four new reinforcement learning algorithm based on actor critic and natural gradient idea and provide their convergence proof actor critic reinforcement learning method are online approximation to policy iteration in which the value function parameter are estimated using temporal difference learning and the policy parameter are updated by stochastic gradient descent method based on policy gradient in this way are of special interest because of their compatibility with function approximation method which are needed to handle large or innite state space the use of temporal difference learning in this way is of interest because in many application it dramatically reduces the variance of the gradient estimate the use of the natural gradient is of interest because it can produce better conditioned parameterizations and ha been shown to further reduce variance in some case our result extend prior two timescale convergence result for actor critic method by konda and tsitsiklis by using temporal difference learning in the actor and by incorporating natural gradient and they extend prior empirical study of natural actor critic method by peter vijayakumar and schaal by providing the rst convergence proof and the rst fully incremental algorithm 
we present two new algorithm for online learning in reproducing kernel hilbert space our first algorithm ilk implicit online learning with kernel employ a new implicit update technique that can be applied to a wide variety of convex loss function we then introduce a bounded memory version silk sparse ilk that maintains a compact representation of the predictor without compromising solution quality even in non stationary environment we prove loss bound and analyze the convergence rate of both experimental evidence show that our proposed algorithm outperform current method on synthetic and real data 
we extend radial basis function rbf network to the scenario in which multiple correlated task are learned simultaneously and present the corresponding learning algorithm we develop the algorithm for learning the network structure in either a supervised or unsupervised manner training data may also be actively selected to improve the network s generalization to test data experimental result based on rea l data demonstrate the advantage of the proposed algorithm and support our conclusion 
it ha been shown that adapting a dictionary of basis function to the statistic of natural image so a to maximize sparsity in the coefficien t result in a set of dictionary element whose spatial property resemble those of v primary visual cortex receptive field however the resulting spar e coefficient still exhibit pronounced statistical dependency thus violating the i ndependence assumption of the sparse coding model here we propose a model that attempt to capture the dependency among the basis function coefficient by in cluding a pairwise coupling term in the prior over the coefficient activity stat e when adapted to the statistic of natural image the coupling term learn a com bination of facilitatory and inhibitory interaction among neighboring basis function these learned interaction may offer an explanation for the function of horizontal connection in v in term of a prior over natural image 
while scalable data mining method are expected to cope with massive web data coping with evolving trend in noisy data in a continuous fashion and without any unnecessary stoppage and reconfigurations is still an open challenge this dynamic and single pas setting can be cast within the framework of mining evolving data stream in this paper we explore the task of mining mass user profile by discovering evolving web session cluster in a single pas with a recently proposed scalable immune based clustering approach tecno stream and study the effect of the choice of different similarity measure on the mining process and on the interpretation of the mined pattern we propose a simple similarity measure that ha the advantage of explicitly coupling the precision and coverage criterion to the early learning stage and furthermore requiring that the affinity of the data to the learned profile or summary be defined by the minimum of their coverage or precision hence requiring that the learned profile are simultaneously precise and complete with no compromise in our experiment we study the task of mining evolving user profile from web clickstream data web usage mining in a single pas and under different trend sequencing scenario showing that compared oto the cosine similarity measure the proposed similarity measure explicitly based on precision and coverage allows the discovery of more correct profile at the same precision or recall quality level 
we propose a mean field approximation that dramatically reduces the computational complexity of solving stochastic dynamic game we provide condition that guarantee our method approximates an equilibrium a the number of agent grow we then derive a performance bound to ass how well the approximation performs for any given number of agent we apply our method to an important class of problem in applied microeconomics we show with numerical experiment that we are able to greatly expand the set of economic problem that can be analyzed computationally 
the two third power law an empirical law stating an inverse non linear relationship between the tangential hand speed and the curvature of it trajectory during curved motion is widely acknowledged to be an invariant of upper limb movement it ha also been shown to exist in eyemotion locomotion and wa even demonstrated in motion perception and prediction this ubiquity ha fostered various attempt to uncover the origin of this empirical relationship in these it wa generally attributed either to smoothness in handor joint space or to the result of mechanism that damp noise inherent in the motor system to produce the smooth trajectory evident in healthy human motion we show here that white gaussian noise also obeys this power law analysis of signal and noise combination show that trajectory that were synthetically created not to comply with the power law are transformed to power law compliant one after combination with low level of noise furthermore there exist colored noise type that drive non power law trajectory to power law compliance and are not affected by smoothing these result suggest caution when running experiment aimed at verifying the power law or assuming it underlying existence without proper analysis of the noise our result could also suggest that the power law might be derived not from smoothness or smoothness inducing mechanism operating on the noise inherent in our motor system but rather from the correlated noise which is inherent in this motor system 
in this paper we propose two new support vector approach for ordinal regression which optimize multiple threshold to define parallel discriminant hyperplanes for the ordinal scale both approach guarantee that the threshold are properly ordered at the optimal solution the size of these optimization problem is linear in the number of training sample the smo algorithm is adapted for the resulting optimization problem it is extremely easy to implement and scale efficiently a a quadratic function of the number of example the result of numerical experiment on benchmark datasets verify the usefulness of these approach 
mixture model form one of the most widely used class of generative model for describing structured and clustered data in this paper we develop a new approach for the analysis of hierarchical mixture model more specifically using a text clustering problem a a motivation we describe a natural generative process that creates a hierarchical mixture model for the data in this process an adversary start with an arbitrary base distribution and then build a topic hierarchy via some evolutionary process where he control the parameter of the process we prove that under our assumption given a subset of topic that represent generalization of one another such a baseball sport base for any document which wa produced via some topic in this hierarchy we can efficiently determine the most specialized topic in this subset it still belongs to the quality of the classification is independent of the total number of topic in the hierarchy and our algorithm doe not need to know the total number of topic in advance our approach also yield an algorithm for clustering and unsupervised topical tree reconstruction we validate our model by showing that property predicted by our theoretical result carry over to real data we then apply our clustering algorithm to two different datasets i newsgroups and ii a snapshot of abstract of arxiv category abstract in both case our algorithm performs extremely well 
we describe an efficient learning procedure for multilayer g enerative model that combine the best aspect of markov random field and deep dir ected belief net the generative model can be learned one layer at a time and when learning is complete they have a very fast inference procedure for computing a good approximation to the posterior distribution in all of the hidden la yers each hidden layer ha it own mrf whose energy function is modulated by the top down directed connection from the layer above to generate from the model each layer in turn must settle to equilibrium given it top down input we show that this type of model is good at capturing the statistic of patch of natur al image 
traditional decomposition based solution to support vector machine svms suffer from the widely known scalability problem for example given a one million training set it take about six day for svmlight to run on a pentium sever with g byte memory in this paper we propose an incremental algorithm which performs approximate matrix factorization operation to speed up svms two approximate factorization scheme kronecker and incomplete cholesky are utilized in the primal dual interior point method ipm to directly solve the quadratic optimization problem in svms we found out that a coarse approximate algorithm enjoys good speedup performance but may suffer from poor training accuracy conversely a fine grained approximate algorithm enjoys good training quality but may suffer from long training time we subsequently propose an incremental training algorithm which us the approximate ipm solution of a coarse factorization to initialize the ipm of a fine grained factorization extensive empirical study show that our proposed incremental algorithm with approximate factorization substantially speed up svm training while maintaining high training accuracy in addition we show that our proposed algorithm is highly parallelizable on an intel dual coreprocessor 
in this paper we propose cccs a new algorithm for classification based on association rule mining the key innovation in cccs is the use of a new measure the complement class support cc whose application result in rule which are guaranteed to be positively correlated furthermore the anti monotonic property that cc posse ha very different semantics vi a vi the traditional support measure in particular good rule have a low cc value this make cc an ideal measure to use in conjunction with a top down algorithm finally the nature of cc allows the pruning of rule without the setting of any threshold parameter to the best of our knowledge this is the first threshold free algorithm in association rule mining for classification 
recently there ha been considerable interest in learning with higher order relation i e three way or higher in the unsupervised and semi supervised setting hypergraphs and tensor have been proposed a the natural way of representing these relation and their corresponding algebra a the natural tool for operating on them in this paper we argue that hypergraphs are not a natural representation for higher order relation indeed pairwise a well a higher order relation can be handled using graph we show that various formulation of the semi supervised and the unsupervised learning problem on hypergraphs result in the same graph theoretic problem and can be analyzed using existing tool 
we study the problem of simultaneously estimating several density where the datasets are organized into overlapping group such a a hierarchy for this problem we propose a maximum entropy formulation which systematically incorporates the group and allows u to share the strength of prediction across similar datasets we derive general performance guarantee and show how some previous approach such a hierarchical shrinkage and hierarchical prior can be derived a special case we demonstrate the proposed technique on synthetic data and in a real world application to modeling the geographic distribution of specie hierarchically grouped in a taxonomy specifically we model the geographic distribution of specie in the australian wet tropic and northeast new south wale in these region small number of sample per specie significantly hinder effective prediction substantial benefit are obtained by combining information across taxonomic group 
the who collaborating centre for international drug monitoring in uppsala sweden maintains and analysis the world s largest database of report on suspected adverse drug reaction incident that occur after drug are introduced on the market a in other post marketing drug safety data set the presence of duplicate record is an important data quality problem and the detection of duplicate in the who drug safety database remains a formidable challenge especially since the report are anonymised before submitted to the database however to our knowledge no work ha been published on method for duplicate detection in post marketing drug safety data in this paper we propose a method for probabilistic duplicate detection based on the hit miss model for statistical record linkage described by copas hilton we present two new generalisation of the standard hit miss model a hit miss mixture model for error in numerical record field and a new method to handle correlated record field we demonstrate the effectiveness of the hit miss model for duplicate detection in the who drug safety database both at identifying the most likely duplicate for a given record accuracy and at discriminating duplicate from random match recall with precision the proposed method allows for more efficient data cleaning in post marketing drug safety data set and perhaps other application throughout the kdd community 
we present the first large scale empirical application of reinforcement learning to the important problem of optimized trade execution in modern financial market our experiment are based on year of millisecond time scale limit order data from nasdaq and demonstrate the promise of reinforcement learning method to market microstructure problem our learning algorithm introduces and exploit a natural low impact factorization of the state space 
there ha been much work on applying multiple instance mi learning to content based image retrieval cbir where the goal is to rank all image in a known repository using a small labeled data set most existing mi learning algorithm are non transductive in that the image in the repository serve only a test data and are not used in the learning process we present missl multiple instance semi supervised learning that transforms any mi problem into an input for a graph based single instance semi supervised learning method that encodes the mi aspect of the problem simultaneously working at both the bag and point level unlike most prior mi learning algorithm missl make use of the unlabeled data 
we introduce a new bayesian model for hierarchical clustering based on a prior over tree called kingman s coalescent we develop novel greedy and sequential monte carlo inference which operate in a bottom up agglomerative fashion we show experimentally the superiority of our algorithm over the state of the art and demonstrate our approach in document clustering and phylolinguistics 
the perceptron algorithm despite it simplicity often pe rforms well in online classification task the perceptron becomes especi ally effective when it is used in conjunction with kernel however a common difficulty encountered when implementing kernel based online algorithm is the amount of memory required to store the online hypothesis which may grow unboundedly in this paper we present and analyze the forgetron algorithm for kernel based online learning on a fixed memory budget to our knowledge this is the first online learning al gorithm which on one hand maintains a strict limit on the number of example it store and on the other hand entertains a relative m istake bound in addition to the formal result we also present experiment with real datasets which underscore the merit of our approach 
in many text classification application it is appealing to take every document a a string of character rather than a bag of word previous research study in this area mostly focused on different variant of generative markov chain model although discriminative machine learning method like support vector machine svm have been quite successful in text classification with word feature it is neither effective nor efficient to apply them straightforwardly taking all substring in the corpus a feature in this paper we propose to partition all substring into statistical equivalence group and then pick those group which are important in the statistical sense a feature named key substring group feature for text classification in particular we propose a suffix tree based algorithm that can extract such feature in linear time with respect to the total number of character in the corpus our experiment on english chinese and greek datasets show that svm with key substring group feature can achieve outstanding performance for various text classification task 
web server on the internet need to maintain high reliability but the cause of intermittent failure of web transaction is non obvious we use approximate bayesian inference to diagnose problem with web service this diagnosis problem is far larger than any previously attempted it requires inference of possible fault from observation further such inference must be performed in le than a second inference can be done at this speed by combining a mean field variational approximation and the use of stochastic gradient descent to optimize a variational cost function we use this fast inference to diagnose a time series of anomalous http request taken from a real web service the inference is fast enough to analyze network log with billion of entry in a matter of hour 
this paper develops a multi frame image super resolution approach from a bayesian view point by marginalizing over the unknown registration parameter relating the set of input low resolution view in tipping a nd bishop s bayesian image super resolution approach the marginalization wa over the superresolution image necessitating the use of an unfavorable image prior by integrating over the registration parameter rather than the hi gh resolution image our method allows for more realistic prior distribution and a lso reduces the dimension of the integral considerably removing the main computational bottleneck of the other algorithm in addition to the motion model used by tipping and bishop illumination component are introduced into the generative model allowing u to handle change in lighting a well a motion we show result on real and synthetic datasets to illustrate the efficacy of this approa ch 
in this paper we review the paradigm of inductive process modeling which us background knowledge about possible component process to construct quantitative model of dynamical system we note that previous method for this task tend to overfit the training data which suggests ensemble learning a a likely response however such technique combine model in way that reduce comprehensibility making their output much le accessible to domain scientist a an alternative we introduce a new approach that induces a set of process model from different sample of the training data and us them to guide a final search through the space of model structure experiment with synthetic and natural data suggest this method reduces error and decrease the chance of including unnecessary process in the model we conclude by discussing related work and suggesting direction for additional research 
neighbor search is a fundamental task in machine learning especially in classification and retrieval efficient nearest neighbor search method have been widely studied with their emphasis on data structure but most of them did not consider the underlying global geometry of a data set recent graph based semi supervised learning method capture the global geometry but suffer from scalability and parameter tuning problem in this paper we present a nearest neighbor search method where the underlying global geometry is incorporated and the parameter tuning is not required to this end we introduce deterministic walk a a deterministic counterpart of markov random walk leading u to use the minimax distance a a global dissimilarity measure then we develop a message passing algorithm for efficient minimax distance calculation which scale linearly in both time and space empirical study reveals the useful behavior of the method in image retrieval and semi supervised learning 
this paper present an approach to build sparse large margin classifier slmc by adding one more constraint to the standard support vector machine svm training problem the added constraint explicitly control the sparseness of the classifier and an approach is provided to solve the formulated problem when considering the dual of this problem it can be seen that building an slmc is equivalent to constructing an svm with a modified kernel function further analysis of this kernel function indicates that the proposed approach essentially find a discriminating subspace that can be spanned by a small number of vector and in this subspace different class of data are linearly well separated experimental result over several classification benchmark show that in most case the proposed approach outperforms the state of art sparse learning algorithm 
kernel function have become an extremely popular tool in machine learning with an attractive theory a well this theory view a kernel a implicitly mapping data point into a possibly very high dimensional space and describes a kernel function a being good for a given learning problem if data is separable by a large margin in that implicit space however while quite elegant this theory doe not directly correspond to one s intuition of a good kernel a a good similarity function furthermore it may be difficult for a domain expert to use the theory to help design an appropriate kernel for the learning task at hand since the implicit mapping may not be easy to calculate finally the requirement of positive semi definiteness may rule out the most natural pairwise similarity function for the given problem domain in this work we develop an alternative more general theory of learning with similarity function i e sufficient condition for a similarity function to allow one to learn well that doe not require reference to implicit space and doe not require the function to be positive semi definite or even symmetric our result also generalize the standard theory in the sense that any good kernel function under the usual definition can be shown to also be a good similarity function under our definition though with some loss in the parameter in this way we provide the first step towards a theory of kernel that describes the effectiveness of a given kernel function in term of natural similarity based property 
in this paper we present a novel algorithm carpediem it significantly improves on the time complexity of viterbi algorithm preserving the optimality of the result this fact ha consequence on machine learning system that use viterbi algorithm during learning or classification we show how the algorithm applies to the supervised sequential learning task and in particular to the hmperceptron algorithm we illustrate carpediem in full detail and provide experimental result that support the proposed approach 
study have shown that program comprehension take up to of software development cost such high cost are caused by the lack of documented specification and further aggravated by the phenomenon of software evolution there is a need for automated tool to extract specification to aid program comprehension in this paper a novel technique to efficiently mine common software temporal pattern from trace is proposed these pattern shed light on program behavior and are termed iterative pattern they capture unique characteristic of software trace typically not found in arbitrary sequence specifically due to loop interesting iterative pattern can occur multiple time within a trace furthermore an occurrence of an iterative pattern in a trace can extend across a sequence of indefinite length since a program behavior can be manifested in numerous way analyzing a single trace will not be sufficient iterative pattern mining extends sequential pattern and episode mining to discover frequent iterative pattern which occur repetitively both within a program trace and across multiple trace in this paper we present cliper closed iterative pattern miner to efficiently mine a closed set of iterative pattern a performance study on several simulated and real datasets show the efficiency of our mining algorithm and effectiveness of our pruning strategy our case study on jboss application server confirms the usefulness of mined pattern in discovering interesting software behavioral specification 
in many application attribute and relationship data areavailable carrying complementary information about real world entity in such case a joint analysis of both type of data can yield more accurate result than classical clustering algorithm that either use only attribute data or only relationship graph data the connected k center ckc ha been proposed a the first joint cluster analysis model to discover k cluster which are cohesive on both attribute and relationship data however it is well known that prior knowledge on the number of cluster is often unavailable in application such a community dentification and hotspot analysis in this paper we introduce and formalize the problem of discovering an a priori unspecified number of cluster in the context of joint cluster analysis of attribute and relationship data called connected x cluster cxc problem true cluster are assumed to be compact and distinctive from their neighboring cluster in term of attribute data and internally connected in term of relationship data different from classical attribute based clustering method the neighborhood of cluster is not defined in term of attribute data but in term of relationship data to efficiently solve the cxc problem we present jointclust an algorithm which adopts a dynamic two phase approach in the first phase we find so called cluster atom we provide a probability analysis for thisphase which give u a probabilistic guarantee that each true cluster is represented by at least one of the initial cluster atom in the second phase these cluster atom are merged in a bottom up manner resulting in a dendrogram the final clustering is determined by our objective function our experimental evaluation on several real datasets demonstrates that jointclust indeed discovers meaningful and accurate clustering without requiring the user to specify the number of cluster 
although non parametric test have already been proposed for that purpose statistical significance test for non standard meas ures different from the classification error are le often used in the lite rature this paper is an attempt at empirically verifying how these test compare with more classical test on various condition more precisel y using a very large dataset to estimate the whole population we analyzed the behavior of several statistical test varying the class unbalanc e the compared model the performance measure and the sample size the main result is that providing big enough evaluation set non parametric test are relatively reliable in all condition 
we present a globally convergent method for regularized risk minimization problem our method applies to support vector estimation regression gaussian process and any other regularized risk minimization setting which lead to a convex optimization problem svmperf can be shown to be a special case of our approach in addition to the unified framework we present tight convergence bound which show that our algorithm converges in o step to precision for general convex problem and in o log step for continuously differentiable problem we demonstrate in experiment the performance of our approach 
computational gene prediction using generative model ha reached a plateau with several group converging to a generalized hidden markov model ghmm incorporating phylogenetic model of nucleotide sequence evolution further improvement in gene calling accuracy are likely to come through new method that incorporate additional data both comparative and specie specific conditional random field crfs which directly model the conditional probability p y x of a vector of hidden state conditioned on a set of observation provide a unified framework for combining probabilistic and non probabilistic information and have been shown to outperform hmms on sequence labeling task in natural language processing we describe the use of crfs for comparative gene prediction we implement a model that encapsulates both a phylogenetic ghmm our baseline comparative model and additional non probabilistic feature we tested our model on the genome sequence of the fungal human pathogen cryptococcus neoformans our baseline comparative model display accuracy comparable to the the best available gene prediction tool for this organism moreover we show that discriminative training and the incorporation of non probabilistic evidence significantly improve performance our software implementation conrad is freely available with an open source license at http www broad mit edu annotation conrad 
the conditional distribution of a discrete variable y given another discrete variable x is often specified by assigning one multinomial distribution to each state of x the cost of this rich parametrization is the loss of statistical power in case where the data actually fit a model with much fewer parameter in this paper we consider a model that partition the state space of x into disjoint set and assigns a single dirichlet multinomial to each set we treat the partition a an unknown variable which is to be integrated away when the interest is in a coarser level task e g variable selection or classification based on two different computational approach we present two exact algorithm for integration over partition respective complexity bound are derived in term of detailed problem characteristic including the size of the data and the size of the state space of x experiment on synthetic data demonstrate the applicability of the algorithm 
cluster analysis of ranking data which occurs in consumer questionnaire voting form or other inquiry of preference attempt to identify typical group of rank choice empirically measured ranking are often incomplete i e different number of filled rank position cause heterogeneity in the data we propose a mixture approach for clustering of heterogeneous rank data ranking of different length can be described and compared by mean of a single probabilistic model a maximum entropy approach avoids hidden assumption about missing rank position parameter estimator and an efficient em algorithm for unsupervised inference are derived for the ranking mixture model experiment on both synthetic data and real world data demonstrate significantly improved parameter estimate on heterogeneous data when the incomplete ranking are included in the inference process 
multinomial logistic regression provides the standard pen alised maximumlikelihood solution to multi class pattern recognition pr oblems more recently the development of sparse multinomial logistic regression model ha found application in text processing and microarray classification where explicit identification of the most informative feature is of value in this p aper we propose a sparse multinomial logistic regression method in which the sparsity arises from the use of a laplace prior but where the usual regularisatio n parameter is integrated out analytically evaluation over a range of benchmark datasets reveals this approach result in similar generalisation performan ce to that obtained using cross validation but at greatly reduced computational ex pense 
a novel approach to measure the interdependence of two time series is proposed referred to a stochastic event synchrony s it quantifi e the alignment of two point process by mean of the following parameter time delay variance of the timing jitter fraction of spurious event and average similarity of event s may be applied to generic one dimensional and multi dimensional point process however the paper mainly focus on point process in time frequency domain the average event similarity is in that case described by two parameter the average frequency offset between event in the time frequency plane and the variance of the frequency offset frequency jitter s then consists of five parameter i n total those parameter quantify the synchrony of oscillatory event and hence they provide an alternative to existing synchrony measure that quantify amplitude or phase synchrony the pairwise alignment of point process is cast a a statistical in ference problem which is solved by applying the max product algorithm on a graphical model the s parameter are determined from the resulting pairwise alignment by maximum a posteriori map estimation the proposed interdependence measure is applied to the problem of detecting anomaly in eeg synchrony of mild cognitive impairment mci patient the result indicate that s significantly improve s the sensitivity of eeg in detecting mci 
quadratic program relaxation are proposed a an alternative to linear program relaxation and tree reweighted belief propagation for the metric labeling or map estimation problem an additional convex relaxation of the quadratic approximation is shown to have additive approximation guarantee that apply even when the graph weight have mixed sign or do not come from a metric the approximation are extended in a manner that allows tight variational relaxation of the map problem although they generally involve non convex optimization experiment carried out on synthetic data show that the quadratic approximation can be more accurate and computationally efficient than the linear programming and propagation based alternative 
we provide a general framework for learning precise compact and fast representation of the bayesian predictive distribution for a model this framework is based on minimizing the kl divergence between the true predictive density and a suitable compact approximation we consider various method for doing this both sampling based approximation and deterministic approximation such a expectation propagation these method are tested on a mixture of gaussians model for density estimation and on binary linear classification with both synthetic data set for visualization and several real data set our result show significant reduction in prediction time and memory footprint 
this paper deal with detecting change of distribution in multi dimensional data set for a given baseline data set and a set of newly observed data point we define a statistical test called the density test for deciding if the observed data point are sampled from the underlying distribution that produced the baseline data set we define a test statistic that is strictly distribution free under the null hypothesis our experimental result show that the density test ha substantially more power than the two existing method for multi dimensional change detection 
we consider a model in which background knowledge on a given domain of interest is available in term of a bayesian network in addition to a large database the mining problem is to discover unexpected pattern our goal is to find the strongest discrepancy between network and database this problem is intrinsically difficult because it requires inference in a bayesian network and processing the entire potentially very large database a sampling based method that we introduce is efficient and yet provably find the approximately most interesting unexpected pattern we give a rigorous proof of the method s correctness experiment shed light on it efficiency and practicality for large scale bayesian network and database 
an important problem in many field is the analysis of count d ata to extract meaningful latent component method like probabilistic latent semantic analysis plsa and latent dirichlet allocation lda have been proposed for this purpose however they are limited in the number of component they can extract and lack an explicit provision to control the expressiveness of the extracted component in this paper we present a learning formulation to ad dress these limitation by employing the notion of sparsity we start with the plsa framework and use an entropic prior in a maximum a posteriori formulation to enforce sparsity we show that this allows the extraction of overcomplete set of latent component which better characterize the data we present experimental evidence of the utility of such representation 
selective attention is the strategy used by biological sensory system to solve the problem of limited parallel processing capacity salient subregions of the input stimulus are serially processed while non salient region are suppressed we present an mixed mode analog digital very large scale integration implementation of a building block for a multi chip neuromorphic hardware model of selective attention we describe the chip s architecture and it behavior when it is part of a multi chip system with a spiking retina a input and show how it can be used to implement in real time flexible model of bottom up attention 
we propose a bayesian network classifier with inverse tree structure bncit for joint classification and variable selection the problem domain of voxelwise magnetic resonance image analysis often involves million of variable but only dozen of sample judicious variable selection may render classification tractable avoid over fitting and improve classifier performance bncit embeds the variable selection process within the classifier training process which make this algorithm scalable bncit is based on a bayesian network model with inverse tree structure i e the class variable c is a leaf node and predictive variable are parent of c thus the classifier training process return a parent set for c which is a subset of the markov blanket of c bncit us voxels in the parent set and voxels that are probabilistically equivalent to them a variable for classification of new image data since the data set ha a limited number of sample we use the jackknife method to determine whether the classifier generated by bncit is a statistical artifact in order to enhance stability and improve classification accuracy we model the state of the probabilistically equivalent voxels with a latent variable we employ an efficient method for determining state of hidden variable thus reducing dramatically the computational cost of model generation experimental result confirm the accuracy and efficiency of bncit 
most of text categorization technique are based on word and or phrase analysis of the text statistical analysis of a term frequency capture the importance of the term within a document only however two term can have the same frequency in their document but one term contributes moreto the meaning of it sentence than the other term thus the underlying model should indicate term that capture these mantics of text in this case the model can capture term that present the concept of the sentence which lead todiscover the topic of the document a new concept based model that analyzes term on the sentence and document level rather than the traditional analysis of document only is introduced the concept based model can effectively discriminate between non important term with respect to sentence semantics and term which hold the concept that represent the sentence meaning the proposed model consists of concept based statistical analyzer conceptual ontological graph representation and concept extractor the term which contributes to the sentence semantics is assigned two different weight by the concept based statistical analyzer and the conceptual ontological graph representation these two weight are combined into a new weight the concept that have maximum combined weight are selected by the concept extractor a set of experiment using the proposed concept basedmodel on different datasets in text categorization is conducted the experiment demonstrate the comparison between traditional weighting and the concept based weighting obtained by the combined approach of the concept based statistical analyzer and the conceptual ontological graph the evaluation of result is relied on two quality measure the macro averaged f and the error rate these quality measure are improved when the newly developedconcept based model is used to enhance the quality of thetext categorization 
p we show that under suitable assumption primarily linearization a simple and perspicuous online learning rule for information bottleneck optimization with spiking neuron can be derived this rule performs on common benchmark task a well a a rather complex rule that ha previously been proposed furthermore the transparency of this new learning rule make a theoretical analysis of it convergence property feasible if this learning rule is applied to an assemble of neuron it provides a theoretically founded method for performing principal component analysis pca with spiking neuron in addition it make it possible to preferentially extract those principal component from incoming signal x that are related to some additional target signal img width height border align middle alt y t src http www igi tugraz at abstract buesingmaass img png this target signal img width height border align middle alt y t src http www igi tugraz at abstract buesingmaass img png also called relevance variable could represent in a biological interpretation proprioception feedback input from other sensory modality or top down signal p 
i consider the setting of transductive learning of vertex label in graph in which a graph with n vertex is sampled according to some unknown distribution there is a true labeling of the vertex such that each vertex is assigned to exactly one of k class but the label of only some random subset of the vertex are revealed to the learner the task is then to find a labeling of the remaining unlabeled vertex that agrees a much a possible with the true labeling several existing algorithm are based on the assumption that adjacent vertex are usually labeled the same in order to better understand algorithm based on this assumption i derive data dependent bound on the fraction of mislabeled vertex based on the number or total weight of edge between vertex differing in predicted label i e the size of the cut 
this contribution develops a theoretical framework that take into account the effect of approximate optimization on learning algorithm the analysis show distinct tradeoff for the case of small scale and large sc ale learning problem small scale learning problem are subject to the usual approximation estimation tradeoff large scale learning problem are subject to a qualitatively different tradeoff involving the computational complexity of the underlying optimization algorithm in non trivial way 
we extend the pac bayes theorem to the sample compression setting where each classifier is represented by two independent source of information a compression set which consists of a small subset of the training data and a message string of the additional information needed to obtain a classifier the new bound is obtained by using a prior over a data independent set of object where each object give a classifier only when the training data is provided the new pac bayes theorem state that a gibbs classifier defined on a posterior over sample compressed classifier can have a smaller risk bound than any such deterministic sample compressed classifier 
we propose a new bayesian method for spatial cluster detection the bayesian spatial scan statistic and compare this method to the standard frequentist scan statistic approach we demonstrate tha t the bayesian statistic ha several advantage over the frequentist appr oach including increased power to detect cluster and since randomization testing is unnecessary much faster runtime we evaluate the bayesian and frequentist method on the task of prospective disease surveillance detecting spatial cluster of disease case resulting from emergi ng disease outbreak we demonstrate that our bayesian method are successful in rapidly detecting outbreak while keeping number of false positive low 
pyramid intersection is an efficient method for computing an approximate partial matching between two set of feature vector we introduce a novel pyramid embedding based on a hierarchy of non uniformly shaped bin that take advantage of the underlying structure of the feature space and remains accurate even for set with high dimensional feature vector the matching similarity is computed in linear time and form a mercer kernel we also show how the matching itself a correspondence field may be extracted for a small increase in computational cost whereas previous matching approximation algorithm suffer from distortion factor that increase linearly with the feature dimension we demonstrate that our approach can maintain constant accuracy even a the feature dimension increase when used a a kernel in a discriminative classifier our approach achieves improved object recognition result over a state of the art set kernel 
we propose a dynamic model for forecasting price in online auction one of the key feature of our model is that it operates during the live auction which make it different from previous approach that only consider static model our model is also different with respect to how information about price is incorporated while one part of the model is based on the more traditional notion of an auction s price level another part incorporates it dynamic in the form of a price s velocity and acceleration in that sense it incorporates key feature of a dynamic environment such a an online auction the use of novel functional data methodology allows u to measure and subsequently include dynamic price characteristic we illustrate our model on a diverse set of ebay auction across many different book category we find significantly higher prediction accuracy compared to standard approach 
female cricket can locate male by phonotaxis to the mating song they produce the behaviour and underlying physiology ha been studied in some depth showing that the cricket auditory system solves this complex problem in a unique manner we present an analogue very large scale integrated avlsi circuit model of this process and show that result from testing the circuit agree with simulation and what is known from the behaviour and physiology of the cricket auditory system the avlsi circuitry is now being extended to use on a robot along with previously modelled neural circuitry to better understand the complete sensorimotor pathway 
a central question in game theory learning and other field is how a rational intelligent agent should behave in a complex environment given that it cannot perform unbounded computation we study strategic aspect of this question by formulating a simple model of a game with additional cost computational or otherwise for each strategy while a zero sum game with strategy cost is no longer zerosum we show that it nash equilibrium have an interesting structure and the game ha a new type of value we also show that potential game with strategy cost remain potential game both zero sum and potential game with strategy cost maintain a very appealing property simple learning dynamic converge to nash equilibrium 
we define a probability distribution over equivalence class of binary matrix with a finite number of row and an unbounded number of column this distribution is suitable for use a a prior in probabilistic model that represent object using a potentially infinite array of feature we derive the distribution by taking the limit of a distribution over n k binary matrix a k a strategy inspired by the derivation of the chinese restaurant process aldous pitman a the limit of a dirichlet multinomial model this strategy preserve the exchangeability of the row of matrix we define several simple generative process that result in the same distribution over equivalence class of binary matrix one of which we call the indian buffet process we illustrate the use of this distribution a a prior in an infinite latent feature model deriving a markov chain monte carlo algorithm for inference in this model and applying this algorithm to an artificial dataset 
we consider the problem of modeling a helicopter s dynamic based on state action trajectory collected from it the contribu tion of this paper is two fold first we consider the linear model such a learned by cifer the industry standard in helicopter identification and s how that the linear parameterization make certain property of dynamical system such a inertia fundamentally difficult to capture w e propose an alternative acceleration based parameterization that d oes not suffer from this deficiency and that can be learned a efficiently from da ta second a markov decision process model of a helicopter s dynamic wo uld explicitly model only the one step transition but we are often in terested in a model s predictive performance over longer timescales in this paper we present an efficient algorithm for approximately minimiz ing the prediction error over long time scale we present empirical result on two different helicopter although this work wa motivated by the problem of modeling helicopter the idea presented here are general and can be applied to modeling large class of vehicular dynamic 
markov network are widely used in a wide variety of application in problem ranging from computer vision to natural language to computational biology in most current application even those that rely heavily on l earned model the structure of the markov network is constructed by hand due to the lack of effective algorithm for learning markov network structure from data in this paper we provide a computationally effective method for learning markov network structure from data our method is based on the use of l regularization on the weight of the log linear model which ha the effect of biasing the model towards solution where many of the parameter are zero this formulation convert the markov network learning problem into a convex optimization problem in a continuous space which can be solved using efficient gradient method a key issue in this setting is the unavoidable use of approximate inference which can lead to error in the gradient computation when the network structure is dense thus we explore the use of different feature introduction scheme a nd compare their performance we provide result for our method on synthetic data and on two real world data set modeling the joint distribution of pixel va lues in the mnist data and modeling the joint distribution of genetic sequence variation in the human hapmap data we show that our l based method achieves considerably higher generalization performance than the more standard l based method a gaussian parameter prior or pure maximum likelihood learning we also show that we can learn mrf network structure at a computational cost that is not much greater than learning parameter alone demonstrating the existence of a feasible method for this important problem 
naive bayes is an effective and efficient learning algorithm in classification in many application however an accurate ranking of instance based on the class probability is more desirable unfortunately naive bayes ha been found to produce poor probability estimate numerous technique have been proposed to extend naive bayes for better classification accuracy of which selective bayesian classifier sbc langley sage tree augmented naive bayes tan friedman et al nbtree kohavi boosted naive bayes elkan and aode webb et al achieve remarkable improvement over naive bayes in term of classification accuracy an interesting question is do these technique also produce accurate ranking in this paper we first conduct a systematic experimental study on their efficacy for ranking then we propose a new approach to augmenting naive bayes for generating accurate ranking called hidden naive bayes hnb in an hnb a hidden parent is created for each attribute to represent the influence from all other attribute and thus a more accurate ranking is expected hnb inherits the structural simplicity of naive bayes and can be easily learned without structure learning our experiment show that hnb outperforms naive bayes sbc boosted naive bayes nbtree and tan significantly and performs slightly better than aode in ranking 
finding pattern of social interaction within a population ha wide ranging application including disease modeling cultural and information transmission and behavioral ecology social interaction are often modeled with network a key characteristic of social interaction is their continual change however most past analysis of social network are essentially static in that all information about the time that social interaction take place is discarded in this paper we propose a new mathematical and computational framework that enables analysis of dynamic social network and that explicitly make use of information about when social interaction occur 
adaboost oc ha shown to be an effective method in boosting weak binary classifier for multi class learning it employ the error correcting output code ecoc method to convert a multi class learning problem into a set of binary classification problem and applies the adaboost algorithm to solve them efficiently in this paper we propose a new boosting algorithm that improves the adaboost oc algorithm in two aspect it introduces a smoothing mechanism into the boosting algorithm to alleviate the potential overfitting problem with the adaboost algorithm and it introduces a probabilistic coding scheme to generate binary code for multiple class such that training error can be efficiently reduced empirical study with seven uci datasets have indicated that the proposed boosting algorithm is more robust and effective than the adaboost oc algorithm for multi class learning 
active learning is the problem in supervised learning to design the location of training input point so that the generalization er ror is minimized existing active learning method often assume that the model used for learning is correctly specified i e the learning target f unction can be expressed by the model at hand in many practical situation however this assumption may not be fulfilled in this paper we first show th at the existing active learning method can be theoretically justifie d under slightly weaker condition the model doe not have to be correctly specified but slightly misspecified model are also allowed however it t urn out that the weakened condition is still restrictive in practice to cope with this problem we propose an alternative active learning method which can be theoretically justified for a wider class of misspecified mod el thus the proposed method ha a broader range of application than the existing method numerical study show that the proposed active learning method is robust against the misspecification of model and i s thus reliable 
we present an algorithm called optimistic linear programming olp for learning to optimize average reward in an irreducible but otherwise unknown markov decision process mdp olp us it experience so far to estimate the mdp it chooses action by optimistically maximizing estimated future reward over a set of next state transition probability that are close to the estimate a computation that corresponds to solving linear program we show that the total expected reward obtained by olp up to time t is within c p logt of the reward obtained by the optimal policy where c p is an explicit mdp dependent constant olp is closely related to an algorithm proposed by burnetas and katehakis with four key difference olp is simpler it doe not require knowledge of the support of transition probability the proof of the regret bound is simpler but our regret bound is a constant factor larger than the regret of their algorithm olp is also similar in flavor to an algorithm recently proposed by auer and ortner but olp is simpler and it regret bound ha a better dependence on the size of the mdp 
mining data stream of changing class distribution is important for real time business decision support the stream classifier must evolve to reflect the current class distribution this pose a serious challenge on the one hand relying on historical data may increase the chance of learning obsolete model on the other hand learning only from the latest data may lead to biased classifier a the latest data is often an unrepresentative sample of the current class distribution the problem is particularly acute in classifying rare event when for example instance of the rare class do not even show up in the most recent training data in this paper we use a stochastic model to describe the concept shifting pattern and formulate this problem a an optimization one from the historical and the current training data that we have observed find the most likely current distribution and learn a classifier based on the most likely distribution we derive an analytic solution and approximate this solution with an efficient algorithm which calibrates the influence of historical data carefully to create an accurate classifier we evaluate our algorithm with both synthetic and real world datasets our result show that our algorithm produce accurate and efficient classification 
motivated by numerous application in which the data may be modeled by a variable subscripted by three or more index we develop a tensor based extension of the matrix cur decomposition the tensor cur decomposition is most relevant a a data analysis tool when the data consist of one mode that is qualitatively different than the others in this case the tensor cur decomposition approximately express the original data tensor in term of a basis consisting of underlying subtensors that are actual data element and thus that have natural interpretation in term ofthe process generating the data in order to demonstrate the general applicability of this tensor decomposition we apply it to problem in two diverse domain of data analysis hyperspectral medical image analysis and consumer recommendation system analysis in the hyperspectral data application the tensor cur decomposition is used to compress the data and we show that classification quality is not substantially reduced even after substantial data compression in the recommendation system application the tensor cur decomposition is used to reconstruct missing entry in a user product product preference tensor and we show that high quality recommendation can be made on the basis of a small number of basis user and a small number of product product comparison from a new user 
we propose a compact low power vlsi network of spiking neuron which can learn to classify complex pattern of mean firing rate on line and in real time the network of integrate and fire neuron is connected by bistable synapsis that can change their weight using a local spike based plasticity mechanism learning is supervised by a teacher which provides an extra input to the output neuron during training the synaptic weight are updated only if the current generated by the plastic synapsis doe not match the output desired by the teacher a in the perceptron learning rule we present experimental result that demonstrate how this vlsi network is able to robustly classify uncorrelated linearly separable spatial pattern of mean firing rate 
we describe a novel method for learning template for recognition and localization of object drawn from category a generative model represents the configuration of multiple object part with respec t to an object coordinate system these part in turn generate image feature the complexity of the model in the number of feature is low meaning our model is much more efficient to train than comparative method mor eover a variational approximation is introduced that allows lear ning to be order of magnitude faster than previous approach while incorporating many more feature this result in both accuracy and localization improvement our model ha been carefully tested on standard datasets we compare with a number of recent template model in particular we demonstrate state of the art result for detection and lo calization 
dirichlet process dp mixture model are promising candidate for clustering application where the number of cluster is unknown a priori due to computational consideration these model are unfortunately unsuitable for large scale data mining application we propose a class of deterministic accelerated dp mixture model that can routinely handle million of data case the speedup is achieved by incorporating kd tree into a variational bayesian algorithm for dp mixture in the stick breaking representation similar to that of blei and jordan our algorithm differs in the use of kd tree and in the way we handle truncation we only assume that the variational distributi ons are fixed at their prior after a certain level experiment show that speedup relative to the standard variational algorithm can be significant 
in this paper we present our design and experiment of a planar biped robot runbot under pure reflexive neuronal control the goal of this study is to combine neuronal mechanism with biomechanics to obtain very fast speed and the on line learning of circuit parameter our controller is built with biologically inspired sensorand motor neuron model including local reflex and not employing any kind of position or trajectory tracking control algorithm instead this reflexive controller allows runbot to exploit it own natural dynamic during critical stage of it walking gait cycle to our knowledge this is the first time that dynamic biped walking is achieved using only a pure reflexive controller in addition this structure allows using a policy gradient reinforcement learning algorithm to tune the parameter of the reflexive controller in real time during walking this way runbot can reach a relative speed of leg length per second after a few minute of online learning which is faster than that of any other biped robot and is also comparable to the fastest relative speed of human walking in addition the stability domain of stable walking is quite large supporting this design strategy 
a a fundamental data mining task frequent pattern mining ha widespread application in many different domain research in frequent pattern mining ha so far mostly focused on developing efficient algorithm to discover various kind of frequent pattern but little attention ha been paid to the important nextstep interpreting the discovered frequent pattern although some recent work ha studied the compression and summarization of frequent pattern the proposed technique can only annotate a frequent pattern with non semantical information e g support which provides only limited help for a user to understand the pattern in this paper we propose the novel problem of generating semantic annotation for frequent pattern the goal is to annotate a frequent pattern with in depth concise and structured information that can better indicate the hidden meaning of the pattern we propose a general approach to generate such anannotation for a frequent pattern by constructing it context model selecting informative context indicator and extracting representative transaction and semantically similar pattern this general approach ha potentially many application such a generating a dictionary like description for a pattern finding synonym pattern discovering semantic relation and summarizing semantic class of a set of frequent pattern experiment on different datasets show that our approach is effective in generating semantic pattern annotation 
in this paper we discus the important practical problem of customer wallet estimation i e estimation of potential spending by customer rather than their expected spending for this purpose we utilize quantile modeling whose goal is to estimate a quantile of the discriminative conditional distribution of the response rather than the mean which is the implicit goal of most standard regression approach we argue that a notion of wallet can be captured through high quantile modeling e g estimating the th percentile and describe a wallet estimation implementation within ibm s market alignment program map we also discus the wide range of domain where high quantile modeling can be practically important estimating opportunity in sale and marketing domain defining surprising pattern for outlier and fraud detection and more we survey some existing approach for quantile modeling and propose adaptation of nearest neighbor and regression tree approach to quantile modeling we demonstrate the various model performance in high quantile estimation in several domain including our motivating problem of estimating the realistic it wallet of ibm customer 
we consider the problem of detecting human and classifying their pose from a single image specifically our goal is to devise a statistical model that simultaneously answer two question is there a human in the image and if so what is a low dimensional representation of her pose we investigate model that can be learned in an unsupervised manner on unlabeled image of human pose and provide information that can be used to match the pose of a new image to the one present in the training set starting from a set of descriptor recently proposed for human detection we apply the latent dirichlet allocation framework to model the statistic of these feature and use the resulting model to answer the above question we show how our model can efficiently describe the space of image of human with their pose by providing an effective representation of pose for task such a classification and matching while performing remarkably well in human non human decision problem thus enabling it use for human detection we validate the model with extensive quantitative experiment and comparison with other approach on human detection and pose matching 
we present a robust distributed algorithm for approximate probabilistic inference in dynamical system such a sensor network and team of mobile robot using assumed density filtering the network node maintain a tractable representation of the belief state in a distributed fashion at each time step the node coordinate to condition this distribution on the observation made throughout the network and to advance this estimate to the next time step in addition we identify a significant challenge for probabilistic inference in dynamical system message loss or network partition can cause node to have inconsistent belief about the current state of the system we address this problem by developing distributed algorithm that guarantee that node will reach an informative consistent distribution when communication is re established we present a suite of experimental result on real world sensor data for two real sensor network deployment one with camera and another with temperature sensor 
this work study the problem of distributed classification in peer to peer p p network while there ha been a significant amount of work in distributed classification most of existing algorithm are not designed for p p network indeed a server le and router le system p p network impose several challenge for distributed classification it is not practical to have global synchronization in large scale p p network there are frequent topology change caused by frequent failure and recovery of peer and there are frequent on the fly data update on each peer in this paper we propose an ensemble paradigm for distributed classification in p p network under this paradigm each peer build it local classifier on the local data and the result from all local classifier are then combined by plurality voting to build local classifier we adopt the learning algorithm of pasting bite to generate multiple local classifierson each peer based on the local data to combine local result we propose a general form of distributed plurality voting dpv protocol in dynamic p p network this protocol keep the single site validity for dynamic network and support the computing mode of both one shot query and continuous monitoring we theoretically prove that the condition bob check this c for sending message used in dpv is locally communication optimal to achieve the above property finally experimental result on real world p p network show that the proposed ensemble paradigm is effective even if there are thousand of local classifier in most case the dpv algorithm is local in the sense that voting is processed using information gathered from a very small vicinity whose size is independent of the network size dpv is significantly more communication efficient than existing algorithm for distributed plurality voting 
we extend position and phase shift tuning concept already well established in the disparity energy neuron literature to motion energy neuron we show that reichardt like detector can be considered example of position tuning and that motion energy filter whose complex valued spatio temporal receptive field are space time separable can be considered example of phase tuning by combining these two type of detector we obtain an architecture for constructing motion energy neuron whose center frequency can be adjusted by both phase and position shift similar to recently described neuron in the primary visual cortex these new motion energy neuron exhibit tuning that is between purely spacetime separable and purely speed tuned we propose a functional role for this intermediate level of tuning by demonstrating that comparison between pair of these motion energy neuron can reliably discriminate between input whose velocity lie above or below a given reference velocity 
we present a theoretical study on the discriminative clustering framework recently proposed for simultaneous subspace selection via linear discriminant analysis lda and clustering empirical result have shown it favorable performance in comparison with several other popular clustering algorithm however the inherent relationship between subspace selection and clustering in this framework is not well understood due to the iterative nature of the algorithm we show in this paper that this iterative subspace selection and clustering is equivalent to kernel k mean with a specific kernel gram matrix this provides significant and new insight into the nature of this subspace selection procedure based on this equivalence relationship we propose the discriminative k mean diskmeans algorithm for simultaneous lda subspace selection and clustering a well a an automatic parameter estimation procedure we also present the nonlinear extension of diskmeans using kernel we show that the learning of the kernel matrix over a convex set of pre specified kernel matrix can be incorporated into the clustering formulation the connection between diskmeans and several other clustering algorithm is also analyzed the presented theory and algorithm are evaluated through experiment on a collection of benchmark data set 
time series motif are approximately repeated pattern foundwithin the data such motif have utility for many data mining algorithm including rule discovery novelty detection summarization and clustering since the formalization of the problem and the introduction of efficient linear time algorithm motif discovery ha been successfully applied tomany domain including medicine motion capture robotics and meteorology in this work we show that most previous application of time series motif have been severely limited by the definition s brittleness to even slight change of uniform scaling the speed at which the pattern develop we introduce a new algorithm that allows discovery of time series motif with invariance to uniform scaling and show that it produce objectively superior result in several important domain apart from being more general than all other motifdiscovery algorithm a further contribution of our work isthat it is simpler than previous approach in particular we have drastically reduced the number of parameter that need to be specified 
the observed physiological dynamic of an infant receiving intensive care are affected by many possible factor including intervention to the baby the operation of the monitoring equipment and the state of health the factorial switching kalman filter can be used to infer the presence of such factor from a sequence of observation and to estimate the true value where these observation have been corrupted we apply this model to clinical time series data and show it to be effective in identifying a number of artifactual and physiological pattern 
a web object is defined to represent any meaningful object embedded in web page e g image music or pointed to by hyperlink e g downloadable file in many case user would like to search for information of a certain object rather than a web page containing the query term to facilitate web object searching and organizing in this paper we propose a novel approach to web object indexing by discovering it inherent structure information with existed domain knowledge in our approach first layered lsi space are built for a better representation of the hierarchically structured domain knowledge in order to emphasize the specific semantics and term space in each layer of the domain knowledge meanwhile the web object representation is constructed by hyperlink analysis and further pruned to remove the noise then an optimal matching between the web object and the domain knowledge is performed in order to pick out the structure attribute of the web object from the knowledge finally the obtained structure attribute are used to re organize and index the web object our approach also indicates a new promising way to use trust worthy deep web knowledge to help organize dispersive information of surface web 
in this paper we introduce and experiment with a framework for learning local perceptual distance function for visual recognition we l earn a distance function for each training image a a combination of elementary distance between patch based visual feature we apply these combined local distance function to the task of image retrieval and classification of novel imag e on the caltech object recognition benchmark we achieve mean recognition across class using training image per class which is better t han the best published performance by zhang et al 
clustering on multi type relational data ha attracted more and more attention in recent year due to it high impact on various important application such a web mining e commerce and bioinformatics however the research on general multi type relational data clustering is still limited and preliminary the contribution of the paper is three fold first we propose a general model the collective factorization on related matrix for multi type relational data clustering the model is applicable to relational data with various structure second under this model we derive a novel algorithm the spectral relational clustering to cluster multi type interrelated data object simultaneously the algorithm iteratively embeds each type of data object into low dimensional space and benefit from the interaction among the hidden structure of different type of data object extensive experiment demonstrate the promise and effectiveness of the proposed algorithm third we show that the existing spectral clustering algorithm can be considered a the special case of the proposed model and algorithm this demonstrates the good theoretic generality of the proposed model and algorithm 
this paper present a learning theoretical analysis of correlation clustering bansal et al in particular we give bound on the error with which correlation clustering recovers the correct partition in a planted partition model condon karp mcsherry using these bound we analyze how the accuracy of correlation clustering scale with the number of cluster and the sparsity of the graph we also propose a statistical test that analyzes the significance of the clustering found by correlation clustering 
existing semi supervised learning method are mostly based on either the cluster assumption or the manifold assumption in this paper we propose an integrated regularization framework for semi supervised kernel machine by incorporating both the cluster assumption and the manifold assumption moreover it support kernel learning in the form of kernel selection the optimization problem involves joint optimization over all the labeled and unlabeled data point a convex set of basic kernel and a discrete space of unknown label for the unlabeled data when the manifold assumption is incorporated graph laplacian kernel are used a the basic kernel for learning an optimal convex combination of graph laplacian kernel comparison with related method on the usps data set show very promising result 
the output of a data mining algorithm is only a good a it input and individual are often unwilling to provide accurate data about sensitive topic such a medical history and personal finance individual maybe willing to share their data but only if they are assured that it will be used in an aggregate study and that it cannot be linked back to them protocol for anonymity preserving data collection provide this assurance in the absence of trusted party by allowing a set of mutually distrustful respondent to anonymously contribute data to an untrusted data miner to effectively provide anonymity a data collection protocol must be collusion resistant which mean that even if all dishonest respondent collude with a dishonest data miner in an attempt to learn the association between honest respondent and their response they will be unable to do so to achieve collusion resistance previously proposed protocol for anonymity preserving data collection have quadratically many communication round in the number of respondent and employ sometimes incorrectly complicated cryptographic technique such a zero knowledge proof we describe a new protocol for anonymity preserving collusion resistant data collection our protocol ha linearly many communication round and achieves collusion resistance without relying on zero knowledge proof this make it especially suitable for data mining scenario with a large number of respondent 
for infant early word learning is a chicken and egg problem one way to learn a word is to observe that it co occurs with a particular referent across different situation another way is to use the social context of an utterance to infer the intended referent of a word here we present a bayesian model of cross situational word learning and an extension of this model that also learns which social cue are relevant to determining reference we test our model on a small corpus of mother infant interaction and find it performs better than competing model finally we show that our model account for experimental phenomenon including mutual exclusivity fast mapping and generalization from social cue to understand the difficulty of an infant word learner imagine walking down the street with a friend who suddenly say dax blicket philbin na fivy while at the same time wagging her elbow if you knew any of these word you might infer from the syntax of her sentence that blicket is a novel noun and hence the name of a novel object at the same time if you knew that this friend indicated her attention by wagging her elbow at object you might infer that she intends to refer to an object in a nearby show window on the other hand if you already knew that blicket meant the object in the window you might be able to infer these element of syntax and social cue thus the problem of early word learning is a classic chicken and egg puzzle in order to learn word meaning learner must use their knowledge of the rest of language including rule of syntax part of speech and other word meaning a well a their knowledge of social situation but in order to learn about the fact of their language they must first learn some word and in order to determine which cue matter for establishing reference for instance pointing and looking at an object but normally not waggling your elbow they must first have a way to know the intended referent in some situation for theory of language acquisition there are two common way out of this dilemma the first involves positing a wide range of innate structure which determine the syntax and category of a language and which social cue are informative though even when all of these element are innately determined using them to learn a language from evidence may not be trivial the other alternative involves bootstrapping learning some word then using those word to learn how to learn more this paper give a proposal for the second alternative we first present a bayesian model of how learner could use a statistical strategy cross situational word learning to learn how word map to object independent of syntactic and social cue we then extend this model to a true bootstrapping situation using social cue to learn word while using word to learn social cue finally we examine several important phenomenon in word learning mutual exclusivity the tendency to assign novel word to novel referent fast mapping the ability to assign a novel word in a linguistic context to a novel referent after only a single use and social generalization the ability to use social context to learn the referent of a novel word without adding additional specialized machinery we show how these can be explained within our model a the result of domain general probabilistic inference mechanism operating over the linguistic domain 
we describe a vision based obstacle avoidance system for off road mobile robot the system is trained from end to end to map raw input image to steering angle it is trained in supervised mode to predict the steering angle provided by a human driver during training run collected in a wide variety of terrain weather condition lighting condition and obstacle type the robot is a cm off road truck with two forwardpointing wireless color camera a remote computer process the video and control the robot via radio the learning system is a large layer convolutional network whose input is a single left right pair of unprocessed low resolution image the robot exhibit an excellent ability to detect obstacle and navigate around them in real time at speed of m s 
we consider the ensemble clustering problem where the task is to aggregate multiple clustering solution into a single consolidated clustering that maximizes the shared information among given clustering solution we obtain several new result for this problem first we note that the notion of agreement under such circumstance can be better captured using an agreement measure based on a d string encoding rather than voting strategy based method proposed in literature using this generalization we first derive a nonlinear optimization model to maximize the new agreement measure we then show that our optimization problem can be transformed into a strict semidefinite program sdp via novel convexification technique which can subsequently be relaxed to a polynomial time solvable sdp our experiment indicate improvement not only in term of the proposed agreement measure but also the existing agreement measure based on voting strategy we discus evaluation on clustering and image segmentation database 
boosting method are known not to usually overfit training data even a the size of the generated classifier becomes large schapire et al attempted to explain this phenomenon in term of the margin the classifier achieves on training example later however breiman cast serious doubt on this explanation by introducing a boosting algorithm arc gv that can generate a higher margin distribution than adaboost and yet performs worse in this paper we take a close look at breiman s compelling but puzzling result although we can reproduce his main finding we find that the poorer performance of arc gv can be explained by the increased complexity of the base classifier it us an explanation supported by our experiment and entirely consistent with the margin theory thus we find maximizing the margin is desirable but not necessarily at the expense of other factor especially base classifier complexity 
in the information regularization framework by corduneanu and jaakkola the distribution of label are propagated on a hypergraph for semi supervised learning the learning is efficiently done by a blahut arimoto like two step algorithm but unfortunately one of the step cannot be solved in a closed form in this paper we propose a dual version of information regularization which is considered a more natural in term of information geometry our learning algorithm ha two step each of which can be solved in a closed form also it can be naturally applied to exponential family distribution such a gaussians in experiment our algorithm is applied to protein classification based on a metabolic network and known functional category 
adaptively optimizing experiment can significantly reduce the number of trial needed to characterize neural response using parametric statistical model however the potential for these method ha been limited to date by severe computational challenge choosing the stimulus which will provide the most information about the typically high dimensional modelparameters requiresevaluatinga high dimensional integration and optimization in near real time here we present a fast algorithm for choosing the optimal most informative stimulus based on a fisher approximation of the shannon information and specialized numerical linear algebra technique this algorithm requires only low rank matrix manipulation and a one dimensional linesearch to choose the stimulus and is therefore efficient even for high dimensional stimulus and parameter space for example we require just millisecond on a desktop computer to optimize a dimensional stimulus our algorithm therefore make real time adaptive experimental design feasible simulation result show that model parameter can be estimated much more efficiently using these adaptive technique than by using random nonadaptive stimulus finally we generalize the algorithm to efficiently handle both fast adaptation due to spike history effect and slow non systematic drift in the model parameter 
this paper view clustering a element of a lattice distance between clustering are analyzed in their relationship to the lattice from this vantage point we rst give an axiomatic characterization of some criterion for comparing clustering including the variation of information and the unadjusted rand index then we study other distance between partition w r t these axiom and prove an impossibility result there is no sensible criterion for comparing clustering that is simultaneously aligned with the lattice of partition convexely additive and bounded 
in this paper we discus a problem of finding risk pattern in medical data we define risk pattern by a statistical metric relative risk which ha been widely used in epidemiological research we characterise the problem of mining risk pattern a an optimal rule discovery problem we study an anti monotone property for mining optimal risk pattern set and present an algorithm to make use of the property in risk pattern discovery the method ha been applied to a real world data set to find pattern associated with an allergic event for ace inhibitor the algorithm ha generated some useful result for medical researcher 
we propose a new approach for dealing with the estimation of the location of change point in one dimensional piecewise constant signal observed in white noise our approach consists in reframing this task in a variable selection context we use a penalized least square criterion with a type penalty for this purpose we prove some theoretical result on the estimated change point and on the underlying piecewise constant estimated function then we explain how to implement this method in practice by combining the lar algorithm and a reduced version of the dynamic programming algorithm and we apply it to synthetic and real data 
task of data mining and information retrieval depend on a good distance function for measuring similarity between data instance the most effective distance function must be formulated in a context dependent also application data and user dependent way in this paper we propose to learn a distance function by capturing the nonlinear relationship among contextual information provided by the application data or user we show that through a process called the kernel trick such nonlinear relationship can be learned efficiently in a projected space theoretically we substantiate that our method is both sound and optimal empirically using several datasets and application we demonstrate that our method is effective and useful 
fair discriminative pedestrian finder are now available in fact these pedestrian finder make most error on pedestrian in configuration that are uncommon in the training data for example mounting a bicycle this is undesirable however the human configuration can itself be estimated discriminatively using structure learning we demonstrate a pedestrian finder which first find the most likely human pose in the window using a discriminative procedure trained with structure learning on a small dataset we then present feature local histogram of oriented gradient and local pca of gradient based on that configuration to an svm classifier we show using the inria person dataset that estimate of configuration significantly improve the accuracy of a discriminative pedestrian finder 
although variant of value iteration have been proposed for finding nash or correlated equilibrium in general sum markov game these variant have not been shown to be effective in general in this paper we demonstrate by construction that existing variant of value iteration cannot find stationary equilibrium policy in arbitrary general sum markov game instead we propose an alternative interpretation of the output of value iteration based on a new non stationary equilibrium concept that we call cyclic equilibrium we prove that value iteration identifies cyclic equilibrium in a class of game in which it fails to find stationary equilibrium we also demonstrate empirically that value iteration find cyclic equilibrium in nearly all example drawn from a random distribution of markov game 
uncovering the haplotype of single nucleotide polymorphism and their population demography is essential for many biological and medical application method for haplotype inference developed thus far including method based on coalescence finite and infinite mixture and maximal parsimony ignore the underlying population structure in the genotype data a noted by pritchard different population can share certain portion of their genetic ancestor a well a have their own genetic component through migration and diversification in this paper we address the problem of multi population haplotype inference we capture cross population structure using a nonparametric bayesian prior known a the hierarchical dirichlet process hdp teh et al conjoining this prior with a recently developed bayesian methodology for haplotype phasing known a dp haplotyper xing et al we also develop an efficient sampling algorithm for the hdp based on a two level nested p lya urn scheme we show that our model outperforms extant algorithm on both simulated and real biological data 
when monitoring spatial phenomenon which are often modeled a gaussian process gps choosing sensor location is a fundamental task a common strategy is to place sensor at the point of highest entropy variance in the gp model we propose a mutual information criterion and show that it produce better placement furthermore we prove that finding the configuration that maximizes mutual information is np complete to address this issue we describe a polynomial time approximation that is within e of the optimum by exploiting the submodularity of our criterion this algorithm is extended to handle local structure in the gp yielding significant speedup we demonstrate the advantage of our approach on two real world data set 
we propose a model that leverage the million of click received by web search engine to predict document relevance this allows the comparison of ranking function when click are available but complete relevance judgment are not after an initial training phase using a set of relevance judgment paired with click data we show that our model can predict the relevance score of document that have not been judged these prediction can be used to evaluate the performance of a search engine using our novel formalization of the confidence of the standard evaluation metric discounted cumulative gain dcg so comparison can be made across time and datasets this contrast with previous method which can provide only pair wise relevance judgment between result shown for the same query when no relevance judgment are available we can identify the better of two ranked list up to of the time and with only two relevance judgment for each query we can identify the better ranking up to of the time while our experiment are on sponsored search result which is the financial backbone of web search our method is general enough to be applicable to algorithmic web search result a well furthermore we give an algorithm to guide the selection of additional document to judge to improve confidence 
we explore the situation in which document have to be categorized into more than one category system a situation we refer to a multiple view categorization more particularly we address the case where two different categorizers have already been built based on non necessarily identical training set each one labeled using one category system on the top of these categorizers considered a black box we propose some algorithm able to exploit a third training set containing a few example annotated in both category system such a situation arises for example in large company where incoming mail have to be routed to several department each one relying on it own category system we focus here on exploiting possible dependency between category system in order to rene the categorization decision made by categorizers trained independently on dieren t category system after a description of the multiple categorization problem we present several possible solution based either on a categorization or reweighting approach and compare them on real data lastly we show how the multimedia categorization problem can be cast a a multiple categorization problem and ass our method in this framework 
many recent study analyze how data from different modality can be combined often this is modeled a a system that optimally combine several source of information about the same variable however it ha long been realized that this information combining depends on the interpretation of the data two cue that are perceived by different modality can have different causal relationship they can both have the same cause in this case we should fully integrate both cue into a joint estimate they can have distinct cause in which case information should be processed independently in many case we will not know if there is one joint cause or two independent cause that are responsible for the cue here we model this situation a a bayesian estimation problem we are thus able to explain some experiment on visual auditory cue combination a well a some experiment on visual proprioceptive cue integration our analysis show that the problem solved by people when they combine cue to produce a movement is much more complicated than is usually assumed because they need to infer the causal structure that is underlying their sensory experience 
the misjudgement of tilt in image lie at the heart of entert aining visual illusion and rigorous perceptual psychophysics a wealth of finding ha attracted many mechanistic model but few clear computational principle we adopt a bayesian approach to perceptual tilt estimation showing how a smoothness prior offer a powerful way of addressing much confusing data in particular we faithfully model recent result showing that confidence in estimation can be systematically affected by the same aspect of image that affect bias confidence is cen tral to bayesian modeling approach and is applicable in many other perceptual domain 
we introduce a gaussian process gp framework stochastic relational model srm for learning social physical and other relational phenomenon where interaction between entity are observed the key idea is to model the stochastic structure of entity relationship i e link via a tensor interaction of multiple gps each defined on one type of entity these model in fact define a set of nonparametric prior on infinite dimensional tensor matrix where each element represents a relationship between a tuple of entity by maximizing the marginalized likelihood information is exchanged between the participating gps through the entire relational network so that the dependency structure of link is messaged to the dependency of entity reflected by the adapted gp kernel the framework offer a discriminative approach to link prediction namely predicting the existence strength or type of relationship based on the partially observed linkage network a well a the attribute of entity if given we discus property and variant of srm and derive an efficient learning algorithm very encouraging experimental result are achieved on a toy problem and a user movie preference link prediction task in the end we discus extension of srm to general relational learning task 
we consider the problem of multiclass classification where both labeled and unlabeled data point are given we introduce and demonstrate a new approach for estimating a distribution over the missing label where data point are viewed a node of a graph and pairwise similarity are used to derive a transition probability matrix p for a markov random walk between them the algorithm associate each point with a particle which move between point according to p labeled point are set to be absorbing state of the markov random walk and the probability of each particle to be absorbed by the different labeled point a the number of step increase is then used to derive a distribution over the associated missing label a computationally efficient algorithm to implement this is derived and demonstrated on both real and artificial data set including a numerical comparison with other method 
we describe a generative model for handwritten digit that us two pair of opposing spring whose stiffness are controlled by a motor program we show how neural network can be trained to infer the motor program required to accurately reconstruct the mnist digit the inferred motor program can be used directly for digit classification but they can also be used in other way by adding noise to the motor program inferred from an mnist image we can generate a large set of very different image of the same class thus enlarging the training set available to other method we can also use the motor program a additional highly informative output which reduce overfitting when training a feed forward classifier 
abstract we present an unsupervised algorithm for registering d surface scan of an object undergoing signicant deformation our algorithm doe not use marker nor doe it assume prior knowledge about object shape the dynamic of it deformation or scan alignment the algorithm register two mesh by optimizing a joint probabilistic model over all point topoint correspondence between them this model enforces preservation of local mesh geometry a well a more global constraint that capture the preservation of geodesic distance between corresponding point pair the algorithm applies even when one of the mesh is an incomplete range scan thus it can be used to automatically ll in the remaining surface for this partial scan even if those surface were previously only seen in a different conguration we evaluate the algorithm on several real world datasets where we demonstrate good result in the presence of signicant movement of articulated part and non rigid surface deformation finally we show that the output of the algorithm can be used for compelling computer graphic task such a interpolation between two scan of a non rigid object and automatic recovery of articulated object model 
there are well established method for reducing the number of support vector in a trained binary support vector machine often with minimal impact on accuracy we show how reduced set method can be applied to multiclass svms made up of several binary svms with significantly better result than reducing each binary svm independently our approach is based on burges approach that construct each reduced set vector a the pre image of a vector in kernel space but we extend this by recomputing the svm weight and bias optimally using the original svm objective function this lead to greater accuracy for a binary reduced set svm and also allows vector to be shared between multiple binary svms for greater multiclass accuracy with fewer reduced set vector we also propose computing pre image using differential evolution which we have found to be more robust than gradient descent alone we show experimental result on a variety of problem and find that this new approach is consistently better than previous multiclass reduced set method sometimes with a dramatic difference 
we consider continuous state continuous action batch reinforcement learning where the goal is to learn a good policy from a sufficiently rich trajectory generated by some policy we study a variant of fitted q iteration where the greedy action selection is replaced by searching for a policy in a restricted set of candidate policy by maximizing the average action value we provide a rigorous analysis of this algorithm proving what we believe is the first finite time bound for value function based algorithm for continuous state and action problem note in retrospect it would have been better to call this algorithm an actor critic algorithm the algorithm that we considers update a policy and a value function action value function in this case 
clustering aim to find useful hidden structure in data in this paper we present a new clustering algorithm that build upon the consistency method zhou et al a semi supervised learning technique with the property of learning very smooth function with respect to the intrinsic structure revealed by the data other method e g spectral clustering obtain good result on data that reveals such a structure however unlike spectral clustering our algorithm effectively detects both global and within class outlier and the most representative example in each class furthermore we specify an optimization framework that estimate all learning parameter including the number of cluster directly from data finally we show that the learned cluster model can be used to add previously unseen point to cluster without re learning the original cluster model encouraging experimental result are obtained on a number of real world problem 
the aim of this paper is to show that machine learning technique can be used to derive a classifying function for human brain signal data measured by magnetoencephalography meg for the use in a brain computer interface bci this is especially helpful for evaluating quickly whether a bci approach based on electroencephalography on which training may be slower due to lower signal to noise ratio is likely to succeed we apply rce and regularized svms to the experimental data of ten healthy subject performing a motor imagery task four subject were able to use a trained classifier to write a short name further analysis give evidence that the proposed imagination task is suboptimal for the possible extension to a multiclass interface to the best of our knowledge this paper is the first working online meg based bci and is therefore a proof of concept 
abstract we present a tree data structure for fast nearest neighbor operation in general npoint metric space where the data set consists of n point the data structure requires o n space regardless of the metric s structure yet maintains all performance property of a navigating net kl a if the point set ha a bounded expansion constant c which is a measure of the intrinsic dimensionality a defined in kr the cover tree data structure can be constructed in o c logn time our experimental result show speedup over the brute force search varying between one and several order of magnitude on natural machine learning datasets 
abstract over the past few year the notion of stability in data clustering ha received growing attention a a cluster validation criterion in a sample based framework however recent work ha shown that a the sample size increase any clustering model will usually become asymptotically stable this led to the conclusion that stability is lacking a a theoretical and practical tool the discrepancy between this conclusion and the success of stability in practice ha remained an open question which we attempt to address our theoretical approach is that stability a used by cluster validation algorithm is similar in certain respect to measure of generalization in a model selection framework in such case the model chosen governs the convergence rate of generalization bound by arguing that these rate are more important than the sample size we are led to the prediction that stability based cluster validation algorithm should not degrade with increasing sample size despite the asymptotic universal stability this prediction is substantiated by a theoretical analysis a well a some empirical result we conclude that stability remains a meaningful cluster validation criterion over finite sample 
correlation mining ha gained great success in many application domain for it ability to capture the underlying dependency between object however the research of correlation mining from graph database is still lacking despite the fact that graph data especially in various scientific domain proliferate in recent year in this paper we propose a new problem of correlation mining from graph database called correlated graph search cgs cgs adopts pearson s correlation coefficient a a correlation measure to take into consideration the occurrence distribution of graph however the problem pose significant challenge since every subgraph of a graph in the database is a candidate but the number of subgraphs is exponential we derive two necessary condition which set bound on the occurrence probability of a candidate in the database with this result we design an efficient algorithm that operates on a much smaller projected database and thus we are able to obtain a significantly smaller set of candidate to further improve the efficiency we develop three heuristic rule and apply them on the candidate set to further reduce the search space our extensive experiment demonstrate the effectiveness of our method on candidate reduction the result also justify the efficiency of our algorithm in mining correlation from large real and synthetic datasets 
in recent year the proliferation of voip data ha created a number of application in which it is desirable to perform quick online classification and recognition of massive voice stream typically such application are encountered in real time intelligence and surveillance in many case the data stream can be in compressed format and the rate of data processing can often run at the rate of gigabit per second all known technique for speaker voice analysis require the use of an offline training phase in which the system is trained with known segment of speech the state of the art method for text independent speaker recognition is known a gaussian mixture modeling gmm and it requires an iterative expectation maximization procedure for training which cannot be implemented in real time in this paper we discus the detail of such an online voice recognition system for this purpose we use our micro clustering algorithm to design concise signature of the target speaker one of the surprising and insightful observation from our experience with such a system is that while it wa originally designed only for efficiency we later discovered that it wa also more accurate than the widely used gaussian mixture model gmm this wa because of the conciseness of the micro cluster model which made it le prone to over training this is evidence of the fact that it is often possible to get the best of both world and do better than complex model both from an efficiency and accuracy perspective 
a wide variety of dirichlet multinomial topic model hav e found interesting application in recent year while gibbs sampling remains an important method of inference in such model variational technique have certain advantage such a easy assessment of convergence easy optimization without the need to maintain detailed balance a bound on the marginal likelihood and side stepping of issue with topic identifiability the most accurate variational technique thus far namely collapsed variational lda cv lda did not deal with model selection nor did it include inference for hyperparameters we address both issue by generalizing their technique obtaining the first variational algorithm to deal with the hdp and to deal with hyperparameters of dirichlet variable experiment show a very significant improvement in accuracy relative to cv lda 
this paper describes the use of machine learning to improve the performance of natural language question answering system we present a model for improving story comprehension through inductive generalization and reinforcement learning based on classified example in the process the model selects the most relevant and useful piece of lexical information to be used by the inference procedure we compare our approach to three prior non learning system and evaluate the condition under which learning is effective we demonstrate that a learning based approach can improve upon matching and extraction only technique 
it is well known that everything that is learnable in the difficult online setting where an arbitrary sequence of example must be labeled one at a time is also learnable in the batch setting where example are drawn independently from a distribution we show a result in the opposite direction we give an efficient conversion algorithm from batch to online that is transductive it us future unlabeled data this demonstrates the equivalence between what is properly and efficiently learnable in a batch model and a transductive online model 
the relevance vector machine rvm is a sparse approximate bayesian kernel method it provides full predictive distribution for test case however the predictive uncertainty have the unintuitive property that they get smaller the further you move away from the training case we give a thorough analysis inspired by the analogy to non degenerate gaussian process we suggest augmentation to solve the problem the purpose of the resulting model rvm is primarily to corroborate the theoretical and experimental analysis although rvm could be used in practical application it is no longer a truly sparse model experiment show that sparsity come at the expense of worse predictive distribution 
extensive game are a powerful model of multiagent decision making scenario with incomplete information finding a nash equilibrium for very large instance of these game ha received a great deal of recent attention in this paper we describe a new technique for solving large game based on regret minimization in particular we introduce the notion of counterfactual regret which exploit the degree of incomplete information in an extensive game we show how minimizing counterfactual regret minimizes overall regret and therefore in self play can be used to compute a nash equilibrium we demonstrate this technique in the domain of poker showing we can solve abstraction of limit texas hold em with a many a state two order of magnitude larger than previous method 
the classical hypothesis that bottom up saliency is a cent er surround process is combined with a more recent hypothesis that all saliency decision are optimal in a decision theoretic sense the combined hypothesis is denoted a discriminant center surround saliency and the corresponding optimal saliency architecture is derived this architecture equates the saliency of each image location to the discriminant power of a set of feature with respect to the classification problem that opposes stimulus at center and surround at that location it is shown that the resulting saliency detector make accurate quantitative predict ion for various aspect of the psychophysics of human saliency including non linear property beyond the reach of previous saliency model furthermore it is shown that discriminant center surround saliency can be easily generalized to vari ous stimulus modality such a color orientation and motion and provides optimal solution for many other saliency problem of interest for computer vision optimal solution under this hypothesis are derived for a number of the former including static natural image dense motion field and even dynamic texture and applied to a number of the latter the prediction of human eye fixation moti on based saliency in the presence of ego motion and motion based saliency in the presence of highly dynamic background in result discriminant saliency is shown to predict eye fixation better than previous model and produce backgro und subtraction algorithm that outperform the state of the art in computer vi sion 
most nervous system encode information about stimulus in the responding activity of large neuronal network this activity ofte n manifest itself a dynamically coordinated sequence of action potential since multiple electrode recording are now a standard tool in neuroscience research it is important to have a measure of such network wide behavioral coordination and information sharing applicable to multiple neural spike train data we propose a new statistic informational coherence which measure how much better one unit can be predicted by knowing the dynamical state of another we argue informational coherence is a measure of association and shared information which is superior to traditional pairwise measure of synchronization and correlation to find the dynamical state we use a recently introduced algorithm which reconstructs effective state space from stochastic time series we then extend the pairwise measure to a multivariate analysis of the network by estimating the network multi information we illustrate our method by testing it on a detailed model of the transition from gamma to beta rhythm much of the most important information in neural system is shared over multiple neuron or cortical area in such form a population code and distributed representation on behavioral time scale neural information is store d in temporal pattern of activity a opposed to static marker therefore a informat ion is shared between neuron or brain region it is physically instantiated a coordina tion between entire sequence of neural spike furthermore neural system and region of the brain often require coordinated neural activity to perform important function ac ting in concert requires multiple neuron or cortical area to share information thus if we want to measure the dynamic network wide behavior of neuron and test hypothesis about them we need reliable practical method to detect and quantify behavioral coordination and the associated information sharing across multiple neural unit these would be especially useful in testing idea about how particular form of coordination relate to distri buted coding e g that of current technique to analyze relation among spike train handle only pair of neuron so we further need a method which is extendible to analyze the coordination in the network system or region a a whole here we propose a new measure of behavioral coordination and information sharing informational coherence based on the notion of dynamical state section argues that coordinated behavior in neural system is often not captured by exist 
lasso regression tends to assign zero weight to most irrelevant or redundant feature and hence is a promising technique for feature selection it limitation however is that it only offer solution to linear model kernel machine with feature scaling technique have been studied for feature selection with non linear model however such approach require to solve hard non convex optimization problem this paper proposes a new approach named the feature vector machine fvm it reformulates the standard lasso regression into a form isomorphic to svm and this form can be easily extended for feature selection with non linear model by introducing kernel defined on feature vector fvm generates sparse solution in the nonlinear feature space and it is much more tractable compared to feature scaling kernel machine our experiment with fvm on simulated data show encouraging result in identifying the small number of dominating feature that are non linearly correlated to the response a task the standard lasso fails to complete 
the paper is concerned with learning to rank which is to construct a model or a function for ranking object learning to rank is useful for document retrieval collaborative filtering and many other application several method for learning to rank have been proposed which take object pair a instance in learning we refer to them a the pairwise approach in this paper although the pairwise approach offer advantage it ignores the fact that ranking is a prediction task on list of object the paper postulate that learning to rank should adopt the listwise approach in which list of object are used a instance in learning the paper proposes a new probabilistic method for the approach specifically it introduces two probability model respectively referred to a permutation probability and top k probability to define a listwise loss function for learning neural network and gradient descent are then employed a model and algorithm in the learning method experimental result on information retrieval show that the proposed listwise approach performs better than the pairwise approach 
in supervised learning we commonly assume that training and test data are sampled from the same distribution however this assumption can be violated in practice and then standard machine learning technique perform poorly this paper focus on revealing and improving the performance of bayesian estimation when the training and test distribution are different we formally analyze the asymptotic bayesian generalization error and establish it upper bound under a very general setting our important finding is that lower order term which can be ignored in the absence of the distribution change play an important role under the distribution change we also propose a novel variant of stochastic complexity which can be used for choosing an appropriate model and hyper parameter under a particular distribution change 
we extend the bayesian skill rating system trueskill to infer entire time series of skill of player by smoothing through time instead of ltering the skill of each participating player say every year is represented by a latent skill variable which is aected by the relevant game outcome that year and coupled with the skill variable of the previous and subsequent year inference in the resulting factor graph is carried out by approximate message passing ep along the time series of skill a before the system track the uncertainty about player skill explicitly model draw can deal with any number of competing entity and can infer individual skill from team result we extend the system to estimate player specic draw margin based on these model we present an analysis of the skill curve of important player in the history of chess over the past year result include plot of player lifetime skill development a well a the ability to compare the skill of dieren t player across time our result indicate that a the overall playing strength ha increased over the past year and b that modelling a player s ability to force a draw provides signican tly better predictive power 
we present new theoretical and empirical result with the ilstd algorithm for policy evaluation in reinforcement learning with linear function approximation ilstd is an incremental method for achieving result similar to lstd the dataefficient least square version of temporal difference learning without incurring the full cost of the lstd computation lstd is o n where n is the number of parameter in the linear function approximator while ilstd is o n in this paper we generalize the previous ilstd algorithm and present three new result the first convergence proof for an ilstd algorithm an extension to incorporate eligibility trace without changing the asymptotic computational complexity and the first empirical result with an ilstd algorithm for a problem mountain car with feature vector large enough n to show substantial computational advantage over lstd 
with an increasing use of data mining tool and technique we envision that a knowledge discovery and data mining system kddms will have to support and optimize for the following scenario sequence of query a user may analyze one or more datasets by issuing a sequence of related complex mining query and multiple simultaneous query several user may be analyzing a set of datasets concurrently and may issue related complex query this paper present a systematic mechanism to optimize for the above case targeting the class of mining query involving frequent pattern mining on one or multiple datasets we present a system architecture and propose new algorithm to simultaneously optimize multiple such query and use a knowledgeable cache to store and utilize the past query result we have implemented and evaluated our system with both real and synthetic datasets our experimental result show that our technique can achieve a speedup of up to a factor of compared with the system which do not support caching or optimize for multiple query 
recent work on predictive state representation psr model ha focused on using prediction of the outcome of open loop action sequence a state these prediction answer question of the form what is the probability of seeing observation sequence o o on if the agent take action sequence a a an from some given history we would like to ask more expressive question in our representation of state such a if i behave according to some policy until i terminate what will be my last observation we extend the linear psr framework to answer question like these about option temporally extended closed loop course of action bounding the size of the linear psr needed to model question about a certain class of option we introduce a hierarchical psr hpsr that can make prediction about both option and primitive action sequence and show empirical result from learning hpsrs in simple domain 
we use knowledge discovery technique to guide the creation of efficient overlay network for peer to peer file sharing an overlay network specifies the logical connection among peer in a network and is distinct from the physical connection of the network it determines the order in which peer will be queried when a user is searching for a specific file to better understand the role of the network overlay structure in the performance of peer to peer file sharing protocol we compare several method for creating overlay network we analyze the network using data from a campus network for peer to peer file sharing that recorded anonymized data on user sharing music file over an day period we propose a novel protocol for overlay creation based on a model of user preference identified by latent variable clustering with hierarchical dirichlet process hdps our simulation and empirical study show that the cluster of song created by hdps effectively model user behavior and can be used to create desirable network overlay that outperform alternative approach 
in this paper we show that classical survival analysis involving censored data can naturally be cast a a ranking problem the concordance index ci which quantifies the quality of ranking is the standard performance measure for model assessment in survival analysis in contrast the standard approach to learning the popular proportional hazard ph model is based on cox s partial likelihood we devise two bound on ci one of which emerges directly from the property of ph model and optimize them directly our experimental result suggest that all three method perform about equally well with our new approach giving slightly better result we also explain why a method designed to maximize the cox s partial likelihood also end up approximately maximizing the ci 
fast and frugal heuristic are well studied model of bounded rationality psychological research ha proposed the take the best heuristic a a successful strategy in decision making with limited resource take thebest search for a sufficiently good ordering of cue featu re in a task where object are to be compared lexicographically we investigate the complexity of the problem of approximating optimal cue permutation for lexicographic strategy we show that no efficient algo rithm can approximate the optimum to within any constant factor if p np we further consider a greedy approach for building lexicographic strategy and derive tight bound for the performance ratio of a new and simple algorithm this algorithm is proven to perform better than take the best 
the local statistical property of photographic image when represented in a multi scale basis have been described using gaussian scale mixture gsms here we use this local description to construct a global field of gaussian scale mixture fogsm specifically we model subbands of wavelet coefficient a a product of an exponentiated homogeneous gaussian markov random field hgmrf and a second independent hgmrf we show that parameter estimation for fogsm is feasible and that sample drawn from an estimated fogsm model have marginal and joint statistic similar to wavelet coefficient of photographic image we develop an algorithm for image denoising based on the fogsm model and demonstrate substantial improvement over current state ofthe art denoising method based on the local gsm model many successful method in image processing and computer vision rely on statistical model for image and it is thus of continuinginterest to developimprovedmodels both in term of their ability to precisely capture image structure and in term of their tractability when used in application constructing such a model is difficult primarily because of the intrinsic high dimensionality of the space of image two simplifying assumption are usually made to reduce model complexity the first is markovianity the density of a pixel conditioned on a small neighborhood is assumed to be independent from the rest of the image the second assumption is homogeneity the local density is assumed to be independent of it absolute position within the image the set of model satisfying both of these assumption constitute the class of homogeneous markov random field hmrfs over the past two decade study of photographic image represented with multi scale multiorientation image decomposition loosely referred to a wavelet have revealed striking nongaussian regularity and inter and intra subband dependency for instance wavelet coefficient generally have highly kurtotic marginal distribution and their amplitude exhibit strong correlation with the amplitude of nearby coefficient one model that can capture the nongaussian marginal behavior is a product of non gaussian scalar variable a number of author have developed non gaussian mrf model based on this sort of local description among which the recently developed field of expert model ha demonstrated impressive performance in denoising albeit at an extremely high computational cost in learning model parameter 
training a learning algorithm is a costly task a major goal of active learning is to reduce this cost in this paper we introduce a n ew algorithm kqbc which is capable of actively learning large scale problem by using selective sampling the algorithm overcomes the costly sampling step of the well known query by committee qbc algorithm by projecting onto a low dimensional space kqbc also enables the use of kernel providing a simple way of extending qbc to the non linear scenario sampling the low dimension space is done using the hit and run random walk we demonstrate the success of this novel algorithm by applying it to both artificial and a real world problem 
many real world classification problem involve the prediction of multiple inter dependent variable forming some structural dependency recent progress in machine learning ha mainly focused on supervised classification of such structured variable in this paper we investigate structured classification in a semi supervised setting we present a discriminative approach that utilizes the intrinsic geometry of input pattern revealed by unlabeled data point and we derive a maximum margin formulation of semi supervised learning for structured variable unlike transductive algorithm our formulation naturally extends to new test point 
we demonstrate that log linear grammar with latent variable can be practically trained using discriminative method central to efficient discriminative training is a hierarchical pruning procedure which allows feature expectation to be efficiently approximated in a gradient based procedure we compare l and l regularization and show that l regularization is superior requiring fewer iteration to converge and yielding sparser solution on full scale treebank parsing experiment the discriminative latent model outperform both the comparable generative latent model a well a the discriminative non latent baseline in this paper we present a hierarchical pruning procedure that exploit the structure of the model and allows feature expectation to be efficiently approxima ted making discriminative training of full scale grammar practical we present a gradient based procedure for training a discriminative grammar on the entire wsj section of the penn treebank roughly sentence containing million word we then compare l and l regularization and show that l regularization is superior requiring fewer iteration to converge and yield ing sparser solution independent of the regularization discriminative grammar significantly ou tperform their generative counterpart in our experiment 
mapping to structured output space string tree partition etc are typically learned using extension of classification algorithm to simple graphical structure eg linear chain in which search and parameter estimation can be performed exactly unfortunately in many complex problem it is rare that exact search or parameter estimation is tractable instead of learning exact model and searching via heuristic mean we embrace this difficulty and treat the structured output problem in term of approximate search we present a framework for learning a search optimization and two parameter update with convergence the orems and bound empirical evidence show that our integrated approach to learning and decoding can outperform exact model at smaller computational cost 
development of advanced anomaly detection and failure diagnosis technology for spacecraft is a quite significant issue in the space industry because the space environment is harsh distant and uncertain while several modern approach based on qualitative reasoning expert system and probabilistic reasoning have been developed recently for this purpose any of them ha a common difficulty in obtaining accurate and complete a priori knowledge on the space system from human expert a reasonable alternative to this conventional anomaly detection method is to reuse a vast amount of telemetry data which is multi dimensional time series continuously produced from a number of system component in the spacecraft this paper proposes a novel knowledge free anomaly detection method for spacecraft based on kernel feature space and directional distribution which construct a system behavior model from the past normal telemetry data from a set of telemetry data in normal operation and monitor the current system status by checking incoming data with the model in this method we regard anomaly phenomenon a unexpected change of causal association in the spacecraft system and hypothesize that the significant causal association inside the system will appear in the form of principal component direction in a high dimensional non linear feature space which is constructed by a kernel function and a set of data we have confirmed the effectiveness of the proposed anomaly detection method by applying it to the telemetry data obtained from a simulator of an orbital transfer vehicle designed to make a rendezvous maneuver with the international space station 
gaussian data is pervasive and many learning algorithm e g k mean model their input a a single sample drawn from a multivariate gaussian however in many real life setting each input object is best described by multiple sample drawn from a multivariate gaussian such data can arise for example in a movie review database where each movie is rated by several user or in time series domain such a sensor network here each input can be naturally described by both a mean vector and covariance matrix which parameterize the gaussian distribution in this paper we consider the problem of clustering such input object each represented a a multivariate gaussian we formulate the problem using an information theoretic approach and draw several interesting theoretical connection to bregman divergence and also bregman matrix divergence we evaluate our method across several domain including synthetic data sensor network data and a statistical debugging application 
we introduce multiple topic tracking mtt for iscore to better recommend news article for user with multiple interest and to address change in user interest over time a an extension of the basic rocchio algorithm traditional topic detection and tracking and single pas clustering mtt maintains multiple interest profile to identify interesting article for a specific user given user feedback focusing on only interesting topic enables iscore to discard useless profile to address change in user interest and to achieve a balance between resource consumption and classification accuracy also by relating a topic s interestingness to an article s interestingness iscore is able to achieve higher quality result than traditional method such a the rocchio algorithm we identify several operating parameter that work well for mtt using the same parameter we show that mtt alone yield high quality result for recommending interesting article from several corpus the inclusion of mtt improves iscore s performance by in recommending news article from the yahoo news r feed and the trec adaptive filter article collection and through a small user study we show that iscore can still perform well when only provided with little user feedback 
we address the problem of automatically constructing basis function for linear approximation of the value function of a markov decision process mdp our work build on result by bertsekas and casta on who proposed a method for automatically aggregating state to speed up value iteration we propose to use neighborhood component analysis goldberger et al a dimensionality reduction technique created for supervised learning in order to map a high dimensional state space to a low dimensional space based on the bellman error or on the temporal difference td error we then place basis function in the lower dimensional space these are added a new feature for the linear function approximator this approach is applied to a high dimensional inventory control problem 
we propose a novel method for linear dimensionality reduction of manifold modeled data first we show that with a small number m of random projectionsof sample point in rn belonging to an unknown k dimensional euclidean manifold the intrinsic dimension id of the sample set can be estimated to high accuracy second we rigorously prove that using only this set of random projection we can estimate the structure of the underlying manifold in both case the number of random projection required is linear in k and logarithmic in n meaning that k m n to handle practical situation we develop a greedy algorithm to estimate the smallest size of the projection space requir ed to perform manifold learning our method is particularly relevant in distribut ed sensing system and lead to significant potential saving in data acquisition storage and transmission cost 
gaussian process temporal difference gptd learning offer a bayesian solution to the policy evaluation problem of reinforcement learning in this paper we extend the gptd framework by addressing two pressing issue which were not adequately treated in the original gptd paper engel et al the first is the issue of stochasticity in the state transition and the second is concerned with action selection and policy improvement we present a new generative model for the value function deduced from it relation with the discounted return we derive a corresponding on line algorithm for learning the posterior moment of the value gaussian process we also present a sarsa based extension of gptd termed gpsarsa that allows the selection of action and the gradual improvement of policy without requiring a world model 
correlation between instance is often modelled via a kernel function using input attribute of the instance relational knowledge can further reveal additional pairwise correlation between variable of interest in this paper we develop a class of model which incorporates both reciprocal relational information and input attribute using gaussian process technique this approach provides a novel non parametric bayesian framework with a data dependent covariance function for supervised learning task we also apply this framework to semi supervised learning experimental result on several real world data set verify the usefulness of this algorithm 
we derive a bayesian ideal observer bio for detecting motion and solving the correspondence problem we obtain barlow and tripathy s classic model a an approximation our psychophysical experiment show that the trend of human performance are similar to the bayesian ideal but overall human performance is far worse we investigate way to degrade the bayesian ideal but show that even extreme degradation do not approach human performance instead we propose that human perform motion task using generic general purpose model of motion we perform more psychophysical experiment which are consistent with human using a slow and smooth model and which rule out an alternative model using slowness in an influential paper barlow and tripathy tested the ab ility of human subject to detect dot moving coherently in a background of random dot they derived an ideal observer model using technique from signal detection theory they showed that their model predicted the trend of the human performance a property of the stimulus changed but that human performed far worse than their model they argued that degrading their model by lowering the spatial resolution would give prediction closer to human performance barlow and tripathy s model ha generated considerable int erest see we formulate this motion problem in term of bayesian decision theory and derive a bayesian ideal observer bio model we describe why barlowand tripathy s bt model is not fully ideal show that it can be obtained a an approximation to the bio and determine condition under which it is a good approximation we perform psychophysical experiment under a range of condition and show that the trend of human subject are more similar to those of the bio we investigate whether degrading the bayesian ideal enables u to reach human performance and conclude that it doe not without implausibly large 
we introduce a robust probabilistic approach to modeling shape contour based on a lowdimensional nonlinear latent variable model in contrast to existing technique that use objective function in data space without explicit noise model we are able to extract complex shape variation from noisy data most approach to learning shape model slide observed data point around fixed contour and hence require a correctly labeled reference shape to prevent degenerate solution in our method unobserved curve are reparameterized to explain the fixed data point so this problem doe not arise the proposed algorithm are suitable for use with arbitrary basis function and are applicable to both open and closed shape their effectiveness is demonstrated through illustrative example quantitative assessment on benchmark data set and a visualization task 
elimination by aspect eba is a probabilistic choice model describing how human decide between several option the option from which the choice is made are characterized by binary feature and associated weight for instance when choosing which mobile phone to buy the feature to consider may be long lasting battery color screen etc existing method for inferring the parameter of the model assume pre specified feature however the feature that lead to the observed choice are not always known here we present a non parametric bayesian model to infer the feature of the option and the corresponding weight from choice data we use the indian buffet process ibp a a prior over the feature inference using markov chain monte carlo mcmc in conjugate ibp model ha been previously described the main contribution of this paper is an mcmc algorithm for the eba model that can also be used in inference for other non conjugate ibp model this may broaden the use of ibp prior considerably 
we propose a non linear generative model for human motion data that us an undirected model with binary latent variable and real valued visible variable that represent joint angle the latent and visible variabl e at each time step receive directed connection from the visible variable at th e last few time step such an architecture make on line inference efficient and a llows u to use a simple approximate learning procedure after training the model find a single set of parameter that simultaneously capture several differe nt kind of motion we demonstrate the power of our approach by synthesizing various motion sequence and by performing on line filling in of data lost during motio n capture website http www c toronto edu gwtaylor publication nip mhmublv 
spiking activity from neurophysiological experiment often exhibit dynamic beyond that driven by external stimulation presumably reflecting the extensive recurrence of neural circuitry characterizing these dynamic may reveal important feature of neural computation particularly during internally driven cognitive operation for example the activity of premotor cortex pmd neuron during an instructed delay period separating movement target specification and a movementinitiation cue is believed to be involved in motor planning we show that the dynamic underlying this activity can be captured by a lowdimensional non linear dynamical system model with underlying recurrent structure and stochastic point process output we present and validate latent variable method that simultaneously estimate the system parameter and the trial by trial dynamical trajectory these method are applied to characterize the dynamic in pmd data recorded from a chronically implanted electrode array while monkey perform delayed reach task 
we investigate the learning of the appearance of an object from a single image of it instead of using a large number of picture of the object to recognize we use a labeled reference database of picture of other object to learn invariance to noise and variation in pose and illumination this acquired knowledge is then used to predict if two picture of new object which do not appear on the training picture actually display the same object we propose a generic scheme called chopping to address this task it relies on hundred of random binary split of the training set chosen to keep together the image of any given object those split are extended to the complete image space with a simple learning algorithm given two image the response of the split predictor are combined with a bayesian rule into a posterior probability of similarity experiment with the coil database and with a database of degraded l atex symbol compare our method to a classical learning with several example of the positive class and to a direct learning of the similarity 
we address privacy preserving classification problem in a distributed system randomization ha been the approach proposed to preserve privacy in such scenario however this approach is now proven to be insecure a it ha been discovered that some privacy intrusion technique can be used to reconstruct private information from the randomized data tuples we introduce an algebraic technique based scheme compared to the randomization approach our new scheme can build classifier more accurately but disclose le private information furthermore our new scheme can be readily integrated a a middleware with existing system 
this paper proposes new approach to rank individual from their group competition result many real world problem are of this type for example ranking player from team game is important in some sport we propose an exponential model to solve such problem to estimate individual ranking through the proposed model we introduce two convex minimization formula with easy and efficient solution procedure experiment on real bridge record and multi class classification demonstrate the viability of the proposed model 
the control of high dimensional continuous non linear dynamical system is a key problem in reinforcement learning and control local trajectory based method using technique such a differential dynamic programming ddp are not directly subject to the curse of dimensionality but generate only local controller in this paper we introduce receding horizon ddp rh ddp an extension to the classic ddp algorithm which allows u to construct stable and robust controller based on a library of local control trajectory we demonstrate the effectiveness of our approach on a series of high dimensional problem using a simulated multi link swimming robot these experiment show that our approach effectively circumvents dimensionality issue and is capable of dealing with problem of at least state and action dimension 
we consider the problem of grasping novel object specically one that are being seen for the rst time through vision we present a learning algorithm that neither requires nor try to build a d model of the object instead it predicts directly a a function of the image a point at which to grasp the object our algorithm is trained via supervised learning using synthetic image for the training set we demonstrate on a robotic manipulation platform that this approach successfully grasp a wide variety of object such a wine glass duct tape marker a translucent box jug knife cutter cellphone key screwdriver stapler toothbrush a thick coil of wire a strangely shaped power horn and others none of which were seen in the training set in contrast to these approach we propose a learning algorithm that neither requires nor try to build a d model of the object instead it predicts directly a a function of the image a point at which to grasp the object informally the algorithm take two or more picture of the object and then try to identify a point within each d image that corresponds to a good point at which to grasp the object for example if trying to grasp a coffee mug it might try to identify the mid point of the handle given these d point in each image we use triangulation to obtain a d position at which to actually attempt the grasp thus rather than trying to triangulate every single point within each image in order to estimate depth a in dense stereo we only attempt to triangulate one or at most a small number of point corresponding to the d point where we will grasp the object this allows u to grasp an object without ever needing to obtain it full d shape and applies even to textureless translucent or reecti ve object on which standard stereo d reconstruction fare poorly to the best of our knowledge our work represents the rst algorithm capable of grasping novel object one where a d model is not available including one from novel object class that we are perceiving for the rst time using vision 
s timulus selectivity of sensory neuron is often characterized by estimating their receptive field property such a orientation selectivity receptive field are usually derived from the mean or covariance of the spike triggered stimulus ensemble this approach treat each spike a an independent message but doe not take into account that information might be conveyed through pattern of neural activity that are distributed across space or time can we find a concise description for the processing of a whole population of neuron analogous to the receptive field for single neuron here we present a generalization of the linear receptive field which is not bound to be triggered on individual spike but can be meaningfully linked to distributed response pattern more precisely we seek to identify those stimulus feature and the corresponding pattern of neural activity that are most reliably coupled we use an extension of reverse correlation method based on canonical correlation analysis the resulting population receptive field span the subspace of stimulus that is most informative about the population response we evaluate our approach using both neuronal model and multi electrode recording from rabbit retinal ganglion cell we show how the model can be extended to capture nonlinear stimulus response relationship using kernel canonical correlation analysis which make it possible to test different coding mechanism our technique can also be used to calculate receptive field from multi dimensional neural measurement such a those obtained from dynamic imaging method 
we address the problem of robust computationally efficient design of biological experiment classical optimal experiment design method have not been widely adopted in biological practice in part because the resulting design can be very brittle if the nominal parameter estimate for the model are poor and in part because of computational constraint we present a method for robust experiment design based on a semidefinite programming relaxation we present an application of this method to the design of experiment for a complex calcium signal transduction pathway where we have found that the parameter estimate obtained from the robust design are better than those obtained from an optimal design 
in this paper we present a new approach to mining binary data we treat each binary feature item a a mean of distinguishing two set of example our interest is in selecting from the total set of item an itemset of specified size such that the database is partitioned with a uniform a distribution over the part a possible to achieve this goal we propose the use of joint entropy a a quality measure for itemsets and refer to optimal itemsets of cardinality k a maximally informative k itemsets we claim that this approach maximises distinctive power a well a minimises redundancy within the feature set a number of algorithm is presented for computing optimal itemsets efficiently 
we are at the beginning of the multicore era computer will have increasingly many core processor but there is still no good programming framework for these architecture and thus no simple and unified way for machine learning to take advantage of the potential speed up in this paper we develop a broadly applicable parallel programming method one that is easily applied to many different learning algorithm our work is in distinct contrast to the tradition in machine learning of designing often ingenious way to speed up a single algorithm at a time specifically we show that algorithm that fit the statistical query model can be written in a certain summation form which allows them to be easily parallelized on multicore computer we adapt google s map reduce paradigm to demonstrate this parallel speed up technique on a variety of learning algorithm including locally weighted linear regression lwlr k mean logistic regression lr naive bayes nb svm ica pca gaussian discriminant analysis gda em and backpropagation nn our experimental result show basically linear speedup with an increasing number of processor 
we propose a new method for learning structured output using gradient descent 
theory of visual attention commonly posit that early parallel process extract conspicuous feature such a color contrast and motion from the visual field these feature are then combined into a saliency map and attention is directed to the most salient region first top down attentional control is achieved by modulating the contribution of different feature type to the saliency map a key source of data concerning attentional control come from behavioral study in which the effect of recent experience is examined a individual repeatedly perform a perceptual discrimination task e g what shape is the odd colored object the robust finding is that repetition of feature of recent trial e g target color facilitates performance we view this facilitation a an adaptation to the statistical structure of the environment we propose a probabilistic model of the environment that is updated after each trial under the assumption that attentional control operates so a to make performance more efficient for more likely environmental state we obtain parsimonious explanation for data from four different experiment further our model provides a rational explanation for why the influence of past experience on attentional control is short lived 
abstract we propose a recursive algorithm for clustering trajectory lying in multiple moving hyperplanes starting from a given or random initial condition we use normalized gradient descent to update the coefficient of a time varying polynomial whose degree is the number of hyperplanes and whose derivative at a trajectory give an estimate of the vector normal to the hyperplane containing that trajectory a time proceeds the estimate of the hyperplane normal are shown to track their true value in a stable fashion the segmentation of th e trajectory is then obtained by clustering their associated normal vector the final result is a simple recursive algorithm for segmenting a variable number of moving hyperplanes we test our algorithm on the segmentation of dynamic scene containing rigid motion and dynamic texture e g a bird floating on water ou r method not only segment the bird motion from the surrounding water motion but also determines pattern of motion in the scene e g periodic motion dire ctly from the temporal evolution of the estimated polynomial coefficient our exp eriments also show that our method can deal with appearing and disappearing motion in the scene 
although support vector machine svms for binary classification give rise to a decision rule that only relies on a subset of the training data point support vector it will in general be based on all available feature in the input space we propose two direct novel convex relaxation of a non convex sparse svm formulation that explicitly constrains the cardinality of the vector of feature weight one relaxation result in a quadratically constrained quadratic program qcqp while the second is based on a semidefinite programming sdp relaxation the qcqp formulation can be interpreted a applying an adaptive soft threshold on the svm hyperplane while the sdp formulation learns a weighted inner product i e a kernel that result in a sparse hyperplane experimental result show an increase in sparsity while conserving the generalization performance compared to a standard a well a a linear programming svm 
we consider the problem of choosing sequentially a map which assigns element of a seta to a few element of a setb on each round the algorithm suer some cost associated with the chosen assignment and the goal is to minimize the cumulative loss of these choice relative to the best map on the entire sequence even though the oine problem of finding the best map is provably hard we show that there is an equivalent online approximation algorithm randomized map prediction rmp that is ecient and performs nearly a well while drawing upon result from the online prediction with expert advice setting we show how rmp can be utilized a an online approach to several standard batch problem we apply rmp to online clustering a well a online feature selection and surprisingly rmp often outperforms the standard batch algorithm on these problem 
most existing approach to outlier detection are based on density estimation method there are two notable issue with these method one is the lack of explanation for outlier flagging decision and the other is the relatively high computational requirement in this paper we present a novel approach to outlier detection based on classification in an attempt to address both of these issue our approach isbased on two key idea first we present a simple reduction of outlier detection to classification via a procedure that involves applying classification to a labeled data set containing artificially generated example that play the role of potential outlier once the task ha been reduced to classification we then invoke a selective sampling mechanism based on active learning to the reduced classification problem we empirically evaluate the proposed approach using a number of data set and find that our method is superior to other method based on the same reduction to classification but using standard classification method we also show that it is competitive to the state of the art outlier detection method in the literature based on density estimation while significantly improving the computational complexity and explanatory power 
a new algorithm for on line learning linear threshold function is proposed which efficiently combine second order statistic about the data with the logarithmic behavior of multiplicative dual norm algorithm an initial theoretical analysis is provided suggesting that our algorithm might be viewed a a standard perceptron algorithm operating on a transformed sequence of example with improved margin property we also report on experiment carried out on datasets from diverse domain with the goal of comparing to known perceptron algorithm first order second order additive multiplicative our learning procedure seems to generalize quite well and converges faster than the corresponding multiplicative baseline algorithm 
we study the problem of learning a classification task in which only a dissimilarity function of the object is accessible that is data are not represented by feature vector but in term of their pairwise dissimilarity we investigate the sufficient condition for dissimilarity function to allow building accurate classifier our result have the advantage that they apply to unbounded dissimilarity and are invariant to order preserving transformation the theory immediately suggests a learning paradigm construct an ensemble of decision stump each depends on a pair of example then find a convex combination of them to achieve a large margin we next develop a practical algorithm called dissimilarity based boosting dboost for learning with dissimilarity function under the theoretical guidance experimental result demonstrate that dboost compare favorably with several existing approach on a variety of database and under different condition 
to escape from the curse of dimensionality we claim that one can learn non local function in the sense that the value and shape of the learned function at x must be inferred using example that may be far from x with this objective we present a non local non parametric density estimator it build upon previously proposed gaussian mixture model with regularized covariance matrix to take into account the local shape of the manifold it also build upon recent work on non local estimator of the tangent plane of a manifold which are able to generalize in place with little training data unlike traditional local nonparametric model 
maximum margin matrix factorization mmmf wa recently suggested srebro et al a a convex infinite dimensional alternative to low rank approximation and standard factor model mmmf can be formulated a a semi definite programming sdp and learned using standard sdp solver however current sdp solver can only handle mmmf problem on matrix of dimensionality up to a few hundred here we investigate a direct gradient based optimization method for mmmf and demonstrate it on large collaborative prediction problem we compare against result obtained by marlin and find that mmmf substantially outperforms all nine method he tested 
mdps are an attractive formalization for planning but realistic problem often have intractably large state space when we only need a partial policy to get from a fixed start state to a goal restricting computation to state relevant to this task can make much larger problem tractable we introduce a new algorithm bounded rtdp which can produce partial policy with strong performance guarantee while only touching a fraction of the state space even on problem where other algorithm would have to visit the full state space to do so bounded rtdp maintains both upper and lower bound on the optimal value function the performance of bounded rtdp is greatly aided by the introduction of a new technique to efficiently find suitable upper bound this technique can also be used to provide informed initialization to a wide range of other planning algorithm 
recent decision theoric planning algorithm are able to find optimal solution in large problem using factored markov decision process fmdps however these algorithm need a perfect knowledge of the structure of the problem in this paper we propose sdyna a general framework for addressing large reinforcement learning problem by trial and error and with no initial knowledge of their structure sdyna integrates incremental planning algorithm based on fmdps with supervised learning technique building structured representation of the problem we describe spiti an instantiation of sdyna that us incremental decision tree induction to learn the structure of a problem combined with an incremental version of the structured value iteration algorithm we show that spiti can build a factored representation of a reinforcement learning problem and may improve the policy faster than tabular reinforcement learning algorithm by exploiting the generalization property of decision tree induction algorithm 
online auction are generating a new class of fine grained data about online transaction this data lends itself to a variety of application and service that can be provided to both buyer and seller in online marketplace we collect data from online auction and use several classification algorithm to predict the probable end price of online auction item this paper describes the feature extraction and selection process and several machine learning formulation of the price prediction problem a a prototype application we developed auction price insurance that us the predicted end price to offer price insurance to seller in online auction we define price insurance a a service that offer insurance to auction seller that guarantee a price for their good for an appropriate premium if the item sell for le than the insured price the seller is reimbursed for the difference we show that our price prediction technique are accurate enough to offer price insurance a a profitable business while this paper deal specifically with online auction we believe that this is an interesting case study that applies to dynamic market where the price of the good is variable and is affected by both internal and external factor that change over time 
the web contains an abundance of useful semistructured information about real world object and our empirical study show that strong sequence characteristic exist for web information about object of the same type across different web site conditional random field crfs are the state of the art approach taking the sequence characteristic to do better labeling however a the information on a web page is two dimensionally laid out previous linear chain crfs have their limitation for web information extraction to better incorporate the two dimensional neighborhood interaction this paper present a two dimensional crf model to automatically extract object information from the web we empirically compare the proposed model with existing linear chain crf model for product information extraction and the result show the effectiveness of our model 
joint mining of multiple data set can often discover interesting novel and reliable pattern which cannot be obtained solely from any single source for example in cross market customer segmentation a group of customer who behave similarly in multiple market should be considered a a more coherent and more reliable cluster than cluster found in a single market a another example in bioinformatics by joint mining of gene expression data and protein interaction data we can find cluster of gene which show coherent expression pattern and also produce interacting protein such cluster may be potential pathway in this paper we investigate a novel data mining problem mining cross graph quasi clique which is generalized from several interesting application such a cross market customer segmentation and joint mining of gene expression data and protein interaction data we build a general model for mining cross graph quasi clique show why the complete set of cross graph quasi clique cannot be found by previous data mining method and study the complexity of the problem while the problem is difficult we develop an efficient algorithm crochet which exploit several interesting and effective technique and heuristic to efficaciously mine cross graph quasi clique a systematic performance study is reported on both synthetic and real data set we demonstrate some interesting and meaningful cross graph quasi clique in bioinformatics the experimental result also show that algorithm crochet is efficient and scalable 
nested sampling is a new monte carlo method by skilling intended for general bayesian computation nested sampling provides a robust alternative to annealing based method for computing normalizing constant it can also generate estimate of other quantity such a posterior expectation the key technical requirement is an ability to draw sample uniformly from the prior subject to a constraint on the likelihood we provide a demonstration with the potts model an undirected graphical model 
a logistic regression classification algorithm is developed for problem in which the feature vector may be missing data feature single or multiple imputation for the missing data is avoided by performing analytic integration with an estimated conditional density function conditioned on the non missing data conditional density function are estimated using a gaussian mixture model gmm with parameter estimation performed using both expectation maximization em and variational bayesian em vb em using widely available real data we demonstrate the general advantage of the vb em gmm estimation for handling incomplete data vi vi the em algorithm moreover it is demonstrated that the approach proposed here is generally superior to standard imputation procedure 
we describe and analyze a simple and effective iterative algorithm for solving the optimization problem cast by support vector machine svm our method alternate between stochastic gradient descent step and projection step we prove that the number of iteration required to obtain a solution of accuracy is o in 
feature selection is often applied to high dimensional data prior to classification learning using the same training dataset in both selection and learning can result in so called feature subset selection bias this bias putatively can exacerbate data over fitting and negatively affect classification performance however in current practice separate datasets are seldom employed for selection and learning because dividing the training data into two datasets for feature selection and classifier learning respectively reduces the amount of data that can be used in either task this work attempt to address this dilemma we formalize selection bias for classification learning analyze it statistical property and study factor that affect selection bias a well a how the bias impact classification learning via various experiment this research endeavor to provide illustration and explanation why the bias may not cause negative impact in classification a much a expected in regression 
calculation that quantify the dependency between variable are vital to many operation with graphical model e g active learning and sensitivity analysis previously pairwise information gain calculation ha involved a cost quadratic in network size in this work we show how to perform a similar computation with cost linear in network size the loss function that allows this is of a form amenable to computation by dynamic programming the message passing algorithm that result is described and empirical result demonstrate large speedup without decrease in accuracy in the cost sensitive domain examined superior accuracy is achieved 
kernel partial least square kpls ha been known a a generic kernel regression method and proven to be competitive with other kernel regression method such a support vector machine for regression svm and kernel ridge regression kernel boosted latent feature kblf is a variant of kpls for any differentiable convex loss function it provides a more flexible framework for various predictive modeling task such a classification with logistic loss and robust regression with l norm loss etc however kpls and kblf solution are dense and thus not suitable for large scale computation sparsification of kpls solution ha been studied for dual and primal form for dual sparsity it requires solving a nonlinear optimization problem at every iteration step and it computational burden limit it applicability to general regression task in this paper we propose simple heuristic to approximate sparse solution for kpls and the framework is also applied for sparsifying kblf solution the algorithm provides an interesting path from a maximum residual criterion based algorithm with orthogonality condition to the dense kpls kblf with the orthogonality it differentiates itself from many existing forward selection type algorithm the computational advantage is illustrated by benchmark datasets and comparison to svm is done 
the popularity of email ha triggered researcher to look for way to help user better organize the enormous amount of information stored in their email folder one challenge that ha not been studied extensively in text mining is the identification and reconstruction of hidden email a hidden email is an original email that ha been quoted in at least one email in a folder but doe not present itself in the same folder it may have been un intentionally deleted or may never have been received the discovery and reconstruction of hidden email is critical for many application including email classification summarization and forensics this paper proposes a framework for reconstructing hidden email using the embedded quotation found in message further down the thread hierarchy we evaluate the robustness and scalability of our framework by using the enron public email corpus our experiment show that hidden email exist widely in that corpus and also that our optimization technique are effective in processing large email folder 
bayesian inference ha become increasingly important in statistical machine learning exact bayesian calculation are often not feasible in practice however a number of approximate bayesian method have been proposed to make such calculation practical among them the variational bayesian vb approach the vb approach while useful can nevertheless suffer from slow convergence to the approximate solution to address this problem we propose parameter expanded variational bayesian px vb method to speed up vb the new algorithm is inspired by parameter expanded expectation maximization px em and parameterexpanded data augmentation px da similar to px em and da px vb expands a model with auxiliary variable to reduce the coupling between variable in the original model we analyze the convergence rate of vb and px vb and demonstrate the superior convergence rate of px vb in variational probit regression and automatic relevance determination 
a bridging rule in this paper ha it antecedent and action from different conceptual cluster we first design two algorithm for mining bridging rule between cluster in a database and then propose two non linear metric for measuring the interestingness of bridging rule bridging rule can be distinct from association rule or frequent itemsets this is because bridging rule can be generated by infrequent itemsets that are pruned in association rule mining and bridging rule are measured by the importance that includes the distance between two conceptual cluster whereas frequent itemsets are measured by only the support 
we present a novel ensemble pruning method based on reordering the classifier obtained from bagging and then selecting a subset for aggregation ordering the classifier generated in bagging make it possible to build subensembles of increasing size by including first those classifier that are expected to perform best when aggregated ensemble pruning is achieved by halting the aggregation process before all the classifier generated are included into the ensemble pruned subensembles containing between and of the initial pool of classifier besides being smaller improve the generalization performance of the full bagging ensemble in the classification problem investigated 
we introduce a novel approach to incorporating domain knowledge into support vector machine to improve their example efficiency domain knowledge is used in an explanation based learning fashion to build justification or explanation for why the training example are assigned their given class label explanation bias the large margin classifier through the interaction of training example and domain knowledge we develop a new learning algorithm for this explanation augmented svm ea svm it naturally extends to imperfect knowledge a stumbling block to conventional ebl experimental result confirm desirable property predicted by the analysis and demonstrate the approach on three domain 
in this paper we introduce rl cd a method for solving reinforcement learning problem in non stationary environment the method is based on a mechanism for creating updating and selecting one among several partial model of the environment the partial model are incrementally built according to the system s capability of making prediction regarding a given sequence of observation we propose formalize and show the efficiency of this method both in a simple non stationary environment and in a noisy scenario we show that rl cd performs better than two standard reinforcement learning algorithm and that it ha advantage over method specifically designed to cope with non stationarity finally we present known limitation of the method and future work 
we propose a class of graphical model appropriate for structure prediction problem where the model structure is a function of the output structure incremental sigmoid belief network isbns avoid the need to sum over the possible model structure by using directed arc and incrementally specifying the model structure exact inference in such directed model is not tractable but we derive two efficient approximation based on mean field method which prove effective in artificial experiment we then demonstrate their effectiveness on a benchmark natural language parsing task where they achieve state of the art accuracy also the model which is a closer approximation to an isbn ha better parsing accuracy suggesting that isbns are an appropriate abstract model of structure prediction task 
probabilistic temporal planning attempt to find good policy for acting in domain with concurrent durative task multiple uncertain outcome and limited resource these domain are typically modelled a markov decision problem and solved using dynamic programming method this paper demonstrates the application of reinforcement learning in the form of a policy gradient method to these domain our emphasis is large domain that are infeasible for dynamic programming our approach is to construct simple policy or agent for each planning task the result is a general probabilistic temporal planner named the factored policy gradient planner fpg planner which can handle hundred of task optimising for probability of success duration and resource use 
in this paper we propose a probabilistic kernel approach to preference learning based on gaussian process a new likelihood function is proposed to capture the preference relation in the bayesian framework the generalized formulation is also applicable to tackle many multiclass problem the overall approach ha the advantage of bayesian method for model selection and probabilistic prediction experimental result compared against the constraint classification approach on several benchmark datasets verify the usefulness of this algorithm 
markov jump process play an important role in a large number of application domain however realistic system are analytically intractable and they have traditionally been analysed using simulation based technique which do not provide a framework for statistical inference we propose a mean eld approximation to perform posterior inference and parameter estimation the approximation allows a practical solution to the inference problem while still retaining a good degree of accuracy we illustrate our approach on two biologically motivated system 
for a markov decision process with finite state size s and action space size a per state we propose a new algorithm delayed q learning we prove it is pac achieving near optimal performance except for sa timesteps using o sa space improving on the s a bound of best previous algorithm this result prof efficient reinforcement learning is possible without learning a model of the mdp from experience learning take place from a single continuous thread of experience no reset nor parallel sampling is used beyond it smaller storage and experience requirement delayed q learning s per experience computation cost is much le than that of previous pac algorithm 
kernel based learning algorithm such a support vector machine svms or perceptron often rely on sequential optimization where a few example are added at each iteration updating the kernel matrix usually requires matrix vector multiplication we propose a new method based on transposition to speedup this computation on sparse data instead of dot product over sparse feature vector our computation incrementally merges list of training example and minimizes access to the data caching and shrinking are also optimized for sparsity on very large natural language task tagging translation text classification with sparse feature representation a to fold speedup over libsvm is observed using the same smo algorithm theory and experiment explain what type of sparsity structure is needed for this approach to work and why it adaptation to maxent sequential optimization is inefficient 
speech dereverberation remains an open problem after more than three decade of research the most challenging step in speech dereverberation is blind channel identification bci although many bci approach have been developed their performance is still far from satisfactory for practical application the main difficulty in bci lie in finding an appropriate acoustic model which not only can effectively resolve solution degeneracy due to the lack of knowledge of the source but also robustly model real acoustic environment this paper proposes a sparse acoustic room impulse response rir model for bci that is an acoustic rir can be modeled by a sparse fir filter under this model we show how to formulate the bci of a single input multiple output simo system into a l norm regularized least square l problem which is convex and can be solved efficiently with guaranteed global convergence the sparseness of solution is controlled by l norm regularization parameter we propose a sparse learning scheme that infers the optimal l norm regularization parameter directly from microphone observation under a bayesian framework our result show that the proposed approach is effective and robust and it yield source estimate in real acoustic environment with high fidelity to anechoic chamber measurement 
it is becoming increasingly important to learn from a partially observed random matrix and predict it missing element we assume that the entire matrix is a single sample drawn from a matrix variate t distribution and suggest a matrixvariate t model mvtm to predict those missing element we show that mvtm generalizes a range of known probabilistic model and automatically performs model selection to encourage sparse predictive model due to the non conjugacy of it prior it is difficult to make prediction by computing the mode or mean of the posterior distribution we suggest an optimization method that sequentially minimizes a convex upper bound of the log likelihood which is very efficient and scalable the experiment on a toy data and eachmovie dataset show a good predictive accuracy of the model 
we present a computational model of human eye movement in an object class detection task the model combine state of the art computer vision object class detection method sift feature trained using adaboost with a biologically plausible model of human eye movement to produce a sequence of simulated fixation culminating with the acquisition of a target we validated the model by comparing it behavior to the behavior of human observer performing the identical object class detection task looking for a teddy bear among visually complex nontarget object we found considerable agreement between the model and human data in multiple eye movement measure including number of fixation cumulative probability of fixating the target and scanpath distance 
online learning algorithm are typically fast memory effic ient and simple to implement however many common learning problem fit more naturally in the batch learning setting the power of online learning algorithm can be exploited in batch setting by using online to batch conversion technique which build a new batch algorithm from an existing online algorithm we first give a unified overview of th ree existing online to batch conversion technique which do not use training data in the conversion process we then build upon these data independent conversion to derive and analyze data driven conversion our conversion find hypothesis with a small risk by explicitly minimiz ing datadependent generalization bound we experimentally demonstrate the usefulness of our approach and in particular show that the data driven conversion consistently outperform the data independent conversion 
we provide a provably efficient algorithm for learning marko v decision process mdps with continuous state and action space in the online setting specifically we take a model based approach and show that a special type of online linear regression allows u to learn mdps with possibly kernalized linearly parameterized dynamic this result build on kearns and singh s w ork that provides a provably efficient algorithm for finite state mdps our appro ach is not restricted to the linear setting and is applicable to other class of c ontinuous mdps provably efficient rl for finite state and action space is accomplished by kearns and singh and hugely contributes to our understanding of the relationship between exploration and sequential decision making the achievement of the current paper is to provide an efficie nt rl algorithm that learns in markov decision process mdps with continuous state and action space we prove that it learns linearly parameterized mdps a model introduced by abbeel and ng with sample or experience complexitythat grows only polynomially with the number of state space dimension our new rl algorithm utilizes a special linear regresser based on least square regression whose analysis may be of interest to the online learning and statis tic community although our primary result is for linearly parameterized mdps our technique is applicable to other class of continuous mdps and our framework is developed specifically with such fu ture application in mind the linear dynamic case should be viewed a only an interesting example of our approach which make substantial progress in the goal of understanding the relat ionship between exploration and generalization in rl an outline of the paper follows in section we discus online linear regression and pose a new online learning framework that requires an algorithm to not only provide prediction for new data point but also provide formal guarantee about it prediction we also develop a specific algorithm and prove that it solves the problem in section using the algorithm and result from the first section we develop a provably efficient rl algorithm final ly we conclude with future work some of the work presented here wa conducted while the author wa at rutgers university 
we present a general model independent approach to the analysis of data in case when these data do not appear in the form of co occurrence of two variable x y but rather a a sample of value of an unknown stochastic function z x y for example in gene expression data the expression level z is a function of gene x and condition y or in movie rating data the rating z is a function of viewer x and movie y the approach represents a consistent extension of the information bottleneck method that ha previously relied on the availability of co occurrence statistic by altering the relevance variable we eliminat e the need in the sample of joint distribution of all input variable this new formula tion also enables simple mdl like model complexity control and prediction of missing value of z the approach is analyzed and shown to be on a par with the best known clustering algorithm for a wide range of domain for the prediction of missing value collaborative filtering it improves the currently best kn own result 
we consider single class classification scc a a two person game between the learner and an adversary in this game the target distribution is completely known to the learner and the learner s goal is to construct a classifier capable of guaranteeing a given tolerance for the false positive error while minimizing the false negative error we identify both hard and soft optimal classification strategy for different type of game and demonstrate that soft classification can provide a significant advantage our optimal strategy and bound provide worst case lower bound for standard finite sample scc and also motivate new approach to solving scc 
nonnegative matrix approximation nnma is a recent technique for dimensionality reduction and data analysis that yield a part s based sparse nonnegative representation for nonnegative input data nnma ha found a wide variety of application including text analysis do cument clustering face image recognition language modeling speech processing and many others despite these numerous application the algorithmic development for computing the nnma factor ha been relatively deficient this paper make algorithmic progress by modeling and solving using multiplicative update new generalized nnma problem that minimize bregman divergence between the input matrix and it lowrank approximation the multiplicative update formula in the pioneering work by lee and seung arise a a special case of our algorithm in addition the paper show how to use penalty function for incorporating constraint other than nonnegativity into the problem further some interesting extension to the use of link function for mo deling nonlinear relationship are also discussed 
we present lungcad a computer aided diagnosis cad system that employ a classification algorithm for detecting solid pulmonary nodule from ct thorax study we briefly describe some of the machine learning technique developed to overcome the real world challenge in this medical domain the most significant hurdle in transitioning from a machine learning research prototype that performs well on an in house dataset into a clinically deployable system is the requirement that the cad system be tested in a clinical trial we describe the clinical trial in which lungcad wa tested a large scale multi reader multi case mrmc retrospective observational study to evaluate the effect of cad in clinical practice for detecting solid pulmonary nodule from ct thorax study the clinical trial demonstrates that every radiologist that participated in the trial had a significantly greater accuracy with lungcad both for detecting nodule and identifying potentially actionable nodule this along with other finding from the trial ha resulted in fda approval for lungcad in late 
we present a non linear simple yet effective feature sub set selection method for regression and use it in analyzing cortical neural activity our algorithm involves a feature weighted version of the k nearest neighbor algorithm it is able to capture complex dependency of the target function on it input and make use of the leave one out error a a natural regularization we explain the characteristic of our algo rithm on synthetic problem and use it in the context of predicting hand velocity from spike recorded in motor cortex of a behaving monkey by applying feature selection we are able to improve prediction quality and suggest a novel way of exploring neural data 
given a redundant dictionary of basis vector or atom our goal is to find maximally sparse representation of signal previous ly we have argued that a sparse bayesian learning sbl framework is particularly well suited for this task showing that it ha far fewer loca l minimum than other bayesian inspired strategy in this paper we prov ide further evidence for this claim by proving a restricted equivalence condition based on the distribution of the nonzero generating model weight whereby the sbl solution will equal the maximally sparse representation we also prove that if these nonzero weight are drawn from an approximate jeffreys prior then with probability approaching one our equivalence condition is satisfied finally we motivate the worst case sce nario for sbl and demonstrate that it is still better than the most widely u sed sparse representation algorithm these include basis pursuit bp which is based on a convex relaxation of the quasi norm and orthogonal matching pursuit omp a simple greedy strategy that iterativel y selects basis vector most aligned with the current residual 
contextual text mining is concerned with extracting topical theme from a text collection with context information e g time and location and comparing analyzing the variation of theme over different context since the topic covered in a document are usually related to the context of the document analyzing topical theme within context can potentially reveal many interesting theme pattern in this paper we generalize some of these model proposed in the previous work and we propose a new general probabilistic model for contextual text mining that can cover several existing model a special case specifically we extend the probabilistic latent semantic analysis plsa model by introducing context variable to model the context of a document the proposed mixture model called contextual probabilistic latent semantic analysis cplsa model can be applied to many interesting mining task such a temporal text mining spatiotemporal text mining author topic analysis and cross collection comparative analysis empirical experiment show that the proposed mixture model can discover theme and their contextual variation effectively 
discriminative training of graphical model can be expensive if the variable have large cardinality even if the graphical structure is tractable in such case pseudolikelihood is an attractive alternative because it running time is linear in the variable cardinality but on some data it accuracy can be poor piecewise training sutton mccallum can have better accuracy but doe not scale a well in the variable cardinality in this paper we introduce piecewise pseudolikelihood which retains the computational efficiency of pseudolikelihood but can have much better accuracy on several benchmark nlp data set piecewise pseudolikelihood ha better accuracy than standard pseudolikelihood and in many case nearly equivalent to maximum likelihood with five to ten time le training time than batch crf training 
given a directed graphical model with binary valued hidden node and real valued noisy observation consider deciding upon the maximum a posteriori map or the maximum posterior marginal mpm assignment under the restriction that each node broadcast only to it child exactly one single bit message we present a variational formulation viewing the processing rule local to all node a degree of freedom that minimizes the loss in expected map or mpm performance subject to such online communication constraint the approach lead to a novel message passing algorithm to be executed offline or before observation are realized which mitigates the performance loss by iteratively coupling all rule in a manner implicitly driven by global stati stics we also provide i illustrative example ii assumption that g uarantee convergence and efficiency and iii connection to active researc h area 
we present a unified model of what wa traditionally viewed a two separate task data association and intensity tracking of multiple topic over time in the data association part the task is to assign a topic a class to each data point and the intensity tracking part model the burst and change in intensity of topic over time our approach to this problem combine an extension of factorial hidden markov model for topic intensity tracking with exponential order statistic for implicit data association experiment on text and email datasets show that the interplay of classification and topic intensity tracking improves the accuracy of both classification and intensity tracking even a little noise in topic assignment can mislead the traditional algorithm however our approach detects correct topic intensity even with topic noise 
we study the mining of interesting pattern in the presence of numerical attribute instead of the usual discretization method we propose the use of rank based measure to score the similarity of set of numerical attribute new support measure for numerical data are introduced based on extension of kendall s tau and spearman s footrule and rho we show how these support measure are related furthermore we introduce a novel type of pattern combining numerical and categorical attribute we give efficient algorithm to find all frequent pattern for the proposed support measure and evaluate their performance on real life datasets 
fusing multiple information source can yield significant benefit to successfully accomplish learning task many study have focussed on fusing information in supervised learning context we present an approach to utilize multiple information source in the form of similarity data for unsupervised learning based on similarity information the clustering task is phrased a a non negative matrix factorization problem of a mixture of similarity measurement the tradeoff between the informativeness of data source and the sparseness of their mixture is controlled by an entropy based weighting mechanism for the purpose of model selection a stability based approach is employed to ensure the selection of the most self consistent hypothesis the experiment demonstrate the performance of the method on toy a well a real world data set 
collaborative recommender system are highly vulnerable to attack attacker can use automated mean to inject a large number of biased profile into such a system resulting in recommendation that favor or disfavor given item since collaborative recommender system must be open to user input it is difficult to design a system that cannot be so attacked researcher studying robust recommendation have therefore begun to identify type of attack and study mechanism for recognizing and defeating them in this paper we propose and study different attribute derived from user profile for their utility in attack detection we show that a machine learning classification approach that includes attribute derived from attack model is more successful than more generalized detection algorithm previously studied 
we present a simple statistical model of molecular function evolution to predict protein function the model description encodes general knowledge of how molecular function evolves within a phylogenetic tree based on the protein sequence input are a phylogeny for a set of evolutionarily related protein sequence and any available function characterization for those protein posterior probability for each protein are used to predict the molecular function of that protein we present result from applying our model to three protein family and compare our prediction result on the extant protein to other available protein function prediction method for the deaminase family our method achieves where related method blast achieves gotcha achieves and orthostrapper achieves in prediction accuracy 
motor imagery attenuates eeg and rhythm over sensorimotor cortex these amplitude change are most successfully captured by the method of common spatial pattern csp and widely used in braincomputer interface bci bci method based on amplitude information however have not incoporated the rich phase dynamic in the eeg rhythm this study report on a bci method based on phase synchrony rate sr sr computed from binarized phase locking value describes the number of discrete synchronization event within a window statistical nonparametric test show that sr contain significant difference between type of motor imagery classifier trained on sr consistently demonstrate satisfactory result for all subject it is further observed that for subject phase is more discriminative than amplitude in the first s which suggests that phase ha the potential to boost the information transfer rate in bcis 
multi instance learning and semi supervised learning are different branch of machine learning the former attempt to learn from a training set consists of labeled bag each containing many unlabeled instance the latter try to exploit abundant unlabeled instance when learning with a small number of labeled example in this paper we establish a bridge between these two branch by showing that multi instance learning can be viewed a a special case of semi supervised learning based on this recognition we propose the misssvm algorithm which address multi instance learning using a special semi supervised support vector machine experiment show that solving multi instance problem from the view of semi supervised learning is feasible and the misssvm algorithm is competitive with state of the art multi instance learning algorithm 
we apply probability theory to the task of proving whether a goal can be achieved by a player in an adversarial game such problem are solved by searching the game tree we view this tree a a graphical model which yield a distribution over the boolean outcome of the search before it terminates experiment show that a best first search algorithm guided by this distribution explores a similar number of node a proof number search to solve go problem knowledge is incorporated into search by using domain specific model to provide prior distribution over the value of leaf node of the game tree these are surrogate for the unexplored part of the tree the parameter of these model can be learned from previous search tree experiment on go show that the speed of problem solving can be increased by order of magnitude by this technique but care must be taken to avoid over fitting 
we present a new algorithm locally smooth manifold learning lsml that learns a warping function from a point on an manifold to it neighbor important characteristic of lsml include the ability to recover the structure of the manifold in sparsely populated region and beyond the support of the provided data application of our proposed technique include embedding with a natural out of sample extension and task such a tangent distance estimation frame rate up conversion video compression and motion transfer we show that lsml can recover the structure of the manifold where data is given and also in region where it is not including region beyond the support of the original data we propose a number of us for the recovered warping function w including embedding with a natural out ofsample extension and in the image domain discus how it can be used for task such a computation of tangent distance image sequence interpolation compression and motion transfer we also show example where lsml is used to simultaneously learn the structure of multiple parallel manifold and even generalize to data on new manifold finally we show that by exploiting the manifold smoothness lsml is robust under condition where many embedding method have difficulty 
several algorithm have been proposed to learn to rank entity modeled a feature vector based on relevance feedback however these algorithm do not model network connection or relation between entity meanwhile pagerank and variant find the stationary distribution of a reasonable but arbitrary markov walk over a network but do not learn from relevance feedback we present a framework for ranking networked entity based on markov walk with parameterized conductance value associated with the network edge we propose two flavor of conductance learning problem in our framework in the first setting relevance feedback comparing node pair hint that the user ha one or more hidden preferred community with large edge conductance and the algorithm must discover these community we present a constrained maximum entropy network flow formulation whose dual can be solved efficiently using a cutting plane approach and a quasi newton optimizer in the second setting edge have type and relevance feedback hint that each edge type ha a potentially different conductance but this is fixed across the whole network our algorithm learns the conductance using an approximate newton method 
this paper considers kernel invariant to translation rotation and dilation we show that no non trivial positive definite p d kernel exist which are radial and dilation invariant only conditionally positive definite c p d one accordingly we discus the c p d case and provide some novel analysis including an elementary derivation of a c p d representer theorem on the practical side we give a support vector machine s v m algorithm for arbitrary c p d kernel for the thinplate kernel this lead to a classifier with only one parameter the amount of regularisation which we demonstrate to be a effective a an s v m with the gaussian kernel even though the gaussian involves a second parameter the length scale 
eigenvalue problem are rampant in machine learning and statistic and appear in the context of classification dimensionality reduction etc in this paper we consider a cardinality constrained variational formulation of generalized eigenvalue problem with sparse principal component analysis pca a a special case using l norm approximation to the cardinality constraint previous method have proposed both convex and non convex solution to the sparse pca problem in contrast we propose a tighter approximation that is related to the negative log likelihood of a student s t distribution the problem is then framed a a d c difference of convex function program and is solved a a sequence of locally convex program we show that the proposed method not only explains more variance with sparse loading on the principal direction but also ha better scalability compared to other method we demonstrate these result on a collection of datasets of varying dimensionality two of which are high dimensional gene datasets where the goal is to find few relevant gene that explain a much variance a possible 
we consider the problem of estimating occurrence rate of rare eventsfor extremely sparse data using pre existing hierarchy to perform inference at multiple resolution in particular we focus on the problem of estimating click rate for webpage advertisement pair called impression where both the page and the ad are classified into hierarchy that capture broad contextual information at different level of granularity typically the click rate are low and the coverage of the hierarchy is sparse to overcome these difficulty we devise a sampling method whereby we analyze aspecially chosen sample of page in the training set and then estimate click rate using a two stage model the first stage imputes the number of webpage ad pair at all resolution of the hierarchy to adjust for the sampling bias the second stage estimate clickrates at all resolution after incorporating correlation among sibling node through a tree structured markov model both model are scalable and suited to large scale data mining application on a real world dataset consisting of billion impression we demonstrate that even with negative non clicked event in the training set our method can effectively discriminate extremely rare event in term of their click propensity 
dynamic bayesian network are structured representation of stochastic process despite their structure exact inference in dbns is generally intractable one approach to approximate inference involves grouping the variable in the process into smaller factor and keeping independent belief over these factor in this paper we present several technique for decomposing a dynamic bayesian network automatically to enable factored inference we examine a number of feature of a dbn that capture different type of dependency that will cause error in factored inference an empirical comparison show that the most useful of these is a heuristic that estimate the mutual information introduced between factor by one step of belief propagation in addition to feature computed over entire factor for efficiency we explored score computed over pair of variable we present search method that use these feature pairwise and not to find a factorization and we compare their result on several datasets automatic factorization extends the applicability of factored inference to large complex model that are undesirable to factor by hand moreover test on real dbns show that automatic factorization can achieve significantly lower error in some case 
a novel semi supervised learning approach is proposed based on a linear neighborhood model which assumes that each data point can be linearly reconstructed from it neighborhood our algorithm named linear neighborhood propagation lnp can propagate the label from the labeled point to the whole dataset using these linear neighborhood with sufficient smoothness we also derive an easy way to extend lnp to out of sample data promising experimental result are presented for synthetic data digit and text classification task 
we derive algorithm for finding a non negative n dimensional tensor factorization n ntf which includes the non negative matrix factorization nmf a a particular case when n we motivate the use of n ntf in three area of data analysis i connection to latent class model in statistic ii sparse image coding in computer vision and iii model selection problem we derive a direct positive preserving gradient descent algorithm and an alternating scheme based on repeated multiple rank problem 
advance in neural information processing system we introduce a class of mpds which greatly simplify reinforcement learning they have discrete state space and continuous control space the control have the effect of rescaling the transition probability of an underlying markov chain a control cost penalizing kl divergence between controlled and uncontrolled transition probability make the minimization problem convex and allows analytical computation of the optimal control given the optimal value function an exponential transformation of the optimal value function make the minimized bellman equation linear apart from their theoretical signicance the new mdps enable efcient approximation to traditional mdps shortest path problem are approximated to arbitrary precision with largest eigenvalue problem yielding an o n algorithm accurate approximation to generic mdps are obtained via continuous embedding reminiscent of lp relaxation in integer programming offpolicy learning of the optimal value function is possible without need for stateaction value the new algorithm z learning outperforms q learning 
linear discriminant analysis lda is a well known method for dimensionality reduction and classification lda in the binaryclass case ha been shown to be equivalent to linear regression with the class label a the output this implies that lda for binary class classification can be formulated a a least square problem previous study have shown certain relationship between multivariate linear regression and lda for the multi class case many of these study show that multivariate linear regression with a specific class indicator matrix a the output can be applied a a preprocessing step for lda however directly casting lda a a least square problem is challenging for the multi class case in this paper a novel formulation for multivariate linear regression is proposed the equivalence relationship between the proposed least square formulation and lda for multi class classification is rigorously established under a mild condition which is shown empirically to hold in many application involving high dimensional data several lda extension based on the equivalence relationship are discussed 
semi supervised learning is more powerful than supervised learning by using both labeled and unlabeled data in particular the manifold regularization framework together with kernel method lead to the laplacian svm lapsvm that ha demonstrated state of the art performance however the lapsvm solution typically involves kernel expansion of all the labeled and unlabeled example and is slow on testing moreover existing semi supervised learning method including the lapsvm can only handle a small number of unlabeled example in this paper we integrate manifold regularization with the core vector machine which ha been used for large scale supervised and unsupervised learning by using a sparsied manifold regularizer and formulating a a center constrained minimum enclosing ball problem the proposed method produce sparse solution with low time and space complexity experimental result show that it is much faster than the lapsvm and can handle a million unlabeled example on a standard pc while the lapsvm can only handle several thousand pattern 
in supervised learning scenario feature selection ha be en studied widely in the literature selecting feature in unsupervis ed learning scenario is a much harder problem due to the absence of class label that would guide the search for relevant information and almost all of previous unsupervised feature selection method are wrapper technique that require a learning algorithm to evaluate the candidate feature subset in this paper we propose a filter method for feature select ion which is independent of any learning algorithm our method can be performed in either supervised or unsupervised fashion the proposed method is based on the observation that in many real world classification pr oblems data from the same class are often close to each other the importance of a feature is evaluated by it power of locality preserving or laplacian score we compare our method with data variance unsupervised and fisher score supervised on two data set experimental result demonstrate the effectiveness and efficiency of our algorithm feature selection method can be classified into wrapper m ethods and filter method the wrapper model technique evaluate the feature using the learning algorithm that will ultimately be employed thus they wrap the selection process around the learning algorithm most of the feature selection method are wrapper method algorithm based on the filter model examine intrinsic property of the data t o evaluate the feature prior to the learning task the filter based approach almost alway s rely on the class label most commonly assessing correlation between feature and the class label in this paper we are particularly interested in the filter method some typi cal filter method include data variance pearson correlation coefficient fisher score and kolmogorov smirnov test most of the existing filter method are supervised data vari ance might be the simplest unsupervised evaluation of the feature the variance along a dimension reflects it representative power data variance can be used a a criterion for feature selection and extraction for example principal component analysis pca is a classical feature extraction method which find a set of mutually orthogonal basis function that capture the direction of maximum variance in the data 
we experimentally study on line investment algorithm first proposed by agarwal and hazan and extended by hazan et al which achieve almost the same wealth a the best constant rebalanced portfolio determined in hindsight these algorithm are the first to combine optimal logarithmic regret bound with efficient deterministic computability they are based on the newton method for offline optimization which unlike previous approach exploit second order information after analyzing the algorithm using the potential function introduced by agarwal and hazan we present extensive experiment on actual financial data these experiment confirm the theoretical advantage of our algorithm which yield higher return and run considerably faster than previous algorithm with optimal regret additionally we perform financial analysis using mean variance calculation and the sharpe ratio 
data mining technique that are successful in transaction and text data may not be simply applied to image data that contain high dimensional feature and have spatial structure it is not a trivial task to discover meaningful visual pattern in image database because the content variation and spatial dependency in the visual data greatly challenge most existing method this paper present a novel approach to coping with these difficulty for mining meaningful visual pattern specifically the novelty of this work lie in the following new contribution a principled solution to the discovery of meaningful itemsets based on frequent itemset mining a self supervised clustering scheme of the high dimensional visual feature by feeding back discovered pattern to tune the similarity measure through metric learning and a pattern summarization method that deal with the measurement noise brought by the image data the experimental result in the real image show that our method can discover semantically meaningful pattern efficiently and effectively 
p nbsp p div the network topology of neuron in the brain exhibit an abundance of feedback connection but the computational function of these feedback connection is largely unknown we present a computational theory that characterizes the gain in computational power achieved through feedback in dynamical system with fading memory it implies that many such system acquire through feedback universal computational capability for analog computing with a non fading memory in particular we show that feedback enables such system to process time varying input stream in diverse way according to rule that are implemented through internal state of the dynamical system in contrast to previous attractor based computational model for neural network these flexible internal state are em high dimensional em attractor of the circuit dynamic that still allow the circuit state to absorb new information from online input stream in this way one arrives at novel model for working memory integration of evidence and reward expectation in cortical circuit we show that they are applicable to circuit of conductance based hodgkin huxley hh neuron with high level of noise that reflect experimental data on in vivo condition div p nbsp p 
the notion of algorithmic stability ha been used effectively in the past to derive tight generalization bound a key advantage of these bound is that they are designed for specific learning algorithm exploiting their particular property but a in much of learning theory existing stability analysis and bound apply only in the scenario where the sample are independently and identically distributed i i d in many machine learning application however this assumption doe not hold the observation received by the learning algorithm often have some inherent temporal dependence which is clear in system diagnosis or time series prediction problem this paper study the scenario where the observation are drawn from a stationary mixing sequence which implies a dependence between observation that weaken over time it prof novel stability based generalization bound that hold even with this more general setting these bound strictly generalize the bound given in the i i d case it also illustrates their application in the case of several general class of learning algorithm including support vector regression and kernel ridge regression 
kernel learning play an important role in many machine learning task however algorithm for learning a kernel matrix often scale poorly with running time that are cubic in the number of data point in this paper we propose efficient algorithm for learning low rank kernel matrix our algorithm scale linearly in the number of data point and quadratically in the rank of the kernel we introduce and employ bregman matrix divergence for rank deficient matrix these divergence are natural for our problem since they preserve the rank a well a positive semi definiteness of the kernel matrix special case of our framework yield faster algorithm for various existing kernel learning problem experimental result demonstrate the effectiveness of our algorithm in learning both low rank and full rank kernel 
a discriminative method is proposed for learning monotonic transformation of the training data while jointly estimating a large margin classier in many domain such a document classication image histogram classication and gene microarray experiment xed monotonic transformation can be useful a a preprocessing step however most classiers only explore these transformation through manual trial and error or via prior domain knowledge the proposed method learns monotonic transformation automatically while training a large margin classier without any prior knowledge of the domain a monotonic piecewise linear function is learned which transforms data for subsequent processing by a linear hyperplane classier two algorithmic implementation of the method are formalized the rst solves a convergent alternating sequence of quadratic and linear program until it obtains a locally optimal solution an improved algorithm is then derived using a convex semidenite relaxation that overcomes initialization issue in the greedy optimization problem the eectiv ene of these learned transformation on synthetic problem text data and image data is demonstrated 
it is common in classification method to first place data in a vector space and then learn decision boundary we propose reversing that process for fixed decision boundary we learn the location of the data this way we i do not need a metric or even stronger structure pairwise dissimilarity suffice and additionally ii produce low dimensional embeddings that can be analyzed visually we achieve this by combining an entropy based embedding method with an entropy based version of semi supervised logistic regression we present result for clustering and semi supervised classification 
we discus our experience in analyzing customer support issue from the unstructured free text field of technical support call log the identification of frequent issue and their accurate quantification is essential in order to track aggregate cost broken down by issue type to appropriately target engineering resource and to provide the best diagnosis support and documentation for most common issue we present a new set of technique for doing this efficiently on an industrial scale without requiring manual coding of call in the call center our approach involves a new text clustering method to identify common and emerging issue a method to rapidly train large number of categorizers in a practical interactive manner and a method to accurately quantify category even in the face of inaccurate classification and training set that necessarily cannot match the class distribution of each new month s data we present our methodology and a tool we developed and deployed that us these method for tracking ongoing support issue and discovering emerging issue at hp 
in this paper we propose a new basis selection criterion for building sparse gp regression model that provides promising gain in accuracy a well a efficiency over previous method our algorithm is much faster than that of smola and bartlett while in generalization it greatly outperforms the information gain approach proposed by seeger et al especially on the quality of predictive distribution 
predicting the value of continuous variable a a function of several independent variable is one of the most important problem for data mining a very large number of regression method both parametric and nonparametric have been proposed in the past however since the list is quite extensive and many of these model make rather explicit strong yet different assumption about the type of applicable problem and involve a lot of parameter and option choosing the appropriate regression methodology and then specifying the parameter value is a none trivial sometimes frustrating task for data mining practitioner choosing the inappropriate methodology can have rather disappointing result this issue is against the general utility of data mining software for example linear regression method are straightforward and well understood however since the linear assumption is very strong it performance is compromised for complicated non linear problem kernel based method perform quite well if the kernel function are selected correctly in this paper we propose a straightforward approach based on summarizing the training data using an ensemble of random decision tree it requires very little knowledge from the user yet is applicable to every type of regression problem that we are currently aware of we have experimented on a wide range of problem including those that parametric method performwell a large selection of benchmark datasets for nonparametric regression a well a highly non linear stochastic problem our result are either significantly better than or identical to many approach that are known to perform well on these problem 
the goal of active learning is to select the most informative example for manual labeling most of the previous study in active learning have focused on selecting a single unlabeled example in each iteration this could be inefficient since the classification model ha to be retrained for every labeled example in this paper we present a framework for batch mode active learning that applies the fisher information matrix to select a number of informative example simultaneously the key computational challenge is how to efficiently identify the subset of unlabeled example that can result in the largest reduction in the fisher information to resolve this challenge we propose an efficient greedy algorithm that is based on the property of submodular function our empirical study with five uci datasets and one real world medical image classification show that the proposed batch mode active learning algorithm is more effective than the state of the art algorithm for active learning 
empirical risk minimization offer well known learning guarantee when training and test data come from the same domain in the real world though we often wish to adapt a classifier from a source domain with a large amount of training data to different target domain with very little training data in this work we give uniform convergence bound for algorithm that minimize a convex combination of source and target empirical risk the bound explicitly model the inherent trade off between training on a large but inaccurate source data set and a small but accurate target training set our theory also give result when we have multiple source domain each of which may have a different number of instance and we exhibit case in which minimizing a non uniform combination of source risk can achieve much lower target error than standard empirical risk minimization 
the maximum margin planning mmp ratliff et al algorithm solves imitation learning problem by learning linear mapping from feature to cost function in a planning domain the learned policy is the result of minimum cost planning using these cost function these mapping are chosen so that example policy or trajectory given by a teacher appear to be lower cost with a lossscaled margin than any other policy for a given planning domain we provide a novel approach mmpboost based on the functional gradient descent view of boosting mason et al friedman a that extends mmp by boosting in new feature this approach us simple binary classification or regression to improve performance of mmp imitation learning and naturally extends to the class of structured maximum margin prediction problem taskar et al our technique is applied to navigation and planning problem for outdoor mobile robot and robotic legged locomotion 
graph clustering also called graph partitioning clustering the node of a graph is an important problem in diverse data mining application traditional approach involve optimization of graph clustering objective such a normalized cut or ratio association spectral method are widely used for these objective but they require eigenvector computation which can be slow recently graph clustering with a general cut objective ha been shown to be mathematically equivalent to an appropriate weighted kernel k mean objective function in this paper we exploit this equivalence to develop a very fast multilevel algorithm for graph clustering multilevel approach involve coarsening initial partitioning and refinement phase all of which may be specialized to different graph clustering objective unlike existing multilevel clustering approach such a metis our algorithm doe not constrain the cluster size to be nearly equal our approach give a theoretical guarantee that the refinement step decrease the graph cut objective under consideration experiment show that we achieve better final objective function value a compared to a state of the art spectral clustering algorithm on a series of benchmark test graph with up to thirty thousand node and one million edge our algorithm achieves lower normalized cut value in of our experiment and higher ratio association value in of our experiment furthermore on large graph our algorithm is significantly faster than spectral method finally our algorithm requires far le memory than spectral method we cluster a million node movie network into cluster which due to memory requirement cannot be done directly with spectral method 
multiple visual cue are used by the visual system to analyze a scene achromatic cue include luminance texture contrast and motion singlecell recording have shown that the mammalian visual cortex contains neuron that respond similarly to scene structure e g orientation of a boundary regardless of the cue type conveying this information this paper show that cue invariant response property of simpleand complex type cell can be learned from natural image data in an unsupervised manner in order to do this we also extend a previous conceptual model of cue invariance so that it can be applied to model simpleand complex cell response our result relate cue invariant response property to natural image statistic thereby showing how the statistical modeling approach can be used to model processing beyond the elemental response property visual neuron this work also demonstrates how to learn from natural image data more sophisticated feature detector than those based on change in mean luminance thereby paving the way for new data driven approach to image processing and computer vision 
in many different application area e g sensor database location based service or face recognition system distance between odjects have to be computed based on vague and uncertain data commonly the distance between these uncertain object description are expressed by one numerical distance value based on such single valued distance function standard data mining algorithm can work without any change in this paper we propose to express the similarity between two fuzzy object by distance probability function these fuzzy distance function assign a probability value to each possible distance value by integrating these fuzzy distance function directly into data mining algorithm the full information provided by these function is exploited in order to demonstrate the benefit of this general approach we enhance the density based clustering algorithm dbscan so that it can work directly on these fuzzy distance function in a detailed experimental evaluation based on artificial and real world data set we show the characteristic and benefit of our new approach 
abstract we describe semi markov conditional random field semi cr f a conditionally trained version of semi markov chain intuitively a semicrf on an input sequence x output a segmentation of x in which label are assigned to segment i e subsequence of x rather than to individual element xi of x importantly feature for semi crfs can measure property of segment and transition within a segment can be non markovian in spite of this additional power exact learning and inference algorithm for semi crfs are polynomial time often only a small constant factor slower than conventional crfs in experiment on five named entity recognition problem semi crfs genus lly outperform conventional crfs 
relational graph are widely used in modeling large scale network such a biological network and social network in this kind of graph connectivity becomes critical in identifying highly associated group and cluster in this paper we investigate the issue of mining closed frequent graph with connectivity constraint in massive relational graph where each graph ha around k node and m edge we adopt the concept of edge connectivity and apply the result from graph theory to speed up the mining process two approach are developed to handle different mining request closecut a pattern growth approach and splat a pattern reduction approach we have applied these method in biological datasets and found the discovered pattern interesting 
a first step in postulating a theory of mobile learning is to distinguish what is special about mobile learning compared to other type of learning activity an obvious yet essential difference is that it start from the assumption that learner are continually on the move we learn across space a we take idea and learning resource gained in one location and apply or develop them in another we learn across time by revisiting knowledge that wa gained earlier in a different context and more broadly through idea and strategy gained in early year providing a framework for a lifetime of learning we move from topic to topic managing a range of personal learning project rather than following a single curriculum we also move in and out of engagement with technology for example a we enter and leave cellphone coverage to portray learning a a labile activity is not to separate it from other form of educational activity since some aspect of informal and workplace learning are fundamentally mobile in the way outlined above even learner within a school will move from room to room and shift from topic to topic rather it illuminates existing practice of learning from a new angle by placing mobility of learning a the object of analysis we may understand better how knowledge and skill can be transferred across context such a home and school how learning can be managed across life transition and how new technology can be designed to support a society in which people on the move increasingly try to cram learning into the interstice of daily life second a theory of mobile learning must therefore embrace the considerable learning that occurs outside classroom and lecture hall a people initiate and structure their activity to enable educational process and outcome a study by vavoula vavoula of everyday adult learning found that of the reported learning episode took place at home or in the learner s own office at the workplace i e at the learner s usual environment the rest occurred in the workplace outside the office outdoors in a friend s house or at place of leisure other location reported included place of worship the doctor s 
it is often expensive to acquire data in real world data mining application most previous data mining and machine learning research however assumes that a fixed set of training example is given in this paper we propose an online cost sensitive framework that allows a learner to dynamically acquire example a it learns and to decide the ideal number of example needed to minimize the total cost we also propose a new strategy for partial example acquisition pa in which the learner can acquire example with a subset of attribute value to reduce the data acquisition cost experiment on uci datasets show that the new pa strategy is an effective method in reducing the total cost for data acquisition 
receiver operator characteristic roc curve are commonly used to present result for binary decision problem in machine learning however when dealing with highly skewed datasets precision recall pr curve give a more informative picture of an algorithm s performance we show that a deep connection exists between roc space and pr space such that a curve dominates in roc space if and only if it dominates in pr space a corollary is the notion of an achievable pr curve which ha property much like the convex hull in roc space we show an efficient algorithm for computing this curve finally we also note difference in the two type of curve are significant for algorithm design for example in pr space it is incorrect to linearly interpolate between point furthermore algorithm that optimize the area under the roc curve are not guaranteed to optimize the area under the pr curve 
semi supervised svms s vm attempt to learn low density separator by maximizing the margin over labeled and unlabeled example the associated optimization problem is non convex to examine the full potential of s vms modulo local minimum problem in current implementation we apply branch and bound technique for obtaining exact globally optimal solution empirical evidence suggests that the globally optimal solution can return excellent generalization performance in situation where other implementation fail completely while our current implementation is only applicable to small datasets we discus variant that can potentially lead to practically useful algorithm 
we provide a framework to exploit dependency among arm in multi armed bandit problem when the dependency are in the form of a generative model on cluster of arm we find an optimal mdp based policy for the discounted reward case and also give an approximation of it with formal error guarantee we discus lower bound on regret in the undiscounted reward scenario and propose a general two level bandit policy for it we propose three different instantiation of our general policy and provide theoretical justification of how the regret of the instantiated policy depend on the characteristic of the cluster finally we empirically demonstrate the efficacy of our policy on large scale real world and synthetic data and show that they significantly outperform classical policy designed for bandit with independent arm 
rosetta is one of the leading algorithm for protein structure prediction today it is a monte carlo energy minimization method requiring many random restarts to find structure with low energy in this paper we present a resampling technique for structure prediction of small alpha beta protein using rosetta from an initial round of rosetta sampling we learn property of the energy landscape that guide a subsequent round of sampling toward lower energy structure rather than attempt to fit the full energy landscape we use feature selection method both l regularized linear regression and decision tree to identify structural feature that give rise to low energy we then enrich these structural feature in the second sampling round result are presented across a benchmark set of nine small alpha beta protein demonstrating that our method seldom impair and frequently improve rosetta s performance 
we study an approach for performing concurrent activity in markov decision process mdps based on the coarticulation framework we assume that the agent ha multiple degree of freedom dof in the action space which enables it to perform activity simultaneously we demonstrate that one natural way for generating concurrency in the system is by coarticulating among the set of learned activity available to the agent in general due to the multiple dof in the system often there exists a redundant set of admissible sub optimal policy associated with each learned activity such flexibility enables the agent to concurrently commit to several subgoals according to their priority level given a new task defined in term of a set of prioritized subgoals we present efficient approximate algorithm for computing such policy and for generating concurrent plan we also evaluate our approach in a simulated domain 
the increasingly popular independent component analysis ica may only be applied to data following the generative ica model in order to guarantee algorithmindependent and theoretically valid result subspace ica model generalize the assumption of component independence to independence between group of component they are attractive candidate for dimensionalit y reduction method however are currently limited by the assumption of equal group size or le general semi parametric model by introducing the concept of irreducible independent subspace or component we present a generalization to a parameter free mixture model moreover we relieve the condition of at most one gaussian by including previous result on non gaussian component analysis after introducing this general model we discus joint block diagonalization with unknown block size on which we base a simple extension of jade to algorithmically perform the subspace analysis simulation confirm the feasibility of the algorithm 
perceptual bistability refers to the phenomenon of spontaneously switching between two or more interpretation of an image under continuous viewing although switching behavior is increasingly well characterized the origin remain elusive we propose that perceptual switching naturally arises from the brain s search for best interpretation while performing bayesian inference in particular we propose that the brain explores a posterior distribution over image interpretationsat a rapid time scale via a sampling likeprocessandupdatesits interpretation when a sampled interpretation is better than the discounted value of it current interpretation we formalize the theory explicitlyderive switching rate distribution and discus qualitative property of the theory including the effect of change in the posterior distribution on switching rate finally prediction of the theory are shown to be consistent with measured change in human switching dynamic to necker cube stimulus induced by context 
we use data mining and machine learning technique to predict upcoming period of high utilization or poor performance in enterprise system the abundant data available and complexity of these system defies human characterization or static model and make the task suitable for data mining technique we formulate the problem a one of classification given current and past information about the system s behavior can we forecast whether the system will meet it performance target over the next hour using real data gathered from several enterprise system in hewlett packard we compare several approach ranging from time series to bayesian network besides establishing the predictive power of these approach our study analyzes three dimension that are important for their application a a stand alone tool first it quantifies the gain in accuracy of multivariate prediction method over simple statistical univariate method second it quantifies the variation in accuracy when using different class of system and workload feature third it establishes that model induced using combined data from various system generalize well and are applicable to new system enabling accurate prediction on system with insufficient historical data together this analysis offer a promising outlook on the development of tool to automate assignment of resource to stabilize performance e g adding server to a cluster and allow opportunistic job scheduling e g backup or virus scan 
we introduce a technique for dimensionality estimation based on the notion of quantization dimension which connects the asymptotic optimal quantization error for a probability distribution on a mani fold to it intrinsic dimension the definition of quantization dimension yie lds a family of estimation algorithm whose limiting case is equivalent to a recent method based on packing number using the formalism of high rate vector quantization we address issue of statistical con istency and analyze the behavior of our scheme in the presence of noise 
the method of stable random projection is a useful tool for efciently computing the l norm and distance in massive data in one pas consider a data matrix a rn d if we multiply a with a projection matrix r rd k k d whose entry are i i d sample of an stable distribution then the projected matrix b a r rn k contains enough information to approximately recover the l property in a we propose very sparse stable random projection by replacing the stable distribution with a much simpler mixture of a symmetric pareto distribution with probability and a point mass at the origin with probability this lead to a signicant fold speedup for small when computing b a r and a fold cost reduction in storing r by analyzing the convergence we show that in reasonable datasets often can be very small e g d without hurting the estimation accuracy 
we present a generalization of temporal difference network to include temporally abstract option on the link of the question network temporal difference td network have been proposed a a way of representing and learning a wide variety of prediction about the interaction between an agent and it environment these prediction are compositional in that their target are defined in term of other prediction and subjunctive in that that they are about what would happen if an action or sequence of action were taken in conventional td network the inter related prediction are at successive time step and contingent on a single action here we generalize them to accommodate extended time interval and contingency on whole way of behaving our generalization is based on the option framework for temporal abstraction the primary contribution of this paper is to introduce a new algorithm for intra option learning in td network with function approximation and eligibility trace we present empirical example of our algorithm s effectiveness and of the greater representational expressiveness of temporallyabstract td network the primary distinguishing feature of temporal difference td network sutton tanner is that they permit a general compositional specification of the goal of learning the goal of learning are thought of a predictive question being asked by the agent in the learning problem such a what will i see if i step forward and look right or if i open the fridge will i see a bottle of beer seeing a bottle of beer is of course a complicated perceptual act it might be thought of a obtaining a set of prediction about what would happen if certain reaching and grasping action were taken about what would happen if the bottle were opened and turned upside down and of what the bottle would look like if viewed from various angle to predict seeing a bottle of beer is thus to make a prediction about a set of other prediction the target for the overall prediction is a composition in the mathematical sense of the first prediction with each of the other prediction td network are the first framework for representing the goal of predictive learning in a compositional machine accessible form each node of a td network represents an individual question something to be predicted and ha associated with it a value representing an answer to the question a prediction of that something the question are represented by a set of directed link between node if node is linked to node then node rep 
patch based appearance model are used in a wide range of computer vision application to learn such model it ha previously been necessary to specify a suitable set of patch size and shape by hand in the jigsaw model presented here the shape size and appearance of patch are learned automatically from the repeated structure in a set of training image by learning such irregularly shaped jigsaw piece we are able to discover both the shape and the appearance of object part without supervision when applied to face image for example the learned jigsaw piece are surprisingly strongly associated with face part of different shape and scale such a eye nose eyebrow and cheek to name a few we conclude that learning the shape of the patch not only improves the accuracy of appearance based part detection but also allows for shape based part detection this enables part of similar appearance but different shape to be distinguished for example while forehead and cheek are both skin colored they have markedly different shape 
we consider the problem of detecting anomaly in high aritycategorical datasets in most application anomaly are defined a datapoints that are abnormal quite often we have access to data which consists mostly of normal record a long with a small percentage of unlabelled anomalous record we are interested in the problem of unsupervised anomaly detection where we use the unlabelled data for training and detect record that do not follow the definition of normality a standard approach is to create a model of normal data and compare test record against it a probabilistic approach build a likelihood model from the training data record are tested for anomaly based on the complete record likelihood given the probability model for categorical attribute bayes net give a standard representation of the likelihood while this approach is good at finding outlier in the dataset it often tends to detect record with attribute value that are rare sometimes just detecting rare value of an attribute is not desired and such outlier are not considered a anomaly in that context we present an alternative definition of anomaly and propose an approach of comparing against marginal distribution of attribute subset we show that this is a more meaningful way of detecting anomaly and ha a better performance over semi synthetic a well a real world datasets 
this paper present a non asymptotic statistical analysis of kernel pca with a focus different from the one proposed in previous work on this topic here instead of considering the reconstruction error of kpca we are interested in approximation error bound for the eigenspaces themselves we prove an upper bound depending on the spacing between eigenvalue but not on the dimensionality of the eigenspace a a consequence this allows to infer stability result for these estimated space 
string data ha recently become important because of it use in a number of application such a computational and molecular biology protein analysis and market basket data in many case these string contain a wide variety of substructure which may have physical significance for that application for example such substructure could represent important fragment of a dna string or an interesting portion of a fraudulent transaction in such a case it is desirable to determine the identity location and extent of that substructure in the data this is a much more difficult generalization of the classification problem since the latter problem label entire string rather than deal with the more complex task of determining string fragment with a particular kind of behavior the problem becomes even more complicated when different kind of substring show complicated nesting pattern therefore we define a somewhat different problem which we refer to a the generalized classification problem we propose a scalable approach based on hidden markov model for this problem we show how to implement the generalized string classification procedure for very large data base and data stream we present experimental result over a number of large data set and data stream 
in this paper we investigate multi task learning in the context of gaussian process gp we propose a model that learns a shared covariance function on input dependent feature and a free form covariance matrix over task this allows for good flexibility when modelling inter task dependency while avoiding the need for large amount of data for training we show that under the assumption of noise free observation and a block design prediction for a given task only depend on it target value and therefore a cancellation of inter task transfer occurs we evaluate the benefit of our model on two practical application a compiler performance prediction problem and an exam score prediction task additionally we make use of gp approximation and property of our model in order to provide scalability to large data set 
in supervised learning there is a typical presumption that the training and test point are taken from the same distribution in practice this assumption is commonly violated the situation where the training and test data are from different distribution is called covariate shift recent work ha examined technique for dealing with covariate shift in term of minimisation of generalisation error a yet the literature lack a bayesian generative perspective on this problem this paper tackle this issue for regression model recent work on covariate shift can be understood in term of mixture regression using this view we obtain a general approach to regression under covariate shift which reproduces previous work a a special case the main advantage of this new formulation over previous model for covariate shift are that we no longer need to presume the test and training density are known the regression and density estimation are combined into a single procedure and previous method are reproduced a special case of this procedure shedding light on the implicit assumption the method are making 
we present a series of theoretical argument supporting the claim that a large class of modern learning algorithm that rely solely on the smoothness prior with similarity between example expressed with a local kernel are sensitive to the curse of dimensionality or more precisely to the variability of the target our discussion cover supervised semisupervised and unsupervised learning algorithm these algorithm are found to be local in the sense that crucial property of the learned function at x depend mostly on the neighbor of x in the training set this make them sensitive to the curse of dimensionality well studied for classical non parametric statistical learning we show in the case of the gaussian kernel that when the function to be learned ha many variation these algorithm require a number of training example proportional to the number of variation which could be large even though there may exist short description of the target function i e their kolmogorov complexity may be low this suggests that there exist non local learning algorithm that at least have the potential to learn about such structured but apparently complex function because locally they have many variation while not using very specific prior domain knowledge 
this paper introduces kernel on attributed pointsets whi ch are set of vector embedded in an euclidean space the embedding give the notion of neighborhood which is used to define positive semidefinite kernel on point set two novel kernel on neighborhood are proposed one evaluating the attribute similarity and the other evaluating shape similarity shape similarity fu nction is motivated from spectral graph matching technique the kernel are tested on three real life application face recognition photo album tagging and shot annotation in video sequence with encouraging result 
clustering is a fundamental problem in machine learning and ha been approached in many way two general and quite different approach include iteratively fitting a mixture model e g using em and linking together pair of training case that have high affinity e g using spectral method pair wise clustering algorithm need not compute sufficient statistic and avoid poor solution by directly placing similar example in the same cluster however many application require that each cluster of data be accurately described by a prototype or model so affinity based clustering and it benefit cannot be directly realized we describe a technique called affinity propagation which combine the advantage of both approach the method learns a mixture model of the data by recursively propagating affinity message we demonstrate affinity propagation on the problem of clustering image patch for image segmentation and learning mixture of gene expression model from microarray data we find that affinity propagation obtains better solution than mixture of gaussians the k medoids algorithm spectral clustering and hierarchical clustering and is both able to find a pre specified number of cluster and is able to automatically determine the number of cluster interestingly affinity propagation can be viewed a belief propagation in a graphical model that account for pairwise training case likelihood function and the identification of cluster center 
mining frequent closed itemsets provides complete and condensed information for non redundant association rule generation extensive study have been done on mining frequent closed itemsets but they are mainly intended for traditional transaction database and thus do not take data stream characteristic into consideration in this paper we propose a novel approach for mining closed frequent itemsets over data stream it computes and maintains closed itemsets online and incrementally and can output the current closed frequent itemsets in real time based on user specified threshold experimental result show that our proposed method is both time and space efficient ha good scalability a the number of transaction processed increase and adapts very rapidly to the change in data stream 
functional magnetic resonance imaging fmri provides an unprecedented window into the complex functioning of the human brain typically detailing the activity of thousand of voxels during hundred of sequential time point unfortunately the interpretation of fmri is complicated due both to the relatively unknown connection between the hemodynamic response and neural activity and the unknown spatiotemporal characteristic of the cognitive pattern themselves here we use data from the experience based cognition competition to explore the combination of local method of prediction with various technique of dimensionality reduction we find effective low dimensional model based on the principal component of cognitive activity in classically defined anatomical region the brodmann area for some of the stimulus the top predictive region were stable across subject and episode including wernicke area for verbal instruction visual cortex for facial and body feature and visual temporal region brodmann area for velocity these interpretation and the relative simplicity of our approach provide a transparent and conceptual basis upon which to build more sophisticated technique for fmri decoding to our knowledge this is the first time that classical area have been used in fmri for an effective prediction of complex natural experience 
in this paper we propose a new method called prototype ranking pr designed for the stock selection problem pr take into account the huge size of real world stock data and applies a modified competitive learning technique to predict the rank of stock the primary target of pr is to select the top performing stock among many ordinary stock pr is designed to perform the learning and testing in a noisy stock sample set where the top performing stock are usually the minority the performance of pr is evaluated by a trading simulation of the real stock data each week the stock with the highest predicted rank are chosen to construct a portfolio in the period of pr s portfolio earns a much higher average return a well a a higher risk adjusted return than cooper s method which show that the pr method lead to a clear profit improvement 
we present an algorithm for learning a quadratic gaussian metric mahalanobis distance for use in classification task our metho d relies on the simple geometric intuition that a good metric is one under which point in the same class are simultaneously near each other and far from point in the other class we construct a convex optimization problem whose solution generates such a metric by trying to collapse all ex amples in the same class to a single point and push example in other class infinitely far away we show that when the metric we learn is used in simple classifier it yield substantial improvement over standard a lternatives on a variety of problem we also discus how the learned metric may be used to obtain a compact low dimensional feature representation of the original input space allowing more efficient classificatio n with very little reduction in performance 
in this paper we study asymmetric proximity measure on directed graph which quantify the relationship between two node or two group of node the measure are useful in several graph mining task including clustering link prediction and connection subgraph discovery our proximity measure is based on the conceptof escape probability this way we strive to summarize the multiple facet of node proximity while avoiding some of the pitfall to which alternative proximity measure are susceptible a unique feature of the measure is accounting for the underlying directional information we put a special emphasis on computational efficiency and develop fast solution that are applicable in several setting our experimental study show the usefulness of our proposed direction aware proximity method for several application and that our algorithm achieve a significant speedup up to x over straight forward implementation 
large repository of source code create new challenge and opportunity for statistical machine learning here we first develop sourcerer an infrastructure for the automated crawling parsing and database storage of open source software sourcerer allows u to gather internet scale source code for instance in one experiment we gather java project from sourceforge and apache totaling over million line of code from developer simple statistical analysis of the data first reveal robust power law behavior for package sloc and lexical containment distribution we then develop and apply unsupervised author topic probabilistic model to automatically discover the topic embedded in the code and extract topic word and author topic distribution in addition to serving a a convenient summary for program function and developer activity these and other related distribution provide a statistical and information theoretic basis for quantifying and analyzing developer similarity and competence topic scattering and document tangling with direct application to software engineering finally by combining software textual content with structural information captured by our coderank approach we are able to significantly improve software retrieval performance increasing the auc metric to roughly better than previous approach based on text alone supplementary material may be found at http sourcerer ic uci edu nip nip html 
we derive an equation for temporal difference learning from statistical principle specifically we start with the variational principle and th en bootstrap to produce an updating rule for discounted state value estimate the resulting equation is similar to the standard equation for temporal difference le arning with eligibility trace so called td however it lack the parameter that specifies the learning rate in the place of this free parameter there is no w an equation for the learning rate that is specific to each state transition we ex perimentally test this new learning rule against td and find that it offer superior performance in various setting finally we make some preliminary investigation into how to extend our new temporal difference algorithm to reinforcement learning to do this we combine our update equation with both watkins q and sarsa and find that it again offer superior performance without a lear ning rate parameter 
chemical reaction network by which individual cell gather and process information about their chemical environment have been dubbed signal transduction network despite this suggestive terminology there have been few attempt to analyze chemical signaling system with the quantitative tool of information theory gradient sensing in the social amoeba dictyostelium discoideum is a well characterized signal transduction system in which a cell estimate the direction of a source of diffusing chemoattractant molecule based on the spatiotemporal sequence of ligand receptor binding event at the cell membrane using monte carlo technique mcell we construct a simulation in which a collection of individual ligand particle undergoing brownian diffusion in a three dimensional volume interact with receptor on the surface of a static amoeboid cell adapting a method for estimation of spike train entropy described by victor originally due to kozachenko and leonenko we estimate lower bound on the mutual information between the transmitted signal direction of ligand source and the received signal spatiotemporal pattern of receptor binding unbinding event hence we provide a quantitative framework for addressing the question how much could the cell know and when could it know it we show that the time course of the mutual information between the cell s surface receptor and the unknown gradient direction is consistent with experimentally measured cellular response time we find that the acquisition of directional information depends strongly on the time constant at which the intracellular response is filtered 
in this paper we present a subspace method for learning nonlinear dynamical system based on stochastic realization in which state vector are chosen using kernel canonical correlation analysis and then state spa ce system are identified through regression with the state vector we construct the theoretical underpinning and derive a concrete algorithm for nonlinear identific ation the obtained algorithm need no iterative optimization procedure and can be implemented on the basis of fast and reliable numerical scheme the simulation result show that our algorithm can express dynamic with a high degree of accuracy 
we discus a method for obtaining a subject s a priori belief from his her behavior in a psychophysics context under the assumption that the behavior is nearly optimal from a bayesian perspective the method is nonparametric in the sense that we do not assume that the prior belongs to any fixed class of distribution e g gaus sian despite this increased generality the method is relatively simple to implement being based in the simplest case on a linear programming algorithm and more generally on a straightforward maximum likelihood or maximum a posteriori formulation which turn out to be a convex optimization problem with no non global local maximum in many important case in addition we develop method for analyzing the uncertainty of these estimate we demonstrate the accuracy of the method in a simple simulated coin flipping setting in particular the method is able to p recisely track the evolution of the subject s posterior distribution a mo re and more data are observed we close by briefly discussing an interesting c onnection to recent model of neural population coding 
traditional analysis method for single trial classificat ion of electroencephalography eeg focus on two type of paradigm phase locked method in which the amplitude of the signal is used a the feature for classification e g event related potential and second order metho d in which the feature of interest is the power of the signal e g event related de synchronization the procedure for deciding which paradigm to use is ad hoc and is typically driven by knowledge of the underlying neurophysiology here we propose a principled method based on a bilinear model in which the algorithm simultaneously learns the best first and second order spatial and temporal feature for classification of eeg the method is demonstrated on simulated data a well a on eeg taken from a benchmark data used to test classification algorithm for brain computer interface 
we consider spectral clustering and transductive inference for data with multiple view a typical example is the web which can be described by either the hyperlink between web page or the word occurring in web page when each view is represented a a graph one may convexly combine the weight matrix or the discrete laplacians for each graph and then proceed with existing clustering or classification technique such a solution might sound natural but it underlying principle is not clear unlike this kind of methodology we develop multiview spectral clustering via generalizing the normalized cut from a single view to multiple view we further build multiview transductive inference on the basis of multiview spectral clustering our framework lead to a mixture of markov chain defined on every graph the experimental evaluation on real world web classification demonstrates promising result that validate our method 
we consider a setting for discriminative semi supervised learning where unlabeled data are used with a generative model to learn effective feature representation for discriminative training within this framework we revisit the two view feature generation model of co training and prove that the optimum predictor can be expressed a a linear combination of a few feature constructed from unlabeled data from this analysis we derive method that employ two view but are very different from co training experiment show that our approach is more robust than co training and em under various data generation condition 
the growth of web and fundamental theoretical breakthrough have led to an avalanche of interest in social network this paper focus on the problem of modeling how social network accomplish task through peer production style collaboration we propose a general interaction model for the underlying social network and then a specific model ilink for social search and message routing a key contribution here is the development of a general learning framework for making such online peer production system work at scale the ilink model ha been used to develop a system for faq generation in a social network faqtory and experience with it application in the context of a full scale learning driven workflow application calo is reported we also discus method of adapting ilink technology for use in military knowledge sharing portal and other message routing system finally the paper show the connection of ilink to sqm a theoretical model for social search that is a generalization of markov decision process and the popular pagerank model 
following the recent devastating blackout in north america uk and italy blackout prevention ha attracted significant attention though it is known a a notoriously difficult task to prevent the blackout it is essential to accurately predict the instable status of power network component in the large scale power network however existing analysis tool fail to perform accurate and in time prediction of component instability because of the sophisticated structure of real world power network and the huge amount of system variable to be analyzed to prevent the blackout we need an accurate and efficient method that a can discover interesting feature and pattern relevant to the blackout from the highly complex structure and ten thousand of system variable of a power network and b can give accurate and fast prediction of system instability whenever required so that the network operator can take necessary action in time in this paper we report our tool developed for power network instability prediction the proposed method consists of two major stage in the first stage a novel type of pattern namely local correlation network pattern lcnp is mined from the structure and system variable of the power network correlation rule which are useful for the network operator to locate potentially instable component can be further generated from the lcnp in the second stage a kernel based network classification method is developed to predict the system instability by testing on a real world power network the new england system we demonstrate that the proposed tool is effective in predicting system instability and thus highly useful for blackout prevention 
we present variable influence structure analysis an algorithm that dynamically performs hierarchical decomposition of factored markov decision process our algorithm determines causal relationship between state variable and introduces temporally extended action that cause the value of state variable to change each temporally extended action corresponds to a subtask that is significantly easier to solve than the overall task result from experiment show great promise in scaling to larger task 
in this paper we present a new interpretation of adaboost ecc and adaboost oc we show that adaboost ecc performs stage wise functional gradient descent on a cost function defined in the domain of margin value and that adaboost oc is a shrinkage version of adaboost ecc these finding strictly explain some property of the two algorithm the gradient minimization formulation of adaboost ecc allows u to derive a new algorithm referred to a adaboost secc by explicitly exploiting shrinkage a regularization in adaboost ecc experiment on diverse database confirm our theoretical finding empirical result show that adaboost secc performs significantly better than adaboost ecc and adaboost oc 
autonomous helicopter flight is widely regarded to be a highl y challenging control problem this paper present the first successful autonomou s completion on a real rc helicopter of the following four aerobatic maneuver forward flip and sideways roll at low speed tail in funnel and nose in funn el our experimental result significantly extend the state of the art in autonomo u helicopter flight we used the following approach first we had a pilot fly the hel icopter to help u find a helicopter dynamic model and a reward cost functi on then we used a reinforcement learning optimal control algorithm to fin d a controller that is optimized for the resulting model and reward function more specifically we used differential dynamic programming ddp an extension of the linear quadratic regulator lqr 
privacy preservation is an important issue in the release of data for mining purpose the k anonymity model ha been introduced for protecting individual identification recent study show that a more sophisticated model is necessary to protect the association of individual to sensitive information in this paper we propose an k anonymity model to protect both identification and relationship to sensitive information in data we discus the property of k anonymity model we prove that the optimal k anonymity problem is np hard we first present an optimal globalrecoding method for the k anonymity problem next we propose a local recoding algorithm which is more scalable and result in le data distortion the effectiveness and efficiency are shown by experiment we also describe how the model can be extended to more general case job birth postcode illness hiv hiv flu fever flu fever 
in this paper we define a family of syntactic kernel for automatic relational learning from pair of natural language sentence we provide an efficient computation of such model by optimizing the dynamic programming algorithm of the kernel evaluation experiment with support vector machine and the above kernel show the effectiveness and efficiency of our approach on two very important natural language task textual entailment recognition and question answering 
computer architect utilize simulation tool to evaluate the merit of a new design feature the time needed to adequately evaluate the tradeoff associated with adding any new feature ha become a critical issue recent work ha found that by identifying execution phase present in common workload used in simulation study we can apply clustering algorithm to significantly reduce the amount of time needed to complete the simulation our goal in this paper is to demonstrate the value of this approach when applied to the set of industry standard benchmark most commonly used in computer architecture study we also look to improve upon prior work by applying more appropriate clustering algorithm to identify phase and to further reduce simulation time we find that the phase clustering in computer architecture simulation ha many similarity to text clustering in prior work on clustering technique to reduce simulation time k mean clustering wa used to identify representative program phase in this paper we apply a mixture of multinomial to the clustering problem and show it advantage over using k mean on simulation data we have implemented these two clustering algorithm and evaluate how well they can characterize program behavior by adopting a mixture of multinomial model we find that we can maintain simulation result fidelity while greatly reducing overall simulation time we report result for a range of application taken from the spec benchmark suite 
markov logic network mlns are a statistical relational model that consists of weighted firstorder clause and generalizes first order logic and markov network the current state of the art algorithm for learning mln structure follows a top down paradigm where many potential candidate structure are systematically generated without considering the data and then evaluated using a statistical measure of their fit to the data even though this existing algorithm outperforms an impressive array of benchmark it greedy search is susceptible to local maximum or plateau we present a novel algorithm for learning mln structure that follows a more bottom up approach to address this problem our algorithm us a propositional markov network learning method to construct template network that guide the construction of candidate clause our algorithm significantly improves accuracy and learning time over the existing topdown approach in three real world domain 
beamformers are spatial filter that pas source signal in particular focused location while suppressing interference from elsewhere the widely used minimum variance adaptive beamformer mvab creates such filter using a sample covariance estimate however the quality of this estimate deteriorates when the source are correlated or the number of sample n is small herein a modified beamformer is derived that replaces this problematic sample covariance with a robust maximum likelihood estimate obtained using the relevance vector machine rvm a bayesian method for learning sparse model from possibly overcomplete feature set we prove that this substitution ha the natural ability to remove the undesirable effect of correlation or limited data when n becomes large and assuming uncorrelated source this method reduces to the exact mvab simulation using direction of arrival data support these conclusion additionally rvms can potentially enhance a variety of traditional signal processing method that rely on robust sample covariance estimate 
this paper describes a new model for human visual classification that enables the recovery of image feature that explain human subject performance on different visual classification task unlike previous method this algorithm doe not model their performance with a single linear classifier operating on raw image pixel instead it represents classification a the combination of multiple feature detector this approach extract more information about human visual classification than previous method and provides a foundation for further exploration 
we present a new and efficient semi supervised training method for parameter estimation and feature selection in conditional random field crfs in real world application such a activity recognition unlabeled sensor trace are relatively easy to obtain whereas labeled example are expensive and tedious to collect furthermore the ability to automatically select a small subset of discriminatory feature from a large pool can be advantageous in term of computational speed a well a accuracy in this paper we introduce the semi supervised virtual evidence boosting sveb algorithm for training crfs a semi supervised extension to the recently developed virtual evidence boosting veb method for feature selection and parameter learning the objective function of sveb combine the unlabeled conditional entropy with labeled conditional pseudo likelihood it reduces the overall system cost a well a the human labeling cost required during training which are both important consideration in building real world inference system experiment on synthetic data and real activity trace collected from wearable sensor illustrate that sveb benefit from both the use of unlabeled data and automatic feature selection and outperforms other semi supervised approach 
much of human knowledge is organized into sophisticated system that are often called intuitive theory we propose that intuitive theory are mentally represented in a logical language and that the subjective complexity of a theory is determined by the length of it representation in this language this complexity measure help to explain how theory are learned from relational data and how they support inductive inference about unobserved relation we describe two experiment that test our approach and show that it provides a better account of human learning and reasoning than an approach developed by goodman what is a theory and what make one theory better than another question like these are of obvious interest to philosopher of science but are also discussed by psychologist who have argued that everyday knowledge is organized into rich and complex system that are similar in many respect to scientific theory even young child for instance have systematic belief about domain including folk physic folk biology and folk psychology intuitive theory like these play many of the same role a scientific theory in particular both kind of theory are used to explain and encode observation of the world and to predict future observation this paper explores the nature use and acquisition of simple theory consider for instance an anthropologist who ha just begun to study the social structure of a remote tribe and observes that certain word are used to indicate relationship between selected pair of individual suppose that term t can be glossed a ancestor and that t can be glossed a friend the anthropologist might discover that the first term is transitive and that the second term is symmetric with a few exception suppose that termt can be glossed asdefers to and that the tribe divide into two caste such that member of the second caste defer to member of the first caste in this case the anthropologist might discover two latent concept caste and caste along with the relationship between these concept a these example suggest a theory can be defined a a system of law and concept that specify the relationship between the element in some domain we will consider how these theory are learned how they are used to encode relational data and how they support prediction about unobserved relation our approach to all three problem relies on the notion of subjective complexity we propose that theory learner prefer simple theory that people remember relational data in term of the simplest underlying theory and that people extend a partially observed data set according to the simplest theory that is consistent with their observation there is no guarantee that a single measure of subjective complexity can do all of the work that we require this paper however explores the strong hypothesis that a single measure will suffice our formal treatment of subjective complexity begin with the question of how theory are mentally represented we suggest that theory are represented in some logical language and propose a specific first order language that serf a a hypothesis about the language of thought we then pursue the idea that the subjective complexity of a theory corresponds to the length of it representation in this language our approach therefore build on the work of feldman and is related to other psychological application of the notion of kolmogorov complexity the complexity measure we describe can be used to define a probability distribution over a space of theory and we develop a model of theory acquisition by using this distribution a the prior for a bayesian learner we also 
given a set of classifier and a probability distribution over their dom ain one can define a metric by taking the distance between a pair of cla ssifiers to be the probability that they classify a random item differently we prove bound on the sample complexity of pac learning in term of the doubling dimension of this metric these bound imply known bound on the sample complexity of learning halfspaces with respect to the uniform distribution that ar e optimal up to a constant factor we prove a bound that hold for any algorithm that output a classifier with zero error whenever this is possible this bound is in term o f the maximum of the doubling dimension and the vc dimension of and strengthens the best known bound in term of the vc dimension alone we show that there is no bound on the doubling dimension in term of the vc dimension of in contrast with the metric dimension 
an increasing number of project in neuroscience requires the statistical analysis of high dimensional data set a for instance in predicting behavior from neural firing or in operating artificial device from brain recording in brain machine interface linear analysis technique remain prevalent in such case but classical linear regression approach are often numerically too fragile in high dimension in this paper we address the question of whether emg data collected from arm movement of monkey can be faithfully reconstructed with linear approach from neural activity in primary motor cortex m to achieve robust data analysis we develop a full bayesian approach to linear regression that automatically detects and excludes irrelevant feature in the data and regularizes against overfitting in comparison with ordinary least square stepwise regression partial least square lasso regression and a brute force combinatorial search for the most predictive input feature in the data we demonstrate that the new bayesian method o er a superior mixture of characteristic in term of regularization against overfitting computational e ciency and ease of use demonstrating it potential a a drop in replacement for other linear regression technique a neuroscientific result our analysis demonstrate that emg data can be well predicted from m neuron further opening the path for possible real time interface between brain and machine 
xml ha become a popular method of data representation both on the web and in database in recent year one of the reason for the popularity of xml ha been it ability to encode structural information about data record however this structural characteristic of data set also make it a challenging problem for a variety of data mining problem one such problem is that of clustering in which the structural aspect of the data result in a high implicit dimensionality of the data representation a a result it becomes more difficult to cluster the data in a meaningful way in this paper we propose an effective clustering algorithm for xml data which us substructure of the document in order to gain insight about the important underlying structure we propose new way of using multiple sub structuralinformation in xml document to evaluate the quality of intermediate cluster solution and guide the algorithm to a final solution which reflects the true structural behavior in individual partition we test the algorithm on a variety of real and synthetic data set 
we introduce a novel framework called blosom for mining frequent boolean expression over binary valued datasets we organize the space of boolean expression into four category pure conjunction pure disjunction conjunction of disjunction and disjunction of conjunction we focus on mining the simplest expression the minimal generator for each class we also propose a closure operator for each class that yield closed boolean expression blosom efficiently mine frequent boolean expression by utilizing a number of methodical pruning technique experiment showcase the behavior of blosom and an application study on a real dataset is also given 
string kernel which compare the set of all common substring between two given string have recently been proposed by vishwanathan smola surprisingly these kernel can be computed in linear time and linear space using annotated suffix tree even though in theory the suffix tree based algorithm requires o n space for an n length string in practice at least n byte are required n byte for storing the suffix tree and an additional n byte for the annotation this large memory requirement coupled with poor locality of memory access inherent due to the use of suffix tree mean that the performance of the suffix tree based algorithm deteriorates on large string in this paper we describe a new linear time yet space efficient and scalable algorithm for computing string kernel based on suffix array our algorithm is a faster and easier to implement b on the average requires only n byte of storage and c exhibit strong locality of memory access we show that our algorithm can be extended to perform linear time prediction on a test string and present experiment to validate our claim 
an essential part of an expert finding task such a matching reviewer to submitted paper is the ability to model the expertise of a person based on document we evaluate several measure of the association between an author in an existing collection of research paper and a previously unseen document we compare two language model based approach with a novel topic model author persona topic apt in this model each author can write under one or more persona which are represented a independent distribution over hidden topic example of previous paper written by prospective reviewer are gathered from the rexa database which extract and disambiguates author mention from document gathered from the web we evaluate the model using a reviewer matching task based on human relevance judgment determining how well the expertise of proposed reviewer match a submission we find that the apt topic model outperforms the other model 
we propose efficient particle smoothing method for generalized state space model particle smoothing is an expensive o n algorithm where n is the number of particle we overcome this problem by integrating dual tree recursion and fast multipole technique with forward backward smoother a new generalized two filter smoother and a maximum a posteriori map smoother our experiment show that these improvement can substantially increase the practicality of particle smoothing 
central and subspace clustering method are at the core of many segmentation problem in computer vision however both method fail to give the correct segmentation in many practical scenario e g when data point are close to the intersection of two subspace or when two cluster center in different subspace are spatially close in this paper we address these challenge by considering the problem of clustering a set of point lying in a union of subspace and distributed around multiple cluster center inside each subspace we propose a generalization of kmeans and ksubspaces that cluster the data by minimizing a cost function that combine both central and subspace distance experiment on synthetic data compare our algorithm favorably against four other clustering method we also test our algorithm on computer vision problem such a face clustering with varying illumination and video shot segmentation of dynamic scene 
bob offer a face detection web service where client can submit their image for analysis alice would very much like to use the service but is reluctant to reveal the content of her image to bob bob for his part is reluctant to release his face detector a he spent a lot of time energy and money constructing it secure multiparty computation use cryptographic tool to solve this problem without leaking any information unfortunately these method are slow to compute and we introduce a couple of machine learning technique that allow the party to solve the problem while leaking a controlled amount of information the first method is an information bottleneck variant of adaboost that let bob find a subset of feature that are enough for classifying an image patch but not enough to actually reconstruct it the second machine learning technique is active learning that allows alice to construct an online classifier based on a small number of call to bob s face detector she can then use her online classifier a a fast rejector before using a cryptographically secure classifier on the remaining image patch 
we propose a new measure of conditional dependence of random variable based on normalized cross covariance operator on reproducing kernel hilbert space unlike previous kernel dependence measure the proposed criterion doe not depend on the choice of kernel in the limit of infinite data for a wide class of kernel at the same time it ha a straightforward empirical estimate with good convergence behaviour we discus the theoretical property of the measure and demonstrate it application in experiment in this paper we propose to use the hilbert schmidt norm of the normalized conditional crosscovariance operator and show that this operator encodes the dependence structure of random variable our criterion includes a measure of unconditional dependence a a special case we prove in the limit of infinite data under assumption on the richness of the rkhs that this measure ha an explicit integral expression which depends only on the probability density of the variable despite being defined in term of kernel we also prove that it empirical estimate converges to the kernelindependent value a the sample size increase furthermore we provide a general formulation for 
motivation in the field of bioinformatics there is an emerging need to integrate all knowledge discovery step into a standardized modular framework indeed component based development can significantly enhance reusability and productivity for short timeline project with a small team we present interactive knowledge discovery and data mining ikdd an application framework written in java that wa specifically designed for these purpose result ikdd consists of a component based architecture and a web based tool for pre clinical research and prototype development the platform provides an intuitive and consistent interface to create and maintain component e g data structure algorithm and utility to load save and visualize data and pipeline the rich featured tool supply database connectivity workflow processing and rapid prototype building the architecture wa carefully designed using an object oriented approach that respect crucial goal usability openness robustness and functionality especially in the abstraction and description of the component which distinguishes it from other package ikdd is well suited to serve a a public repository of component to run scientific experiment with a high level of reproducibility and also to rapidly build prototype this paper describes the general architecture and demonstrates through example the ease by which a complex scenario implementation can be facilitated with ikdd 
we present a new semi supervised extension of discriminative random field drfs that efficiently exploit labeled and unlabeled training data to achieve improved accuracy in a variety of image processing task we formulate drf training a a form of map estimation that combine conditional loglikelihood on labeled data given a data dependent prior with a conditional entropy regularizer defined on unlabeled data although the training objective is no longer concave we develop an efficient local optimization procedure that improves standard supervised drf training we then apply semi supervised drfs to a set of image segmentation problem on synthetic and real data set and achieve significant improvement over supervised drfs in each case 
in medical diagnosis doctor often have to order set of medical test in sequence in order to make an accurate diagnosis of patient disease while doing so they have to make a trade off between the cost of the test and possible misdiagnosis in this paper we use cost sensitive learning to model this process we assume that test example new patient may contain missing value and their actual value can be acquired at cost similar to doing medical test in order to reduce misclassification error misdiagnosis we propose a novel sequential batch test algorithm that can acquire set of attribute value in sequence similar to set of medical test ordered by doctor in sequence the goal of our algorithm is to minimize the total cost i e the trade off of acquiring attribute value and misclassifications we demonstrate the effectiveness of our algorithm and show that it outperforms previous method significantly our algorithm can be readily applied in real world diagnosis task a case study on the heart disease is given in the paper 
we describe a hierarchy of motif based kernel for multiple alignment of biological sequence particularly suitable to process regulatory region of gene the kernel incorporate progressively more information with the most complex kernel accounting for a multiple alignment of orthologous region the phylogenetic tree relating the specie and the prior knowledge that relevant sequence pattern occur in conserved motif block these kernel can be used in the presence of a library of known transcription factor binding site or de novo by iterating over all k mers of a given length in the latter mode a discriminative classifier built from such a kernel not only recognizes a given class of promoter region but a a side effect simultaneously identifies a collection of relevant discriminative sequence motif we demonstrate the utility of the motif based multiple alignment kernel by using a collection of aligned promoter region from five yeast specie to recognize class of cell cycle regulated gene 
we address the issue of clustering numerical vector with a network the problem setting is basically equivalent to constrained clustering by wagstaff and cardie and semi supervised clustering by basu et al but our focus is more on the optimal combination of two heterogeneous data source an application of this setting is web page which can be numerically vectorized by their content e g term frequency and which are hyperlinked to each other showing a network another typical application is gene whose behavior can be numerically measured and a gene network can be given from another data source we first define a new graph clustering measure which we call normalized network modularity by balancing the cluster size of the original modularity we then propose a new clustering method which integrates the cost of clustering numerical vector with the cost of maximizing the normalized network modularity into a spectral relaxation problem our learning algorithm is based on spectral clustering which make our issue an eigenvalue problem and us k mean for final cluster assignment a significant advantage of our method is that we can optimize the weight parameter for balancing the two cost from the given data by choosing the minimum total cost we evaluated the performance of our proposed method using a variety of datasets including synthetic data a well a real world data from molecular biology experimental result showed that our method is effective enough to have good result for clustering by numerical vector and a network 
we introduce a flexible visual data mining framework which combine advanced projection algorithm from the machine learning domain and visual technique developed in the information visualization domain the advantage of such an interface is that the user is directly involved in the data mining process we integrate principled projection algorithm such a generative topographic mapping gtm and hierarchical gtm hgtm with powerful visual technique such a magnification factor directional curvature parallel coordinate and billboarding to provide a visual data mining framework result on a real life chemoinformatics dataset using gtm are promising and have been analytically compared with the result from the traditional projection method it is also shown that the hgtm algorithm provides additional value for large datasets the computational complexity of these algorithm is discussed to demonstrate their suitability for the visual data mining framework 
heterogeneous data co clustering ha attracted more and more attention in recent year due to it high impact on various application while the co clustering algorithm for two type of heterogeneous data denoted by pair wise co clustering such a document and term have been well studied in the literature the work on more type of heterogeneous data denoted by high order co clustering is still very limited a an attempt in this direction in this paper we worked on a specific case of high order co clustering in which there is a central type of object that connects the other type so a to form a star structure of the inter relationship actually this case could be a very good abstract for many real world application such a the co clustering of category document and term in text mining in our philosophy we treated such kind of problem a the fusion of multiple pair wise co clustering sub problem with the constraint of the star structure accordingly we proposed the concept of consistent bipartite graph co partitioning and developed an algorithm based on semi definite programming sdp for efficient computation of the clustering result experiment on toy problem and real data both verified the effectiveness of our proposed method 
reliably recovering d human pose from monocular video requires model that bias the estimate towards typical human pose and motion we construct prior for people tracking using the laplacian eigenmaps latent variable model lelvm lelvm is a recently introduced probabilistic dimensionality reduction model that combine the advantage of latent variable model a multimodal probability density for latent and observed variable and globally differentiable nonlinear mapping for reconstruction and dimensionality reduction with those of spectral manifold learning method no local optimum ability to unfold highly nonlinear manifold and good practical scaling to latent s pace of high dimension lelvm is computationally efficient simple to learn fr om sparse training data and compatible with standard probabilistic tracker such a particle filter we analyze the performance of a lelvm based probabilistic sigma point mixture tracker in several real and synthetic human motion sequence and demonstrate that lelvm not only provides sufficient constraint for robust op eration in the presence of missing noisy and ambiguous image measurement but also compare favorably with alternative tracker based on pca or gplvm prior recent research in reconstructing articulated human motion ha focused on method that can exploit available prior knowledge on typical human pose or motion in an attempt to build more reliable algorithm the high dimensionality of human ambient pose space between joint angle or joint position depending on the desired accuracy level make exhaustive search prohibitively expensive this ha negative impact on existing tracker which are often not sufficiently reliable at reconstructing human like pose self initializing or re covering from failure such difficulty have stimulated research in algorithm and model that reduce the effective working space either using generic search focusing method annealing state space decomposition covariance scaling or by exploiting specific problem structure e g kinematic ju mp experience with these procedure ha nevertheless shown that any search strategy no matter how effective can be made significantly more reliable if restricted to low dimensional state space s this permit a more thorough exploration of the typical solution space for a given comparati vely similar computational effort a a high dimensional method the argument correlate well with the belief that the human pose space although high dimensional in it natural ambient parameterization ha a significantly lower perceptual latent or intrinsic dimensionality at least in a pra ctical sense many pose that are possible are so improbable in many real world situation that it pay off to encode them with low accuracy a perceptual representation ha to be powerful enough to capture the diversity of human pose in a sufficiently broad domain of applicability the task domain yet compact and analytically tractable for search and optimization this justifies the use of model that are nonlinear and low dimensional able to unfold highly nonlinear manifold with low distortion yet probabilistically motivated and globally continuous for efficient optimization reducing d imensionality is not the only goal perceptual representation have to preserve critical propert y of the ambient space reliable tracking need locality nearby region in ambient space have to be mapped to nearby region in latent space if this doe not hold the tracker is forced to make unrealist ically large and difficult to predict jump in latent space in order to follow smooth trajectory in the joint angle ambient space 
we investigate a family of inference problem on markov model where many sample path are drawn from a markov chain and partial information is revealed to an observer who attempt to reconstruct the sample path we present algorithm and hardness result for several variant of this problem which arise by revealing different information to the observer and imposing different requirement for the reconstruction of sample path our algorithm are analogous to the classical viterbi algorithm for hidden markov model which find the single most probable sample path given a sequence of observation our work is motivated by an important application in ecology inferring bird migration path from a large database of observation 
assessing similarity between feature is a key step in object recognition and scene categorization task we argue that knowledge on the distribution of distance generated by similarity function is crucial in deciding whether feature are similar or not intuitively one would expect that similarity between feature could arise from any distribution in this paper we will derive th e contrary and report the theoretical result that lp norm a class of commonly applied distance metricsfrom one feature vector to other vector are weibull distributed if the feature value are correlated and non identically distrib uted besides these assumption being realistic for image we experimentally show them to hold for various popular feature extraction algorithm for a diver se range of image this fundamental insight open new direction in the assessment of feature similarity with projected improvement in object and scene recognition algorithm 
we investigate kernel based quantile regression based on the pinball loss and support vector regression based on the insensitive loss condition are given which quarantee that the set of exact minimizers contains only one function some result about oracle inequality and learning rate of these method are presented 
we describe a data mining system to detect fraud that are camouflaged to look like normal activity in domain with high number of known relationship example include accounting fraud detection for rating and investment insider attack on corporate network and health care insurance fraud our goal is to help analyst who are overwhelmed with information about company or on line system access log or insurance claim to focus their attention on feature that cause damage in the future we focused on accounting fraud where the task is to detect the subset of company that were potentially committing accounting fraud within the total population of public company that file quarterly and annual filing with the security and exchange commission sec using a representation of change b a mix of decision tree learning locally weighted logistic regression k mean clustering and constant regression in a two phase pipe line we developed model that rank company based on the probability of forecasting future damaging performance the learned model were tested extensively over four year with public data available from sec filing and private data available from rating company and investment firm cross validation experiment and analyst based validation of private experiment were found to show that the approach performed a well a or better than domain expert and discovered new relationship that domain expert did not use on a regular basis finally the detection preceded public knowledge of such problem by six to eighteen month 
in this paper we report a deployed data mining application system for motorola originally it intended use wa for identifying cause of cellular phone failure but it ha been found to be useful for many other engineering data set a well for this report the case study is a dataset containing cellular phone call record this data set is like any dataset used in classification application i e with a set of attribute which can be continuous or discrete and a discrete class attribute in our application the class are normally ended call call which failed to setup and call which failed while in progress however the task is not to predict any failure but to identify possible cause that resulted in failure then engineering effort may focus on improvement that can be made to the phone in the course of the project various classification technique e g decision tree na ve bayesian classification and svm were tried however the result were unsatisfactory after several demonstration and interaction with domain expert we finally designed and implemented an effective approach to perform the task the final system is based on class association rule general impression and visualization the system ha been deployed and is in regular use at motorola in this paper we first describe our experience with some existing classification system and discus why they are not suitable for the task we then present our technique a an illustration we show several visualization screen in the case study which reveal some important knowledge due to confidentiality we will not give specific but only present a general discussion about the result 
online feature selection ofs provides an efficient way to sort through a large space of feature particularly in a scenario where the feature space is large and feature take a significant amount of memory to store image processing operator and especially combination of image processing operator provide a rich space of potential feature for use in machine learning for image processing task but they are expensive to generate and store in this paper we apply ofs to the problem of edge detection in grayscale imagery we use a standard data set and compare our result to those obtained with traditional edge detector a well a with result obtained more recently using statistical edge detection we compare several different ofs approach including hill climbing best first search and grafting 
in many area of science and engineering the problem arises how to discover low dimensional representation of high dimensional data recently a number of researcher have converged on common solution to this problem using method from convex optimization in particular many result have been obtained by constructing semidefinite program sdps with low rank solution while the rank of matrix variable in sdps cannot be directly constrained it ha been observed that low rank solution emerge naturally by computing high variance or maximal trace solution that respect local distance constraint in this paper we show how to solve very large problem of this type by a matrix factorization that lead to much smaller sdps than those previously studied the matrix factorization is derived by expanding the solution of the original problem in term of the bottom eigenvectors of a graph laplacian the smaller sdps obtained from this matrix factorization yield very good approximation to solution of the original problem moreover these approximation can be further refined by conjugate gradient descent we illustrate the approach on localization in large scale sensor network where optimization involving ten of thousand of node can be solved in just a few minute 
eigendecomposition of kernel matrix is an indispensable procedure in many learning and vision task however the cubic complexity o n is impractical for large problem where n is the data size in this paper we propose an efficient approach to solve the eigendecomposition of the kernel matrix w the idea is to approximate w with w that is composed of m constant block the eigenvectors of w which can be solved in o m time is then used to recover the eigenvectors of the original kernel matrix the complexity of our method is only o mn m which scale more favorably than state of the art low rank approximation and sampling based approach o m n m and the approximation quality can be controlled conveniently our method demonstrates encouraging scaling behavior in experiment of image segmentation by spectral clustering and kernel principal component analysis 
we address the problem of learning a kernel for a given supervised learning task our approach consists in searching within the convex hull of a prescribed set of basic kernel for one which minimizes a convex regularization functional a unique feature of this approach compared to others in the literature is that the number of basic kernel can be infinite we only require that they are continuously parameterized for example the basic kernel could be isotropic gaussians with variance in a prescribed interval or even gaussians parameterized by multiple continuous parameter our work build upon a formulation involving a minimax optimization problem and a recently proposed greedy algorithm for learning the kernel although this optimization problem is not convex it belongs to the larger class of dc difference of convex function program therefore we apply recent result from dc optimization theory to create a new algorithm for learning the kernel our experimental result on benchmark data set show that this algorithm outperforms a previously proposed method 
online information service have grown too large for user to navigate without the help of automated tool such a collaborative filtering which make recommendation to user based on their collective past behavior while many similarity measure have been proposed and individually evaluated they have not been evaluated relative to each other in a large real world environment we present an extensive empirical comparison of six distinct measure of similarity for recommending online community to member of the orkut social network we determine the usefulness of the different recommendation by actually measuring user propensity to visit and join recommended community we also examine how the ordering of recommendation influenced user selection a well a interesting social issue that arise in recommending community within a real social network 
feature selection aim to reduce dimensionality for building comprehensible learning model with good generalization performance feature selection algorithm are largely studied separately according to the type of learning supervised or unsupervised this work exploit intrinsic property underlying supervised and unsupervised feature selection algorithm and proposes a unified framework for feature selection based on spectral graph theory the proposed framework is able to generate family of algorithm for both supervised and unsupervised feature selection and we show that existing powerful algorithm such a relieff supervised and laplacian score unsupervised are special case of the proposed framework to the best of our knowledge this work is the first attempt to unify supervised and unsupervised feature selection and enable their joint study under a general framework experiment demonstrated the efficacy of the novel algorithm derived from the framework 
compressed sensing is an emerging field based on the revelati on that a small group of linear projection of a sparse signal contains enough inf ormation for reconstruction in this paper we introduce a new theory for distributed compressed sensing dc that enables new distributed coding algorithm for multi signal ensemble that exploit both intraand inter signal correlation stru ctures the dc theory rest on a new concept that we term the joint sparsity of a signal ensemble we study three simple model for jointly sparse signal propose algorithm for joint recovery of multiple signal from incoherent projection and characterize theoretically and empirically the number of measurement per sensor required for accurate reconstruction in some sense dc is a framework for distributed compression of source with memory which ha remained a challenging problem in information theory for some time dc is immediately applicable to a range of problem in sensor network and array 
supervised learning is difficult with high dimensional input space and very small training set but accurate classification may be possible if the data lie on a low dimensional manifold gaussian process latent variable model can discover low dimensional manifold given only a small number of example but learn a latent space without regard for class label existing method for discriminative manifold learning e g lda gda do constrain the class distribution in the latent space but are generally deterministic and may not generalize well with limited training data we introduce a method for gaussian process classification using latent variable model trained with discriminative prior over the latent space which can learn a discriminative latent space from a small training set 
we present an efficient method for maximizing energy function with first and second order potential suitable for map labeling estimation problem that arise in undirected graphical model our approach is to relax the integer constraint on the solution in two step first we efficiently obtain the relaxed global optimum following a procedure similar to the iterative power method for finding the largest eigenvector of a matrix next we map the relaxed optimum on a simplex and show that the new energy obtained ha a certain optimal bound starting from this energy we follow an efficient coordinate ascent procedure that is guaranteed to increase the energy at every step and converge to a solution that obeys the initial integral constraint we also present a sufficient condition for ascent procedure that guarantee the increase in energy at every step 
spectral clustering enjoys it success in both data cluster ing and semisupervised learning but most spectral clustering algori thm cannot handle multi class clustering problem directly additio nal strategy are needed to extend spectral clustering algorithm to multi class clustering problem furthermore most spectral clustering algorithm employ hard cluster membership which is likely to be trapped by the local optimum in this paper we present a new spectral clustering algorithm named soft cut it improves the normalized cut algorithm by introducing soft membership and can be efficiently computed usin g a bound optimization algorithm our experiment with a variety of datasets have shown the promising performance of the proposed clustering algorithm 
in problem where input feature have varying amount of noise using distinct regularization hyperparameters for different feature pr ovides an effective mean of managing model complexity while regularizers for neural network and support vector machine often rely on multiple hyperparameters regularizers for structured prediction model used in task such a sequence labeling or parsing typically rely only on a single shared hyperparameter f or all feature in this paper we consider the problem of choosing regularization hyperparameters for log linear model a class of structured prediction probab ilistic model which includes conditional random field crfs using an implicit d ifferentiation trick we derive an efficient gradient based method for learning ga ussian regularization prior with multiple hyperparameters in both simulation and the real world task of computational rna secondary structure prediction we fin d that multiple hyperparameter learning can provide a significant boost in acc uracy compared to using only a single regularization hyperparameter 
despite of the large number of algorithm developed for clustering the study on comparing clustering result is limited in this paper we propose a measure for comparing clustering result to tackle two issue insufficiently addressed or even overlooked by existing method a taking into account the distance between cluster representative when assessing the similarity of clustering result b constructing a unified framework for defining a distance based on either hard or soft clustering and ensuring the triangle inequality under the definition our measure is derived from a complete and globally optimal matching between cluster in two clustering result it is shown that the distance is an instance of the mallow distance a metric between probability distribution in statistic a a result the defined distance inherits desirable property from the mallow distance experiment show that our clustering distance measure successfully handle case difficult for other measure 
in this paper we propose a new way to automatically model and predict human behavior of receiving and disseminating information by analyzing the contact and content of personal communication a personal profile called communitynet is established for each individual based on a novel algorithm incorporating contact content and time information simultaneously it can be used for personal social capital management cluster of communitynets provide a view of informal network for organization management our new algorithm is developed based on the combination of dynamic algorithm in the social network field and the semantic content classification method in the natural language processing and machine learning literature we tested communitynets on the enron email corpus and report experimental result including filtering prediction and recommendation capability we show that the personal behavior and intention are somewhat predictable based on these model for instance to whom a person is going to send a specific email can be predicted by one s personal social network and content analysis experimental result show the prediction accuracy of the proposed adaptive algorithm is better than the social network based prediction and is better than an aggregated model based on latent dirichlet allocation with social network enhancement two online demo system we developed that allow interactive exploration of communitynet are also discussed 
in this paper we discus a prototype application deployed at the u s national science foundation for assisting program director in identifying reviewer for proposal the application help program director sort proposal into panel and find reviewer for proposal to accomplish these task it extract information from the full text of proposal both to learn about the topic of proposal and the expertise of reviewer we discus a variety of alternative that were explored the solution that wa implemented and the experience in using the solution within the workflow of nsf 
this paper present an lda style topic model that capture not only the low dimensional structure of data but also how the structure change over time unlike other recent work that relies on markov assumption or discretization of time here each topic is associated with a continuous distribution over timestamps and for each generated document the mixture distribution over topic is influenced by both word co occurrence and the document s timestamp thus the meaning of a particular topic can be relied upon a constant but the topic occurrence and correlation change significantly over time we present result on nine month of personal email year of nip research paper and over year of presidential state of the union address showing improved topic better timestamp prediction and interpretable trend 
we know how to measure distance from beijing to toronto however do you know how to measure the distance between two information carrying entity for example two genome two music score two program two article two email or from a question to an answer furthermore such a distance measure must be application independent must be universal in the sense it is provably better than all other distance and must be applicable from a simple and accepted assumption in thermodynamics we have developed such a theory i will present this theory and will present one of the new application of this theory a question answering system 
we propose a novel statistical method to predict large scale dyadic response variable in the presence of covariate information our approach simultaneously incorporates the effect of covariates and estimate local structure that is induced by interaction among the dyad through a discrete latent factor model the discovered latent factor provide a redictive model that is both accurate and interpretable we illustrate our method by working in a framework of generalized linear model which include commonly used regression technique like linear regression logistic regression and poisson regression a special case we also provide scalable generalized em based algorithm for model fitting using both hard and soft cluster assignment we demonstrate the generality and efficacy of our approach through large scale simulation study and analysis of datasets obtained from certain real world movie recommendation and internet advertising application 
we propose an algorithm that us gaussian process regression to learn common hidden structure shared between corresponding set of heterogenous observation the observation space are linked via a single reduced dimensionality latent variable space we present result from two datasets demonstrating the algorithm s ability to synthesize novel data from learned correspondence we first show that the method can be used to learn the nonlinear mapping between corresponding view of object filling in missing data a needed to synthesize novel view we then show that the method can be used to acquire a mapping between human degree of freedom and robotic degree of freedom for a humanoid robot allowing robotic imitation of human pose from motion capture data 
we propose a new class of spatio temporal cluster detection method designed for the rapid detection of emerging space time cluster we focus on the motivating application of prospective disease surveillance detecting space time cluster of disease case resulting from an emerging disease outbreak automatic real time detection of outbreak can enable rapid epidemiological response potentially reducing rate of morbidity and mortality building on the prior work on spatial and space time scan statistic our method combine time series analysis to determine how many case we expect to observe for a given spatial region in a given time interval with new emerging cluster space time scan statistic to decide whether an observed increase in case in a region is significant enabling fast and accurate detection of emerging outbreak we evaluate these method on two type of simulated outbreak aerosol release of inhalational anthrax e g from a bioterrorist attack and floo fictional linear onset outbreak injected into actual baseline data emergency department record and over the counter drug sale data from allegheny county we demonstrate that our method are successful in rapidly detecting both outbreak type while keeping the number of false positive low and show that our new emerging cluster scan statistic consistently outperform the standard persistent cluster scan statistic approach 
generalized linear model are the most commonly used tool to describe the stimulus selectivity of sensory neuron here we present a bayesian treatment of such model using the expectation propagation algorithm we are able to approximate the full posterior distribution over all weight in addition we use a laplacian prior to favor sparse solution therefore stimulus feature that do not critically influence neural activity will be assigned zero weight and thus be effectively excluded by the model this feature selection mechanism facilitates both the interpretation of the neuron model a well a it predictive ability the posterior distribution can be used to obtain confidence interval which make it possible to ass the statistical significance of the solution in neural data analysis the available amount of experimental measurement is often limited whereas the parameter space is large in such a situation both regularization by a sparsity prior and uncertainty estimate for the model parameter are essential we apply our method to multi electrode recording of retinal ganglion cell and use our uncertainty estimate to test the statistical significance of functional coupling between neuron furthermore we used the sparsity of the laplace prior to select those filter from a spike triggered covariance analysis that are most informative about the neural response 
clustering is an essential data mining task with numerous application however data in most real life application are high dimensional in nature and the related information often spread across multiple relation to ensure effective and efficient high dimensional cross relational clustering we propose a new approach called crossclus which performs cross relational clustering with user s guidance we believe that user s guidance even likely in very simple form could be essential for effective high dimensional clustering since a user know well the application requirement and data semantics crossclus is carried out a follows a user specifies a clustering task and selects one or a small set of feature pertinent to the task crossclus extract the set of highly relevant feature in multiple relation connected via linkage defined in the database schema evaluates their effectiveness based on user s guidance and identifies interesting cluster that fit user s need this method take care of both quality in feature extraction and efficiency in clustering our comprehensive experiment demonstrate the effectiveness and scalability of this approach 
summarization is an important task in data mining a major challenge over the past year ha been the efficient construction of fixed space synopsis that provide a deterministic quality guarantee often expressed in term of a maximum error metric histogram and several hierarchical technique have been proposed for this problem however their time and or space complexity remain impractically high and depend not only on the data set size n but also on the space budget b these handicap stem from a requirement to tabulate all allocation of synopsis space to different region of the data in this paper we develop an alternative methodology that dispels these deficiency thanks to a fruitful application of the solution to the dual problem given a maximum allowed error determine the minimum space synopsis that achieves it compared to the state of the art our histogram construction algorithm reduces time complexity by at least a blog n over log factor and our hierarchical synopsis algorithm reduces the complexity by at least a factor of log b over log logn in time and b log b over log n in space where is the optimal error these complexity advantage offer both a space efficiency and a scalability that previous approach lacked we verify the benefit of our approach in practice by experimentation 
when trying to understand the brain it is of fundamental importance to analyse e g from eeg meg measurement what part of the cortex interact with each other in order to infer more accurate model of brain activity common technique like blind source separation bs can estimate brain source and single out artifact by using the underlying assumption of source signal independence however physiologically interesting brain source typically interact so bs will by construction fail to characterize them properly noting that there are truly interacting source and signal that only seemingly interact due to effect of volume conduction this work aim to contribute by distinguishing these effect for this a new bs technique is proposed that us anti symmetrized cross correlation matrix and subsequent diagonalization the resulting decomposition consists of the truly interacting brain source and suppresses any spurious interaction stemming from volume conduction our new concept of interacting source analysis isa is successfully demonstrated on meg data 
the choice of the kernel function which determines the mapping between the input space and the feature space is of crucial importance to kernel method the past few year have seen many effort in learning either the kernel function or the kernel matrix in this paper we address this model selection issue by learning the hyperparameter of the kernel function for a support vector machine svm we trace the solution path with respect to the kernel hyperparameter without having to train the model multiple time given a kernel hyperparameter value and the optimal solution obtained for that value we find that the solution of the neighborhood hyperparameters can be calculated exactly however the solution path doe not exhibit piecewise linearity and extends nonlinearly a a result the breakpoints cannot be computed in advance we propose a method to approximate the breakpoints our method is both efficient and general in the sense that it can be applied to many kernel function in common use 
although each iteration of the popular k mean clustering heuristic scale well to larger problem size it often requires an unacceptably high number of iteration to converge to a solution this paper introduces an enhancement of k mean in which local search is used to accelerate convergence without greatly increasing the average computational cost of the iteration the local search involves a carefully controlled number of swap operation resembling those of the more robust k medoids clustering heuristic we show empirically that the proposed method improves convergence result when compared to standard k mean 
we address the problem of efficiently learning naive bayes classifier under class conditional classification noise cccn naive bayes classifier rely on the hypothesis that the distribution associated to each class are product distribution when data is subject to ccc noise these conditional distribution are themselves mixture of product distribution we give analytical formula which make it possible to identify them from data subject to cccn then we design a learning algorithm based on these formula able to learn naive bayes classifier under cccn we present result on artificial datasets and datasets extracted from the uci repository database these result show that cccn can be efficiently and successfully handled 
we propose machine learning method for the estimation of deformation field that transform two given object into each other thereby establishing a dense point to point correspondence the field are computed using a modified support vector machine containing a penalty enforcing that point of one object will be mapped to similar point on the other one our system which contains little engineering or domain knowledge delivers state of the art performance we present application result including close to photorealistic morphs of d head model 
in many real world application labeled data are in short supply it often happens that obtaining labeled data in a new domain is expensive and time consuming while there may be plenty of labeled data from a related but different domain traditional machine learning is not able to cope well with learning across different domain in this paper we address this problem for a text mining task where the labeled data are under one distribution in one domain known a in domain data while the unlabeled data are under a related but different domain known a out of domain data our general goal is to learn from the in domain and apply the learned knowledge to out of domain we propose a co clustering based classification cocc algorithm to tackle this problem co clustering is used a a bridge to propagate the class structure and knowledge from the in domain to the out of domain we present theoretical and empirical analysis to show that our algorithm is able to produce high quality classification result even when the distribution between the two data are different the experimental result show that our algorithm greatly improves the classification performance over the traditional learning algorithm 
protein interaction network are one of the most promising type of biological data for the discovery of functional module and the prediction of individual protein function however it is known that these network are both incomplete and inaccurate i e they have spurious edge and lackbiologically valid edge one way to handle this problem is by transforming the original interaction graph into new graph that remove spurious edge add biologically valid one and assign reliability score to the edge constituting the final network we investigate currently existing method a well a propose a robust association analysis based method for this task this method is based on the concept of h confidence which is a measure that can be used to extract group of object having high similarity with each other experimental evaluation on several protein interaction data set show that hyperclique based transformation enhance the performance of standard function prediction algorithm significantly and thus have merit 
dimension reduction is a critical data preprocessing step for many database and data mining application such a efficient storage and retrieval of high dimensional data in the literature a well known dimension reduction algorithm is linear discriminant analysis lda the common aspect of previously proposed lda based algorithm is the use of singular value decomposition svd due to the difficulty of designing an incremental solution for the eigenvalue problem on the product of scatter matrix in lda there ha been little work on designing incremental lda algorithm that can efficiently incorporate new data item a they become available in this paper we propose an lda based incremental dimension reduction algorithm called idr qr which applies qr decomposition rather than svd unlike other lda based algorithm this algorithm doe not require the whole data matrix in main memory this is desirable for large data set more importantly with the insertion of new data item the idr qr algorithm can constrain the computational cost by applying efficient qr updating technique finally we evaluate the effectiveness of the idr qr algorithm in term of classification error rate on the reduced dimensional space our experiment on several real world data set reveal that the classification error rate achieved by the idr qr algorithm is very close to the best possible one achieved by other lda based algorithm however the idr qr algorithm ha much le computational cost especially when new data item are inserted dynamically 
this paper present a framework for user oriented text mining it is then illustrated with an example of discovering knowledge from competitor website the knowledge to be discovered is in the form of association rule a user s background knowledge is represented a a concept hierarchy developed from document on his her own website the concept hierarchy capture the semantic usage of word and relationship among word in background document association rule are identified among the noun phrase extracted from document on competitor website the interestingness measure i e novelty which measure the semantic distance between the antecedent and the consequent of a rule in the background knowledge is computed from the co occurrence frequency of word and the connection length among word in the concept hierarchy a user evaluation of the novelty of discovered rule demonstrates that the correlation between the algorithm and the human judge is comparable to that between human judge 
recent work ha shown the effectiveness of leveraging layout and tag tree structure for segmenting webpage and labeling html element however how to effectively segment and label the text content inside html element is still an open problem since many text content on a webpage are often text fragment and not strictly grammatical traditional natural language processing technique that typically expect grammatical sentence are no longer directly applicable in this paper we examine how to use layout and tag tree structure in a principled way to help understand text content on webpage we propose to segment and label the page structure and the text content of a webpage in a joint discriminative probabilistic model in this model semantic label of page structure can be leveraged to help text content understanding and semantic label ofthe text phrase can be used in page structure understanding task such a data record detection thus integration of both page structure and text content understanding lead to an integrated solution of webpage understanding experimental result on research homepage extraction show the feasibility and promise of our approach 
we present an infinite mixture model in which each component comprises a multivariate gaussian distribution over an input space and a gaussian process model over an output space our model is neatly able to deal with non stationary covariance function discontinuity multimodality and overlapping output signal the work is similar to that by rasmussen and ghahramani however we use a full generative model over input and output space rather than just a conditional model this allows u to deal with incomplete data to perform inference over inverse functional mapping a well a for regression and also lead to a more powerful and consistent bayesian specification of the effective gating network for the different expert 
we propose a highly efficient framework for kernel multi cla s model with a large and structured set of class kernel parameter are l earned automatically by maximizing the cross validation log likelihood and predictive probability are estimated we demonstrate our approach on large scale text classification task with hierarchical class structure achieving state of th e art result in an order of magnitude le time than previous work in this paper we propose a general framework for learning in probabilistic kernel classification model while the basic model is standard a major feature of our approach is the high computational efficiency with which the primary fitting for fixed hyperpara meter is done allowing u to deal with hundred of class and thousand of datapoints within a few minute the primary fitting scale linearly in c and depends on n mainly via a fixed number of matrix vector multiplication mvm with n n kernel matrix in many situation these mvm primitive can be computed very efficiently a will be demonstrated furthermore we o ptimize hyperparameters automatically by minimizing the cross validation log likelihood making use of our primary fitting technology a inner loop in order to compute the cv criterion and it gradient our approach can be used to learn 
we determine the asymptotic limit of the function computed by support vector machine svm and related algorithm that minimize a regularized empirical convex loss function in the reproducing kernel hilbert space of the gaussian rbf kernel in the situation where the number of example tends to infinity the bandwidth of the gaussian ker nel tends to and the regularization parameter is held fixed non asympt otic convergence bound to this limit in the l sense are provided together with upper bound on the classification error that is shown to conv erge to the bayes risk therefore proving the bayes consistency of a variety of method although the regularization term doe not vanish these result are particularly relevant to the one class svm for which the regularization can not vanish by construction and which is shown for the fir t time to be a consistent density level set estimator 
given a large real graph how can we generate a synthetic graph that match it property i e it ha similar degree distribution similar small diameter similar spectrum etc we propose to use kronecker graph which naturally obey all of the above property and we present kronfit a fast and scalable algorithm for fitting the kronecker graph generation model to real network a naive approach to fitting would take super exponential time in contrast kronfit take linear time by exploiting the structure of kronecker product and by using sampling experiment on large real and synthetic graph show that kronfit indeed mimic very well the pattern found in the target graph once fitted the model parameter and the resulting synthetic graph can be used for anonymization extrapolation and graph summarization 
information retrieval ir aim at solving a ranking problem given a query q and a corpus c the document of c should be ranked such that the document relevant to q appear above the others this task is generally performed by ranking the document d c according to their similarity with respect to q sim q d the identification of an effective function a b sim a b could be performed using a large set of query with their corresponding relevance assessment however such data are especially expensive to label thus a an alternative we propose to rely on hyperlink data which convey analogous semantic relationship we then empirically show that a measure sim inferred from hyperlinked document can actually outperform the state of the art okapi approach when applied over a non hyperlinked retrieval corpus 
we combine linear discriminant analysis lda and k mean clustering into a coherent framework to adaptively select the most discriminative subspace we use k mean clustering to generate class label and use lda to do subspace selection the clustering process is thus integrated with the subspace selection process and the data are then simultaneously clustered while the feature subspace are selected we show the rich structure of the general lda km framework by examining it variant and their relationship to earlier approach relation among pca lda k mean are clarified extensive experimental result on real world datasets show the effectiveness of our approach 
we present femine an automatic system for image based gene expression analysis we perform experiment on the largest publicly available collection of drosophila ish in situ hybridization image showing that our femine system achieves excellent performance in classification clustering and content based image retrieval the major innovation of femine is the use of automatically discovered latent spatial theme of gene expression lges in the whole embryo context a opposed to pattern in nearly disjoint portion of an embryo proposed in previous method 
in this paper we consider collaborative filtering a a ranking problem we present a method which us maximum margin matrix factorization and optimizes ranking instead of rating we employ structured output prediction to optimize for specific non uniform ranking score experimental result show that our method give very good ranking score and scale well on collaborative filtering task 
recent work ha shown the feasibility and promise of template independent web data extraction however existing approach use decoupled strategy attempting to do data record detection and attribute labeling in two separate phase in this paper we show that separately extracting data record and attribute is highly ineffective and propose a probabilistic model to perform these two task simultaneously in our approach record detection can benefit from the availability of semantics required in attribute labeling and at the same time the accuracy of attribute labeling can be improved when data record are labeled in a collective manner the proposed model is called hierarchical conditional random field it can efficiently integrate all useful feature by learning their importance and it can also incorporate hierarchical interaction which are very important for web data extraction we empirically compare the proposed model with existing decoupled approach for product information extraction and the result show significant improvement in both record detection and attribute labeling 
dimensionality reduction is one of the important preprocessing step in high dimensional data analysis in this paper we consider the supervised dimensionality reduction problem where sample are accompanied with class label traditional fisher discriminant analysis is a popular and powerful method for this purpose however it tends to give undesired result if sample in some class form several separate cluster i e multimodal in this paper we propose a new dimensionality reduction method called local fisher discriminant analysis lfda which is a localized variant of fisher discriminant analysis lfda take local structure of the data into account so the multimodal data can be embedded appropriately we also show that lfda can be extended to non linear dimensionality reduction scenario by the kernel trick 
this paper is concerned with the problem of predicting relative performance of classification algorithm it focus on method that use result on small sample and discus the shortcoming of previous approach a new variant is proposed that exploit a some previous approach meta learning the method requires that experiment be conducted on few sample the information gathered is used to identify the nearest learning curve for which the sampling procedure wa carried out fully this in turn permit to generate a prediction regard the relative performance of algorithm experimental evaluation show that the method competes well with previous approach and provides quite good and practical solution to this problem 
social tag are user generated keywords associated with some resource on the web in the case of music social tag have become an important component of web recommender system allowing user to generate playlist based on use dependent term such a chill or jogging that have been applied to particular song in this paper we propose a method for predicting these social tag directly from mp file using a set of boosted classifier we map audio feature onto social tag collected from the web the resulting automatic tag or autotags furnish information about music that is otherwise untagged or poorly tagged allowing for insertion of previously unheard music into a social recommender this avoids the cold start problem common in such system autotags can also be used to smooth the tag space from which similarity and recommendation are made by providing a set of comparable baseline tag for all track in a recommender system 
in this paper we describe the experience of introducing data mining to a large chemical manufacturing company the multi national nature of doing business with multiple business unit present a unique opportunity for the deployment of data mining while each business unit ha it own objective and challenge which may be at odds with those of other unit they also share many common interest and resource in this environment data mining can be used to identify potential value creating opportunity through large site integration of multiple asset and synergy from the use of common asset such a site wide manufacturing facility and world wide supply chain purchasing and other shared service however issue arise on one hand from overly complex system and on the other hand from the danger of reaching sub optimal solution if a big enough picture is not considered when executing project the company wide initiative and use of six sigma at all level of the company provided a fertile ground for making the case for data mining and facilitating it acceptance the six sigma mindset of measuring the performance of process and analyzing data promotes data based decision making therefore making data mining a natural extension of this methodology we will describe the approach for launching a data mining capability within this framework the strategy for securing upper management support drawing from internal modeling statistical and other community and from external consultant and university lesson learned from industrial case study enterprise wide tool evaluation and peer benchmarking will be discussed 
we study the problem of maximum entropy density estimation in the presence of known sample selection bias we propose three bias correction approach the first one take advantage of unbiase d sufficient statistic which can be obtained from biased sample the second one estimate the biased distribution and then factor the bias ou t the third one approximates the second by only using sample from the sampling distribution we provide guarantee for the first two approach an d evaluate the performance of all three approach in synthetic experiment and on real data from specie habitat modeling where maxent ha been successfully applied and where sample selection bias is a significan t problem 
survival in a non stationary potentially adversarial env ironment requires animal to detect sensory change rapidly yet accurately two oft competing desideratum neuron subserving such detection are faced with the corresponding challenge to discern real change in input a quickly a possible while ignoring noisy fluctuation mathematically this is an example of a change detection problem that is actively researched in the controlled stochastic pr ocesses community in this paper we utilize sophisticated tool developed in tha t community to formalize an instantiation of the problem faced by the nervous system and characterize the bayes optimal decision policy under certain assumption we will derive from this optimal strategy an information accumulation and decision process that remarkably resembles the dynamic of a leaky integrate and fire neuron this correspondence suggests that neuron are optimized for tracking input change and shed new light on the computational import of intracellular property such a resting membrane potential voltage dependent conductance and post spike reset voltage we also explore the influence that factor such a ti ming uncertainty neuromodulation and reward should and do have on neuronal dynamic and sensitivity a the optimal decision strategy depends critica lly on these factor 
when constructing a classifier from labeled data it is important not to assign too much weight to any single input feature in order to increase the robustness of the classifier this is particularly important in domain with nonstationary feature distribution or with input sensor failure a common approach to achieving such robustness is to introduce regularization which spread the weight more evenly between the feature however this strategy is very generic and cannot induce robustness specifically tailored to the classification task at hand in this work we introduce a new algorithm for avoiding single feature over weighting by analyzing robustness using a game theoretic formalization we develop classifier which are optimally resilient to deletion of feature in a minimax sense and show how to construct such classifier using quadratic programming we illustrate the applicability of our method on spam filtering and handwritten digit recognition task where feature deletion is indeed a realistic noise model 
data mining is increasingly being applied in environment having very high rate of data generation like network intrusion detection where router generate about connection every minute in such rare class data domain the cost of missing a rare class instance is much higher than that of other class however the high cost for manual labeling of instance the high rate at which data is collected a well a real time response constraint do not always allow one to determine the actual class for the collected unlabeled datasets in our previous work this problem of missed false negative wa explained in context of two different domain network intrusion detection and business opportunity classification in such case an estimate for the number of such missed high cost rare instance will aid in the evaluation of the performance of the modeling technique e g classification used a capture recapture method wa used for estimating false negative using two or more learning method i e classifier this paper focus on the dependence between the class label assigned by such learner we define the conditional independence for classifier given a class label and show it relation to the conditional independence of the feature set used by the classifier given a class label the later is a computationally expensive problem and hence a heuristic algorithm is proposed for obtaining conditionally independent or le dependent feature set for the classifier initial result of this algorithm on synthetic datasets are promising and further research is being pursued 
weblogs and message board provide online forum for discussion that record the voice of the public woven into this mass of discussion is a wide range of opinion and commentary about consumer product this present an opportunity for company to understand and respond to the consumer by analyzing this unsolicited feedback given the volume format and content of the data the appropriate approach to understand this data is to use large scale web and text data mining technology this paper argues that application for mining large volume of textual data for marketing intelligence should provide two key element a suite of powerful mining and visualization technology and an interactive analysis environment which allows for rapid generation and testing of hypothesis this paper present such a system that gather and annotates online discussion relating to consumer product using a wide variety of state of the art technique including crawling wrapping search text classification and computational linguistics marketing intelligence is derived through an interactive analysis framework uniquely configured to leverage the connectivity and content of annotated online discussion 
this paper explores the use of a maximal average margin mam optimality principle for the design of learning algorithm it is shown that the application of this risk minimization principle result in a class of co mputationally simple learning machine similar to the classical parzen window classifier a direct relation with the rademacher complexity is established a such facilitating analysis and providing a notion of certainty of prediction this anal ysis is related to support vector machine by mean of a margin transformation the power of the mam principle is illustrated further by application to ordinal regression task resulting in an o n algorithm able to process large datasets in reasonable time 
we present a risk minimization formulation for learning from both text and graph structure which is motivated by the problem of collective inference for hypertext document categorization the method is based on graph regularization formulated a a well formed convex optimization problem we present numerical algorithm for our formulation and show that such combination of local text feature and link information can lead to improved predictive accuracy 
we consider support vector machine for binary classification a opposed to most approach we use the number of support vector the l norm a a regularizing term instead of the l or l norm in order to solve the optimization problem we use the cross entropy method to search over the possible set of support vector the algorithm consists of solving a sequence of efficient linear program we report experiment where our method produce generalization error that are similar to support vector machine while using a considerably smaller number of support vector 
when predicting class label for object within a relationa l database it is often helpful to consider a model for relationship this allows f or information between class label to be shared and to improve prediction performance however there are different way by which object can be related within a relational database one traditional way corresponds to a markov network structure each existing relation is represented by an undirected edge this encodes that conditioned on input feature each object label is independent of other ob ject label given it neighbor in the graph however there is no reason why markov network should be the only representation of choice for symmetric dependence structure here we discus the case when relationship are postulated to exist due to hidden common cause we discus how the resulting graphical model differs from markov network and how it describes different type of real world relational process a bayesian nonparametric classification model is built upon this graphical representation and evaluated with several empirical study 
when monitoring spatial phenomenon such a the ecological condition of a river deciding where to make observation is a challenging task in these setting a fundamental question is when an active learning or sequential design strategy where location are selected based on previous measurement will perform significantly better than sensing at an a priori specified set of location for gaussian process gps which often accurately model spatial phenomenon we present an analysis and efficient algorithm that address this question central to our analysis is a theoretical bound which quantifies the performance difference between active and a priori design strategy we consider gps with unknown kernel parameter and present a nonmyopic approach for trading off exploration i e decreasing uncertainty about the model parameter and exploitation i e near optimally selecting observation when the parameter are approximately known we discus several exploration strategy and present logarithmic sample complexity bound for the exploration phase we then extend our algorithm to handle nonstationary gps exploiting local structure in the model we also present extensive empirical evaluation on several real world problem 
many robot control problem of practical importance including operational space control can be reformulated a immediate reward reinforcement learning problem however few of the known optimization or reinforcement learning algorithm can be used in online learning control for robot a they are either prohibitively slow do not scale to interesting domain of complex robot or require trying out policy generated by random search which are infeasible for a physical system using a generalization of the em base reinforcement learning framework suggested by dayan hinton we reduce the problem of learning with immediate reward to a reward weighted regression problem with an adaptive integrated reward transformation for faster convergence the resulting algorithm is efficient learns smoothly without dangerous jump in solution space and work well in application of complex high degree of freedom robot 
p the extraction of statistically independent component from high dimensional multi sensory input stream is assumed to b e an essential component of sensory processing in the brain such independent component analysis or blind source separat ion could provide a le redundant representation of inform ation about the external world another powerful processing strategy is to extract preferentially those component from high dimensional input stream that are related to other inf ormation source such a internal prediction or propriocep tive feedback this strategy allows the optimization of inte rnal representation according to the information bottleneck method however concrete learning rule that implement thes e general unsupervised learning principle for spiking neuro n are still missing we show how both information bottlenec k optimization and the extraction of independent component can in principle be implemented with stochastically spiking neuron with refractoriness the new learning rule that achi eve this is derived from abstract information optimization principle p 
people perform a remarkable range of task that require search of the visual environment for a target item among distractors the guided search model wolfe or g is perhaps the best developed psychological account of human visual search to prioritize search g assigns saliency to location in the visual field saliency is a linear combination of activation from retinotopic map representing primitive visual feature g includes heuristic for setting the gain coefficient associated with each map variant of g have formalized the notion of optimization a a principle of attentional control e g baldwin mozer cave navalpakkam itti rao et al but every g like model must be dumbed down to match human data e g by corrupting the saliency map with noise and by imposing arbitrary restriction on gain modulation we propose a principled probabilistic formulation of g called experience guided search egs based on a generative model of the environment that make three claim feature detector produce poisson spike train whose rate are conditioned on feature type and whether the feature belongs to a target or distractor the environment and or task is nonstationary and can change over a sequence of trial and a prior specifies that feature are more likely to be present for target than for distractors through experience egs infers latent environment variable that determine the gain for guiding search control is thus cast a probabilistic inference not optimization we show that egs can replicate a range of human data from visual search including data that g doe not address 
in a principal agent problem a principal seek to motivate an agent to take a certain action beneficial to the principal while spending a little a possible on the reward this is complicated by the fact that the principal doe not know the agent s utility function or type we study the online setting where at each round the principal encounter a new agent and the principal set the reward anew at the end of each round the principal only find out the action that the agent took but not his type the principal must learn how to set the reward optimally we show that this setting generalizes the setting of selling a digital good online we study and experimentally compare three main approach to this problem first we show how to apply a standard bandit algorithm to this setting second for the case where the distribution of agent type is fixed but unknown to the principal we introduce a new gradient ascent algorithm third for the case where the distribution of agent type is fixed and the principal ha a prior belief distribution over a limited class of type distribution we study a bayesian approach 
linear and quadratic discriminant analysis have been used widely in many area of data mining machine learning and bioinformatics friedman proposed a compromise between linear and quadratic discriminant analysis called regularized discriminant analysis rda which ha been shown to be more flexible in dealing with various class distribution rda applies the regularization technique by employing two regularization parameter which are chosen to jointly maximize the classification performance the optimal pair of parameter is commonly estimated via cross validation from a set of candidate pair it is computationally prohibitive for high dimensional data especially when the candidate set is large which limit the application of rda to low dimensional data in this paper a novel algorithm for rda is presented for high dimensional data it can estimate the optimal regularization parameter from a large set of parameter candidate efficiently experiment on a variety of datasets confirm the claimed theoretical estimate of the efficiency and also show that for a properly chosen pair of regularization parameter rda performs favorably in classification in comparison with other existing classification method 
we consider a framework for semi supervised learning using spectral decomposition based un supervised kernel design this approach subsumes a class of previously proposed semi supervised learning method on data graph we examine various theoretical property of such method in particular we derive a generalization performance bound and obtain the optimal kernel design by minimizing the bound based on the theoretical analysis we are able to demonstrate why spectral kernel design based method can often improve the predictive performance experiment are used to illustrate the main consequence of our analysis 
hierarchical model have been shown to be effective in content classification however we observe through empirical study that the performance of a hierarchical model varies with given taxonomy even a semantically sound taxonomy ha potential to change it structure for better classification by scrutinizing typical case we elucidate why a given semantics based hierarchy doe not work well in content classification and how it could be improved for accurate hierarchical classification with these understanding we propose effective localized solution that modify the given taxonomy for accurate hierarchical classification we conduct extensive experiment on both toy and real world data set report improved performance and interesting finding and provide further analysis of algorithmic issue such a time complexity robustness and sensitivity to the number of feature 
protein fold recognition is a key step towards inferring the tertiary structure from amino acid sequence complex fold such a those consisting of interacting structural repeat are prevalent in protein involved in a wide spectrum of biological function however extant approach often perform inadequately due to their inability to capture long range interaction between structural unit and to handle low sequence similarity across protein under identity in this paper we propose a chain graph model built on a causally connected series of segmentation conditional random field scrfs to address these issue specifically the scrf model capture long range interaction within recurring structural unit and the bayesian network backbone decomposes cross repeat interaction into locally computable module consisting of repeat specific scrfs and a model for sequence motif we applied this model to predict helix and leucine rich repeat and found it significantly outperforms extant method in predictive accuracy and or computational efficiency 
abstract our understanding of the input output function of single cell ha been substantially advanced by biophysically accurate multi compartmental model the large number of parameter needing hand tuning in these model ha however somewhat hampered their applicability and interpretability here we propose a simple and well founded method for automatic estimation of many of these key parameter the spatial distribution of channel density on the cell s membrane the spat iotemporal pattern of synaptic input the channel reversal potent ials the intercompartmental conductance and the noise level in each compartment we assume experimental access to a the spatiotemporal voltage signal in the dendrite or some contiguous subpart thereof e g via voltage sensitive imaging technique b an approximate kinetic description of the channel and synapsis present in each compartment and c the morphology of the part of the neuron under investigation the key observation is that given data a c all of the parameter may be simultaneously inferred by a version of constrained linear regression this regression in turn is efficiently solved using standard al gorithms without any local minimum problem despite the large number of parameter and complex dynamic the noise level may also be estimated by standard technique we demonstrate the method s accuracy on several model datasets and describe technique for quantifying the uncertainty in our estimate 
we consider feature selection for text classification both theoretically and empirically our main result is an unsupervised feature selection strategy for which we give worst case theoretical guarantee on the generalization power of the resultant classification function f with respect to the classification function f obtained when keeping all the feature to the best of our knowledge this is the first feature selection method with such guarantee in addition the analysis lead to insight a to when and why this feature selection strategy will perform well in practice we then use the techtc newsgroups and reuters rcv data set to evaluate empirically the performance of this and two simpler but related feature selection strategy against two commonly used strategy our empirical evaluation show that the strategy with provable performance guarantee performs well in comparison with other commonly used feature selection strategy in addition it performs better on certain datasets under very aggressive feature selection 
we apply stochastic meta descent smd a stochastic gradient optimization method with gain vector adaptation to the training of conditional random field crfs on several large data set the resulting optimizer converges to the same quality of solution over an order of magnitude faster than limited memory bfgs the leading method reported to date we report result for both exact and inexact inference technique 
learning ranking or preference function ha been a major issue in the machine learning community and ha produced many application in information retrieval svms support vector machine a classification and regression methodology have also shown excellent performance in learning ranking function they effectively learn ranking function of high generalization based on the large margin principle and also systematically support nonlinear ranking by the kernel trick in this paper we propose an svm selective sampling technique for learning ranking function svm selective sampling or active learning with svm ha been studied in the context of classification such technique reduce the labeling effort in learning classification function by selecting only the most informative sample to be labeled however they are not extendable to learning ranking function a the labeled data in ranking is relative ordering or partial order of data our proposed sampling technique effectively learns an accurate svm ranking function with fewer partial order we apply our sampling technique to the data retrieval application which enables fuzzy search on relational database by interacting with user for learning their preference experimental result show a significant reduction of the labeling effort in inducing accurate ranking function 
motivated by the problem of learning to detect and recognize object with minimal supervision we develop a hierarchical probabilistic model for the spatial structure of visual scene in contrast with most existing model our approach explicitly capture uncertainty in the number of object instance depicted in a given image our scene model is based on the transformed dirichlet process tdp a novel extension of the hierarchical dp in which a set of stochastically transformed mixture component are shared between multiple group of data for visual scene mixture component describe the spatial structure of visual feature in an object centered coordinate frame while transformation model the object position in a particular image learning and inferenc e in the tdp which ha many potential application beyond computer vision is based on an empirically effective gibbs sampler applied to a dataset of partially labeled street scene we show that the tdp s inclusi on of spatial structure improves detection performance flexibly exploi ting partially labeled training image 
we address the problem of feature selection in a kernel space to select the most discriminative and informative feature for classification and data analysis this is a difficult problem because the dimension of a kernel space may be infinite in the past little work ha been done on feature selection in a kernel space to solve this problem we derive a basis set in the kernel space a a first step for feature selection using the basis set we then extend the margin based feature selection algorithm that are proven effective even when many feature are dependent the selected feature form a subspace of the kernel space in which different state of the art classification algorithm can be applied for classification we conduct extensive experiment over real and simulated data to compare our proposed method with four baseline algorithm both theoretical analysis and experimental result validate the effectiveness of our proposed method 
to accelerate the training of kernel machine we propose to map the input data to a randomized low dimensional feature space and then apply existing fast linear method the feature are designed so that the inner product of the transformed data are approximately equal to those in the feature space of a user specified shiftinvariant kernel we explore two set of random feature provide convergence bound on their ability to approximate various radial basis kernel and show that in large scale classification and regression task linear machine learning algorithm applied to these feature outperform state of the art large scale kernel machine 
publication repository contain an abundance of information about the evolution of scientific research area we address the problem of creating a visualization of a research area that describes the flow of topic between paper quantifies the impact that paper have on each other and help to identify key contribution to this end we devise a probabilistic topic model that explains the generation of document the model incorporates the aspect of topical innovation and topical inheritance via citation we evaluate the model s ability to predict the strength of influence of citation against manually rated citation 
how to assign appropriate weight to term is one of the critical issue in information retrieval many term weighting scheme are unsupervised they are either based on the empirical observation in information retrieval or based on generative approach for language modeling a a result the existing term weighting scheme are usually insufficient in distinguishing informative word from the uninformative one which is crucial to the performance of information retrieval in this paper we present supervised term weighting scheme that automatically learn term weight based on the correlation between word frequency and category information of document empirical study with the imageclef dataset have indicated that the proposed method perform substantially better than the state of the art approach for term weighting and other alternative that exploit category information for information retrieval 
an important problem in data mining is detecting change in large datasets although there are a variety of change detection algorithm that have been developed in practice it can be a problem to scale these algorithm to large data set due to the heterogeneity of the data in this paper we describe a case study involving payment card data in which we built and monitored a separate change detection model for each cell in a multi dimensional data cube we describe a system that ha been in operation for the past two year that build and monitor over separate baseline model and the process that isused for generating and investigating alert using these baseline 
recurrent network that perform a winner take all computation have been studied extensively although some of these study include spiking network they consider only analog input rate we present result of this winner take all computation on a network of integrate and fire neuron which receives spike train a input we show how we can configure the connectivity in the network so that the winner is selected after a pre determined number of input spike we discus spiking input with both regular frequency and poisson distributed rate the robustness of the computation wa tested by implementing the winner take all network on an analog vlsi array of integrate and fire neuron which have an innate variance in their operating parameter 
optimization algorithm for large margin multiclass recognizers are often too costly to handle ambitious problem with structured output and exponential number of class optimization algorithm that rely on the full gradient are not effective because unlike the solution the gradient is not sparse and is very large the larank algorithm sidestep this difficulty by relying on a randomized exploration inspired by the perceptron algorithm we show that this approach is competitive with gradient based optimizers on simple multiclass problem furthermore a single larank pas over the training example delivers test error rate that are nearly a good a those of the final solution 
under natural viewing condition small movement of the eye and body prevent the maintenance of a steady direction of gaze it is known that stimulus tend to fade when they are stabilized on the retina for several second however it is unclear whether the physiological self motion of the retinal image serf a visual purpose during the brief period of natural visual fixation this study examines the impact of fixational instability on the statistic of visual input to the retina and on the structure of neural activity in the early visual system fixational instability introduces fluctuation in the retinal input signal that in the presence of natural image lack spatial correlation these input fluctuation strongly influence neural activity in a model of the lgn they decorrelate cell response even if the contrast sensitivity function of simulated cell are not perfectly tuned to counter balance the power law spectrum of natural image a decorrelation of neural activity ha been proposed to be beneficial for discarding statistical redundancy in the input signal fixational instability might therefore contribute to establishing efficient representation of natural stimulus 
this paper present a new filter for online data association p roblems in high dimensional space the key innovation is a representation of the data association posterior in information form in which the proximity of object and track are expressed by numerical link updating these link requires linear time compared to exponential t ime required for computing the exact posterior probability the paper derives the algorithm formally and provides comparative result using data obtained by a real world camera array and by a large scale sensor network simulation 
kdd is a complex and demanding task while a large number of method ha been established for numerous problem many challenge remain to be solved new task emerge requiring the development of new method or processing scheme like in software development the development of such solution demand for careful analysis specification implementation and testing rapid prototyping is an approach which allows crucial design decision a early a possible a rapid prototyping system should support maximal re use and innovative combination of existing method a well a simple and quick integration of new one this paper describes yale a free open source environment forkdd and machine learning yale provides a rich variety of method whichallows rapid prototyping for new application and make costlyre implementation unnecessary additionally yale offer extensive functionality for process evaluation and optimization which is a crucial property for any kdd rapid prototyping tool following the paradigm of visual programming eas the design of processing scheme while the graphical user interface support interactive design the underlying xml representation enables automated application after the prototyping phase after a discussion of the key concept of yale we illustrate the advantage of rapid prototyping for kdd on case study ranging from data pre processing to result visualization these case study cover task like feature engineering text mining data stream mining and tracking drifting concept ensemble method and distributed data mining this variety of application is also reflected in a broad user base we counted more than downloads during the last twelve month 
we present a probability distribution over non negative in teger valued matrix with possibly an infinite number of column we also derive a s tochastic process that reproduces this distribution over equivalence classe s this model can play the role of the prior in nonparametric bayesian learning sce narios where multiple latent feature are associated with the observed data and ea ch feature can have multiple appearance or occurrence within each data point such data arise naturally when learning visual object recognition system fro m unlabelled image together with the nonparametric prior we consider a likelih ood model that explains the visual appearance and location of local image patch inference with this model is carried out using a markov chain monte carlo algorithm 
in this paper we propose a novel scalable clustering based ordinal regression formulation which is an instance of a second order cone program socp with one second order cone soc constraint the main contribution of the paper is a fast algorithm cb or which solves the proposed formulation more eficiently than general purpose solver another main contribution of the paper is to pose the problem of focused crawling a a large scale ordinal regression problem and solve using the proposed cb or focused crawling is an efficient mechanism for discovering resource of interest on the web posing the problem of focused crawling a an ordinal regression problem avoids the need for a negative class and topic hierarchy which are the main drawback of the existing focused crawling method experiment on large synthetic and benchmark datasets show the scalability of cb or experiment also show that the proposed focused crawler outperforms the state of the art 
reward modulated spike timing dependent plasticity stdp ha recently emerged a a candidate for a learning rule that could explain how local learning rule at single synapsis support behaviorally relevant ada ptive change in complex network of spiking neuron however the potential and limitation of this learning rule could so far only be tested through computer si mulations this article provides tool for an analytic treatment of reward mo dulated stdp which allow u to predict under which condition reward modulated stdp will be able to achieve a desired learning effect in particular we can p roduce in this way a theoretical explanation and a computer model for a fundamental experimental finding on biofeedback in monkey reported in 
we give a new class of outer bound on the marginal polytope and propose a cutting plane algorithm for efficiently optimizing over these constraint when combined with a concave upper bound on the entropy this give a new variational inference algorithm for probabilistic inference in discrete markov random field mrfs valid constraint on the marginal polytope are derived through a series of projection onto the cut polytope a a result we obtain tighter upper bound on the log partition function we also show empirically that the approximation of the marginals are significantly more accurate when using the tighter outer bound finally we demonstrate the advantage of the new constraint for finding the map assignment in protein structure prediction 
diffusion process are a family of continuous time continuous state stochastic process that are in general only partially observed the joint estimation of the forcing parameter and the system noise volatility in these dynamical system is a crucial but non trivial task especially when the system is nonlinear and multimodal we propose a variational treatment of diffusion process which allows u to compute type ii maximum likelihood estimate of the parameter by simple gradient technique and which is computationally le demanding than most mcmc approach we also show how a cheap estimate of the posterior over the parameter can be constructed based on the variational free energy 
point based algorithm have been surprisingly successful in computing approximately optimal solution for partially observable markov decision process pomdps in high dimensional belief space in this work we seek to understand the belief space property that allow some pomdp problem to be approximated efficiently and thus help to explain the point based algorithm success often observed in the experiment we show that an approximately optimal pomdp solution can be computed in time polynomial in the covering number of a reachable belief space which is the subset of the belief space reachable from a given belief point we also show that under the weaker condition of having a small covering number for an optimal reachable space which is the subset of the belief space reachable under an optimal policy computing an approximately optimal solution is np hard however given a suitable set of point that cover an optimal reachable space well an approximate solution can be computed in polynomial time the covering number highlight several interesting property that reduce the complexity of pomdp planning in practice e g fully observed state variable belief with sparse support smooth belief and circulant state transition matrix 
dimension attribute in data warehouse are typically hierarchical e g geographic location in sale data url in web traffic log olap tool are used to summarize the measure attribute e g total sale along a dimension hierarchy and to characterize change e g trend and anomaly in a hierarchical summary over time when thenumber of change identified is large e g total sale in many store differed from their expected value a parsimonious explanation of the most significant change is desirable in this paper we propose a natural model of parsimonious explanation a a composition of node weight along the root to leaf path in a dimension hierarchy which permit change to be aggregated with maximal generalization along the dimension hierarchy we formalize this model of explaining change in hierarchical summary and investigate the problem of identifying optimally parsimonious explanation on arbitrary rooted one dimensional tree hierarchy we show that such explanation can be computed efficiently in time essentially proportional to the number of leaf and the depth of the hierarchy further our method can produce parsimonious explanation from the output of any statistical model that provides prediction and confidence interval making it widely applicable our experiment use real data set to demonstrate the utility and robustness of our proposed model for explaining significant change a well a it superior parsimony compared to alternative 
we consider the problem of support vector machine transduction which involves a combinatorial problem with exponential computational complexity in the number of unlabeled example although several study are devoted to transductive svm they suffer either from the high computation complexity or from the solution of local optimum to address this problem we propose solving transductive svm via a convex relaxation which convert the np hard problem to a semi definite programming compared with the other sdp relaxation for transductive svm the proposed algorithm is computationally more efficient with the number of free parameter reduced from o n to o n where n is the number of example empirical study with several benchmark data set show the promising performance of the proposed algorithm in comparison with other state of the art implementation of transductive svm 
in this paper we consider the problem of finding set of point that conform to a given underlying model from within a dense noisy set of observation this problem is motivated by the task of efficien tly linking faint asteroid detection but is applicable to a range of sp atial query we survey current tree based approach showing a trade off exists between single tree and multiple tree algorithm to this end we present a new type of multiple tree algorithm that us a variable number of tree to exploit the advantage of both approach we empirically show that this algorithm performs well using both simulated and astronomical data 
reinforcement learning model have long promised to unify computational psychological and neural account of appetitively conditioned behavior however the bulk of data on animal conditioning come from free operant experiment measuring how fast animal will work for reinforcement existing reinforcement learning rl model are silent about these task because they lack any notion of vigor they thus fail to address the simple observation that hungrier animal will work harder for food a well a stranger fact such a their sometimes greater productivity even when working for irrelevant outcome such a water here we develop an rl framework for free operant behavior suggesting that subject choose how vigorously to perform selected action by optimally balancing the cost and benefit of quick responding motivational state such a hunger shift these factor skewing the tradeoff this account normatively for the effect of motivation on response rate a well a many other classic finding finally we suggest that tonic level of dopamine may be involved in the computation linking motivational state to optimal responding thereby explaining the complex vigor related effect of pharmacological manipulation of dopamine 
for both classification and retrieval of natural language text document the standard document representation is a term vector where a term is simply a morphological normal form of the corresponding word a potentially better approach would be to map every word onto a concept the proper word sense and use this additional information in the learning process in this paper we address the problem of automatically classifying natural language text document we investigate the effect of word to concept mapping and word sense disambiguation technique on improving classification accuracy we use the wordnet thesaurus a a background knowledge base and propose a generative language model approach to document classification we show experimental result comparing the performance of our model with naive bayes and svm classifier 
we present a novel cochlear model implemented in analog very large scale integration vlsi technology that emulates nonlinear active cochlear behavior this silicon cochlea includes outer hair cell ohc electromotility through active bidirectional coupling abc a mechanism we proposed in which ohc motile force through the microanatomical organization of the organ of corti realize the cochlear amplifier our chip measurement demonstrate that frequency response become larger and more sharply tuned when abc is turned on the degree of the enhancement decrease with input intensity a abc includes saturation of ohc force 
in many structured prediction problem the highest scoring labeling is hard to compute exactly leading to the use of approximate inference method however when inference is used in a learning algorithm a good approximation of the score may not be sufficient we show in particular that learning can fail even with an approximate inference method with rigorous approximation guarantee there are two reason for this first approximate method can effectively reduce the expressivity of an underlying model by making it impossible to choose parameter that reliably give good prediction second approximation can respond to parameter change in such a way that standard learning algorithm are misled in contrast we give two positive result in the form of learning bound for the use of lp relaxed inference in structured perceptron and empirical risk minimization setting we argue that without understanding combination of inference and learning such a these that are appropriately compatible learning performance under approximate inference cannot be guaranteed 
we phrase k mean clustering a an empirical risk minimization procedure over a class hk and explicitly calculate the covering number for this class next we show that stability of k mean clustering is characterized by the geometry of hk with respect to the underlying distribution we prove that in the case of a unique global minimizer the clustering solution is stable with respect to complete change of the data while for the case of multiple minimizers the change of n sample defines the transition between stability and instability while for a finite number of minimizers this result follows from multinomial distribution estimate the case of infinite minimizers requires more refined tool we conclude by proving that stability of the function in hk implies stability of the actual center of the cluster since stability is often used for selecting the number of cluster in practice we hope that our analysis serf a a starting point for finding theoretically grounded recipe for the choice of k 
we present a new kernel method for extracting semantic relation between entity in natural language text based on a generali zation of subsequence kernel this kernel us three type of subsequence pattern that are typically employed in natural language to assert re lationships between two entity experiment on extracting protein interaction from biomedical corpus and top level relation from newspaper corpus demonstrate the advantage of this approach 
predictive state representation psrs are a recently developed way to model discrete time controlled dynamical system we present and describe two algorithm for learning a psr model a monte carlo algorithm and a temporal difference td algorithm both of these algorithm can learn model for system without requiring a reset action a wa needed by the previously available general psr model learning algorithm we present empirical result that compare our two algorithm and also compare their performance with that of existing algorithm including an em algorithm for learning pomdp model 
given q node in a social network say authorship network how can we find the node author that is the center piece and ha direct or indirect connection to all or most of them for example this node could be the common advisor or someone who started the research area that the q node belong to isomorphic scenario appear in law enforcement find the master mind criminal connected to all current suspect gene regulatory network find the protein that participates in pathway with all or most of the given q protein viral marketing and many more connection subgraphs is an important first step handling the case of q query node then the connection subgraph algorithm find the b intermediate node that provide a good connection between the two original query node here we generalize the challenge in multiple dimension first we allow more than two query node second we allow a whole family of query ranging from or to and with softand in between finally we design and compare a fast approximation and study the quality speed trade off we also present experiment on the dblp dataset the experiment confirm that our proposed method naturally deal with multi source query and that the resulting subgraphs agree with our intuition wall clock timing result on the dblp dataset show that our proposed approximation achieve good accuracy for about speedup 
we consider the task of driving a remote control car at high speed through unstructured outdoor environment we present an approach in which supervised learning is first used to estimate depth from single monocular image the learning algorithm can be trained either on real camera image labeled with ground truth distance to the closest obstacle or on a training set consisting of synthetic graphic image the resulting algorithm is able to learn monocular vision cue that accurately estimate the relative depth of obstacle in a scene reinforcement learning policy search is then applied within a simulator that render synthetic scene this learns a control policy that selects a steering direction a a function of the vision system s output we present result evaluating the predictive ability of the algorithm both on held out test data and in actual autonomous driving experiment 
while kernel canonical correlation analysis kernel cca ha been applied in many problem the asymptotic convergence of the function estimated from a finite sample to the true function ha not yet been established this paper give a rigorous proof of the statistical convergence of kernel cca and a related method nocco which provides a theoretical justification for these method the result also give a sufficient condition on the decay of the regularization coefficient in the method to ensure convergence 
this paper proposes an algorithm to convert a t stage stochastic decision problem with a continuous state space to a sequence of supervised learning problem the optimization problem associated with the trajectory tree and random trajectory method of kearns mansour and ng is solved using the gauss seidel method the algorithm break a multistage reinforcement learning problem into a sequence of single stage reinforcement learning subproblems each of which is solved via an exact reduction to a weighted classification problem that can be s olved using off the self method thus the algorithm convert a reinforcement learning problem into simpler supervised learning subproblems it is shown that the method converges in a finite number of step to a solut ion that cannot be further improved by componentwise optimization the implication of the proposed algorithm is that a plethora of classi fication method can be applied to find policy in the reinforcement learn ing problem 
wireless sensor network wsn composed of large number of small device that self organize are being investigated for a wide variety of application two key advantage of these network over more traditional sensor network are that they can be dynamically and quickly deployed and that they can provide fine grained sensing application such a emergency response to natural or manmade disaster detection and tracking and fine grained sensing of the environment are key example of application that can benefit from these type of wsn current research for these system is widespread however many of the proposed solution are developed with simplifying assumption about wireless communication and the environment even though the reality of wireless communication and environmental sensing are well known many of the solution are evaluated only by simulation in this talk i describe a fully implemented system consisting of a suite of more than synthesized protocol the system support a power aware surveillance tracking and classification application running on xsm mote and evaluated in a realistic large area environment technical detail and evaluation are presented i end with a discussion of opportunity and problem for data mining related to wsn 
subgroup discovery is a learning task that aim at finding interesting rule from classified example the search is guided by a utility function trading off the coverage of rule against their statistical unusualness one shortcoming of existing approach is that they do not incorporate prior knowledge to this end a novel generic sampling strategy is proposed it allows to turn pattern mining into an iterative process in each iteration the focus of subgroup discovery lie on those pattern that are unexpected with respect to prior knowledge and previously discovered pattern the result of this technique is a small diverse set of understandable rule that characterise a specified property of interest a another contribution this article derives a simple connection between subgroup discovery and classifier induction for a popular utility function this connection allows to apply any standard rule induction algorithm to the task of subgroup discovery after a step of stratified resampling the proposed technique are empirically compared to state of the art subgroup discovery algorithm 
in real world planning problem we must reason not only about our own goal but about the goal of other agent with which we may interact often these agent goal are neither completely aligned with our own nor directly opposed to them instead there are opportunity for cooperation by joining force the agent can all achieve higher utility than they could separately but in order to cooperate the agent must negotiate a mutually acceptable plan from among the many possible one and each agent must trust that the others will follow their part of the deal research in multi agent planning ha often avoided the problem of making sure that all agent have an incentive to follow a proposed joint plan on the other hand while game theoretic algorithm handle incentive correctly they often don t scale to large planning problem in this paper we attempt to bridge the gap between these two line of research we present an efficient game theoretic approximate planning algorithm along with a negotiation protocol which encourages agent to compute and agree on joint plan that are fair and optimal in a sense defined below we demonstrate our algorithm and protocol on two simple robotic planning problem 
we present a bayesian framework for explaining how people reason about and predict the action of an intentional agent based on observing it behavior action understanding is cast a a problem of inverting a probabilistic generative model which assumes that agent tend to act rationally in order to achieve their goal given the constra ints of their environment working in a simple sprite world domain we show how this model can be used to infer the goal of an agent and predict how the agent will act in novel situation or when environmental constraint change the model provides a qualitative account of several kind of inference that preverbal infant have been shown to perform and also fi t quantitative prediction that adult observer make in a new experiment 
we introduce a new em framework in which it is possible not only to optimize the model parameter but also the number of model component a key feature of our approach is that we use nonparametric density estimation to improve parametric density estimation in the em framework while the classical em algorithm estimate model parameter empirically using the data point themselves we estimate them using nonparametric density estimate there exist many possible application that require optimal adjustment of model component we present experimental result in two domain one is polygonal approximation of laser range data which is an active research topic in robot navigation the other is grouping of edge pixel to contour boundary which still belongs to unsolved problem in computer vision 
this paper address the bottom up influence of local image i nformation on human eye movement most existing computational model use a set of biologically plausible linear filter e g gabor or differenceof gaussians filter a a front end the output of which are nonlinearly combined into a real number that indicates visual saliency unfortunately this requires m any design parameter such a the number type and size of the front end filter a well a the choice of nonlinearities weighting and normalization scheme etc for which biological plausibility cannot always be justified a a result these p arameters have to be chosen in a more or le ad hoc way here we propose to learn a visual saliency model directly from human eye movement data the model is rather simplistic and essentially parameter free and therefore contrast rece nt development in the field that usually aim at higher prediction rate at the cost of add itional parameter and increasing model complexity experimental result show that despite the lack of any biological prior knowledge our model performs comparably to existing approach and in fact learns image feature that resemble fin ding from several previous study in particular it maximally excitatory sti muli have center surround structure similar to receptive field in the early human vi ual system 
inference in markov decision process ha recently received interest a a mean to infer goal of an observed action policy recognition and also a a tool to compute policy a particularly interesting aspect of the approach is that any existing inference technique in dbns now becomes available for answering behavioral question including those on continuous factorial or hierarchical state representation here we present an expectation maximization algorithm for computing optimal policy unlike previous approach we can show that this actually optimizes the discounted expected future return for arbitrary reward function and without assuming an ad hoc finite total time the algorithm is generic in that any inference technique can be utilized in the e step we demonstrate this for exact inference on a discrete maze and gaussian belief state propagation in continuous stochastic optimal control problem 
in this paper application of sparse representation factorization of signal over an overcomplete basis dictionary for signal classification is discussed searching for the sparse representation of a signal over an overcomplete dictionary is achieved by optimizing an objective function that includes two term one that measure the signal reconstruction error and another that measure the sparsity this objective function work well in application where signal need to be reconstructed like coding and denoising on the other hand discriminative method such a linear discriminative analysis lda are better suited for classification task however discriminative method are usually sensitive to corruption in signal due to lacking crucial property for signal reconstruction in this paper we present a theoretical framework for signal classification with sparse representation the approach combine the discrimination power of the discriminative method with the reconstruction property and the sparsity of the sparse representation that enables one to deal with signal corruption noise missing data and outlier the proposed approach is therefore capable of robust classification with a sparse representation of signal the theoretical result are demonstrated with signal classification task showing that the proposed approach outperforms the standard discriminative method and the standard sparse representation in the case of corrupted signal 
we address the problem of blind motion deblurring from a single image caused by a few movingobjects in such situation only part of the image may be blurred and the scene consists of layer blurred in different degree most of of existing blind deconvolution research concentrate at recovering a single blurring kernel for the entire image however in the case of different motion the blur cannot be modeled with a single kernel and trying to deconvolve the entire image with the same kernel will cause serious artifact thus the task of deblurring need to involve segmentation of the image into region with different blur our approach relies on the observation that the statistic of derivative filter in image are significantly changed by blur assuming the blur result from a constant velocity motion we can limit the search to one dimensional box filter blur this enables u to model the expected derivative distribution a a function of the width of the blur kernel those distribution are surprisingly powerful in discriminating region with different blur the approach produce convincing deconvolution result on real world image with rich texture 
principal component analysis pca ha been extensively applied in data mining pattern recognition and information retrieval for unsupervised dimensionality reduction when label of data are available e g in a classification or regression task pca is however not able to use this information the problem is more interesting if only part of the input data are labeled i e in a semi supervised setting in this paper we propose a supervised pca model called sppca and a semi supervised pca model called s ppca both of which are extension of a probabilistic pca model the proposed model are able to incorporate the label information into the projection phase and can naturally handle multiple output i e in multi task learning problem we derive an ecient em learning algorithm for both model and also provide theoretical justification of the model behavior sppca and s ppca are compared with other supervised projection method on various learning task and show not only promising performance but also good scalability 
we prove a quantitative connection between the expected sum of reward of a policy and binary classification performance on created subproblems this connection hold without any unobservable assumption no assumption of independence small mixing time fully observable state or even hidden state and the resulting statement is independent of the number of state or action the statement is critically dependent on the size of the reward and prediction performance of the created classifier we also provide some general guideline for obtaining good classification performance on the created subproblems in particular we discus possible method for generating training example for a classifier learning algorithm 
in many application one ha to actively select among a set of expensive observation before making an informed decision often we want to select observation which perform well when evaluated with an objective function chosen by an adversary example include minimizing the maximum posterior variance in gaussian process regression robust experimental design and sensor placement for outbreak detection in this paper we present the submodular saturation algorithm a simple and efficient algorithm with strong theoretical approximation guarantee for the case where the possible objective function exhibit submodularity an intuitive diminishing return property moreover we prove that better approximation algorithm do not exist unless np complete problem admit efficient algorithm we evaluate our algorithm on several real world problem for gaussian process regression our algorithm compare favorably with state of the art heuristic described in the geostatistics literature while being simpler faster and providing theoretical guarantee for robust experimental design our algorithm performs favorably compared to sdp based algorithm 
we propose a hybrid unsupervised document clustering approach that combine a hierarchical clustering algorithm with expectation maximization we developed several heuristic to automatically select a subset of the cluster generated by the first algorithm a the initial point of the second one furthermore our initialization algorithm generates not only an initial model for the iterative refinement algorithm but also an estimate of the model dimension thus eliminating another important element of human supervision we have evaluated the proposed system on five real world document collection the result show that our approach generates clustering solution of higher quality than both it individual component 
sparse pca seek approximate sparse eigenvectors whose projection capture the maximal variance of data a a cardinality constrained and non convex optimization problem it is np hard and yet it is encountered in a wide range of applied eld from bio informatics to nance recent progress ha focused mainly on continuous approximation and convex relaxation of the hard cardinality constraint in contrast we consider an alternative discrete spectral formulation based on variational eigenvalue bound and provide an effective greedy strategy a well a provably optimal solution using branch and bound search moreover the exact methodology used reveals a simple renormalization step that improves approximate solution obtained by any continuous method the resulting performance gain of discrete algorithm is demonstrated on real world benchmark data and in extensive monte carlo evaluation trial 
the problem of time series classification ha attracted great interest in the last decade however current research assumes the existence of large amount of labeled training data in reality such data may be very difficult or expensive to obtain for example it may require the time and expertise of cardiologist space launch technician or other domain specialist a in many other domain there are often copious amount of unlabeled data available for example the physiobank archive contains gigabyte of ecg data in this work we propose a semisupervised technique for building time series classifier while such algorithm are well known in text domain we will show that special consideration must be made to make them both efficient and effective for the time series domain we evaluate our work with a comprehensive set of experiment on diverse data source including electrocardiogram handwritten document manufacturing and video datasets the experimental result demonstrate that our approach requires only a handful of labeled example to construct accurate classifier 
we consider how a search engine should select advertisement to display with search result in order to maximize it revenue under the standard pay per click arrangement revenue depends on how well the displayed advertisement appeal to user the main diculty stem from new advertisement whose degree of appeal ha yet to be determined often the only reliable way of determining appeal is exploration via display to user which detracts from exploitation of other advertisement known to have high appeal budget constraint and finite advertisement lifetime make it necessary to explore a well a exploit in this paper we study the tradeo between exploration and exploitation modeling advertisement placement a a multi armed bandit problem we extend traditional bandit formulation to account for budget constraint that occur in search engine advertising market and derive theoretical bound on the performance of a family of algorithm we measure empirical performance via extensive experiment over real world data 
the problem of nonlinear dimensionality reduction is considered we focus on problem where prior information is available namely semi supervised dimensionality reduction it is shown that basic nonlinear dimensionality reduction algorithm such a locally linear embedding lle isometric feature mapping isomap and local tangent space alignment ltsa can be modified by taking into account prior information on exact mapping of certain data point the sensitivity analysis of our algorithm show that prior information will improve stability of the solution we also give some insight on what kind of prior information best improves the solution we demonstrate the usefulness of our algorithm by synthetic and real life example 
most work on preference learning ha focused on pairwise preference or ranking over individual item in this paper we present a method for learning preference over set of item our learning method take a input a collection of positive example that is one or more set that have been identified by a user a desirable kernel density estimation is used to estimate the value function for individual item and the desired set diversity is estimated from the average set diversity observed in the collection since this is a new learning problem we introduce a new evaluation methodology and evaluate the learning method on two data collection synthetic block world data and a new real world music data collection that we have gathered 
linear text classification algorithm work by computing an inner product between a test document vector and a parameter vector in many such algorithm including naive bayes and most tfidf variant the parameter are determined by some simple closed form function of training set statistic we call this mapping mapping from statistic to parameter the parameter function much research in text classification over the last few decade ha consisted of manual effort to identify better parameter function in this paper we propose an algorithm for automatically learning this function from related classification problem the parameter function found by our algorithm then defines a new learning algorithm for text classification which we can apply to novel classification task we find that our learned classifier outperforms existing method on a variety of multiclass text classification task 
one fundamental task in near neighbor search a well a other similarity matching effort is to find a distance function that can efficiently quantify the similarity between two object in a meaningful way in dna microarray analysis the expression level of two closely related gene may rise and fall synchronously in response to a set of experimental stimulus although the magnitude of their expression level may not be close the pattern they exhibit can be very similar unfortunately none of the conventional distance metric such a the lp norm can model this similarity effectively in this paper we study the near neighbor search problem based on this new type of similarity we propose to measure the distance between two gene by subspace pattern similarity i e whether they exhibit a synchronous pattern of rise and fall on a subset of dimension we then present an efficient algorithm for subspace near neighbor search based on pattern similarity distance and we perform test on various data set to show it effectiveness 
although information extraction and data mining appear together in many application their interface in most current system would better be described a serial juxtaposition than a tight integration information extraction populates slot in a database by identifying relevant subsequence of text but is usually not aware of the emerging pattern and regularity in the database data mining method begin from a populated database and are often unaware of where the data came from or it inherent uncertainty the result is that the accuracy of both suffers and accurate mining of complex text source ha been beyond reach in this talk i will describe work in probabilistic model that perform joint inference across multiple component of an information processing pipeline in order to avoid the brittle accumulation of error after briefly introducing conditional random field i will describe recent work in information extraction leveraging factorial state representation entity resolution and transfer learning a well a scalable method of inference and learning i ll close with some recent work on probabilistic model for social network analysis and a demonstration of rexa info a new research paper search engine 
guided by the goal of obtaining an optimization algorithm that is both fast and yield good generalization we study the descent direction maximizing the decrease in generalization error or the probability of not increasing generalization error the surprising result is that from both the bayesian and frequentist perspective this can yield the natural gradient direction although that direction can be very expensive to compute we develop an efcient general online approximation to the natural gradient descent which is suited to large scale problem we report experimental result showing much faster convergence in computation time and in number of iteration with tonga topmoumoute online natural gradient algorithm than with stochastic gradient descent even on very large datasets 
in this paper we present a general learning framework which treat the ranking problem for various information retrieval task we extend the training set generalization error bound proposed by to the ranking case and show that the use of unlabeled data can be beneficial for learning a ranking function we finally discus open issue regarding the use of the unlabeled data during training a ranking function 
the core tenet of bayesian modeling is that subject represent belief a distribution over possible hypothesis such model have fruitfully been applied to the study of learning in the context of animal conditioning experiment and analogously designed human learning task where they explain phenomenon such a retrospective revaluation that seem to demonstrate that subject entertain multiple hypothesis simultaneously however a recent quantitative analysis of individual subject record by gallistel and colleague cast doubt on a very broad family of conditioning model by showing that all of the key feature the model capture about even simple learning curve are artifact of averaging over subject rather than smooth learning curve which bayesian model interpret a revealing the gradual tradeoff from prior to posterior a data accumulate subject acquire suddenly and their prediction continue to fluctuate abruptly these data demand revisiting the model of the individual versus the ensemble and also raise the worry that more sophisticated behavior thought to support bayesian model might also emerge artifactually from averaging over the simpler behavior of individual we suggest that the suddenness of change in subject belief a expressed in conditioned behavior can be modeled by assuming they are conducting inference using sequential monte carlo sampling with a small number of sample one in our simulation ensemble behavior resembles exact bayesian model since a in particle filter it average over many sample further the model is capable of exhibiting sophisticated behavior like retrospective revaluation at the ensemble level even given minimally sophisticated individual that do not track uncertainty from trial to trial these result point to the need for more sophisticated experimental analysis to test bayesian model and refocus theorizing on the individual while at the same time clarifying why the ensemble may be of interest 
mining data stream is important in both science and commerce two major challenge are the data may grow without limit so that it is difficult to retain a long history and the underlying concept of the data may change over time different from common practice that keep recent raw data this paper us a measure of conceptual equivalence to organize the data history into a history of concept along the journey of concept change it identifies new concept a well a re appearing one and learns transition pattern among concept to help prediction different from conventional methodology that passively wait until the concept change this paper incorporates proactive and reactive prediction in a proactive mode it anticipates what the new concept will be if a future concept change take place and prepares prediction strategy in advance if the anticipation turn out to be correct a proper prediction model can be launched instantly upon the concept change if not it promptly resort to a reactive mode adapting a prediction model to the new data a system repro is proposed to implement these new idea experiment compare the system with representative existing prediction method on various benchmark data set that represent diversified scenario of concept change empirical evidence demonstrates that the proposed methodology is an effective and efficient solution to prediction for data stream 
for intrusion detection the lerad algorithm learns a succinct set of comprehensible rule for detecting anomaly which could be novel attack lerad validates the learned rule on a separate held out validation set and remove rule that cause false alarm however removing rule with possible high coverage can lead to missed detection we propose to retain these rule and associate weight to them we present three weighting scheme and our empirical result indicate that for lerad rule weighting can detect more attack than pruning with minimal computational overhead 
in this paper an algorithm called time driven document partition tdd is proposed to construct an event hierarchy in a text corpus based on a given query specifically assume that a query contains only one feature election election is directly related to the event such a u midterm election campaign u presidential election campaign and taiwan presidential election campaign where these event may further be divided into several smaller event e g the u midterm election campaign can be broken down into event such a campaign for vote election result and the resignation of donald h rumsfeld a such an event hierarchy is resulted our proposed algorithm tdd tackle the problem by three major step identify the feature that are related to the query according to both the timestamps and the content of the document the feature identified are regarded a bursty feature extract the document that are highly related to the bursty feature based on time partition the extracted document to form event and organize them in a hierarchicalstructure to the best of our knowledge there is little work targeting for constructing a feature based event hierarchy for a text corpus practically event hierarchy can assist u to efficiently locate our target information in a text corpus easily again assume that election is used for a query without an event hierarchy it is very difficult to identify what are the major event related to it when do these event happened a well a the feature and the news article that are related to each of these event we have archived two year news article to evaluate the feasibility of tdd the encouraging result indicated that tdd is practically sound and highly effective 
this paper introduces adaptor grammar a class of probabil istic model of language that generalize probabilistic context free grammar s pcfgs adaptor grammar augment the probabilistic rule of pcfgs with adaptor that can induce dependency among successive us with a particular choice of adaptor based on the pitman yor process nonparametric bayesian model of language using dirichlet process and hierarchical dirichlet proc es can be written a simple grammar we present a general purpose inference algorithm for adaptor grammar making it easy to define and use such model and ill ustrate how several existing nonparametric bayesian model can be expressed within this framework 
it is often thought that learning algorithm that track the best solution a opposed to converging to it are important only on nonstationary problem we present three result suggesting that this is not so first we illustrate in a simple concrete example the black and white problem that tracking can perform better than any converging algorithm on a stationary problem second we show the same point on a larger more realistic problem an application of temporal difference learning to computer go our third result suggests that tracking in stationary problem could be important for metalearning research e g learning to learn feature selection transfer we apply a metalearning algorithm for step size adaptation idbd sutton a to the black and white problem showing that meta learning ha a dramatic long term effect on performance whereas on an analogous converging problem meta learning ha only a small second order effect this small result suggests a way of eventually overcoming a major obstacle to meta learning research the lack of an independent methodology for task selection 
this paper concern the experimental assessment of tempering a a technique for improving bayesian inference for c rt model full bayesian inference requires the computation of a posterior over all possible tree since exact computation is not possible markov chain monte carlo mcmc method are used to produce an approximation c rt posterior have many local mode tempering aim to prevent the markov chain getting stuck in these mode our result show that a clear improvement is achieved using tempering 
predictive state representation psrs are a method of modeling dynamical system using only observable data such a action and observation to describe their model psrs use prediction about the outcome of future test to summarize the system state the best existing t echniques for discovery and learning of psrs use a monte carlo approach to explicitly estimate these outcome probability in this pap er we present a new algorithm for discovery and learning of psrs that us a gradient descent approach to compute the prediction for the current state the algorithm take advantage of the large amount of structure inherent in a valid prediction matrix to constrain it prediction f urthermore the algorithm can be used online by an agent to constantly improve it prediction quality something that current state of the art discovery and learning algorithm are unable to do we give empirical result to show that our constrained gradient algorithm is able to discover core test using very small amount of data and with larger amount of data can compute accurate prediction of the system dynamic 
the web contains lot of interesting factual information about entity such a celebrity movie or product this paper describes a robust bootstrapping approach to corroborate fact and learn more fact simultaneously this approach start with retrieving relevant page from a crawl repository for each entity in the seed set in each learning cycle known fact of an entity are corroborated first in a relevant page to find fact mention when fact mention are found they are taken a example for learning new fact from the page via html pattern discovery extracted new fact are added to the known fact set for the next learning cycle the bootstrapping process continues until no new fact can be learned this approach is language independent it demonstrated good performance in experiment on country fact result of a large scale experiment will also be shown with initial fact imported from wikipedia 
coreference analysis also known a record linkage or identity uncertainty is a difficult and important problem in natural language processing database citation matching and many other task this paper introduces several discriminative conditional probability model for coreference analysis all example of undirected graphical model unlike many historical approach to coreference the model presented here are relational they do not assume that pairwise coreference decision should be made independently from each other unlike other relational model of coreference that are generative the conditional model here can incorporate a great variety of feature of the input without having to be concerned about their dependency paralleling the advantage of conditional random field over hidden markov model we present positive result on noun phrase coreference in two standard text data set 
maximum margin discriminant analysis mmda wa proposed that us the margin idea for feature extraction it often outperforms traditional method like kernel principal component analysis kpca and kernel fisher discriminant analysis kfd however a in other kernel method it time complexity is cubic in the number of training point m and is thus computationally inefficient on massive data set in this paper we propose an approximation algorithm for obtaining the mmda feature by extending the core vector machine the resultant time complexity is only linear in m while it space complexity is independent of m extensive comparison with the original mmda kpca and kfd on a number of large data set show that the proposed feature extractor can improve classification accuracy and is also faster than these kernel based method by more than an order of magnitude 
given a set of point and a set of prototype representing them how to create a graph of the prototype whose topology account for that of the point this problem had not yet been explored in the framework of statistical learning theory in this work we propose a generative model based on the delaunay graph of the prototype and the expectationmaximization algorithm to learn the parameter this work is a rst step towards the construction of a topological model of a set of point grounded on statistic 
a new hierarchical nonparametric bayesian model is proposed for the problem of multitask learning mtl with sequential data sequential data are typically modeled with a hidden markov model hmm for which one often must choose an appropriate model structure number of state before learning here we model sequential data from each task with an infinite hidden markov model ihmm avoiding the problem of model selection the mtl for ihmms is implemented by imposing a nested dirichlet process ndp prior on the base distribution of the ihmms the ndp ihmm mtl method allows u to perform task level clustering and data level clustering simultaneously with which the learning for individual ihmms is enhanced and between task similarity are learned learning and inference for the ndp ihmm mtl are based on a gibbs sampler the effectiveness of the framework is demonstrated using synthetic data a well a real music data 
we develop an algorithmic framework to decompose a collection of time stamped text document into semantically coherent thread our formulation lead to a graph decomposition problem on directed acyclic graph for which we obtain three algorithm an exact algorithm that is based on minimum cost flow and two more efficient algorithm based on maximum matching and dynamic programming that solve specific version of the graph decomposition problem application of our algorithm include superior summarization of news search result improved browsing paradigm for large collection of text intensive corpus and integration of time stamped document from a variety of source experimental result based on over news article from a major newspaper over a period of four year demonstrate that our algorithm efficiently identify robust thread of varying length and time span 
we study the problem of learning kernel machine transductively for structured output variable transductive learning can be reduced to combinatorial optimization problem over all possible labelings of the unlabeled data in order to scale transductive learning to structured variable we transform the corresponding non convex combinatorial constrained optimization problem into continuous unconstrained optimization problem the discrete optimization parameter are eliminated and the resulting differentiable problem can be optimized efficiently we study the effectiveness of the generalized tsvm on multiclass classification and label sequence learning problem empirically 
in machine learning problem with ten of thousand of feature and only dozen or hundred of independent training example dimensionality reduction is essential for good learning performance in previous work many researcher have treated the learning problem in two separate phase first use an algorithm such a singular value decomposition to reduce the dimensionality of the data set and then use a classification algorithm such a na ve bayes or support vector machine to learn a classifier we demonstrate that it is possible to combine the two goal of dimensionality reduction and classification into a single learning objective and present a novel and efficient algorithm which optimizes this objective directly we present experimental result in fmri analysis which show that we can achieve better learning performance and lower dimensional representation than two phase approach can 
along with the blossom of open source project come the convenience for software plagiarism a company if le self disciplined may be tempted to plagiarize some open source project for it own product although current plagiarism detection tool appear sufficient for academic use they are nevertheless short for fighting against serious plagiarist for example disguise like statement reordering and code insertion can effectively confuse these tool in this paper we develop a new plagiarism detection tool called gplag which detects plagiarism by mining program dependence graph pdgs a pdg is a graphic representation of the data and control dependency within a procedure because pdgs are nearly invariant during plagiarism gplag is more effective than state of the art tool for plagiarism detection in order to make gplag scalable to large program a statistical lossy filter is proposed to prune the plagiarism search space experiment study show that gplag is both effective and efficient it detects plagiarism that easily slip over existing tool and it usually take a few second to find simulated plagiarism in program having thousand of line of code 
knowledge discovery in time series usually requires symbolic time series many discretization method that convert numeric time series to symbolic time series ignore the temporal order of value this often lead to symbol that do not correspond to state of the process generating the time series and cannot be interpreted meaningfully we propose a new method for meaningful unsupervised discretization of numeric time series called persist the algorithm is based on the kullback leibler divergence between the marginal and the self transition probability distribution of the discretization symbol it performance is evaluated on both artificial and real life data in comparison to the most common discretization method persist achieves significantly higher accuracy than existing static method and is robust against noise it also outperforms hidden markov model for all but very simple case 
we investigate the problem of learning a widely used latent variable model the latent dirichlet allocation lda or topic model using distributed computation where each of processor only see of the total data set we propose two distributed inference scheme that are motivated from different perspective the first scheme us local gibbs sampling on each processor with periodic update it is simple to implement and can be viewed a an approximation to a single processor implementation of gibbs sampling the second scheme relies on a hierarchical bayesian extension of the standard lda model to directly account for the fact that data are distributed across processor it ha a theoretical guarantee of convergence but is more complex to implement than the approximate method using five real world text corpus we show that distributed learning work very well for lda model i e perplexity and precision recall score for distributed learning are indistinguishable from those obtained with single processor learning our extensive experimental result include large scale distributed computation on virtual processor and speedup experiment of learning topic in a million word corpus using processor 
we describe a novel noisy logical distribution for representing the distribution of a binary output variable conditioned on multiple binary input variable the distribution is represented in term of noisy or s and noisy and not s of causal feature which are conjunction of the binary input the standard noisy or and noisy andnot model used in causal reasoning and artificial intelligence are special case of the noisy logical distribution we prove that the noisy logical distribution is complete in the sense that it can represent all conditional distribution provided a sufficient number of causal factor are used we illustrate the noisy logical distribution by showing that it can account for new experimental finding on how human perform causal reasoning in complex context we speculate on the use of the noisy logical distribution for causal reasoning and artificial intelligence 
this paper focus on kernel method for multi instance learning existing method require the prediction of the bag to be identical to the maximum of those of it individual instance however this is too restrictive a only the sign is important in classification in this paper we provide a more complete regularization framework for mi learning by allowing the use of different loss function between the output of a bag and it associated instance this is especially important a we generalize this for multi instance regression moreover both bag and instance information can now be directly used in the optimization instead of using heuristic to solve the resultant non linear optimization problem we use the constrained concave convex procedure which ha well studied convergence property experiment on both classification and regression data set show that the proposed method lead to improved performance 
in this paper we propose a novel probabilistic approach to summarize frequent itemset pattern such technique are useful for summarization post processing and end user interpretation particularly for problem where the resulting set of pattern are huge in our approach item in the dataset are modeled a random variable we then construct a markov random field mrf on these variable based on frequent itemsets and their occurrence statistic the summarization proceeds in a level wise iterative fashion occurrence statistic of itemsets at the lowest level are used to construct an initial mrf statistic of itemsets at the next level can then be inferred from the model we use those pattern whose occurrence can not be accurately inferred from the model to augment the model in an iterative manner repeating the procedure until all frequent itemsets can be modeled the resulting mrf model affords a concise and useful representation of the original collection of itemsets extensive empirical study on real datasets show that the new approach can effectively summarize a large number of itemsets and typically significantly outperforms extant approach 
we present a novel unsupervised learning scheme that simultaneously cluster variable of several type e g document word and author based on pairwise interaction between the type a observed in co occurrence data in this scheme multiple clustering system are generated aiming at maximizing an objective function that measure multiple pairwise mutual information between cluster variable to implement this idea we propose an algorithm that interleaf top down clustering of some variable and bottom up clustering of the other variable with a local optimization correction routine focusing on document clustering we present an extensive empirical study of two way three way and four way application of our scheme using six real world datasets including the news group ng and the enron email collection our multi way distributional clustering mdc algorithm consistently and significantly outperform previous state of the art information theoretic clustering algorithm 
in this paper we survey the current state of art model for structured learning problem including hidden markov model hmm conditional random field crf averaged perceptron ap structured svms svmstruct max margin markov network m n and an integration of search and learning algorithm searn with all due tuning effort of various parameter of each model on the data set we have applied the model to we found that svmstruct enjoys better performance compared with the others in addition we also propose a new method which we call the structured learning ensemble sle to combine these structured learning model empirical result show that our sle algorithm provides more accurate solution compared with the best result of the individual model 
to investigate the weighting of top down td and bottom up bu information in guiding human search behavior we manipulate the proportion of bu and td component in a saliency based model the model is biologically plausible and implement an artificial retina and neuronal population code the bu component is based on feature contrast the td component is defined by a feature template match to a stored target representation we compared the model s behavior at different mixture of td and bu component to the eye movement behavior of human observer performing the identical search task we found that a purely td model provides a much closer match to human behavior than any mixture model using bu information only when biological constraint are removed e g eliminating the retina did a bu td mixture model begin to approximate human behavior 
a novel algorithm for performing independent subspace analysis the estimation of hidden independent subspace is introduced this task is a generalization of independent component analysis the algorithm work by estimating the multi dimensional differential entropy the estimation utilizes minimal geodesic spanning tree matched to the sample point numerical study include i illustrative example ii a generalization of the cocktail party problem to song played by band and iii an example on mixed independent subspace where subspace have dependent source which are pairwise independent 
the time histogram method is a handy tool for capturing the instantaneous rate of spike occurrence in most of the neurophysiological literature the bin size that critically determines the goodness of the fit of the time his togram to the underlying rate ha been selected by individual researcher in an unsystematic manner we propose an objective method for selecting the bin size of a time histogram from the spike data so that the time histogram best approximates the unknown underlying rate the resolution of the histogram increase or the optimal bin size decrease with the number of spike sequence sampled it is notable that the optimal bin size diverges if only a small number of experimental trial are available from a moderately fluctuating rate process in this case any attempt to characterize the underlying spike rate will lead to spurious result given a paucity of data our method can also suggest how many more trial are needed until the set of data can be analyzed with the required resolution 
diffusion tensor magnetic resonance imaging dt mri is a non invasive method for brain neuronal fiber delineation here we show a modification for dt mri that allows delineation of neuronal fiber which are infiltrated by edema we use the muliple tensor variational mtv framework which replaces the diffusion model of dt mri with a multiple component model and fit it to the signal attenuation with a variational regularization mechanism in order to reduce free water contamination we estimate the free water compartment volume fraction in each voxel remove it and then calculate the anisotropy of the remaining compartment the variational framework wa applied on data collected with conventional clinical parameter containing only six diffusion direction by using the variational framework we were able to overcome the highly ill posed fitting the result show that we were able to find fiber that were not found by dt mri 
the cornell laboratory of ornithology s mission is to interpret and conserve the earth s biological diversity through research education and citizen science focused on bird over the year the lab ha accumulated one of the largest and longest running collection of environmental data set in existence the data set are not only large but also have many attribute contain many missing value and potentially are very noisy the ecologist are interested in identifying which feature have the strongest effect on the distribution and abundance of bird specie a well a describing the form of these relationship we show how data mining can be successfully applied enabling the ecologist to discover unanticipated relationship we compare a variety of method for measuring attribute importance with respect to the probability of a bird being observed at a feeder and present initial result for the impact of important attribute on bird prevalence 
web user display their preference implicitly by navigating through a sequence of page or by providing numeric rating to some item web usage mining technique are used to extract useful knowledge about user interest from such data the discovered user model are then used for a variety of application such a personalized recommendation web site content or semantic feature of object provide another source of knowledge for deciphering user need or interest we propose a novel web recommendation system in which collaborative feature such a navigation or rating data a well a the content feature accessed by the user are seamlessly integrated under the maximum entropy principle both the discovered user pattern and the semantic relationship among web object are represented a set of constraint that are integrated to fit the model in the case of content feature we use a new approach based on latent dirichlet allocation lda to discover the hidden semantic relationship among item and derive constraint used in the model experiment on real web site usage data set show that this approach can achieve better recommendation accuracy when compared to system using only usage information the integration of semantic information also allows for better interpretation of the generated recommendation 
brain computer interface can suffer from a large variance of the subject condition within and across session for example vigilance fluc tuations in the individual variable task involvement workload etc alter the characteristic of eeg signal and thus challenge a stable bci operation in the present work we aim to define feature based on a variant of the common spatial patte rn csp algorithm that are constructed invariant with respect to such nonstationarities we enforce invariance property by adding term to the denominator of a rayleigh coefficient representation of csp such a disturbance covariance matrix from fluctuation in visual processing in this manner physiological prior kn owledge can be used to shape the classification engine for bci a a proof of conce pt we present a bci classifier that is robust to change in the level of pariet al activity in other word the eeg decoding still work when there are lapse in vigilance 
fast gradient based method for maximum margin matrix factorization mmmf were recently shown to have great promise rennie srebro including significantly outperforming the previous state of the art method on some standard collaborative prediction benchmark including movielens in this paper we investigate way to further improve the performance of mmmf by casting it within an ensemble approach we explore and evaluate a variety of alternative way to define such ensemble we show that our resulting ensemble can perform significantly better than a single mmmf model along multiple evaluation metric in fact we find that ensemble of partially trained mmmf model can sometimes even give better prediction in total training time comparable to a single mmmf model 
parsing and translating natural language can be viewed a problem of predicting tree structure for machine learning approach to these prediction the diversity and high dimensionality of the structure involved mandate very large training set this paper present a purely discriminative learning method that scale up well to problem of this size it accuracy wa at least a good a other comparable method on a standard parsing task to our knowledge it is the first purely discriminative learning algorithm for translation with treestructured model unlike other popular method this method doe not require a great deal of feature engineering a priori because it performs feature selection over a compound feature space a it learns experiment demonstrate the method s versatility accuracy and e ciency relevant software is freely available at http nlp c nyu edu parser and http nlp c nyu edu genpar 
we consider the problem of training a conditional random fiel d crf to maximize per label predictive accuracy on a training set an ap proach motivated by the principle of empirical risk minimization we give a gradient based procedure for minimizing an arbitrarily accurate approximation of the empirical risk under a hamming loss function in experiment with both simulated and real data our optimization procedure give significantly better testing performance than several current approach for crf training especially in situati ons of high label noise 
the octopus arm is a highly versatile and complex limb how the octopus control such a hyper redundant arm not to mention eight of them is a yet unknown robotic arm based on the same mechanical principle may render present day robotic arm obsolete in this paper we tackle this control problem using an online reinforcement learning algorithm based on a bayesian approach to policy evaluation known a gaussian process temporal difference gptd learning our substitute for the real arm is a computer simulation of a dimensional model of an octopus arm even with the simplification inherent to thi s model the state space we face is a high dimensional one we apply a gptdbased algorithm to this domain and demonstrate it operation on several learning task of varying degree of difficulty 
we show that any weak ranker that can achieve an area under the roc curve slightly better than which can be achieved by random guessing can be efficiently boosted to achieve an area under the roc curve arbitrarily close to we further show that this boosting can be performed even in the presence of independent misclassification noise given access to a noise toler ant weak ranker our result it is not hard to see that an auc of can be achieved by random guessing see thus it is natural to define a weak ranker to be an algorithm t hat can achieve auc slightly above we show that any weak ranker can be boosted to a strong ranker that achieves auc arbitrarily close to the best possible value of we also consider the standard independent classification no ise model in which the label of each example is flipped with probability we show that in this setting given a noise tolerant weak ranker that achieves nontrivial auc in the presence of noisy data a described above we can boost to a strong ranker that achieves auc at least for any and any related work freund iyer schapire and singer introduced rankboost which performs ranking with more fine grained control over preference between pair of item than we consider here they performed an analysis that implies a bound on the auc of the boosted ranking function in term of a different measure of the quality of weak ranker cortes and mohri theoretically analyzed the typical relationship between the error rate of a classifier based on thresholding a scoring function and the auc obtained through the scoring function they also pointed out the close relationship between the loss function optimized by rankboost and the auc rudin cortes mohri and schapire showed that when each of two class are equally likely the loss function optimized by adaboost coincides with the loss function of rankboost noise tolerant boosting ha previously been studied for classification kalai and serve dio showed that if data is corrupted 
determining the similarity of short text snippet such a search query work poorly with traditional document similarity measure e g cosine since there are often few if any term in common between two short text snippet we address this problem by introducing a novel method for measuring the similarity between short text snippet even those without any overlapping term by leveraging web search result to provide greater context for the short text in this paper we define such a similarity kernel function and provide example of it efficacy we also show the use of this kernel function in a large scale system for suggesting related query to search engine user 
large scale logistic regression arises in many application such a document classification and natural language processing in this paper we apply a trust region newton method to maximize the log likelihood of the logistic regression model the proposed method us only approximate newton step in the beginning but achieves fast convergence in the end experiment show that it is faster than the commonly used quasi newton approach for logistic regression we also compare it with linear svm implementation 
we report that mixture of m multinomial logistic regression can be used to approximate a class of smooth probability model for multiclass response with bounded second derivative of log odds the approximation rate is o m s in hellinger distance or o m s in kullback leibler divergence here s dim x is the dimension of the input space or the number of predictor with the availability of training data of size n we also show that consistency in multiclass regression and classification can be achieved simultaneously for all class when posterior based inference is performed in a bayesian framework loosely speaking such consistency refers to performance being often close to the best possible for large n consistency can be achieved either by taking m mn or by taking m to be uniformly distributed among mn according to the prior where pr mn pr na in order a n grows for some a 
we present a new analysis for the combination of binary classifier our analysis make use of the neyman pearson lemma a a theoretical basis to analyze combination of classifier we give a method for finding the optimum l decision rule for a combination of classifier and prove that it ha the optimal r oc curve we show how our method generalizes and improves previous work on combining classifier and generating roc curve n possible boolean rule we prove that our method is optimal in this space we prove that the boolean and and or rule are always part of the optimal set for the special case of independent classifier though in general we make no indepe ndence assumption we prove a sufficient condition for provost and fawcett s method to be o ptimal 
this paper proposes constraint propagation relaxation cpr a probabilistic approach to classical constraint propagation that provides another view on the whole parametric family of survey propagation algorithm sp more importantly the approach elucidates the implicit but fundamental assumption underlying sp thus shedding some light on it effectiveness and leading to application beyond k sat 
most model of decision making in neuroscience assume an infinite horizon which yield an optimal solution that integrates evidence up to a fixed decision threshold however under most experimental a well a naturalistic behavioral setting the decision ha to be made before some finite deadl ine which is often experienced a a stochastic quantity either due to variabl e external constraint or internal timing uncertainty in this work we formulate thi s problem a sequential hypothesis testing under a stochastic horizon we use dynamic programming tool to show that for a large class of deadline distribution th e bayes optimal solution requires integrating evidence up to a threshold that declin e monotonically over time we use numerical simulation to illustrate the optimal policy in the special case of a fixed deadline and one that is drawn from a gamma dist ribution 
the discovery of subset with special property from binary data hasbeen one of the key theme in pattern discovery pattern class suchas frequent itemsets stress the co occurrence of the value in the data while this choice make sense in the context of sparse binary data it disregard potentially interesting subset of attribute that have some other type of dependency structure we consider the problem of finding all subset of attribute that have low complexity the complexity is measured by either the entropy of the projection of the data on the subset or the entropy of the data for the subset when modeled using a bayesian tree with downward or upward pointing edge we show that the entropy measure on set ha a monotonicity property and thus a levelwise approach can find all low entropy itemsets we also show that the tree based measure are bounded above by the entropy of the corresponding itemset allowing similar algorithm to be used for finding low entropy tree we describe algorithm for finding all subset satisfying an entropy condition we give an extensive empirical evaluation of the performance of the method both on synthetic and on real data we also discus the search for high entropy subset and the computation of the vapnik chervonenkis dimension of the data 
we present the first truly polynomial algorithm for pac lear ning the structure of bounded treewidth junction tree an attractive subclass of probabilistic graphical model that permit both the compact representation of probability distribution and efficient exact inference for a constant treewidth our algorithm ha polynomial time and sample complexity if a junction tree with suffi ciently strong intraclique dependency exists we provide strong theoretical guarantee in term of kl divergence of the result from the true distribution we also present a lazy extension of our approach that lead to very significant spee d ups in practice and demonstrate the viability of our method empirically on several real world datasets one of our key new theoretical insight is a method for bounding the conditional mutual information of arbitrarily large set of variable w ith only polynomially many mutual information computation on fixed size subset of variable if the underlying distribution can be approximated by a bounded treewidth junction tree 
beam search is used to maintain tractability in large search space at the expense of completeness and optimality we study supervised learning of linear ranking function for controlling beam search the goal is to learn ranking function that allow for beam search to perform nearly a well a unconstrained search while gaining computational efficiency we first study the computational complexity of the learning problem showing that even for exponentially large search space the general consistency problem is in np we also identify tractable and intractable subclass of the learning problem next we analyze the convergence of recently proposed and modified online learning algorithm we first provide a counter example to an existing convergence result and then introduce alternative notion of margin that do imply convergence finally we study convergence property for ambiguous training data 
we investigate the problem of automatically constructing efficient representation or basis function for approximating value fu nctions based on analyzing the structure and topology of the state space i n particular two novel approach to value function approximation are explored based on automatically constructing basis function on state space that can be represented a graph or manifold one approach us the eigenfunctions of the laplacian in effect performing a global fourier analysis on the graph the second approach is based on diffusion wavelet which generalize classical wavelet to graph using multiscale d ilations induced by power of a diffusion operator or random walk on the graph together these approach form the foundation of a new generation of method for solving large markov decision process in which the underlying representation and policy are simultaneously learned 
this article discus a latent variable model for inference and prediction of symmetric relational data the model based on the idea of the eigenvalue decomposition represents the relationship between two node a the weighted inner product of node specific vector of latent characteristic this eigenmodel generalizes other popular latent variable model such a latent class and distance model it is shown mathematically that any latent class or distance model ha a representation a an eigenmodel but not vice versa the practical implication of this are examined in the context of three real datasets for which the eigenmodel ha a good or better out of sample predictive performance than the other two model 
an approximate region based value iteration rbvi algorithm is proposed to find the optimal policy for a partially observable markov decision process pomdp the proposed rbvi approximates the true polyhedral partition of the belief simplex with an ellipsoidal partition such that the optimal value function is linear in each of the ellipsoidal region the position and shape of each region a well a the gradient alpha vector of the optimal value function in the region are parameterized explicitly and are estimated via efficient expectation maximization em and variational bayesian em vbem based on a set of selected sample belief point the rbvi maintains a much smaller number of alpha vector than point based method and yield a more parsimonious representation that approximates the true value function in the maximum likelihood ml sense the result on benchmark problem show that the proposed rbvi is comparable in performance to state of the art algorithm despite of the small number of alpha vector that are used 
simulated annealing is a popular method for approaching the solution of a global optimization problem existing result on it performance apply to discrete combinatorial optimization where the optimization variable can assume only a finite set of possible value we introduce a new general formulation of simulated annealing which allows one to guarantee finite time performan ce in the optimization of function of continuous variable the result hold universally for any optimization problem on a bounded domain and establish a connection between simulated annealing and up to date theory of convergence of markov chain monte carlo method on continuous domain this work is inspired by the concept of finite time learning with known accuracy and confidence deve loped in statistical learning theory 
this work provides a framework for learning sequential attention in real world visual object recognition using an architecture of three processing stage the first stage reject irrelevant local descriptor based on an information theoretic saliency measure providing candidate for focus of interest foi the second stage investigates the information in the foi using a codebook matcher and providing weak object hypothesis the third stage integrates local information via shift of attention resulting in chain of descriptor action pair that characterize object discrimination a q learner adapts then from explorative search and evaluative feedback from entropy decrease on the attention sequence eventually prioritizing shift that lead to a geometry of descriptor action scanpaths that is highly discriminative with respect to object recognition the methodology is successfully evaluated on indoors coil database and outdoors tsg database imagery demonstrating significant impact by learning outperforming standard local descriptor based method both in recognition accuracy and processing time 
we extend tree based method to the prediction of structured output using a kernelization of the algorithm that allows one to grow tree a soon a a kernel can be defined on the output space the resulting algorithm called output kernel tree ok generalizes classification and regression tree a well a tree based ensemble method in a principled way it inherits several feature of these method such a interpretability robustness to irrelevant variable and input scalability when only the gram matrix over the output of the learning sample is given it learns the output kernel a a function of input we show that the proposed algorithm work well on an image reconstruction task and on a biological network inference problem 
we present a new gaussian process gp regression model whose covariance is parameterized by the the location of m pseudo input point which we learn by a gradient based optimization we take m n where n is the number of real data point and hence obtain a sparse regression method which ha o m n training cost and o m prediction cost per test case we also find hyperparameters of the covariance function in the same joint optimization the method can be viewed a a bayesian regression model with particular input dependent noise the method turn out to be closely related to several other sparse gp approach and we discus the relation in detail we finally demonstrate it performance on some large data set and make a direct comparison to other sparse gp method we show that our method can match full gp performance with small m i e very sparse solution and it significantly outperforms other approach in this regime 
commercial datasets are often large relational and dynamic they contain many record of people place thing event and their interaction over time such datasets are rarely structured appropriately for knowledge discovery and they often contain variable whose meaning change across different subset of the data we describe how these challenge were addressed in a collaborative analysis project undertaken by the university of massachusetts amherst and the national association of security dealer nasd we describe several method for data pre processing that we applied to transform a large dynamic and relational dataset describing nearly the entirety of the u s security industry and we show how these method made the dataset suitable for learning statistical relational model to better utilize social structure we first applied known consolidation and link formation technique to associate individual with branch office location in addition we developed an innovative technique to infer professional association by exploiting dynamic employment history finally we applied normalization technique to create a suitable class label that adjusts for spatial temporal and other heterogeneity within the data we show how these pre processing technique combine to provide the necessary foundation for learning high performing statistical model of fraudulent activity 
we consider online learning where the target concept can change over time previous work on expert prediction algorithm ha bounded the worst case performance on any subsequence of the training data relative to the performance of the best expert however because these expert may be difficult to implement we take a more general approach and bound performance relative to the actual performance of any online learner on this single subsequence we present the additive expert ensemble algorithm addexp a new general method for using any online learner for drifting concept we adapt technique for analyzing expert prediction algorithm to prove mistake and loss bound for a discrete and a continuous version of addexp finally we present pruning method and empirical result for data set with concept drift 
in many prediction task selecting relevant feature is essential for achieving good generalization performance most feature selection algorithm consider all feature to be a priori equally likely to be relevant in this paper we use transfer learning learning on an ensemble of related task to construct an informative prior on feature relevance we assume that feature themselves have meta feature that are predictive of their relevance to the prediction task and model their relevance a a function of the meta feature using hyperparameters called meta prior we present a convex optimization algorithm for simultaneously learning the meta prior and feature weight from an ensemble of related prediction task which share a similar relevance structure our approach transfer the meta prior among different task which make it possible to deal with setting where task have nonoverlapping feature or the relevance of the feature vary over the task we show that learning feature relevance improves performance on two real data set which illustrate such setting predicting rating in a collaborative filtering task and distinguishing argument of a verb in a sentence 
we revisit gaussian blurring mean shift gbms a procedure that iteratively sharpens a dataset by moving each data point according to the gaussian mean shift algorithm gm we give a criterion to stop the procedure a soon a clustering structure ha arisen and show that this reliably produce image segmentation a good a those of gm but much faster we prove that gbms ha convergence of cubic order with gaussian cluster much faster than gm s which is of linear order and that the local principal component converges last which explains the powerful clustering and denoising property of gbms we show a connection with spectral clustering that suggests gbms is much faster we further accelerate gbms by interleaving connected component and blurring step achieving x x speedup without introducing an approximation error in summary our accelerated gbms is a simple fast nonparametric algorithm that achieves segmentation of state of the art quality 
we develop and analyze an algorithm for nonparametric estimation of divergence functionals and the density ratio of two probability distri butions our method is based on a variational characterization of f divergence which turn the estimation into a penalized convex risk minimization problem we present a derivation of our kernel based estimation algorithm and an analysis of convergence rate for the estimator our simulation result demonstrate the convergence behavior of the method which compare favorably with existing method in the literature 
in streaming feature selection sfs new feature are sequentially considered for addition to a predictive model when the space of potential feature is large sfs offer many advantage over traditional feature selection method which assume that all feature are known in advance feature can be generated dynamically focusing the search for new feature on promising subspace and overfitting can be controlled by dynamically adjusting the threshold for adding feature to the model we describe investing an adaptive complexity penalty method for sfs which dynamically adjusts the threshold on the error reduction required for adding a new feature investing give false discovery rate style guarantee against overfitting it differs from standard penalty method such a aic bic or ric which always drastically overor under fit in the limit of infinite number of non predictive feature empirical result show that sfs is competitive with much more compute intensive feature selection method such a stepwise regression and allows feature selection on problem with over a million potential feature 
we introduce a new algorithm for off policy temporal difference learning with function approximation that ha lower variance and requires le knowledge of the behavior policy than prior method we develop the notion of a recognizer a filter on action that distorts the behavior policy to produce a related target policy with low variance importance sampling correction we also consider target policy that are deviation from the state distribution of the behavior policy such a potential temporally abstract option which further reduces variance this paper introduces recognizers and their potential advantage then develops a full algorithm for linear function approximation and prof that it update are in the same direction a on policy td update which implies asymptotic convergence even though our algorithm is based on importance sampling we prove that it requires absolutely no knowledge of the behavior policy for the case of state aggregation function approximators 
we demonstrate the first fully hardware implementation of retinotopic self organization from photon transduction to neural map formation a silicon retina transduces patterned illumination into correlated spike train that drive a population of silicon growth cone to automatically wire a topographic mapping by migrating toward source of a diffusible guidance cue that is released by postsynaptic spike we varied the pattern of illumination to steer growth cone projected by different retinal ganglion cell type to self organize segregated or coordinated retinotopic map 
the internet take behavioral consumer research to a new level by providing the ability to passively and continuously monitor the complete online behavior of million of consumer in an opt in privacy protected manner imagine the analytical possibility if every site visited every page viewed content seen transaction conducted all of this granularity in behavior wa continuously captured with explicit consumer permission for million of consumer and privacy wa protected what unique insight could one gain into consumer behavior their interest passion and lifestyle what behavior could be predicted what commercial application would be possible 
we describe a statistical approach to software debugging in the presence of multiple bug due to sparse sampling issue and complex interaction between program predicate many generic off the shelf algorithm fail to select useful bug predictor taking inspiration from bi clustering algorithm we propose an iterative collective voting scheme for the program run and predicate we demonstrate successful debugging result on several real world program and a large debugging benchmark suite 
this paper focus on detecting how concept are linked across multiple textdocuments by generating an evidence trail explaining the connection a traditional search involving for example two or more person name willattempt to find document mentioning both of these individual this researchfocuses on a different interpretation of such a query what is the best evidencetrail across document that explains a connection between these individual for example allmay be good golfer a generalization ofthis task involves query term representing general concept e g indictment foreign policy such query reflect a special case oftext mining previous attempt to solve this problem have focused on graphapproaches involving hyperlinked document and link analysis tool exploiting named entity a new robust framework is presented based on i generating concept chain graph a hybrid content representation ii performing graph matching to select candidate subgraphs and iii subsequently using graphical model to validate hypothesis using ranked evidence trail we adapt the duc data set for cross document summarization to evaluate evidence trail generated by this approach 
we introduce flexible algorithm that can automatically learn mapping from image to action by interacting with their environment they work by introducing an image classifier in front of a reinforcement learning algorithm the classifier partition the visual space according to the presence or absence of highly informative local descriptor the image classifier is incrementally refined by selecting new local descriptor when perceptual aliasing is detected thus we reduce the visual input domain down to a size manageable by reinforcement learning permitting u to learn direct percept to action mapping experimental result on a continuous visual navigation task illustrate the applicability of the framework 
we present a novel linear clustering framework diffrac which relies on a linear discriminative cost function and a convex relaxation of a combinatorial optimization problem the large convex optimization problem is solved through a sequence of lower dimensional singular value decomposition this framework ha several attractive property although apparentl y similar to k mean it exhibit superior clustering performance than k mean in particular in term of robustness to noise it can be readily extended to non lin ear clustering if the discriminative cost function is based on positive definite k ernels and can then be seen a an alternative to spectral clustering prior inf ormation on the partition is easily incorporated leading to state of the art perfo rmance for semi supervised learning for clustering or classification we present empi rical evaluation of our algorithm on synthetic and real medium scale datasets 
this paper describes tippps time interleaved product purchase prediction system which analysis billing data of corporate customer in a large telecommunication company in order to predict high value upsell opportunity the challenge presented by this prediction problem are significant firstly the diversity of product used by corporate telecommunication customer is huge this coupled with low product take up rate make this a problem of learning from a very high dimensional feature space with very few minority example further it is important to give priority specifically to the identification of those new customer who are of high value these challenge are overcome by introducing a number of modification to standard data pre processing and machine learning algorithm the most important of which are time interleaving of data and value weighting time interleaving is the concatenation of example from multiple time period thus increasing the number of training example and hence the number of minority example value weighting assigns importance to minority example in proportion to the dollar value of take up thus biasing the system to identify high value customer these modification create a novel algorithm that make the prediction system practical and usable comparison with other technique designed for similar problem show that the expected average improvement in ranking accuracy achieved using these modification is tippps ha been in operation for several month and ha been successful in identifying many upsell opportunity that were not identified by using the previous manual system 
human are extremely adept at learning new skill by imitating the action of others a progression of imitative ability ha been observed in child ranging from imitation of simple body movement to goalbased imitation based on inferring intent in this paper we show that the problem of goal based imitation can be formulated a one of inferring goal and selecting action using a learned probabilistic graphical model of the environment we first describe algorithm for planning action to achieve a goal state using probabilistic inference we then describe how planning can be used to bootstrap the learning of goal dependent policy by utilizing feedback from the environment the resulting graphical model is then shown to be powerful enough to allow goal based imitation using a simple maze navigation task we illustrate how an agent can infer the goal of an observed teacher and imitate the teacher even when the goal are uncertain and the demonstration is incomplete to appear in advance in nip 
in this paper we present a general framework to discover spatial association and spatio temporal episode for scientific datasets in contrast to previous work in this area feature are modeled a geometric object rather than point we define multiple distance metric that take into account object extent and thus are more robust in capturing the influence of an object on other object in spatial neighborhood we have developed algorithm to discover four different type of spatial object interaction association pattern we also extend our approach to accommodate temporal information and propose a simple algorithm to derive spatio temporal episode we show that such episode can be used to reason about critical event we evaluate our framework on real datasets to demonstrate it efficacy the datasets originate from two different area computational molecular dynamic and computational fluid flow we present result highlighting the importance of the identified pattern and episode by using knowledge from the underlying domain we also show that the proposed algorithm scale linearly with respect to the dataset size 
the use of bayesian network for classification problem ha received significant recent attention although computationally efficient the standard maximum likelihood learning method tends to be suboptimal due to the mismatch between it optimization criterion data likelihood and the actual goal for classification label prediction recent approach to optimizing the classification performance during parameter or structure learning show promise but lack the favorable computational property of maximum likelihood learning in this paper we present the boosted augmented naive bayes ban classifier we show that a combination of discriminative data weighting with generative training of intermediate model can yield a computationally efficient method for discriminative parameter learning and structure selection 
we present a new unsupervised algorithm for training structured predictor that is discriminative convex and avoids the use of em the idea is to formulate an unsupervised version of structured learning method such a maximum margin markov network that can be trained via semidefinite programming the result is a discriminative training criterion for structured predictor like hidden markov model that remains unsupervised and doe not create local minimum to reduce training cost we reformulate the training procedure to mitigate the dependence on semidefinite programming and finally propose a heuristic procedure that avoids semidefinite programming entirely experimental result show that the convex discriminative procedure can produce better conditional model than conventional baum welch em training 
regularized kernel discriminant analysis rkda performs linear discriminant analysis in the feature space via the kernel trick the performance of rkda depends on the selection of kernel in this paper we consider the problem of learning an optimal kernel over a convex set of kernel we show that the kernel learning problem can be formulated a a semidefinite program sdp in the binary class case we further extend the sdp formulation to the multi class case it is based on a key result established in this paper that is the multi class kernel learning problem can be decomposed into a set of binary class kernel learning problem in addition we propose an approximation scheme to reduce the computational complexity of the multi class sdp formulation the performance of rkda also depends on the value of the regularization parameter we show that this value can be learned automatically in the framework experimental result on benchmark data set demonstrate the efficacy of the proposed sdp formulation 
statistic on network have become vital to the study of relational data drawn from area such a bibliometrics fraud detection bioinformatics and the internet calculating many of the most important measure such a betweenness centrality closeness centrality and graph diameter requires identifying short path in these network however finding these short path can be intractable for even moderate size network we introduce the concept of a network structure index nsi a composition of a set of annotation on every node in the network and a function that us the annotation to estimate graph distance between pair of node we present several variety of nsis examine their time and space complexity and analyze their performance on synthetic and real data set we show that creating an nsi for a given network enables extremely efficient and accurate estimation of a wide variety of network statistic on that network 
we introduce the controlled predictive linear gaussian model cplg a model that us predictive state to model discrete time dynamical system with real valued observation and vector valued action this extends the plg an uncontrolled model recently introduced by rudary et al we show that the cplg subsumes controlled linear dynamical system lds also called kalman filter model of equal dimension but requires fewer parameter we also introduce the predictive linear quadratic gaussian problem a cost minimization problem based on the cplg that we show is equivalent to linear quadratic gaussian problem lqg sometimes called lqr we present an algorithm to estimate cplg parameter from data and show that our algorithm is a consistent estimation procedure finally we present empirical result suggesting that our algorithm performs favorably compared to expectation maximization on controlled lds model 
semi supervised learning algorithm have been successfully applied in many application with scarce labeled data by utilizing the unlabeled data one important category is graph based semi supervised learning algorithm for which the performance depends considerably on the quality of the graph or it hyperparameters in this paper we deal with the le explored problem of learning the graph we propose a graph learning method for the harmonic energy minimization method this is done by minimizing the leave one out prediction error on labeled data point we use a gradient based method and designed an efficient algorithm which significantly accelerates the calculation of the gradient by applying the matrix inversion lemma and using careful pre computation experimental result show that the graph learning method is effective in improving the performance of the classification algorithm 
we present a new subgoal based method for automatically creating useful skill in reinforcement learning our method identifies subgoals by partitioning local state transition graph those that are constructed using only the most recent experience of the agent the local scope of our subgoal discovery method allows it to successfully identify the type of subgoals we seek state that lie between two densely connected region of the state space while producing an algorithm with low computational cost 
policy evaluation is a critical step in the approximate solution of large markov decision process mdps typically requiring o s to directly solve the bellman system of s linear equation where s is the state space size in the discrete case and the sample size in the continuous case in this paper we apply a recently introduced multiscale framework for analysis on graph to design a faster algorithm for policy evaluation for a fixed policy this framework efficiently construct a multiscale decomposition of the random walk p associated with the policy this enables efficiently computing medium and long term state distribution approximation of value function and the direct computation of the potential operator i p needed to solve bellman s equation we show that even a preliminary non optimized version of the solver competes with highly optimized iterative technique requiring in many case a complexity of o s 
due to it occurrence in engineering domain and implication for natural learning the problem of utilizing unlabeled data is attracting increasing attention in machine learning a large body of recent literature ha focussed on the transductive setting where label of unlabeled example are estimated by learning a function defined only over the point cloud data in a truly semi supervised setting however a learning machine ha access to labeled and unlabeled example and must make prediction on data point never encountered before in this paper we show how to turn transductive and standard supervised learning algorithm into semi supervised learner we construct a family of data dependent norm on reproducing kernel hilbert space rkhs these norm allow u to warp the structure of the rkhs to reflect the underlying geometry of the data we derive explicit formula for the corresponding new kernel our approach demonstrates state of the art performance on a variety of classification task 
we develop a mixture based approach to robust density modeling and outlier detection for experimental multivariate data that includes measurement error information our model is designed to infer atypical measurement that are not due to error aiming to retrieve potentially interesting peculiar object since exact inference is not possible in this model we develop a tree structured variational em solution this compare favorably against a fully factorial approximation scheme approaching the accuracy of a markov chain em while maintaining computational simplicity we demonstrate the benefit of including measurement error in the model in term of improved outlier detection rate in varying measurement uncertainty condition we then use this approach for detecting peculiar quasar from an astrophysical survey given photometric measurement with error 
we present a probabilistic generative model of entity relationship and their attribute that simultaneously discovers group among the entity and topic among the corresponding textual attribute block model of relationship data have been studied in social network analysis for some time here we simultaneously cluster in several modality at once incorporating the attribute here word associated with certain relationship significantly joint inference allows the discovery of topic to be guided by the emerging group and vice versa we present experimental result on two large data set sixteen year of bill put before the u s senate comprising their corresponding text and voting record and thirteen year of similar data from the united nation we show that in comparison with traditional separate latent variable model for word or blockstructures for vote the group topic model s joint inference discovers more cohesive group and improved topic 
abstract significant vulnerability have recently been identified in collaborative filtering recommender system these vulnerability mostly emanate from the open nature of such system and their reliance on user specified judgment for building profile attacker can easily introduce biased data in an attempt to force the system to adapt in a manner advantageous to them our research in secure personalization is exam ining a range of attack model from the simple to the complex and a variety of recommendation technique in this chapter we explore an at tack model that focus on a subset of user with similar taste and show that such an attack can be highly successful against both user based and item based collaborative filtering we also introduce a detection model that can significantly decrease the impact of this attack 
this paper describes a method for unsupervised classification of event in multi camera indoors surveillance video this research is a part of the multiple sensor indoor surveillance msis project which us axis webcam that observe an office environment the research wa inspired by the following practical problem how automatically classify and visualize a hour long video captured by camera raw data are sequence of jpeg image captured by webcam at the rate hz the following feature are extracted from the image data foreground pixel spatial distribution and color histogram the data are integrated by event by averaging motion and color feature and creating a summary frame which accumulates all foreground pixel of frame of the event into one image the self organizing map som approach is applied to event data for clustering and visualization one level and two level som clustering are used a tool for browsing result allows exploring unit of the som map at different level of hierarchy cluster of unit and distance between unit in d space a special technique ha been developed to visualize rare event the result are presented and discussed 
up to now even subject that are expert in the use of machine learning based bci system still have to undergo a calibration session of about min from this data their movement intention are so far infered we now propose a new paradigm that allows to completely omit such calibration and instead transfer knowledge from prior session to achieve this goal we first d efine normalized csp feature and distance in between second we derive prototypical feature across session a by clustering or b by feature concate nation method finally we construct a classifier based on these individualized prot otypes and show that indeed classifier can be successfully transferred to a new session for a number of subject 
abstract there ha been a surge of interest in learning non linear manifold model to approximate high dimensional data both for computational complexity reason and for generalization capability sparsity is a desired feature in such model this usually mean dimensionality reduction which naturally implies estimating the intrinsic dimension but it can also mean selecting a subset of the data to use a landmark which is especially important because many existing algorithm have quadratic complexity in the number of observation this paper present an algorithm for selecting landmark based on lasso regression which is well known to favor sparse approximation because it us regularization with an l norm a an added benefit a continuous manifold parameterization based on the landmark is also found experimental result with synthetic and real data illustrate the algorithm 
the variational bayesian framework ha been widely used to approximate the bayesian learning in various application it ha provided computational tractability and good generalization performance in this paper we discus the variational bayesian learning of the mixture of exponential family and provide some additional theoretical support by deriving the asymptotic form of the stochastic complexity the stochastic complexity which corresponds to the minimum free energy and a lower bound of the marginal likelihood is a key quantity for model selection it also enables u to discus the effect of hyperparameters and the accuracy of the variational bayesian approach a an approximation of the true bayesian learning 
probabilistic modelling of text data in the bag of word representation ha been dominated by directed graphical model such a plsi lda nmf and discrete pca recently state of the art performance on visual object recognition ha also been reported using variant of these model we introduce an alternative undirected graphical model suitable for modelling count data this rate adapting poisson rap model is shown to generate superior dimensionally reduced representation for subsequent retrieval or classification model are trained using contrastive divergence while inference of latent topical representation is efficiently achieved through a simple matrix multiplication 
in this paper we consider the problem of classification in the presence of pairwise constraint which consist of pair of example a well a a binary variable indicating whether they belong to the same class or not we propose a method which can effectively utilize pairwise constraint to construct an estimator of the decision boundary and we show that the resulting estimator is sign insensitive consistent with respect to the optimal linear decision boundary we also study the asymptotic variance of the estimator and extend the method to handle both labeled and pairwise example in a natural way several experiment on simulated datasets and real world classification datasets are conducted the result not only verify the theoretical property of the proposed method but also demonstrate it practical value in application 
the support confidence framework is the most common measure used in itemset mining algorithm for it antimonotonicity that effectively simplifies the search lattice this computational convenience brings both quality and statistical flaw to the result a observed by many previous study in this paper we introduce a novel algorithm that produce itemsets with ranked statistical merit under sophisticated test statistic such a chi square risk ratio odds ratio etc our algorithm is based on the concept of equivalence class an equivalence class is a set of frequent itemsets that always occur together in the same set of transaction therefore itemsets within an equivalence class all share the same level of statistical significance regardless of the variety of test statistic a an equivalence class can be uniquely determined and concisely represented by a closed pattern and a set of generator we just mine closed pattern and generator taking a simultaneous depth first search scheme this parallel approach ha not been exploited by any prior work we evaluate our algorithm on two aspect in general we compare to lcm and fpclose which are the best algorithm tailored for mining only closed pattern in particular we compare to epminer which is the most recent algorithm for mining a type of relative risk pattern known a minimal emerging pattern experimental result show that our algorithm is faster than all of them sometimes even multiple order of magnitude faster these statistically ranked pattern and the efficiency have a high potential for real life application especially in biomedical and financial field where classical test statistic are of dominant interest 
this paper suggests a method for multiclass learning with many class by simultaneously learning shared characteristic common to the class and predictor for the class in term of these characteristic we cast this a a convex optimization problem using trace norm regularization and study gradient based optimization both for the linear case and the kernelized setting 
we present a novel algorithm called click that find cluster in categorical datasets based on a search for k partite maximal clique unlike previous method click mine subspace cluster it us a selective vertical method to guarantee complete search click outperforms previous approach by over an order of magnitude and scale better than any of the existing method for high dimensional datasets these result are demonstrated in a comprehensive performance study on real and synthetic datasets 
we present a learning algorithm for undiscounted reinforcement learning our interest lie in bound for the algorithm s online performance after some finite number of step in the spirit of similar method already successfully applied for the exploration exploitation tradeoff in multi armed bandit problem we use upper confidence bound to show that our ucrl algorithm achieves logarithmic online regret in the number of step taken with respect to an optimal policy 
in many domain a bayesian network s topological structure is not known a priori and must be inferred from data this requires a scoring function to measure how well a proposed network topology describes a set of data many commonly used score such a bd bde bdeu etc are not well suited for class discrimination instead score such a the class conditional likelihood ccl should be employed unfortunately ccl doe not decompose and it application to large domain is not feasible we introduce a decomposable score approximate conditional likelihood acl that is capable of identifying class discriminative structure we show that dynamic bayesian network dbns trained with acl have classification efficacy competitive to those trained with ccl on a set of simulated data experiment we also show that acl trained dbns outperform bde trained dbns gaussian na ve bayes network and support vector machine within a neuroscience domain too large for ccl 
this work introduces distance based criterion for segmentation of object trajectory segmentation lead to simplification of the original object into smaller le complex primitive that are better suited for storage and retrieval purpose previous work on trajectory segmentation attacked the problem locally segmenting separately each trajectory of the database therefore they did not directly optimize the inter object separability which is necessary for mining operation such a searching clustering and classification on large database in this paper we analyze the trajectory segmentation problem from a global perspective utilizing data aware distance based optimization technique which optimize pairwise distance estimate hence leading to more efficient object pruning we first derive exact solution of the distance based formulation due to the intractable complexity of the exact solution we present anapproximate greedy solution that exploit forward searching of locally optimal solution since the greedy solution also imposes a prohibitive computational cost we also put forward more light weight variance based segmentation technique which intelligently relax the pairwise distance only in the area that affect the least the mining operation 
we present a comprehensive suite of experimentation on the subject of learning from imbalanced data when class are imbalanced many learning algorithm can suffer from the perspective of reduced performance can data sampling be used to improve the performance of learner built from imbalanced data is the effectiveness of sampling related to the type of learner do the result change if the objective is to optimize different performance metric we address these and other issue in this work showing that sampling in many case will improve classifier performance 
we propose new pac bayes bound for the risk of the weighted majority vote that depend on the mean and variance of the error of it associated gibbs classifier we show that these bound can be smaller than the risk of the gibbs classifier and can be arbitrarily close to zero even if the risk of the gibbs classifier is close to moreover we show that these bound can be uniformly estimated on the training data for all possible posterior q moreover they can be improved by using a large sample of unlabelled data 
classifier that refrain from classification in certain case can significantly reduce the misclassification cost however the parameter for such abstaining classifier are often set in a rather ad hoc manner we propose a method to optimally build a specific type of abstaining binary classifier using roc analysis these classifier are built based on optimization criterion in the following three model cost based bounded abstention and bounded improvement we demonstrate the usage and application of these model to effectively reduce misclassification cost in real classification system the method ha been validated with a roc building algorithm and cross validation on uci kdd datasets 
the introduction of relational reinforcement learning and the rrl algorithm gave rise to the development of several first order regression algorithm so far these algorithm have employed either a model based approach or an instance based approach a a consequence they suer from the typical drawback of model based learning such a coarse function approximation or those of lazy learning such a high computational intensity in this paper we develop a new regression algorithm that combine the strong point of both approach and try to avoid the normally inherent draw back by combining model based and instance based learning we produce an incremental first order regression algorithm that is both computationally ecient and produce better prediction earlier in the learning experiment 
analyzing data on board a spacecraft a it is collected enables several advanced spacecraft capability such a prioritizing observation to make the best use of limited bandwidth and reacting to dynamic event a they happen in this paper we describe how we addressed the unique challenge associated with on board mining of data a it is collected uncalibrated data noisy observation and severe limitation on computational and memory resource the goal of this effort which fall into the emerging application area of spacecraft based data mining wa to study three specific science phenomenon on mar following previous work that used a linear support vector machine svm on board the earth observing eo spacecraft we developed three data mining technique for use on board the mar odyssey spacecraft these method range from simple thresholding to state of the art reduced set svm technology we tested these algorithm on archived data in a flight software testbed we also describe a significant serendipitous science discovery of this data mining effort the confirmation of a water ice annulus around the north polar cap of mar we conclude with a discussion on lesson learned in developing algorithm for use on board a spacecraft 
brain computer interface bci system create a novel communication channel from the brain to an output device by bypassing conventional motor output pathway of nerve and muscle therefore they could provide a new communication and control option for paralyzed patient modern bci technology is essentially based on technique for the classification of single trial brain signal here we present a n ovel technique that allows the simultaneous optimization of a spatial and a spectral filter enhancing discriminability of multi channel eeg single trial the evaluation of experiment involving different subject demonstrates the superiority of the proposed algorithm apart from the enhanced classification the spatial and or the spectral filter that are de termined by the algorithm can also be used for further analysis of the data e g for source localization of the respective brain rhythm 
we address feature selection problem for classification of small sample and high dimensionality a practical example is microarray based cancer classification problem where sample size is typically le than and number of feature is several thousand or higher one of the commonly used method in addressing this problem is recursive feature elimination rfe method which utilizes the generalization capability embedded in support vector machine and is thus suitable for small sample problem we propose a novel method using minimum reference set mr generated by the nearest neighbor rule mr is the set of minimum number of sample that correctly classify all the training sample it is related to structural risk minimization principle and thus lead to good generalization the proposed mr based method is compared to rfe method with several real datasets and experimental result show that the mr method produce better classification performance 
functional magnetic resonance imaging fmri ha enabled scientist to look into the active brain however interactivity between functional brain region is still little studied in this paper we contribute a novel framework for modeling the interaction between multiple active brain region using dynamic bayesian network dbns a generative mod el for brain activation pattern this framework is applied to modeling of neuronal circuit associated with reward the novelty of our framework from a machine learning perspective lie in the use of dbns to reveal the brain connectivity and interactivity such interactivity model which are derived from fmri data are then validated through a group classifica tion task we employ and compare four different type of dbns parallel hidden markov model coupled hidden markov model fully linked hidden markov model and dynamically multi linked hmms dml hmm moreover we propose and compare two scheme of learning dml hmms experimental result show that by using dbns group classification can be performed even if the dbns are constructed from a few a brain region we also demonstrate that by using the pro posed learning algorithm different dbn structure characterize drug addicted subject v control subject this finding provides an indepen dent test for the effect of psychopathology on brain function in general we demonstrate that incorporation of computer science principle into functional neuroimaging clinical study provides a novel approach for probing human brain function 
conditional random field crfs are an effective tool for a variety of different data segmentation and labelling task including visual scene interpretation which seek to partition image into their constituent semantic level region and assign appropriate class label to each region for accurate labelling it is important to capture the global context of the image a well a local information we introduce a crf based scene labelling model that incorporates both local feature and feature aggregated over the whole image or large section of it secondly traditional crf learning requires fully labelled datasets complete labellings are typically costly and troublesome to produce we introduce an algorithm that allows crf model to be learned from datasets where a substantial fraction of the node are unlabeled it work by marginalizing out the unknown label so that the log likelihood of the known one can be maximized by gradient ascent loopy belief propagation is used to approximate the marginals needed for the gradient and log likelihood calculation and the bethe free energy approximation to the log likelihood is monitored to control the step size our experimental result show thatincorporatingtop downaggregatefeaturessignificantlyimprovesthesegmentations and that effective model can be learned from fragmentary labellings the resulting method give scene segmentation result comparable to the state of theart on three different image database 
rare category detection is an open challenge for active learning especially in the de novo case no labeled example but of significant practical importance for data mining e g detecting new financial transaction fraud pattern where normal legitimate transaction dominate this paper develops a new method for detecting an instance of each minority class via an unsupervised local density differential sampling strategy essentially a variable scale nearest neighbor process is used to optimize the probability of sampling tightly grouped minority class subject to a local smoothness assumption of the majority class result on both synthetic and real data set are very positive detecting each minority class with only a fraction of the actively sampled point required by random sampling and by pelleg s interleave method the prior best technique in the sparse literature on this topic 
we describe an unsupervised method for learning a probabilistic grammar of an object from a set of training example our approach is invariant to the scale and rotation of the object we illustrate our approach using thirteen object from the caltech database in addition we learn the model of a hybrid object class where we do not know the specific object or it position scale or pose this is illustrated by learning a hybrid class consisting of face motorbike and airplane the individual object can be recovered a different aspect of the grammar for the object class in all case we validate our result by learning the probability grammar from training datasets and evaluating them on the test datasets we compare our method to alternative approach the advantage of our approach is the speed of inference under one second the parsing of the object and increased accuracy of performance moreover our approach is very general and can be applied to a large range of object and structure 
data clustering is an important task in many discipline a large number of study have attempted to improve clustering by using the side information that is often encoded a pairwise constraint however these study focus on designing special clustering algorithm that can effectively exploit the pairwise constraint we present a boosting framework for data clustering termed a boostcluster that is able to iteratively improve the accuracy of any given clustering algorithm by exploiting the pairwise constraint the key challenge in designing a boosting framework for data clustering is how to influence an arbitrary clustering algorithm with the side information since clustering algorithm by definition are unsupervised the proposed framework address this problem by dynamically generating new data representation at each iteration that are on the one hand adapted to the clustering result at previous iteration by the given algorithm and on the other hand consistent with the given side information our empirical study show that the proposed boosting framework is effective in improving the performance of a number of popular clustering algorithm k mean partitional singlelink spectral clustering and it performance is comparable to the state of the art algorithm for data clustering with side information 
in this paper we extend the recently proposed core vector machine algorithm to the regression setting by generalizing the underlying minimum enclosing ball problem the resultant core vector regression cvr algorithm can be used with any linear nonlinear kernel and can obtain provably approximately optimal solution it asymptotic time complexity is linear in the number of training pattern m while it space complexity is independent of m experiment show that cvr ha comparable performance with svr but is much faster and produce much fewer support vector on very large data set it is also successfully applied to large d point set in computer graphic for the modeling of implicit surface 
a number of update for density matrix have been developed recently that are motivated by relative entropy minimization problem the update involve a softmin calculation based on matrix log and matrix exponential we show that these update can be kernelized this is important because the bound provable for these algorithm are logarithmic in the feature dimension provided that the norm of feature vector is bounded by a constant the main problem we focus on is the kernelization of an online pca algorithm which belongs to this family of update 
abstract many real world machine learning problem can be recast a multi task learning problem whose objectiveis to utilize the relation among those task in order to obtain a better generalization performancethan learning them individually in this paper we present two probabilistic model for solving multi tasklearning problem which have a sparsity underlying assumption in particular our model are specialcases of hierarchical bayesian model which associate the generation of task 
we consider the problem of learning a similarity function from a set of positive equivalence constraint i e similar point pair we define the similarity in information theoretic term a the gain in coding length when shifting from independent encoding of the pair to joint encoding under simple gaussian assumption this formulation lead to a non mahalanobis similarity function which is efficient and simple to learn this function can be viewed a a likelihood ratio test and we show that the optimal similarity preserving projection of the data is a variant of fisher linear discriminant we also show that under some naturally occurring sampling condition of equivalence constraint this function converges to a known mahalanobis distance rca the suggested similarity function exhibit superior performance over alternative mahalanobis distance learnt from the same data it superiority is demonstrated in the context of image retrieval and graph based clustering using a large number of data set 
we consider the task of depth estimation from a single monocular image we take a supervised learning approach to this problem in which we begin by collecting a training set of monocular image of unstructured outdoor environment which include forest tree building etc and their corresponding ground truth depthmaps then we apply supervised learning to predict the depthmap a a function of the image depth estimation is a challenging problem since local feature alone are insufficient to estimate depth at a point and one need to consider the global context of the image our model us a discriminatively trained markov random field mrf that incorporates multiscale localand global image feature and model both depth at individual point a well a the relation between depth at different point we show that even on unstructured scene our algorithm is frequently able to recover fairly accurate depthmaps 
given it importance the problem of predicting rare class in large scale multi labeled data set ha attracted great attention in the literature however the rare class problem remains a critical challenge because there is no natural way developed for handling imbalanced class distribution this paper thus fill this crucial void by developing a method for classification using local clustering cog specifically for a data set with an imbalanced class distribution we perform clustering within each large class and produce sub class with relatively balanced size then we apply traditional supervised learning algorithm such a support vector machine svms for classification indeed our experimental result on various real world data set show that our method produce significantly higher prediction accuracy on rare class than state of the art method furthermore we show that cog can also improve the performance of traditional supervised learning algorithm on data set with balanced class distribution 
for hidden markov model hmms with fully connected transition model the three fundamental problem of evaluating the likelihood of an observation sequence estimating an optimal state sequence for the observation and learning the model parameter all have quadratic time complexity in the number of state we introduce a novel class of non sparse markov transition matrix called dense mostly constant dmc transition matrix that allow u to derive new algorithm for solving the basic hmm problem in sub quadratic time we describe the dmc hmm model and algorithm and attempt to convey some intuition for their usage empirical result for these algorithm show dramatic speedup for all three problem in term of accuracy the dmc model yield strong result and outperforms the baseline algorithm even in domain known to violate the dmc assumption 
we present a solution for inferring hidden state from sensorimotor experience when the environment take the form of a pomdp with deterministic transition and observation function such environment can appear to be arbitrarily complex and non deterministic on the surface but are actually deterministic with respect to the unobserved underlying state we show that there always exists a finite history based representation that fully capture the unobserved world state allowing for perfect prediction of action effect this representation take the form of a looping prediction suffix tree pst we derive a sound and complete algorithm for learning a looping pst from a sufficient sample of sensorimotor experience we also give empirical illustration of the advantage conferred by this approach and characterize the approximation to the looping pst that are made by existing algorithm such a variable length markov model utile suffix memory and causal state splitting reconstruction 
the problem of computing a resample estimate for the reconstruction error in pca is reformulated a an inference problem with the help of the replica method using the expectation consistent ec approximation the intractable inference problem can be solved efficiently with two variational parameter a perturbative correction to the result is computed and an alternative simplified derivation is also presented 
a geometric construction is presented which is shown to be an effective tool for understanding and implementing multi category support vector classification it is demonstrated how this construction can be used to extend many other existing two class kernel based classification methodology in a straightforward way while still preserving attractive property of individual algorithm reducing training time through incorporating the result of pairwise classification is also discussed and experimental result presented 
correspondence algorithm typically struggle with shape that display part based variation we present a probabilistic approach that match shape using independent part transformation where the part themselves are learnt during matching idea from semi supervised learning are used to bias the algorithm towards finding perceptually valid part structure shape are repre sented by unlabeled point set of arbitrary size and a background component is used to handle occlusion local dissimilarity and clutter thus unlike many shape matching technique our approach can be applied to shape extracted from real image model parameter are estimated using an em algorithm that alternate between finding a soft correspondence and computing the optimal part transformation using procrustes analysis 
in this paper we focus on the issue of normalization of the affinity matrix in spectral clustering we show that the difference between n cut and ratio cut is in the error measure being used relative entropy versus l norm in finding the closest doubly stochastic matrix to the input affinity matrix we then develop a scheme for finding the optimal under frobenius norm doubly stochastic approximation using von neumann s successive projection lemma the new normalization scheme is simple and efficient and provides superior clustering performance over many of the standardized test 
an effective approach to detecting anomalous point in a data setis distance based outlier detection this paper describes a simplesampling algorithm to effciently detect distance based outlier indomains where each and every distance computation is veryexpensive unlike any existing algorithm the sampling algorithmrequires a xed number of distance computation and can return goodresults with accuracy guarantee the most computationallyexpensive aspect of estimating the accuracy of the result issorting all of the distance computed by the sampling algorithm the experimental study on two expensive domain a well a tenadditional real life datasets demonstrates both the effciency andeffectiveness of the sampling algorithm in comparison with thestate of the art algorithm and there liability of the accuracyguarantees 
many of today s best classification result are obtained by combining the response of a set of base classifier to produce an answer for the query this paper explores a novel query specific combination rule after learning a set of simple belief network classifier we produce an answer to each query by combining their individual response using weight based inversely on their respective variance around their response these variance are based on the uncertainty of the network parameter which in turn depend on the training datasample in essence this variance quantifies the base classifier s confidence of it response to this query our experimental result show that these mixture using variance belief net classifier muvs work effectively especially when the base classifier are learned using balanced bootstrap sample and when their result are combined using james stein shrinkage we also found that our variance based combination rule performed better than both bagging and adaboost even on the set of base classifier produced by adaboost itself finally this framework is extremely efficient a both the learning and the classification component require only straight line code 
graph clustering ha become ubiquitous in the study of relational data set we examine two simple algorithm a new graphical adaptation of the k medoids algorithm and the girvan newman method based on edge betweenness centrality we show that they can be effective at discovering the latent group or community that are defined by the link structure of a graph however both approach rely on prohibitively expensive computation given the size of modern relational data set network structure index nsis are a proven technique for indexing network structure and efficiently finding short path we show how incorporating nsis into these graph clustering algorithm can overcome these complexity limitation we also present promising quantitative and qualitative evaluation of the modified algorithm on synthetic and real data set 
the goal of our research is to develop a general framework for representing and reasoning about temporal relation between event in smart home datasets and to determine the benefit of this reasoning for event prediction in earlier work we performed prediction based solely on the sequence of observed activity in this work we supplement evidence for a particular action using the temporal relation information we compare the predictive accuracy with and without temporal information and illustrate the benefit of temporal relationship for prediction of smart home event and activity 
the viterbi algorithm is an efficient and optimal method for decoding linear chain markov model however the entire input sequence must be observed before the label for any time step can be generated and therefore viterbi cannot be directly applied to online interactive streaming scenario without incurring significant possibly unbounded latency a widely used approach is to break the input stream into fixed size window and apply viterbi to each window larger window lead to higher accuracy but result in higher latency we propose several alternative algorithm to the fixed sized window decoding approach these approach compute a certainty measure on predicted label that allows u to trade off latency for expected accuracy dynamically without having to choose a fixed window size up front not surprisingly this more principled approach give u a substantial improvement over choosing a fixed window we show the effectiveness of the approach for the task of spotting semi structured information in large document when compared to full viterbi the approach suffers a percent error degradation with a average latency of time step versus the potentially infinite latency of viterbi when compared to fixed window viterbi we achieve a x reduction in error and x reduction in latency 
we describe a new algorithm relaxed survey propagation rsp for finding map configuration in markov random field we compare it performance with state of the art algorithm including the max product belief propagation it sequential tree reweighted variant residual sum product belief propagation and tree structured expectation propagation we show that it outperforms all approach for ising model with mixed coupling a well a on a web person disambiguation task formulated a a supervised clustering problem 
under natural viewing condition human observer shift their gaze to allocate processing resource to subset of the visual input many computational model try to predict such voluntary eye and attentional shift although the important role of high level stimulus property e g semantic information in search stand undisputed most model are based on low level image property we here demonstrate that a combined model of face detection and low level saliency significantly outperforms a low level model in predicting location human fixate on based on eye movement recording of human observing photograph of natural scene most of which contained at least one person observer even when not instructed to look for anything particular fixate on a face with a probability of over within their first two fixation furthermore they exhibit more similar scanpaths when face are present remarkably our model s predictive performance in image that do not contain face is not impaired and is even improved in some case by spurious face detector response 
protein function prediction i e classification of protein according to their biological function is an important task in bioinformatics in this chapter we illustrate that the presence of sequence motif element that are conserved across different protein are highly discriminative feature for predicting the function of a protein this is in agreement with the biological thinking that considers motif to be the building block of protein sequence we focus on protein annotated a enzyme and show that despite the fact that motif composition is a very high dimensional representation of a sequence that most class of enzyme can be classified using a handful of motif yielding accurate and interpretable classifier the enzyme data fall into a large number of class we find that the one against the rest multi class method work better than the one against one method on this data 
we provide a pac bayesian bound for the expected loss of convex combination of classifier under a wide class of loss function which includes the exponential loss and the logistic loss our numerical experiment with adaboost indicate that the proposed upper bound computed on the training set behaves very similarly a the true loss estimated on the testing set 
we consider the problem of learning density mixture model for classification traditional learning of mixture for density estimation focus on model that correctly represent the density at all point in the sample space discriminative learning on the other hand aim at representing the density at the decision boundary we introduce a novel discriminative learning method for mixture of generative model unlike traditional discriminative learning method that often resort to computationally demanding gradient search optimization the proposed method is highly efficient a it reduces to generative learning of individual mixture component on weighted data hence it is particularly suited to domain with complex component model such a hidden markov model or bayesian network in general that are usually too complex for effective gradient search we demonstrate the benefit of the proposed method in a comprehensive set of evaluation on time series sequence classification problem 
one of the most important assumption made by many classification algorithm is that the training and test set are drawn from the same distribution i e the so called stationary distribution assumption that the future and the past data set are identical from a probabilistic standpoint in many domain of real world application such a marketing solicitation fraud detection drug testing loan approval sub population survey school enrollment among others this is rarely the case this is because the only labeled sample available for training is biased in different way due to a variety of practical reason and limitation in these circumstance traditional method to evaluate the expected generalization error of classification algorithm such a structural risk minimization ten fold cross validation and leave one out validation usually return poor estimate of which classification algorithm when trained on biased dataset will be the most accurate for future unbiased dataset among a number of competing candidate sometimes the estimated order of the learning algorithm accuracy could be so poor that it is not even better than random guessing therefore a method to determine the most accurate learner is needed for data mining under sample selection bias for many real world application we present such an approach that can determine which learner will perform the best on an unbiased test set given a possibly biased training set in a fraction of the computational cost to use cross validation based approach 
most decision tree algorithm base their splitting decision on a piecewise constant model often these splitting algorithm are extrapolated to tree with non constant model at the leaf node the motivation behind look ahead linear regression tree llrt is that out of all the method proposed to date there ha been no scalable approach to exhaustively evaluate all possible model in the leaf node in order to obtain an optimal split using several optimization llrt is able to generate and evaluate thousand of linear regression model per second this allows for a near exhaustive evaluation of all possible split in a node based on the quality of fit of linear regression model in the resulting branch we decompose the calculation of the residual sum of square in such a way that a large part of it is pre computed the resulting method is highly scalable we observe it to obtain high predictive accuracy for problem with strong mutual dependency between attribute we report on experiment with two simulated and seven real data set 
evolutionary clustering is an emerging research area essential to important application such a clustering dynamic web and blog content and clustering data stream in evolutionary clustering a good clustering result should fit the current data well while simultaneously not deviate too dramatically from the recent history to fulfill this dual purpose a measure of temporal smoothness is integrated in the overall measure of clustering quality in this paper we propose two framework that incorporate temporal smoothness in evolutionary spectral clustering for both framework we start with intuition gained from the well known k mean clustering problem and then propose and solve corresponding cost function for the evolutionary spectral clustering problem our solution to the evolutionary spectral clustering problem provide more stable and consistent clustering result that are le sensitive to short term noise while at the same time are adaptive to long term cluster drift furthermore we demonstrate that our method provide the optimal solution to the relaxed version of the corresponding evolutionary k mean clustering problem performance experiment over a number of real and synthetic data set illustrate our evolutionary spectral clustering method provide more robust clustering result that are not sensitive to noise and can adapt to data drift 
increasingly large collection of structured data necessitate the development of efficient noise tolerant retrieval tool in this work we consider this issue and describe an approach to learn a similarity function that is not only accurate but that also increase the effectiveness of retrieval data structure we present an algorithm that us functional gradient boosting to maximize both retrieval accuracy and the retrieval efficiency of vantage point tree we demonstrate the effectiveness of our approach on two datasets including a moderately sized real world dataset of folk music 
we study a setting that is motivated by the problem of filtering spam message for many user each user receives message according to an individual unknown distribution reflected only in the unlabeled inbox the spam filter for a user is required to perform well with respect to this distribution labeled message from publicly available source can be utilized but they are governed by a distinct distribution not adequately representing most inboxes we devise a method that minimizes a loss function with respect to a user s personal distribution based on the available biased sample a nonparametric hierarchical bayesian model furthermore generalizes across user by learning a common prior which is imposed on new email account empirically we observe that bias corrected learning outperforms naive reliance on the assumption of independent and identically distributed data dirichlet enhanced generalization across user outperforms a single one size fit all filter a well a independent filter for all user 
this paper describes a novel classification method for computer aided detection cad that identifies structure of interest from medical image cad problem are challenging largely due to the following three characteristic typical cad training data set are large and extremely unbalanced between positive and negative class when searching for descriptive feature researcher often deploy a large set of experimental feature which consequently introduces irrelevant and redundant feature finally a cad system ha to satisfy stringent real time requirement this work is distinguished by three key contribution the first is a cascade classification approach which is able to tackle all the above difficulty in a unified framework by employing an asymmetric cascade of sparse classifier each trained to achieve high detection sensitivity and satisfactory false positive rate the second is the incorporation of feature computational cost in a linear program formulation that allows the feature selection process to take into account different evaluation cost of various feature the third is a boosting algorithm derived from column generation optimization to effectively solve the proposed cascade linear program we apply the proposed approach to the problem of detecting lung nodule from helical multi slice ct image our approach demonstrates superior performance in comparison against support vector machine linear discriminant analysis and cascade adaboost especially the resulting detection system is significantly sped up with our approach 
in many regression problem the variable to be predicted depends not only on a sample specific feature vector but also on an unknown latent manifold that must satisfy known constraint an example is house price which depend on the characteristic of the house and on the desirability of the neighborhood which is not directly measurable the proposed method comprises two trainable component the first one is a parametric model that predicts the intrinsic price of the house from it description the second one is a smooth non parametric model of the latent desirability manifold the predicted price of a house is the product of it intrinsic price and desirability the two component are trained simultaneously using a deterministic form of the em algorithm the model wa trained on a large dataset of house from los angeles county it produce better prediction than pure parametric and non parametric model it also produce useful estimate of the desirability surface at each location 
this panel will discus possible exciting and motivating grand challenge problem for data mining focusing on bioinformatics multimedia mining link mining text mining and web mining 
correlation clustering aim at grouping the data set into correlation cluster such that the object in the same cluster exhibit a certain density and are all associated to a common arbitrarily oriented hyperplane of arbitrary dimensionality several algorithm for this task have been proposed recently however all algorithm only compute the partitioning of the data into cluster this is only a first step in the pipeline of advanced data analysis and system modelling the second post clustering step of deriving a quantitative model for each correlation cluster ha not been addressed so far in this paper we describe an original approach to handle this second step we introduce a general method that can extract quantitative information on the linear dependency within a correlation clustering our concept are independent of the clustering model and can thus be applied a a post processing step to any correlation clustering algorithm furthermore we show how these quantitative model can be used to predict the probability distribution that an object is created by these model our broad experimental evaluation demonstrates the beneficial impact of our method on several application of significant practical importance 
recently several learning algorithm relying on model with deep architecture have been proposed though they have demonstrated impressive performance to date they have only been evaluated on relatively simple problem such a digit recognition in a controlled environment for which many machine learning algorithm already report reasonable result here we present a series of experiment which indicate that these model show promise in solving harder learning problem that exhibit many factor of variation these model are compared with well established algorithm such a support vector machine and single hidden layer feed forward neural network 
the paper present a kernel for learning from ordered hypergraphs a formalization that capture relational data a used in inductive logic programming ilp the kernel generalizes previous approach to graph kernel in calculating similarity based on walk in the hypergraph experiment on challenging chemical datasets demonstrate that the kernel outperforms existing ilp method and is competitive with state of the art graph kernel the experiment also demonstrate that the encoding of graph data can affect performance dramatically a fact that can be useful beyond kernel method 
separation of music signal is an interesting but difficult problem it is helpful for many other music research such a audio content analysis in this paper a new music signal separation method is proposed which is based on harmonic structure modeling the main idea of harmonic structure modeling is that the harmonic structure of a music signal is stable so a music signal can be represented by a harmonic structure model accordingly a corresponding separation algorithm is proposed the main idea is to learn a harmonic structure model for each music signal in the mixture and then separate signal by using these model to distinguish harmonic structure of different signal experimental result show that the algorithm can separate signal and obtain not only a very high signalto noise ratio snr but also a rather good subjective audio quality 
we report the main finding of the final evaluation study of the myartspace project myartspace is a combined mobile phone and web based service to support learning between school and museum on arriving for a museum visit the child are loaned mobile phone running the myartspace software they can view multimedia presentation of museum exhibit take photo make voice recording write note and see who else ha viewed the exhibit after each action the content is automatically transmitted over the phone connection to a website which store a personal record of their visit back in the classroom they can review their visit and the medium they have collected share material with other child and create presentation myartspace ha been deployed in three museum for a year long trial during which over school student used the service on organised visit from local school the final user study took place in one museum during november with a group of twenty three student aged and their teacher it covered usability educational and organisational issue through focus group observational study questionnaire survey and face to face telephone and email interview the study showed that myartspace had a positive impact on school museum visit and identified area for improvement in the technical and educational aspect of the service 
a general framework is proposed for gradient boosting in supervised learning problem where the loss function is defined using a kernel over the output space it extends boosting in a principled way to complex output space image text graph etc and can be applied to a general class of base learner working in kernelized output space empirical result are provided on three problem a regression problem an image completion task and a graph prediction problem in these experiment the framework is combined with tree based base learner which have interesting algorithmic property the result show that gradient boosting significantly improves these base learner and provides competitive result with other tree based ensemble method based on randomization 
we study generalization property of ranking algorithm in the setting of the k partite ranking problem in the k partite ranking problem one is given example of instance labeled with one of k ordered rating and the goal is to learn from these example a real valued ranking function that rank instance in accordance with their rating this form of ranking problem arises naturally in a variety of application and formally constitutes a generalization of the bipartite ranking problem that ha recently been studied we start by defining notion of ranking error suitable for measuring the quality of a ranking function in the k partite setting we then give distribution free probabilistic bound on the expected error of a ranking function learned by a k partite ranking algorithm 
we introduce a hierarchical bayesian model for the discovery of putative regulator from gene expression data only the hierarchy incorporates the knowledge that there are just a few regulator that by themselves only regulate a handful of gene this is implemented through a so called spike and slab prior a mixture of gaussians with different width with mixing weight from a hierarchical bernoulli model for efficient inference we implemented expectation propagation running the model on a malaria parasite data set we found four gene with significant homology to transcription factor in an amoebe one rna regulator and three gene of unknown function out of the top ten gene considered 
in this paper we propose a general framework to study the generalization property of binary classifier trained with data which may be dependent but are deterministically generated upon a sample of independent example it provides generalization bound for binary classification and some case of ranking problem and clarifies the relationship between these learning task the study of the generalization property of ranking problem is a challenging task since the pair of example violate the central i i d assumption of binary classification using task specific study this issue ha recently been the focus of a large amount of work showed that svm like algorithm optimizing the auc have good generalization guarantee and showed that maximizing the margin of the pair defined by the quantity g x g x lead to the minimization of the generalization error while these result suggest some similarity between the classification of the pair of example and the classification of independent data no common framework ha been established a a major drawback it is not possible to directly deduce result for ranking from those obtained in classification in this paper we present a new framework to study the generalization property of classifier over data which can exhibit a suitable dependency structure among others the problem of binary classification bipartite ranking and the ranking risk defined in are special case of our study it show that it is possible to infer generalization bound for clas 
advance in computer networking and database technology have enabled the collection and storage of vast quantity of data data mining can extract valuable knowledge from this data and organization have realized that they can often obtain better result by pooling their data together however the collected data may contain sensitive or private information about the organization or their customer and privacy concern are exacerbated if data is shared between multiple organization distributed data mining is concerned with the computation of model from data that is distributed among multiple participant privacy preserving distributed data mining seek to allow for the cooperative computation of such model without the cooperating party revealing any of their individual data item our paper make two contribution in privacy preserving data mining first we introduce the concept of arbitrarily partitioned data which is a generalization of both horizontally and vertically partitioned data second we provide an efficient privacy preserving protocol for k mean clustering in the setting of arbitrarily partitioned data 
a plausible representation of relational information among entity in dynamic system such a a living cell or a social community is a stochastic network which is topologically rewiring and semantically evolving over time while there is a rich literature on modeling static or temporally invariant network much le ha been done toward modeling the dynamic process underlying rewiring network and on recovering such network when they are not observable we present a class of hidden temporal exponential random graph model htergms to study the yet unexplored topic of modeling and recovering temporally rewiring network from time series of node attribute such a activity of social actor or expression level of gene we show that one can reliably infer the latent time specific topology of the evolving network from the observation we report empirical result on both synthetic data and a drosophila lifecycle gene expression data set in comparison with a static counterpart of htergm 
we have developed a novel probabilistic model that estimate neural source activity measured by meg and eeg data while suppressing the effect of interference and noise source the model estimate contribution to sensor data from evoked source interference source and sensor noise using bayesian method and by exploiting knowledge about their timing and spatial covariance property full posterior distribution are computed rather than just the map estimate in simulation the algorithm can accurately localize and estimate the time course of several simultaneously active dipole with rotating or fixed orientation at noise level typical for averaged meg data the algorithm even performs reasonably at noise level typical of an average of just a few trial the algorithm is superior to beamforming technique which we show to be an approximation to our graphical model in estimation of temporally correlated source success of this algorithm using meg data for localizing bilateral auditory cortex low snr somatosensory activation and for localizing an epileptic spike source are also demonstrated 
starting with the work of jaakkola and haussler a variety of approach have been proposed for coupling domain specific generative model with statistical learning method the link is established by a kernel function which provides a similarity measure based inherently on the underlying model in computational biology the full promise of this framework ha rarely ever been exploited a most kernel are derived from very generic model such a sequence profile or hidden markov model here we introduce the mtreemix kernel which is based on a generative model tailored to the underlying biological mechanism specifically the kernel quantifies the similarity of evolutionary escape from antiviral drug pressure between two viral sequence sample we compare this novel kernel to a standard evolution agnostic amino acid encoding in the prediction of hiv drug resistance from genotype using support vector regression the result show significant improvement in predictive performance across anti hiv drug thus in our study the generative discriminative paradigm is key to bridging the gap between population genetic modeling and clinical decision making 
non linear dimensionality reduction of noisy data is a challenging problem encountered in a variety of data analysis application recent result in the literature show that spectral decomposition a used for example by the laplacian eigenmaps algorithm provides a powerful tool for non linear dimensionality reduction and manifold learning in this paper we discus a significant shortcoming of these approach which we refer to a the repeated eigendirections problem we propose a novel approach that combine successive dimensional spectral embeddings with a data advection scheme that allows u to address this problem the proposed method doe not depend on a non linear optimization scheme hence it is not prone to local minimum experiment with artificial and real data illustrate the advantage of the proposed method over existing approach we also demonstrate that the approach is capable of correctly learning manifold corrupted by significant amount of noise 
multinomial distribution are often used to model text document however they do not capture well the phenomenon that word in a document tend to appear in burst if a word appears once it is more likely to appear again in this paper we propose the dirichlet compound multinomial model dcm a an alternative to the multinomial the dcm model ha one additional degree of freedom which allows it to capture burstiness we show experimentally that the dcm is substantially better than the multinomial at modeling text data measured by perplexity we also show using three standard document collection that the dcm lead to better classification than the multinomial model dcm performance is comparable to that obtained with multiple heuristic change to the multinomial model 
let r be a set of object an object o r is an outlier if there exist le than k object in r whose distance to o are at most r the value of k r and the distance metric are provided by a user at the run time the objective is to return all outlier with the smallest i o cost this paper considers a generic version of the problem where no information is available for outlier computation except for object mutual distance we prove an upper bound for the memory consumption which permit the discovery of all outlier by scanning the dataset time the upper bound turn out to be extremely low in practice e g le than of r since the actual memory capacity of a realistic dbms is typically larger we develop a novel algorithm which integrates our theoretical finding with carefully designed heuristic that leverage the additional memory to improve i o efficiency our technique report all outlier by scanning the dataset at most twice in some case even once and significantly outperforms the existing solution by a factor up to an order of magnitude 
we present a novel paradigm for statistical machine translation smt based on a joint modeling of word alignment and the topical aspect underlying bilingual document pair via a hidden markov bilingual topic admixture hm bitam in this paradigm parallel sentence pair from a parallel d ocument pair are coupled via a certain semantic flow to ensure coherence of topi cal context in the alignment of mapping word between language likelihood based training of topic dependent translational lexicon a well a in the i nference of topic representation in each language the learned hm bitam can not only display topic pattern like method such a lda but now for bilingual corpus it also offer a principled way of inferring optimal translati on using document context our method integrates the conventional model of hmm a key component for most of the state of the art smt system with the recently proposed bitam model we report an extensive empirical analysis in many way complementary to the description oriented of our method in thre e aspect bilingual topic representation word alignment and translation 
many application in text processing require significant human effort for either labeling large document collection when learning statistical model or extrapolating rule from them when using knowledge engineering in this work we describe away to reduce this effort while retaining the method accuracy by constructing a hybrid classifier that utilizes human reasoning over automatically discovered text pattern to complement machine learning using a standard sentiment classification dataset and real customer feedback data we demonstrate that the resulting technique result in significant reduction of the human effort required to obtain a given classification accuracy moreover the hybrid text classifier also result in a significant boost in accuracy over machine learning based classifier when a comparable amount of labeled data is used 
there is little consensus about the computational function of top down synaptic connection in the visual system here we explore the hypothesis that top down connection like bottom up connection reflect partwhole relationship we analyze a recurrent network with bidirectional synaptic interaction between a layer of neuron representing part and a layer of neuron representing whole within each layer there is lateral inhibition when the network detects a whole it can rigorously enforce part whole relationship by ignoring part that do not belong the network can complete the whole by filling in missing part the network can refuse to recognize a whole if the activated part do not conform to a stored part whole relationship parameter regime in which these behavior happen are identified using the theory of permitted and forbidden set the network behavior are illustrated by recreating rumelhart and mcclelland s interactive activation model 
if appropriately used prior knowledge can significantly improve the predictive accuracy of learning algorithm or reduce the amount of training data needed in this paper we introduce a simple method to incorporate prior knowledge in support vector machine by modifying the hypothesis space rather than the optimization problem the optimization problem is amenable to solution by the constrained concave convex procedure which find a local optimum the paper discus different kind of prior knowledge and demonstrates the applicability of the approach in some characteristic experiment 
the increasing pervasiveness of the internet ha dramatically changed the way that consumer shop for good consumer generated product review have become a valuable source of information for customer who read the review and decide whether to buy the product based on the information provided in this paper we use technique that decompose the review into segment that evaluate the individual characteristic of a product e g image quality and battery life for a digital camera then a a major contribution of this paper we adapt method from the econometrics literature specifically the hedonic regression concept to estimate a the weight that customer place on each individual product feature b the implicit evaluation score that customer assign to each feature and c how these evaluation affect the revenue for a given product towards this goal we develop a novel hybrid technique combining text mining and econometrics that model consumer product review a element in a tensor product of feature and evaluation space we then impute the quantitative impact of consumer review on product demand a a linear functional from this tensor product space we demonstrate how to use a low dimension approximation of this functional to significantly reduce the number of model parameter while still providing good experimental result we evaluate our technique using a data set from amazon com consisting of sale data and the related consumer review posted over a month period for product our experimental evaluation show that we can extract actionable business intelligence from the data and better understand the customer preference and action we also show that the textual portion of the review can improve product sale prediction compared to a baseline technique that simply relies on numeric data 
one of the intuition underlying many graph based method for clustering and semi supervised learning is that class or cluster boundary pas through area of low probability density in this paper we provide some formal analysis of that notion for a probability distribution we introduce a notion of weighted boundary volume which measure the length of the class cluster boundary weighted by the density of the underlying probability distribution we show that size of the cut of certain commonly used data adjacency graph converge to this continuous weighted volume of the boundary 
most current multi task learning framework ignore the robustness issue which mean that the presence of outlier task may greatly reduce overall system performance we introduce a robust framework for bayesian multitask learning t process tp which are a generalization of gaussian process gp for multi task learning tp allows the system to effectively distinguish good task from noisy or outlier task experiment show that tp not only improves overall system performance but can also serve a an indicator for the informativeness of different task 
we develop a novel multi class classification method based on output code for the problem of classifying a sequence of amino acid into one of many known protein structural class called fold our method learns relative weight between one v all classifier and encodes information about the protein structural hierarchy for multi class prediction our code weighting approach significantly improves on the standard one v all method for the fold recognition problem in order to compare against widely used method in protein sequence analysis we also test nearest neighbor approach based on the psi blast algorithm our code weight learning algorithm strongly outperforms these psi blast method on every structure recognition problem we consider 
we consider the well studied problem of learning decision list using few example when many irrelevant feature are present we show that smooth boosting algorithm such a madaboost can efciently learn decision list of length k over n boolean variable using poly k log n many example provided that the marginal distribution over the relevant variable is not too concentrated in an l norm sense using a recent result of h astad we extend the analysis to obtain a similar though quantitatively weaker result for learning arbitrary linear threshold function with k nonzero coefcients experimental result indicate that the use of a smooth boosting algorithm which play a crucial role in our analysis ha an impact on the actual performance of the algorithm 
computational model of visual cortex and in particular those based on sparse coding have enjoyed much recent attention despite this currency the question of how sparse or how over complete a sparse representation should be ha gone without principled answer here we use bayesian model selection method to address these question for a sparse coding model based on a student t prior having validated our method on toy data we find that natural ima ge are indeed best modelled by extremely sparse distribution although for the student t prior the associated optimal basis size is only modestly over complete 
p we use multi electrode recording from cat primary visual cortex and investigate whether a simple linear classifier ca n extract information about the presented stimulus we find that information is extractable and that it even la t for several hundred millisecond after the stimulus ha b een removed in a fast sequence of stimulus presentation information about both new and old stimulus is present simultaneously and nonlinear relation between these stimulus can be extracted these result suggest nonlinear property of cortical representat ion the implication of these property for the nonlinear brain theory are discussed p 
the detection of face in image is fundamentally a rare event detection problem cascade classifier provide an efficient computational solution by leveraging the asymmetry in the distribution of face v non face training a cascade classifier in turn requires a solution for the following subproblems design a classifier for each node in the cascade with very high detection rate but only moderate false positive rate while there are a few strategy in the literature for indirectly addressing this asymmetric node learning goal none of them are based on a satisfactory theoretical framework we present a mathematical characterization of the node learning problem and describe an effective closed form approximation to the optimal solution which we call the linear asymmetric classifier lac we first use adaboost or asymboost to select feature and use lac to learn a linear discriminant function to achieve the node learning goal experimental result on face detection show that lac can improve the detection performance in comparison to standard method we also show that fisher discriminant analysis on the feature selected by adaboost yield better performance than adaboost itself 
locally adaptive classifier are usually superior to the use of a single global classifier however there are two major problem in designing locally adaptive classifier first how to place the local classifier and second how to combine them together in this paper instead of placing the classifier based on the data distribution only we propose a responsibility mixture model that us the uncertainty associated with the classification at each training sample using this model the local classifier are placed near the decision boundary where they are most effective a set of local classifier are then learned to form a global classifier by maximizing an estimate of the probability that the sample will be correctly classified with a nearest neighbor classifier experimental result on both artificial and real world data set demonstrate it superiority over traditional algorithm 
technique such a probabilistic topic model and latent semantic indexing have been shown to be broadly useful at automatically extracting the topical or semantic content of document or more generally for dimension reduction of sparse count data these type of model and algorithm can be viewed a generating an abstraction from the word in a document to a lower dimensional latent variable representation that capture what the document is generally about beyond the specific word it contains in this paper we propose a new probabi listic model that temper this approach by representing each document a a combination of a a background distribution over common word b a mixture distribution over general topic and c a distribution over word that are treat ed a being specific to that document we illustrate how this model can be used for information retrieval by matching document both at a general topic level and at a specific word level providing an advantage over technique that only match document at a general level such a topic model or latent sematic indexing or t hat only match document at the specific word level such a tf idf 
this paper introduces a new problem for which machine learning tool may make an impact the problem considered is termed compressive sensing in which a real signal of dimension n is measured accurately based on k real measurement this is achieved under the assumption that the underlying signal ha a sparse representation in some basis e g wavelet in this paper we demonstrate how technique developed in machine learning specifically sparse bayesian regression and active learning may be leveraged to this new problem we also point out future research direction in compressive sensing of interest to the machine learning community 
content based image suggestion cbis target the recommendation of product based on user preference on the visual content of image in this paper we motivate both feature selection and model order identification a two key issue for a successful cbis we propose a generative model in which the visual feature and user are clustered into separate class we identify the number of both user and image class with the simultaneous selection of relevant visual feature using the message length approach the goal is to ensure an accurate prediction of rating for multidimensional non gaussian and continuous image descriptor experiment on a collected data have demonstrated the merit of our approach 
the object in many real world domain can be organized into hierarchy where each internal node pick out a category of object given a collection of feature and relation defined over a set of object an annotated hierarchy includes a specification of the category that are most useful for desc ribing each individual feature and relation we define a generative model for annota ted hierarchy and the feature and relation that they describe and develop a markov chain monte carlo scheme for learning annotated hierarchy we show that our model discovers interpretable structure in several real world data set s 
privacy preserving data processing ha become an important topic recently because of advance in hardware technology which have lead to widespread proliferation of demographic and sensitive data a rudimentary way to preserve privacy is to simply hide the information in some of the sensitive field picked by a user however such a method is far from satisfactory in it ability to prevent adversarial data mining real data record are not randomly distributed a a result some field in the record may be correlated with one another if the correlation is sufficiently high it may be possible for an adversary to predict some of the sensitive field using other field in this paper we study the problem of privacy preservation against adversarial data mining which is to hide a minimal set of entry so that the privacy of the sensitive field are satisfactorily preserved in other word even by data mining an adversary still cannot accurately recover the hidden data entry we model the problem concisely and develop an efficient heuristic algorithm which can find good solution in practice an extensive performance study is conducted on both synthetic and real data set to examine the effectiveness of our approach 
we consider the problem of network anomaly detection in large distributed system in this setting principal component analysis pca ha been proposed a a method for discovering anomaly by continuously tracking the projection of the data onto a residual subspace this method wa shown to work well empirically in highly aggregated network that is those with a limited number of large node and at coarse time scale this approach however ha scalability limitation to overcome these limitation we develop a pca based anomaly detector in which adaptive local data lters send to a coordinator just enough data to enable accurate global detection our method is based on a stochastic matrix perturbation analysis that characterizes the tradeoff between the accuracy of anomaly detection and the amount of data communicated over the network 
data may often contain multiple plausible clustering in order to discover a clustering which is useful to the user constrained clustering technique have been proposed to guide the search typically these technique assume background knowledge in the form of explicit information about the desired clustering in contrast we consider the setting in which the background knowledge is instead about an undesired clustering such knowledge may be obtained from an existing classification or precedent algorithm the problem is then to find a novel orthogonal clustering in the data we present a general algorithmic framework which make use of cluster ensemble method to solve this problem one key advantage of this approach is that it take a base clustering method which is used a a black box allowing the practitioner to select the most appropriate clustering method for the domain we present experimental result on synthetic and text data which establish the competitiveness of this framework 
we present a novel spectral clustering method that enables user to incorporate prior knowledge of the size of cluster into the clustering process the cost function which is named size regularized cut srcut is defined a the sum of the inter cluster similarity and a regularization term measuring the relative size of two cluster finding a partition of the data set to minimize srcut is proved to be np complete an approximation algorithm is proposed to solve a relaxed version of the optimization problem a an eigenvalue problem evaluation over different data set demonstrate that the method is not sensitive to outlier and performs better than normalized cut 
in many real world application euclidean distance in the original space is not good due to the curse of dimensionality in this paper we propose a new method called discriminant neighborhood embedding dne to learn an appropriate metric space for classification given finite training sample we define a discriminant adjacent matrix in favor of classification task i e neighboring sample in the same class are squeezed but those in different class are separated a far a possible the optimal dimensionality of the metric space can be estimated by spectral analysis in the proposed method which is of great significance for high dimensional pattern experiment with various datasets demonstrate the effectiveness of our method 
recent experimental study have focused on the specialization of different neural structure for different type of instrumental behavior recent theoretical work ha provided normative account for why there should be more than one control system and how the output of different controller can be integrated two particlar controller have been identified one associated with a forward model and the prefrontal cortex and a second associated with computationally simpler habitual actor critic method and part of the striatum we argue here for the normative appropriateness of an additional but so far marginalized control system associated with episodic memory and involving the hippocampus and medial temporal cortex we analyze in depth a class of simple environment to show that episodic control should be useful in a range of case characterized by complexity and inferential noise and most particularly at the very early stage of learning long before habitization ha set in we interpret data on the transfer of control from the hippocampus to the striatum in the light of this hypothesis 
we argue that when object are characterized by many attribute clustering them on the basis of a relatively small random subset of these attribute can capture information on the unobserved attri butes a well moreover we show that under mild technical condition clustering the object on the basis of such a random subset performs almost a well a clustering with the full attribute set we prove a finite samp le generalization theorem for this novel learning scheme that extends analogous result from the supervised learning setting the scheme is demonstrated for collaborative filtering of user with movie rating a at tribute 
we propose a new ranking method which combine recommender system with information search tool for better search and browsing our method us a collaborative ltering algorithm to generate personal item authority for each user and combine them with item proximity for better ranking to demonstrate our approach we build a prototype movie search and browsing engine called mad movie actor and director degree of separation we conduct oine and online test of our ranking algorithm for oine testing we use yahoo search query that resulted in a click on a yahoo movie or internet movie database imdb movie url our online test involved yahoo employee providing subjective assessment of result quality in both test our ranking method show signicantly better recall and quality than imdb search and yahoo movie current search 
linear gaussian state space model are widely used and a bayesian treatment of parameter is therefore of considerable interest the ap proximate variational bayesian method applied to these model is an attractive approach used successfully in application ranging from acoustic to bioinformatics the most challenging aspect of implementing the method is in performing inference on the hidden state sequence of the model we show how to convert the inference problem so that standard kalman filtering smoothing recursion from the literature may be applied this is in contrast to previously published approa ches based on belief propagation our framework both simplifies and unifies the in ference problem so that future application may be more easily developed we demonstrate the elegance of the approach on bayesian temporal ica with an application to finding independent dynamical process underlying noisy eeg signal 
we present a hierarchical bayesian model for set of related but different class of time series data our model performs alignment simultaneously across all class while detecting and characterizing class specific difference during inference the model produce for each class a distribution over a canonical representation of the class these class specific canonical representation are automatically aligned to one another preserving common sub structure and highlighting difference we apply our model to compare and contrast solenoid valve current data and also liquid chromatography ultraviolet diode array data from a study of the plant arabidopsis thaliana aligning time series from different class many practical problem over a wide range of domain require synthesizing information from several noisy example of one or more category in order to build a model which capture common structure and also learns the pattern of variability between category in time series analysis these modeling goal manifest themselves in the task of alignment and difference detection these task have diverse applicability spanning speech music processing equipment industrial plant diagnosis monitoring and analysis of biological time series such a microarray liquid gas chromatography based laboratory data including mass spectrometry and ultraviolet diode array although alignment and difference detection have been extensively studied a separate problem in the signal processing and statistical pattern recognition community to our knowledge no existing model performs both task in a unified way single class alignment algorithm attempt to align a set of time series all together assuming that variability across different time series is attributable purely to noise in many real world situation however we have time series from multiple class category and our prior belief is that there is both substantial shared structure between the class distribution and simultaneously systematic although often rare difference between them while in some circumstance if difference are small and infrequent single class alignment can be applied to multi class data it is much more desirable to have a model which performs true multi class alignment in a principled way allowing for more refined and accurate modeling of the data in this paper we introduce a novel hierarchical bayesian model which simultaneously solves the multi class alignment and difference detection task in a unified manner a illustrated in figure the single class alignment shown in this figure coerces the feature in region a for class to be inappropriately collapsed in time and the overall width of the main broad peak in class to be inappropriately narrowed in contrast our multi class model handle these feature correctly furthermore because our algorithm doe inference for a fully probabilistic model we are able to obtain quantitative measure of the posterior uncertainty in our result which unlike the point estimate produced by most current approach allow u to ass our relative confidence in difference learned by the model our basic setup for multi class alignment assumes the class label are known 
in this paper we formalize multi instance multi label learning where each training example is associated with not only multiple instance but also multiple class label such a problem can occur in many real world task e g an image usually contains multiple patch each of which can be described by a feature vector and the image can belong to multiple category since it semantics can be recognized in different way we analyze the relationship between multi instance multi label learning and the learning framework of traditional supervised learning multiinstance learning and multi label learning then we propose the mimlboost and mimlsvm algorithm which achieve good performance in an application to scene classification 
in application such a paleontology and medical genetics the data ha an underlying unknown order the age of the fossil site the location of marker in the genome the order might be total or partial for example two site in different part of the globe might be ecologically incomparable or the ordering of certain marker might be different in different subgroup of the data we consider the following problem given a table over a set of variable find a partial order for the row minimizing a score function and being a specific a possible the score function can be e g the number of change from to in a column for paleontology or the likelihood of the marker sequence for genomic data our solution for this task first construct small totally ordered fragment of the partial order then find good orientation for the fragment and finally us a simple and efficient heuristic method for finding a partial order that corresponds well with the collection of fragment we describe the method discus it property and give empirical result on paleontological data demonstrating the usefulness of the method in the application the use of the method highlighted some previously unknown property of the data and pointed out probable error in the data 
we propose a new kernel based data transformation technique it is founded on the principle of maximum entropy maxent preservation hence named kernel maxent the key measure is renyi s entropy estimated via parzen windowing we show that kernel maxent is based on eigenvectors and is in that sense similar to kernel pca but may produce strikingly different transformed data set an enhanced spectral clustering algorithm is proposed by replacing kernel pca by kernel maxent a an intermediate step this ha a major impact on performance recently it ha been shown that there is a close connection between the kernel method and information theoretic learning we propose a new kernel based data transformation technique based on the idea of maximum entropy preservation the new method named kernel maxent is based on renyi s quadratic entropy estimated via parzen windowing the data transformation is obtained using eigenvectors of the data afnity matrix these eigenvectors are in general not the same a those used in kernel pca we show that kernel maxent may produce strikingly different transformed data set than kernel pca we propose an enhanced spectral clustering algorithm by replacing kernel pca by kernel maxent a an intermediate step this seemingly minor adjustment ha a huge impact on the performance of the algorithm 
in this paper we formulate a novel and or graph representation capable of describing the different configuration of deformable articulated object such a horse the representation make use of the summarization principle so that lower level node in the graph only pas on summary statistic to the higher level node the probability distribution are invariant to position orientation and scale we develop a novel inference algorithm that combined a bottom up process for proposing configuration for horse together with a top down process for refining and validating these proposal the strategy of surround suppression is applied to ensure that the inference time is polynomial in the size of input data the algorithm wa applied to the task of detecting segmenting and parsing horse we demonstrate that the algorithm is fast and comparable with the state of the art approach 
given a covariance matrix we consider the problem of maximizing the variance explained by a particular linear combination of the input variable while constraining the number of nonzero coefficient in this combination this problem arises in the decomposition of a covariance matrix into sparse factor or sparse principal component analysis pca and ha wide application ranging from biology to finance we use a modification of the classical variational representation of the largest eigenvalue of a symmetric matrix where cardinality is constrained and derive a semidefinite programming based relaxation for our problem we also discus nesterov s smooth minimization technique applied to the semidefinite program arising in the semidefinite relaxation of the sparse pca problem the method ha complexity o n sqrt log n epsilon where n is the size of the underlying covariance matrix and epsilon is the desired absolute accuracy on the optimal value of the problem 
we present a new machine learning framework called self taught learning for using unlabeled data in supervised classification task we do not assume that the unlabeled data follows the same class label or generative distribution a the labeled data thus we would like to use a large number of unlabeled image or audio sample or text document randomly downloaded from the internet to improve performance on a given image or audio or text classification task such unlabeled data is significantly easier to obtain than in typical semi supervised or transfer learning setting making self taught learning widely applicable to many practical learning problem we describe an approach to self taught learning that us sparse coding to construct higher level feature using the unlabeled data these feature form a succinct input representation and significantly improve classification performance when using an svm for classification we further show how a fisher kernel can be learned for this representation 
mining frequent pattern is a general and important issue in data mining complex and unstructured or semi structured datasets have appeared in major data mining application including text mining web mining and bioinformatics mining pattern from these datasets is the focus of many of the current data mining approach we focus on labeled ordered tree typical datasets of semi structured data in data mining and propose a new probabilistic model and it efficient learning scheme for mining labeled ordered tree the proposed approach significantly improves the time and space complexity of an existing probabilistic modeling for labeled ordered tree while maintaining it expressive power we evaluated the performance of the proposed model comparing it with that of the existing model using synthetic a well a real datasets from the field of glycobiology experimental result showed that the proposed model drastically reduced the computation time of the competing model keeping the predictive power and avoiding overfitting to the training data finally we assessed our result using the proposed model on real data from a variety of biological viewpoint verifying known fact in glycobiology 
we propose a new algorithm for dimensionality reduction and unsupervised text classification we use mixture model a underlying process of generating corpus and utilize a novel l norm based approach introduced by kleinberg and sandler we show that our algorithm performs extremely well on large datasets with peak accuracy approaching that of supervised learning based on support vector machine svms with large training set the method is based on the same idea that underlies latent semantic indexing lsi we find a good low dimensional subspace of a feature space and project all document into it however our projection minimizes different error and unlike lsi we build a basis that in many case corresponds to the actual topic we present result of testing of our algorithm on the abstract of arxiv an electronic repository of scientific paper and the newsgroup dataset a small snapshot of specific newsgroups 
learning the structure of graphical model is an important task but one of considerable difficulty when latent variable are involved because conditional independence using hidden variable cannot be directly observed one ha to rely on alternative method to identify the d separation that define the graphical structure this paper describes new distribution free technique for identifying d separation in continuous latent variable model when non linear dependency are allowed among hidden variable 
efficient coding model predict that the optimal code for natural image is a population of oriented gabor receptive field these result match response property of neuron in primary visual cortex but not those in the retina doe the retina use an optimal code and if so what is it optimized for previous theory of retinal coding have assumed that the goal is to encode the maximal amount of information about the sensory signal however the image sampled by retinal photoreceptors is degraded both by the optic of the eye and by the photoreceptor noise therefore de blurring and de noising of the retinal signal should be important aspect of retinal coding furthermore the ideal retinal code should be robust to neural noise and make optimal use of all available neuron here we present a theoretical framework to derive code that simultaneously satisfy all of these desideratum when optimized for natural image the model yield filter that show strong similarity to retinal ganglion cell rgc receptive field importantly the characteristic of receptive field vary with retinal eccentricity where the optical blur and the number of rgcs are significantly different the proposed model provides a unified account of retinal coding and more generally it may be viewed a an extension of the wiener filter with an arbitrary number of noisy unit 
we propose a new model for the probabilistic estimation of continuous state variable from a sequence of observation such a tracking the position of an object in video this mapping is modeled a a product of dynamic expert feature relating the state at adjacent time step and observation expert feature relating the state to the image sequence individual feature are flexible in that they can switch on or off at each time step depending on their inferred relevance or on additional side information and discriminative in that they need not model the full generative likelihood of the data when trained conditionally this permit the inclusion of a broad range of rich feature for example feature relying on observation from multiple time step and allows the relevance of feature to be learned from labeled sequence 
in this paper we show that the hinge loss can be interpreted a the neg log likelihood of a semi parametric model of posterior probability from this point of view svms represent the parametric component of a semi parametric model fitted by a maximum a posteriori estim ation procedure this connection enables to derive a mapping from svm score to estimated posterior probability unlike previous pro posals the suggested mapping is interval valued providing a set of posterior probability compatible with each svm score this framework offer a new way to adapt the svm optimization problem to unbalanced classification when decision result in unequal asymmetric loss experiment show improvement over state of the art procedure in this paper we show that support vector machine svms are the solution of a relaxed maximum a posteriori map estimation problem this relaxed problem result from fitting a semi parametric model of posterior probability this model is decomposed into two component the parametric component which is a function of the svm score and the non parametric component which we call a nuisance function given a proper binding of the nuisance function adapted to the considered problem this decomposition enables to concentrate on selected range of the probability spectrum the estimation process can thus allocate model capacity to the neighborhood of decision boundary 
we consider apprenticeship learning learning from expert demonstration in the setting of large complex domain past work in apprenticeship learning requires that the expert demonstrate complete trajectorie s through the domain however in many problem even an expert ha difficulty contr olling the system which make this approach infeasible for example consider the task of teaching a quadruped robot to navigate over extreme terrain demonstrating an optimal policy i e an optimal set of foot location over the entir e terrain is a highly non trivial task even for an expert in this paper we propos e a method for hierarchical apprenticeship learning which allows the algorithm to accept isolated advice at different hierarchical level of the control task this type of advice is often feasible for expert to give even if the expert is unab le to demonstrate complete trajectory this allows u to extend the apprentice ship learning paradigm to much larger more challenging domain in particular in this paper we apply the hierarchical apprenticeship learning algorithm to the task of quadruped locomotion over extreme terrain and achieve to the best of our knowledge result superior to any previously published work 
nonlinear ica may not result in nonlinear blind source separation since solution to nonlinear ica are highly non unique in practice the nonlinearity in the data generation procedure is usually not strong thus it is reasonable to select the solution with the mixing procedure close to linear in this paper we propose to solve nonlinear ica with the minimal nonlinear distortion principle this is achieved by incorporating a regularization term to minimize the mean square error between the mixing mapping and the best fitting linear one a an application the proposed method help to identify linear non gaussian and acyclic causal model when mild nonlinearity exists in the data generation procedure using this method to separate daily return of a set of stock we successfully identify their linear causal relation the resulting causal relation give some interesting insight into the stock market 
we introduce quadratically gated mixture of expert qgme a statistical model for multi class nonlinear classification the qgme is formulated in the setting of incomplete data where the data value are partially observed we show that the missing value entail joint estimation of the data manifold and the classifier which allows adaptive imputation during classifier learning the expectation maximization em algorithm is derived for joint likelihood maximization with adaptive imputation performed analytically in the e step the performance of qgme is evaluated on three benchmark data set and the result show that the qgme yield significant improvement over competing method 
it is becoming increasingly evident that organism acting in uncertain dynamical environment often employ exact or approximate bayesian statistical calculation in order to continuously estimate the environmental state integrate information from multiple sensory modality form prediction and choose action what is le clear is how these putative computation are implemented by cortical neural network an additional level of complexity is introduced because these network observe the world through spike train received from primary sensory afferent rather than directly a recent line of research ha describe d mechanism by which such computation can be implemented using a network of neuron whose activity directly represents a probability distribution across the possible world state much of this work however us various approximation which severely restrict the domain of applicability of these implementation s here we make use of rigorous mathematical result from the theory of continuous time point process filtering and show how optimal real time state estimation a nd prediction may be implemented in a general setting using linear neural network we demonstrate the applicability of the approach with several example and relate the required network property to the statistical nature of the environ ment thereby quantifying the compatibility of a given network with it environment 
in this paper we consider sampling based fitted value iteration for discounted large possibly infinite state space finite action markovian decision problem where only a generative model of the transition probability and reward is available at each step the image of the current estimate of the optimal value function under a monte carlo approximation to the bellman operator is projected onto some function space pac style bound on the weighted lp norm approximation error are obtained a a function of the covering number and the approximation power of the function space the iteration number and the sample size 
we study the following question is the two dimensional structure of image a very strong prior or is it something that can be learned with a few example of natural image if someone gave u a learning task involving image for which the two dimensional topology of pixel wa not known could we discover it automatically and exploit it for example suppose that the pixel had been permuted in a x ed but unknown way could we recover the relative two dimensional location of pixel on image the surprising result presented here is that not only the answer is yes but that about a few a a thousand image are enough to approximately recover the relative location of about a thousand pixel this is achieved using a manifold learning algorithm applied to pixel associated with a measure of distributional similarity between pixel intensity we compare different topologyextraction approach and show how having the two dimensional topology can be exploited 
motivated by the problem of customer wallet estimation we propose a new setting for multi view regression where we learn a completely unobserved target in our case customer wallet by modeling it a a central link in a directed graphical model connecting multiple set of observed variable the resulting conditional independence allows u to reduce the discriminative maximum likelihood estimation problem to a convex optimization problem for parametric form corresponding to exponential linear model we show that under certain modeling assumption in particular when we have two conditionally independent view and the noise is gaussian we can reduce this problem to a single least square regression thus for this speciflc but widely applicable setting the unsupervised multi view problem can be solved via a simple supervised learning approach this reduction also allows u to test the statistical independence assumption underlying the graphical model and perform variable selection we demonstrate our approach on our motivating problem of customer wallet estimation and on simulation data 
the general approach for automatically driving data collection using information from previously acquired data is called active learning traditional active learning address the problem of choosing the unlabeled example for which the class label are queried with the goal of learning a classifier in contrast we address the problem of active feature sampling for detecting useless feature we propose a strategy to actively sample the value of new feature on class labeled example with the objective of feature relevance assessment we derive an active feature sampling algorithm from an information theoretic and statistical formulation of the problem we present experimental result on synthetic uci and real world datasets to demonstrate that our active sampling algorithm can provide accurate estimate of feature relevance with lower data acquisition cost than random sampling and other previously proposed sampling algorithm 
principal component analysis pca minimizes the sum of squared error l norm and is sensitive to the presence of outlier we propose a rotational invariant l norm pca r pca r pca is similar to pca in that it ha a unique global solution the solution are principal eigenvectors of a robust covariance matrix re weighted to soften the effect of outlier the solution is rotational invariant these property are not shared by the l norm pca a new subspace iteration algorithm is given to compute r pca efficiently experiment on several real life datasets show r pca can effectively handle outlier we extend r norm to k mean clustering and show that l norm k mean lead to poor result while r k mean outperforms standard k mean 
we describe an analog vlsi neural network for face recognition based on subspace method the system us a dimensionality reduction network whose coecients can be either programmed or learned on chip to perform pca or programmed to perform lda a second network with userprogrammed coecients performs classification with manhattan distance the system us on chip compensation technique to reduce the eects of device mismatch using the orl database with x pixel image our circuit achieves up to classification performance of an equivalent software implementation 
abstract in this paper we propose a new receiver for digital communication we focus on the application of gaussian process gps to the multiuser detection mud in code division multiple access cdma system to solve the near far problem hence we aim to reduce the interference from other user sharing the same frequency band while usual ap proaches minimize the mean square error mmse to linearly retrieve the user of interest we exploit the same criterion but in the design of a nonlinear mud since the optimal solution is known to be nonlinear the performance of this novel method clearly improves that of the mmse de tectors furthermore the gp based mud achieves excellent interference suppression even for short training sequence we also include some ex periments to illustrate that other nonlinear detector such a those based on support vector machine svms exhibit a worse performance 
we present a new statistical approach to rule learning doing so we address two of the problem inherent in traditional rule learning the computational hardness of finding rule set with low training error and the need for capacity control to avoid overfitting the chosen representation involves weight attached to rule instead of optimizing the error rate directly we optimize for rule set that have large margin and low variance this can be formulated a a convex optimization problem allowing for ecient computation given the representation and the optimization procedure we effectively yield weighted clause in a cnflike representation to avoid overfitting we propose a model selection strategy that utilizes a novel concentration inequality empirical test show that the system is competitive with existing rule learning algorithm and that it flexible learning bias can be adjusted to improve predictive accuracy considerably 
we introduce a game theoretic model for network formation inspired by earlier stochastic model that mix localized and long distance connectivity in this model player may purchase edge at distance d at a cost of dfi and wish to minimize the sum of their edge purchase and their average distance to other player in this model we show there is a striking small world threshold phenomenon in two dimension if fi then every nash equilibrium result in a network of constant diameter independent of network size and if fi then every nash equilibrium result in a network whose diameter grows a a root of the network size and thus is unbounded we contrast our result with those of kleinberg in a stochastic model and empirically investigate the navigability of equilibrium network our theoretical result all generalize to higher dimension 
we consider the problem of inferring the structure of a network from cooccurrence data observation that indicate which node occur in a signaling pathway but do not directly reveal node order within the pathway this problem is motivated by network inference problem arising in computational biology and communication system in which it is difficult or impossible to obtain precise time ordering information without order information every permutation of the activated node lead to a different feasible solution resulting in combinatorial explosion of the feasible set however physical principle underlying most networked system suggest that not all feasible solution are equally likely intuitively node that co occur more frequently are probably more closely connected building on this intuition we model path co occurrence a randomly shuffled sample of a random walk on the network we derive a computationally efficient network inference algorithm and via novel concentration inequality for importance sampling estimator prove that a polynomial complexity monte carlo version of the algorithm converges with high probability 
in previous study quadratic modelling of natural image ha resulted in cell model that react strongly to edge and bar here we apply quadratic independent component analysis to natural image patch and show that up to a small approximation error the estimated component are computing conjunction of two linear feature these conjunctive feature appear to represent not only edge and bar but also inherently two dimensional stimulus such a corner in addition we show that for many of the component the underlying linear feature have essentially v simple cell receptive field characteristic our result indicate that the development of the v cell preferring angle and corner may be partly explainable by the principle of unsupervised sparse coding of natural image 
for difficult classification or regression problem practitioner often segment the data into relatively homogenous group and then build a model for each group this two step procedure usually result in simpler more interpretable and actionable model without any lossin accuracy we consider problem such a predicting customer behavior across product where the independent variable can be naturally partitioned into two group a pivoting operation can now result in the dependent variable showing up a entry in a customer by product data matrix we present a model based co clustering meta algorithm that interleaf clustering and construction of prediction model to iteratively improve both cluster assignment and fit of the model this algorithm provably converges to a local minimum of a suitable cost function the framework not only generalizes co clustering and collaborative filtering to model basedco clustering but can also be viewed a simultaneous co segmentation and classification or regression which is better than independently clustering the data first and then building model moreover it applies to a wide range of bi modal or multimodal data and can be easily specialized to address classification and regression problem we demonstrate the effectiveness of our approach on both these problem through experimentation on real and synthetic data 
we introduce a new inference algorithm for dirichlet process mixture model while gibbs sampling and variational method focus on local move the new algorithm make more global move this is done by introducing a permutation of the data point a an auxiliary variable the algorithm is a blocked sampler which alternate between sampling the clustering and sampling the permutation the key to the efficiency of this approach is that it is possible to use dynamic programming to consider all exponentially many clustering consistent with a given permutation we also show that random projection can be used to effectively sample the permutation the result is a stochastic hill climbing algorithm that yield burn in time significantly smaller than those of collapsed gibbs sampling 
a number of supervised learning method have been introduced in the last decade unfortunately the last comprehensive empirical evaluation of supervised learning wa the statlog project in the early s we present a large scale empirical comparison between ten supervised learning method svms neural net logistic regression naive bayes memory based learning random forest decision tree bagged tree boosted tree and boosted stump we also examine the effect that calibrating the model via platt scaling and isotonic regression ha on their performance an important aspect of our study is the use of a variety of performance criterion to evaluate the learning method 
we present a general boosting method extending functional gradient boosting to optimize complex loss function that are encountered in many machine learning problem our approach is based on optimization of quadratic upper bound of the loss function which allows u to present a rigorous convergence analysis of the algorithm more importantly this general framework enables u to use a standard regression base learner such a single regression tree for fitting any loss function we illustrate an application of the proposed method in learning ranking function for web search by combining both preference data and labeled data for training we present experimental result for web search using data from a commercial search engine that show significant improvement of our proposed method over some existing method 
property of ensemble classification can be studied using the framework of monte carlo stochastic algorithm within this framework it is also possible to define a new ensemble classifier whose accuracy probability distribution can be computed exactly this paper ha two goal first an experimental comparison between the theoretical prediction and experimental result second a systematic comparison between bagging and monte carlo ensemble classification 
in the online linear optimization problem a learner must choose in each round a decision from a set d rn in order to minimize an unknown and changing linear cost function we present sharp rate of convergence with respect to additive regret for both the full information setting where the cost function is revealed at the end of each round and the bandit setting where only the scalar cost incurred is revealed in particular this paper is concerned with the price of bandit information by which we mean the ratio of the best achievable regret in the bandit setting to that in the full information setting for the full information case the upper bound on the regret is o p nt where n is the ambient dimension andt is the time horizon for the bandit case we present an algorithm which achieveso n p t regret all previous nontrivial bound here were o poly n t or worse it is striking that the convergence rate for the bandit setting is only a factor of n worse than in the full information case in stark contrast to the k arm bandit setting where the gap in the dependence on k is exponential p tk v p t logk we also present lower bound showing that this gap is at least p n which we conjecture to be the correct order the bandit algorithm we present can be implemented efficiently in special case of particular interest such a path planning and markov decision problem 
this paper extends previous work on skewing an approach to problematic function in decision tree induction the previous algorithm were applicable only to function of binary variable in this paper we extend skewing to directly handle function of continuous and nominal variable we present experiment with randomly generated function and a number of real world datasets to evaluate the algorithm s accuracy our result indicate that our algorithm almost always outperforms an information gain based decision tree learner 
we study the problem of discovering a manifold that best preserve information relevant to a nonlinear regression solving this problem involves extending and uniting two thread of research on the one hand the literature on sufficient dimension reduction ha focused on method for finding the best linear subspace for nonlinear regression we extend this to manifold on the other hand the literature on manifold learning ha focused on unsupervised dimensionality reduction we extend this to the supervised setting our approach to solving the problem involves combining the machinery of kernel dimension reduction with laplacian eigenmaps specifically we optimize cross covariance operator in kernel feature space that are induced by the normalized graph laplacian the result is a highly flexible method in which no strong assumption are made on the regression function or on the distribution of the covariates we illustrate our methodology on the analysis of global temperature data and image manifold 
if language user are rational they might choose to structu re their utterance so a to optimize communicative property in particular in formation theoretic and psycholinguistic consideration suggest that this may inc lude maximizing the uniformity of information density in an utterance we investigate this possibility in the context of syntactic reduction where the speaker ha the option of either marking a higher order unit a phrase with an extra word or leaving it unmarked we demonstrate that speaker are more likely to reduce le information dense phrase in a second step we combine a stochastic model of structured utterance production with a logistic regression model of syntactic r eduction to study which type of cue speaker employ when estimating the predictability of upcoming element we demonstrate that the trend toward predictability sensitive syntactic reduction jaeger is robust in the face of a wide variety of control variable and present evidence that speaker use both surface and structural cue for predictability estimation 
this article try to give an answer to a fundamental question intemporal data mining under what condition a temporal rule extracted from up to date temporal data keep it confidence support for future data a possible solution is given by using on the one hand a temporal logic formalism which allows the definition of the main notion event temporal rule support confidence in a formal way and on the other hand the stochastic limit theory under this probabilistic temporal framework the equivalence between the existence of the support of a temporal rule and the law of large number is systematically analyzed 
we discus two intrinsic weakness of the spectral graph partitioning method both of which have practical consequence the first is that spectral embeddings tend to hide the best cut from the commonly used hyperplane rounding method rather than cleaning up the resulting suboptimal cut with local search we recommend the adoption of flow based rounding the second weakness is that for many power law graph the spectral method produce cut that are highly unbalanced thus decreasing the usefulness of the method for visualization see figure b or a a basis for divide and conquer algorithm these balance problem which occur even though the spectral method s quotient style objective function doe encourage balance can be fixed with a stricter balance constraint that turn the spectral mathematical program into an sdp that can be solved for million node graph by a method of burer and monteiro 
we design an on line algorithm for principal component analysis in each trial the current instance is projected onto a probabilistically chosen low dimensional subspace the total expected quadratic approximation error equal the total quadratic approximation error of the best subspace chosen in hindsight plus some additional term that grows linearly in dimension of the subspace but logarithmically in the dimension of the instance 
support vector machine svms suffer from an o n training cost where n denotes the number of training instance in this paper we propose an algorithm to select boundary instance a training data to substantially reduce n our proposed algorithm is motivated by the result of burges that removing non support vector from the training set doe not change svm training result our algorithm eliminates instance that are likely to be non support vector in the concept independent preprocessing step of our algorithm we prepare nearest neighbor list for training instance in the concept specific sampling step we can then effectively select useful training data for each target concept empirical study show our algorithm to be effective in reducing n outperforming other competing downsampling algorithm without significantly compromising testing accuracy 
we present a simple and scalable algorithm for large margin estimation of structured model including an important class of markov network and combinatorial model we formulate the estimation problem a a convex concave saddle point problem and apply the extragradient method yielding an algorithm with linear convergence using simple gradient and projection calculation the projection step can be solved using combinatorial algorithm for min cost quadratic flow this make the approach an efficient alternative to formulation based on reduction to a quadratic program qp we present experiment on two very different structured prediction task d image segmentation and word alignment illustrating the favorable scaling property of our algorithm 
neural spike train present challenge to analytical effort due to their noisy spiking nature many study of neuroscientic and neural prosthetic importance rely on a smoothed denoised estimate of the spike train s underlying ring rate current technique to nd time varying ring rate require ad hoc choice of parameter offer no condence interval on their estimate and can obscure potentially important single trial variability we present a new method based on a gaussian process prior for inferring probabilistically optimal estimate of ring rate function underlying single or multiple neural spike train we test the performance of the method on simulated data and experimentally gathered neural spike train and we demonstrate improvement over conventional estimator 
biological sensory system are faced with the problem of encoding a high fidelity sensory signal with a population of noisy low fidelity neuron this problem can be expressed in information theoretic term a coding and transmitting a multi dimensional analog signal over a set of noisy channel previously we have shown that robust overcomplete code can be learned by minimizing the reconstruction error with a constraint on the channel capacity here we present a theoretical analysis that characterizes the optimal linear coder and decoder for oneand twodimensional data the analysis allows for an arbitrary number of coding unit thus including both underand over complete representation and provides a number of important insight into optimal coding strategy in particular we show how the form of the code adapts to the number of coding unit and to different data and noise condition to achieve robustness we also report numerical solution for robust coding of highdimensional image data and show that these code are substantially more robust compared against other image code such a ica and wavelet 
we consider the problem of constructing an aggregated estimator from a finite class of base function which approximately minimiz e a convex risk functional under the constraint for this purpose we propose a stochastic procedure the mirror descent which performs gradient descent in the dual space the generated estimate are additionally averaged in a recursive fashion with specific weight mirror de cent algorithm have been developed in different context and they are known to be particularly efficient in high dimensional problem mor eover their implementation is adapted to the online setting the main result of the paper is the upper bound on the convergence rate for the generalization error 
in this paper we introduce and study the minimum consistent subset cover mcsc problem given a finite ground set x and a constraint t find the minimum number of consistent subset that cover x where a subset of x is consistent if it satisfies t the mcsc problem generalizes the traditional set covering problem and ha minimum clique partition a dual problem of graph coloring a an instance many practical data mining problem in the area of rule learning clustering and frequent pattern mining can be formulated a mcsc instance in particular we discus the minimum rule set problem that minimizes model complexity of decision rule a well a some converse k clustering problem that minimize the number of cluster satisfying certain distance constraint we also show how the mcsc problem can find application in frequent pattern summarization for any of these mcsc formulation our proposed novel graph based generic algorithm cag can be directly applicable cag start by constructing a maximal optimal partial solution then performs an example driven specific to general search on a dynamically maintained bipartite assignment graph to simultaneously learn a set of consistent subset with small cardinality covering the ground set our experiment on benchmark datasets show that cag achieves good result compared to existing popular heuristic 
we present new general purpose kernel for protein structure analysis and describe how to apply them to structural motif discovery and function classification experiment show that our new method are faster than conventional technique are capable of finding structural motif and are very effective in function classification in addition to strong cross validation result we found possible new oxidoreductase and cytochrome p reductase and a possible new structural motif in cytochrome p reductase 
we present a new local approximation algorithm for computing map and logpartition function for arbitrary exponential family distr ibution represented by a finite valued pair wise markov random field mrf say g our algorithm is based on decomposingg into appropriatelychosen small component computing estimate locally in each of these component and then producing a good global solution we prove that the algorithm can provide approximate solution within arbitrary accuracy wheng excludes some finite sized graph a it minor and g ha bounded degree all planar graph with bounded degree are example of such graph the running time of the algorithm is n n is the number of node in g with constant dependent on accuracy degree of graph and size of the graph that is excluded a a minor constant for planar graph our algorithm for minor excluded graph us the decomposition scheme of klein plotkin and rao in general our algorithm work with any decomposition scheme and provides quantifiable approximation gu arantee that depends on the decomposition scheme 
we propose a semi supervised learning algorithm for visual object categorization which utilizes statistical information from unlabelled data to increase classification performance we build on an earlier hybrid generative discriminative approach by holub et al which extract fisher score from generative model the hybrid model allows u to combine the modelling power and flexibility of generative model with discriminative classifier here we illustrate how the generative framework can be used to add prior knowledge obtained from unlabelled image which the discriminative classifier can subsequently exploit we illustrate the effect of using different set of image a prior knowledge and find that the greatest benefit are incurred when the prior exemplar have similar statistic to the class being discriminated between our test show that strong performance can be obtained in discriminating between the face of two different people using prior knowledge and only training example furthermore we extend our approach to multi class discrimination and show state of the art performance on the caltech 
estimation of three dimensional articulated human pose and motion from image is a central problem in computer vision much of the previous work ha been limited by the use of crude generative model of human represented a articulated collection of simple part such a cylinder automatic initialization of such model ha proved difficult and most approach assume t hat the size and shape of the body part are known a priori in this paper we propose a method for automatically recovering a detailed parametric model of non rigid body shape and pose from monocular imagery specifically we represent the body using a parameterized triangulated mesh model that is learned from a database of human range scan we demonstrate a discriminative method to directly recover the model parameter from monocular image using a conditional mixture of kernel regressors this predicted pose and shape are used to initialize a generative model for more detailed pose and shape estimation the resulting approach allows fully automatic pose and shape recovery from monocular and multi camera imagery experimental result show that our method is capable of robustly recovering articulated pose shape and biometric measurement e g height weight etc in both calibrated and uncalibrated camera environment 
linear implementation of the efficient coding hypothesis such a independent component analysis ica and sparse coding model have provided functional explanation for property of simple cel l in v these model however ignore the non linear behavior of neuron and fail to match individual and population property of neural receptive field in subtle but important way hierarchical model in cluding gaussian scale mixture and other generative statistical model can capture higher order regularity in natural image an d explain nonlinear aspect of neural processing such a normalization and context effect previously it had been assumed that the lower l evel representation is independent of the hierarchy and had been fixed whe n training these model here we examine the optimal lower level representation derived in the context of a hierarchical model and find that th e resulting representation are strikingly different from those based on linear model unlike the the basis function and filter learned by ica or sparse coding these function individually more closely resemble simple cell receptive field and collectively span a broad range of spati al scale our work unifies several related approach and observation ab out natural image structure and suggests that hierarchical model might yield better representation of image structure throughout the hierarc hy 
accurate localization of mobile object is a major research problem in sensor network and an important data mining application specifically the localization problem is to determine the location of a client device accurately given the radio signal strength value received at the client device from multiple beacon sensor or access point conventional data mining and machine learning method can be applied to solve this problem however all of them require large amount of labeled training data which can be quite expensive in this paper we propose a probabilistic semi supervised learning approach to reduce the calibration effort and increase the tracking accuracy our method is based on semi supervised conditional random field which can enhance the learned model from a small set of training data with abundant unlabeled data effectively to make our method more efficient we exploit a generalized em algorithm coupled with domain constraint we validate our method through extensive experiment in a real sensor network using crossbow mica sensor the result demonstrate the advantage of method compared to other state of the art object tracking algorithm 
frequent pattern mining ha been studied extensively on scalable method for mining various kind of pattern including itemsets sequence and graph however the bottleneck of frequent pattern mining is not at the efficiency but at the interpretability due to the huge number of pattern generated by the mining process in this paper we examine how to summarize a collection of itemset pattern using only k representative a small number of pattern that a user can handle easily the k representative should not only cover most of the frequent pattern but also approximate their support a generative model is built to extract and profile these representative under which the support of the pattern can be easily recovered without consulting the original dataset based on the restoration error we propose a quality measure function to determine the optimal value of parameter k polynomial time algorithm are developed together with several optimization heuristic for efficiency improvement empirical study indicate that we can obtain compact summarization in real datasets 
in this study a novel multidimensional time series classification technique namely support feature machine sfm is proposed sfm is inspired by the optimization model of support vector machine and the nearest neighbor rule to incorporate both spatial and temporal of the multi dimensional time series data this paper also describes an application of sfm for detecting abnormal brain activity epilepsy is a case in point in this study in epilepsy study electroencephalogram eeg acquired in multidimensional time series format have been traditionally used a a gold standard tool for capturing the electrical change in the brain from multi dimensional eeg time series data sfm wa used to identify seizure pre cursor and detect seizure susceptibility pre seizure period the empirical result showed that sfm achieved over correct classification of per seizure eeg on average in patient using fold cross validation the proposed optimization model of sfm is very compact and scalable and can be implemented a an online algorithm the outcome of this study suggests that it is possible to construct a computerized algorithm used to detect seizure pre cursor and warn of impending seizure through eeg classification 
we consider the wavelet synopsis construction problem for data stream where given n number we wish to estimate the data by constructing a synopsis whose size say b is much smaller than n the b number are chosen to minimize a suitable error between the original data and the estimate derived from the synopsis several good one pas wavelet construction streaming algorithm minimizing the l error exist for other error measure the problem is le understood we provide the first one pas small space streaming algorithm with provable error guarantee additive approximation for minimizing a variety of non euclidean error measure including all weighted lp including l and relative error lp metric in several previous work solution for weighted l l and maximum relative error where the b synopsis coefficient are restricted to be wavelet coefficient of the data were proposed this restriction yield suboptimal solution on even fairly simple example other line of research such a probabilistic synopsis imposed restriction on how the synopsis wa arrived at to the best of our knowledge this paper is the first paper to address the general problem without any restriction on how the synopsis is arrived at a well a provide the first streaming algorithm with guaranteed performance for these class of error measure 
many perceptual process and neural computation such a speech recognition motor control and learning depend on the ability to measure and mark the passage of time however the process that make such temporal judgement possible are unknown a number of different hypothetical mechanism have been advanced all of which depend on the known temporally predictable evolution of a neural or psychological state possibly through oscillation or the gradual decay of a memory trace alternatively judgement of elapsed time might be based on observation of temporally structured but stochastic process such process need not be specific to the sense of time typical neural and sensor y process contain at least some statistical structure across a range of time scal e here we investigate the statistical property of an estimator of elapsed time w hich is based on a simple family of stochastic process 
the problem of finding frequent pattern from graph based datasets is an important one that find application in drug discovery protein structure analysis xml querying and social network analysis among others in this paper we propose a framework to mine frequent large scale structure formally defined a frequent topological structure from graph datasets key element of our framework include fast algorithm for discovering frequent topological pattern based on the well known notion of a topological minor algorithm for specifying and pushing constraint deep into the mining process for discovering constrained topological pattern and mechanism for specifying approximate match when discovering frequent topological pattern in noisy datasets we demonstrate the viability and scalability of the proposed algorithm on real and synthetic datasets and also discus the use of the framework to discover meaningful topological structure from protein structure data 
deep multi layer neural network have many level of non linearity which allows them to potentially represent very compactly highly non linear and highly varying function however until recently it wa not clear how to train such deep network since gradient based optimization starting from random initialization appears to often get stuck in poor solution hinton et al recently introduced a greedy layer wise unsupervised learning algorithm for deep belief network dbn a generative model with many layer of hidden causal variable in the context of the above optimization problem we study this algorithm empirically and explore variant to better understand it success and extend it to case where the input are continuous or where the structure of the input distribution is not revealing enough about the variable to be predicted in a supervised task 
frequent coherent subgraphs can provide valuable knowledge about the underlying internal structure of a graph database and mining frequently occurring coherent subgraphs from large dense graph database ha been witnessed several application and received considerable attention in the graph mining community recently in this paper we study how to efficiently mine the complete set of coherent closed quasi clique from large dense graph database which is an especially challenging task due to the downward closure property no longer hold by fully exploring some property of quasi clique we propose several novel optimization technique which can prune the unpromising and redundant sub search space effectively meanwhile we devise an efficient closure checking scheme to facilitate the discovery of only closed quasi clique we also develop a coherent closed quasi clique mining algorithm cocain thorough performance study show that cocain is very efficient and scalable for large dense graph database 
data mining in large collection of polyphonic music ha recently received increasing interest by company along with the advent of commercial online distribution of music important application include the categorization of song into genre and the recommendation of song according to musical similarity and the customer s musical preference modeling genre or timbre of polyphonic music is at the core of these task and ha been recognized a a difficult problem many audio feature have been proposed but they do not provide easily understandable description of music they do not explain why a genre wa chosen or in which way one song is similar to another we present an approach that combine large scale feature generation with meta learning technique to obtain meaningful feature for musical similarity we perform exhaustive feature generation based on temporal statistic and train regression model to summarize a subset of these feature into a single descriptor of a particular notion of music using several such model we produce a concise semantic description of each song genre classification model based on these semantic feature are shown to be better understandable and almost a accurate a traditional method 
in recent year there ha been a proliferation of theoretical graph model e g preferential attachment and small world model motivated by real world graph such a the internet topology to address the natural question of which model is best for a particular data set we propose a model selection criterion for graph model since each model is in fact a probability distribution over graph we suggest using maximum likelihood to compare graph model and select their parameter interestingly for the case of graph model computing likelihood is a difficult algorithmic task however we design and implement mcmc algorithm for computing the maximum likelihood for four popular model a power law random graph model a preferential attachment model a small world model and a uniform random graph model we hope that this novel use of ml will objectify comparison between graph model 
we investigate under what condition clustering by learning a mixture of spherical gaussians is a computationally tractable and b statistically possible we show that using principal component projection greatly aid in recovering the clustering using em present empirical evidence that even using such a projection there is still a large gap between the number of sample needed to recover the clustering using em and the number of sample needed without computational restriction and characterize the regime in which such a gap exists 
we show how to apply the efficient bayesian changepoint detection technique of fearnhead in the multivariate setting we model the joint density of vector valued observation using undirected gaussian graphical model whose structure we estimate we show how we can exactly compute the map segmentation a well a how to draw perfect sample from the posterior over segmentation simultaneously accounting for uncertainty about the number and location of changepoints a well a uncertainty about the covariance structure we illustrate the technique by applying it to financial data and to bee tracking data 
in some application such a filling in a customer information form on the web some missing value may not be explicitly represented a such but instead appear a potentially valid data value such missing value are known a disguised missing data which may impair the quality of data analysis severely such a causing significant bias and misleading result in hypothesis test correlation analysis and regression the very limited previous study on cleaning disguised missing data use outlier mining and distribution anomaly detection they highly rely on domain background knowledge in specific application and may not work well for the case where the disguise value are inliers to tackle the problem of cleaning disguised missing data in this paper we first model the distribution of disguised missing data and propose the embedded unbiased sample heuristic then we develop an effective and efficient method to identify the frequently used disguise value which capture the major body of the disguised missing data our method doe not require any domain background knowledge to find the suspicious disguise value we report an empirical evaluation using real data set which show that our method is effective the frequently used disguise value found by our method match the value identified by the domain expert nicely our method is also efficient and scalable for processing large data set 
the supremacy of n gram model in statistical language modelling ha recently been challenged by parametric model that use distributed representation to counteract the difficulty caused by data sparsity we propose three new probabilistic language model that define the distribution of the next word in a sequence given several preceding word by using distributed representation of those word we show how real valued distributed representation for word can be learned at the same time a learning a large set of stochastic binary hidden feature that are used to predict the distributed representation of the next word from previous distributed representation adding connection from the previous state of the binary hidden feature improves performance a doe adding direct connection between the real valued distributed representation one of our model significantly outperforms the very best n gram model 
currently most research on nonnegative matrix factorization nmf focus on factor x fg t factorization we provide a systematicanalysis of factor x fsg t nmf while it unconstrained factor nmf is equivalent to it unconstrained factor nmf itconstrained factor nmf brings new feature to it constrained factor nmf we study the orthogonality constraint because it leadsto rigorous clustering interpretation we provide new rule for updating f s g and prove the convergenceof these algorithm experiment on datasets and a real world casestudy are performed to show the capability of bi orthogonal factornmf on simultaneously clustering row and column of the input datamatrix we provide a new approach of evaluating the quality ofclustering on word using class aggregate distribution andmulti peak distribution we also provide an overview of various nmf extension andexamine their relationship 
the additive clustering model is widely used to infer the feature of a set of stimulus from their similarity on the assumption that similarity is a weighted linear function of common feature this paper develops a fully bayesian formulation of the additive clustering model using method from nonparametric bayesian statistic to allow the number of feature to vary we use this to explore several approach to parameter estimation showing that the nonparametric bayesian approach provides a straightforward way to obtain estimate of both the number of feature used in producing similarity judgment and their importance 
abstract we describe a novel unsupervised method for learning sparse overcomplete feature the model us a linear encoder and a linear decoder p receded by a sparsifying non linearity that turn a code vector into a quasibinary sparse code vector given an input the optimal code minimizes the distance between the output of the decoder and the input patch while being a similar a possible to the encoder output learning proceeds in a two phase em like fashion compute the minimum energy code vector adjust the parameter of the encoder and decoder so a to decrease the energy the model produce stroke detector when trained on handwritten numeral and gabor like filter whe n trained on natural image patch inference and learning are very fast requir ing no preprocessing and no expensive sampling using the proposed unsupervised method to initialize the first layer of a convolutional network we achieved an err or rate slightly lower than the best reported result on the mnist dataset finally an extension of the method is described to learn topographical filter map 
we study boosting in the filtering setting where the booster draw example from an oracle instead of using a fixed training set and so may train efficiently on very large datasets our algorithm which is based on a logistic regression technique proposed by collins schapire singer requires fewer assumption to achieve bound equivalent to or better than previous work moreover we give the first proof that the algorithm of collins et al is a strong pac learner albeit within the filtering setting our proof demonstrate the algorithm s strong theoretical property for both classification and conditional probability estimation and we validate these result through extensive experiment empirically our algorithm prof more robust to noise and overfitting than batch booster in conditional probability estimation and prof competitive in classification 
hierarchical model have been extensively studied in various domain however existing model assume fixed model structure or incorporate structural uncertainty generatively in this paper we propose dynamic hierarchical markov random field dhmrfs to incorporate structural uncertainty in a discriminative manner dhmrfs consist of two part structure model and class label model both are defined a exponential family distribution conditioned on observation dhmrfs relax the independence assumption a made in directed model a exact inference is intractable a variational method is developed to learn parameter and to find the map model structure and label assignment we apply the model to a real world web data extraction task which automatically extract product item for sale on the web the result show promise 
while classical experiment on spike timing dependent plasticity analyzed synaptic change a a function of the timing of pair of preand postsynaptic spike more recent experiment also point to the effect of spike triplet here we develop a mathematical framework that allows u to characterize timing based learning rule moreover we identify a candidate learning rule with five variable and free param eters that capture a variety of experimental data including the dependence of potentiation and depression upon preand postsynaptic firing frequency the relation to the bienenstock cooper munro rule a well a to some timing based rule is discussed 
we present a family of algorithm to uncover tribe group of individual who share unusual sequence of affiliation while much work inferring community structure describes large scale trend we instead search for small group of tightly linked individual who behave anomalously with respect to those trend we apply the algorithm to a large temporal and relational data set consisting of million of employment record from the national association of security dealer the resulting tribe contain individual at higher risk for fraud are homogenous with respect to risk score and are geographically mobile all at significant level compared to random or to other set of individual who share affiliation 
motivated in part by the hierarchical organization of the cortex a number of algorithm have recently been proposed that try to learn hierarchical or deep structure from unlabeled data while several author have formally or informally compared their algorithm to computation performed in visual area v and the cochlea little attempt ha been made thus far to evaluate these algorithm in term of their fidelity for mimicking computation at deeper level in the cortical hierarchy this paper present an unsupervised learning model that faithfully mimic certain property of visual area v specifically we develop a sparse variant of the deep belief network of hinton et al we learn two layer of node in the network and demonstrate that the first layer similar to prior work on sparse coding and ica result in localized oriented edge filter similar to the gabor function known to model v cell receptive field further the second layer in our model encodes correlation of the first layer response in the data specifically it pick up both colinear contour feature a well a corner and junction more interestingly in a quantitative comparison the encoding of these more complex corner feature match well with the result from the ito komatsu s study of biological v response this suggests that our sparse variant of deep belief network hold promise for modeling more higher order feature 
in online handwriting recognition the trajectory of the pen is recorded during writing although the trajectory provides a compact and complete representation of the written output it is hard to transcribe directly because each letter is spread over many pen location most recognition system therefore employ sophisticated preprocessing technique to put the input into a more localised form however these technique require considerable human effort and are specific to particular language and alphabet this paper describes a system capable of directly transcribing raw online handwriting data the system consists of an advanced recurrent neural network with an output layer designed for sequence labelling combined with a probabilistic language model in experiment on an unconstrained online database we record excellent result using either raw or preprocessed data well outperforming a state of the art hmm based system in both case 
we present a family of incremental perceptron like algorithm plas with margin in which both the effective learning rate defined a the ratio of the learning rate to the length of the weight vector and the misclassification condition are entirely controlled by rule involving power of the number of mistake we examine the convergence of such algorithm in a finite number of step and show that under some rather mild condition there exists a limit of the parameter involved in which convergence lead to classification with maximum margin an experimental comparison of algorithm belonging to this family with other large margin plas and decomposition svms is also presented 
semi supervised learning i e learning from both labeled and unlabeled data ha received signicant attention in the machine learning literature in recent year still our understanding of the theoretical foundation of the usefulness of unlabeled data remains somewhat limited the simplest and the best understood situation is when the data is described by an identiable mixture model and where each class come from a pure component this natural setup and it implication ware analyzed in one important result wa that in certain regime labeled data becomes exponentially more valuable than unlabeled data however in most realistic situation one would not expect that the data come from a parametric mixture distribution with identiable component there have been recent effort to analyze the non parametric situation for example cluster and manifold assumption have been suggested a a basis for analysis still a satisfactory and fairly complete theoretical understanding of the nonparametric problem similar to that in ha not yet been developed in this paper we investigate an intermediate situation when the data come from a probability distribution which can be modeled but not perfectly by an identiable mixture distribution this seems applicable to many situation when for example a mixture of gaussians is used to model the data the contribution of this paper is an analysis of the role of labeled and unlabeled data depending on the amount of imperfection in the model 
the standard support vector machine formulation doe not provide it user with the ability to explicitly control the number of support vector used to define the generated classifier we present a modified version of svm that allows the user to set a budget parameter b and focus on minimizing the loss attained by the b worst classified example while ignoring the remaining example this idea can be used to derive sparse version of both l svm and l svm technically we obtain these new svm variant by replacing the norm in the standard svm formulation with various interpolation norm we also adapt the smo optimization algorithm to our setting and report on some preliminary experimental result 
electrical power management in large scale it system such a commercial datacenters is an application area of rapidly growing interest f rom both an economic and ecological perspective with billion of dollar and mi llions of metric ton of co emission at stake annually business want to save power without sacrificing performance this paper present a reinforcement l earning approach to simultaneous online management of both performance and power consumption we apply rl in a realistic laboratory testbed using a blade cluster and dynamically varying http workload running on a commercial web application middleware platform we embed a cpu frequency controller in the blade server firmware and we train policy for this controller using a mu lti criterion reward signal depending on both application performance and cpu power consumption our testbed scenario posed a number of challenge to successful use of rl including multiple disparate reward function limited deci sion sampling rate and pathology arising when using multiple sensor reading a state variable we describe innovative practical solution to these challeng e and demonstrate clear performance improvement over both hand designed policy a well a obvious cookbook rl implementation 
we now have incrementally grown database of text document ranging back for over a decade in area ranging from personal email to news article and conference proceeding while accessing individual document is easy method for overviewing and understanding these collection a a whole are lacking in number and in scope in this paper we address one such global analysis task namely the problem of automatically uncovering how idea spread through the collection over time we refer to this problem a information genealogy in contrast to bibliometric method that are limited to collection with explicit citation structure we investigate content based method requiring only the text and timestamps of the document in particular we propose a language modeling approach and a likelihood ratio test to detect influence between document in a statistically well founded way furthermore we show how this method can be used to infer citation graph and to identify the most influential document in the collection experiment on the nip conference proceeding and the physic arxiv show that our method is more effective than method based on document similarity 
in the paper we propose a new type of regularization procedure for training sparse bayesian method for classification transforming hessian matrix of log likelihood function to diagonal form with further regularization of it eigenvectors allows u to optimize evidence explicitly a a product of one dimensional integral the process of automatic regularization coefficient determination then converges in one iteration we show how to use the proposed approach for gaussian and laplace prior both algorithm show comparable performance with the state of the art relevance vector machine rvm but require le time for training and produce more sparse decision rule in term of degree of freedom 
maximum entropy analysis of binary variable provides an elegant way for studying the role of pairwise correlation in neural population unfortunately these approach suffer from their poor scalability to high dimension in sensory coding however high dimensional data is ubiquitous here we introduce a new approach using a near maximum entropy model that make this type of analysis feasible for very high dimensional data the model parameter can be derived in closed form and sampling is easy therefore our nearmaxent approach can serve a a tool for testing prediction from a pairwise maximum entropy model not only for low dimensional marginals but also for high dimensional measurement of more than thousand unit we demonstrate it usefulness by studying natural image with dichotomized pixel intensity our result indicate that the statistic of such higher dimensional measurement exhibit additional structure that are not predicted by pairwise correlation despite the fact that pairwise correlation explain the lower dimensional marginal statistic surprisingly well up to the limit of dimensionality where estimation of the full joint distribution is feasible 
it ha been established that a neuron reproduces highly precise spike response to identical fluctuating input current we wish to accurately predict the firing time of a given neuron for any input current for this purpose we adopt a model that mimic the dynamic of the membrane potential and then take a cue from it dynamic for predicting the spike occurrence for a novel input current it is found that the prediction is significantly improved by observing the state space of the membrane potential and it time derivative s in advance of a possible spike in comparison to simply thresholding an instantaneous value of the estimated potential 
spatial scan statistic are used to determine hotspot in spatial data and are widely used in epidemiology and biosurveillance in recent year there ha been much effort invested in designing efficient algorithm for finding such high discrepancy region with method ranging from fast heuristic for special case to general grid based method and to efficient approximation algorithm with provable guarantee on performance and quality in this paper we make a number of contribution to the computational study of spatial scan statistic first we describe a simple exact algorithm for finding the largest discrepancy region in a domain second we propose a new approximation algorithm for a large class of discrepancy function including the kulldorff scan statistic that improves the approximation versus run time trade off of prior method third we extend our simple exact and our approximation algorithm to data set which lie naturally on a grid or are accumulated onto a grid fourth we conduct a detailed experimental comparison of these method with a number of known method demonstrating that our approximation algorithm ha far superior performance in practice to prior method and exhibit a good performance accuracy trade off all extant method including those in this paper are suitable for data set that are modestly sized if data set are of the order of million of data point none of these method scale well for such massive data setting it is natural to examine whether small space streaming algorithm might yield accurate answer here we provide some negative result showing that any streaming algorithm that even provide approximately optimal answer to the discrepancy maximization problem must use space linear in the input 
the top web search result is crucial for user satisfaction with the web search experience we argue that the importance of the relevance at the top position necessitates special handling of the top web search result for some query we propose an effective approach of leveraging million of past user interaction with a web search engine to automatically detect best bet top result preferred by majority of user interestingly this problem can be more effectively addressed with classification than using state of the art general ranking method furthermore we show that our general machine learning approach achieves precision comparable to a heavily tuned domain specific algorithm with significantly higher coverage our experiment over million of user interaction for thousand of query demonstrate the effectiveness and robustness of our technique 
predictive state representation psrs are powerful model of non markovian decision process that differ from traditional model e g hmms pomdps by representing state using only observable quantity because of this psrs can be learned solely using data from interaction with the process the majority of existing technique though explicitly or implicitly require that this data be gathered using a blind policy where action are selected independently of preceding observation this is a severe limitation for practical learning of psrs we present two method for fixing this limitation in most of the existing psr algorithm one when the policy is known and one when it is not we then present an efficient optimization for computing good exploration policy to be used when learning a psr the exploration policy which are not blind significantly lower the amount of data needed to build an accurate model thus demonstrating the importance of non blind policy 
ontology are being successfully used to overcome semanticheterogeneity and are becoming fundamental element of the semanticweb recently it ha also been shown that ontology can be used tobuild more accurate and more personalized recommendation system byinferencing missing user s preference however these systemsassume the existence of ontology without considering theirconstruction with product catalog changing continuously newtechniques are required in order to build these ontology in realtime and autonomously from any expert intervention this paper focus on this problem and show that it is possible tolearn ontology autonomously by using clustering algorithm result on the movielens and jester data set show that recommendersystem with learnt ontology significantly outperform the classical recommendation approach 
we focus on the problem of estimating the graph structure associated with a discrete markov random field we describe a method based on regularized logistic regression in which the neighborhood of any given node is estimated by performing logistic regression subject to an constraint our framework applies to the high dimensional setting in which both the number of node p and maximum neighborhood size d are allowed to grow a a function of the number of observation n our main result is to establish sufficient condition on the triple n p d for the method to succeed in consistently estimating the neighborhood of every node in the graph simultaneously under certain mutual incoherence condition analogous to those imposed in previous work on linear regression we prove that consistent neighborhood selection can be obtained a long a the number of observation n grows more quickly than d log d d log p thereby establishing that logarithmic growth in the number of sample n relative to graph size p is sufficient to achieve neighborhood consistency 
online convex programming ha recently emerged a a powerful primitive for designing machine learning algorithm for example ocp can be used for learning a linear classifier dynamically rebalancing a binary se arch tree finding the shortest path in a graph with unknown edge length solving a structured classification problem or finding a good strategy in an extensive f orm game several researcher have designed no regret algorithm for ocp but compared to algorithm for special case of ocp such a learning from expert advice these algorithm are not very numerous or flexible in learning fro m expert advice one tool which ha proved particularly valuable is the correspondence between no regret algorithm and convex potential function by reasoning about these potential function researcher have designed algorithm with a wide variety of useful guarantee such a good performance when the target hypothesis is sparse until now there ha been no such recipe for the more general ocp problem and therefore no ability to tune ocp algorithm to take advantage of property of the problem or data in this paper we derive a new class of no regret learning algorithm for ocp these lagrangian hedgingalgorithms are based on a general class of potential function and are a direct generalizati on of known learning rule like weighted majority and external regret matching in addition to proving regret bound we demonstrate our algorithm learning to play one card poker 
automotive company such a ford motor company have no shortage of large database with abundant opportunity for cost reduction and revenue enhancement the data mining group at ford ha worked in the area of quality customer satisfaction and warranty analytics for close to ten year in this time we have developed a number of method for building system to help the business one area of particular success ha been in warranty analysis while traditional hazard analysis ha been applied at ford for a number of year we have used technique from other industry e g retail a well a text mining to view warranty analytics in a new way however our success ha been tempered by serious challenge particularly in the area of data understanding computing meaningful aggregation and implementation case study from the automobile industry warranty quality forecasting etc a well a from other industry will be used 
neural motor prosthesis nmps require the accurate decoding of motor cortical population activity for the control of an artificial motor system previous work on cortical decoding for nmps ha focused on the recovery of hand kinematics human nmps however may require the control of computer cursor or robotic device with very different physical and dynamical property here we show that the firing rate of cell in the primary motor cortex of non human primate can be used to control the parameter of an artificial physical system exhibiting realistic dynamic the model represents d hand motion in term of a point mass connected to a system of idealized spring the nonlinear spring coefficient are estimated from the firing rate of neuron in the motor cortex we evaluate linear and a nonlinear decoding algorithm using neural recording from two monkey performing two different task we found that the decoded spring coefficient produced accurate hand trajectory compared with state of the art method for direct decoding of hand kinematics furthermore using a physically based system produced decoded movement that were more natural in that their frequency spectrum more closely matched that of natural hand movement 
temporal text mining ttm is concerned with discovering temporal pattern in text information collected over time since most text information bear some time stamp ttm ha many application in multiple domain such a summarizing event in news article and revealing research trend in scientific literature in this paper we study a particular ttm task discovering and summarizing the evolutionary pattern of theme in a text stream we define this new text mining problem and present general probabilistic method for solving this problem through discovering latent theme from text constructing an evolution graph of theme and analyzing life cycle of theme evaluation of the proposed method on two different domain i e news article and literature show that the proposed method can discover interesting evolutionary theme pattern effectively 
we propose a family of kernel for structured object which is based on the bag ofcomponents paradigm however rather than decomposing each complex object into the single histogram of it component we use for each object a family of nested histogram where each histogram in this hierarchy describes the object seen from an increasingly granular perspective we use this hierarchy of histogram to define elementary kernel which can detect coarse and fine similarity between the object we compute through an efficient averagi ng trick a mixture of such specific kernel to propose a final kernel value which weight efficiently local and global match we propose experimental result on an image retrieval experiment which show that this mixture is an effective template procedure to be used with kernel on histogram 
we consider criterion for variational representation of non gaussian latent variable and derive variational em algorithm in general form we establish a general equivalence among convex bounding method evidence based method and ensemble learning variational bayes method which ha previously been demonstrated only for particular case 
the collaborative filtering approach to recommender system predicts user preference for product or service by learning past user item relationship in this work we propose novel algorithm for predicting user rating of item by integrating complementary model that focus on pattern at different scale at a local scale we use a neighborhood based technique that infers rating from observed rating by similar user or of similar item unlike previous local approach our method is based on a formal model that account for interaction within the neighborhood leading to improved estimation quality at a higher regional scale we use svd like matrix factorization for recovering the major structural pattern in the user item rating matrix unlike previous approach that require imputation in order to fill in the unknown matrix entry our new iterative algorithm avoids imputation because the model involve estimation of million or even billion of parameter shrinkage of estimated value to account for sampling variability prof crucial to prevent overfitting both the local and the regional approach and in particular their combination through a unifying model compare favorably with other approach and deliver substantially better result than the commercial netflix cinematch recommender system on a large publicly available data set 
recently a very appealing approach wa proposed to compute the entire solution path for support vector classification svc with very low extra computational cost this approach wa later extended to a support vector regression svr model called svr however the method requires that the error parameter be set a priori which is only possible if the desired accuracy of the approximation can be specified in advance in this paper we show that the solution path for svr is also piecewise linear with respect to we further propose an efficient algorithm for exploring the two dimensional solution space defined by the regularization and error parameter a opposed to the algorithm for svc our proposed algorithm for svr initializes the number of support vector to zero and then increase it gradually a the algorithm proceeds a such a good regression function possessing the sparseness property can be obtained after only a few iteration 
inference in conditional random field and hidden markov model is done using the viterbi algorithm an efficient dynamic programming algorithm in many case general non local and non sequential constraint may exist over the output sequence but cannot be incorporated and exploited in a natural way by this inference procedure this paper proposes a novel inference procedure based on integer linear programming ilp and extends crf model to naturally and efficiently support general constraint structure for sequential constraint this procedure reduces to simple linear programming a the inference process experimental evidence is supplied in the context of an important nlp problem semantic role labeling 
we study the label complexity of pool based active learning in the agnostic pac model specifically we derive general bound on the number of label request made by the a algorithm proposed by balcan beygelzimer langford balcan et al this represents the first nontrivial general purpose upper bound on label complexity in the agnostic pac model 
we propose a new kernel function for attributed molecular graph which is based on the idea of computing an optimal assignment from the atom of one molecule to those of another one including information on neighborhood membership to a certain structural element and other characteristic for each atom a a byproduct this lead to a new class of kernel function we demonstrate how the necessary computation can be carried out efficiently compared to marginalized graph kernel our method in some case lead to a significant reduction of the prediction error further improvement can be gained if expert knowledge is combined with our method we also investigate a reduced graph representation of molecule by collapsing certain structural element like e g ring into a single node of the molecular graph 
in this paper we provide a general theorem that establishes a correspondence between surrogate loss function in classification and the family of f divergence moreover we provide constructive procedure for determining the f divergence induced by a given surrogate loss and conversely for finding all surrogate loss function that realize a given f divergence next we introduce the notion of universal equivalence among loss function and corresponding f divergence and provide necessary and sufficient condition for universal equivalence to hold these idea have application to classification problem that also involve a component of experiment design in particular we leverage our result to prove consistency of a procedure for learning a classifier under decentralization requirement 
many kernel learning method have to assume parametric form for the target kernel function which significantly limit the capability of kernel in fitting diverse pattern some kernel learning method assume the target kernel matrix to be a linear combination of parametric kernel matrix this assumption again importantly limit the flexibility of the target kernel matrix the key challenge with nonparametric kernel learning arises from the difficulty in linking the nonparametric kernel to the input pattern in this paper we resolve this problem by introducing the graph laplacian of the observed data a a regularizer when optimizing the kernel matrix with respect to the pairwise constraint we formulate the problem into semi definite program sdp and propose an efficient algorithm to solve the sdp problem the extensive evaluation on clustering with pairwise constraint show that the proposed nonparametric kernel learning method is more effective than other state of the art kernel learning technique 
spike sorting involves clustering spike recorded by a micro electrode according to the source neuron it is a complicated task which requires much human labor in part due to the non stationary nature of the data we propose to automate the clustering process in a bayesian framework with the source neuron modeled a a non stationary mixture of gaussians at a first search stage the data are divided into short time frame and candidate description of the data a mixture of gaussians are computed for each frame separately at a second stage transition probability between candidate mixture are computed and a globally optimal clustering solution is found a the maximum a posteriori solution of the resulting probabilistic model the transition probability are computed using local stationarity assumption and are based on a gaussian version of the jensen shannon divergence we employ synthetically generated spike data to illustrate the method and show that it outperforms other spike sorting method in a non stationary scenario we then use real spike data and find high agreement of the method with expert human sorter in two mode of operation a fully unsupervised and a semi supervised mode thus this method differs from other method in two aspect it ability to account for non stationary data and it close to human performance 
learning the common structure shared by a set of supervised task is an important practical and theoretical problem knowledge of this structure may lead to better generalization performance on the task and may also facilitate learning new task we propose a framework for solving this problem which is based on regularization with spectral function of matrix this class of regularization problem exhibit appealing computational property and can be optimized efciently by an alternating minimization algorithm in addition we provide a necessary and sufcient condition for convexity of the regularizer we analyze concrete example of the framework which are equivalent to regularization with lp matrix norm experiment on two real data set indicate that the algorithm scale well with the number of task and improves on state of the art statistical performance 
in kernel fisher discriminant analysis kfda we carry out fisher linear discriminant analysis in a high dimensional feature space defined implicitly by a kernel the performance of kfda depends on the choice of the kernel in this paper we consider the problem of finding the optimal kernel over a given convex set of kernel we show that this optimal kernel selection problem can be reformulated a a tractable convex optimization problem which interior point method can solve globally and efficiently the kernel selection method is demonstrated with some uci machine learning benchmark example 
we discus the problem of fitting an implicit shape model to a set of point sampled from a co dimension one manifold of arbitrary topology the method solves a non convex optimisation problem in the embedding function that defines the implicit by way of it zero level set by assuming that the solution is a mixture of radial basis function of varying width we attain the globally optimal solution by way of an equivalent eigenvalue problem without using or constructing a an intermediate step the normal vector of the manifold at each data point we demonstrate the system on two and three dimensional data with example of missing data interpolation and set operation on the resultant shape 
we propose a new technique for sampling the solution of combinatorial problem in a near uniform manner we focus on problem specified a a boolean formula i e on sat instance sampling for sat problem ha been shown to have interesting connection with probabilistic reasoning making practical sampling algorithm for sat highly desirable the best current approach are based on markov chain monte carlo method which have some practical limitation our approach exploit combinatorial property of random parity xor constraint to prune away solution near uniformly the final sample is identified amongst the remaining one using a state of the art sat solver the resulting sampling distribution is provably arbitrarily close to uniform our experiment show that our technique achieves a significantly better sampling quality than the best alternative 
there is experimental evidence that cortical neuron show avalanche activity with the intensity of firing event being distributed a a power law we present a biologically plausible extension of a neural network which exhibit a power law avalanche distribution for a wide range of connectivity parameter 
the receiver operating characteristic roc ha become a standard tool for the analysis and comparision of classiers when the cost of misclassication are unknown there ha been relatively little work however examining roc for more than two class here we dene the roc surface for the qclass problem in term of a multi objective optimisation problem in which the goal is to simultaneously minimise the q q misclassication rate when the misclassication cost and parameter governing the classier s behaviour are unknown we present an evolutionary algorithm to locate the optimal trade o surface between misclassications of dieren t type the performance of the evolutionary algorithm is illustrated on a synthetic three class problem in addition the use of the pareto optimal surface to compare classiers is discussed and we present a straightforward multi class analogue of the gini coecien t this is illustrated on synthetic and standard machine learning data 
abstract there have been many graph based approach for semi supervised classification one problem is that of hyperparameter learning performance depends greatly on the hyperparameters of the similarity graph transformation of the graph laplacian and the noise model we present a bayesian framework for learning hyperparameters for graph based semisupervised classification given some labeled data which can contain inaccurate label we pose the semi supervised classification a an inference problem over the unknown label expectation propagation is used for approximate inference and the mean of the posterior is used for classification the hyperparameters are learned using em for evidence maximization we also show that the posterior mean can be written in term of the kernel matrix providing a bayesian classifier to classify new point test on synthetic and real datasets show case where there are significant improvement in performance over the existing approach 
the increasing complexity of today s system make fast and accurate failure detection essential for their use in mission critical application various monitoring method provide a large amount of data about system s behavior analyzing this data with advanced statistical method hold the promise of not only detecting the error faster but also detecting error which are difficult to catch with current monitoring tool two challenge to building such detection tool are the high dimensionality of observation data which make the model expensive to apply and frequent system change which make the model expensive to update in this paper we present algorithm to reduce the dimensionality of data in a way that make it easy to adapt to system change we decompose the observation data into signal and noise subspace two statistic the hotelling t score and squared prediction error spe are calculated to represent the data characteristic in signal and noise subspace respectively instead of tracking the original data we use a sequentially discounting expectation maximization sdem algorithm to learn the distribution of the two extracted statistic a failure event can then be detected based on the abnormal change of the distribution applying our technique to component interaction data in a simple e commerce application show better accuracy than building independent profile for each component additionally experiment on synthetic data show that the detection accuracy is high even for changing system 
to achieve good generalization in supervised learning the training and testing example are usually required to be drawn from the same source distribution in this paper we propose a method to relax this requirement in the context of logistic regression assuming dp and da are two set of example drawn from two mismatched distribution where da are fully labeled and dp partially labeled our objective is to complete the label of dp we introduce an auxiliary variable for each example in da to reflect it mismatch with dp under an appropriate constraint the s are estimated a a byproduct along with the classifier we also present an active learning approach for selecting the labeled example in dp the proposed algorithm called migratory logit or m logit is demonstrated successfully on simulated a well a real data set 
sparse grid were recently introduced for classification and regression problem in this article we apply the sparse grid approach to semi supervised classification we formulate the semi supervised learning problem by a regularization approach here besides a regression formulation for the labeled data an additional term is involved which is based on the graph laplacian for an adjacency graph of all labeled and unlabeled data point it reflects the intrinsic geometric structure of the data distribution we discretize the resulting problem in function space by the sparse grid method and solve the arising equation using the so called combination technique in contrast to recently proposed kernel based method which currently scale cubic in regard to the number of overall data our method scale only linear provided that a sparse graph laplacian is used this allows to deal with huge data set which involve million of point we show experimental result with the new approach 
abstract we consider the semi supervised learning problem where a decision rule is to be learned from labeled and unlabeled data in this framework we motivate minimum entropy regularization which enables to incorporate unlabeled data in the standard supervised learning our approach includes other approach to the semi supervised problem a particular or limiting case a series of experiment illustrates that th e proposed solution benefit from unlabeled data the method challenge mix ture model when the data are sampled from the distribution class spanned by the generative model the performance are definitely in favor o f minimum entropy regularization when generative model are misspecified and the weighting of unlabeled data provides robustness to the violation of the cluster assumption finally we also illustrate that the method can also be far superior to manifold learning in high dimension space 
the neuron of the neocortex communicate by asynchronous event called action potential or spike however for simplicity of simulation most model of processing by cortical neural network have assumed that the activation of their neuron can be approximated by event rate rather than taking account of individual spike the obstacle to exploring the more detailed spike processing of these network ha been reduced considerably in recent year by the development of hybrid analog digital very large scale integrated hvlsi neural network composed of spiking neuron that are able to operate in real time in this paper we describe such a hvlsi neural network that performs an interesting task of selective attentional processing that wa previously described for a simulated pointer map rate model by hahnloser and colleague we found that most of the computational feature of their rate model can be reproduced in the spiking implementation but that spike based processing requires a modification of the original network architecture in order to memorize a previously attended target 
data arriving in time order a data stream arises in field including physic finance medicine and music to name a few often the data come from sensor in physic and medicine for example whose data rate continue to improve dramatically a sensor technology improves further the number of sensor is increasing so correlating data between sensor becomes ever more critical in order to distill knowlege from the data in many application such a finance recent correlation are of far more interest than long term correlation so correlation over sliding window windowed correlation is the desired operation fast response is desirable in many application e g to aim a telescope at an activity of interest or to perform a stock trade these three factor data size windowed correlation and fast response motivate this work previous work showed how to compute pearson correlation using fast fourier transforms and wavelet transforms but such technique don t work for time series in which the energy is spread over many frequency component thus resembling white noise for such uncooperative time series this paper show how to combine several simple technique sketch random projection convolution structured random vector grid structure and combinatorial design to achieve high performance windowed pearson correlation over a variety of data set 
in non cooperative multi agent situation there cannot exist a globally optimal yet opponent independent learning algorithm regret minimization over a set of strategy optimized for potential opponent model is proposed a a good framework for deciding how to behave in such situation using longer playing horizon and expert that learn a they play the regret minimization framework can be extended to overcome several shortcoming of earlier approach to the problem of multi agent learning 
the world wide web provides a nearly endless source of knowledge which is mostly given in natural language a first step towards exploiting this data automatically could be to extract pair of a given semantic relation from text document for example all pair of a person and her birthdate one strategy for this task is to find text pattern that express the semantic relation to generalize these pattern and to apply them to a corpus to find new pair in this paper we show that this approach profit significantly when deep linguistic structure are used instead of surface text pattern we demonstrate how linguistic structure can be represented for machine learning and we provide a theoretical analysis of the pattern matching approach we show the benefit of our approach by extensive experiment with our prototype system leila 
in transfer learning we aim to solve new problem using fewer example using information gained from solving related problem transfer learning ha been successful in practice and extensive pac analysis of these method ha been developed however it is not yet clear how to define relatedness between task this is considered a a major problem a it is conceptually troubling and it make it unclear how much information to transfer and when and how to transfer it in this paper we propose to measure the amount of information one task contains about another using conditional kolmogorov complexity between the task we show how existing theory neatly solves the problem of measuring relatedness and transferring the right amount of information in sequenti al transfer learning in a bayesian setting the theory also suggests that in a very formal and precise sense no other reasonable transfer method can do much better than our kolmogorov complexity theoretic transfer method and that sequential transfer is always justified we also develop a practical approximation to the method and use it to transfer information between arbitrarily chosen database from the uci ml repository 
syslog monitoring technology have recently received vast attention in the area of network management and network monitoring they are used to address a wide range of important issue including network failure symptom detection and event correlation discovery syslogs are intrinsically dynamic in the sense that they form a time series and that their behavior may change over time this paper proposes a new methodology of dynamic syslog mining in order to detect failure symptom with higher confidence and to discover sequential alarm pattern among computer device the key idea of dynamic syslog mining are to represent syslog behavior using a mixture of hidden markov model to adaptively learn the model using an on line discounting learning algorithm in combination with dynamic selection of the optimal number of mixture component and to give anomaly score using universal test statistic with a dynamically optimized threshold using real syslog data we demonstrate the validity of our methodology in the scenario of failure symptom detection emerging pattern identification and correlation discovery 
a new class of association polynomial itemsets and polynomial association rule is presented which allows for discovering nonlinear relationship between numeric attribute without discretization for binary attribute proposed association reduce to classic itemsets and association rule many standard association rule mining algorithm can be adapted to finding polynomial itemsets and association rule we applied polynomial association to add non linear term to logistic regression model significant performance improvement wa achieved over stepwise method traditionally used in statistic with comparable accuracy 
existing data stream clustering algorithm such a clustream arebased on k mean these clustering algorithm are incompetent tofind cluster of arbitrary shape and cannot handle outlier further they require the knowledge of k and user specified time window to address these issue this paper proposes d stream a framework for clustering stream data using adensity based approach the algorithm us an online component which map each input data record into a grid and an offline component which computes the grid density and cluster the grid based on the density the algorithm adopts a density decaying technique to capture the dynamic change of a data stream exploiting the intricate relationship between the decay factor data density and cluster structure our algorithm can efficiently and effectively generate and adjust the cluster in real time further a theoretically sound technique is developed to detect and remove sporadic grid mapped to by outlier in order to dramatically improve the space and time efficiency of the system the technique make high speed data stream clustering feasible without degrading the clustering quality the experimental result show that our algorithm ha superior quality and efficiency can find cluster of arbitrary shape and can accurately recognize the evolving behavior of real time data stream 
the web ha been rapidly deepened by myriad searchable database online where data are hidden behind query interface a an essential task toward integrating these massive deep web source large scale schema matching i e discovering semantic correspondence of attribute across many query interface ha been actively studied recently in particular many work have emerged to address this problem by holistically matching many schema at the same time and thus pursuing mining approach in nature however while holistic schema matching ha built it promise upon the large quantity of input schema it also suffers the robustness problem caused by noisy data quality such noise often inevitably arise in the automatic extraction of schema data which is mandatory in large scale integration for holistic matching to be viable it is thus essential to make it robust against noisy schema to tackle this challenge we propose a data ensemble framework with sampling and voting technique which is inspired by bagging predictor specifically our approach creates an ensemble of matcher by randomizing input schema data into many independently downsampled trial executing the same matcher on each trial and then aggregating their ranked result by taking majority voting a a principled basis we provide analytic justification of the effectiveness of this data ensemble framework further empirically our experiment on real web data show that the ensemblization indeed significantly boost the matching accuracy under noisy schema input and thus maintains the desired robustness of a holistic matcher 
we introduce a novel document clustering approach that overcomes those problem by combining a semantic based bipartite graph representation and a mutual refinement strategy the primary contribution of this paper are the following first we introduce a new representation of document using a bipartite graph between document and co occurrence concept in the document second we show how to enhance clustering quality by applying the mutual refinement strategy to the initial clustering result third through the experiment on medline document we show that our integrated method significantly enhances cluster quality and clustering reliability compared to existing clustering method our approach improves on the average cluster quality and clustering reliability in term of misclassification index over bisecting k mean with the best parameter 
many algorithm have been proposed for the problem of time series classification however it is clear that one nearest neighbor with dynamic time warping dtw distance is exceptionally difficult to beat this approach ha one weakness however it is computationally too demanding for many realtime application one way to mitigate this problem is to speed up the dtw calculation nonetheless there is a limit to how much this can help in this work we propose an additional technique numerosity reduction to speed up one nearest neighbor dtw while the idea of numerosity reduction for nearest neighbor classifier ha a long history we show here that we can leverage off an original observation about the relationship between dataset size and dtw constraint to produce an extremely compact dataset with little or no loss in accuracy we test our idea with a comprehensive set of experiment and show that it can efficiently produce extremely fast accurate classifier 
expense reimbursement is a time consuming and labor intensive process across organization in this paper we present a prototype expense reimbursement system that dramatically reduces the elapsed time and cost involved by eliminating paper from the process life cycle our complete solution involves an electronic submission infrastructure that provides multichannel image capture secure transport and centralized storage of paper document an unconstrained data mining approach to extracting relevant named entity from un structured document image automation of auditing procedure that enables automatic expense validation with minimum human interaction extracting relevant named entity robustly from document image with unconstrained layout and diverse formatting is a fundamental technical challenge to image based data mining question answering and other information retrieval task in many application that require such capability applying traditional language modeling technique to the stream of ocr text doe not give satisfactory result due to the absence of linguistic context we present an approach for extracting relevant named entity from document image by combining rich page layout feature in the image space with language content in the ocr text using a discriminative conditional random field crf framework we integrate this named entity extraction engine into our expense reimbursement solution and evaluate the system performance on large collection of real world receipt image provided by ibm world wide reimbursement center 
