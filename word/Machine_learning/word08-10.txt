abstract cap tchas are computer generated test that human can pas but current computer system cannot cap tchas provide a method for automatically distinguishing a human from a computer program and therefore can protect web service from abuse by so called bot most cap tchas consist of distorted image usually text for which a user must provide some description unfortunately visual cap tchas limit access to the million of visually impaired people using the web audio cap tchas were created to solve this accessibility issue however the security of audio 
massive quantity of digital data are being collected in every aspect of modern life example include personal photo and video biological and medical image and recording from sensor array to transform these massive data stream into useful information we use a sequence of winnowing stage each step reduces the size of the data by an order of magnitude extracting the wheat form the chaff in this talk i will describe this approach in a variety of context ranging from the analysis of genetic pathway in fruit fly embryo and c elegans worm to counting bird and helping elderly people living alone keep in touch with their family and caregiver 
the problem of clustering is considered for the case when each data point is a sample generated by a stationary ergodic process we propose a very natural asymptotic notion of consistency and show that simple consistent algorithm exist under most general non parametric assumption the notion of consistency is a follows two sample should be put into the same cluster if and only if they were generated by the same distribution with this notion of consistency clustering generalizes such classical statistical problem a homogeneity testing and process classification we show that for the case of a known number of cluster consistency can be achieved under the only assumption that the joint distribution of the data is stationary ergodic no parametric or markovian assumption no assumption of independence neither between nor within the sample if the number of cluster is unknown consistency can be achieved under appropriate assumption on the mixing rate of the process again no parametric or independence assumption in both case we give example of simple at most quadratic in each argument algorithm which are consistent 
we define a copula process which describes the dependency between arbitrarily many random variable independently of their marginal distribution a an example we develop a stochastic volatility model gaussian copula process volatility gcpv to predict the latent standard deviation of a sequence of random variable to make prediction we use bayesian inference with the laplace approximation and with markov chain monte carlo a an alternative we find both method comparable we also find our model can outperform garch on simulated and financial data and unlike garch gcpv can easily handle missing data incorporate covariates other than time and model a rich class of covariance structure 
human and animal learn much better when the example are not randomly presented but organized in a meaningful order which illustrates gradually more concept and gradually more complex one here we formalize such training strategy in the context of machine learning and call them curriculum learning in the context of recent research studying the difficulty of training in the presence of non convex training criterion for deep deterministic and stochastic neural network we explore curriculum learning in various set ups the experiment show that significant improvement in generalization can be achieved we hypothesize that curriculum learning ha both an effect on the speed of convergence of the training process to a minimum and in the case of non convex criterion on the quality of the local minimum obtained curriculum learning can be seen a a particular form of continuation method a general strategy for global optimization of non convex function 
hidden markov model hmms have received considerable attention in various community e g speech recognition neurology and bioinformatic since many application that use hmm have emerged the goal of this work is to identify efficiently and correctly the model in a given dataset that yield the state sequence with the highest likelihood with respect to the query sequence we propose spiral a fast search method for hmm datasets to reduce the search cost spiral efficiently prune a significant number of search candidate by applying successive approximation when estimating likelihood we perform several experiment to verify the effectiveness of spiral the result show that spiral is more than time faster than the naive method 
we propose abc boost adaptive base class boost for multi class classification and present abc mart an implementation of abc boost based on the multinomial logit model the key idea is that at each boosting iteration we adaptively and greedily choose a base class our experiment on public datasets demonstrate the improvement of abc mart over the original mart algorithm 
abstract cap tchas are computer generated test that human can pas but current computer system cannot cap tchas provide a method for automatically distinguishing a human from a computer program and therefore can protect web service from abuse by so called bot most cap tchas consist of distorted image usually text for which a user must provide some description unfortunately visual cap tchas limit access to the million of visually impaired people using the web audio cap tchas were created to solve this accessibility issue however the security of audio 
synchronization is a powerful basic concept in nature regulating a large variety of complex process ranging from the metabolism in the cell to social behavior in group of individual therefore synchronization phenomenon have been extensively studied and model robustly capturing the dynamical synchronization process have been proposed e g the extensive kuramoto model inspired by the powerful concept of synchronization we propose sync a novel approach to clustering the basic idea is to view each data object a a phase oscillator and simulate the interaction behavior of the object over time a time evolves similar object naturally synchronize together and form distinct cluster inherited from synchronization sync ha several desirable property the cluster revealed by dynamic synchronization truly reflect the intrinsic structure of the data set sync doe not rely on any distribution assumption and allows detecting cluster of arbitrary number shape and size moreover the concept of synchronization allows natural outlier handling since outlier do not synchronize with cluster object for fully automatic clustering we propose to combine sync with the minimum description length principle extensive experiment on synthetic and real world data demonstrate the effectiveness and efficiency of our approach 
phishing email usually contain a message from a credible looking source requesting a user to click a link to a website where she he is asked to enter a password or other confidential information most phishing email aim at withdrawing money from financial institution or getting access to private information phishing ha increased enormously over the last year and is a serious threat to global security and economy there are a number of possible countermeasure to phishing these range from communication oriented approach like authentication protocol over blacklisting to content based filtering approach we argue that the first two approach are currently not broadly implemented or exhibit deficit therefore content based phishing filter are necessary and widely used to increase communication security a number of feature are extracted capturing the content and structural property of the email subsequently a statistical classifier is trained using these feature on a training set of email labeled a ham legitimate spam or phishing this classifier may then be applied to an email stream to estimate the class of new incoming email antiphish is a specific targeted research project funded under framework program by the european union it is aim at developing improved anti phishing technology that help to protect and secure the global email communication infrastructure the project on the one hand developed the filter methodology in a test laboratory setting but on the other hand implemented this technology in real world setting to be used to filter all email traffic online in real time in this talk we summarize our experience with phishing filtering with benchmark data and in addition with different real life email stream first we describe a number of novel feature that are particularly well suited to identify phishing email these include statistical model for the low dimensional description of email topic sequential analysis of email text and external link the detection of embedded logo a well a indicator for hidden salting hidden salting is the intentional addition or distortion of content not perceivable by the reader for empirical evaluation we have obtained a large realistic corpus of email pre labeled a spam phishing and ham legitimate in experiment with benchmark data our method outperform other published approach for classifying phishing email the second part of the talk describes the application of these approach to real life email stream on the one hand we investigate how we can identify new phishing email arriving from a honeypot system this allows to spot new type of phishing mail subsequently the characteristic of these new phishing email can be used to update client based phishing filter a second experiment investigates the capability of the antiphish system when monitoring email in an isp framework it turn out that active learning approach are very efficient to maintain and improve filtering accuracy we discus the implication of these result for the practical application of this approach in the workflow of an email provider finally we describe a strategy how the filter may be updated and adapted to new type of phishing 
identifying the appropriate kernel function matrix for a given dataset is essential to all kernel based learning technique a number of kernel learning algorithm have been proposed to learn kernel function or matrix from side information e g either labeled example or pairwise constraint however most previous study are limited to passive kernel learning in which side information is provided beforehand in this paper we present a framework of active kernel learning akl that actively identifies the most informative pairwise constraint for kernel learning the key challenge of active kernel learning is how to measure the informativeness of an example pair given it class label is unknown to this end we propose a min max approach for active kernel learning that selects the example pair that result in a large classification margin regardless of it assigned class label we furthermore approximate the related optimization problem into a convex programming problem we evaluate the effectiveness of the proposed algorithm by comparing it to two other implementation of active kernel learning empirical study with nine datasets on semi supervised data clustering show that the proposed algorithm is more effective than it competitor 
we address the problem of evaluating the risk of a given model accurately at minimal labeling cost this problem occurs in situation in which risk estimate cannot be obtained from held out training data because the training data are unavailable or do not reect the desired test distribution we study active risk estimation process in which instance are actively selected by a sampling process from a pool of unlabeled test instance and their label are queried we derive the sampling distribution that minimizes the estimation error of the active risk estimator when used to select instance from the pool an analysis of the distribution that governs the estimator lead to condence interval we empirically study condition under which the active risk estimate is more accurate than a standard risk estimate that draw equally many instance from the test distribution 
when the transition probability and reward of a markov decision process mdp are known an agent can obtain the optimal policy without any interaction with the environment however exact transition probability are difficult for expert to specify one option left to an agent is a long and potentially costly exploration of the environment in this paper we propose another alternative given initial possibly inaccurate specification of the mdp the agent determines the sensitivity of the optimal policy to change in transition and reward it then focus it exploration on the region of space to which the optimal policy is most sensitive we show that the proposed exploration strategy performs well on several control and planning problem 
online learning algorithm have recently risen to prominence due to their strong theoretical guarantee and an increasing number of practical application for large scale data analysis problem in this paper we analyze a class of online learning algorithm based on fixed potential and nonlinearized loss which yield algorithm with implicit update rule we show how to efficiently compute these update and we prove regret bound for the algorithm we apply our formulation to several special case where our approach ha benefit over existing online learning method in particular we provide improved algorithm and bound for the online metric learning problem and show improved robustness for online linear prediction problem result over a variety of data set demonstrate the advantage of our framework 
a kernel method is proposed for realizing bayes rule based on representation of probability distribution in reproducing kernel hilbert space rkhs the empirical rkhs embeddings of the conditional probability and prior are expressed a feature mapping of sample and an rkhs embedding of the posterior distribution is computed again based on a feature mapping of a sample this kernel bayes rule can be applied to a wide variety of nonparametric bayesian inference problem a an example the approach is used in filtering with a nonparametric state space model consistency of the posterior estimator is established with respect to the rkhs embedding of the population posterior distribution 
undirected graphical model encode in a graph g the dependency structure of a random vector y in many application it is of interest to model y given another random vector x a input we refer to the problem of estimating the graph g x of y conditioned on x x a graph valued regression in this paper we propose a semiparametric method for estimating g x that build a tree on the x space just a in cart classification and regression tree but at each leaf of the tree estimate a graph we call the method graph optimized cart or go cart we study the theoretical property of go cart using dyadic partitioning tree establishing oracle inequality on risk minimization and tree partition consistency we also demonstrate the application of go cart to a meteorological dataset showing how graph valued regression can provide a useful tool for analyzing complex data 
a crucial technique for scaling kernel method to very large data set reaching or exceeding million of instance is based on low rank approximation of kernel matrix we introduce a new family of algorithm based on mixture of nystr om approximation ensemble nystr om algorithm that yield more accurate low rank approximation than the standard nystrom method we give a detailed study of variant of these algorithm based on simple averaging an exponential weight method or regression based method we also present a theoretical analysis of these algorithm including novel error bound guaranteeing a better convergence rate than the standard nystr om method finally we report result of extensive experiment with several data set containing up to m point demonstrating the significant improvement over the standard nystr om approximation 
the support vector machine svm is an acknowledged powerful tool for building classifier but it lack flexibility in the sense that the kernel is chosen prior to learning multiple kernel learning mkl enables to learn the kernel from an ensemble of basis kernel whose combination is optimized in the learning process here we propose composite kernel learning to address the situation where distinct component give rise to a group structure among kernel our formulation of the learning problem encompasses several setup putting more or le emphasis on the group structure we characterize the convexity of the learning problem and provide a general wrapper algorithm for computing solution finally we illustrate the behavior of our method on multi channel data where group correpond to channel 
research in relational data mining ha two major direction finding global model of a relational database and the discovery of local relational pattern within a database while relational pattern show how attribute value co occur in detail their huge number hamper their usage in data analysis global model on the other hand only provide a summary of how different table and their attribute relate to each other lacking detail of what is going on at the local level in this paper we introduce a new approach that combine the positive property of both direction it provides a detailed description of the complete database using a small set of pattern more in particular we utilise a rich pattern language and show how a database can be encoded by such pattern then based on the mdlprinciple the novel rdb krimp algorithm selects the set of pattern that allows for the most succinct encoding of the database this set the code table is a compact description of the database in term of local relational pattern we show that this resulting set is very small both in term of database size and in number of it local relational pattern a reduction of up to order of magnitude is attained 
sampling is a popular way of scaling up machine learning algorithm to large datasets the question often is how many sample are needed adaptive stopping algorithm monitor the performance in an online fashion and they can stop early saving valuable resource we consider problem where probabilistic guarantee are desired and demonstrate how recently introduced empirical bernstein bound can be used to design stopping rule that are efficient we provide upper bound on the sample complexity of the new rule a well a empirical result on model selection and boosting in the filtering setting 
in some stochastic environment the well known reinforcement learning algorithm q learning performs very poorly this poor performance is caused by large overestimation of action value which result from a positive bias that is introduced because q learning us the maximum action value a an approximation for the maximum expected action value we introduce an alternative way to approximate the maximum expected value for any set of random variable the obtained double estimator method is shown to sometimes underestimate rather than overestimate the maximum expected value we apply the double estimator to q learning to construct double q learning a new off policy reinforcement learning algorithm we show the new algorithm converges to the optimal policy and that it performs well in some setting in which q learning performs poorly due to it overestimation 
how do online conversation build is there a common model that human communication follows in this work we explore these question in detail we analyze the structure of conversation in three different social datasets namely usenet group yahoo group and twitter we propose a simple mathematical model for the generation of basic conversation structure and then refine this model to take into account the identity of each member of the conversation 
the k q flat algorithm is a generalization of the popular k mean algorithm where q dimensional best fit affine set replace centroid a the cluster prototype in this work a modification of the k q flat framework for pattern classification is introduced the basic idea is to replace the original reconstruction only energy which is optimized to obtain the k affine space by a new energy that incorporates discriminative term this way the actual classification task is introduced a part of the design and optimization the presentation of the proposed framework is complemented with experimental result showing that the method is computationally very efficient and give excellent result on standard supervised learning benchmark 
in this paper we consider approximate policy iteration based reinforcement learning algorithm in order to implement a flexible function approximation scheme we propose the use of non parametric method with regularization providing a convenient way to control the complexity of the function approximator we propose two novel regularized policy iteration algorithm by adding l regularization to two widely used policy evaluation method bellman residual minimization brm and least square temporal difference learning lstd we derive efficient implementation for our algorithm when the approximate value function belong to a reproducing kernel hilbert space we also provide finite sample performance bound for our algorithm and show that they are able to achieve optimal rate of convergence under the studied condition 
subgroup discovery is the task of identifying the top k pattern in a database with most significant deviation in the distribution of a target attribute y subgroup discovery is a popular approach for identifying interesting pattern in data because it combine statistical significance with an understandable representation of pattern a a logical formula however it is often a problem that some subgroup even if they are statistically highly significant are not interesting to the user we present an approach based on the work on ranking support vector machine that rank subgroup with respect to the user s concept of interestingness and find more interesting subgroup this approach can significantly increase the quality of the subgroup 
it is now well established that sparse signal model are well suited to restoration task and can effectively be learned from audio image and video data recent research ha been aimed at learning discriminative sparse model instead of purely reconstructive one this paper proposes a new step in that direction with a novel sparse representation for signal belonging to different class in term of a shared dictionary and multiple class decision function the linear variant of the proposed model admits a simple probabilistic interpretation while it most general variant admits an interpretation in term of kernel an optimization framework for learning all the component of the proposed model is presented along with experimental result on standard handwritten digit and texture classification task 
structure preserving embedding spe is an algorithm for embedding graph in euclidean space such that the embedding is low dimensional and preserve the global topological property of the input graph topology is preserved if a connectivity algorithm such a k nearest neighbor can easily recover the edge of the input graph from only the coordinate of the node after embedding spe is formulated a a semidefinite program that learns a low rank kernel matrix constrained by a set of linear inequality which capture the connectivity structure of the input graph traditional graph embedding algorithm do not preserve structure according to our definition and thus the resulting visualization can be misleading or le informative spe provides significant improvement in term of visualization and lossless compression of graph outperforming popular method such a spectral embedding and laplacian eigen map we find that many classical graph and network can be properly embedded using only a few dimension furthermore introducing structure preserving constraint into dimensionality reduction algorithm produce more accurate representation of high dimensional data 
this paper focus on a new clustering task called self taught clustering self taught clustering is an instance of unsupervised transfer learning which aim at clustering a small collection of target unlabeled data with the help of a large amount of auxiliary unlabeled data the target and auxiliary data can be different in topic distribution we show that even when the target data are not sufficient to allow effective learning of a high quality feature representation it is possible to learn the useful feature with the help of the auxiliary data on which the target data can be clustered effectively we propose a co clustering based self taught clustering algorithm to tackle this problem by clustering the target and auxiliary data simultaneously to allow the feature representation from the auxiliary data to influence the target data through a common set of feature under the new data representation clustering on the target data can be improved our experiment on image clustering show that our algorithm can greatly outperform several state of the art clustering method when utilizing irrelevant unlabeled auxiliary data 
the syntactic topic model stm is a bayesian nonparametric model of language that discovers latent distribution of word topic that are both semantically and syntactically coherent the stm model dependency parsed corpus where sentence are grouped into document it assumes that each word is drawn from a latent topic chosen by combining document level feature and the local syntactic context each document ha a distribution over latent topic a in topic model which provides the semantic consistency each element in the dependency parse tree also ha a distribution over the topic of it child a in latent state syntax model which provides the syntactic consistency these distribution are convolved so that the topic of each word is likely under both it document and syntactic context we derive a fast posterior inference algorithm based on variational method we report qualitative and quantitative study on both synthetic data and hand parsed document we show that the stm is a more predictive model of language than current model based only on syntax or only on topic 
we introduce the spherical admixture model sam a bayesian topic model over arbitrary normalized data sam model document a point on a highdimensional spherical manifold and is capable of representing negative wordtopic correlation and word presence absence unlike model with multinomial document likelihood such a lda in this paper we evaluate sam a a topic browser focusing on it ability to model negative topic feature and also a a dimensionality reduction method using topic proportion a feature for difficult classification task in natural language processing and computer vision 
we propose a deterministic method to evaluate the integral of a positive function based on soft binning function that smoothly cut the integral into smaller integral that are easier to approximate in combination with mean field approximation for each individual sub part this lead to a tractable algorithm that alternate between the optimization of the bin and the approximation of the local integral we introduce suitable choice for the binning function such that a standard mean field approximation can be extended to a split mean field approximation without the need for extra derivation the method can be seen a a revival of the idea underlying the mixture mean field approach the latter can be obtained a a special case by taking soft max function for the binning 
we present a simple and scalable graph clustering method called power iteration clustering pic pic flnds a very low dimensional embedding of a dataset using truncated power iteration on a normalized pair wise similarity matrix of the data this embedding turn out to be an efiective cluster indicator consistently outperforming widely used spectral method such a ncut on real datasets pic is very fast on large datasets running over time faster than an ncut implementation based on the state of the art iram eigenvector computation technique 
one crucial assumption made by both principal component analysis pca and probabilistic pca ppca is that the instance are independent and identically distributed i i d however this common i i d assumption is unreasonable for relational data in this paper by explicitly modeling covariance between instance a derived from the relational information we propose a novel probabilistic dimensionality reduction method called probabilistic relational pca prpca for relational data analysis although the i i d assumption is no longer adopted in prpca the learning algorithm for prpca can still be devised easily like those for ppca which make explicit use of the i i d assumption experiment on real world data set show that prpca can effectively utilize the relational information to dramatically outperform pca and achieve state of the art performance 
positive definite kernel on probability measure have been recently applied in structured data classification problem some of these kernel are related to classic information theoretic quantity such a mutual information and the jensen shannon divergence meanwhile driven by recent advance in tsallis statistic nonextensive generalization of shannon s information theory have been proposed this paper bridge these two trend we introduce the jensen tsallis q difference a generalization of the jensen shannon divergence we then define a new family of nonextensive mutual information kernel which allow weight to be assigned to their argument and which includes the boolean jensen shannon and linear kernel a particular case we illustrate the performance of these kernel on text categorization task 
dimensionality reduction is a commonly used step in many algorithm for visualization classication clustering and modeling most dimensionality reduction algorithm nd a low dimensional embedding that preserve the structure of high dimensional data point this paper proposes local minimum embedding lme a technique to nd a lowdimensional embedding that preserve the local minimum structure of a given objective function lme provides an embedding that is useful for visualizing and understanding the relation between the original variable that create local minimum additionally the embedding can potentially be used to sample the original function to discover new local minimum the function in the embedded space take an analytic form and hence the gradient can be computed analytically we illustrate the benet of lme in both syn 
many real world datasets can be clustered along multiple dimension for example text document can be clustered not only by topic but also by the author s gender or sentiment unfortunately traditional clustering algorithm produce only a single clustering of a dataset effectively providing a user with just a single view of the data in this paper we propose a new clustering algorithm that can discover in an unsupervised manner each clustering dimension along which a dataset can be meaningfully clustered it ability to reveal the important clustering dimension of a dataset in an unsupervised manner is particularly appealing for those user who have no idea of how a dataset can possibly be clustered we demonstrate it viability on several challenging text classification task 
this paper introduces a novel machine learning model called multiple instance ranking mirank that enables ranking to be performed in a multiple instance learning setting the motivation for mirank stem from the hydrogen abstraction problem in computational chemistry that of predicting the group of hydrogen atom from which a hydrogen is abstracted removed during metabolism the model predicts the preferred hydrogen group within a molecule by ranking the group with the ambiguity of not knowing which hydrogen atom within the preferred group is actually abstracted this paper formulates mirank in it general context and proposes an algorithm for solving mirank problem using successive linear programming the method outperforms multiple instance classification model on several real and synthetic datasets 
we introduce the problem of query decomposition where we are given a query and a document retrieval system and we want to produce a small set of query whose union of resulting document corresponds approximately to that of the original query ideally these query should represent coherent conceptually well separated topic we provide an abstract formulation of the query decomposition problem and we tackle it from two different perspective we first show how the problem can be instantiated a a specific variant of a set cover problem for which we provide an efficient greedy algorithm next we show how the same problem can be seen a a constrained clustering problem with a very particular kind of constraint i e clustering with predefined cluster we develop a two phase algorithm based on hierarchical agglomerative clustering followed by dynamic programming our experiment conducted on a set of actual query in a web scale search engine confirm the effectiveness of the proposed solution 
current graph kernel suffer from two limitation graph kernel based on counting particular type of subgraphs ignore the relative position of these subgraphs to each other while graph kernel based on algebraic method are limited to graph without node label in this paper we present the graphlet spectrum a system of graph invariant derived by mean of group representation theory that capture information about the number a well a the position of labeled subgraphs in a given graph in our experimental evaluation the graphlet spectrum outperforms state of the art graph kernel 
we consider regularized stochastic learning and online optimization problem where the objective function is the sum of two convex term one is the loss function of the learning task and the other is a simple regularization term traditional online algorithm such a stochastic gradient descent ha limited capability of exploiting the problem structure and their inherent low accuracy often make it hard to obtain the desired regularization effect e g sparsity under regularization in this paper we develop extension of nesterov s dual averaging method that can explicitly exploit the regularization structure in an online setting for general convex regularization we show that the regularized dual averaging method achieves the optimal convergence rate o p t where t is the number of iteration or sample in an online algorithm for strongly convex regularization we develop a variant that ha a faster convergence rate o lnt t computational experiment are presented for the special case of regularized online learning 
in large scale online system like search ecommerce or social network application user query represent an important dimension of activity that can be used to study the impact on the system and even the business in this paper we describe how to detect characterize and classify burst in user query in a large scale ecommerce system we build upon the approach discussed in kdd bursty and hierarchical structure in stream and apply them to a high volume industrial context we describe how to identify burst on a near real time basis classify them and apply them to build interesting merchandizing application 
this paper describes and evaluates privacy friendly method for extracting quasi social network from browser behavior on user generated content site for the purpose of finding good audience for brand advertising a opposed to click maximizing for example targeting social network neighbor resonates well with advertiser and on line browsing behavior data counterintuitively can allow the identification of good audience anonymously besides being one of the first paper to our knowledge on data mining for on line brand advertising this paper make several important contribution we introduce a framework for evaluating brand audience in analogy to predictive modeling holdout evaluation we introduce method for extracting quasi social network from data on visitation to social networking page without collecting any information on the identity of the browser or the content of the social network page we introduce measure of brand proximity in the network and show that audience with high brand proximity indeed show substantially higher brand affinity finally we provide evidence that the quasi social network embeds a true social network which along with result from social theory offer one explanation for the increase in brand affinity of the selected audience 
peculiarity oriented mining pom aiming to discover peculiarity rule hidden in a dataset is a new data mining method in the past few year many result and application on pom have been reported however there is still a lack of theoretical analysis in this paper we prove that the peculiarity factor pf one of the most important concept in pom can accurately characterize the peculiarity of data with respect to the probability density function of a normal distribution but is unsuitable for more general distribution thus we propose the concept of local peculiarity factor lpf it is proved that the lpf ha the same ability a the pf for a normal distribution and is the so called sensitive peculiarity description for general distribution to demonstrate the effectiveness of the lpf we apply it to outlier detection problem and give a new outlier detection algorithm called lpf outlier experimental result show that lpf outlier is an effective outlier detection algorithm 
abstract we study the problem of domain transfer for a supervised classification task in mrna splicing we consider a number of recent domain transfer method from machine learning including some that are novel and evaluate them on genomic sequence data from model organism of varying evolutionary distance we find that in case where the organism are not closely related the use of domain adaptation method can help improve classification performance 
recent advance in multiple kernel learning mkl have positioned it a an attractive tool for tackling many supervised learning task the development of efficient gradient descent based optimization scheme ha made it possible to tackle large scale problem simultaneously mkl based algorithm have achieved very good result on challenging real world application yet despite their success mkl approach are limited in that they focus on learning a linear combination of given base kernel in this paper we observe that existing mkl formulation can be extended to learn general kernel combination subject to general regularization this can be achieved while retaining all the efficiency of existing large scale optimization algorithm to highlight the advantage of generalized kernel learning we tackle feature selection problem on benchmark vision and uci database it is demonstrated that the proposed formulation can lead to better result not only a compared to traditional mkl but also a compared to state of the art wrapper and filter method for feature selection 
conditional random field crfs are an effective tool for a variety of different data segmentation and labeling task including visual scene interpretation which seek to partition image into their constituent semantic level region and assign appropriate class label to each region for accurate labeling it is important to capture the global context of the image a well a local information we introduce a crf based scene labeling model that incorporates both local feature and feature aggregated over the whole image or large section of it secondly traditional crf learning requires fully labeled datasets which can be costly and troublesome to produce we introduce a method for learning crfs from datasets with many unlabeled node by marginalizing out the unknown label so that the log likelihood of the known one can be maximized by gradient ascent loopy belief propagation is used to approximate the marginals needed for the gradient and log likelihood calculation and the bethe free energy approximation to the log likelihood is monitored to control the step size our experimental result show that effective model can be learned from fragmentary labelings and that incorporating top down aggregate feature significantly improves the segmentation the resulting segmentation are compared to the state of the art on three different image datasets 
we propose a family of supervised dimensionality reduction sdr algorithm that combine feature extraction dimensionality reduction with learning a predictive model in a unified optimization framework using dataand class appropriate generalized linear model glms and handling both classification and regression problem our approach us simple closed form update rule and is provably convergent promising empirical result are demonstrated on a variety of high dimensional datasets 
in this paper we investigate two aspect of ranking problem on large graph first we augment the deterministic pruning algorithm in sarkar and moore with sampling technique to compute approximately correct ranking with high probability under random walk based proximity measure at query time second we prove some surprising locality property of these proximity measure by examining the short term behavior of random walk the proposed algorithm can answer query on the fly without caching any information about the entire graph we present empirical result on a node author word citation graph from the citeseer domain on a single cpu machine where the average query processing time is around second we present quantifiable link prediction task on most of them our technique outperform personalized pagerank a well known diffusion based proximity measure 
we address the task of actively learning a segmentation system given a large number of unsegmented image and access to an oracle that can segment a given image decide which image to provide to quickly produce a segmenter here a discriminative random field that is accurate over this distribution of image we extend the standard model for active learner to define a system for this task that first selects the image whose expected label will reduce the uncertainty of the other unlabeled image the most and then after greedily selects from the pool of unsegmented image the most informative image the result of our experiment over two real world datasets segmenting brain tumor within magnetic resonance image and segmenting the sky in real image show that training on very few informative image here a few a can produce a segmenter that is a good a training on the entire dataset 
detecting inference in document is critical for ensuring privacy when sharing information in this paper we propose a refined and practical model of inference detection using a reference corpus our model is inspired by association rule mining inference are based on word co occurrence using the model and taking the web a the reference corpus we can find inference and measure their strength through web mining algorithm that leverage search engine such a google or yahoo our model also includes the important case of private corpus to model inference detection in enterprise setting in which there is a large private document repository we find inference in private corpus by using analogue of our web mining algorithm relying on an index for the corpus rather than a web search engine we present result from two experiment the first experiment demonstrates the performance of our technique in identifying all the keywords that allow for inference of a particular topic e g hiv with confidence above a certain threshold the second experiment us the public enron e mail dataset we postulate a sensitive topic and use the enron corpus and the web together to find inference for the topic these experiment demonstrate that our technique are practical and that our model of inference based on word co occurrence is well suited to efficient inference detection 
the synchronous brain activity measured via meg or eeg can be interpreted a arising from a collection possibly large of current dipole or source located throughout the cortex estimating the number location and orientation of these source remains a challenging task one that is signicantly compounded by the effect of source correlation and the presence of interference from spontaneous brain activity sensor noise and other artifact this paper derives an empirical bayesian method for addressing each of these issue in a principled fashion the resulting algorithm guarantee descent of a cost function uniquely designed to handle unknown orientation and arbitrary correlation robust interference suppression is also easily incorporated in a restricted setting the proposed method is shown to have theoretically zero bias estimating both the location and orientation of multi component dipole even in the presence of correlation unlike a variety of existing bayesian localization method or common signal processing technique such a beamforming and sloreta empirical result on both simulated and real data set verify the efcac y of this approach 
the conditional restricted boltzmann machine crbm is a recently proposed model for time series that ha a rich distributed hidden state and permit simple exact inference we present a new model based on the crbm that preserve it most important computational property and includes multiplicative three way interaction that allow the effective interaction weight between two unit to be modulated by the dynamic state of a third unit we factor the three way weight tensor implied by the multiplicative model reducing the number of parameter from o n to o n the result is an efficient compact model whose effectiveness we demonstrate by modeling human motion like the crbm our model can capture diverse style of motion with a single set of parameter and the three way interaction greatly improve the model s ability to blend motion style or to transition smoothly among them 
polysemy is a problem for method that exploit image search engine to build object category model existing unsupervised approach do not take word sense into consideration we propose a new method that us a dictionary to learn model of visual word sense from a large collection of unlabeled web data the use of lda to discover a latent sense space make the model robust despite the very limited nature of dictionary definition the definition ar e used to learn a distribution in the latent space that best represents a sense the a lgorithm then us the text surrounding image link to retrieve image with high probability of a particular dictionary sense an object classifier is trained on the r esulting sense specific image we evaluate our method on a dataset obtained by searching the web for polysemous word category classification experiment sho w that our dictionarybased approach outperforms baseline method 
error in map making task using computer vision are sparse we demonstrate this by considering the construction of digital elevation model that employ stereo matching algorithm to triangulate real world point this sparsity coupled with a geometric theory of error recently developed by the author allows for autonomous agent to calculate their own precision independently of ground truth we connect these development with recent advance in the mathematics of sparse signal reconstruction or compressed sensing the theory presented here extends the autonomy of d model reconstruction discovered in the s to their error 
a recent surge in research in kernelized approach to reinforcement learning ha sought to bring the benefit of kernelized machine learning technique to reinforcement learning kernelized reinforcement learning technique are fairly new and different author have approached the topic with different assumption and goal neither a unifying view nor an understanding of the pro and con of different approach ha yet emerged in this paper we offer a unifying view of the different approach to kernelized value function approximation for reinforcement learning we show that except for different approach to regularization kernelized lstd klstd is equivalent to a modelbased approach that us kernelized regression to find an approximate reward and transition model and that gaussian process temporal difference learning gptd return a mean value function that is equivalent to these other approach we also discus the relationship between our modelbased approach and the earlier gaussian process in reinforcement learning gprl finally we decompose the bellman error into the sum of transition error and reward error term and demonstrate through experiment that this decomposition can be helpful in choosing regularization parameter 
mining discrete pattern in binary data is important for subsampling compression and clustering we consider rankone binary matrix approximation that identify the dominant pattern of the data while preserving it discrete property a best approximation on such data ha a minimum set of inconsistent entry i e mismatch between the given binary data and the approximate matrix due to the hardness of the problem previous account of such problem employ heuristic and the resulting approximation may be far away from the optimal one in this paper we show that the rank one binary matrix approximation can be reformulated a a integer linear program ilp however the ilp formulation is computationally expensive even for small size matrix we propose a linear program lp relaxation which is shown to achieve a guaranteed approximation error bound we further extend the proposed formulation using the regularization technique which is commonly employed to address overfltting the lp formulation is restricted to medium size matrix due to the large number of variable involved for large matrix interestingly we show that the proposed approximate formulation can be transformed into an instance of the minimum s t cut problem which can be solved e ciently by flnding maximum ows our empirical study show the e ciency of the proposed algorithm based on the maximum ow result also conflrm the established theoretical bound 
the general stochastic optimal control soc problem in robotics scenario is often too complex to be solved exactly and in near real time a classical approximate solution is to first compute an optimal deterministic trajectory and then solve a local linear quadratic gaussian lqg perturbation model to handle the system stochasticity we present a new algorithm for this approach which improves upon previous algorithm like ilqg we consider a probabilistic model for which the maximum likelihood ml trajectory coincides with the optimal trajectory and which in the lqg case reproduces the classical soc solution the algorithm then utilizes approximate inference method similar to expectation propagation that efficiently generalize to non lqg system we demonstrate the algorithm on a simulated dof humanoid robot 
in realistic setting the prevalence of a class may change after a classifier is induced and this will degrade the performance of the classifier further complicating this scenario is the fact that labeled data is often scarce and expensive in this paper we address the problem where the class distribution change and only unlabeled example are available from the new distribution we design and evaluate a number of method for coping with this problem and compare the performance of these method our quantification based method estimate the class distribution of the unlabeled data from the changed distribution and adjust the original classifier accordingly while our semi supervised method build a new classifier using the example from the new unlabeled distribution which are supplemented with predicted class value we also introduce a hybrid method that utilizes both quantification and semi supervised learning all method are evaluated using accuracy and f measure on a set of benchmark data set our result demonstrate that our method yield substantial improvement in accuracy and f measure 
graph or network can be used to model complex system detecting community structure from large network data is a classic and challenging task in this paper we propose a novel community detection algorithm which utilizes a dynamic process by contradicting the network topology and the topology based propinquity where the propinquity is a measure of the probability for a pair of node involved in a coherent community structure through several round of mutual reinforcement between topology and propinquity the community structure are expected to naturally emerge the overlapping vertex shared between community can also be easily identified by an additional simple postprocessing to achieve better efficiency the propinquity is incrementally calculated we implement the algorithm on a vertex oriented bulk synchronous parallel bsp model so that the mining load can be distributed on thousand of machine we obtained interesting experimental result on several real network data 
a recent trend in exemplar based unsupervised learning is to formulate the learning problem a a convex optimization problem convexity is achieved by restricting the set of possible prototype to training exemplar in particular this ha been done for clustering vector quantization and mixture model density estimation in this paper we propose a novel algorithm that is theoretically and practically superior to these convex formulation this is possible by posing the unsupervised learning problem a a single convex master problem with non convex subproblems we show that for the above learning task the subproblems are extremely well behaved and can be solved efficiently 
we consider the problem of estimating the graph structure associated with a gaussian markov random field gmrf from i i d sample we study the p erformance of study the performance of the regularized maximum likelihood estimator in the high dimensional setting where the number of node in the graph p the number of edge in the graph s and the maximum node degree d are allowed to grow a a function of the number of sample n our main result provides sufficient condition on n p d for the regularized mle estimator to recover all the edge of the graph with high probability under some condition on the model covariance we show that model selection can be achieved for sample size n d log p with the error decaying a o exp clog p for some constant c we illustrate our theoretical result via simulation and sho w good correspondence between the theoretical prediction and behavior in simulation the area of high dimensional statistic deal with estimat ion in the large p small n setting where p and n correspond respectively to the dimensionality of the dat a and the sample size such highdimensional problem arise in a variety of application among them remote sensing computational biology and natural language processing where the model dimension may be comparable or substantially larger than the sample size it is well known tha t such high dimensional scaling can lead to dramatic breakdown in many classical procedure in the absence of additional model assumption it is frequently impossible to obtain consistent procedure s when p n accordingly an active line of statistical research is based on imposing various restri ctions on the model for instance sparsity manifold structure or graphical model structure and then studying the scaling behavior of different estimator a a function of sample size n ambient dimension p and additional parameter related to these structural assumption in this paper we study the problem of estimating the graph structure of a gauss markov random field gmrf in the high dimensional setting this graphical model selection problem can be reduced to the problem of estimating the zero pattern of the inverse covariance or concentration matrix a line of recent work ha studied estimator based on minimizing gaussian log likelihood penalized by the norm of the entry or the off diagonal entry of the conc entration matrix the resulting optimization problem is a log determinant program which can be solved in polynomial time with interior point method or by faster co ordina te descent algorithm in recent work rothman et al have analyzed some aspect of high dimensional behavior in particular establishing consistency in frobenius norm under certain condition on the model covariance and under certain scaling of the sparsity sample size and ambient model dimension 
accurate and efficient inference in evolutionary tree is a central problem in computational biology while classical treatment have made unrealistic site independence assumption ignoring insertion and deletion realistic approach require tracking insertion and deletion along the phylogenetic tree a challenging and unsolved computational problem we propose a new ancestry resampling procedure for inference in evolutionary tree we evaluate our method in two problem domain multiple sequence alignment and reconstruction of ancestral sequence and show substantial improvement over the current state of the art 
a common task in many text mining application is to generate a multi faceted overview of a topic in a text collection such an overview not only directly serf a an informative summary of the topic but also provides a detailed view of navigation to different facet of the topic existing work ha cast this problem a a categorization problem and requires training example for each facet this ha three limitation all facet are predefined which may not fit the need of a particular user training example for each facet are often unavailable such an approach only work for a predefined type of topic in this paper we break these limitation and study a more realistic new setup of the problem in which we would allow a user to flexibly describe each facet with keywords for an arbitrary topic and attempt to mine a multi faceted overview in an unsupervised way we attempt a probabilistic approach to solve this problem empirical experiment on different genre of text data show that our approach can effectively generate a multi faceted overview for arbitrary topic the generated overview are comparable with those generated by supervised method with training example they are also more informative than unstructured flat summary the method is quite general thus can be applied to multiple text mining task in different application domain 
nasa ha some of the largest and most complex data source in the world with data source ranging from the earth science space science and massive distributed engineering data set from commercial aircraft and spacecraft this talk will discus some of the issue and algorithm developed to analyze and discover pattern in these data set we will also provide an overview of a large research program in integrated vehicle health management the goal of this program is to develop advanced technology to automatically detect diagnose predict and mitigate adverse event during the flight of an aircraft a case study will be presented on a recent data mining analysis performed to support the flight readiness review of the space shuttle mission sts 
how doe one extract unknown but stereotypical event that are linearly superimposed within a signal with variable latency and variable amplitude one could think of using template matching or matching pursuit to flnd the arbitrarily shifted linear component however traditional matching approach require that the template be known a priori to overcome this restriction we use instead semi non negative matrix factorization seminmf that we extend to allow for time shift when matching the template to the signal the algorithm estimate template directly from the data along with their non negative amplitude the resulting method can be thought of a an adaptive template matching procedure we demonstrate the procedure on the task of extracting spike from single channel extracellular recording on these data the algorithm essentially performs spike detection and unsupervised spike clustering result on simulated data and extracellular recording indicate that the method performs well for signalto noise ratio of db or higher and that spike template are recovered accurately provided they are su ciently difierent 
in the absence of explicit query an alternative is to try to infer user interest from implicit feedback signal such a clickstreams or eye tracking the interest formulated a an implicit query can then be used in further search we formulate this task a a probabilistic model which can be interpreted a a kind of transfer or meta learning the probabilistic model is demonstrated to outperform an earlier kernel based method in a small scale information retrieval task 
recent work in deduplication ha shown that collective deduplication of different attribute type can improve performance but although these technique cluster the attribute collectively they do not model them collectively for example in citation in the research literature canonical venue string and title string are dependent because venue tend to focus on a few research area but this dependence is not modeled by current unsupervised technique we call this dependence between field in a record a cross field dependence in this paper we present an unsupervised generative model for the deduplication problem that explicitly model cross field dependence our model us a single set of latent variable to control two disparate clustering model a dirichlet multinomial model over title and a non exchangeable string edit model over venue we show that modeling cross field dependence yield a substantial improvement in performance a reduction in error over a standard dirichlet process mixture 
in this article we report our effort in mining the information encoded a clickthrough data in the server log to evaluate and monitor the relevance ranking quality of a commercial web search engine we describe a metric called pskip that aim to quantify the ranking quality by estimating the probability of user encountering non relevant result that cost them the effort to read and skip a search engine with a lower pskip is regarded a having a better ranking quality a key design goal of pskip is to integrate the finding from two set of user study that utilize eye tracking device to track user browsing pattern on the search result page and that use specially instrumented browser to actively solicit user explicit judgment on their search activity we present the derivation of the maximum likelihood estimation of pskip and demonstrate it efficacy in describing the user study data the mathematical property of pskip are further analyzed and compared with several objective metric a well a the cumulated gain method that us subjective judgment experimental data show that pskip can measure aspect of the search quality that these existing metric are not designed or fail to address such a identifying the real search intent expressed in the ambiguous query although effective and superior in many way we also report a series of experiment that show pskip may be influenced by system issue that are not directly related to relevance ranking suggesting that measurement complementary to pskip are still needed in order to form a holistic and accurate characterization of the ranking quality 
influence maximization is the problem of finding a small subset of node seed node in a social network that could maximize the spread of influence in this paper we study the efficient influence maximization from two complementary direction one is to improve the original greedy algorithm of and it improvement to further reduce it running time and the second is to propose new degree discount heuristic that improves influence spread we evaluate our algorithm by experiment on two large academic collaboration graph obtained from the online archival database arxiv org our experimental result show that a our improved greedy algorithm achieves better running time comparing with the improvement of with matching influence spread b our degree discount heuristic achieve much better influence spread than classic degree and centrality based heuristic and when tuned for a specific influence cascade model it achieves almost matching influence thread with the greedy algorithm and more importantly c the degree discount heuristic run only in millisecond while even the improved greedy algorithm run in hour in our experiment graph with a few ten of thousand of node based on our result we believe that fine tuned heuristic may provide truly scalable solution to the influence maximization problem with satisfying influence spread and blazingly fast running time therefore contrary to what implied by the conclusion of that traditional heuristic are outperformed by the greedy approximation algorithm our result shed new light on the research of heuristic algorithm 
we cast the ranking problem a multiple classification multiple ordinal classification which lead to computationally tractable learning algorithm for relevance ranking in web search we consider the dcg criterion discounted cumulative gain a standard quality measure in information retrieval our approach is motivated by the fact that perfect classification naturally result in perfect dcg score and the dcg error are bounded by classification error we propose using the expected relevance to convert the class probability into ranking score the class probability are learned using a gradient boosting tree algorithm evaluation on large scale datasets show that our approach can improve lambdarank and the regression based ranker in term of the normalized dcg score the general ranking problem ha widespread application including commercial search engine and recommender system in this study we develop a computationally tractable learning algorithm for the general ranking problem and we present our approach in the context of ranking in web search 
in one class classification we seek a rule to find a coherent subset of instance similar to a few positive example in a large pool of instance the problem can be formulated and analyzed naturally in a rate distortion framework leading to an efficient algorithm that compare well with two previous one class method the model can be also be extended to remove background clutter in clustering to improve cluster purity 
recently many application for restricted boltzmann machine rbms have been developed for a large variety of learning problem however rbms are usually used a feature extractor for another learning algorithm or to provide a good initialization for deep feed forward neural network classifier and are not considered a a standalone solution to classification problem in this paper we argue that rbms provide a self contained framework for deriving competitive non linear classifier we present an evaluation of different learning algorithm for rbms which aim at introducing a discriminative component to rbm training and improve their performance a classifier this approach is simple in that rbms are used directly to build a classifier rather than a a stepping stone finally we demonstrate how discriminative rbms can also be successfully employed in a semi supervised setting 
in this paper we study the problem of local triangle counting in large graph namely given a large graph g v e we want to estimate a accurately a possible the number of triangle incident to every node v in the graph the problem of computing the global number of triangle in a graph ha been considered before but to our knowledge this is the first paper that address the problem of local triangle counting with a focus on the efficiency issue arising in massive graph the distribution of the local number of triangle and the related local clustering coefficient can be used in many interesting application for example we show that the measure we compute can help to detect the presence of spamming activity in large scale web graph a well a to provide useful feature to ass content quality in social network for computing the local number of triangle we propose two approximation algorithm which are based on the idea of min wise independent permutation broder et al our algorithm operate in a semi streaming fashion using o jv j space in main memory and performing o log jv j sequential scan over the edge of the graph the first algorithm we describe in this paper also us o jej space in external memory during computation while the second algorithm us only main memory we present the theoretical analysis a well a experimental result in massive graph demonstrating the practical efficiency of our approach 
semi supervised learning ssl is classification where additional unlabeled data can be used to improve accuracy generative approach are appealing in this situation a a model of the data s probability density can assist in identifying cluster nonparametric bayesian method while ideal in theory due to their principled motivation have been difficult to apply to ssl in practice we present a nonparametric bayesian method that us gaussian process for the generative model avoiding many of the problem associated with dirichlet process mixture model our model is fully generative and we take advantage of recent advance in markov chain monte carlo algorithm to provide a practical inference method our method compare favorably to competing approach on synthetic and real world multi class data 
it is commonly agreed that account receivable ar can be a source of financial difficulty for firm when they are not efficiently managed and are underperforming experience across multiple industry show that effective management of ar and overall financial performance of firm are positively correlated in this paper we address the problem of reducing outstanding receivables through improvement in the collection strategy specifically we demonstrate how supervised learning can be used to build model for predicting the payment outcome of newly created invoice thus enabling customized collection action tailored for each invoice or customer our model can predict with high accuracy if an invoice will be paid on time or not and can provide estimate of the magnitude of the delay we illustrate our technique in the context of real world transaction data from multiple firm finally simulation result show that our approach can reduce collection time up to a factor of four compared to a baseline that is not model driven 
we introduce a new reinforcement learning model for the role of the hippocampus in classical conditioning focusing on the difference between trace and delay conditioning in the model all stimulus are represented both a unindividuated whole and a a series of temporal element with varying delay these two stimulus representation interact producing different pattern of learning in trace and delay conditioning the model proposes that hippocampal lesion eliminate long latency temporal element but preserve short latency temporal element for trace conditioning with no contiguity between cue and reward these long latency temporal element are necessary for learning adaptively timed response for delay conditioning the continued presence of the cue support conditioned responding and the short latency element suppress responding early in the cue in accord with the empirical data simulated hippocampal damage impairs trace conditioning but not delay conditioning at medium length interval with longer interval learning is impaired in both procedure and with shorter interval in neither in addition the model make novel prediction about the response topography with extended cue or post training lesion these result demonstrate how temporal contiguity a in delay conditioning change the timing problem faced by animal rendering it both easier and le susceptible to disruption by hippocampal lesion the hippocampus is an important structure in many type of learning and memory with prominent involvement in spatial navigation episodic and working memory stimulus configuration and contextual conditioning one empirical phenomenon that ha eluded many theory of the hippocampus is the dependence of aversive trace conditioning on an intact hippocampus but see rodriguez levy schmajuk dicarlo yamazaki tanaka for example trace eyeblink conditioning disappears following hippocampal lesion solomon et al moyer jr et al induces hippocampal neurogenesis gould et al and produce unique activity pattern in hippocampal neuron mcechron disterhoft in this paper we present a new abstract computational model of hippocampal function during trace conditioning we build on a recent extension of the temporal difference td model of conditioning ludvig sutton kehoe sutton barto to demonstrate how the detail of stimulus representation can qualitatively alter learning during trace and delay conditioning by gently tweaking this stimulus representation and reducing long latency temporal element trace conditioning is severely impaired whereas delay conditioning is hardly affected in the model the hippocampus is responsible for maintaining these long latency element thus explaining the selective importance of this brain structure in trace conditioning 
attribution of climate change to causal factor ha been based predominantly on simulation using physical climate model which have inherent limitation in describing such a complex and chaotic system we propose an alternative data centric approach that relies on actual measurement of climate observation and human and natural forcing factor specifically we develop a novel method to infer causality from spatial temporal data a well a a procedure to incorporate extreme value modeling into our method in order to address the attribution of extreme climate event such a heatwaves our experimental result on a real world dataset indicate that change in temperature are not solely accounted for by solar radiance but attributed more significantly to co and other greenhouse gas combined with extreme value modeling we also show that there ha been a significant increase in the intensity of extreme temperature and that such change in extreme temperature are also attributable to greenhouse gas these preliminary result suggest that our approach can offer a useful alternative to the simulation based approach to climate modeling and attribution and provide valuable insight from a fresh perspective 
we show that an important and computationally challenging solution space feature of the graph coloring problem col namely the number of cluster of solution can be accurately estimated by a technique very similar to one for counting the number of solution this cluster counting approach can be naturally written in term of a new factor graph derived from the factor graph representing the col instance using a variant of the belief propagation inference framework we can efciently approximate cluster count in random col problem over a large range of graph density we illustrate the algorithm on instance with up to vertex moreover we supply a methodology for computing the number of cluster exactly using advanced technique from the knowledge compilation literature this methodology scale up to several hundred variable 
massive quantity of digital data are being collected in every aspect of modern life example include personal photo and video biological and medical image and recording from sensor array to transform these massive data stream into useful information we use a sequence of winnowing stage each step reduces the size of the data by an order of magnitude extracting the wheat form the chaff in this talk i will describe this approach in a variety of context ranging from the analysis of genetic pathway in fruit fly embryo and c elegans worm to counting bird and helping elderly people living alone keep in touch with their family and caregiver 
the problem of clustering is considered for the case when each data point is a sample generated by a stationary ergodic process we propose a very natural asymptotic notion of consistency and show that simple consistent algorithm exist under most general non parametric assumption the notion of consistency is a follows two sample should be put into the same cluster if and only if they were generated by the same distribution with this notion of consistency clustering generalizes such classical statistical problem a homogeneity testing and process classification we show that for the case of a known number of cluster consistency can be achieved under the only assumption that the joint distribution of the data is stationary ergodic no parametric or markovian assumption no assumption of independence neither between nor within the sample if the number of cluster is unknown consistency can be achieved under appropriate assumption on the mixing rate of the process again no parametric or independence assumption in both case we give example of simple at most quadratic in each argument algorithm which are consistent 
we define a copula process which describes the dependency between arbitrarily many random variable independently of their marginal distribution a an example we develop a stochastic volatility model gaussian copula process volatility gcpv to predict the latent standard deviation of a sequence of random variable to make prediction we use bayesian inference with the laplace approximation and with markov chain monte carlo a an alternative we find both method comparable we also find our model can outperform garch on simulated and financial data and unlike garch gcpv can easily handle missing data incorporate covariates other than time and model a rich class of covariance structure 
human and animal learn much better when the example are not randomly presented but organized in a meaningful order which illustrates gradually more concept and gradually more complex one here we formalize such training strategy in the context of machine learning and call them curriculum learning in the context of recent research studying the difficulty of training in the presence of non convex training criterion for deep deterministic and stochastic neural network we explore curriculum learning in various set ups the experiment show that significant improvement in generalization can be achieved we hypothesize that curriculum learning ha both an effect on the speed of convergence of the training process to a minimum and in the case of non convex criterion on the quality of the local minimum obtained curriculum learning can be seen a a particular form of continuation method a general strategy for global optimization of non convex function 
hidden markov model hmms have received considerable attention in various community e g speech recognition neurology and bioinformatic since many application that use hmm have emerged the goal of this work is to identify efficiently and correctly the model in a given dataset that yield the state sequence with the highest likelihood with respect to the query sequence we propose spiral a fast search method for hmm datasets to reduce the search cost spiral efficiently prune a significant number of search candidate by applying successive approximation when estimating likelihood we perform several experiment to verify the effectiveness of spiral the result show that spiral is more than time faster than the naive method 
we propose abc boost adaptive base class boost for multi class classification and present abc mart an implementation of abc boost based on the multinomial logit model the key idea is that at each boosting iteration we adaptively and greedily choose a base class our experiment on public datasets demonstrate the improvement of abc mart over the original mart algorithm 
abstract cap tchas are computer generated test that human can pas but current computer system cannot cap tchas provide a method for automatically distinguishing a human from a computer program and therefore can protect web service from abuse by so called bot most cap tchas consist of distorted image usually text for which a user must provide some description unfortunately visual cap tchas limit access to the million of visually impaired people using the web audio cap tchas were created to solve this accessibility issue however the security of audio 
synchronization is a powerful basic concept in nature regulating a large variety of complex process ranging from the metabolism in the cell to social behavior in group of individual therefore synchronization phenomenon have been extensively studied and model robustly capturing the dynamical synchronization process have been proposed e g the extensive kuramoto model inspired by the powerful concept of synchronization we propose sync a novel approach to clustering the basic idea is to view each data object a a phase oscillator and simulate the interaction behavior of the object over time a time evolves similar object naturally synchronize together and form distinct cluster inherited from synchronization sync ha several desirable property the cluster revealed by dynamic synchronization truly reflect the intrinsic structure of the data set sync doe not rely on any distribution assumption and allows detecting cluster of arbitrary number shape and size moreover the concept of synchronization allows natural outlier handling since outlier do not synchronize with cluster object for fully automatic clustering we propose to combine sync with the minimum description length principle extensive experiment on synthetic and real world data demonstrate the effectiveness and efficiency of our approach 
phishing email usually contain a message from a credible looking source requesting a user to click a link to a website where she he is asked to enter a password or other confidential information most phishing email aim at withdrawing money from financial institution or getting access to private information phishing ha increased enormously over the last year and is a serious threat to global security and economy there are a number of possible countermeasure to phishing these range from communication oriented approach like authentication protocol over blacklisting to content based filtering approach we argue that the first two approach are currently not broadly implemented or exhibit deficit therefore content based phishing filter are necessary and widely used to increase communication security a number of feature are extracted capturing the content and structural property of the email subsequently a statistical classifier is trained using these feature on a training set of email labeled a ham legitimate spam or phishing this classifier may then be applied to an email stream to estimate the class of new incoming email antiphish is a specific targeted research project funded under framework program by the european union it is aim at developing improved anti phishing technology that help to protect and secure the global email communication infrastructure the project on the one hand developed the filter methodology in a test laboratory setting but on the other hand implemented this technology in real world setting to be used to filter all email traffic online in real time in this talk we summarize our experience with phishing filtering with benchmark data and in addition with different real life email stream first we describe a number of novel feature that are particularly well suited to identify phishing email these include statistical model for the low dimensional description of email topic sequential analysis of email text and external link the detection of embedded logo a well a indicator for hidden salting hidden salting is the intentional addition or distortion of content not perceivable by the reader for empirical evaluation we have obtained a large realistic corpus of email pre labeled a spam phishing and ham legitimate in experiment with benchmark data our method outperform other published approach for classifying phishing email the second part of the talk describes the application of these approach to real life email stream on the one hand we investigate how we can identify new phishing email arriving from a honeypot system this allows to spot new type of phishing mail subsequently the characteristic of these new phishing email can be used to update client based phishing filter a second experiment investigates the capability of the antiphish system when monitoring email in an isp framework it turn out that active learning approach are very efficient to maintain and improve filtering accuracy we discus the implication of these result for the practical application of this approach in the workflow of an email provider finally we describe a strategy how the filter may be updated and adapted to new type of phishing 
identifying the appropriate kernel function matrix for a given dataset is essential to all kernel based learning technique a number of kernel learning algorithm have been proposed to learn kernel function or matrix from side information e g either labeled example or pairwise constraint however most previous study are limited to passive kernel learning in which side information is provided beforehand in this paper we present a framework of active kernel learning akl that actively identifies the most informative pairwise constraint for kernel learning the key challenge of active kernel learning is how to measure the informativeness of an example pair given it class label is unknown to this end we propose a min max approach for active kernel learning that selects the example pair that result in a large classification margin regardless of it assigned class label we furthermore approximate the related optimization problem into a convex programming problem we evaluate the effectiveness of the proposed algorithm by comparing it to two other implementation of active kernel learning empirical study with nine datasets on semi supervised data clustering show that the proposed algorithm is more effective than it competitor 
we address the problem of evaluating the risk of a given model accurately at minimal labeling cost this problem occurs in situation in which risk estimate cannot be obtained from held out training data because the training data are unavailable or do not reect the desired test distribution we study active risk estimation process in which instance are actively selected by a sampling process from a pool of unlabeled test instance and their label are queried we derive the sampling distribution that minimizes the estimation error of the active risk estimator when used to select instance from the pool an analysis of the distribution that governs the estimator lead to condence interval we empirically study condition under which the active risk estimate is more accurate than a standard risk estimate that draw equally many instance from the test distribution 
when the transition probability and reward of a markov decision process mdp are known an agent can obtain the optimal policy without any interaction with the environment however exact transition probability are difficult for expert to specify one option left to an agent is a long and potentially costly exploration of the environment in this paper we propose another alternative given initial possibly inaccurate specification of the mdp the agent determines the sensitivity of the optimal policy to change in transition and reward it then focus it exploration on the region of space to which the optimal policy is most sensitive we show that the proposed exploration strategy performs well on several control and planning problem 
online learning algorithm have recently risen to prominence due to their strong theoretical guarantee and an increasing number of practical application for large scale data analysis problem in this paper we analyze a class of online learning algorithm based on fixed potential and nonlinearized loss which yield algorithm with implicit update rule we show how to efficiently compute these update and we prove regret bound for the algorithm we apply our formulation to several special case where our approach ha benefit over existing online learning method in particular we provide improved algorithm and bound for the online metric learning problem and show improved robustness for online linear prediction problem result over a variety of data set demonstrate the advantage of our framework 
a kernel method is proposed for realizing bayes rule based on representation of probability distribution in reproducing kernel hilbert space rkhs the empirical rkhs embeddings of the conditional probability and prior are expressed a feature mapping of sample and an rkhs embedding of the posterior distribution is computed again based on a feature mapping of a sample this kernel bayes rule can be applied to a wide variety of nonparametric bayesian inference problem a an example the approach is used in filtering with a nonparametric state space model consistency of the posterior estimator is established with respect to the rkhs embedding of the population posterior distribution 
undirected graphical model encode in a graph g the dependency structure of a random vector y in many application it is of interest to model y given another random vector x a input we refer to the problem of estimating the graph g x of y conditioned on x x a graph valued regression in this paper we propose a semiparametric method for estimating g x that build a tree on the x space just a in cart classification and regression tree but at each leaf of the tree estimate a graph we call the method graph optimized cart or go cart we study the theoretical property of go cart using dyadic partitioning tree establishing oracle inequality on risk minimization and tree partition consistency we also demonstrate the application of go cart to a meteorological dataset showing how graph valued regression can provide a useful tool for analyzing complex data 
a crucial technique for scaling kernel method to very large data set reaching or exceeding million of instance is based on low rank approximation of kernel matrix we introduce a new family of algorithm based on mixture of nystr om approximation ensemble nystr om algorithm that yield more accurate low rank approximation than the standard nystrom method we give a detailed study of variant of these algorithm based on simple averaging an exponential weight method or regression based method we also present a theoretical analysis of these algorithm including novel error bound guaranteeing a better convergence rate than the standard nystr om method finally we report result of extensive experiment with several data set containing up to m point demonstrating the significant improvement over the standard nystr om approximation 
the support vector machine svm is an acknowledged powerful tool for building classifier but it lack flexibility in the sense that the kernel is chosen prior to learning multiple kernel learning mkl enables to learn the kernel from an ensemble of basis kernel whose combination is optimized in the learning process here we propose composite kernel learning to address the situation where distinct component give rise to a group structure among kernel our formulation of the learning problem encompasses several setup putting more or le emphasis on the group structure we characterize the convexity of the learning problem and provide a general wrapper algorithm for computing solution finally we illustrate the behavior of our method on multi channel data where group correpond to channel 
research in relational data mining ha two major direction finding global model of a relational database and the discovery of local relational pattern within a database while relational pattern show how attribute value co occur in detail their huge number hamper their usage in data analysis global model on the other hand only provide a summary of how different table and their attribute relate to each other lacking detail of what is going on at the local level in this paper we introduce a new approach that combine the positive property of both direction it provides a detailed description of the complete database using a small set of pattern more in particular we utilise a rich pattern language and show how a database can be encoded by such pattern then based on the mdlprinciple the novel rdb krimp algorithm selects the set of pattern that allows for the most succinct encoding of the database this set the code table is a compact description of the database in term of local relational pattern we show that this resulting set is very small both in term of database size and in number of it local relational pattern a reduction of up to order of magnitude is attained 
sampling is a popular way of scaling up machine learning algorithm to large datasets the question often is how many sample are needed adaptive stopping algorithm monitor the performance in an online fashion and they can stop early saving valuable resource we consider problem where probabilistic guarantee are desired and demonstrate how recently introduced empirical bernstein bound can be used to design stopping rule that are efficient we provide upper bound on the sample complexity of the new rule a well a empirical result on model selection and boosting in the filtering setting 
in some stochastic environment the well known reinforcement learning algorithm q learning performs very poorly this poor performance is caused by large overestimation of action value which result from a positive bias that is introduced because q learning us the maximum action value a an approximation for the maximum expected action value we introduce an alternative way to approximate the maximum expected value for any set of random variable the obtained double estimator method is shown to sometimes underestimate rather than overestimate the maximum expected value we apply the double estimator to q learning to construct double q learning a new off policy reinforcement learning algorithm we show the new algorithm converges to the optimal policy and that it performs well in some setting in which q learning performs poorly due to it overestimation 
how do online conversation build is there a common model that human communication follows in this work we explore these question in detail we analyze the structure of conversation in three different social datasets namely usenet group yahoo group and twitter we propose a simple mathematical model for the generation of basic conversation structure and then refine this model to take into account the identity of each member of the conversation 
the k q flat algorithm is a generalization of the popular k mean algorithm where q dimensional best fit affine set replace centroid a the cluster prototype in this work a modification of the k q flat framework for pattern classification is introduced the basic idea is to replace the original reconstruction only energy which is optimized to obtain the k affine space by a new energy that incorporates discriminative term this way the actual classification task is introduced a part of the design and optimization the presentation of the proposed framework is complemented with experimental result showing that the method is computationally very efficient and give excellent result on standard supervised learning benchmark 
in this paper we consider approximate policy iteration based reinforcement learning algorithm in order to implement a flexible function approximation scheme we propose the use of non parametric method with regularization providing a convenient way to control the complexity of the function approximator we propose two novel regularized policy iteration algorithm by adding l regularization to two widely used policy evaluation method bellman residual minimization brm and least square temporal difference learning lstd we derive efficient implementation for our algorithm when the approximate value function belong to a reproducing kernel hilbert space we also provide finite sample performance bound for our algorithm and show that they are able to achieve optimal rate of convergence under the studied condition 
subgroup discovery is the task of identifying the top k pattern in a database with most significant deviation in the distribution of a target attribute y subgroup discovery is a popular approach for identifying interesting pattern in data because it combine statistical significance with an understandable representation of pattern a a logical formula however it is often a problem that some subgroup even if they are statistically highly significant are not interesting to the user we present an approach based on the work on ranking support vector machine that rank subgroup with respect to the user s concept of interestingness and find more interesting subgroup this approach can significantly increase the quality of the subgroup 
it is now well established that sparse signal model are well suited to restoration task and can effectively be learned from audio image and video data recent research ha been aimed at learning discriminative sparse model instead of purely reconstructive one this paper proposes a new step in that direction with a novel sparse representation for signal belonging to different class in term of a shared dictionary and multiple class decision function the linear variant of the proposed model admits a simple probabilistic interpretation while it most general variant admits an interpretation in term of kernel an optimization framework for learning all the component of the proposed model is presented along with experimental result on standard handwritten digit and texture classification task 
structure preserving embedding spe is an algorithm for embedding graph in euclidean space such that the embedding is low dimensional and preserve the global topological property of the input graph topology is preserved if a connectivity algorithm such a k nearest neighbor can easily recover the edge of the input graph from only the coordinate of the node after embedding spe is formulated a a semidefinite program that learns a low rank kernel matrix constrained by a set of linear inequality which capture the connectivity structure of the input graph traditional graph embedding algorithm do not preserve structure according to our definition and thus the resulting visualization can be misleading or le informative spe provides significant improvement in term of visualization and lossless compression of graph outperforming popular method such a spectral embedding and laplacian eigen map we find that many classical graph and network can be properly embedded using only a few dimension furthermore introducing structure preserving constraint into dimensionality reduction algorithm produce more accurate representation of high dimensional data 
this paper focus on a new clustering task called self taught clustering self taught clustering is an instance of unsupervised transfer learning which aim at clustering a small collection of target unlabeled data with the help of a large amount of auxiliary unlabeled data the target and auxiliary data can be different in topic distribution we show that even when the target data are not sufficient to allow effective learning of a high quality feature representation it is possible to learn the useful feature with the help of the auxiliary data on which the target data can be clustered effectively we propose a co clustering based self taught clustering algorithm to tackle this problem by clustering the target and auxiliary data simultaneously to allow the feature representation from the auxiliary data to influence the target data through a common set of feature under the new data representation clustering on the target data can be improved our experiment on image clustering show that our algorithm can greatly outperform several state of the art clustering method when utilizing irrelevant unlabeled auxiliary data 
the syntactic topic model stm is a bayesian nonparametric model of language that discovers latent distribution of word topic that are both semantically and syntactically coherent the stm model dependency parsed corpus where sentence are grouped into document it assumes that each word is drawn from a latent topic chosen by combining document level feature and the local syntactic context each document ha a distribution over latent topic a in topic model which provides the semantic consistency each element in the dependency parse tree also ha a distribution over the topic of it child a in latent state syntax model which provides the syntactic consistency these distribution are convolved so that the topic of each word is likely under both it document and syntactic context we derive a fast posterior inference algorithm based on variational method we report qualitative and quantitative study on both synthetic data and hand parsed document we show that the stm is a more predictive model of language than current model based only on syntax or only on topic 
we introduce the spherical admixture model sam a bayesian topic model over arbitrary normalized data sam model document a point on a highdimensional spherical manifold and is capable of representing negative wordtopic correlation and word presence absence unlike model with multinomial document likelihood such a lda in this paper we evaluate sam a a topic browser focusing on it ability to model negative topic feature and also a a dimensionality reduction method using topic proportion a feature for difficult classification task in natural language processing and computer vision 
we propose a deterministic method to evaluate the integral of a positive function based on soft binning function that smoothly cut the integral into smaller integral that are easier to approximate in combination with mean field approximation for each individual sub part this lead to a tractable algorithm that alternate between the optimization of the bin and the approximation of the local integral we introduce suitable choice for the binning function such that a standard mean field approximation can be extended to a split mean field approximation without the need for extra derivation the method can be seen a a revival of the idea underlying the mixture mean field approach the latter can be obtained a a special case by taking soft max function for the binning 
we present a simple and scalable graph clustering method called power iteration clustering pic pic flnds a very low dimensional embedding of a dataset using truncated power iteration on a normalized pair wise similarity matrix of the data this embedding turn out to be an efiective cluster indicator consistently outperforming widely used spectral method such a ncut on real datasets pic is very fast on large datasets running over time faster than an ncut implementation based on the state of the art iram eigenvector computation technique 
one crucial assumption made by both principal component analysis pca and probabilistic pca ppca is that the instance are independent and identically distributed i i d however this common i i d assumption is unreasonable for relational data in this paper by explicitly modeling covariance between instance a derived from the relational information we propose a novel probabilistic dimensionality reduction method called probabilistic relational pca prpca for relational data analysis although the i i d assumption is no longer adopted in prpca the learning algorithm for prpca can still be devised easily like those for ppca which make explicit use of the i i d assumption experiment on real world data set show that prpca can effectively utilize the relational information to dramatically outperform pca and achieve state of the art performance 
positive definite kernel on probability measure have been recently applied in structured data classification problem some of these kernel are related to classic information theoretic quantity such a mutual information and the jensen shannon divergence meanwhile driven by recent advance in tsallis statistic nonextensive generalization of shannon s information theory have been proposed this paper bridge these two trend we introduce the jensen tsallis q difference a generalization of the jensen shannon divergence we then define a new family of nonextensive mutual information kernel which allow weight to be assigned to their argument and which includes the boolean jensen shannon and linear kernel a particular case we illustrate the performance of these kernel on text categorization task 
dimensionality reduction is a commonly used step in many algorithm for visualization classication clustering and modeling most dimensionality reduction algorithm nd a low dimensional embedding that preserve the structure of high dimensional data point this paper proposes local minimum embedding lme a technique to nd a lowdimensional embedding that preserve the local minimum structure of a given objective function lme provides an embedding that is useful for visualizing and understanding the relation between the original variable that create local minimum additionally the embedding can potentially be used to sample the original function to discover new local minimum the function in the embedded space take an analytic form and hence the gradient can be computed analytically we illustrate the benet of lme in both syn 
many real world datasets can be clustered along multiple dimension for example text document can be clustered not only by topic but also by the author s gender or sentiment unfortunately traditional clustering algorithm produce only a single clustering of a dataset effectively providing a user with just a single view of the data in this paper we propose a new clustering algorithm that can discover in an unsupervised manner each clustering dimension along which a dataset can be meaningfully clustered it ability to reveal the important clustering dimension of a dataset in an unsupervised manner is particularly appealing for those user who have no idea of how a dataset can possibly be clustered we demonstrate it viability on several challenging text classification task 
this paper introduces a novel machine learning model called multiple instance ranking mirank that enables ranking to be performed in a multiple instance learning setting the motivation for mirank stem from the hydrogen abstraction problem in computational chemistry that of predicting the group of hydrogen atom from which a hydrogen is abstracted removed during metabolism the model predicts the preferred hydrogen group within a molecule by ranking the group with the ambiguity of not knowing which hydrogen atom within the preferred group is actually abstracted this paper formulates mirank in it general context and proposes an algorithm for solving mirank problem using successive linear programming the method outperforms multiple instance classification model on several real and synthetic datasets 
we introduce the problem of query decomposition where we are given a query and a document retrieval system and we want to produce a small set of query whose union of resulting document corresponds approximately to that of the original query ideally these query should represent coherent conceptually well separated topic we provide an abstract formulation of the query decomposition problem and we tackle it from two different perspective we first show how the problem can be instantiated a a specific variant of a set cover problem for which we provide an efficient greedy algorithm next we show how the same problem can be seen a a constrained clustering problem with a very particular kind of constraint i e clustering with predefined cluster we develop a two phase algorithm based on hierarchical agglomerative clustering followed by dynamic programming our experiment conducted on a set of actual query in a web scale search engine confirm the effectiveness of the proposed solution 
current graph kernel suffer from two limitation graph kernel based on counting particular type of subgraphs ignore the relative position of these subgraphs to each other while graph kernel based on algebraic method are limited to graph without node label in this paper we present the graphlet spectrum a system of graph invariant derived by mean of group representation theory that capture information about the number a well a the position of labeled subgraphs in a given graph in our experimental evaluation the graphlet spectrum outperforms state of the art graph kernel 
we consider regularized stochastic learning and online optimization problem where the objective function is the sum of two convex term one is the loss function of the learning task and the other is a simple regularization term traditional online algorithm such a stochastic gradient descent ha limited capability of exploiting the problem structure and their inherent low accuracy often make it hard to obtain the desired regularization effect e g sparsity under regularization in this paper we develop extension of nesterov s dual averaging method that can explicitly exploit the regularization structure in an online setting for general convex regularization we show that the regularized dual averaging method achieves the optimal convergence rate o p t where t is the number of iteration or sample in an online algorithm for strongly convex regularization we develop a variant that ha a faster convergence rate o lnt t computational experiment are presented for the special case of regularized online learning 
in large scale online system like search ecommerce or social network application user query represent an important dimension of activity that can be used to study the impact on the system and even the business in this paper we describe how to detect characterize and classify burst in user query in a large scale ecommerce system we build upon the approach discussed in kdd bursty and hierarchical structure in stream and apply them to a high volume industrial context we describe how to identify burst on a near real time basis classify them and apply them to build interesting merchandizing application 
this paper describes and evaluates privacy friendly method for extracting quasi social network from browser behavior on user generated content site for the purpose of finding good audience for brand advertising a opposed to click maximizing for example targeting social network neighbor resonates well with advertiser and on line browsing behavior data counterintuitively can allow the identification of good audience anonymously besides being one of the first paper to our knowledge on data mining for on line brand advertising this paper make several important contribution we introduce a framework for evaluating brand audience in analogy to predictive modeling holdout evaluation we introduce method for extracting quasi social network from data on visitation to social networking page without collecting any information on the identity of the browser or the content of the social network page we introduce measure of brand proximity in the network and show that audience with high brand proximity indeed show substantially higher brand affinity finally we provide evidence that the quasi social network embeds a true social network which along with result from social theory offer one explanation for the increase in brand affinity of the selected audience 
peculiarity oriented mining pom aiming to discover peculiarity rule hidden in a dataset is a new data mining method in the past few year many result and application on pom have been reported however there is still a lack of theoretical analysis in this paper we prove that the peculiarity factor pf one of the most important concept in pom can accurately characterize the peculiarity of data with respect to the probability density function of a normal distribution but is unsuitable for more general distribution thus we propose the concept of local peculiarity factor lpf it is proved that the lpf ha the same ability a the pf for a normal distribution and is the so called sensitive peculiarity description for general distribution to demonstrate the effectiveness of the lpf we apply it to outlier detection problem and give a new outlier detection algorithm called lpf outlier experimental result show that lpf outlier is an effective outlier detection algorithm 
abstract we study the problem of domain transfer for a supervised classification task in mrna splicing we consider a number of recent domain transfer method from machine learning including some that are novel and evaluate them on genomic sequence data from model organism of varying evolutionary distance we find that in case where the organism are not closely related the use of domain adaptation method can help improve classification performance 
recent advance in multiple kernel learning mkl have positioned it a an attractive tool for tackling many supervised learning task the development of efficient gradient descent based optimization scheme ha made it possible to tackle large scale problem simultaneously mkl based algorithm have achieved very good result on challenging real world application yet despite their success mkl approach are limited in that they focus on learning a linear combination of given base kernel in this paper we observe that existing mkl formulation can be extended to learn general kernel combination subject to general regularization this can be achieved while retaining all the efficiency of existing large scale optimization algorithm to highlight the advantage of generalized kernel learning we tackle feature selection problem on benchmark vision and uci database it is demonstrated that the proposed formulation can lead to better result not only a compared to traditional mkl but also a compared to state of the art wrapper and filter method for feature selection 
conditional random field crfs are an effective tool for a variety of different data segmentation and labeling task including visual scene interpretation which seek to partition image into their constituent semantic level region and assign appropriate class label to each region for accurate labeling it is important to capture the global context of the image a well a local information we introduce a crf based scene labeling model that incorporates both local feature and feature aggregated over the whole image or large section of it secondly traditional crf learning requires fully labeled datasets which can be costly and troublesome to produce we introduce a method for learning crfs from datasets with many unlabeled node by marginalizing out the unknown label so that the log likelihood of the known one can be maximized by gradient ascent loopy belief propagation is used to approximate the marginals needed for the gradient and log likelihood calculation and the bethe free energy approximation to the log likelihood is monitored to control the step size our experimental result show that effective model can be learned from fragmentary labelings and that incorporating top down aggregate feature significantly improves the segmentation the resulting segmentation are compared to the state of the art on three different image datasets 
we propose a family of supervised dimensionality reduction sdr algorithm that combine feature extraction dimensionality reduction with learning a predictive model in a unified optimization framework using dataand class appropriate generalized linear model glms and handling both classification and regression problem our approach us simple closed form update rule and is provably convergent promising empirical result are demonstrated on a variety of high dimensional datasets 
in this paper we investigate two aspect of ranking problem on large graph first we augment the deterministic pruning algorithm in sarkar and moore with sampling technique to compute approximately correct ranking with high probability under random walk based proximity measure at query time second we prove some surprising locality property of these proximity measure by examining the short term behavior of random walk the proposed algorithm can answer query on the fly without caching any information about the entire graph we present empirical result on a node author word citation graph from the citeseer domain on a single cpu machine where the average query processing time is around second we present quantifiable link prediction task on most of them our technique outperform personalized pagerank a well known diffusion based proximity measure 
we address the task of actively learning a segmentation system given a large number of unsegmented image and access to an oracle that can segment a given image decide which image to provide to quickly produce a segmenter here a discriminative random field that is accurate over this distribution of image we extend the standard model for active learner to define a system for this task that first selects the image whose expected label will reduce the uncertainty of the other unlabeled image the most and then after greedily selects from the pool of unsegmented image the most informative image the result of our experiment over two real world datasets segmenting brain tumor within magnetic resonance image and segmenting the sky in real image show that training on very few informative image here a few a can produce a segmenter that is a good a training on the entire dataset 
detecting inference in document is critical for ensuring privacy when sharing information in this paper we propose a refined and practical model of inference detection using a reference corpus our model is inspired by association rule mining inference are based on word co occurrence using the model and taking the web a the reference corpus we can find inference and measure their strength through web mining algorithm that leverage search engine such a google or yahoo our model also includes the important case of private corpus to model inference detection in enterprise setting in which there is a large private document repository we find inference in private corpus by using analogue of our web mining algorithm relying on an index for the corpus rather than a web search engine we present result from two experiment the first experiment demonstrates the performance of our technique in identifying all the keywords that allow for inference of a particular topic e g hiv with confidence above a certain threshold the second experiment us the public enron e mail dataset we postulate a sensitive topic and use the enron corpus and the web together to find inference for the topic these experiment demonstrate that our technique are practical and that our model of inference based on word co occurrence is well suited to efficient inference detection 
the synchronous brain activity measured via meg or eeg can be interpreted a arising from a collection possibly large of current dipole or source located throughout the cortex estimating the number location and orientation of these source remains a challenging task one that is signicantly compounded by the effect of source correlation and the presence of interference from spontaneous brain activity sensor noise and other artifact this paper derives an empirical bayesian method for addressing each of these issue in a principled fashion the resulting algorithm guarantee descent of a cost function uniquely designed to handle unknown orientation and arbitrary correlation robust interference suppression is also easily incorporated in a restricted setting the proposed method is shown to have theoretically zero bias estimating both the location and orientation of multi component dipole even in the presence of correlation unlike a variety of existing bayesian localization method or common signal processing technique such a beamforming and sloreta empirical result on both simulated and real data set verify the efcac y of this approach 
the conditional restricted boltzmann machine crbm is a recently proposed model for time series that ha a rich distributed hidden state and permit simple exact inference we present a new model based on the crbm that preserve it most important computational property and includes multiplicative three way interaction that allow the effective interaction weight between two unit to be modulated by the dynamic state of a third unit we factor the three way weight tensor implied by the multiplicative model reducing the number of parameter from o n to o n the result is an efficient compact model whose effectiveness we demonstrate by modeling human motion like the crbm our model can capture diverse style of motion with a single set of parameter and the three way interaction greatly improve the model s ability to blend motion style or to transition smoothly among them 
polysemy is a problem for method that exploit image search engine to build object category model existing unsupervised approach do not take word sense into consideration we propose a new method that us a dictionary to learn model of visual word sense from a large collection of unlabeled web data the use of lda to discover a latent sense space make the model robust despite the very limited nature of dictionary definition the definition ar e used to learn a distribution in the latent space that best represents a sense the a lgorithm then us the text surrounding image link to retrieve image with high probability of a particular dictionary sense an object classifier is trained on the r esulting sense specific image we evaluate our method on a dataset obtained by searching the web for polysemous word category classification experiment sho w that our dictionarybased approach outperforms baseline method 
error in map making task using computer vision are sparse we demonstrate this by considering the construction of digital elevation model that employ stereo matching algorithm to triangulate real world point this sparsity coupled with a geometric theory of error recently developed by the author allows for autonomous agent to calculate their own precision independently of ground truth we connect these development with recent advance in the mathematics of sparse signal reconstruction or compressed sensing the theory presented here extends the autonomy of d model reconstruction discovered in the s to their error 
a recent surge in research in kernelized approach to reinforcement learning ha sought to bring the benefit of kernelized machine learning technique to reinforcement learning kernelized reinforcement learning technique are fairly new and different author have approached the topic with different assumption and goal neither a unifying view nor an understanding of the pro and con of different approach ha yet emerged in this paper we offer a unifying view of the different approach to kernelized value function approximation for reinforcement learning we show that except for different approach to regularization kernelized lstd klstd is equivalent to a modelbased approach that us kernelized regression to find an approximate reward and transition model and that gaussian process temporal difference learning gptd return a mean value function that is equivalent to these other approach we also discus the relationship between our modelbased approach and the earlier gaussian process in reinforcement learning gprl finally we decompose the bellman error into the sum of transition error and reward error term and demonstrate through experiment that this decomposition can be helpful in choosing regularization parameter 
mining discrete pattern in binary data is important for subsampling compression and clustering we consider rankone binary matrix approximation that identify the dominant pattern of the data while preserving it discrete property a best approximation on such data ha a minimum set of inconsistent entry i e mismatch between the given binary data and the approximate matrix due to the hardness of the problem previous account of such problem employ heuristic and the resulting approximation may be far away from the optimal one in this paper we show that the rank one binary matrix approximation can be reformulated a a integer linear program ilp however the ilp formulation is computationally expensive even for small size matrix we propose a linear program lp relaxation which is shown to achieve a guaranteed approximation error bound we further extend the proposed formulation using the regularization technique which is commonly employed to address overfltting the lp formulation is restricted to medium size matrix due to the large number of variable involved for large matrix interestingly we show that the proposed approximate formulation can be transformed into an instance of the minimum s t cut problem which can be solved e ciently by flnding maximum ows our empirical study show the e ciency of the proposed algorithm based on the maximum ow result also conflrm the established theoretical bound 
the general stochastic optimal control soc problem in robotics scenario is often too complex to be solved exactly and in near real time a classical approximate solution is to first compute an optimal deterministic trajectory and then solve a local linear quadratic gaussian lqg perturbation model to handle the system stochasticity we present a new algorithm for this approach which improves upon previous algorithm like ilqg we consider a probabilistic model for which the maximum likelihood ml trajectory coincides with the optimal trajectory and which in the lqg case reproduces the classical soc solution the algorithm then utilizes approximate inference method similar to expectation propagation that efficiently generalize to non lqg system we demonstrate the algorithm on a simulated dof humanoid robot 
in realistic setting the prevalence of a class may change after a classifier is induced and this will degrade the performance of the classifier further complicating this scenario is the fact that labeled data is often scarce and expensive in this paper we address the problem where the class distribution change and only unlabeled example are available from the new distribution we design and evaluate a number of method for coping with this problem and compare the performance of these method our quantification based method estimate the class distribution of the unlabeled data from the changed distribution and adjust the original classifier accordingly while our semi supervised method build a new classifier using the example from the new unlabeled distribution which are supplemented with predicted class value we also introduce a hybrid method that utilizes both quantification and semi supervised learning all method are evaluated using accuracy and f measure on a set of benchmark data set our result demonstrate that our method yield substantial improvement in accuracy and f measure 
graph or network can be used to model complex system detecting community structure from large network data is a classic and challenging task in this paper we propose a novel community detection algorithm which utilizes a dynamic process by contradicting the network topology and the topology based propinquity where the propinquity is a measure of the probability for a pair of node involved in a coherent community structure through several round of mutual reinforcement between topology and propinquity the community structure are expected to naturally emerge the overlapping vertex shared between community can also be easily identified by an additional simple postprocessing to achieve better efficiency the propinquity is incrementally calculated we implement the algorithm on a vertex oriented bulk synchronous parallel bsp model so that the mining load can be distributed on thousand of machine we obtained interesting experimental result on several real network data 
a recent trend in exemplar based unsupervised learning is to formulate the learning problem a a convex optimization problem convexity is achieved by restricting the set of possible prototype to training exemplar in particular this ha been done for clustering vector quantization and mixture model density estimation in this paper we propose a novel algorithm that is theoretically and practically superior to these convex formulation this is possible by posing the unsupervised learning problem a a single convex master problem with non convex subproblems we show that for the above learning task the subproblems are extremely well behaved and can be solved efficiently 
we consider the problem of estimating the graph structure associated with a gaussian markov random field gmrf from i i d sample we study the p erformance of study the performance of the regularized maximum likelihood estimator in the high dimensional setting where the number of node in the graph p the number of edge in the graph s and the maximum node degree d are allowed to grow a a function of the number of sample n our main result provides sufficient condition on n p d for the regularized mle estimator to recover all the edge of the graph with high probability under some condition on the model covariance we show that model selection can be achieved for sample size n d log p with the error decaying a o exp clog p for some constant c we illustrate our theoretical result via simulation and sho w good correspondence between the theoretical prediction and behavior in simulation the area of high dimensional statistic deal with estimat ion in the large p small n setting where p and n correspond respectively to the dimensionality of the dat a and the sample size such highdimensional problem arise in a variety of application among them remote sensing computational biology and natural language processing where the model dimension may be comparable or substantially larger than the sample size it is well known tha t such high dimensional scaling can lead to dramatic breakdown in many classical procedure in the absence of additional model assumption it is frequently impossible to obtain consistent procedure s when p n accordingly an active line of statistical research is based on imposing various restri ctions on the model for instance sparsity manifold structure or graphical model structure and then studying the scaling behavior of different estimator a a function of sample size n ambient dimension p and additional parameter related to these structural assumption in this paper we study the problem of estimating the graph structure of a gauss markov random field gmrf in the high dimensional setting this graphical model selection problem can be reduced to the problem of estimating the zero pattern of the inverse covariance or concentration matrix a line of recent work ha studied estimator based on minimizing gaussian log likelihood penalized by the norm of the entry or the off diagonal entry of the conc entration matrix the resulting optimization problem is a log determinant program which can be solved in polynomial time with interior point method or by faster co ordina te descent algorithm in recent work rothman et al have analyzed some aspect of high dimensional behavior in particular establishing consistency in frobenius norm under certain condition on the model covariance and under certain scaling of the sparsity sample size and ambient model dimension 
accurate and efficient inference in evolutionary tree is a central problem in computational biology while classical treatment have made unrealistic site independence assumption ignoring insertion and deletion realistic approach require tracking insertion and deletion along the phylogenetic tree a challenging and unsolved computational problem we propose a new ancestry resampling procedure for inference in evolutionary tree we evaluate our method in two problem domain multiple sequence alignment and reconstruction of ancestral sequence and show substantial improvement over the current state of the art 
a common task in many text mining application is to generate a multi faceted overview of a topic in a text collection such an overview not only directly serf a an informative summary of the topic but also provides a detailed view of navigation to different facet of the topic existing work ha cast this problem a a categorization problem and requires training example for each facet this ha three limitation all facet are predefined which may not fit the need of a particular user training example for each facet are often unavailable such an approach only work for a predefined type of topic in this paper we break these limitation and study a more realistic new setup of the problem in which we would allow a user to flexibly describe each facet with keywords for an arbitrary topic and attempt to mine a multi faceted overview in an unsupervised way we attempt a probabilistic approach to solve this problem empirical experiment on different genre of text data show that our approach can effectively generate a multi faceted overview for arbitrary topic the generated overview are comparable with those generated by supervised method with training example they are also more informative than unstructured flat summary the method is quite general thus can be applied to multiple text mining task in different application domain 
nasa ha some of the largest and most complex data source in the world with data source ranging from the earth science space science and massive distributed engineering data set from commercial aircraft and spacecraft this talk will discus some of the issue and algorithm developed to analyze and discover pattern in these data set we will also provide an overview of a large research program in integrated vehicle health management the goal of this program is to develop advanced technology to automatically detect diagnose predict and mitigate adverse event during the flight of an aircraft a case study will be presented on a recent data mining analysis performed to support the flight readiness review of the space shuttle mission sts 
how doe one extract unknown but stereotypical event that are linearly superimposed within a signal with variable latency and variable amplitude one could think of using template matching or matching pursuit to flnd the arbitrarily shifted linear component however traditional matching approach require that the template be known a priori to overcome this restriction we use instead semi non negative matrix factorization seminmf that we extend to allow for time shift when matching the template to the signal the algorithm estimate template directly from the data along with their non negative amplitude the resulting method can be thought of a an adaptive template matching procedure we demonstrate the procedure on the task of extracting spike from single channel extracellular recording on these data the algorithm essentially performs spike detection and unsupervised spike clustering result on simulated data and extracellular recording indicate that the method performs well for signalto noise ratio of db or higher and that spike template are recovered accurately provided they are su ciently difierent 
in the absence of explicit query an alternative is to try to infer user interest from implicit feedback signal such a clickstreams or eye tracking the interest formulated a an implicit query can then be used in further search we formulate this task a a probabilistic model which can be interpreted a a kind of transfer or meta learning the probabilistic model is demonstrated to outperform an earlier kernel based method in a small scale information retrieval task 
recent work in deduplication ha shown that collective deduplication of different attribute type can improve performance but although these technique cluster the attribute collectively they do not model them collectively for example in citation in the research literature canonical venue string and title string are dependent because venue tend to focus on a few research area but this dependence is not modeled by current unsupervised technique we call this dependence between field in a record a cross field dependence in this paper we present an unsupervised generative model for the deduplication problem that explicitly model cross field dependence our model us a single set of latent variable to control two disparate clustering model a dirichlet multinomial model over title and a non exchangeable string edit model over venue we show that modeling cross field dependence yield a substantial improvement in performance a reduction in error over a standard dirichlet process mixture 
in this article we report our effort in mining the information encoded a clickthrough data in the server log to evaluate and monitor the relevance ranking quality of a commercial web search engine we describe a metric called pskip that aim to quantify the ranking quality by estimating the probability of user encountering non relevant result that cost them the effort to read and skip a search engine with a lower pskip is regarded a having a better ranking quality a key design goal of pskip is to integrate the finding from two set of user study that utilize eye tracking device to track user browsing pattern on the search result page and that use specially instrumented browser to actively solicit user explicit judgment on their search activity we present the derivation of the maximum likelihood estimation of pskip and demonstrate it efficacy in describing the user study data the mathematical property of pskip are further analyzed and compared with several objective metric a well a the cumulated gain method that us subjective judgment experimental data show that pskip can measure aspect of the search quality that these existing metric are not designed or fail to address such a identifying the real search intent expressed in the ambiguous query although effective and superior in many way we also report a series of experiment that show pskip may be influenced by system issue that are not directly related to relevance ranking suggesting that measurement complementary to pskip are still needed in order to form a holistic and accurate characterization of the ranking quality 
influence maximization is the problem of finding a small subset of node seed node in a social network that could maximize the spread of influence in this paper we study the efficient influence maximization from two complementary direction one is to improve the original greedy algorithm of and it improvement to further reduce it running time and the second is to propose new degree discount heuristic that improves influence spread we evaluate our algorithm by experiment on two large academic collaboration graph obtained from the online archival database arxiv org our experimental result show that a our improved greedy algorithm achieves better running time comparing with the improvement of with matching influence spread b our degree discount heuristic achieve much better influence spread than classic degree and centrality based heuristic and when tuned for a specific influence cascade model it achieves almost matching influence thread with the greedy algorithm and more importantly c the degree discount heuristic run only in millisecond while even the improved greedy algorithm run in hour in our experiment graph with a few ten of thousand of node based on our result we believe that fine tuned heuristic may provide truly scalable solution to the influence maximization problem with satisfying influence spread and blazingly fast running time therefore contrary to what implied by the conclusion of that traditional heuristic are outperformed by the greedy approximation algorithm our result shed new light on the research of heuristic algorithm 
we cast the ranking problem a multiple classification multiple ordinal classification which lead to computationally tractable learning algorithm for relevance ranking in web search we consider the dcg criterion discounted cumulative gain a standard quality measure in information retrieval our approach is motivated by the fact that perfect classification naturally result in perfect dcg score and the dcg error are bounded by classification error we propose using the expected relevance to convert the class probability into ranking score the class probability are learned using a gradient boosting tree algorithm evaluation on large scale datasets show that our approach can improve lambdarank and the regression based ranker in term of the normalized dcg score the general ranking problem ha widespread application including commercial search engine and recommender system in this study we develop a computationally tractable learning algorithm for the general ranking problem and we present our approach in the context of ranking in web search 
in one class classification we seek a rule to find a coherent subset of instance similar to a few positive example in a large pool of instance the problem can be formulated and analyzed naturally in a rate distortion framework leading to an efficient algorithm that compare well with two previous one class method the model can be also be extended to remove background clutter in clustering to improve cluster purity 
recently many application for restricted boltzmann machine rbms have been developed for a large variety of learning problem however rbms are usually used a feature extractor for another learning algorithm or to provide a good initialization for deep feed forward neural network classifier and are not considered a a standalone solution to classification problem in this paper we argue that rbms provide a self contained framework for deriving competitive non linear classifier we present an evaluation of different learning algorithm for rbms which aim at introducing a discriminative component to rbm training and improve their performance a classifier this approach is simple in that rbms are used directly to build a classifier rather than a a stepping stone finally we demonstrate how discriminative rbms can also be successfully employed in a semi supervised setting 
in this paper we study the problem of local triangle counting in large graph namely given a large graph g v e we want to estimate a accurately a possible the number of triangle incident to every node v in the graph the problem of computing the global number of triangle in a graph ha been considered before but to our knowledge this is the first paper that address the problem of local triangle counting with a focus on the efficiency issue arising in massive graph the distribution of the local number of triangle and the related local clustering coefficient can be used in many interesting application for example we show that the measure we compute can help to detect the presence of spamming activity in large scale web graph a well a to provide useful feature to ass content quality in social network for computing the local number of triangle we propose two approximation algorithm which are based on the idea of min wise independent permutation broder et al our algorithm operate in a semi streaming fashion using o jv j space in main memory and performing o log jv j sequential scan over the edge of the graph the first algorithm we describe in this paper also us o jej space in external memory during computation while the second algorithm us only main memory we present the theoretical analysis a well a experimental result in massive graph demonstrating the practical efficiency of our approach 
semi supervised learning ssl is classification where additional unlabeled data can be used to improve accuracy generative approach are appealing in this situation a a model of the data s probability density can assist in identifying cluster nonparametric bayesian method while ideal in theory due to their principled motivation have been difficult to apply to ssl in practice we present a nonparametric bayesian method that us gaussian process for the generative model avoiding many of the problem associated with dirichlet process mixture model our model is fully generative and we take advantage of recent advance in markov chain monte carlo algorithm to provide a practical inference method our method compare favorably to competing approach on synthetic and real world multi class data 
it is commonly agreed that account receivable ar can be a source of financial difficulty for firm when they are not efficiently managed and are underperforming experience across multiple industry show that effective management of ar and overall financial performance of firm are positively correlated in this paper we address the problem of reducing outstanding receivables through improvement in the collection strategy specifically we demonstrate how supervised learning can be used to build model for predicting the payment outcome of newly created invoice thus enabling customized collection action tailored for each invoice or customer our model can predict with high accuracy if an invoice will be paid on time or not and can provide estimate of the magnitude of the delay we illustrate our technique in the context of real world transaction data from multiple firm finally simulation result show that our approach can reduce collection time up to a factor of four compared to a baseline that is not model driven 
we introduce a new reinforcement learning model for the role of the hippocampus in classical conditioning focusing on the difference between trace and delay conditioning in the model all stimulus are represented both a unindividuated whole and a a series of temporal element with varying delay these two stimulus representation interact producing different pattern of learning in trace and delay conditioning the model proposes that hippocampal lesion eliminate long latency temporal element but preserve short latency temporal element for trace conditioning with no contiguity between cue and reward these long latency temporal element are necessary for learning adaptively timed response for delay conditioning the continued presence of the cue support conditioned responding and the short latency element suppress responding early in the cue in accord with the empirical data simulated hippocampal damage impairs trace conditioning but not delay conditioning at medium length interval with longer interval learning is impaired in both procedure and with shorter interval in neither in addition the model make novel prediction about the response topography with extended cue or post training lesion these result demonstrate how temporal contiguity a in delay conditioning change the timing problem faced by animal rendering it both easier and le susceptible to disruption by hippocampal lesion the hippocampus is an important structure in many type of learning and memory with prominent involvement in spatial navigation episodic and working memory stimulus configuration and contextual conditioning one empirical phenomenon that ha eluded many theory of the hippocampus is the dependence of aversive trace conditioning on an intact hippocampus but see rodriguez levy schmajuk dicarlo yamazaki tanaka for example trace eyeblink conditioning disappears following hippocampal lesion solomon et al moyer jr et al induces hippocampal neurogenesis gould et al and produce unique activity pattern in hippocampal neuron mcechron disterhoft in this paper we present a new abstract computational model of hippocampal function during trace conditioning we build on a recent extension of the temporal difference td model of conditioning ludvig sutton kehoe sutton barto to demonstrate how the detail of stimulus representation can qualitatively alter learning during trace and delay conditioning by gently tweaking this stimulus representation and reducing long latency temporal element trace conditioning is severely impaired whereas delay conditioning is hardly affected in the model the hippocampus is responsible for maintaining these long latency element thus explaining the selective importance of this brain structure in trace conditioning 
attribution of climate change to causal factor ha been based predominantly on simulation using physical climate model which have inherent limitation in describing such a complex and chaotic system we propose an alternative data centric approach that relies on actual measurement of climate observation and human and natural forcing factor specifically we develop a novel method to infer causality from spatial temporal data a well a a procedure to incorporate extreme value modeling into our method in order to address the attribution of extreme climate event such a heatwaves our experimental result on a real world dataset indicate that change in temperature are not solely accounted for by solar radiance but attributed more significantly to co and other greenhouse gas combined with extreme value modeling we also show that there ha been a significant increase in the intensity of extreme temperature and that such change in extreme temperature are also attributable to greenhouse gas these preliminary result suggest that our approach can offer a useful alternative to the simulation based approach to climate modeling and attribution and provide valuable insight from a fresh perspective 
we show that an important and computationally challenging solution space feature of the graph coloring problem col namely the number of cluster of solution can be accurately estimated by a technique very similar to one for counting the number of solution this cluster counting approach can be naturally written in term of a new factor graph derived from the factor graph representing the col instance using a variant of the belief propagation inference framework we can efciently approximate cluster count in random col problem over a large range of graph density we illustrate the algorithm on instance with up to vertex moreover we supply a methodology for computing the number of cluster exactly using advanced technique from the knowledge compilation literature this methodology scale up to several hundred variable 
boosting is a very successful classification algorithm that produce a linear combination of weak classifier a k a base learner to obtain high quality classification model in this paper we propose a new boosting algorithm where base learner have structure relationship in the functional space though such relationship are generic our work is particularly motivated by the emerging topic of pattern based classification for semi structured data including graph towards an efficient incorporation of the structure information we have designed a general model where we use an undirected graph to capture the relationship of subgraph based base learner in our method we combine both l norm and laplacian based l norm penalty with logit loss function of logit boost in this approach we enforce model sparsity and smoothness in the functional space spanned by the basis function we have derived efficient optimization algorithm based on coordinate decent for the new boosting formulation and theoretically prove that it exhibit a natural grouping effect for nearby spatial or overlapping feature using comprehensive experimental study we have demonstrated the effectiveness of the proposed learning method 
closed pattern are powerful representative of frequent pattern since they eliminate redundant information we propose a new approach for mining closed unlabeled rooted tree adaptively from data stream that change over time our approach is based on an efficient representation of tree and a low complexity notion of relaxed closed tree and lead to an on line strategy and an adaptive sliding window technique for dealing with change over time more precisely we first present a general methodology to identify closed pattern in a data stream using galois lattice theory using this methodology we then develop three closed tree mining algorithm an incremental one inctreenat a sliding window based one wintreenat and finally one that mine closed tree adaptively from data stream adatreenat to the best of our knowledge this is the first work on mining frequent closed tree in streaming data varying with time we give a first experimental evaluation of the proposed algorithm 
in the traditional link prediction problem a snapshot of a social network is used a a starting point to predict by mean of graph theoretic measure the link that are likely to appear in the future in this paper we introduce cold start link prediction a the problem of predicting the structure of a social network when the network itself is totally missing while some other information regarding the node is available we propose a two phase method based on the bootstrap probabilistic graph the first phase generates an implicit social network under the form of a probabilistic graph the second phase applies probabilistic graph based measure to produce the final prediction we ass our method empirically over a large data collection obtained from flickr using interest group a the initial information the experiment confirm the effectiveness of our approach 
many real world data mining task require the achievement of two distinct goal when applied to unseen data first to induce an accurate preference ranking and second to give good regression performance in this paper we give an efficient and effective combined regression and ranking method crr that optimizes regression and ranking objective simultaneously we demonstrate the effectiveness of crr for both family of metric on a range of large scale task including click prediction for online advertisement result show that crr often achieves performance equivalent to the best of both ranking only and regression only approach in the case of rare event or skewed distribution we also find that this combination can actually improve regression performance due to the addition of informative ranking constraint 
we describe a new method for learning the conditional probability distribution of a binary valued variable from labelled training example our proposed compositional noisy logical learning cnll approach learns a noisy logical distribution in a compositional manner cnll is an alternative to the well known adaboost algorithm which performs coordinate descent on an alternative error measure we describe two cnll algorithm and test their performance compared to adaboost on two type of problem i noisy logical data such a noisy exclusive or and ii four standard datasets from the uci repository our result show that we outperform adaboost while using significantly fewer weak classifier thereby giving a more transparent classifier suitable for knowledge extraction 
we describe a new method for learning the conditional probability distribution of a binary valued variable from labelled training example our proposed compositional noisy logical learning cnll approach learns a noisy logical distribution in a compositional manner cnll is an alternative to the well known adaboost algorithm which performs coordinate descent on an alternative error measure we describe two cnll algorithm and test their performance compared to adaboost on two type of problem i noisy logical data such a noisy exclusive or and ii four standard datasets from the uci repository our result show that we outperform adaboost while using signiflcantly fewer weak classiflers thereby giving a more transparent classifler suitable for knowledge extraction 
the increasing availability of electronic communication data such a that arising from e mail exchange present social and information scientist with new possibility for characterizing individual behavior and by extension identifying latent structure in human population here we propose a model of individual e mail communication that is sufficiently rich to capture meaningful variability across individual while remaining simple enough to be interpretable we show that the model a cascading non homogeneous poisson process can be formulated a a double chain hidden markov model allowing u to use an efficient inference algorithm to estimate the model parameter from observed data we then apply this model to two e mail data set consisting of and user respectively that were collected from two university in different country and year we find that the resulting best estimate parameter distribution for both data set are surprisingly similar indicating that at least some feature of communication dynamic generalize beyond specific context we also find that variability of individual behavior over time is significantly le than variability across the population suggesting that individual can be classified into persistent type we conclude that communication pattern may prove useful a an additional class of attribute data complementing demographic and network data for user classification and outlier detection a point that we illustrate with an interpretable clustering of user based on their inferred model parameter 
co clustering is based on the duality between data point e g document and feature e g word i e data point can be grouped based on their distribution on feature while feature can be grouped based on their distribution on the data point in the past decade several co clustering algorithm have been proposed and shown to be superior to traditional one side clustering however existing co clustering algorithm fail to consider the geometric structure in the data which is essential for clustering data on manifold to address this problem in this paper we propose a dual regularized co clustering drcc method based on seminonnegative matrix tri factorization we deem that not only the data point but also the feature are sampled from some manifold namely data manifold and feature manifold respectively a a result we construct two graph i e data graph and feature graph to explore the geometric structure of data manifold and feature manifold then our coclustering method is formulated a semi nonnegative matrix tri factorization with two graph regularizers requiring that the cluster label of data point are smooth with respect to the data manifold while the cluster label of feature are smooth with respect to the feature manifold we will show that drcc can be solved via alternating minimization and it convergence is theoretically guaranteed experiment of clustering on many benchmark data set demonstrate that the proposed method outperforms many state of the art clustering method 
this paper proposes a new method for comparing clustering both partitionally and geometrically our approach is motivated by the following observation the vast majority of previous technique for comparing clustering are entirely partitional i e they examine assignment of point in set theoretic term after they have been partitioned in doing so these method ignore the spatial layout of the data disregarding the fact that this information is responsible for generating the clustering to begin with we demonstrate that this lead to a variety of failure mode previous comparison technique often fail to dierentiate between signicant change made in data being clustered we formulate a new measure for comparing clustering that combine spatial and partitional information into a single measure using optimization theory doing so eliminates pathological condition in previous approach it also simultaneously remove common limitation such a that each clustering must have the same number of cluster or they are over identical datasets this approach is stable easily implemented and ha strong intuitive appeal 
in the context of civil right law discrimination refers to unfair or unequal treatment of people based on membership to a category or a minority without regard to individual merit rule extracted from database by data mining technique such a classification or association rule when used for decision task such a benefit or credit approval can be discriminatory in the above sense in this paper the notion of discriminatory classification rule is introduced and studied providing a guarantee of non discrimination is shown to be a non trivial task a na ve approach like taking away all discriminatory attribute is shown to be not enough when other background knowledge is available our approach lead to a precise formulation of the redlining problem along with a formal result relating discriminatory rule with apparently safe one by mean of background knowledge an empirical assessment of the result on the german credit dataset is also provided 
in this paper we address the problem of learning when some case are fully labeled while other case are only partially labeled in the form of partial label partial label are represented a a set of possible label for each training example one of which is the correct label we introduce a discriminative learning approach that incorporates partial label information into the conventional margin based learning framework the partial label learning problem is formulated a a convex quadratic optimization minimizing the l norm regularized empirical risk using hinge loss we also present an efficient algorithm for classification in the presence of partial label experiment with different data set show that partial label information improves the performance of classification when there is traditional fully labeled data and also yield reasonable performance in the absence of any fully labeled data 
we introduce confidence weighted linear classifier which add parameter confidence information to linear classifier online learner in this setting update both classifier parameter and the estimate of their confidence the particular online algorithm we study here maintain a gaussian distribution over parameter vector and update the mean and covariance of the distribution with each instance empirical evaluation on a range of nlp task show that our algorithm improves over other state of the art online and batch method learns faster in the online setting and lends itself to better classifier combination after parallel training 
the goal of transfer learning is to improve the learning of a new target concept given knowledge of related source concept s we introduce the first boosting based algorithm for transfer learning that apply to regression task first we describe two existing classification transfer algorithm expboost and tradaboost and show how they can be modified for regression we then introduce extension of these algorithm that improve performance significantly on controlled experiment in a wide range of test domain 
principal component analysis pca ha become established a one of the key tool for dimensionality reduction when dealing with real valued data approach such a exponential family pca and non negative matrix factorisation have successfully extended pca to non gaussian data type but these technique fail to take advantage of bayesian inference and can suffer from problem of overfitting and poor generalisation this paper present a fully probabilistic approach to pca which is generalised to the exponential family based on hybrid monte carlo sampling we describe the model which is based on a factorisation of the observed data matrix and show performance of the model on both synthetic and real data 
we derive generalization of adaboost and related gradient based coordinate descent method that incorporate sparsity promoting penalty for the norm of the predictor that is being learned the end result is a family of coordinate descent algorithm that integrate forward feature induction and back pruning through regularization and give an automatic stopping criterion for feature induction we study penalty based on the and norm of the predictor and introduce mixed norm penalty that build upon the initial penalty the mixed norm regularizers facilitate structural sparsity in parameter space which is a useful property in multiclass prediction and other related task we report empirical result that demonstrate the power of our approach in building accurate and structurally sparse model 
in real world machine learning problem it is very common that part of the input feature vector is incomplete either not available missing or corrupted in this paper we present a boosting approach that integrates feature with incomplete information and those with complete information to form a strong classifier by introducing hidden variable to model missing information we form loss function that combine fully labeled data with partially labeled data to effectively learn normalized and unnormalized model the primal problem of the proposed optimization problem with these loss function are provided to show their close relationship and the motivation behind them we use auxiliary function to bound the change of the loss function and derive explicit parameter update rule for the learning algorithm we demonstrate encouraging result on two real world problem visual object recognition in computer vision and named entity recognition in natural language processing to show the effectiveness of the proposed boosting approach 
we present a novel method for inducing synchronous context free grammar scfgs from a corpus of parallel string pair scfgs can model equivalence between string in term of substitution insertion and deletion and the reordering of sub string we develop a non parametric bayesian model and apply it to a machine translation task using prior to replace the various heuristic commonly used in this field using a variational bayes training procedure we learn the latent structure of translation equivalence through the induction of synchronous grammar category for phrasal translation showing improvement in translation performance over maximum likelihood model 
spectral clustering refers to a flexible class of clustering procedure that can produce high quality clustering on small data set but which ha limited applicability to large scale problem due to it computational complexity of o n with n the number of data point we extend the range of spectral clustering by developing a general framework for fast approximate spectral clustering in which a distortion minimizing local transformation is fir t applied to the data this framework is based on a theoretical analysis that provides a statistical characterization of the effect of lo cal distortion on the mi clustering rate we develop two concrete instance of our general framework one based on local k mean clustering kasp and one based on random projection tree rasp extensive experiment show that these algorithm can achieve significant speedup with little degradation in clustering accuracy s pecifically our algorithm outperform k mean by a large margin in term of accuracy and run several time faster than approximate spectral clustering based on the nystr m method with comparable accuracy and significantly smaller memory footprint remar kably our algorithm make it possible for a single machine to spectral cluster data set with a million observation within severa l minute 
constrained clustering ha been well studied for algorithm like k mean and hierarchical agglomerative clustering however how to encode constraint into spectral clustering remains a developing area in this paper we propose a flexible and generalized framework for constrained spectral clustering in contrast to some previous effort that implicitly encode must link and cannot link constraint by modifying the graph laplacian or the resultant eigenspace we present a more natural and principled formulation which preserve the original graph laplacian and explicitly encodes the constraint our method offer several practical advantage it can encode the degree of belief weight in must link and cannot link constraint it guarantee to lower bound how well the given constraint are satisfied using a user specified threshold and it can be solved deterministically in polynomial time through generalized eigendecomposition furthermore by inheriting the objective function from spectral clustering and explicitly encoding the constraint much of the existing analysis of spectral clustering technique is still valid consequently our work can be posed a a natural extension to unconstrained spectral clustering and be interpreted a finding the normalized min cut of a labeled graph we validate the effectiveness of our approach by empirical result on real world data set with application to constrained image segmentation and clustering benchmark data set with both binary and degree of belief constraint 
rock art is an archaeological term for human made marking on stone it is believed that there are million of petroglyph in north america alone and the study of this valued cultural resource ha implication even beyond anthropology and history surprisingly although image processing information retrieval and data mining have had large impact on many human endeavor they have had essentially zero impact on the study of rock art in this work we identify the reason for this and introduce a novel distance measure and algorithm which allow efficient and effective data mining of large collection of rock art 
social tagging system have become increasingly popular for sharing and organizing web resource tag prediction is a common feature of social tagging system social tagging by nature is an incremental process meaning that once a user ha saved a web page with tag the tagging system can provide more accurate prediction for the user based on user s incremental behavior however existing tag prediction method do not consider this important factor in which their training and test datasets are either split by a fixed time stamp or randomly sampled from a larger corpus in our temporal experiment we perform a time sensitive sampling on an existing public dataset resulting in a new scenario which is much closer to real world in this paper we address the problem of tag prediction by proposing a probabilistic model for personalized tag prediction the model is a bayesian approach and integrates three factor ego centric effect environmental effect and web page content two method both intuitive calculation and learning optimization are provided for parameter estimation pure graphbased method which may have significant constraint such a every user every item and every tag ha to occur in at least p post cannot make a prediction in most of real world case while our model improves the f measure by over compared to a leading algorithm in our real world use case 
optimal coding provides a guiding principle for understanding the representation of sensory variable in neural population here we consider the influence of a prior probability distribution over sensory variable on the optimal allocation of neuron and spike in a population we model the spike of each cell a sample from an independent poisson process with rate governed by an associated tuning curve for this response model we approximate the fisher information in term of the density and amplitude of the tuning curve under the assumption that tuning width varies inversely with cell density we consider a family of objective function based on the expected value over the sensory prior of a functional of the fisher information this family includes lower bound on mutual information and perceptual discriminability a special case in all case we find a closed form expression for the optimum in which the density and gain of the cell in the population are power law function of the stimulus prior this also implies a power law relationship between the prior and perceptual discriminability we show preliminary evidence that the theory successfully predicts the relationship between empirically measured stimulus prior physiologically measured neural response property cell density tuning width and firing rate and psychophysically measured discrimination threshold 
in this paper we introduce the first algorithm for efficiently learning a simulation policy for monte carlo search our main idea is to optimise the balance of a simulation policy so that an accurate spread of simulation outcome is maintained rather than optimising the direct strength of the simulation policy we develop two algorithm for balancing a simulation policy by gradient descent the first algorithm optimises the balance of complete simulation using a policy gradient algorithm whereas the second algorithm optimises the balance over every two step of simulation we compare our algorithm to reinforcement learning and supervised learning algorithm for maximising the strength of the simulation policy we test each algorithm in the domain of x and x computer go using a softmax policy that is parameterised by weight for a hundred simple pattern when used in a simple monte carlo search the policy learnt by simulation balancing achieved significantly better performance with half the mean squared error of a uniform random policy and similar overall performance to a sophisticated go engine 
in many setting such a protein interaction and gene regulatory network collection of author recipient email and social network the data consist of pairwise measurement e g presence or absence of link between pair of object analyzing such data with probabilistic model requires non standard assumption since the usual independence or exchangeability assumption no longer hold in this paper we introduce a class of latent variable model for pairwise measurement mixed membership stochastic blockmodels model in this class combine a global model of dense patch of connectivity blockmodel and a local model to instantiate node specific variability in the connection mixed membership we develop a general variational inference algorithm for fast approximate posterior inference we demonstrate the advantage of mixed membership stochastic blockmodel with application to social network and protein interaction network 
we propose a new rule induction algorithm for solving classification problem via probability estimation the main advantage of decision rule is their simplicity and good interpretability while the early approach to rule induction were based on sequential covering we follow an approach in which a single decision rule is treated a a base classifier in an ensemble the ensemble is built by greedily minimizing the negative loglikelihood which result in estimating the class conditional probability distribution the introduced approach is compared with other decision rule induction algorithm such a slipper lri and rulefit 
we formulate and study a new variant of the k armed bandit problem motivated by e commerce application in our model arm have stochastic lifetime after which they expire in this setting an algorithm need to continuously explore new arm in contrast to the standardk armed bandit model in which arm are available indefinitely and exploration is reduced once an optimal arm is identified with nearcertainty the main motivation for our setting is online advertising where ad have limited lifetime due to for example the nature of their content and their campaign budget an algorithm need to choose among a large collection of ad more than can be fully explored within the typical ad lifetime we present an optimal algorithm for the state aware deterministic reward function case and build on this technique to obtain an algorithm for the state oblivious stochastic reward function case empirical study on various reward distribution including one derived from a real world ad serving application show that the proposed algorithm significantly outperform the standard multi armed bandit approach applied to these setting 
hidden markov model assume that observation in time series data stem from some hidden process that can be compactly represented a a markov chain we generalize this model by assuming that the observed data stem from multiple hidden process whose output interleave to form the sequence of observation exact inference in this model is np hard however a tractable and effective inference algorithm is obtained by extending structured approximate inference method used in factorial hidden markov model the proposed model is evaluated in an activity recognition domain where multiple activity interleave and together generate a stream of sensor observation it is shown to be more accurate than a standard hidden markov model in this domain 
motivated by structural property of the web graph that support efficient data structure for in memory adjacency query we study the extent to which a large network can be compressed boldi and vigna www showed that web graph can be compressed down to three bit of storage per edge we study the compressibility of social network where again adjacency query are a fundamental primitive to this end we propose simple combinatorial formulation that encapsulate efficient compressibility of graph we show that some of the problem are np hard yet admit effective heuristic some of which can exploit property of social network such a link reciprocity our extensive experiment show that social network and the web graph exhibit vastly different compressibility characteristic 
this paper address the problem of noisy generalized binary search gb gb is a well known greedy algorithm for determining a binary valued hypothesis through a sequence of strategically selected query at each step a query is selected that most evenly split the hypothesis under consideration into two disjoint subset a natural generalization of the idea underlying classic binary search gb is used in many application including fault testing machine diagnostics disease diagnosis job scheduling image processing computer vision and active learning in most of these case the response to query can be noisy past work ha provided a partial characterization of gb but existing noise tolerant version of gb are suboptimal in term of query complexity this paper present an optimal algorithm for noisy gb and demonstrates it application to learning multidimensional threshold function 
we consider the problem of selecting a subset of m most informative feature where m is the number of required feature this feature selection problem is essentially a combinatorial optimization problem and is usually solved by an approximation conventional feature selection method address the computational challenge in two step a ranking all the feature by certain score that are usually computed independently from the number of specified feature m and b selecting the top m ranked feature one major shortcoming of these approach is that if a feature f is chosen when the number of specified feature is m it will always be chosen when the number of specified feature is larger than m we refer to this property a the monotonic property of feature selection in this work we argue that it is important to develop efficient algorithm for non monotonic feature selection to this end we develop an algorithm for non monotonic feature selection that approximates the related combinatorial optimization problem by a multiple kernel learning mkl problem we also present a strategy that derives a discrete solution from the approximate solution of mkl and show the performance guarantee for the derived discrete solution when compared to the global optimal solution for the related combinatorial optimization problem an empirical study with a number of benchmark data set indicates the promising performance of the proposed framework compared with several state of the art approach for feature selection 
pseudo likelihood and contrastive divergence are two well known example of contrastive method these algorithm trade o the probability of the correct label with the probability of other nearby instantiation in this paper we explore more general type of contrastive objective which trade o the probability of the correct label against an arbitrary set of other instantiation we prove that a large class of contrastive objective are consistent with maximum likelihood even for finite amount of data this result generalizes asymptotic consistency for pseudo likelihood the proof give significant insight into contrastive objective suggesting that they enforce soft probabilityratio constraint between pair of instantiation based on this insight we propose contrastive constraint generation ccg an iterative constraint generation style algorithm that allows u to learn a log linear model using only map inference we evaluate ccg on a scene classification task showing that it significantly outperforms pseudolikelihood contrastive divergence and a wellknown margin based method 
we introduce a kernel based method for change point analysis within a sequence of temporal observation change point analysis of an unlabelled sample of observation consists in first testing whether a change in the di stribution occurs within the sample and second if a change occurs estimating the change point instant after which the distribution of the observation switch f rom one distribution to another different distribution we propose a test statisti c based upon the maximum kernel fisher discriminant ratio a a measure of homogeneity between segment we derive it limiting distribution under the null hypothesis no change occurs and establish the consistency under the alternative hypoth esis a change occurs this allows to build a statistical hypothesis testing proce dure for testing the presence of a change point with a prescribed false alarm probability and detection probability tending to one in the large sample setting if a change actually occurs the test statistic also yield an estimator of the change po int location promising experimental result in temporal segmentation of mental task from bci data and pop song indexation are presented 
we present a novel approach for learning nonlinear dynamic model which lead to a new set of tool capable of solving problem that are otherwise difficult we provide theory showing this new approach is consistent for model with long range structure and apply the approach to motion capture and highdimensional video data yielding result superior to standard alternative 
we show how the regularizer of transductive support vector machine tsvm can be trained by stochastic gradient descent for linear model and multi layer architecture the resulting method can be trained online have vastly superior training and testing speed to existing tsvm algorithm can encode prior knowledge in the network architecture and obtain competitive error rate we then go on to propose a natural generalization of the tsvm loss function that take into account neighborhood and manifold information directly unifying the twostage low density separation method into a single criterion and leading to state of theart result 
we develop an extension of sliced inverse regression sir that we call localized sliced inverse regression lsir this method allows for supervised dimension reduction by projection onto a linear subspace that capture the n onlinear subspace relevant to predicting the response the method is also extended to the semi supervised setting where one is given labeled and unlabeled data we introduce a simple algorithm that implement this method and illustrate it utility on real an d simulated data 
behavioral targeting bt leverage historical user behavior to select the ad most relevant to user to display the state of the art of bt derives a linear poisson regression model from ne grained user behavioral data and predicts click through rate ctr from user history we designed and implemented a highly scalable and ecient solution to bt using hadoop mapreduce framework with our parallel algorithm and the resulting system we can build above bt category model from the entire yahoo s user base within one day the scale that one can not even imagine with prior system moreover our approach ha yielded ctr lift over the existing production system by leveraging the well grounded probabilistic model tted from a much larger training dataset specically our major contribution include a mapreduce statistical learning algorithm and implementation that achieve optimal data parallelism task parallelism and load balance in spite of the typically skewed distribution of domain data an in place feature vector generation algorithm with linear time complexity o n regardless of the granularity of sliding target window an in memory caching scheme that signicantly reduces the number of disk io to make large scale learning practical highly ecient data structure and sparse representation of model and data to enable fast model update we believe that our work make signicant contribution to solving large scale machine learning problem of industrial relevance in general finally we report comprehensive experimental result using industrial proprietary codebase and datasets 
existing cost sensitive learning method require that the unequal misclassification cost should be given a precise value in many real world application however it is generally difficult to have a precise cost value since the user maybe only know that one type of mistake is much more severe than another type yet it is infeasible to give a precise description in such situation it is more meaningful to work with a cost interval instead of a precise cost value in this paper we report the first study along this direction we propose the cisvm method a support vector machine to work with cost interval information experiment show that when there are only cost interval available cisvm is significantly superior to standard cost sensitive svms using any of the minimal cost mean cost and maximal cost to learn moreover considering that in some case other information about cost can be obtained in addition to cost interval such a the distribution of cost we propose a general approach codis for using the distribution information to help improve performance experiment show that this approach can reduce more risk than the standard cost sensitive svm which assumes the expected cost is the true value 
this paper investigates a new learning formulation called structured sparsity which is a natural extension of the standard sparsity concept in statistical learning and compressive sensing by allowing arbitrary structure on the feature set this concept generalizes the group sparsity idea a general theory is developed for learning with structured sparsity based on the notion of coding complexity associated with the structure moreover a structured greedy algorithm is proposed to efficiently solve the structured sparsity problem experiment demonstrate the advantage of structured sparsity over standard sparsity 
selective attention is a most intensively studied psycholo gical phenomenon rife with theoretical suggestion and schism a critical idea i s that of limited capacity the allocation of which ha produced continual conflict abou t such phenomenon a early and late selection an influential resolution of this debate is based on the notion of perceptual load lavie which suggests that low load easy task because they underuse the total capacity of attentio n mandatorily lead to the processing of stimulus that are irrelevant to the current attentional set whereas high load difficult task grab all resource for themselve s leaving distractors high and dry we argue that this theory present a challenge to bayesian theory of attention and suggest an alternative statistical accou nt of key supporting data 
we consider semi supervised learning from multiple outlook of the same learning task that is learning from different representation of the same type of data a opposed to learning from multiple view where it is assumed that the exact same instance have multiple representation we only assume the availability of sample of the same learning task in different domain we develop an algorithmic framework that is based on mapping the unlabeled data followed by adjusting the mapping using the scarcer labeled data the mapped data from all the outlook can then be used for a generic classification algorithm we further provide sample complexity result under the assumption that the different outlook are inherently low dimension gaussian mixture experiment with real world data indicate the performance boost from using multiple outlook 
we study online learning in an oblivious changing environment the standard measure of regret bound the difference between the cost of the online learner and the best decision in hindsight hence regret minimizing algorithm tend to converge to the static best optimum clearly a suboptimal behavior in changing environment on the other hand various metric proposed to strengthen regret and allow for more dynamic algorithm produce inefficient algorithm we propose a different performance metric which strengthens the standard metric of regret and measure performance with respect to a changing comparator we then describe a series of data streaming based reduction which transform algorithm for minimizing standard regret into adaptive algorithm albeit incurring only poly logarithmic computational overhead using this reduction we obtain efficient low adaptive regret algorithm for the problem of online convex optimization this can be applied to various learning scenario i e online portfolio selection for which we describe experimental result showing the advantage of adaptivity 
information diffusion viral marketing and collective classification all attempt to model and exploit the relationship in a network to make inference about the label of node a variety of technique have been introduced and method that combine attribute information and neighboring label information have been shown to be effective for collective labeling of the node in a network however in part because of the correlation between node label that the technique exploit it is easy to find case in which once a misclassification is made incorrect information propagates throughout the network this problem can be mitigated if the system is allowed to judiciously acquire the label for a small number of node unfortunately under relatively general assumption determining the optimal set of label to acquire is intractable here we propose an acquisition method that learns the case when a given collective classification algorithm make mistake and suggests acquisition to correct those mistake we empirically show on both real and synthetic datasets that this method significantly outperforms a greedy approximate inference approach a viral marketing approach and approach based on network structural measure such a node degree and network clustering in addition to significantly improving accuracy with just a small amount of labeled data our method is tractable on large network 
recently instead of selecting a single kernel multiple kernel learning mkl ha been proposed which us a convex combination of kernel where the weight of each kernel is optimized during training however mkl assigns the same weight to a kernel over the whole input space in this paper we develop a localized multiple kernel learning lmkl algorithm using a gating model for selecting the appropriate kernel function locally the localizing gating model and the kernel based classifier are coupled and their optimization is done in a joint manner empirical result on ten benchmark and two bioinformatics data set validate the applicability of our approach lmkl achieves statistically similar accuracy result compared with mkl by storing fewer support vector lmkl can also combine multiple copy of the same kernel function localized in different part for example lmkl with multiple linear kernel give better accuracy result than using a single linear kernel on bioinformatics data set 
we introduce a natural generalization of submodular set cover and exact active learning with a finite hypothesis class query learning we call this new problem interactive submodular set cover application include advertising in social network with hidden information we give an approximation guarantee for a novel greedy algorithm and give a hardness of approximation result which match up to constant factor we also discus negative result for simpler approach and present encouraging early experimental result 
a matrix decomposition express a matrix a a product of at least two factor matrix equivalently it express each column of the input matrix a a linear combination of the column in the first factor matrix the interpretability of the decomposition is a key issue in many data analysis task we propose two new matrix decomposition problem the nonnegative cx and nonnegative cur problem that give naturally interpretable factor they extend the recently proposed column and column row based decomposition and are aimed to be used with nonnegative matrix our decomposition represent the input matrix a a nonnegative linear combination of a subset of it column or column and row we present two algorithm to solve these problem and provide an extensive experimental evaluation where we ass the quality of our algorithm result a well a the intuitiveness of nonnegative cx and cur decomposition we show that our algorithm return intuitive answer with smaller reconstruction error than the previously proposed method for column and column row decomposition 
a series of correction is developed for the fixed point of ex pectation propagation ep which is one of the most popular method for approximate probabilistic inference these correction can lead to improvement of the inference approximation or serve a a sanity check indicating when ep yield unrealiable result in this paper method are developed to ass the quality of the ep approximation we compute explicit expression for the remainder term of the approximation this lead to various correction for partition function and posterior distribution unde r the hypothesis that the ep approximation work well we identify quantity which can be assumed to be small and can be used in a series expansion of the correction with increasing complexity the computation of low order correction in this expansion is often feasible typically require only moderate computational effort and can lead to an improvement to the ep approximation or to the indication that the approximation cannot be trusted 
topic model such a latent dirichlet allocation lda and correlated topic model ctm have recently emerged a powerful statistical tool for text document modeling in this paper we improve upon ctm and propose independent factor topic model iftm which use linear latent variable model to uncover the hidden source of correlation between topic there are main contribution of this work first by using a sparse source prior model we can directly visualize sparse pattern of topic correlation secondly the conditional independence assumption implied in the use of latent source variable allows the objective function to factorize leading to a fast newton raphson based variational inference algorithm experimental result on synthetic and real data show that iftm run on average time faster than ctm while giving competitive performance a measured by perplexity and loglikelihood of held out data 
in this paper we introduce a generic framework for semi supervised kernel learning given pairwise dis similarity constraint we learn a kernel matrix over the data that respect the provided side information a well a the local geometry of the data our framework is based on metric learning method where we jointly model the metric kernel over the data along with the underlying manifold furthermore we show that for some important parameterized form of the underlying manifold model we can estimate the model parameter and the kernel matrix efficiently our resulting algorithm is able to incorporate local geometry into the metric learning task at the same time it can handle a wide class of constraint finally our algorithm is fast and scalable unlike most of the existing method it is able to exploit the low dimensional manifold structure and doe not require semi definite programming we demonstrate wide applicability and effectiveness of our framework by applying to various machine learning task such a semisupervised classification colored dimensionality reduction manifold alignment etc on each of the task our method performs competitively or better than the respective state of the art method 
recent experimental work ha suggested that the neural firing rate can be interpreted a a fractional derivative at least when signal variation induces neural adaptation here we show that the actual neural spike train itself can be considered a the fractional derivative provided that the neural signal is approximated by a sum of power law kernel a simple standard thresholding spiking neuron suffices to carry out such an approximation given a suitable refractory response empirically we find that the online approximation of signal with a sum of power law kernel is beneficial for encoding signal with slowly varying component like long memory self similar signal for such signal the online power law kernel approximation typically required le than half the number of spike for similar snr a compared to sum of similar but exponentially decaying kernel a power law kernel can be accurately approximated using sum or cascade of weighted exponential we demonstrate that the corresponding decoding of spike train by a receiving neuron allows for natural and transparent temporal signal filtering by tuning the weight of the decoding kernel 
concise representation of frequent itemsets sacrifice readability and direct interpretability by a data analyst of the concise pattern extracted in this paper we introduce an extension of itemsets called regular with an immediate semantics and interpretability and a conciseness comparable to closed itemsets regular itemsets allow for specifying that an item may or may not be present that any subset of an itemset may be present and that any non empty subset of an itemset may be present we devise a procedure called regularmine for mining a set of regular itemsets that is a concise representation of frequent itemsets the procedure computes a covering in term of regular itemsets of the frequent itemsets in the class of equivalence of a closed one we report experimental result on several standard dense and sparse datasets that validate the proposed approach 
inducing causal relationship from observation is a classic problem in scientific inference statistic and machine learning it is also a central part of human learning and a task that people perform remarkably well given it notorious difficulty people can learn causal structure in various setting from diverse form of data observation of the co occurrence frequency between cause and effect interaction between physical object or pattern of spatial or temporal coincidence these different mode of learning are typically thought of a distinct psychological process and are rarely studied together but at heart they present the same inductive challenge identifying the unobservable mechanism that generate observable relation between variable object or event given only sparse and limited data we present a computational level analysis of this inductive problem and a framework for it solution which allows u to model all these form of causal learning in a common language in this framework causal induction is the product of domain general statistical inference guided by domain specific prior knowledge in the form of an abstract causal theory we identify key aspect of abstract prior knowledge the ontology of entity property and relation that organizes a domain the plausibility of specific causal relationship and the functional form of those relationship and show how they provide the constraint that people need to induce useful causal model from sparse data in sir edmond halley wa computing the orbit of a set of comet for inclusion in newton s principia mathematica when he noticed a surprising regularity the comet of and took remarkably similar path across the sky and visited the earth approximately year apart newton had already shown that comet should follow orbit corresponding to conic section parabola hyperbola and ellipsis although no elliptical orbit had yet been observed halley inferred that the sighting of these comet were not three independent event but three consequence of a single common cause a comet that had visited the earth three time travelling in an elliptical orbit he went on to predict that it would return along the same orbit in the comet returned a predicted and ha continued to visit the earth approximately every year since providing a sensational confirmation of newton s physic halley s discovery is an example of causal induction inferring causal structure from data explaining this discovery requires appealing to two factor abstract prior knowledge in the form of a causal theory and statistical inference the prior knowledge that guided halley wa the mathematical theory of physic laid out by newton this theory identified the entity and property relevant to understanding a physical system formalizing notion such a velocity and acceleration and characterized the relation that can hold among these entity using this theory halley could generate a set of hypothesis about the causal structure responsible for his astronomical observation they could have been produced by 
labeling text data is quite time consuming but essential for automatic text classification especially manually creating multiple label for each document may become impractical when a very large amount of data is needed for training multi label text classifier to minimize the human labeling effort we propose a novel multi label active learning approach which can reduce the required labeled data without sacrificing the classification accuracy traditional active learning algorithm can only handle single label problem that is each data is restricted to have one label our approach take into account the multi label information and select the unlabeled data which can lead to the largest reduction of the expected model loss specifically the model loss is approximated by the size of version space and the reduction rate of the size of version space is optimized with support vector machine svm an effective label prediction method is designed to predict possible label for each unlabeled data point and the expected loss for multi label data is approximated by summing up loss on all label according to the most confident result of label prediction experiment on several real world data set all are publicly available demonstrate that our approach can obtain promising classification result with much fewer labeled data than state of the art method 
we analyse matching pursuit for kernel principal component analysis kpca by proving that the sparse subspace it produce is a sample compression scheme we show that this bound is tighter than the kpca bound of shawe taylor et al and highly predictive of the size of the subspace needed to capture most of the variance in the data we analyse a second matching pursuit algorithm called kernel matching pursuit kmp which doe not correspond to a sample compression scheme however we give a novel bound that view the choice of subspace of the kmp algorithm a a compression scheme and hence provide a vc bound to upper bound it future loss finally we describe how the same bound can be applied to other matching pursuit related algorithm 
identifying hot spot of moving vehicle in an urban area is essential to many smart city application the practical research on hot spot in smart city present many unique feature such a highly mobile environment supremely limited size of sample object and the non uniform biased sample all these feature have raised new challenge that make the traditional density based clustering algorithm fail to capture the real clustering property of object making the result le meaningful in this paper we propose a novel non density based approach called mobility based clustering the key idea is that sample object are employed a sensor to perceive the vehicle crowdedness in nearby area using their instant mobility rather than the object representative a such the mobility of sample is naturally incorporated several key factor beyond the vehicle crowdedness have been identified and technique to compensate these effect are proposed we evaluate the performance of mobility based clustering based on real traffic situation experimental result show that using of vehicle a the sample mobility based clustering can accurately identify hot spot which can hardly be obtained by the latest representative algorithm umicro 
in this work inspired by buhler hein strang and zhang et al we give a continuous relaxation of the cheeger cut problem on a weighted graph we show that the relaxation is actually equivalent to the original problem we then describe an algorithm for finding good cut suggested by the similarity of the energy of the relaxed problem and various well studied energy in image processing finally we provide experimental validation of the proposed algorithm demonstrating it efficiency in finding high quality cut 
in this paper we propose a unified algorithmic framework for solving many known variant of md our algorithm is a simple iterative scheme with guaranteed convergence and is modular by changing the internals of a single subroutine in the algorithm we can switch cost function and target space easily in addition to the formal guarantee of convergence our algorithm are accurate in most case they converge to better quality solution than existing method in comparable time moreover they have a small memory footprint and scale effectively for large data set we expect that this framework will be useful for a number of md variant that have not yet been studied our framework extends to embedding high dimensional point lying on a sphere to point on a lower dimensional sphere preserving geodesic distance a a complement to this result we also extend the johnson lindenstrauss lemma to this spherical setting by showing that projecting to a random o log n dimensional sphere cause only an eps distortion in the geodesic distance 
almost all tree kernel proposed in the literature match substructure without taking into account their relative positioning with respect to one another in this paper we propose a novel family of kernel which explicitly focus on this type of information specifically after defining a family of tree kernel based on route between node we present an efficient implementation for a member of this family experimental result on four different datasets show that our method is able to reach state of the art performance obtaining in some case performance better than computationally more demanding tree kernel 
the random projection tree structure proposed in freund dasgupta stoc are space partitioning data structure that automatically adapt to various notion of intrinsic dimensionality of data we prove new result for both the rptreemax and the rptreemean data structure our result for rptreemax give a near optimal bound on the number of level required by this data structure to reduce the size of it cell by a factor s geq we also prove a packing lemma for this data structure our final result show that low dimensional manifold have bounded local covariance dimension a a consequence we show that rptreemean adapts to manifold dimension a well 
by now online social network have become an indispensable part of both online and offline life of human being a large fraction of time spent online by a user is directly influence by the social network to which he she belongs this call for a deeper examination of social network a large scale dynamic object that foster efficient person person interaction the goal of our panel is to discus social network from various research angle in particular we plan to focus on the following broad research related topic large scale data mining algorithmic question sociological aspect privacy web search etc we will also discus the business and societal impact of social network each of these topic ha generated a lot of research in recent year and while taking stock of what ha been done we will also be discussing the direction in which these topic are headed from both science and society point of view our panel will consist of eminent researcher who have worked been working on an eclectic and diverse mix of problem in social network 
this paper present a new sequential version of the latent semantic indexing method based on the notion of the relative error of approximation of the matrix of observation the paper give theoretical and experimental justification of the effectiveness of the proposed method 
online learning algorithm have impressive convergence property when it come to risk minimization and convex game on very large problem however they are inherently sequential in their design which prevents them from taking advantage of modern multi core architecture in this paper we prove that online learning with delayed update converges well thereby facilitating parallel online learning 
semi supervised learning aim at taking advantage of unlabeled data to improve the efficiency of supervised learning procedure for discriminative model however this is a challenging task in this contribution we introduce an original methodology for using unlabeled data through the design of a simple semi supervised objective function we prove that the corresponding semi supervised estimator is asymptotically optimal the practical consequence of this result are discussed for the case of the logistic regression model 
one of the most common problem in machine learning and statistic consists of estimating the mean response x from a vector of observation y assuming y x where x is known is a vector of parameter of interest and a vector of stochastic error we are particularly interested here in the case where the dimension k of is much higher than the dimension of y we propose some flexible bayesian model which can yield sparse estimate of we show that a k these model are closely related to a class of l vy process simulation demonstrate that our model outperform significantly a range of popular alternative 
x abstract several key problem in machine learning can be formulated a submodular set function maximization we present herein a novel algorithm for maximizing a submodular set function under a cardinality constraint the algorithm is based on a cutting plane method and is implemented a an iterative small scale binary integer linear programming procedure it is well known that this problem is np hard and the approximation factor achieved by the greedy algorithm is the theoretical limit for polynomial time a for nonpolynomial time exact algorithm that perform reasonably in practice there ha been very little in the literature although the problem is quite important for many application our algorithm is guaranteed to find the exact solution in finite iteration and it converges fast in practice due to the eciency of the cutting plane mechanism moreover we also provide a method that produce successively decreasing upper bound of the optimal solution while our algorithm provides successively increasing lower bound thus the accuracy of the current solution can be estimated at any point and the algorithm can be stopped early once a desired degree of tolerance is met we evaluate our algorithm on sensor placement and feature selection application showing good performance 
traditional spectral classification ha been proved to be effective in dealing with both labeled and unlabeled data when these data are from the same domain in many real world application however we wish to make use of the labeled data from one domain called in domain to classify the unlabeled data in a different domain out of domain this problem often happens when obtaining labeled data in one domain is difficult while there are plenty of labeled data from a related but different domain in general this is a transfer learning problem where we wish to classify the unlabeled data through the labeled data even though these data are not from the same domain in this paper we formulate this domain transfer learning problem under a novel spectral classification framework where the objective function is introduced to seek consistency between the in domain supervision and the out of domain intrinsic structure through optimization of the cost function the label information from the in domain data is effectively transferred to help classify the unlabeled data from the out of domain we conduct extensive experiment to evaluate our method and show that our algorithm achieves significant improvement on classification performance over many state of the art algorithm 
prediction suffix tree pst are a popular tool for modeling sequence and have been successfully applied in many domain such a compression and language modeling in this work we adapt the well studied winnow algorithm to the task of learning pst the proposed algorithm automatically grows the tree so that it provably remains competitive with any fixed pst determined in hindsight at the same time we prove that the depth of the tree grows only logarithmically with the number of mistake made by the algorithm finally we empirically demonstrate it effectiveness in two different task 
we study learning formulation with non convex regularizaton that are natural for sparse linear model there are two approach to this problem heuristic method such a gradient descent that only find a local minimum a drawback of this approach is the lack of theoretical guarantee showing that the local minimum give a good solution convex relaxation such a l regularization that solves the problem under some condition however it often lead to sub optimal sparsity in reality this paper try to remedy the above gap between theory and practice in particular we investigate a multi stage convex relaxation scheme for solving problem with non convex regularization theoretically we analyze the behavior of a resulting two stage relaxation scheme for the capped l regularization our performance bound show that the procedure is superior to the standard l convex relaxation for learning sparse target experiment confirm the effectiveness of this method on some simulation and real data 
which ad should we display in sponsored search in order to maximize our revenue how should we dynamically rank information source to maximize the value of the ranking these application exhibit strong diminishing return redundancy decrease the marginal utility of each ad or information source we show that these and other problem can be formalized a repeatedly selecting an assignment of item to position to maximize a sequence of monotone submodular function that arrive one by one we present an efficient algorithm for this general problem and analyze it in the no regret model our algorithm posse strong theoretical guarantee such a a performance ratio that converges to the optimal constant of e we empirically evaluate our algorithm on two real world online optimization problem on the web ad allocation with submodular utility and dynamically ranking blog to detect information cascade 
we prove strong noise tolerance property of a potential based boosting algorithm similar to madaboost domingo and watanabe and smoothboost servedio our analysis is in the agnostic framework of kearns schapire and sellie giving polynomial time guarantee in presence of arbitrary noise a remarkable feature of our algorithm is that it can be implemented without reweighting example by randomly relabeling them instead our boosting theorem give a easy corollary alternative derivation of two recent nontrivial result in computational learning theory agnostically learning decision tree gopalan et al and agnostically learning halfspaces kalai et al experiment suggest that the algorithm performs similarly to madaboost 
the exploration exploitation dilemma ha been an intriguing and unsolved problem within the framework of reinforcement learning optimism in the face of uncertainty and model building play central role in advanced exploration method here we integrate several concept and obtain a fast and simple algorithm we show that the proposed algorithm find a near optimal policy in polynomial time and give experimental evidence that it is robust and efficient compared to it ascendant 
one of the main current challenge in itemset mining is to discover a small set of high quality itemsets in this paper we propose a new and general approach for measuring the quality of itemsets the method is solidly founded in bayesian statistic and decrease monotonically allowing for efficient discovery of all interesting itemsets the measure is defined by connecting statistical model and collection of itemsets this allows u to score individual itemsets with the probability of them occuring in random model built on the data a a concrete example of this framework we use exponential model this class of model posse many desirable property most importantly occam s razor in bayesian model selection provides a defence for the pattern explosion a general exponential model are infeasible in practice we use decomposable model a large sub class for which the measure is solvable for the actual computation of the score we sample model from the posterior distribution using an mcmc approach experimentation on our method demonstrates the measure work in practice and result in interpretable and insightful itemsets for both synthetic and real world data 
this paper address the important tradeoff between privacy and learnability when designing algorithm for learning from private database we focus on privacy preserving logistic regression first we apply an idea of dwork et al to design a privacy preserving logistic regression algorithm this involves bounding the sensitivity of regularized logistic regression and perturbing the learned classifier with noise proportional to the sensitivity we then provide a privacy preserving regularized logistic regression algorithm based on a new privacy preserving technique solving a perturbed optimization problem we prove that our algorithm preserve privacy in the model due to we provide learning guarantee for both algorithm which are tighter for our new algorithm in case in which one would typically apply logistic regression experiment demonstrate improved learning performance of our method versus the sensitivity method our privacy preserving technique doe not depend on the sensitivity of the function and extends easily to a class of convex loss function our work also reveals an interesting connection between regularization and privacy 
we consider the problem of distributed reinforcement learning drl from private perception in our setting agent perception such a state reward and action are not only distributed but also should be kept private conventional drl algorithm can handle multiple agent but do not necessarily guarantee privacy preservation and may not guarantee optimality in this work we design cryptographic solution that achieve optimal policy without requiring the agent to share their private information 
finding good representation of text document is crucial in information retrieval and classification system today the most popular document representation is based on a vector of word count in the document this representation neither capture dependency between related word nor handle synonym or polysemous word in this paper we propose an algorithm to learn text document representation based on semi supervised autoencoders that are stacked to form a deep network the model can be trained efficiently on partially labeled corpus producing very compact representation of document while retaining a much class information and joint word statistic a possible we show that it is advantageous to exploit even a few labeled sample during training 
re identification is a major privacy threat to public datasets containing individual record many privacy protection algorithm rely on generalization and suppression of quasi identifier attribute such a zip code and birthdate their objective is usually syntactic sanitization for example k anonymity requires that each quasi identifier tuple appear in at least k record while l diversity requires that the distribution of sensitive attribute for each quasi identifier have high entropy the utility of sanitized data is also measured syntactically by the number of generalization step applied or the number of record with the same quasi identifier in this paper we ask whether generalization and suppression of quasi identifier offer any benefit over trivial sanitization which simply separate quasi identifier from sensitive attribute previous work showed that k anonymous database can be useful for data mining but k anonymization doe not guarantee any privacy by contrast we measure the tradeoff between privacy how much can the adversary learn from the sanitized record and utility measured a accuracy of data mining algorithm executed on the same sanitized record for our experimental evaluation we use the same datasets from the uci machine learning repository a were used in previous research on generalization and suppression our result demonstrate that even modest privacy gain require almost complete destruction of the data mining utility in most case trivial sanitization provides equivalent utility and better privacy than k anonymity l diversity and similar method based on generalization and suppression 
we present an approach to low level vision that combine two main idea the use of convolutional network a an image processing architecture and an unsupervised learning procedure that synthesizes training sample from specific noise model we demonstrate this approach on the challenging problem of natural image denoising using a test set with a hundred natural image we find that convolutional network provide comparable and in some case superior performance to state of the art wavelet and markov random field mrf method moreover we find that a convolutional network offer similar performance in the blind denoising setting a compared to other technique in the non blind setting we also show how convolutional network are mathematically related to mrf approach by presenting a mean field theory for an mrf specially designed for image denoising although these approach are related convolutional network avoid computational difficulty in mrf approach that arise from probabilistic learning and inference this make it possible to learn image processing architecture that have a high degree of representational power we train model with over parameter but whose computational expense is significantly le than that associated with inference in mrf approach with even hundred of parameter 
we propose a new method for detecting pattern of anomaly in categorical datasets we assume that anomaly are generated by some underlying process which affect only a particular subset of the data our method consists of two step we first use a local anomaly detector to identify individual record with anomalous attribute value and then detect pattern where the number of anomalous record is higher than expected given the set of anomaly flagged by the local anomaly detector we search over all subset of the data defined by any set of fixed value of a subset of the attribute in order to detect self similar pattern of anomaly we wish to detect any such subset of the test data which display a significant increase in anomalous activity a compared to the normal behavior of the system a indicated by the training data we perform significance testing to determine if the number of anomaly in any subset of the test data is significantly higher than expected and propose an efficient algorithm to perform this test over all such subset of the data we show that this algorithm is able to accurately detect anomalous pattern in real world hospital container shipping and network intrusion data 
we are interested in learning program for multiple related task given only a few training example per task since the program for a single task is underdetermined by it data we introduce a nonparametric hierarchical bayesian prior over program which share statistical strength across multiple task the key challenge is to parametrize this multi task sharing for this we introduce a new representation of program based on combinatory logic and provide an mcmc algorithm that can perform safe program transformation on this representation to reveal shared inter program substructure 
we present a new approach to large scale graph mining based on so called backbone refinement class the method efficiently mine tree shaped subgraph descriptor under minimum frequency and significance constraint using class of fragment to reduce feature set size and running time the class are defined in term of fragment sharing a common backbone the method is able to optimize structural inter feature entropy a opposed to occurrence which is characteristic for open or closed fragment mining in the experiment the proposed method reduces feature set size by and compared to complete tree mining and open tree mining respectively evaluation using crossvalidation run show that their classification accuracy is similar to the complete set of tree but significantly better than that of open tree compared to open or closed fragment mining a large part of the search space can be pruned due to an improved statistical constraint dynamic upper bound adjustment which is also confirmed in the experiment in lower running time compared to ordinary static upper bound pruning further analysis using large scale datasets yield insight into important property of the proposed descriptor such a the dataset coverage and the class size represented by each descriptor a final cross validation run confirms that the novel descriptor render large training set feasible which previously might have been intractable 
active learning al is an increasingly popular strategy for mitigating the amount of labeled data required to train classifier thereby reducing annotator effort we describe a real world deployed application of al to the problem of biomedical citation screening for systematic review at the tuft medical center s evidence based practice center we propose a novel active learning strategy that exploit a priori domain knowledge provided by the expert specifically labeled feature and extend this model via a linear programming algorithm for situation where the expert can provide ranked labeled feature our method outperform existing al strategy on three real world systematic review datasets we argue that evaluation must be specific to the scenario under consideration to this end we propose a new evaluation framework for finite pool scenario wherein the primary aim is to label a fixed set of example rather than to simply induce a good predictive model we use a method from medical decision theory for eliciting the relative cost of false positive and false negative from the domain expert constructing a utility measure of classification performance that integrates the expert preference our finding suggest that the expert can and should provide more information than instance label alone in addition to achieving strong empirical result on the citation screening problem this work outline many important step for moving away from simulated active learning and toward deploying al for real world application 
we introduce a new algorithm for binary classification in the selective sampling protocol our algorithm us regularized least square rls a base classifier and for this reason it can be efficiently run in any rkhs unlike previous margin based semi supervised algorithm our sampling condition hinge on a simultaneous upper bound on bias and variance of the rls estimate under a simple linear label noise model this fact allows u to prove performance bound that hold for an arbitrary sequence of instance in particular we show that our sampling strategy approximates the margin of the bayes optimal classifier to any desired accuracy by asking d query in the rkhs case d is replaced by a suitable spectral quantity while these are the standard rate in the fully supervised i i d case the best previously known result in our harder setting wa d preliminary experiment show that some of our algorithm also exhibit a good practical performance 
in this paper we propose a discriminant learning framework for problem in which data consist of linear subspace instead of vector by treating subspace a basic element we can make learning algorithm adapt naturally to the problem with linear invariant structure we propose a unifying view on the subspace based learning method by formulating the problem on the grassmann manifold which is the set of fixed dimensional linear subspace of a euclidean space previous method on the problem typically adopt an inconsistent strategy feature extraction is performed in the euclidean space while non euclidean distance are used in our approach we treat each sub space a a point in the grassmann space and perform feature extraction and classification in the same space we show feasibility of the approach by using the grassmann kernel function such a the projection kernel and the binet cauchy kernel experiment with real image database show that the proposed method performs well compared with state of the art algorithm 
generalized linear model glms are an increasingly popular framework for modeling neural spike train they have been linked to the theory of stochastic point process and researcher have used this relation to ass goodness of fit using method from point process theory e g the time rescaling theorem however high neural firing rate or coarse discretization lead to a breakdown of the assumption necessary for this connection here we show how goodness of fit test from point process theory can still be applied to glms by constructing equivalent surrogate point process out of time series observation furthermore two additional test based on thinning and complementing point process are introduced they augment the instrument available for checking model adequacy of point process a well a discretized model 
in this paper we aim to train deep neural network for rapid visual recognition the task is highly challenging largely due to the lack of a meaningful regularizer on the function realized by the network we propose a novel regularization method that take advantage of kernel method where an oracle kernel function represents prior knowledge about the recognition task of interest we derive an efficient algorithm using stochastic gradient descent and demonstrate encouraging result on a wide range of recognition task in term of both accuracy and speed 
in ranking with the pairwise classification approach the loss associated to a predicted ranked list is the mean of the pairwise classification loss this loss is inadequate for task like information retrieval where we prefer ranked list with high precision on the top of the list we propose to optimize a larger class of loss function for ranking based on an ordered weighted average owa yager of the classification loss convex owa aggregation operator range from the max to the mean depending on their weight and can be used to focus on the top ranked element a they give more weight to the largest loss when aggregating hinge loss the optimization problem is similar to the svm for interdependent output space moreover we show that owa aggregate of margin based classification loss have good generalization property experiment on the letor benchmark dataset for information retrieval validate our approach 
the recent emergence of graphic processing unit gpus a general purpose parallel computing device provides u with new opportunity to develop scalable learning method for massive data in this work we consider the problem of parallelizing two inference method on gpus for latent dirichlet allocation lda model collapsed gibbs sampling cgs and collapsed variational bayesian cvb to address limited memory constraint on gpus we propose a novel data partitioning scheme that effectively reduces the memory cost this partitioning scheme also balance the computational cost on each multiprocessor and enables u to easily avoid memory access conflict we use data streaming to handle extremely large datasets extensive experiment showed that our parallel inference method consistently produced lda model with the same predictive power a sequential training method did but with x speedup for cgs and x speedup for cvb on a gpu with multiprocessor the proposed partitioning scheme and data streaming make our approach scalable with more multiprocessor furthermore they can be used a general technique to parallelize other machine learning model 
stochastic relational model srms provide a rich family of choice for learning and predicting dyadic data between two set of entity the model generalize matrix factorization to a supervised learning problem that utilizes attribute of entity in a hierarchical bayesian framework previously variational bayes inference wa applied for srms which is however not scalable when the size of either entity set grows to ten of thousand in this paper we introduce a markov chain monte carlo mcmc algorithm for equivalent model of srms in order to scale the computation to very large dyadic data set both superior scalability and predictive accuracy are demonstrated on a collaborative filtering problem which involves ten of thousand user and half million item 
in real sequence labeling task statistic of many higher order feature are not sufficient due to the training data sparseness very few of them are useful we describe sparse higher order conditional random field sho crfs which are able to handle local feature and sparse higher order feature together using a novel tractable exact inference algorithm our main insight is that state and transition with same potential function can be grouped together and inference is performed on the grouped state and transition though the complexity is not polynomial sho crfs are still efficient in practice because of the feature sparseness experimental result on optical character recognition and chinese organization name recognition show that with the same higher order feature set sho crfs significantly outperform previous approach 
high dimensional inference refers to problem of statistical estimation in which the ambient dimension of the data may be comparable to or possibly even larger than the sample size we study an instance of high dimensional inference in which the goal is to estimate a matrix theta in real k time p on the basis of n noisy observation and the unknown matrix theta is assumed to be either exactly low rank or near low rank meaning that it can be well approximated by a matrix with low rank we consider an m estimator based on regularization by the trace or nuclear norm over matrix and analyze it performance under high dimensional scaling we provide non asymptotic bound on the frobenius norm error that hold for a general class of noisy observation model and then illustrate their consequence for a number of specific matrix model including low rank multivariate or multi task regression system identification in vector autoregressive process and recovery of low rank matrix from random projection simulation result show excellent agreement with the high dimensional scaling of the error predicted by our theory 
active learning may hold the key for solving the data scarcity problem in supervised learning i e the lack of labeled data indeed labeling data is a costly process yet an active learner may request label of only selected instance thus reducing labeling work dramatically most previous work of active learning are however pool based that is a pool of unlabeled example is given and the learner can only select example from the pool to query for their label this type of active learning ha several weakness in this paper we propose novel active learning algorithm that construct example directly to query for label we study both a specific active learner based on the decision tree algorithm and a general active learner that can work with any base learning algorithm a there is no restriction on what example to be queried our method are shown to often query fewer example to reduce the predictive error quickly this cast doubt on the usefulness of the pool in pool based active learning nevertheless our method can be easily adapted to work with a given pool of unlabeled example 
in this paper we investigate a simple mistake driven learning algorithm for discriminative training of continuous density hidden markov model cd hmms most cd hmms for automatic speech recognition use multivariate gaussian emission density or mixture thereof parameterized in term of their mean and covariance matrix for discriminative training of cd hmms we reparameterize these gaussian distribution in term of positive semidefinite matrix that jointly encode their mean and covariance statistic we show how to explore the resulting parameter space in cdhmms with perceptron style update that minimize the distance between viterbi decoding and target transcription we experiment with several form of update systematically comparing the effect of different matrix factorization initialization and averaging scheme on phone accuracy and convergence rate we present experimental result for context independent cd hmms trained in this way on the timit speech corpus our result show that certain type of perceptron training yield consistently significant and rapid reduction in phone error rate 
in multiple instance learning mil how the instance determine the bag label is an essential issue both algorithmically and intrinsically in this paper we show that the mechanism of how the instance determine the bag label is different for different application domain and doe not necessarily obey the traditional assumption of mil we therefore propose an adaptive framework for mil that adapts to different application domain by learning the domain specific mechanism merely from labeled bag our approach is especially attractive when we are encountered with novel application domain for which the mechanism may be different and unknown specifically we exploit mixture model to represent the composition of each bag and an adaptable kernel function to represent the relationship between the bag we validate on synthetic mil datasets that the kernel function automatically adapts to different mechanism of how the instance determine the bag label we also compare our approach with state of the art mil technique on real world benchmark datasets 
we present a large margin formulation and algorithm for structured output prediction that allows the use of latent variable our proposal cover a large range of application problem with an optimization problem that can be solved efficiently using concave convex programming the generality and performance of the approach is demonstrated through three application including motiffinding noun phrase coreference resolution and optimizing precision at k in information retrieval 
how can a search engine automatically provide the best and most appropriate title for a result url link title so that user will be persuaded to click on the url we consider the problem of automatically generating link title for url and propose a general statistical framework for solving this problem the framework is based on using information from a diverse collection of source each of which can be thought of a contributing one or more candidate link title for the url it can also incorporate the context in which the link title will be used along with constraint on it length our framework is applicable to several scenario obtaining succinct title for displaying quicklinks obtaining title for url that lack a good title constructing succinct sitemaps etc extensive experiment show that our method is very effective producing result that are at least better than non trivial baseline 
extracting entity such a people movie from document and identifying the category such a painter writer they belong to enable structured querying and data analysis over unstructured document collection in this paper we focus on the problem of categorizing extracted entity most prior approach developed for this task only analyzed the local document context within which entity occur in this paper we significantly improve the accuracy of entity categorization by i considering an entity s context across multiple document containing it and ii exploiting existing large list of related entity e g list of actor director book these approach introduce computational challenge because a the context of entity ha to be aggregated across several document and b the list of related entity may be very large we develop technique to address these challenge we present a thorough experimental study on real data set that demonstrates the increase in accuracy and the scalability of our approach 
classification of time series ha been attracting great interest over the past decade recent empirical evidence ha strongly suggested that the simple nearest neighbor algorithm is very difficult to beat for most time series problem while this may be considered good news given the simplicity of implementing the nearest neighbor algorithm there are some negative consequence of this first the nearest neighbor algorithm requires storing and searching the entire dataset resulting in a time and space complexity that limit it applicability especially on resource limited sensor second beyond mere classification accuracy we often wish to gain some insight into the data in this work we introduce a new time series primitive time series shapelets which address these limitation informally shapelets are time series subsequence which are in some sense maximally representative of a class a we shall show with extensive empirical evaluation in diverse domain algorithm based on the time series shapelet primitive can be interpretable more accurate and significantly faster than state of the art classifier 
a recent study by two prominent finance researcher fama and french introduces a new framework for studying risk v return the migration of stock across size value portfolio space given the financial event of this first attempt to disentangle the relationship between migration behavior and stock return is especially timely their work however derives result only for market segment not individual company and only for one year move thus we see a new challenge for financial data mining how to capture and categorize the migration of individual company and how such behavior affect their return we propose a novel data mining approach to study the multi year movement of individual company specifically we address the question how doe one discover frequent migration pattern in the stock market we present a new trajectory mining algorithm to discover migration motif in financial market novel feature of this algorithm are it handling of approximate pattern matching through a graph theoretical method maximal clique identification and incorporation of temporal and spatial constraint we have performed a detailed study of the nasdaq nyse and amex stock market over a year span we successfully find migration motif that confirm existing finance theory and other motif that may lead to new financial model 
we present a general inference framework for inter domain gaussian process gps and focus on it usefulness to build sparse gp model the state of the art sparse gp model introduced by snelson and ghahramani in relies on finding a small representative pseudo data set of m element from the same domain a the n available data element which is able to explain existing data well and then us it to perform inference this reduces inference and model selection computation time fromo n too m n wherem n inter domain gps can be used to find a possibly more compact representative set of feature lying in a different domain at the same computational cost being able to specify a different domain for the representative feature allows to incorporate prior knowledge about relevant characteristic of data and detaches the functional form of the covariance and basis function we will show how previously existing model fit into this framework and will use it to develop two new sparse gp model test on large representative regression data set suggest that significant improvement can be achieved while retaining computational efficiency 
we propose an analytic moment based filter for nonlinear stochastic dynamic system modeled by gaussian process exact expression for the expected value and the covariance matrix are provided for both the prediction step and the filter step where an additional gaussian assumption is exploited in the latter case our filter doe not require further approximation in particular it avoids finite sample approximation we compare the filter to a variety of gaussian filter that is the ekf the ukf and the recent gp ukf proposed by ko et al 
in this paper resting on the analysis of instruction frequency and function based instruction sequence we develop an automatic malware categorization system amcs for automatically grouping malware sample into family that share some common characteristic using a cluster ensemble by aggregating the clustering solution generated by different base clustering algorithm we propose a principled cluster ensemble framework for combining individual clustering solution based on the consensus partition the domain knowledge in the form of sample level constraint can be naturally incorporated in the ensemble framework in addition to account for the characteristic of feature representation we propose a hybrid hierarchical clustering algorithm which combine the merit of hierarchical clustering and k medoids algorithm and a weighted subspace k medoids algorithm to generate base clustering the categorization result of our amcs system can be used to generate signature for malware family that are useful for malware detection the case study on large and real daily malware collection from kingsoft anti virus lab demonstrate the effectiveness and efficiency of our amcs system 
we consider the problem of computing the euclidean projection of a vector of length n onto a closed convex set including the l ball and the specialized polyhedron employed in shalev shwartz singer these problem have played building block role in solving several l norm based sparse learning problem existing method have a worst case time complexity of o n log n in this paper we propose to cast both euclidean projection a root finding problem associated with specific auxiliary function which can be solved in linear time via bisection we further make use of the special structure of the auxiliary function and propose an improved bisection algorithm empirical study demonstrate that the proposed algorithm are much more efficient than the competing one for computing the projection 
trajectory planning and optimization is a fundamental problem in articulated robotics algorithm used typically for this problem compute optimal trajectory from scratch in a new situation in effect extensive data is accumulated containing situation together with the respective optimized trajectory but this data is in practice hardly exploited the aim of this paper is to learn from this data given a new situation we want to predict a suitable trajectory which only need minor refinement by a conventional optimizer our approach ha two essential ingredient first to generalize from previous situation to new one we need an appropriate situation descriptor we propose a sparse feature selection approach to find such well generalizing feature of situation second the transfer of previously optimized trajectory to a new situation should not be made in joint angle space we propose a more efficient task space transfer of old trajectory to new situation experiment on a simulated humanoid reaching problem show that we can predict reasonable motion prototype in new situation for which the refinement is much faster than an optimization from scratch 
we develop a semi supervised learning method that constrains the posterior distribution of latent variable under a generative model to satisfy a rich set of feature expectation constraint estimated with labeled data this approach encourages the generative model to discover latent structure that is relevant to a prediction task we estimate parameter with a coordinate ascent algorithm one step of which involves training a discriminative log linear model with an embedded generative model this hybrid model can be used for test time prediction unlike other high performance semi supervised method the proposed algorithm converges to a stationary point of a single objective function and aords additional exibility for example to use dierent latent and output space we conduct experiment on three sequence labeling task achieving the best reported result on two of them and showing promising result on conll ner 
deep belief network dbn s are generative model that contain many layer of hidden variable efficient greedy algorithm for learning and approximate inference have allowed these model to be applied successfully in many application domain the main building block of a dbn is a bipartite undirected graphical model called a restricted boltzmann machine rbm due to the presence of the partition function model selection complexity control and exact maximum likelihood learning in rbm s are intractable we show that annealed importance sampling ai can be used to efficiently estimate the partition function of an rbm and we present a novel ai scheme for comparing rbm s with different architecture we further show how an ai estimator along with approximate inference can be used to estimate a lower bound on the log probability that a dbn model with multiple hidden layer assigns to the test data this is to our knowledge the first step towards obtaining quantitative result that would allow u to directly ass the performance of deep belief network a generative model of data 
metric learning algorithm can provide useful distance function for a variety of domain and recent work ha shown good accuracy for problem where the learner can access all distance constraint at once however in many real application constraint are only available incrementally th u necessitating method that can perform online update to the learned metric existing online algorithm offer bound on worst case performance but typically do not perform well in practice a compared to their offline counterpart we prese nt a new online metric learning algorithm that update a learned mahalanobis metric based on logdet regularization and gradient descent we prove theoretical worst case performance bound and empirically compare the proposed method against existing online metric learning algorithm to further boost the practical ity of our approach we develop an online locality sensitive hashing scheme which lead to efficient update to data structure used for fast approximate similari ty search we demonstrate our algorithm on multiple datasets and show that it outperforms relevant baseline 
our setting is a partially observable markov decision process with continuous state observation and action space decision are based on a particle filter for estimating the belief state given past observation we consider a policy gradient approach for parameterized policy optimization for that purpose we investigate sensitivity analysis of the performance measure with respect to the parameter of the policy focusing on finite difference fd technique we show that the naive fd is subject to variance explosion because of the non smoothness of the resampling procedure we propose a more sophisticated fd method which overcomes this problem and establish it consistency 
nowadays enormous amount of data are continuously generated not only in massive scale but also from different sometimes conflicting view therefore it is important to consolidate different concept for intelligent decision making for example to predict the research area of some people the best result are usually achieved by combining and consolidating prediction obtained from the publication network co authorship network and the textual content of their publication multiple supervised and unsupervised hypothesis can be drawn from these information source and negotiating their difference and consolidating decision usually yield a much more accurate model due to the diversity and heterogeneity of these model in this paper we address the problem of consensus learning among competing hypothesis which either rely on outside knowledge supervised learning or internal structure unsupervised clustering we argue that consensus learning is an np hard problem and thus propose to solve it by an efficient heuristic method we construct a belief graph to first propagate prediction from supervised model to the unsupervised and then negotiate and reach consensus among them their final decision is further consolidated by calculating each model s weight based on it degree of consistency with other model experiment are conducted on newsgroups data cora research paper dblp author conference network and yahoo movie datasets and the result show that the proposed method improves the classification accuracy and the clustering quality measure nmi over the best base model by up to furthermore it run in time proportional to the number of instance which is very efficient for large scale data set 
transactional data are ubiquitous several method including frequent itemsets mining and co clustering have been proposed to analyze transactional database in this work we propose a new research problem to succinctly summarize transactional database solving this problem requires linking the high level structure of the database to a potentially huge number of frequent itemsets we formulate this problem a a set covering problem using overlapped hyperrectangles we then prove that this problem and it several variation are np hard we develop an approximation algorithm hyper which can achieve a ln k approximation ratio in polynomial time we propose a pruning strategy that can significantly speed up the processing of our algorithm additionally we propose an efficient algorithm to further summarize the set of hyperrectangles by allowing false positive condition a detailed study using both real and synthetic datasets show the effectiveness and efficiency of our approach in summarizing transactional database 
learning temporal graph structure from time series data reveals important dependency relationship between current observation and history most previous work focus on learning and predicting with static temporal graph only however in many application such a mechanical system and biology system the temporal dependency might change over time in this paper we develop a dynamic temporal graphical model based on hidden markov model regression and lasso type algorithm our method is able to integrate two usually separate task i e inferring underlying state and learning temporal graph in one unified model the output temporal graph provide better understanding about complex system i e how their dependency graph evolve over time and achieve more accurate prediction we examine our model on two synthetic datasets a well a a real application dataset for monitoring oil production equipment to capture different stage of the system and achieve promising result 
this paper argues that automatic speech recognition asr should accommodate dysarthric speech by incorporating knowledge of the production characteristic of these speaker we describe the acquisition of a new database of dysarthric speech that includes aligned acoustic and articulatory data obtained by electromagnetic articulography this database is used to train theoretical and empirical model of the vocal tract within asr which are compared against discriminative model such a neural network support vector machine and conditional random field result show significant improvement in accuracy over the baseline through the use of production knowledge 
this paper experimentally evaluates multiagent learning algorithm playing repeated matrix game to maximize their cumulative return previous work assessed that qlearning surpassed nash based multi agent learning algorithm based on all againstall repeated matrix game tournament this paper update the state of the art of multiagent learning experiment in a first stage it show that m qubed s and bandit based algorithm such a ucb are the best algorithm on general sum game exp being the best on cooperative game and zero sum game in a second stage our experiment show that two feature forgetting the far past and using recent history with state improve the learning algorithm finally the best algorithm are two new algorithm qlearning and ucb enhanced with the two feature and m qubed 
we consider the problem of learning dissimilarity between point via formulation which preserve a specified ordering between point rather than the numerical value of the dissimilarity dissimilarity ranking d ranking learns from instance like a is more similar to b than c is to d or the distance between e and f is larger than that between g and h three formulation of d ranking problem are presented and new algorithm are presented for two of them one by semidefinite programming sdp and one by quadratic programming qp among the novel capability of these approach are out of sample prediction and scalability to large problem 
a the number and size of large timestamped collection e g sequence of digitized newspaper periodical blog increase the problem of efficiently indexing and searching such data becomes more important term burstiness ha been extensively researched a a mechanism to address event detection in the context of such collection in this paper we explore how burstiness information can be further utilized to enhance the search process we present a novel approach to model the burstiness of a term using discrepancy theory concept this allows u to build a parameter free linear time approach to identify the time interval of maximum burstiness for a given term finally we describe the first burstiness driven search framework and thoroughly evaluate our approach in the context of different scenario 
many learning application are characterized by high dimension usually not all of these dimension are relevant and some are redundant there are two main approach to reduce dimensionality feature selection and feature transformation when one wish to keep the original meaning of the feature feature selection is desired feature selection and transformation are typically presented separately in this paper we introduce a general approach for converting transformationbased method to feature selection method through regularization instead of solving feature selection a a discrete optimization we relax and formulate the problem a a continuous optimization problem an additional advantage of our formulation is that our optimization criterion optimizes for feature relevance and redundancy removal automatically here we illustrate how our approach can be utilized to convert linear discriminant analysis lda and the dimensionality reduction version of the hilbert schmidt independence criterion hsic to two new feature selection algorithm experiment show that our new feature selection method out perform related state of the art feature selection approach 
we consider the minimization of a smooth loss function regularized by the trace norm of the matrix variable such formulation find application in many machine learning task including multi task learning matrix classification and matrix completion the standard semidefinite programming formulation for this problem is computationally expensive in addition due to the non smooth nature of the trace norm the optimal first order black box method for solving such class of problem converges a o k where k is the iteration counter in this paper we exploit the special structure of the trace norm based on which we propose an extended gradient algorithm that converges a o k we further propose an accelerated gradient algorithm which achieves the optimal convergence rate of o k for smooth problem experiment on multi task learning problem demonstrate the efficiency of the proposed algorithm 
we propose low rank representation lrr to segment data drawn from a union of multiple linear or a ne subspace given a set of data vector lrr seek the lowestrank representation among all the candidate that represent all vector a the linear combination of the base in a dictionary unlike the well known sparse representation sr which computes the sparsest representation of each data vector individually lrr aim at flnding the lowest rank representation of a collection of vector jointly lrr better capture the global structure of data giving a more efiective tool for robust subspace segmentation from corrupted data both theoretical and experimental result show that lrr is a promising tool for subspace segmentation 
many agent environment interaction can be framed a dynamical system in which agent take action and receive observation these dynamical system are diverse representing such thing a a biped walking a stock price changing over time the trajectory of a missile or the shifting fish population in a lake often interacting successfully with the environment requires the use of a model which allows the agent to predict something about the future by summarizing the past two of the basic problem in modeling partially observable dynamical system are selecting a representation of state and selecting a mechanism for maintaining that state this thesis explores both problem from a learning perspective we are interested in learning a predictive model directly from the data that arises a an agent interacts with it environment this thesis develops model for dynamical system which represent state a a set of statistic about the short term future a opposed to treating state a a latent unobservable quantity in other word the agent summarizes the past into prediction about the short term future which allow the agent to make further prediction about the infinite future because all parameter in the model are defined using only observable quantity the learning algorithm for such model are often straightforward and have attractive theoretical property we examine in depth the case where state is represented a the parameter of an exponential family distribution over a short term window of future observation we unify a number of different existing model under this umbrella and predict and analyze new model derived from the generalization one goal of this research is to push model with predictively defined state towards real world application we contribute model and companion learning algorithm for domain with partial observability continuous observation structured observation high dimensional observation and or continuous action our model successfully capture standard pomdps and benchmark nonlinear timeseries problem with performance comparable to state of the art model they also allow u to perform well on novel domain which are larger than those captured by other model with predictively defined state including traffic prediction problem and domain analogous to autonomous mobile robot with camera sensor 
many application require optimizing an unknown noisy function that is expensive to evaluate we formalize this task a a multi armed bandit problem where the payoff function is either sampled from a gaussian process gp or ha low rkhs norm we resolve the important open problem of deriving regret bound for this setting which imply novel convergence rate for gp optimization we analyze gp ucb an intuitive upper confidence based algorithm and bound it cumulative regret in term of maximal information gain establishing a novel connection between gp optimization and experimental design moreover by bounding the latter in term of operator spectrum we obtain explicit sublinear regret bound for many commonly used covariance function in some important case our bound have surprisingly weak dependence on the dimensionality in our experiment on real sensor data gp ucb compare favorably with other heuristical gp optimization approach 
in the realm of multilabel classification mlc it ha become an opinio communis that optimal predictive performance can only be achieved by learner that explicitly take label dependence into account the goal of this paper is to elaborate on this postulate in a critical way to this end we formalize and analyze mlc within a probabilistic setting thus it becomes possible to look at the problem from the point of view of risk minimization and bayes optimal prediction moreover inspired by our probabilistic setting we propose a new method for mlc that generalizes and outperforms another approach called classifier chain that wa recently introduced in the literature 
the inhomogeneous poisson process is a point process that ha varying intensity across it domain usually time or space for nonparametric bayesian modeling the gaussian process is a useful way to place a prior distribution on this intensity the combination of a poisson process and gp is known a a gaussian cox process or doubly stochastic poisson process likelihood based inference in these model requires an intractable integral over an infinite dimensional random function in this paper we present the first approach to gaussian cox process in which it is possible to perform inference without introducing approximation or finitedimensional proxy distribution we call our method the sigmoidal gaussian cox process which us a generative model for poisson data to enable tractable inference via markov chain monte carlo we compare our method to competing method on synthetic data and apply it to several real world data set 
classical game theoretic approach that make strong rationality assumption have difficulty modeling human behaviour in economic game we investigate the role of finite level of iterated reasoning and non selfish utility function in a partially observable markov decision process model that incorporates game theoretic notion of interactivity our generative model capture a broad class of characteristic behaviour in a multi round investor trustee game we invert the generative process for a recognition model that is used to classify subject playing this game against randomly matched opponent 
currently the most significant line of defense against malware is anti virus product which focus on authenticating valid software from a white list blocking invalid software from a black list and running any unknown software i e the gray list in a controlled manner the gray list containing unknown software program which could be either normal or malicious is usually authenticated or rejected manually by virus analyst unfortunately along with the development of the malware writing technique the number of file sample in the gray list that need to be analyzed by virus analyst on a daily basis is constantly increasing in this paper we develop an intelligent file scoring system ifss for short for malware detection from the gray list by an ensemble of heterogeneous base level classifier derived by different learning method using different feature representation on dynamic training set to the best of our knowledge this is the first work of applying such ensemble method for malware detection ifss make it practical for virus analyst to identify malware sample from the huge gray list and improves the detection ability of anti virus software it ha already been incorporated into the scanning tool of kingsoft s anti virus software the case study on large and real daily collection of the gray list illustrate that the detection ability and efficiency of our ifss system outperforms other popular scanning tool such a nod and kaspersky 
in a dynamic social or biological environment interaction between the underlying actor can undergo large and systematic change each actor can assume multiple role and their degree of affiliation to these role can also exhibit rich temporal phenomenon we propose a state space mixed membership stochastic blockmodel which can track across time the evolving role of the actor we also derive an efficient variational inference procedure for our model and apply it to the enron email network and rewiring gene regulatory network of yeast in both case our model reveals interesting dynamical role of the actor 
in this paper we propose a set of novel regression based approach to effectively and efficiently summarize frequent itemset pattern specifically we show that the problem of minimizing the restoration error for a set of itemsets based on a probabilistic model corresponds to a non linear regression problem we show that under certain condition we can transform the nonlinear regression problem to a linear regression problem we propose two new method k regression and tree regression to partition the entire collection of frequent itemsets in order to minimize the restoration error the k regression approach employing a k mean type clustering method guarantee that the total restoration error achieves a local minimum the tree regression approach employ a decision tree type of top down partition process in addition we discus alternative to estimate the frequency for the collection of itemsets being covered by the k representative itemsets the experimental evaluation on both real and synthetic datasets demonstrates that our approach significantly improve the summarization performance in term of both accuracy restoration error and computational cost 
planning in partially observable environment remains a challenging problem despite significant recent advance in offline approximation technique a few online method have also been proposed recently and proven to be remarkably scalable but without the theoretical guarantee of their offline counterpart thus it seems natural to try to unify offline and online technique preserving the theoretical property of the former and exploiting the scalability of the latter in this paper we provide theoretical guarantee on an anytime algorithm for pomdps which aim to reduce the error made by approximate offline value iteration algorithm through the use of an efficient online searching procedure the algorithm us search heuristic based on an error analysis of lookahead search to guide the online search towards reachable belief with the most potential to reduce error we provide a general theorem showing that these search heuristic are admissible and lead to complete and optimal algorithm this is to the best of our knowledge the strongest theoretical result available for online pomdp solution method we also provide empirical evidence showing that our approach is also practical and can find provably near optimal solution in reasonable time 
markov logic network mlns use rstorder formula to dene feature of markov network current mln structure learner can only learn short clause literal due to extreme computational cost and thus are unable to represent complex regularity in data to address this problem we present lsm the rst mln structure learner capable of eciently and accurately learning long clause lsm is based on the observation that relational data typically contains pattern that are variation of the same structural motif by constraining the search for clause to occur within motif lsm can greatly speed up the search and thereby reduce the cost of nding long clause lsm us random walk to identify densely connected object in data and group them and their associated relation into a motif our experiment on three real world datasets show that our approach is order of magnitude faster than the state of the art one while achieving the same or better predictive performance 
website traffic varies through time in consistent and predictable way with highest traffic in the middle of the day when providing medium content to visitor it is important to present repeat visitor with new content so that they keep coming back in this paper we present an algorithm to balance the need to keep a website fresh with new content with the desire to present the best content to the most visitor at time of peak traffic we formulate this a the medium scheduling problem where we attempt to maximize total click given the overall traffic pattern and the time varying clickthrough rate of available medium content we present an efficient algorithm to perform this scheduling under certain condition and apply this algorithm to real data obtained from server log showing evidence of significant improvement in traffic from our algorithmic schedule finally we analyze the click data presenting model for why and how the clickthrough rate for new content decline a it age 
modern data mining setting involve a combination of attribute valued descriptor over entity a well a specified relationship between these entity we present an approach to cluster such non homogeneous datasets by using the relationship to impose either dependent clustering or disparate clustering constraint unlike prior work that view constraint a boolean criterion we present a formulation that allows constraint to be satisfied or violated in a smooth manner this enables u to achieve dependent clustering and disparate clustering using the same optimization framework by merely maximizing versus minimizing the objective function we present result on both synthetic data a well a several real world datasets 
we describe a recommender system in the domain of grocery shopping while recommender system have been widely studied this is mostly in relation to leisure product e g movie book and music with non repeated purchase in grocery shopping however consumer will make multiple purchase of the same or very similar product more frequently than buying entirely new item the proposed recommendation scheme offer several advantage in addressing the grocery shopping problem namely a product similarity measure that suit a domain where no rating information is available a basket sensitive random walk model to approximate product similarity by exploiting incomplete neighborhood information online adaptation of the recommendation based on the current basket and a new performance measure focusing on product that customer have not purchased before or purchase infrequently empirical result benchmarking on three real world data set demonstrate a performance improvement of the proposed method over other existing collaborative filtering model 
network data is ubiquitous encoding collection of relationship between entity such a people place gene or corporation while many resource for network of interesting entity are emerging most of these can only annotate connection in a limited fashion although relationship between entity are rich it is impractical to manually devise complete characterization of these relationship for every pair of entity on large real world corpus in this paper we present a novel probabilistic topic model to analyze text corpus and infer description of it entity and of relationship between those entity we develop variational method for performing approximate inference on our model and demonstrate that our model can be practically deployed on large corpus such a wikipedia we show qualitatively and quantitatively that our model can construct and annotate graph of relationship and make useful prediction 
one common predictive modeling challenge occurs in text mining problem is that the training data and the operational testing data are drawn from different underlying distribution this pose a great difficulty for many statistical learning method however when the distribution in the source domain and the target domain are not identical but related there may exist a shared concept space to preserve the relation consequently a good feature representation can encode this concept space and minimize the distribution gap to formalize this intuition we propose a domain adaptation method that parameterizes this concept space by linear transformation under which we explicitly minimize the distribution difference between the source domain with sufficient labeled data and target domain with only unlabeled data while at the same time minimizing the empirical loss on the labeled data in the source domain another characteristic of our method is it capability for considering multiple class and their interaction simultaneously we have conducted extensive experiment on two common text mining problem namely information extraction and document classification to demonstrate the effectiveness of our proposed method 
there ha been much interest in unsupervised learning of hierarchical generative model such a deep belief network scaling such model to full sized high dimensional image remains a difficult problem to address this problem we present the convolutional deep belief network a hierarchical generative model which scale to realistic image size this model is translation invariant and support efficient bottom up and top down probabilistic inference key to our approach is probabilistic max pooling a novel technique which shrink the representation of higher layer in a probabilistically sound way our experiment show that the algorithm learns useful high level visual feature such a object part from unlabeled image of object and natural scene we demonstrate excellent performance on several visual recognition task and show that our model can perform hierarchical bottom up and top down inference over full sized image 
previous work ha shown that the difficulty in learning deep generative or discriminative model can be overcome by an initial unsupervised learning step that map input to useful intermediate representation we introduce and motivate a new training principle for unsupervised learning of a representation based on the idea of making the learned representation robust to partial corruption of the input pattern this approach can be used to train autoencoders and these denoising autoencoders can be stacked to initialize deep architecture the algorithm can be motivated from a manifold learning and information theoretic perspective or from a generative model perspective comparative experiment clearly show the surprising advantage of corrupting the input of autoencoders on a pattern classification benchmark suite 
there ha been considerable work on user browsing model for search engine result both organic and sponsored the click through rate ctr of a result is the product of the probability of examination will the user look at the result time the perceived relevance of the result probability of a click given examination past paper have assumed that when the ctr of a result varies based on the pattern of click in prior position this variation is solely due to change in the probability of examination we show that for sponsored search result a substantial portion of the change in ctr when conditioned on prior click is in fact due to a change in the relevance of result for that query instance not just due to a change in the probability of examination we then propose three new user browsing model which attribute ctr change solely to change in relevance solely to change in examination with an enhanced model of user behavior or to both change in relevance and examination the model that attribute all the ctr change to relevance yield substantially better predictor of ctr than model that attribute all the change to examination and doe only slightly worse than the model that attribute ctr change to both relevance and examination for predicting relevance the model that attribute all the ctr change to relevance again doe better than the model that attribute the change to examination surprisingly we also find that one model might do better than another in predicting ctr but worse in predicting relevance thus it is essential to evaluate user browsing model with respect to accuracy in predicting relevance not just ctr 
max margin markov network m n have shown great promise in structured prediction and relational learning due to the kkt condition the m n enjoys dual sparsity however the existing m n formulation doe not enjoy primal sparsity which is a desirable property for selecting significant feature and reducing the risk of over fitting in this paper we present an l norm regularized max margin markov network l m n which enjoys dual and primal sparsity simultaneously to learn an l m n we present three method including projected sub gradient cutting plane and a novel em style algorithm which is based on an equivalence between l m n and an adaptive m n we perform extensive empirical study on both synthetic and real data set our experimental result show that l m n can effectively select significant feature l m n can perform a well a the pseudo primal sparse laplace m n in prediction accuracy while consistently outperforms other competing method that enjoy either primal or dual sparsity and the em algorithm is more robust than the other two in pre diction accuracy and time efficiency 
a heterogeneous information network is an information network composed of multiple type of object clustering on such a network may lead to better understanding of both hidden structure of the network and the individual role played by every object in each cluster however although clustering on homogeneous network ha been studied over decade clustering on heterogeneous network ha not been addressed until recently a recent study proposed a new algorithm rankclus for clustering on bi typed heterogeneous network however a real world network may consist of more than two type and the interaction among multi typed object play a key role at disclosing the rich semantics that a network carry in this paper we study clustering of multi typed heterogeneous network with a star network schema and propose a novel algorithm netclus that utilizes link across multityped object to generate high quality net cluster an iterative enhancement method is developed that lead to effective ranking based clustering in such heterogeneous network our experiment on dblp data show that netclus generates more accurate clustering result than the baseline topic model algorithm plsa and the recently proposed algorithm rankclus further netclus generates informative cluster presenting good ranking and cluster membership information for each attribute object in each net cluster 
record label company would like to identify potential artist a early a possible in their career before other company approach the artist with competing contract the vast number of candidate make the process of identifying the one with high success potential time consuming and laborious this paper demonstrates how datamining of p p query string can be used in order to mechanize most of this detection process using a unique intercepting system over the gnutella network we were able to capture an unprecedented amount of geographically identified geo aware query allowing u to investigate the diffusion of music related query in time and space our solution is based on the observation that emerging artist especially rapper have a discernible stronghold of fan in their hometown area where they are able to perform and market their music in a file sharing network this is reflected a a delta function spatial distribution of content query using this observation we devised a detection algorithm for emerging artist that look for performer with sharp increase in popularity in a small geographic region though still unnoticable nation wide the algorithm can suggest a short list of artist with breakthrough potential from which we showed that about translate the potential to national success 
in this work we address the question of finding symmetry of a given mdp we show that the problem is isomorphism complete that is the problem is polynomially equivalent to verifying whether two graph are isomorphic apart from the theoretical importance of this result it ha an important practical application the reduction presented can be used together with any off the shelf graph isomorphism solver which performs well in the average case to find symmetry of an mdp in fact we present result of using nauty the best graph isomorphism solver currently available to find symmetry of mdps 
attributed graph are increasingly more common in many application domain such a chemistry biology and text processing a central issue in graph mining is how to collect informative subgraph pattern for a given learning task we propose an iterative mining method based on partial least square regression pls to apply pls to graph data a sparse version of pls is developed first and then it is combined with a weighted pattern mining algorithm the mining algorithm is iteratively called with different weight vector creating one latent component per one mining call our method graph pls is efficient and easy to implement because the weight vector is updated with elementary matrix calculation in experiment our graph pls algorithm showed competitive prediction accuracy in many chemical datasets and it efficiency wa significantly superior to graph boosting gboost and the naive method based on frequent graph mining 
discovering additive structure is an important step towards understanding a complex multi dimensional function because it allows the function to be expressed a the sum of lower dimensional component when variable interact however their effect are not additive and must be modeled and interpreted simultaneously we present a new approach for the problem of interaction detection our method is based on comparing the performance of unrestricted and restricted prediction model where restricted model are prevented from modeling an interaction in question we show that an additive model based regression ensemble additive grove can be restricted appropriately for use with this framework and thus ha the right property for accurately detecting variable interaction 
p many model for computation in recurrent network of neuron assume that the network state move from some initial state to some fixed point attractor or limit cycle that represents the output of the computation however experimental data show that in response to a sensory stimulus the network state move from it initial state through a trajectory of network state and eventually return to the initial state without reaching an attractor or limit cycle in between this type of network response where salient information about external stimulus is encoded in characteristic trajectory of continuously varying network state raise the question how a neural system could compute with such code and arrive for example at a temporally stable classification of the external stimulus we show that a known unsupervised learning algorithm slow feature analysis sfa could be an important ingredient for extracting stable information from these network trajectory in fact if sensory stimulus are more often followed by another stimulus from the same class than by a stimulus from another class sfa approach the classification capability of fisher rsquo s linear discriminant fld a powerful algorithm for supervised learning we apply this principle to simulated cortical microcircuit and show that it enables readout neuron to learn discrimination of spoken digit and detection of repeating firing pattern within a stream of spike train with the same firing statistic without requiring any supervision for learning p 
restricted boltzmann machine rbms are a type of probability model over the boolean cube n that have recently received much attention we establish the intractability of two basic computational task involving rbms even if only a coarse approximation to the correct output is required we first show that assuming p np for any fixed positive constant k which may be arbitrarily large there is no polynomial time algorithm for the following problem given an n bit input string x and the parameter of a rbm m output an estimate of the probability assigned to x by m that is accurate to within a multiplicative factor of e kn this hardness result hold even if the parameter of m are constrained to be at most n for any function n that grows faster than linearly and if the number of hidden node of m is at most n we then show that assuming rp np there is no polynomial time randomized algorithm for the following problem given the parameter of an rbm m generate a random example from a probability distribution whose total variation distance from the distribution defined by m is at most 
we consider the problem of learning to follow a desired trajectory when given a small number of demonstration from a sub optimal expert we present an algorithm that i extract the initially unknown desired trajectory from the sub optimal expert s demonstration and ii learns a local model suitable for control along the learned trajectory we apply our algorithm to the problem of autonomous helicopter flight in all case the autonomous helicopter s performance exceeds that of our expert helicopter pilot s demonstration even stronger our result significantly extend the state of the art in autonomous helicopter aerobatics in particular our result include the first autonomous tic tocs loop and hurricane vastly superior performance on previously performed aerobatic maneuver such a in place flip and roll and a complete airshow which requires autonomous transition between these and various other maneuver 
for two class classification it is common to classify by setting a threshold on class probability estimate where the threshold is determined by roc curve analysis an analog for multi class classification is learning a new class partitioning of the multiclass probability simplex to minimize empirical misclassification cost we analyze the interplay between systematic error in the class probability estimate and cost matrix for multiclass classification we explore the effect on the class partitioning of five different transformation of the cost matrix experiment on benchmark datasets with naive bayes and quadratic discriminant analysis show the effectiveness of learning a new partition matrix compared to previously proposed method 
malicious web site are a cornerstone of internet criminal activity a a result there ha been broad interest in developing system to prevent the end user from visiting such site in this paper we describe an approach to this problem based on automated url classification using statistical method to discover the tell tale lexical and host based property of malicious web site url these method are able to learn highly predictive model by extracting and automatically analyzing ten of thousand of feature potentially indicative of suspicious url the resulting classifier obtain accuracy detecting large number of malicious web site from their url with only modest false positive 
we propose a visualization method based on a topic model for discrete data such a document unlike conventional visualization method based on pairwise distance such a multi dimensional scaling we consider a mapping from the visualization space into the space of document a a generative process of document in the model both document and topic are assumed to have latent coordinate in a twoor three dimensional euclidean space or visualization space the topic proportion of a document are determined by the distance between the document and the topic in the visualization space and each word is drawn from one of the topic according to it topic proportion a visualization i e latent coordinate of document can be obtained by fitting the model to a given set of document using the em algorithm resulting in document with similar topic being embedded close together we demonstrate the effectiveness of the proposed model by visualizing document and movie data set and quantitatively compare it with conventional visualization method 
display ad proliferate on the web but are they effective or are they irrelevant in light of all the other advertising that people see we describe a way to answer these question quickly and accurately without randomized experiment survey focus group or expert data analyst doubly robust estimation protects against the selection bias that is inherent in observational data and a nonparametric test that is based on irrelevant outcome provides further defense simulation based on realistic scenario show that the resulting estimate are more robust to selection bias than traditional alternative such a regression modeling or propensity scoring moreover computation are fast enough that all processing from data retrieval through estimation testing validation and report generation proceeds in an automated pipeline without anyone needing to see the raw data 
we developed a model based on nonparametric bayesian modeling for automatic discovery of semantic relationship between word taken from a corpus it is aimed at discovering semantic knowledge about word in particular domain which ha become increasingly important with the growing use of text mining information retrieval and speech recognition the subject predicate structure is taken a a syntactic structure with the noun a the subject and the verb a the predicate this structure is regarded a a graph structure the generation of this graph can be modeled using the hierarchical dirichlet process and the pitman yor process the probabilistic generative model we developed for this graph structure consists of subject predicate structure extracted from a corpus evaluation of this model by measuring the performance of graph clustering based on wordnet similarity demonstrated that it outperforms other baseline model 
the development of mechanism to understand and model the expected behaviour of multiagent learner is becoming increasingly important a the area rapidly find application in a variety of domain in this paper we present a framework to model the behaviour of q learning agent using the greedy exploration mechanism for this we analyse a continuous time version of the q learning update rule and study how the presence of other agent and the greedy mechanism affect it we then model the problem a a system of difference equation which is used to theoretically analyse the expected behaviour of the agent the applicability of the framework is tested through experiment in typical game selected from the literature 
memory leak are caused by software program that prevent the reclamation of memory that is no longer in use they can cause significant slowdown exhaustion of available storage space and eventually application crash detecting memory leak is challenging because real world application are built on multiple layer of software framework making it difficult for a developer to know whether observed reference to object are legitimate or the cause of a leak we present a graph mining solution to this problem wherein we analyze heap dump to automatically identify subgraphs which could represent potential memory leak source although heap dump are commonly analyzed in existing heap profiling tool our work is the first to apply a graph grammar mining solution to this problem unlike classical graph mining work we show that it suffices to mine the dominator tree of the heap dump which is significantly smaller than the underlying graph our approach identifies not just leaking candidate and their structure but also provides aggregate information about the access path to the leak we demonstrate several synthetic a well a real world example of heap dump for which our approach provides more insight into the problem than state of the art tool such a eclipse s mat 
network science focus on relationship between social entity it is used widely in the social and behavioral science a well a in political science economics organizational science and industrial engineering the social network perspective ha been developed over the last sixty year by researcher in psychology sociology and anthropology and morerecently to a lesser extent in physic network science is gaining recognition and standing in the general social and behavioral science community a the theoretical basis for examining social structure this basis ha been clearly defined by many theorist and the paradigm convincingly applied to important substantive problem however the paradigm requires a new and different set of concept and analytic tool beyond those provided by standard quantitative particularly statistical method these concept and tool are the topic of this talk 
we study in this paper the problem of incremental crawling of web forum which is a very fundamental yet challenging step in many web application traditional approach mainly focus on scheduling the revisiting strategy of each individual page however simply assigning different weight for different individual page is usually inefficient in crawling forum site because of the different characteristic between forum site and general website instead of treating each individual page independently we propose a list wise strategy by taking into account the site level knowledge such site level knowledge is mined through reconstructing the linking structure called sitemap for a given forum site with the sitemap post from the same thread but distributed on various page can be concatenated according to their timestamps after that for each thread we employ a regression model to predict the time when the next post arrives based on this model we develop an efficient crawler which is faster than some state of the art method in term of fetching new generated content and meanwhile our crawler also ensure a high coverage ratio experimental result show promising performance of coverage bandwidth utilization and timeliness of our crawler on various forum 
given multiple possible model b b bn for a protein structure a common sub task in in silico protein structure prediction is ranking these model according to their quality extant approach use mle estimate of parameter ri to obtain point estimate of the model quality we describe a bayesian alternative to assessing the quality of these model that build an mrf over the parameter of each model and performs approximate inference to integrate over them hyperparameters w are learnt by optimizing a list wise loss function over training data our result indicate that our bayesian approach can significantly outperform mle estimate and that optimizing the hyper parameter can further improve result 
given a model family and a set of unlabeled example one could either label specific example or state general constraint both provide information about the desired model in general what is the most cost effective way to learn to address this question we introduce measurement a general class of mechanism for providing information about a target model we present a bayesian decision theoretic framework which allows u to both integrate diverse measurement and choose new measurement to make we use a variational inference algorithm which exploit exponential family duality the merit of our approach are demonstrated on two sequence labeling task 
the generalization error or probability of misclassification of ensemble classifier ha been shown to be bounded above by a function of the mean correlation between the constituent i e base classifier and their average strength this bound suggests that increasing the strength and or decreasing the correlation of an ensemble s base classifier may yield improved performance under the assumption of equal error cost however this and other existing bound do not directly address application space in which error cost are inherently unequal for application involving binary classification receiver operating characteristic roc curve performance curve that explicitly trade off false alarm and missed detection are often utilized to support decision making to address performance optimization in this context we have developed a lower bound for the entire roc curve that can be expressed in term of the class specific strength and correlation of the base classifier we present empirical analysis demonstrating the efficacy of these bound in predicting relative classifier performance in addition we specify performance region of the roc curve that are naturally delineated by the class specific strength of the base classifier and show that each of these region can be associated with a unique set of guideline for performance optimization of binary classifier within unequal error cost regime 
decision making lie at the very heart of many psychiatric disease it is also a central theoretical concern in a wide variety of field and ha s undergone detailed in depth analysis we take a an example major depressive disorder mdd applying insight from a bayesian reinforcement learning framework we focus on anhedonia and helplessness helplessness a core element in the conceptualization of mdd that ha lead to major advance in it treatment pharmacological and neurobiological understanding is formalized a a simple prior over the outcome entropy of action in uncertain environment anhedonia which is an equally fundamental aspect of the disease is related to the effective reward size these formulation allow for the design of specific task to m easure anhedonia and helplessness behaviorally we show that these behavioral measure capture explicit questionnaire based cognition we also provide evidence that these task may allow classification of subject into healthy and mdd gro ups based purely on a behavioural measure and avoiding any verbal report 
archived data often describe entity that participate in multiple role each of these role may influence various aspect of the data for example a register transaction collected at a retail store may have been initiated by a person who is a woman a mother an avid reader and an action movie fan each of these role can influence various aspect of the customer s purchase the fact that the customer is a mother may greatly influence the purchase of a toddler sized pair of pant but have no influence on the purchase of an action adventure novel the fact that the customer is an action move fan and an avid reader may influence the purchase of the novel but will have no effect on the purchase of a shirt in this paper we present a generic bayesian framework for capturing exactly this situation in our framework it is assumed that multiple role exist and each data point corresponds to an entity such a a retail customer or an email or a news article that selects various role which compete to influence the various attribute associated with the data point we develop robust mcmc algorithm for learning the model under the framework 
learning from multi view data is important in many application such a image classification and annotation in this paper we present a large margin learning framework to discover a predictive latent subspace representation shared by multiple view our approach is based on an undirected latent space markov network that fulfills a weak conditional independence assumption that multi view observation and response variable are independent given a set of latent variable we provide efficient inference and parameter estimation method for the latent subspace model finally we demonstrate the advantage of large margin learning on real video and web image data for discovering predictive latent representation and improving the performance on image classification annotation and retrieval 
we present and evaluate a machine learning approach to constructing patient specific classifier that detect the onset of an epileptic seizure through analysis of the scalp eeg a non invasive measure of the brain s electrical activity this problem is challenging because the brain s electrical activity is composed of numerous class with overlapping characteristic the key step involved in realizing a high performance algorithm included shaping the problem into an appropriate machine learning framework and identifying the feature critical to separating seizure from other type of brain activity when trained on or more seizure per patient and tested on hour of continuous eeg from patient our algorithm detected of test seizure with a median detection delay of second and a median false detection rate of false detection per hour period we also provide information about how to download the chb mit database which contains the data used in this study 
naive bayes and logistic regression perform well in different regime while the former is a very simple generative model which is efficient to train and performs well empirically in many application the latter is a discriminative model which often achieves better accuracy and can be shown to outperform naive bayes asymptotically in this paper we propose a novel hybrid model partitioned logistic regression which ha several advantage over both naive bayes and logistic regression this model separate the original feature space into several disjoint feature group individual model on these group of feature are learned using logistic regression and their prediction are combined using the naive bayes principle to produce a robust final estimation we show that our model is better both theoretically and empirically in addition when applying it in a practical application email spam filtering it improves the normalized auc score at false positive rate by and compared to naive bayes and logistic regression when using the exact same training example 
this paper present an approach for improving the performance of kernel classiers applied to object categorization problem the approach is based on the use of distribution centered around each training point which are exploited for inter class invariant image representation with local invariant feature furthermore we propose an extensive use of unlabeled image for improving the svmbased classier 
we describe a way of learning matrix representation of object and relationship the goal of learning is to allow multiplication of matrix t o represent symbolic relationship between object and symbolic relationship between relationship which is the main novelty of the method we demonstrate that this lead to excellent generalization in two different domain modular arithmetic and family relationship we show that the same system can learn first o rder proposition such a or christopher penelope ha wife and higher order proposition such a plus and inverse or ha husband ha wife higher oppsex we further demonstrate that the system understands how higher order proposition are related to first order on e by showing that it can correctly answer question about first order proposition involving the relation or ha wife even though it ha not been trained on any first order example s involving these relation 
this paper examines important factor for link prediction in network and provides a general high performance framework for the prediction task link prediction in sparse network present a significant challenge due to the inherent disproportion of link that can form to link that do form previous research ha typically approached this a an unsupervised problem while this is not the first work to explore supervised learning many factor significant in influencing and guiding classification remain unexplored in this paper we consider these factor by first motivating the use of a supervised framework through a careful investigation of issue such a network observational period generality of existing method variance reduction topological cause and degree of imbalance and sampling approach we also present an effective flow based predicting algorithm offer formal bound on imbalance in sparse network link prediction and employ an evaluation method appropriate for the observed imbalance our careful consideration of the above issue ultimately lead to a completely general framework that outperforms unsupervised link prediction method by more than auc 
we have developed a new linear support vector machine svm training algorithm called oca it computational effort scale linearly with the sample size in an extensive empirical evaluation oca significantly outperforms current state of the art svm solver like svmlight svmperf and bmrm achieving speedup of over on some datasets over svmlight and over svmperf while obtaining the same precise support vector solution oca even in the early optimization step show often faster convergence than the so far in this domain prevailing approximative method sgd and pegasos effectively parallelizing oca we were able to train on a dataset of size million example itself about gb in size in just second a competing string kernel svm required second to train on million example sub sampled from this dataset 
relative to the large literature on upper bound on complexity of convex optimization lesser attention ha been paid to the fundamental hardness of these problem given the extensive use of convex optimization in machine learning and statistic gaining an understanding of these complexity theoretic issue is important in this paper we study the complexity of stochastic convex optimization in an oracle model of computation we improve upon known result and obtain tight minimax complexity estimate for various function class 
learning to rank from relevance judgment is an active research area itemwise score regression pairwise preference satisfaction and listwise structured learning are the major technique in use listwise structured learning ha been applied recently to optimize important non decomposable ranking criterion like auc area under roc curve and map mean average precision we propose new almost linear time algorithm to optimize for two other criterion widely used to evaluate search system mrr mean reciprocal rank and ndcg normalized discounted cumulative gain in the max margin structured learning framework we also demonstrate that for different ranking criterion one may need to use different feature map search application should not be optimized in favor of a single criterion because they need to cater to a variety of query e g mrr is best for navigational query while ndcg is best for informational query a key contribution of this paper is to fold multiple ranking loss function into a multi criterion max margin optimization the result is a single robust ranking model that is close to the best accuracy of learner trained on individual criterion in fact experiment over the popular letor and trec data set show that contrary to conventional wisdom a test criterion is often not best served by training with the same individual criterion 
conjoint analysis is one of the most popular market research methodology for assessing how customer with heterogeneous preference appraise various objective characteristic in product or service which provides critical input for many marketing decision e g optimal design of new product and target market selection nowadays it becomes practical in e commercial application to collect million of sample quickly however the large scale data set make traditional conjoint analysis coupled with sophisticated monte carlo simulation for parameter estimation computationally prohibitive in this paper we report a successful large scale case study of conjoint analysis on click through stream in a real world application at yahoo we consider identifying user heterogenous preference from million of click view event and building predictive model to classify new user into segment of distinct behavior pattern a scalable conjoint analysis technique known a tensor segmentation is developed by utilizing logistic tensor regression in standard partworth framework for solution in offline analysis on the sample collected from a random bucket of yahoo front page today module we compare tensor segmentation against other segmentation scheme using demographic information and study user preference on article content within tensor segment our knowledge acquired in the segmentation result also provides assistance to editor in content management and user targeting the usefulness of our approach is further verified by the observation in a bucket test launched in dec 
compressing social network can substantially facilitate mining and advanced analysis of large social network preferably social network should be compressed in a way that they still can be queried efficiently without decompression arguably neighbor query which search for all neighbor of a query vertex are the most essential operation on social network can we compress social network effectively in a neighbor query friendly manner that is neighbor query still can be answered in sublinear time using the compression in this paper we develop an effective social network compression approach achieved by a novel eulerian data structure using multi position linearizations of directed graph our method come with a nontrivial theoretical bound on the compression rate to the best of our knowledge our approach is the first that can answer both out neighbor and in neighbor query in sublinear time an extensive empirical study on more than a dozen benchmark real data set verifies our design 
we show how variational bayesian inference can be implemented for very large generalized linear model our relaxation is proven to be a convex problem for any log concave model we provide a generic double loop algorithm for solving this relaxation on model with arbitrary super gaussian potential by iteratively decoupling the criterion most of the work can be done by solving large linear system rendering our algorithm order of magnitude faster than previously proposed solver for the same problem we evaluate our method on problem of bayesian active learning for large binary classification model and show how to address setting with many candidate and sequential inclusion step 
heavy tailed distribution naturally occur in many real life problem unfortunately it is typically not possible to compute inference in closed form in graphical model which involve such heavy tailed distribution in this work we propose a novel simple linear graphical model for independent latent random variable called linear characteristic model lcm defined in the characteristic function domain using stable distribution a heavy tailed family of distribution which is a generalization of cauchy l evy and gaussian distribution we show for the first time how to compute both exact and approximate inference in such a linear multivariate graphical model lcm are not limited to stable distribution in fact lcm are always defined for any random variable discrete continuous or a mixture of both we provide a realistic problem from the field of computer network to demonstrate the applicability of our construction other potential application is iterative decoding of linear channel with non gaussian noise 
group lasso estimator useful in many application suffer from lack of meaningful variance estimate for regression coefficient to overcome such problem we propose a full bayesian treatment of the group lasso extending the standard bayesian lasso using hierarchical expansion the method is then applied to poisson model for contingency table using a highly efficient mcmc algorithm the simulated experiment validate the performance of this method on artificial datasets with known ground truth when applied to a breast cancer dataset the method demonstrates the capability of identifying the difference in interaction pattern of marker protein between different patient group 
we have proposed the deep structured conditional random field crfs for sequential labeling and classification recently the core of this model is it deep structure and it discriminative nature this paper outline the learning strategy and algorithm we have developed for the deep structured crfs with a focus on the new strategy that combine the layer wise unsupervised pre training using entropy based multi objective optimization and the conditional likelihood based back propagation fine tuning a inspired by the recent development in learning deep belief network 
the central challenge in temporal data analysis is to obtain knowledge about it underlying dynamic in this paper we address the observation of noisy stochastic process and attempt to detect temporal segment that are related to inconsistency and irregularity in it dynamic many conventional anomaly detection approach detect anomaly based on the distance between pattern and often provide only limited intuition about the generative process of the anomaly meanwhile model based approach have difficulty in identifying a small clustered set of anomaly we propose information theoretic meta clustering itmc a formalization of model based clustering principled by the theory of lossy data compression itmc identifies a unique cluster whose distribution diverges significantly from the entire dataset furthermore itmc employ a regularization term derived from the preference for high compression rate which is critical to the precision of detection for empirical evaluation we apply itmc to two temporal anomaly detection task datasets are taken from generative process involving heterogeneous and inconsistent dynamic a comparison to baseline method show that the proposed algorithm detects segment from irregular state with significantly high precision and recall 
we provide statistical performance guarantee for a recently introduced kernel classifier that optimizes the l or integrated squared error ise of a difference of density the classifier is similar to a support vector machine svm in that it is the solution of a quadratic program and yield a sparse classifier unlike svms however the l kernel classifier doe not involve a regularization parameter we prove a distribution free concentration inequality for a cross validation based estimate of the ise and apply this result to deduce an oracle inequality and consistency of the classifier on the sense of both ise and probability of error our result also specialize to give performance guarantee for an existing method of l kernel density estimation 
embeddings of random variable in reproducing kernel hilbert space rkhss may be used to conduct statistical inference based on higher order moment for sufficiently rich characteristic rkhss each probability distribution ha a unique embedding allowing all statistical property of the distribution to be taken into consideration necessary and sufficient condition for an rkhs to be characteristic exist for rn in the present work condition are established for an rkhs to be characteristic on group and semigroups illustrative example are provided including characteristic kernel on periodic domain rotation matrix andrn 
we consider the problem of learning the structure of ising model pairwise binary markov random field from i i d sample while several method have been proposed to accomplish this task their relative merit and limitation remain somewhat obscure by analyzing a number of concrete example we show that low complexity algorithm systematically fail when the markov random field develops long range correlation more precisely this phenomenon appears to be related to the ising model phase transition although it doe not coincide with it 
many human interaction involve piece of information being passed from one person to another raising the question of how this process of information transmission is affected by the capacity of the agent involved in the s sir frederic bartlett explored the influence of memory bias in serial reproduction of information in which one person s reconstruction of a st imulus from memory becomes the stimulus seen by the next person these experiment were done using relatively uncontrolled stimulus such a picture and st ories but suggested that serial reproduction would transform information in a way that reflected the bias inherent in memory we formally analyze serial reproduction using a bayesian model of reconstruction from memory giving a general result characterizing the effect of memory bias on information transmission we then test the prediction of this account in two experiment using simple one dimensional stimulus our result provide theoretical and empirical justificatio n for the idea that serial reproduction reflects memory bias 
we present a correlated bigram lsa approach for unsupervised lm adaptation for automatic speech recognition the model is trained using efficient variational em and smoothed using the proposed fractional kneser ney smoothing which handle fractional count we address the scalability issue to large training corpus via bootstrapping of bigram lsa from unigram lsa for lm adaptation unigram and bigram lsa are integrated into the background n gram lm via marginal adaptation and linear interpolation respectively experimental result on the mandarin rt test set show that applying unigram and bigram lsa together yield relative perplexity reduction and relative character error rate reduction which is statistically significant compared to applying only unigram lsa on the large scale evaluation on arabic word error rate reduction from bigram lsa is statistically significant compared to the unadapted baseline 
system biology ha made massive stride in recent year with capability to model complex system including cell division stress response energy metabolism and signaling pathway concomitant with their improved modeling capability however such biochemical network model have also become notoriously complex for human to comprehend we propose network comprehension a a key problem for the kdd community where the goal is to create explainable representation of complex biological network we formulate this problem a one of extracting temporal signature from multi variate time series data where the signature are composed of ordinal comparison between time series component we show how such signature can be inferred by formulating the data mining problem a one of feature selection in rank order space we propose five new feature selection strategy for rank order space and ass their selective superiority experimental result on budding yeast cell cycle model demonstrate compelling result comparable to human interpretation of the cell cycle 
the drosophila gene expression pattern image document the spatial and temporal dynamic of gene expression and they are valuable tool for explicating the gene function interaction and network during drosophila embryogenesis to provide text based pattern searching the image in the berkeley drosophila genome project bdgp study are annotated with ontology term manually by human curator we present a systematic approach for automating this task because the number of image needing text description is now rapidly increasing we consider both improved feature representation and novel learning formulation to boost the annotation performance for feature representation we adapt the bag of word scheme commonly used in visual recognition problem so that the image group information in the bdgp study is retained moreover image from multiple view can be integrated naturally in this representation to reduce the quantization error caused by the bag of word representation we propose an improved feature representation scheme based on the sparse learning technique in the design of learning formulation we propose a local regularization framework that can incorporate the correlation among term explicitly we further show that the resulting optimization problem admits an analytical solution experimental result show that the representation based on sparse learning outperforms the bag of word representation significantly result also show that incorporation of the term term correlation improves the annotation performance consistently 
dimension reduction is popular for learning predictive model in high dimensional space it can highlight the relevant part of the feature space and avoid the curse of dimensionality however it can also be harmful because any reduction loses information in this paper we propose the projection penalty framework to make use of dimension reduction without losing valuable information reducing the feature space before learning predictive model can be viewed a restricting the model search to some parameter subspace the idea of projection penalty is that instead of restricting the search to a parameter subspace we can search in the full space but penalize the projection distance to this subspace dimension reduction is used to guide the search rather than to restrict it we propose projection penalty for linear dimension reduction and then generalize to kernel based reduction and other nonlinear method we test projection penalty with various dimension reduction technique in different prediction task including principal component regression and partial least square in regression task kernel dimension reduction in face recognition and latent topic modeling in text classification experimental result show that projection penalty are a more effective and reliable way to make use of dimension reduction technique 
scientist frequently have multiple type of experiment and data set on which they can test the validity of their parameterized model and locate plausible region for the model parameter by examining multiple data set scientist can obtain inference which typically are much more informative than the deduction derived from each of the data source independently several standard data combination technique result in target function which are a weighted sum of the observed data source thus computing constraint on the plausible region of the model parameter space can be formulated a finding a level set of a target function which is the sum of observable function we propose an active learning algorithm for this problem which selects both a sample from the parameter space and an observable function upon which to compute the next sample empirical test on synthetic function and on real data for an eight parameter cosmological model show that our algorithm significantly reduces the number of sample required to identify the desired level set 
in this paper we explore an application of basis pursuit to audio scene analysis the goal of our work is to detect when certain sound are present in a mixed audio signal we focus on the regime where out of a large number of possible source a small but unknown number combine and overlap to yield the observed signal to infer which sound are present we decompose the observed signal a a linear combination of a small number of active source we cast the inference a a regularized form of linear regression whose sparse solution yield decomposition with few active source we characterize the acoustic variability of individual source by autoregressive model of their time domain waveform when we do not have prior knowledge of the individual source the coefficient of these autoregressive model must be learned from audio example we analyze the dynamical stability of these model and show how to estimate stable model by substituting a simple convex optimization for a difficult eigenvalue problem we demonstrate our approach by learning dictionary of musical note and using these dictionary to analyze polyphonic recording of piano cello and violin 
the increasing availability of large scale location trace creates unprecedent opportunity to change the paradigm for knowledge discovery in transportation system a particularly promising area is to extract energy efficient transportation pattern green knowledge which can be used a guidance for reducing inefficiency in energy consumption of transportation sector however extracting green knowledge from location trace is not a trivial task conventional data analysis tool are usually not customized for handling the massive quantity complex dynamic and distributed nature of location trace to that end in this paper we provide a focused study of extracting energy efficient transportation pattern from location trace specifically we have the initial focus on a sequence of mobile recommendation a a case study we develop a mobile recommender system which ha the ability in recommending a sequence of pick up point for taxi driver or a sequence of potential parking position the goal of this mobile recommendation system is to maximize the probability of business success along this line we provide a potential travel distance ptd function for evaluating each candidate sequence this ptd function posse a monotone property which can be used to effectively prune the search space based on this ptd function we develop two algorithm lcp and skyroute for finding the recommended route finally experimental result show that the proposed system can provide effective mobile sequential recommendation and the knowledge extracted from location trace can be used for coaching driver and leading to the efficient use of energy 
this work introduces a new family of link based dissimilarity measure between node of a weighted directed graph this measure called the randomized shortest path rsp dissimilarity depends on a parameter and ha the interesting property of reducing on one end to the standard shortest path distance when is large and on the other end to the commute time or resistance distance when is small near zero intuitively it corresponds to the expected cost incurred by a random walker in order to reach a destination node from a starting node while maintaining a constant entropy related to spread in the graph the parameter is therefore biasing gradually the simple random walk on the graph towards the shortest path policy by adopting a statistical physic approach and computing a sum over all the possible path discrete path integral it is shown that the rsp dissimilarity from every node to a particular node of interest can be computed efficiently by solving two linear system of n equation where n is the number of node on the other hand the dissimilarity between every couple of node is obtained by inverting an n x n matrix the proposed measure can be used for various graph mining task such a computing betweenness centrality finding dense community etc a shown in the experimental section 
search log which contain rich and up to date information about user need and preference have become a critical data source for search engine recently more and more data driven application are being developed in search engine based on search log such a query suggestion keyword bidding and dissatisfactory query analysis in this paper by observing that many data driven application in search engine highly rely on online mining of search log we develop an olap system on search log which serf a an infrastructure supporting various data driven application an empirical study using real data of over two billion query session demonstrates the usefulness and feasibility of our design 
effective diagnosis of alzheimer s disease ad the most common type of dementia in elderly patient is of primary importance in biomedical research recent study have demonstrated that ad is closely related to the structure change of the brain network i e the connectivity among different brain region the connectivity pattern will provide useful imaging based biomarkers to distinguish normal control nc patient with mild cognitive impairment mci and patient with ad in this paper we investigate the sparse inverse covariance estimation technique for identifying the connectivity among different brain region in particular a novel algorithm based on the block coordinate descent approach is proposed for the direct estimation of the inverse covariance matrix one appealing feature of the proposed algorithm is that it allows the user feedback e g prior domain knowledge to be incorporated into the estimation process while the connectivity pattern can be discovered automatically we apply the proposed algorithm to a collection of fdg pet image from nc mci and ad subject our experimental result demonstrate that the proposed algorithm is promising in revealing the brain region connectivity difference among these group 
clustering validation is a long standing challenge in the clustering literature while many validation measure have been developed for evaluating the performance of clustering algorithm these measure often provide inconsistent information about the clustering performance and the best suitable measure to use in practice remain unknown this paper thus fill this crucial void by giving an organized study of external validation measure for k mean clustering specifically we first introduce the importance of measure normalization in the evaluation of the clustering performance on data with imbalanced class distribution we also provide normalization solution for several measure in addition we summarize the major property of these external measure these property can serve a the guidance for the selection of validation measure in different application scenario finally we reveal the interrelationship among these external measure by mathematical transformation we show that some validation measure are equivalent also some measure have consistent validation performance most importantly we provide a guide line to select the most suitable validation measure for k mean clustering 
many different topic model have been used successfully for a variety of application however even state of the art topic model suffer from the important flaw that they do not capture the tendency of word to appear in burst it is a fundamental property of language that if a word is used once in a document it is more likely to be used again we introduce a topic model that us dirichlet compound multinomial dcm distribution to model this burstiness phenomenon on both text and non text datasets the new model achieves better held out likelihood than standard latent dirichlet allocation lda it is straightforward to incorporate the dcm extension into topic model that are more complex than lda 
the problem of graph classification ha attracted great interest in the last decade current research on graph classification assumes the existence of large amount of labeled training graph however in many application the label of graph data are very expensive or difficult to obtain while there are often copious amount of unlabeled graph data available in this paper we study the problem of semi supervised feature selection for graph classification and propose a novel solution called gssc to efficiently search for optimal subgraph feature with labeled and unlabeled graph different from existing feature selection method in vector space which assume the feature set is given we perform semi supervised feature selection for graph data in a progressive way together with the subgraph feature mining process we derive a feature evaluation criterion named gsemi to estimate the usefulness of subgraph feature based upon both labeled and unlabeled graph then we propose a branch and bound algorithm to efficiently search for optimal subgraph feature by judiciously pruning the subgraph search space empirical study on several real world task demonstrate that our semi supervised feature selection approach can effectively boost graph classification performance with semi supervised feature selection and is very efficient by pruning the subgraph search space using both labeled and unlabeled graph 
advance in data collection and storage capacity have made it increasingly possible to collect highly volatile graph data for analysis existing graph analysis technique are not appropriate for such data especially in case where streaming or near real time result are required an example that ha drawn significant research interest is the cyber security domain where internet communication trace are collected and real time discovery of event behavior pattern and anomaly is desired we propose metricforensics a scalable framework for analysis of volatile graph metricforensics combine a multi level drill down approach a collection of user selected graph metric and a collection of analysis technique at each successive level more sophisticated metric are computed and the graph is viewed at finer temporal resolution in this way metricforensics scale to highly volatile graph by only allocating resource for computationally expensive analysis when an interesting event is discovered at a coarser resolution first we test metricforensics on three real world graph an enterprise ip trace a trace of legitimate and malicious network traffic from a research institution and the mit reality mining proximity sensor data our largest graph ha m vertex and m edge spanning day the result demonstrate the scalability and capability of metricforensics in analyzing volatile graph and highlight four novel phenomenon in such graph elbow broken correlation prolonged spike and lightweight star 
we consider the general widely applicable problem of selecting from n real valued random variable a subset of size m of those with the highest mean based on a few sample a possible this problem which we denote explore m is a core aspect in several stochastic optimization algorithm and application of simulation and industrial engineering the theoretical basis for our work is an extension of a previous formulation using multi armed bandit that is devoted to identifying just the one best of n random variable explore in addition to providing pac bound for the general case we tailor our theoretically grounded approach to work efficiently in practice empirical comparison of the resulting sampling algorithm against stateof the art subset selection strategy demonstrate significant gain in sample efficiency 
in multi label learning each training example is associated with a set of label and the task is to predict the proper label set for the unseen example due to the tremendous exponential number of possible label set the task of learning from multi label example is rather challenging therefore the key to successful multi label learning is how to effectively exploit correlation between different label to facilitate the learning process in this paper we propose to use a bayesian network structure to efficiently encode the conditional dependency of the label a well a the feature set with the feature set a the common parent of all label to make it practical we give an approximate yet efficient procedure to find such a network structure with the help of this network multi label learning is decomposed into a series of single label classification problem where a classifier is constructed for each label by incorporating it parental label a additional feature label set of unseen example are predicted recursively according to the label ordering given by the network extensive experiment on a broad range of data set validate the effectiveness of our approach against other well established method 
we propose a nonparametric extension to the factor analysis problem using a beta process prior this beta process factor analysis bp fa model allows for a dataset to be decomposed into a linear combination of a sparse set of factor providing information on the underlying structure of the observation a with the dirichlet process the beta process is a fully bayesian conjugate prior which allows for analytical posterior calculation and straightforward inference we derive a varia tional bayes inference algorithm and demonstrate the model on the mnist digit and hgdp ceph cell line panel datasets 
we consider the problem of embedding arbitrary object e g image audio document into euclidean space subject to a partial order over pair wise distance partial order constraint arise naturally when modeling human perception of similarity our partial order framework enables the use of graph theoretic tool to more efficiently produce the embedding and exploit global structure within the constraint set we present an embedding algorithm based on semidefinite programming which can be parameterized by multiple kernel to yield a unified space from heterogeneous feature 
t r v t v v p p t r p t 
we develop and evaluate an approach to causal modeling based on time series data collectively referred to a grouped graphical granger modeling method graphical granger modeling us graphical modeling technique on time series data and invokes the notion of granger causality to make assertion on causality among a potentially large number of time series variable through inference on time lagged effect the present paper proposes a novel enhancement to the graphical granger methodology by developing and applying family of regression method that are sensitive to group information among variable to leverage the group structure present in the lagged temporal variable according to the time series they belong to additionally we propose a new family of algorithm we call group boosting a an improved component of grouped graphical granger modeling over the existing regression method with grouped variable selection in the literature e g group lasso the introduction of group boosting method is primarily motivated by the need to deal with non linearity in the data we perform empirical evaluation to confirm the advantage of the grouped graphical granger method over the standard non grouped method a well a that specific to the method based on group boosting this advantage is also demonstrated for the real world application of gene regulatory network discovery from time course microarray data 
we consider multi armed bandit problem where the number of arm is larger than the possible number of experiment we make a stochastic assumption on the mean reward of a new selected arm which characterizes it probability of being a near optimal arm our assumption is weaker than in previous work we describe algorithm based on upper confidence bound appl ied to a restricted set of randomly selected arm and provide upper bound on the resulting expected regret we also derive a lower bound which match up to a logarithmic factor the upper bound in some case 
user generated information in online community ha been characterized with the mixture of a text stream and a network structure both changing over time a good example is a web blogging community with the daily blog post and a social network of blogger an important task of analyzing an online community is to observe and track the popular event or topic that evolve over time in the community existing approach usually focus on either the burstiness of topic or the evolution of network but ignoring the interplay between textual topic and network structure in this paper we formally define the problem of popular event tracking in online community pet focusing on the interplay between text and network we propose a novel statistical method that model the the popularity of event over time taking into consideration the burstiness of user interest information diffusion on the network structure and the evolution of textual topic specifically a gibbs random field is defined to model the influence of historic status and the dependency relationship in the graph thereafter a topic model generates the word in text content of the event regularized by the gibbs random field we prove that two classic model in information diffusion and text burstiness are special case of our model under certain situation empirical experiment with two different community and datasets i e twitter and dblp show that our approach is effective and outperforms existing approach 
the availability and the accuracy of the data dictate the success of a data mining application increasingly there is a need to resort to on line data collection to address the problem of data availability however participant in on line data collection application are naturally distrustful of the data collector a well a their peer respondent resulting in inaccurate data collected a the respondent refuse to provide truthful data in fear of collusion attack the current anonymity preserving solution for on line data collection are unable to adequately resist such attack in a scalable fashion in this paper we present an efficient anonymous data collection protocol for a malicious environment such a the internet the protocol employ cryptographic and random shuffling technique to preserve participant anonymity the proposed method is collusion resistant and guarantee that an attacker will be unable to breach an honest participant s anonymity unless she control all n participant in addition our method is efficient and achieved communication overhead reduction in comparison to the prior state of the art method 
we address the problem of estimating the ratio of two probability density function a k a the importance the importance value can be used for various succeeding task such a non stationarity adaptationor outlier detection in this paper we propose a new importance estimation method that ha a closed form solution the leave one out cross validation score can also be computed analytically therefore the proposed method is computationally very efficient and numerically stable we also elucidate theoretical property of the proposed method such a the convergence rate and approximation error bound numerical experiment show that the proposed method is comparable to the best existing method in accuracy while it is computationally more efficient than competing approach 
an important step for understanding the semantic content of text is the extraction of semantic relation between entity in natural language document automatic extraction technique have to be able to identify different version of the same relation which usually may be expressed in a great variety of way therefore these technique benefit from taking into account many syntactic and semantic feature especially parse tree generated by automatic sentence parser typed dependency parse tree are edge and node labeled parse tree whose label and topology contains valuable semantic clue this information can be exploited for relation extraction by the use of kernel over structured data for classification in this paper we present new tree kernel for relation extraction over typed dependency parse tree on a public benchmark data set we are able to demonstrate a significant improvement in term of relation extraction quality of our new kernel over other state of the art kernel 
offline handwriting recognition the transcription of image of handwritten text is an interesting task in that it combine computer vision with sequence learning in most system the two element are handled separately with sophisticated preprocessing technique used to extract the image feature and sequential model such a hmms used to provide the transcription by combining two recent innovation in neural network multidimensional recurrent neural network and connectionist temporal classification this paper introduces a globally trained offline handwriting recogniser that take raw pixel data a input unlike competing system it doe not require any alphabet specific preprocessing and can therefore be used unchanged for any language evidence of it generality and power is provided by data from a recent international arabic recognition competition where it outperformed all entry accuracy compared to for the competition winner despite the fact that neither author understands a word of arabic 
we introduce a new sequential algorithm for making robust prediction in the presence of changepoints unlike previous approach which focus on the problem of detecting and locating changepoints our algorithm focus on the problem of making prediction even when such change might be present we introduce nonstationary covariance function to be used in gaussian process prediction that model such change then proceed to demonstrate how to effectively manage the hyperparameters associated with those covariance function by using bayesian quadrature we can integrate out the hyperparameters allowing u to calculate the marginal predictive distribution furthermore if desired the posterior distribution over putative changepoint location can be calculated a a natural byproduct of our prediction algorithm 
we describe analyze and experiment with a new framework for empirical loss minimization with regularization our algorithmic framework alternate between two phase on each iteration we first perform an unconstrained gradient descent step we then cast and solve an instantaneous optimization problem that trade off minimization of a regularization term while keeping close proximity to the result of the first phase this yield a simple yet effective algorit hm for both batch penalized risk minimization and online learning furthermore the two phase approach enables sparse solution when used in conjunction with regularization function that promote sparsity such a we derive concrete and very simple algorithm for minimization of loss function with and regularization we also show how to construct efficient algorithm for mixed no rm q regularization we further extend the algorithm and give efficient imp lementations for very high dimensional data with sparsity we demonstrate the potential of the proposed framework in experiment with synthetic and natural datasets 
document classification present difficult challenge due to the sparsity and the high dimensionality of text data and to the complex semantics of the natural language the traditional document representation is a word based vector bag of word or bow where each dimension is associated with a term of the dictionary containing all the word that appear in the corpus although simple and commonly used this representation ha several limitation it is essential to embed semantic information and conceptual pattern in order to enhance the prediction capability of classification algorithm in this paper we overcome the shortage of the bow approach by embedding background knowledge derived from wikipedia into a semantic kernel which is then used to enrich the representation of document our empirical evaluation with real data set demonstrates that our approach successfully achieves improved classification accuracy with respect to the bow technique and to other recently developed method 
approximate linear programming alp is a reinforcement learning technique with nice theoretical property but it often performs poorly in practice we identify some reason for the poor quality of alp solution in problem where the approximation induces virtual loop we then introduce two method for improving solution quality one method roll out selected constraint of the alp guided by the dual information the second method is a relaxation of the alp based on external penalty method the latter method is applicable in domain in which rolling out constraint is impractical both approach show promising empirical result for simple benchmark problem a well a for a realistic blood inventory management problem 
surjectivity of linear projection between distribution family with fixed mean and covariance regardless of dimension is re derived by a new proof we further extend this property to distribution family that respect additional constraint such a symmetry unimodality and log concavity by combining our result with classic univariate inequality we provide new worst case analysis for natural risk criterion arising in classification optimization portfolio selection and markov decision process 
frequent subgraph mining ha been extensively studied on certain graph data however uncertainty are inherently accompanied with graph data in practice and there is very few work on mining uncertain graph data this paper investigates frequent subgraph mining on uncertain graph under probabilistic semantics specifically a measure called frequent probability is introduced to evaluate the degree of recurrence of subgraphs given a set of uncertain graph and two number s with probability at least s where s is the number of edge of s in addition it is thoroughly discussed how to set to guarantee the overall approximation quality of the algorithm the extensive experiment on real uncertain graph data verify that the algorithm is efficient and that the mining result have very high quality 
conditional random field crfs are a class of undirected graphical model which have been widely used for classifying and labeling sequence data the training of crfs is typically formulated a an unconstrained optimization problem that maximizes the conditional likelihood however maximum likelihood training is prone to overfitting to address this issue we propose a novel constrained nonlinear optimization formulation in which the prediction accuracy of cross validation set are included a constraint instead of requiring multiple pass of training the constrained formulation allows the cross validation be handled in one pas of constrained optimization the new formulation is discontinuous and classical lagrangian based constraint handling method are not applicable a new constrained optimization algorithm based on the recently proposed extended saddle point theory is developed to learn the constrained crf model experimental result on gene and stock price prediction task show that the constrained formulation is able to significantly improve the generalization ability of crf training 
mining high utility itemsets from a transactional database refers to the discovery of itemsets with high utility like profit although a number of relevant approach have been proposed in recent year they incur the problem of producing a large number of candidate itemsets for high utility itemsets such a large number of candidate itemsets degrades the mining performance in term of execution time and space requirement the situation may become worse when the database contains lot of long transaction or long high utility itemsets in this paper we propose an efficient algorithm namely up growth utility pattern growth for mining high utility itemsets with a set of technique for pruning candidate itemsets the information of high utility itemsets is maintained in a special data structure named up tree utility pattern tree such that the candidate itemsets can be generated efficiently with only two scan of the database the performance of up growth wa evaluated in comparison with the state of the art algorithm on different type of datasets the experimental result show that up growth not only reduces the number of candidate effectively but also outperforms other algorithm substantially in term of execution time especially when the database contains lot of long transaction 
recent research in privacy preserving data mining ppdm ha become increasingly popular due to the wide application of data mining and the increased concern regarding the protection of private and personal information lately numerous method of privacy preserving data mining have been proposed most of these method are based on an assumption that semi honest is and collusion is not present in other word every party follows such protocol properly with the exception that it keep a record of all it intermediate computation without sharing the record with others in this paper we focus our attention on the problem of collusion in which some party may collude and share their record to deduce the private information of other party in particular we consider a general problem in ppdm multiparty secure computation of some function of secure summation of data spreading around multiple party to solve such a problem we propose a new method that entail a high level of security full privacy with this method no sensitive information of a party will be revealed even when all other party collude in addition this method is efficient with a running time of o m we will also show that by applying this general method a large number of problem in ppdm can be solved with enhanced security 
we present a nonparametric bayesian method for texture learning and synthesis a texture image is represented by a d hidden markov model dhmm where the hidden state correspond to the cluster labeling of textons and the transition matrix encodes their spatial layout the compatibility between adjacent textons the dhmm is coupled with the hierarchical dirichlet process hdp which allows the number of textons and the complexity of transition matrix grow a the input texture becomes irregular the hdp make use of dirichlet process prior which favor regular texture by penalizing the model complexity this framework hdp dhmm learns the texton vocabulary and their spatial layout jointly and automatically the hdp dhmm result in a compact representation of texture which allows fast texture synthesis with comparable rendering quality over the state of the art patch based rendering method we also show that the hdp dhmm can be applied to perform image segmentation and synthesis the preliminary result suggest that hdp dhmm is generally useful for further application in low level vision problem 
correlated or discriminative pattern mining is concerned with finding the highest scoring pattern w r t a correlation measure such a information gain by reinterpreting correlation measure in roc space and formulating correlated itemset mining a a constraint programming problem we obtain new theoretical insight with practical benefit more specifically we contribute an improved bound for correlated itemset miner a novel iterative pruning algorithm to exploit the bound and an adaptation of this algorithm to mine all itemsets on the convex hull in roc space the algorithm doe not depend on a minimal frequency threshold and is shown to outperform several alternative approach by order of magnitude both in runtime and in memory requirement 
online forum represent one type of social medium that is particularly rich for studying human behavior in information seeking and diffusing the way user join community is a reflection of the changing and expanding of their interest toward information in this paper we study the pattern of user participation behavior and the feature factor that influence such behavior on different forum datasets we find that despite the relative randomness and lesser commitment of structural relationship in online forum user community joining behavior display some strong regularity one particularly interesting observation is that the very weak relationship between user defined by online reply have similar diffusion curve a those of real friendship or co authorship we build social selection model bipartite markov random field bimrf to quantitatively evaluate the prediction performance of those feature factor and their relationship using these model we show that some feature carry supplementary information and the effectiveness of different feature vary in different type of forum moreover the result of bimrf with two star configuration suggest that the feature of user similarity defined by frequency of communication or number of common friend is inadequate to predict grouping behavior but adding node level feature can improve the fit of the model 
we describe a primal dual framework for the design and analysis of online strongly convex optimization algorithm our framework yield the tightest known logarithmic regret bound for follow the leader and for the gradient descent algorithm proposed in hazan et al we then show that one can interpolate between these two extreme case in particular we derive a new algorithm that share the computational simplicity of gradient descent but achieves lower regret in many practical situation finally we further extend our framework for generalized strongly convex function 
we tackle the fundamental problem of bayesian active learning with noise where we need to adaptively select from a number of expensive test in order to identify an unknown hypothesis sampled from a known prior distribution in the case of noise free observation a greedy algorithm called generalized binary search gb is known to perform near optimally we show that if the observation are noisy perhaps surprisingly gb can perform very poorly we develop ec a novel greedy active learning algorithm and prove that it is competitive with the optimal policy thus obtaining the first competitiveness guarantee for bayesian active learning with noisy observation our bound rely on a recently discovered diminishing return property called adaptive submodularity generalizing the classical notion of submodular set function to adaptive policy our result hold even if the test have non uniform cost and their noise is correlated we also propose effecxtive a particularly fast approximation of ec and evaluate it on a bayesian experimental design problem involving human subject intended to tease apart competing economic theory of how people make decision under uncertainty 
we relate compressed sensing c with bayesian experimental design and provide a novel efficient approximate method for the latter based on expectation propagation in a large comparative study about linearly measuring natural image we show that the simple standard heuristic of measuring wavelet coefficient top down systematically outperforms c method using random measurement the sequential projection optimisation approach of ji carin performs even worse we also show that our own approximate bayesian method is able to learn measurement filter on full image efficiently which outperform the wavelet heuristic to our knowledge ours is the first successful attempt at learning compressed sensing for image of realistic size in contrast to common c method our framework is not restricted to sparse signal but can readily be applied to other notion of signal complexity or noise model we give concrete idea how our method can be scaled up to large signal representation 
information network contains abundant knowledge about relationship among people or entity unfortunately such kind of knowledge is often hidden in a network where different kind of relationship are not explicitly categorized for example in a research publication network the advisor advisee relationship among researcher are hidden in the coauthor network discovery of those relationship can benefit many interesting application such a expert finding and research community analysis in this paper we take a computer science bibliographic network a an example to analyze the role of author and to discover the likely advisor advisee relationship in particular we propose a time constrained probabilistic factor graph model tpfg which take a research publication network a input and model the advisor advisee relationship mining problem using a jointly likelihood objective function we further design an efficient learning algorithm to optimize the objective function based on that our model suggests and rank probable advisor for every author experimental result show that the proposed approach infer advisor advisee relationship efficiently and achieves a state of the art accuracy we also apply the discovered advisor advisee relationship to bole search a specific expert finding task and empirical study show that the search performance can be effectively improved by ndcg 
we consider the exploration exploitation problem in reinforcement learning rl the bayesian approach to model based rl offer an elegant solution to this problem by considering a distribution over possible model and acting to maximize expected reward unfortunately the bayesian solution is intractable for all but very restricted case in this paper we present a simple algorithm and prove that with high probability it is able to perform close to the true intractable optimal bayesian policy after some small polynomial in quantity describing the system number of time step the algorithm and analysis are motivated by the so called pac mdp approach and extend such result into the setting of bayesian rl in this setting we show that we can achieve lower sample complexity bound than existing algorithm while using an exploration strategy that is much greedier than the extremely cautious exploration of pac mdp algorithm 
modern communication network generate massive volume of operational event data e g alarm alert and metric which can be used by a network management system nm to diagnose potential fault in this work we introduce a new class of indexable fault signature that encode temporal evolution of event generated by a network fault a well a topological relationship among the node where these event occur we present an efficient learning algorithm to extract such fault signature from noisy historical event data and with the help of novel space time indexing structure we show how to perform efficient online signature matching we provide result from extensive experimental study to explore the efficacy of our approach and point out potential application of such signature for many different type of network including social and information network 
it problem management call for quick identification of resolvers to reported problem the efficiency of this process highly depends on ticket routing transferring problem ticket among various expert group in search of the right resolver to the ticket to achieve efficient ticket routing wise decision need to be made at each step of ticket transfer to determine which expert group is likely to be or to lead to the resolver in this paper we address the possibility of improving ticket routing efficiency by mining ticket resolution sequence alone without accessing ticket content to demonstrate this possibility a markov model is developed to statistically capture the right decision that have been made toward problem resolution where the order of the markov model is carefully chosen according to the conditional entropy obtained from ticket data we also design a search algorithm called variable order multiple active state search vms that generates ticket transfer recommendation based on our model the proposed framework is evaluated on a large set of real world problem ticket the result demonstrate that vms significantly improves human decision problem resolvers can often be identified with fewer ticket transfer 
in some application such a filling in a customer information form on the web some missing value may not be explicitly represented a such but instead appear a potentially valid data value such missing value are known a disguised missing data which may impair the quality of data analysis severely the very limited previous study on cleaning disguised missing data highly rely on domain background knowledge in specific application and may not work well for the case where the disguise value are inliers recently we have studied the problem of cleaning disguised missing data systematically and proposed an effective heuristic approach in this paper we present a demonstration of dimac a disguised missing data cleaning tool which can find the frequently used disguise value in data set without any domain background knowledge in this demo we will show the critical technique of finding suspicious disguise value the architecture and user interface of dimac system an empirical case study on both real and synthetic data set which verifies the effectiveness and the efficiency of the technique and some challenge arising from real application and several direction for future work 
distributed learning is a problem of fundamental interest in machine learning and cognitive science in this paper we present asynchronous distributed learning algorithm for two well known unsupervised learning framework latent dirichlet allocation lda and hierarchical dirichlet process hdp in the proposed approach the data are distributed across p processor and processor independently perform gibbs sampling on their local data and communicate their information in a local asynchronous manner with other processor we demonstrate that our asynchronous algorithm are able to learn global topic model that are statistically a accurate a those learned by the standard l da and hdp sampler but with significant improvement in computation time and me mory we show speedup result on a million word text corpus using processor and we provide perplexity result for up to virtual processor s a a stepping stone in the development of asynchronous hdp a parallel hdp sampler is also introduced 
in this paper we introduce the meannn approach for estimation of main information theoretic measure such a differential entropy mutual information and divergence a opposed to other nonparametric approach the meannn result in smooth differentiable function of the data sample with clear geometrical interpretation then we apply the proposed estimator to the ica problem and obtain a smooth expression for the mutual information that can be analytically optimized by gradient descent method the improved performance of the proposed ica algorithm is demonstrated on several test example in comparison with state ofthe art technique 
approximate map inference in graphical model is an important and challenging problem for many domain including computer vision computational biology and natural language understanding current state of theart approach employ convex relaxation of these problem a surrogate objective but only provide weak running time guarantee in this paper we develop an approximate inference algorithm that is both ecient and ha strong theoretical guarantee specically our algorithm is guaranteed to converge to an accurate solution of the convex relaxation in o time we demonstrate our approach on synthetic and real world problem and show that it outperforms current stateof the art technique 
a major source of information often the most crucial and informative part in scholarly article from scientific journal proceeding and book are the figure that directly provide image and other graphical illustration of key experimental result and other scientific content in biological article a typical figure often comprises multiple panel accompanied by either scoped or global captioned text moreover the text in the caption contains important semantic entity such a protein name gene ontology tissue label etc relevant to the image in the figure due to the avalanche of biological literature in recent year and increasing popularity of various bio imaging technique automatic retrieval and summarization of biological information from literature figure ha emerged a a major unsolved challenge in computational knowledge extraction and management in the life science we present a new structured probabilistic topic model built on a realistic figure generation scheme to model the structurally annotated biological figure and we derive an efficient inference algorithm based on collapsed gibbs sampling for information retrieval and visualization the resulting program constitutes one of the key ir engine in our slif system that ha recently entered the final round out competing system of the elsevier grand challenge on knowledge enhancement in the life science here we present various evaluation on a number of data mining task to illustrate our method 
we develop a new algorithm based on em for learning the linear dynamical system model called the method of approximated second order statistic asos our approach achieves dramatically superior computational performance over standard em through it use of approximation which we justify with both intuitive explanation and rigorous convergence result in particular after an inexpensive precomputation phase the iteration of asos can be performed in time independent of the length of the training dataset 
this paper describes an unsupervised learning technique for modeling human locomotion style such a distinct related activity e g running and striding or variation of the same motion performed by different subject modeling motion style requires identifying the common structure in the motion and detecting style specific characteristic we propose an algorithm that learns a hierarchical model of style from unlabeled motion capture data by exploiting the cyclic property of human locomotion we assume that sequence with the same style contain locomotion cycle generated by noisy temporally warped version of a single latent cycle we model these style specific latent cycle a random variable drawn from a common parent cycle distribution representing the structure shared by all motion given these hierarchical prior the algorithm learns in a completely unsupervised fashion temporally aligned latent cycle distribution each modeling a specific locomotion style and computes for each example the style label posterior distribution the segmentation into cycle and the temporal warping with respect to the latent cycle we demonstrate the flexibility of the model on several application problem such a style clustering animation style blending and filling in of missing data 
we study pool based active learning in the presence of noise i e the agnostic setting previous work have shown that the effectiveness of agnostic active learning depends on the learning problem and the hypothesis space although there are many case on which active learning is very useful it is also easy to construct example that no active learning algorithm can have advantage in this paper we propose intuitively reasonable sufficient condition under which agnostic active learning algorithm is strictly superior to passive supervised learning we show that under some noise condition if the bayesian classification boundary and the underlying distribution are smooth to a finite order active learning achieves polynomial improvement in the label complexity if the boundary and the distribution are infinitely smooth the improvement is exponential 
p the principle by which spiking neuron contribute to the astounding computational power of generic cortical microcircuit and how spike timing dependent plasticity stdp of synaptic weight could generate and maintain this computational function are unknown we show here that stdp in conjunction with a stochastic soft winner take all wta circuit induces spiking neuron to generate through their synaptic weight implicit internal model for subclass or ldquo cause rdquo of the high dimensional spike pattern of hundred of pre synaptic neuron hence these neuron will fire after learning whenever the current input best match their internal model the resulting computational function of soft wta circuit a common network motif of cortical microcircuit could therefore be a drastic dimensionality reduction of information stream together with the autonomous creation of internal model for the probability distribution of their input pattern we show that the autonomous generation and maintenance of this computational function can be explained on the basis of rigorous mathematical principle in particular we show that stdp is able to approximate a stochastic online expectation maximization em algorithm for modeling the input data a corresponding result is shown for hebbian learning in artificial neural network p 
we present a prototype of an inductive database our system enables the user to query not only the data stored in the database but also generalization e g rule or tree over these data through the use of virtual mining view the mining view are relational table that virtually contain the complete output of data mining algorithm executed over a given dataset the prototype implemented into postgresql currently integrates frequent itemset association rule and decision tree mining we illustrate the interactive and iterative capability of our system with a description of a complete data mining scenario 
canonical correlation analysis cca is a well known technique for finding the correlation between two set of multi dimensional variable it project both set of variable into a lower dimensional space in which they are maximally correlated cca is commonly applied for supervised dimensionality reduction in which one of the multi dimensional variable is derived from the class label it ha been shown that cca can be formulated a a least square problem in the binaryclass case however their relationship in the more general setting remains unclear in this paper we show that under a mild condition which tends to hold for high dimensional data cca in multi label classification can be formulated a a least square problem based on this equivalence relationship we propose several cca extension including sparse cca using norm regularization experiment on multi label data set confirm the established equivalence relationship result also demonstrate the effectiveness of the proposed cca extension 
this paper is concerned with the consistency analysis on listwise ranking method among various ranking method the listwise method have competitive performance on benchmark datasets and are regarded a one of the state of the art approach most listwise ranking method manage to optimize ranking on the whole list permutation of object however in practical application such a information retrieval correct ranking at the top k position is much more important this paper aim to analyze whether existing listwise ranking method are statistically consistent in the top k setting for this purpose we define a top k ranking framework where the true loss and thus the risk are defined on the basis of top k subgroup of permutation this framework can include the permutation level ranking framework proposed in previous work a a special case based on the new framework we derive sufficient condition for a listwise ranking method to be consistent with the top k true loss and show an effective way of modifying the surrogate loss function in existing method to satisfy these condition experimental result show that after the modification the method can work significantly better than their original version 
the discovery of biclusters which denote group of item that show coherent value across a subset of all the transaction in a data set is an important type of analysis performed on real valued data set in various domain such a biology several algorithm have been proposed to find different type of biclusters in such data set however these algorithm are unable to search the space of all possible biclusters exhaustively pattern mining algorithm in association analysis also essentially produce biclusters a their result since the pattern consist of item that are supported by a subset of all the transaction however a major limitation of the numerous technique developed in association analysis is that they are only able to analyze data set with binary and or categorical variable and their application to real valued data set often involves some lossy transformation such a discretization or binarization of the attribute in this paper we propose a novel association analysis framework for exhaustively and efficiently mining range support pattern from such a data set on one hand this framework reduces the loss of information incurred by the binarizationand discretization based approach and on the other it enables the exhaustive discovery of coherent biclusters we compared the performance of our framework with two standard biclustering algorithm through the evaluation of the similarity of the cellular function of the gene constituting the pattern biclusters derived by these algorithm from microarray data these experiment show that the real valued pattern discovered by our framework are better enriched by small biologically interesting functional class also through specific example we demonstrate the ability of the rap framework to discover functionally enriched pattern that are not found by the commonly used biclustering algorithm isa the source code and data set used in this paper a well a the supplementary material are available at http www c umn edu vk gaurav rap 
structured model often achieve excellent performance but can be slow at test time we investigate structure compilation where we replace structure with feature which are often computationally simpler but unfortunately statistically more complex we analyze this tradeoff theoretically and empirically on three natural language processing task we also introduce a simple method to transfer predictive power from structure to feature via unlabeled data while incurring a minimal statistical penalty 
information network are widely used to characterize the relationship between data item such a text document many important retrieval and mining task rely on ranking the data item based on their centrality or prestige in the network beyond prestige diversity ha been recognized a a crucial objective in ranking aiming at providing a non redundant and high coverage piece of information in the top ranked result nevertheless existing network based ranking approach either disregard the concern of diversity or handle it with non optimized heuristic usually based on greedy vertex selection we propose a novel ranking algorithm divrank based on a reinforced random walk in an information network this model automatically balance the prestige and the diversity of the top ranked vertex in a principled way divrank not only ha a clear optimization explanation but also well connects to classical model in mathematics and network science we evaluate divrank using empirical experiment on three different network a well a a text summarization task divrank outperforms existing network based ranking method in term of enhancing diversity in prestige 
this paper introduces a novel multiagent learning algorithm convergence with model learning and safety or cmles in short which achieves convergence targeted optimality against memory bounded adversary and safety in arbitrary repeated game the most novel aspect of cmles is the manner in which it guarantee in a pac sense targeted optimality against memory bounded adversary via efficient exploration and exploitation cmles is fully implemented and we present empirical result demonstrating it effectiveness 
a new procedure for learning cost sensitive svm classifier is proposed the svm hinge loss is extended to the cost sensitive setting and the cost sensitive svm is derived a the minimizer of the associated risk the extension of the hinge loss draw on recent connection between risk minimization and probability elicitation these connection are generalized to costsensitive classification in a manner that guarantee consistency with the cost sensitive bayes risk and associated bayes decision rule this ensures that optimal decision rule under the new hinge loss implement the bayes optimal costsensitive classification boundary minimization of the new hinge loss is shown to be a generalization of the classic svm optimization problem and can be solved by identical procedure the resulting algorithm avoids the shortcoming of previous approach to cost sensitive svm design and ha superior experimental performance 
this paper present a novel wrapper based feature selection method for support vector regression svr using it probabilistic prediction the method computes the importance of a feature by aggregating the difference over the feature space of the conditional density function of the svr prediction with and without the feature a the exact computation of this importance measure is expensive two approximation are proposed the effectiveness of the measure using these approximation in comparison to several other existing feature selection method for svr is evaluated on both artificial and real world problem the result of the experiment show that the proposed method generally performs better and at least a well a the existing method with notable advantage when the data set is sparse 
we analyze the application of ensemble learning to recommender system on the netflix prize dataset for our analysis we use a set of diverse state of the art collaborative filtering cf algorithm which include svd neighborhood based approach restricted boltzmann machine asymmetric factor model and global effect we show that linearly combining blending a set of cf algorithm increase the accuracy and outperforms any single cf algorithm furthermore we show how to use ensemble method for blending predictor in order to outperform a single blending algorithm the dataset and the source code for the ensemble blending are available online 
given a real and weighted person to person network which change over time what can we say about the clique that it contains do the incident of communication or weight on the edge of a clique follow any pattern real and in person social network have many more triangle than chance would dictate a it turn out there are many more clique than one would expect in surprising pattern in this paper we study massive real world social network formed by direct contact among people through various personal communication service such a phone call sm im etc the contribution are the following a we discover surprising pattern with the clique b we report power law of the weight on the edge of clique c our real network follow these pattern such that we can trust them to spot outlier and finally d we propose the first utility driven graph generator for weighted time evolving network which match the observed pattern our study focused on three large datasets each of which is a different type of communication service with over one million record and span several month of activity 
the problem of ethnicity identification from name ha a variety of important application including biomedical research demographic study and marketing here we report on the development of an ethnicity classifier where all training data is extracted from public non confidential and hence somewhat unreliable source our classifier us hidden markov model hmms and decision tree to classify name into cultural ethnic group with individual group accuracy comparable accuracy to earlier binary e g spanish non spanish classifier we have applied this classifier to over million name from a large scale news corpus identifying interesting temporal and spatial trend on the representation of particular cultural ethnic group 
query suggestion play an important role in improving the usability of search engine although some recently proposed method can make meaningful query suggestion by mining query pattern from search log none of them are context aware they do not take into account the immediately preceding query a context in query suggestion in this paper we propose a novel context aware query suggestion approach which is in two step in the offine model learning step to address data sparseness query are summarized into concept by clustering a click through bipartite then from session data a concept sequence suffix tree is constructed a the query suggestion model in the online query suggestion step a user s search context is captured by mapping the query sequence submitted by the user to a sequence of concept by looking up the context in the concept sequence sufix tree our approach suggests query to the user in a context aware manner we test our approach on a large scale search log of a commercial search engine containing billion search query billion click and million query session the experimental result clearly show that our approach outperforms two baseline method in both coverage and quality of suggestion 
fast retrieval method are increasingly critical for many l arge scale analysis task and there have been several recent method that attempt to learn hash function for fast and accurate nearest neighbor search in this paper we develop an algorithm for learning hash function based on explicitly minimizing the reconstruction error between the original distance and the hamming distance of the corresponding binary embeddings we develop a scalable coordinate descent algorithm for our proposed hashing objective that is able to efficiently learn hash function in a variety of setting unlike existing method such a semantic hashing and spectral hashing our method is easily kernelized and doe not require restrictive assumption about the underlying distribution of the data we present result over several domain to demonstrate that our method outperforms existing state of the art technique 
many interesting problem including bayesian network structure search can be cast in term of finding the optimum value of a function over the space of graph however this function is often expensive to compute exactly we here present a method derived from the study of reproducingkernel hilbert space which take advantage of the regular structure of the space of all graph on a fixed number of node to obtain approximation to the desired function quickly and with reasonable accuracy we then test this method on both a small testing set and a real world bayesian network the result suggest that not only is this method reasonably accurate but that the bde score itself varies quadratically over the space of all graph 
query expansion is a long studied approach for improving retrieval effectiveness by enhancing the user s original query with additional rela ted word current algorithm for automatic query expansion can often improve retrieval accuracy on average but are not robust that is they are highly unstable and have poor worst case performance for individual query to address this problem we introduce a novel formulation of query expansion a a convex optimization problem over a word graph the model combine initial weight from a baseline feedback algorithm with edge weight based on word similarity and integrates simple constraint to enforce set based criterion such a aspect ba lance aspect coverage and term centrality result across multiple standard test collection show consistent and significant reduction in the number and magnitude o f expansion failure while retaining the strong positive gain of the baseline al gorithm our approach doe not assume a particular retrieval model making it appl icable to a broad class of existing expansion algorithm 
we propose a novel application of formal concept analysis fca to neural decoding instead of just trying to figure out which stimulus wa presented we demonstrate how to explore the semantic relationship in the neural representation of large set of stimulus fca provides a way of displaying and interpreting such relationship via concept lattice we explore the effect of neural code sparsity on the lattice we then analyze neurophysiological data from high level visual cortical area stsa using an exact bayesian approach to construct the formal context needed by fca prominent feature of the resulting concept lattice are discussed including hierarchical face representation and indication for a product of expert code in real neuron 
this paper present a new algorithm for sequence prediction over long categorical event stream the input to the algorithm is a set of target event type whose occurrence we wish to predict the algorithm examines window of event that precede occurrence of the target event type in historical data the set of significant frequent episode associated with each target event type is obtained based on formal connection between frequent episode and hidden markov model hmms each significant episode is associated with a specialized hmm and a mixture of such hmms is estimated for every target event type the likelihood of the current window of event under these mixture model are used to predict future occurrence of target event in the data the only user defined model parameter in the algorithm is the length of the window of event used during model estimation we first evaluate the algorithm on synthetic data that wa generated by embedding in varying level of noise pattern which are preselected to characterize occurrence of target event we then present an application of the algorithm for predicting targeted user behavior from large volume of anonymous search session interaction log from a commercially deployed web browser tool bar 
clustering data in high dimension is believed to be a hard problem in general a number of efficient clustering algorithm developed in recent year address this problem by projecting the data into a lower dimensional subspace e g via principal component analysis pca or random projection before clustering here we consider constructing such projection using multiple view of the data via canonical correlation analysis cca under the assumption that the view are un correlated given the cluster label we show that the separation condition required for the algorithm to be successful are significantly weaker than prior result in the literature we provide result for mixture of gaussians and mixture of log concave distribution we also provide empirical support from audio visual speaker clustering where we desire the cluster to correspond to speaker id and from hierarchical wikipedia document clustering where one view is the word in the document and the other is the link structure 
in this article we describe a visual analytic tool for the interrogation of evolving interaction network data such a those found in social bibliometric www and biological application the tool we have developed incorporates common visualization paradigm such a zooming coarsening and filtering while naturally integrating information extracted by a previously described event driven framework for characterizing the evolution of such network the visual front end provides feature that are specifically useful in the analysis of interaction network capturing the dynamic nature of both individual entity a well a interaction among them the tool provides the user with the option of selecting multiple view designed to capture different aspect of the evolving graph from the perspective of a node a community or a subset of node of interest standard visual template and cue are used to highlight critical change that have occurred during the evolution of the network a key challenge we address in this work is that of scalability handling large graph both in term of the efficiency of the back end and in term of the efficiency of the visual layout and rendering two case study based on bibliometric and wikipedia data are presented to demonstrate the utility of the toolkit for visual knowledge discovery 
probabilistic frequent itemset mining in uncertain transaction database semantically and computationally differs from traditional technique applied to standard certain transaction database the consideration of existential uncertainty of item set indicating the probability that an item set occurs in a transaction make traditional technique inapplicable in this paper we introduce new probabilistic formulation of frequent itemsets based on possible world semantics in this probabilistic context an itemset x is called frequent if the probability that x occurs in at least minsup transaction is above a given threshold to the best of our knowledge this is the first approach addressing this problem under possible world semantics in consideration of the probabilistic formulation we present a framework which is able to solve the probabilistic frequent itemset mining pfim problem efficiently an extensive experimental evaluation investigates the impact of our proposed technique and show that our approach is order of magnitude faster than straight forward approach 
we develop an ecient learning framework to construct signal dictionary for sparse representation by selecting the dictionary column from multiple candidate base by sparse we mean that only a few dictionary element compared to the ambient signal dimension can exactly represent or well approximate the signal of interest we formulate both the selection of the dictionary column and the sparse representation of signal a a joint combinatorial optimization problem the proposed combinatorial objective maximizes variance reduction over the set of training signal by constraining the size of the dictionary a well a the number of dictionary column that can be used to represent each signal we show that if the available dictionary column vector are incoherent our objective function satises approximate submodularity we exploit this property to develop sdsomp and sdsma two greedy algorithm with approximation guarantee we also describe how our learning framework enables dictionary selection for structured sparse representation e g where the sparse coecients occur in restricted pattern we evaluate our approach on synthetic signal and natural image for representation and inpainting problem 
given a task t a pool of individual x with different skill and a social network g that capture the compatibility among these individual we study the problem of finding x a subset of x to perform the task we call this the team formation problem we require that member of x not only meet the skill requirement of the task but can also work effectively together a a team we measure effectiveness using the communication cost incurred by the subgraph in g that only involves x we study two variant of the problem for two different communication cost function and show that both variant are np hard we explore their connection with existing combinatorial problem and give novel algorithm for their solution to the best of our knowledge this is the first work to consider the team formation problem in the presence of a social network of individual experiment on the dblp dataset show that our framework work well in practice and give useful and intuitive result 
clustering stability is an increasingly popular family of m ethods for performing model selection in data clustering the basic idea is that th e chosen model should be stable under perturbation or resampling of the data despite being reasonably effective in practice these method are not well understood theoretically and present some difficulty in particular when the data is a sumed to be sampled from an underlying distribution the solution returned by the clustering algorithm will usually become more and more stable a the sample size increase this raise a potentially serious practical difficulty with these metho d because it mean there might be some hard to compute sample size beyond which clustering stability estimator break down and become unreliable in detecting th e most stable model in this paper we provide a set of general sufficient conditio n which ensure the reliability of clustering stability estimator in the larg e sample regime in contrast to previous work which concentrated on specific toy di stributions or specific idealized clustering framework here we make no such assumption we then exemplify how these condition apply to several important family of clustering algorithm such a maximum likelihood clustering certain type of kernel clustering and centroid based clustering with any bregman divergence in addition we explicitly derive the non trivial asymptotic behavior of these estimator for any framework satisfying our condition this may help u understand what is considered a stable model by these estimator at least fo r large enough sample 
a non parametric bayesian model is proposed for processing multiple image the analysis employ image feature and when present the word associated with accompanying annotation the model cluster the image into class and each image is segmented into a set of object also allowing the opportunity to assign a word to each object localized labeling each object is assumed to be represented a a heterogeneous mix of component with this realized via mixture model linking image feature to object type the number of image class number of object type and the characteristic of the object feature mixture model are inferred nonparametrically to constitute spatially contiguous object a new logistic stick breaking process is developed inference is performed efficiently via variational bayesian analysis with example result presented on two image database 
a new algorithm is proposed for a unsupervised learning of sparse representation from subsampled measurement and b estimating the parameter required for linearly reconstructing signal from the sparse code we verify that the new algorithm performs efficient data compression on par with the recent method of compressive sampling further we demonstrate that the algorithm performs robustly when stacked in several stage or when applied in undercomplete or overcomplete situation the new algorithm can explain how neural population in the brain that receive subsampled input through fiber bottleneck are able to form coherent response property 
this paper study global ranking problem by learning to rank method conventional learning to rank method are usually designed for local ranking in the sense that the ranking model is defined on a single object for example a document in information retrieval for many application this is a very loose approximation relation always exist between object and it is better to define the ranking model a a function on all the object to be ranked i e the relation are also included this paper refers to the problem a global ranking and proposes employing a continuous conditional random field crf for conducting the learning task the continuous crf model is defined a a conditional probability distribution over ranking score of object conditioned on the object it can naturally represent the content information of object a well a the relation information between object necessary for global ranking taking two specific information retrieval task a example the paper show how the continuous crf method can perform global ranking better than baseline 
the fused lasso penalty enforces sparsity in both the coefficient and their successive difference which is desirable for application with feature ordered in some meaningful way the resulting problem is however challenging to solve a the fused lasso penalty is both non smooth and non separable existing algorithm have high computational complexity and do not scale to large size problem in this paper we propose an efficient fused lasso algorithm efla for optimizing this class of problem one key building block in the proposed efla is the fused lasso signal approximator flsa to efficiently solve flsa we propose to reformulate it a the problem of finding an appropriate subgradient of the fused penalty at the minimizer and develop a subgradient finding algorithm sfa we further design a restart technique to accelerate the convergence of sfa by exploiting the special structure of both the original and the reformulated flsa problem our empirical evaluation show that both sfa and efla significantly outperform existing solver we also demonstrate several application of the fused lasso 
large relational factor graph with structure defined by first order logic or other language give rise to notoriously difficult inference problem because unrolling the structure necessary to represent distribution over all hypothesis ha exponential blow up solution are often derived from mcmc however because of limitation in the design and parameterization of the jump function these samplingbased method suffer from local minimum the system must transition through lower scoring configuration before arriving at a better map solution this paper present a new method of explicitly selecting fruitful downward jump by leveraging reinforcement learning rl rather than setting parameter to maximize the likelihood of the training data parameter of the factor graph are treated a a log linear function approximator and learned with method of temporal difference td map inference is performed by executing the resulting policy on held out test data our method allows efficient gradient update since only factor in the neighborhood of variable affected by an action need to be computed we bypass the need to compute marginals entirely our method yield dramatic empirical success producing new state of the art result on a complex joint model of ontology alignment with a reduction in error over state of the art in that domain 
identifying similar keywords known a broad match is an important task in online advertising that ha become a standard feature on all major keyword advertising platform effective broad matching lead to improvement in both relevance and monetization while increasing advertiser reach and making campaign management easier in this paper we present a learning based approach to broad matching that is based on exploiting implicit feedback in the form of advertisement clickthrough log our method can utilize arbitrary similarity function by incorporating them a feature we present an online learning algorithm amnesiac averaged perceptron that is highly efficient yet able to quickly adjust to the rapidly changing distribution of bidded keywords advertisement and user behavior experimental result obtained from historical log and live trial on a large scale advertising platform demonstrate the effectiveness of the proposed algorithm and the overall success of our approach in identifying high quality broad match mapping 
working memory is a central topic of cognitive neuroscience because it is critical for solving real world problem in which information from multiple temporally distant source must be combined to generate appropriate behavior however an often neglected fact is that learning to use working memory effectively is itself a difficult problem the gating framework is a collection of psychological model that show how dopamine can train the basal ganglion and prefrontal cortex to form useful working memory representation in certain type of problem we unite gating with machine learning theory concerning the general problem of memory based optimal control we present a normative model that learns by online te mporal difference method to use working memory to maximize discounted future reward in partially observable setting the model successfully solves a benchmark working memory problem and exhibit limitation similar to those observed in human our purpose is to introduce a concise normative definition of high level cognitive concept such a working memory and cognitive control in term of maximizing discounted future reward in trod u cti on 
an ensemble is a set of learned model that make decision collectively although an ensemble is usually more accurate than a single learner existing ensemble method often tend to construct unnecessarily large ensemble which increase the memory consumption and computational cost ensemble pruning tackle this problem by selecting a subset of ensemble member to form subensembles that are subject to le resource consumption and response time with accuracy that is similar to or better than the original ensemble in this paper we analyze the accuracy diversity trade off and prove that classifier that are more accurate and make more prediction in the minority group are more important for subensemble construction based on the gained insight a heuristic metric that considers both accuracy and diversity is proposed to explicitly evaluate each individual classifier s contribution to the whole ensemble by incorporating ensemble member in decreasing order of their contribution subensembles are formed such that user can select the top p percent of ensemble member depending on their resource availability and tolerable waiting time for prediction experimental result on uci data set show that subensembles formed by the proposed epic ensemble pruning via individual contribution ordering algorithm outperform the original ensemble and a state of the art ensemble pruning method orientation ordering oo 
learning to rank is becoming an increasingly popular research area in machine learning the ranking problem aim to induce an ordering or preference relation among a set of instance in the input space however collecting labeled data is growing into a burden in many rank application since labeling requires eliciting the relative ordering over the set of alternative in this paper we propose a novel active learning framework for svm based and boosting based rank learning our approach suggests sampling based on maximizing the estimated loss differential over unlabeled data experimental result on two benchmark corpus show that the proposed model substantially reduces the labeling effort and achieves superior performance rapidly with a much a relative improvement over the margin based sampling baseline 
we study the problem of segmenting specific white matter structure of interest from diffusion tensor dt mr image of the human brain this is an important requirement in many neuroimaging study for instance to evaluate whether a brain structure exhibit group level difference a a function of disease in a set of image typically interactive expert guided segmentation ha been the method of choice for such application but this is tedious for large datasets common today to address this problem we endow an image segmentation algorithm with advice encoding some global characteristic of the region s we want to extract this is accomplished by constructing using expert segmented image an epitome of a specific region a a histogram over a bag of word e g suitable feature descriptor now given such a representation the problem reduces to segmenting a new brain image with additional constraint that enforce consistency between the segmented foreground and the pre specified histogram over feature we present combinatorial approximation algorithm to incorporate such domain specific constraint for markov random field mrf segmentation making use of recent result on image co segmentation we derive effective solution strategy for our problem we provide an analysis of solution quality and present promising experimental evidence showing that many structure of interest in neuroscience can be extracted reliably from d brain image volume using our algorithm 
ranking a set of retrieved document according to their relevance to a query is a popular problem in information retrieval method that learn ranking function are difficult to optimize a ranking performance is typically judged by metric that are not smooth in this paper we propose a new listwise approach to learning to rank our method creates a conditional probability distribution over ranking assigned to document for a given query which permit gradient ascent optimization of the expected value of some performance measure the rank probability take the form of a boltzmann distribution based on an energy function that depends on a scoring function composed of individual and pairwise potential including pairwise potential is a novel contribution allowing the model to encode regularity in the relative score of document existing model assign score at test time based only on individual document with no pairwise constraint between document experimental result on the letor data set show that our method out performs existing learning approach to ranking 
there are well known algorithm for learning the structure of directed and undirected graphical model from data but nearly all assume that the data consists of a single i i d sample in context such a fmri analysis data may consist of an ensemble of independent sample from a common data generating mechanism which may not have identical distribution pooling such data can result in a number of well known statistical problem so each sample must be analyzed individually which offer no increase in power due to the presence of multiple sample we show how existing constraint based method can be modified to learn structure from the aggregate of such data in a statistically sound manner the prescribed method is simple to implement and based on existing statistical method employed in metaanalysis and other area but work surprisingly well in this context where there are increased concern due to issue such a retesting we report result for directed model but the method given is just a applicable to undirected model 
dyadic data arises in many real world application such a social network analysis and information retrieval in order to discover the underlying or hidden structure in the dyadic data many topic modeling technique were proposed the typical algorithm include probabilistic latent semantic analysis plsa and latent dirichlet allocation lda the probability density function obtained by both of these two algorithm are supported on the euclidean space however many previous study have shown naturally occurring data may reside on or close to an underlying submanifold we introduce a probabilistic framework for modeling both the topical and geometrical structure of the dyadic data that explicitly take into account the local manifold structure specifically the local manifold structure is modeled by a graph the graph laplacian analogous to the laplace beltrami operator on manifold is applied to smooth the probability density function a a result the obtained probabilistic distribution are concentrated around the data manifold experimental result on real data set demonstrate the effectiveness of the proposed approach 
in this paper we present an outline of a software system for buzz based recommendation this system is based on a large source of query in an ecommerce application the buzz event are detected based on query burst linked to external entity like news and inventory information a semantic neighborhood of the chosen buzz query is selected and appropriate recommendation are made on product that relate to this neighborhood the system follows the paradigm of limited quantity merchandizing in the sense that on a per day basis the system show recommendation around a single buzz query with the intent of increasing user curiosity and promoting user activity and stickiness the system demonstrates the deployment of an interesting application based on kdd principle applied to a high volume industrial context 
conventional clustering method typically assume that each data item belongs to a single cluster this assumption doe not hold in general in order to overcome this limitation we propose a generative method for clustering vectorial data where each object can be assigned to multiple cluster using a deterministic annealing scheme our method decomposes the observed data into the contribution of individual cluster and infers their parameter experiment on synthetic boolean data show that our method achieves higher accuracy in the source parameter estimation and superior cluster stability compared to stateof the art approach we also apply our method to an important problem in computer security known a role mining experiment on real world access control data show performance gain in generalization to new employee against other multi assignment method in challenging situation with high noise level our approach maintains it good performance while alternative state of the art technique lack robustness 
in many multiclass learning scenario the number of class is relatively large thousand or the space and time efficiency of the learning system can be crucial we investigate two online update technique especially suited to such problem these update share a sparsity preservation capacity they allow for constraining the number of prediction connection that each feature can make we show that one method exponential moving average is solving a discrete regression problem for each feature changing the weight in the direction of minimizing the quadratic loss we design the other method to improve a hinge loss subject to constraint for better accuracy we empirically explore the method and compare performance to previous indexing technique developed with the same goal a well a other online algorithm based on prototype learning we observe that while the classification accuracy are very promising improving over previous indexing technique the scalability benefit are preserved 
aode aggregating one dependence estimator is considered one of the most interesting representative of the bayesian classifier taking into account not only the low error rate it provides but also it efficiency until now all the attribute in a dataset have had to be nominal to build an aode classifier or they have had to be previously discretized in this paper we propose two different approach in order to deal directly with numeric attribute one of them us conditional gaussian network to model a dataset exclusively with numeric attribute and the other one keep the superparent on each model discrete and us univariate gaussians to estimate the probability for the numeric attribute and multinomial distribution for the categorical one it also being able to model hybrid datasets both of them obtain competitive result compared to aode the latter in particular being a very attractive alternative to aode in numeric datasets 
the importance of event log a a source of information in system and network management cannot be overemphasized with the ever increasing size and complexity of today s event log the task of analyzing event log ha become cumbersome to carry out manually for this reason recent research ha focused on the automatic analysis of these log file in this paper we present iplom iterative partitioning log mining a novel algorithm for the mining of cluster from event log through a step hierarchical partitioning process iplom partition log data into it respective cluster in it th and final stage iplom produce cluster description or line format for each of the cluster produced unlike other similar algorithm iplom is not based on the apriori algorithm and it is able to find cluster in data whether or not it instance appear frequently evaluation show that iplom outperforms the other algorithm statistically significantly and it is also able to achieve an average f measure performance when the closest other algorithm achieves an f measure performance of 
software is a ubiquitous component of our daily life we often depend on the correct working of software system due to the difficulty and complexity of software system bug and anomaly are prevalent bug have caused billion of dollar loss in addition to privacy and security threat in this work we address software reliability issue by proposing a novel method to classify software behavior based on past history or run with the technique it is possible to generalize past known error and mistake to capture failure and anomaly our technique first mine a set of discriminative feature capturing repetitive series of event from program execution trace it then performs feature selection to select the best feature for classification these feature are then used to train a classifier to detect failure experiment and case study on trace of several benchmark software system and a real life concurrency bug from mysql server show the utility of the technique in capturing failure and anomaly on average our pattern based classification technique outperforms the baseline approach by in accuracy 
we present a simple new monte carlo algorithm for evaluating probability of observation in complex latent variable model such a deep belief network while the method is based on markov chain estimate based on short run are formally unbiased in expectation the log probability of a test set will be underestimated and this could form the basis of a probabilistic bound the method is much cheaper than gold standard annealing based method and only slightly more expensive than the cheapest monte carlo method we give example of the new method substantially improving simple variational bound at modest extra cost 
we present mi crf a conditional random eld crf model for multiple instance learning mil mi crf model bag a node in a crf with instance a their state it combine discriminative unary instance classiers and pairwise dissimilarity measure we show that both force improve the classication performance unlike other approach mi crf considers all bag jointly during training a well a during testing this make it possible to classify test bag in an imputation setup the parameter of mi crf are learned using constraint generation furthermore we show that mi crf can incorporate previous mil algorithm to improve on their result micrf obtains competitive result on ve standard mil datasets 
standard inductive learning requires that training and test instance come from the same distribution transfer learning seek to remove this restriction in shallow transfer test instance are from the same domain but have a different distribution in deep transfer test instance are from a different domain entirely i e described by different predicate human routinely perform deep transfer but few learning system if any are capable of it in this paper we propose an approach based on a form of second order markov logic our algorithm discovers structural regularity in the source domain in the form of markov logic formula with predicate variable and instantiates these formula with predicate from the target domain using this approach we have successfully transferred learned knowledge among molecular biology social network and web domain the discovered pattern include broadly useful property of predicate like symmetry and transitivity and relation among predicate such a various form of homophily 
the automatic consolidation of database record from many heterogeneous source into a single repository requires solving several information integration task although task such a coreference schema matching and canonicalization are closely related they are most commonly studied in isolation system that do tackle multiple integration problem traditionally solve each independently allowing error to propagate from one task to another in this paper we describe a discriminatively trained model that reason about schema matching coreference and canonicalization jointly we evaluate our model on a real world data set of people and demonstrate that simultaneously solving these task reduces error over a cascaded or isolated approach our experiment show that a joint model is able to improve substantially over system that either solve each task in isolation or with the conventional cascade we demonstrate nearly a error reduction for coreference and a error reduction for schema matching 
many application in surveillance monitoring scientific discovery and data cleaning require the identification of anomaly although many method have been developed to identify statistically significant anomaly a more difficult task is to identify anomaly that are both interesting and statistically significant category detection is an emerging area of machine learning that can help address this issue using a human in the loop approach in this interactive setting the algorithm asks the user to label a query data point under an existing category or declare the query data point to belong to a previously undiscovered category the goal of category detection is to bring to the user s attention a representative data point from each category in the data in a few query a possible in a data set with imbalanced category the main challenge is in identifying the rare category or anomaly hence the task is often referred to a rare category detection we present a new approach to rare category detection based on hierarchical mean shift in our approach a hierarchy is created by repeatedly applying mean shift with an increasing bandwidth on the data this hierarchy allows u to identify anomaly in the data set at different scale which are then posed a query to the user the main advantage of this methodology over existing approach is that it doe not require any knowledge of the dataset property such a the total number of category or the prior probability of the category result on real world data set show that our hierarchical mean shift approach performs consistently better than previous technique 
what type of algorithm and statistical technique support learning from very large datasets over long stretch of time we address this question through a memory bounded version of a variational em algorithm that approximates inference in a topic model the algorithm alternate two phase model building and model compression in order to always satisfy a given memory constraint the model building phase expands it internal representation the number of topic a more data arrives through bayesian model selection compression is achieved by merging data item in clump and only caching their sufficient statistic empirically the resulting algorithm is able to handle datasets that are order of magnitude larger than the standard batch version 
user typically rate only a small fraction of all available item we show that the absence of rating carry useful information for improving the top k hit rate concerning all item a natural accuracy measure for recommendation a to test recommender system we present two performance measure that can be estimated under mild assumption without bias from data even when rating are missing not at random mnar a to achieve optimal test result we present appropriate surrogate objective function for efficient training on mnar data their main property is to account for all rating whether observed or missing in the data concerning the top k hit rate on test data our experiment indicate dramatic improvement over even sophisticated method that are optimized on observed rating only 
the hierarchical dirichlet process hidden markov model hdp hmm is a flexible nonparametric model which allows state space of unknown size to be learned from data we demonstrate some limitation of the original hdp hmm formulation teh et al and propose a sticky extension which allows more robust learning of smoothly varying dynamic using dp mixture this formulation also allows learning of more complex multimodal emission distribution we further develop a sampling algorithm that employ a truncated approximation of the dp to jointly resample the full state sequence greatly improving mixing rate via extensive experiment with synthetic data and the nist speaker diarization database we demonstrate the advantage of our sticky extension and the utility of the hdp hmm in real world application 
this paper formalizes feature selection a a reinforcement learning problem leading to a provably optimal though intractable selection policy a a second contribution this paper present an approximation thereof based on a one player game approach and relying on the monte carlo tree search uct upper confidence tree proposed by kocsis and szepesvari more precisely the presented fuse feature uct selection algorithm extends uct to deal with i a finite unknown horizon the target number of relevant feature ii a huge branching factor of the search tree the size of the initial feature set additionally a frugal reward function is proposed a a rough but unbiased estimate of the relevance of a feature subset a proof of concept of fuse is shown on the nip feature selection challenge 
in many retrieval task one important goal involves retrieving a diverse set of result e g document covering a wide range of topic for a search query first of all this reduces redundancy effectively showing more information with the presented result secondly query are often ambiguous at some level for example the query jaguar can refer to many different topic such a the car or feline a set of document with high topic diversity ensures that fewer user abandon the query because no result are relevant to them unlike existing approach to learning retrieval function we present a method that explicitly train to diversify result in particular we formulate the learning problem of predicting diverse subset and derive a training method based on structural svms 
the purpose of this paper is three fold first we formalize and study a problem of learning probabilistic concept in the recently proposed kwik framework we give detail of an algorithm known a the adaptive k meteorologist algorithm analyze it sample complexity upper bound and give a matching lower bound second this algorithm is used to create a new reinforcement learning algorithm for factored state problem that enjoys significant improvement over the previous state of the art algorithm finally we apply the adaptive k meteorologist algorithm to remove a limiting assumption in an existing reinforcement learning algorithm the effectiveness of our approach is demonstrated empirically in a couple benchmark domain a well a a robotics navigation problem 
we describe an algorithm for constructing a set of acyclic conjunctive relational feature by combining smaller conjunctive block unlike traditional level wise approach which preserve the monotonicity of frequency our block wise approach preserve a form of monotonicity of the irreducibility and relevancy feature property which are important in propositionalization employed in the context of classification learning with pruning based on these property our block wise approach efficiently scale to feature including ten of first order literal far beyond the reach of state of the art propositionalization or inductive logic programming system 
rich representation in reinforcement learning have been studied for the purpose of enabling generalization and making learning feasible in large state space we introduce object oriented mdps oo mdps a representation based on object and their interaction which is a natural way of modeling environment and offer important generalization opportunity we introduce a learning algorithm for deterministic oo mdps and prove a polynomial bound on it sample complexity we illustrate the performance gain of our representation and algorithm in the well known taxi domain plus a real life videogame 
spacecraft increasingly employ onboard data analysis to inform further data collection and prioritization decision however many spacecraft operate in high radiation environment in which the reliability of dataintensive computation is not known this paper present the first study of radiation sensitivity for k mean clustering our key finding are k mean data structure differ in sensitivity which is not determined solely by the amount of memory exposed no special radiation protection is needed below a data set dependent radiation threshold enabling the use of faster smaller and cheaper onboard memory and subsampling improves radiation tolerance slightly but the use of kd tree unfortunately reduces tolerance our conclusion can help tailor k mean for use in future high radiation environment 
abstract recently supervised dimensionality reduction ha been gaining attention owing to the realization that data label are often available and i ndicate important underlying structure in the data in this paper we present a novel convex supervised dimensionality reduction approach based on exponential family pca which is able to avoid the local optimum of typical em learning moreover by introducing a sample based approximation to exponential family model it overcomes the limitation of the prevailing gaussian assumption of standard pca and produce a kernelized formulation for nonlinear supervised dimensi onality reduction a training algorithm is then devised based on a subgradient bundle method whose scalability can be gained using a coordinate descent procedure the advantage of our global optimization approach is demonstrated by empirical result over both synthetic and real data 
data uncertainty is inherent in application such a sensor monitoring system location based service and biological database to manage this vast amount of imprecise information probabilistic database have been recently developed in this paper we study the discovery of frequent pattern and association rule from probabilistic data under the possible world semantics this is technically challenging since a probabilistic database can have an exponential number of possible world we propose two effcient algorithm which discover frequent pattern in bottom up and top down manner both algorithm can be easily extended to discover maximal frequent pattern we also explain how to use these pattern to generate association rule extensive experiment using real and synthetic datasets were conducted to validate the performance of our method 
residual gradient rg wa proposed a an alternative to td for policy evaluation when function approximation is used but there exists little formal analysis comparing them except in very limited case this paper employ technique from online learning of linear function and provides a worst case non probabilistic analysis to compare these two type of algorithm when linear function approximation is used no statistical assumption are made on the sequence of observation so the analysis applies to non markovian and even adversarial domain a well in particular our result suggest that rg may result in smaller temporal difference while td is more likely to yield smaller prediction error these phenomenon can be observed even in two simple markov chain example that are non adversarial 
in multi instance learning there are two kind of prediction failure i e false negative and false positive current research mainly focus on avoiding the former we attempt to utilize the geometric distribution of instance inside positive bag to avoid both the former and the latter based on kernel principal component analysis we define a projection constraint for each positive bag to classify it constituent instance far away from the separating hyperplane while place positive instance and negative instance at opposite side we apply the constrained concave convex procedure to solve the resulted problem empirical result demonstrate that our approach offer improved generalization performance 
we consider the problem of learning a sparse multi task regression with an application to a genetic association mapping problem for discovering genetic marker that influence expression level of multiple gene jointly in particular we consider the case where the structure over the output can be represented a a tree with leaf node a output and internal node a cluster of the output at multiple granularity and aim to recover the common set of relevant input for each output cluster assuming that the tree structure is available a a prior knowledge we formulate this problem a a new multi task regularized regression called tree guided group lasso our structured regularization is based on a group lasso penalty where the group is defined with respect to the tree structure we describe a systematic weighting scheme for the group in the penalty such that each output variable is penalized in a balanced manner even if the group overlap we present an efficient optimization method that can handle a large scale problem a is typically the case in association mapping that involve thousand of gene a output and million of genetic marker a input using simulated and yeast datasets we demonstrate that our method show a superior performance in term of both prediction error and recovery of true sparsity pattern compared to other method for multi task learning 
the structure of a markov network is typically learned using top down search at each step the search specializes a feature by conjoining it to the variable or feature that most improves the score this is inecient testing many feature variation with no support in the data and highly prone to local optimum we propose bottom up search a an alternative inspired by the analogous approach in the eld of rule induction our blm algorithm start with each complete training example a a long feature and repeatedly generalizes a feature to match it k nearest example by dropping variable an extensive empirical evaluation demonstrates that blm is both faster and more accurate than the standard top down approach and also outperforms other state of the art method 
recently there ha been a lot of interest in graph based analysis one of the most important aspect of graph based analysis is to measure similarity between node in a graph simrank is a simple and influential measure of this kind based on a solid graph theoretical model however existing method on simrank computation suffer from two limitation the computing cost can be very high in practice and they can only be applied on static graph in this paper we exploit the inherent parallelism and high memory bandwidth of graphic processing unit gpu to accelerate the computation of simrank on large graph furthermore based on the observation that simrank is essentially a first order markov chain we propose to utilize the iterative aggregation technique for uncoupling markov chain to compute simrank score in parallel for large graph the iterative aggregation method can be applied on dynamic graph moreover it can handle not only the link updating problem but also the node updating problem extensive experiment on synthetic and real data set verify that the proposed method are efficient and effective 
moment matching is a popular mean of parametric density estimation we extend this technique to nonparametric estimation of mixture model our approach work by embedding distribution into a reproducing kernel hilbert space and performing moment matching in that space this allows u to tailor density estimator to a function class of interest i e for which we would like to compute expectation we show our density estimation approach is useful in application such a message compression in graphical model and image classification and retrieval 
email is one of the most prevalent communication tool today and solving the email overload problem is pressingly urgent a good way to alleviate email overload is to automatically prioritize received message according to the priority of each user however research on statistical learning method for fully personalized email prioritization pep ha been sparse due to privacy issue since people are reluctant to share personal message and importance judgment with the research community it is therefore important to develop and evaluate pep method under the assumption that only limited training example can be available and that the system can only have the personal email data of each user during the training and testing of the model for that user this paper present the first study to the best of our knowledge under such an assumption specifically we focus on analysis of personal social network to capture user group and to obtain rich feature that represent the social role from the viewpoint of a particular user we also developed a novel semi supervised transductive learning algorithm that propagates importance label from training example to test example through message and user node in a personal email network these method together enable u to obtain an enriched vector representation of each new email message which consists of both standard feature of an email message such a word in the title or body sender and receiver id etc and the induced social feature from the sender and receiver of the message using the enriched vector representation a the input in svm classifier to predict the importance level for each test message we obtained significant performance improvement over the baseline system without induced social feature in our experiment on a multi user data collection we obtained significant performance improvement over the baseline system without induced social feature in our experiment on a multi user data collection the relative error reduction in mae wa in micro averaging and in macro averaging 
tag recommendation is the task of predicting a personalized list of tag for a user given an item this is important for many website with tagging capability like last fm or delicious in this paper we propose a method for tag recommendation based on tensor factorization tf in contrast to other tf method like higher order singular value decomposition hosvd our method rtf ranking with tensor factorization directly optimizes the factorization model for the best personalized ranking rtf handle missing value and learns from pairwise ranking constraint our optimization criterion for tf is motivated by a detailed analysis of the problem and of interpretation scheme for the observed data in tagging system in all rtf directly optimizes for the actual problem using a correct interpretation of the data we provide a gradient descent algorithm to solve our optimization problem we also provide an improved learning and prediction method with runtime complexity analysis for rtf the prediction runtime of rtf is independent of the number of observation and only depends on the factorization dimension besides the theoretical analysis we empirically show that our method outperforms other state of the art tag recommendation method like folkrank pagerank and hosvd both in quality and prediction runtime 
in this paper we consider a smoothing kernel based classification rule and propose an algorithm for optimizing the performance of the rule by learning the bandwidth of the smoothing kernel along with a data dependent distance metric the data dependent distance metric is obtained by learning a function that embeds an arbitrary metric space into a euclidean space while minimizing an upper bound on the resubstitution estimate of the error probability of the kernel classification rule by restricting this embedding function to a reproducing kernel hilbert space we reduce the problem to solving a semidefinite program and show the resulting kernel classification rule to be a variation of the k nearest neighbor rule we compare the performance of the kernel rule using the learned data dependent distance metric to state of the art distance metric learning algorithm designed for k nearest neighbor classification on some benchmark datasets the result show that the proposed rule ha either better or a good classification accuracy a the other metric learning algorithm 
young child demonstrate the ability to make inference about the preference of other agent based on their choice however there exists no overarching account of what child are doing when they learn about preference or how they use that knowledge we use a rational model of preference learning drawing on idea from economics and computer science to explain the behavior of child in several recent experiment specifically we show how a simple econometric model can be extended to capture twoto four year old use of statistical information in inferring preference and their generalization of these preference 
k mean is a well known and widely used partitional clustering method while there are considerable research effort to characterize the key feature of the k mean clustering algorithm further investigation is needed to understand how data distribution can have impact on the performance of k mean clustering to that end in this paper we provide a formal and organized study of the effect of skewed data distribution on k mean clustering along this line we first formally illustrate that k mean tends to produce cluster of relatively uniform size even if input data have varied true cluster size in addition we show that some clustering validation measure such a the entropy measure may not capture this uniform effect and provide misleading information on the clustering performance viewed in this light we provide the coefficient of variation cv a a necessary criterion to validate the clustering result our finding reveal that k mean tends to produce cluster in which the variation of cluster size a measured by cv are in a range of about specifically for data set with large variation in true cluster size e g cv k mean reduces variation in resultant cluster size to le than in contrast for data set with small variation in true cluster size e g cv k mean increase variation in resultant cluster size to greater than in other word for the earlier two case k mean produce the clustering result which are away from the true cluster distribution 
learning probabilistic graphical model from high dimensional datasets is a computationally challenging task in many interesting application the domain dimensionality is such a to prevent state of the art statistical learning technique from delivering accurate model in reasonable time this paper present a hybrid random field model for pseudo likelihood estimation in high dimensional domain a theoretical analysis prof that the class of pseudo likelihood distribution representable by hybrid random field strictly includes the class of joint probability distribution representable by bayesian network in order to learn hybrid random field from data we develop the markov blanket merging algorithm theoretical and experimental evidence show that markov blanket merging scale up very well to high dimensional datasets a compared to other widely used statistical learning technique markov blanket merging delivers accurate result in a number of link prediction task while achieving also significant improvement in term of computational efficiency our software implementation of the model investigated in this paper is publicly available at http www dii unisi it freno the same website also host the datasets used in this work that are not available elsewhere in the same preprocessing used for our experiment 
regularized least square rls algorithm have the ability to avoid over fitting problem and to express solution a kernel expansion however we observe that the current rls algorithm cannot provide a satisfactory interpretation even on a constant function on the other hand while kernel based algorithm have been developed in such a tendency that almost all learning algorithm are kernelized or being kernelized a basic fact is often ignored the learned function from the data and the kernel fit the data well but may not be consistent with the kernel based on these consideration and on the intuition that a good kernel based inductive function should be consistent with both the data and the kernel a novel learning scheme is proposed the advantage of this scheme lie in it corresponding representer theorem it strong interpretation ability about what kind of function should not be penalized and it promising accuracy improvement shown in a number of experiment furthermore we provide a detailed technical description about heat kernel which serf a an example for the reader to apply similar technique for other kernel our work provides a preliminary step in a new direction to explore the varying consistency between inductive function and kernel under various distribution 
by attempting to simultaneously partition both the row example and column feature of a data matrix co clustering algorithm often demonstrate surprisingly impressive performance improvement over traditional one sided row clustering technique a good clustering of feature may be seen a a combinatorial transformation of the data matrix effectively enforcing a form of regularization that may lead to a better clustering of example and vice versa in many application partial supervision in the form of a few row label a well a column label may be available to potentially assist co clustering in this paper we develop two novel semi supervised multi class classification algorithm motivated respectively by spectral bipartite graph partitioning and matrix approximation formulation for co clustering these algorithm i support dual supervision in the form of label for both example and or feature ii provide principled predictive capability on out of sample test data and iii arise naturally from the classical representer theorem applied to regularization problem posed on a collection of reproducing kernel hilbert space empirical result demonstrate the effectiveness and utility of our algorithm 
we introduce a new interpretation of multiscale random field msrfs that admits efficient optimization in the frameworkof regular single level randomfields rf it is based on a new operator called append that combine set of random variable rv to single rv we assume that a msrf can be decomposed into disjoint tree that link rv at different pyramid level the append operator is then applied to map rv in each tree structure to a single rv we demonstrate the usefulness of the proposed approach on a challenging task involving grouping contour of target shape in image it provides a natural representation of multiscale contour model which is needed in order to cope with unstable contour decomposition the append operator allows u to find optimal image segment label using the classical framework of relaxation labeling alternative method like markov chain monte carlo mcmc could also be used the main difference between the proposed interpretation of msrfs or mcff a known in the literature e g and the proposed msrf is the interpretation of the connection between different scale level in the proposed approach the random variable rv linked by a tree substructure across differentlevels competefor their label assignment while in the existing approach the goal is to cooperate in the label assigns which is usually achieved by averaging in other word usually the label assignment of a parent node is enforced to be compatible with the label assignment of it childrenby averaging in contrast in the proposedapproachthe parentnode and all it child compete for the best possible label assignment contour grouping is one of key approach to object detection and recognition which is a fundamental goal of computer vision we introduce a novel msrf interpretation and show it benefit in solving the contour grouping problem the msrf allows u to cast contour grouping a contour matching detection and grouping by shape ha been investigated in earlier work the basic 
we address the problem of learning classifier for several related task that may differ in their joint distribution of input and output variable for each task small possibly even empty labeled sample and large unlabeled sample are available while the unlabeled sample reflect the target distribution the labeled sample may be biased this setting is motivated by the problem of predicting sociodemographic feature for user of web portal based on the content which they have accessed here questionnaire offered to a portion of each portal s user produce biased sample we derive a transfer learning procedure that produce resampling weight which match the pool of all example to the target distribution of any given task transfer learning enables u to make prediction even for new portal with few or no training data and improves the overall prediction accuracy 
we define a metric for measuring behavior similarity between state in a markov decision process mdp which take action similarity into account we show that the kernel of our metric corresponds exactly to the class of state defined by mdp homomorphism ravindran barto we prove that the difference in the optimal value function of different state can be upper bounded by the value of this metric and that the bound is tighter than pr evious bound provided by bisimulation metric fern et al our result hold both for discrete and for continuous action we provide an algorithm for constructing approximate homomorphism by using this metric to identify state that can be grouped together a well a action that can be matched previous research on this topic is based mainly on heuristic 
web log record the primary interaction of user with web page in general and search engine in particular there are two source for such log user trail obtained from toolbars and query click information obtained from search engine in this talk we will address the task of mining this rich data to improve user experience on the web we will illustrate a few application together with the modeling and algorithmic challenge that stem from these application we will also discus the privacy issue that arise in this context 
to take the first step beyond keyword based search toward entity based search suitable token span spot on document must be identified a reference to real world entity from an entity catalog several system have been proposed to link spot on web page to entity in wikipedia they are largely based on local compatibility between the text around the spot and textual metadata associated with the entity two recent system exploit inter label dependency but in limited way we propose a general collective disambiguation approach our premise is that coherent document refer to entity from one or a few related topic or domain we give formulation for the trade off between local spot to entity compatibility and measure of global coherence between entity optimizing the overall entity assignment is np hard we investigate practical solution based on local hill climbing rounding integer linear program and pre clustering entity followed by local optimization within cluster in experiment involving over a hundred manually annotated web page and ten of thousand of spot our approach significantly outperform recently proposed algorithm 
recent development in programmable highly parallel graphic processing unit gpus have enabled high performance implementation of machine learning algorithm we describe a solver for support vector machine training running on a gpu using the sequential minimal optimization algorithm and an adaptive first and second order working set selection heuristic which achieves speedup of x over libsvm running on a traditional processor we also present a gpu based system for svm classification which achieves speedup of x over libsvm x over our own cpu based svm classifier 
learning from data stream is a research area of increasing importance nowadays several stream learning algorithm have been developed most of them learn decision model that continuously evolve over time run in resource aware environment detect and react to change in the environment generating data one important issue not yet conveniently addressed is the design of experimental work to evaluate and compare decision model that evolve over time there are no golden standard for assessing performance in non stationary environment this paper proposes a general framework for assessing predictive stream learning algorithm we defend the use of predictive sequential method for error estimate the prequential error the prequential error allows u to monitor the evolution of the performance of model that evolve over time nevertheless it is known to be a pessimistic estimator in comparison to holdout estimate to obtain more reliable estimator we need some forgetting mechanism two viable alternative are sliding window and fading factor we observe that the prequential error converges to an holdout estimator when estimated over a sliding window or using fading factor we present illustrative example of the use of prequential error estimator using fading factor for the task of i assessing performance of a learning algorithm ii comparing learning algorithm iii hypothesis testing using mcnemar test and iv change detection using page hinkley test in these task the prequential error estimated using fading factor provide reliable estimator in comparison to sliding window fading factor are faster and memory le a requirement for streaming application this paper is a contribution to a discussion in the good practice on performance assessment when learning dynamic model that evolve over time 
mining cluster evolution from multiple correlated time varying text corpus is important in exploratory text analytics in this paper we propose an approach called evolutionary hierarchical dirichlet process evohdp to discover interesting cluster evolution pattern from such text data we formulate the evohdp a a series of hierarchical dirichlet process hdp by adding time dependency to the adjacent epoch and propose a cascaded gibbs sampling scheme to infer the model this approach can discover different evolving pattern of cluster including emergence disappearance evolution within a corpus and across different corpus experiment over synthetic and real world multiple correlated time varying data set illustrate the effectiveness of evohdp on discovering cluster evolution pattern 
the vast majority of earlier work ha focused on graph which are both connected typically by ignoring all but the giant connected component and unweighted here we study numerous real weighted graph and report surprising discovery on the way in which new node join and form link in a social network the motivating question were the following how do connected component in a graph form and change over time what happens after new node join a network how common are repeated edge we study numerous diverse real graph citation network network in social medium internet traffic and others and make the following contribution a we observe that the non giant connected component seem to stabilize in size b we observe the weight on the edge follow several power law with surprising exponent and c we propose an intuitive generative model for graph growth that obeys observed pattern 
object detection and multi class image segmentation are two closely related task that can be greatly improved when solved jointly by feeding information from one task to the other however current state of th e art model use a separate representation for each task making joint inferen ce clumsy and leaving the classification of many part of the scene ambiguous in this work we propose a hierarchical region based approa ch to joint object detection and image segmentation our approach simultaneously reason about pixel region and object in a coherent probabilistic mod el pixel appearance feature allow u to perform well on classifying amorphous background class while the explicit representation of region facilitate th e computation of more sophisticated feature necessary for object detection impo rtantly our model give a single unified description of the scene we explain every pixel in the image and enforce global consistency between all random variable in our model we run experiment on the challenging street scene dataset and show significant improvement over state of the art result for object detection accuracy 
the temporal restricted boltzmann machine trbm is a probabilistic model for sequence that is able to successfully model i e generat e nice looking sample of several very high dimensional sequence such a motion capture data and the pixel of low resolution video of ball bouncing in a box the major disadvantage of the trbm is that exact inference is extremely hard since even computing a gibbs update for a single variable of the posterior is exponentially expensive this difficulty ha necessitated the use of a heuristic infer ence procedure that nonetheless wa accurate enough for successful learning in this paper we introduce the recurrent trbm which is a very slight modification o f the trbm for which exact inference is very easy and exact gradient learning is almost tractable we demonstrate that the rtrbm is better than an analogous trbm at generating motion capture and video of bouncing ball 
we present an on line learning framework tailored towards real time learning from observed user behavior in search engine and other information retrieval system in particular we only require pairwise comparison which were shown to be reliably inferred from implicit feedback joachim et al radlinski et al b we will present an algorithm with theoretical guarantee a well a simulation result 
we address the question of how the approximation error bellman residual at each iteration of the approximate policy value iteration algorithm influence the quality of the resulted policy we quantify the performance loss a the l p norm of the approximation error bellman residual at each iteration moreover we show that the performance loss depends on the expectation of the squared radon nikodym derivative of a certain distribution rather than it supremum a opposed to what ha been suggested by the previous result also our result indicate that the contribution of the approximation bellman error to the performance loss is more prominent in the later iteration of api avi and the effect of an error term in the earlier iteration decay exponentially fast 
low rank matrix approximation method provide one of the simplest and most effective approach to collaborative filtering such model are usually fitted to data by finding a map estimate of the model parameter a procedure that can be performed efficiently even on very large datasets however unless the regularization parameter are tuned carefully this approach is prone to overfitting because it find a single point estimate of the parameter in this paper we present a fully bayesian treatment of the probabilistic matrix factorization pmf model in which model capacity is controlled automatically by integrating over all model parameter and hyperparameters we show that bayesian pmf model can be efficiently trained using markov chain monte carlo method by applying them to the netflix dataset which consists of over million movie rating the resulting model achieve significantly higher prediction accuracy than pmf model trained using map estimation 
there is a wide variety of data mining method available and it is generally useful in exploratory data analysis to use many different method for the same dataset this however lead to the problem of whether the result found by one method are a reflection of the phenomenon shown by the result of another method or whether the result depict in some sense unrelated property of the data for example using clustering can give indication of a clear cluster structure and computing correlation between variable can show that there are many significant correlation in the data however it can be the case that the correlation are actually determined by the cluster structure in this paper we consider the problem of randomizing data so that previously discovered pattern or model are taken into account the randomization method can be used in iterative data mining at each step in the data mining process the randomization produce random sample from the set of data matrix satisfying the already discovered pattern or model that is given a data set and some statistic e g cluster center or co occurrence count of the data the randomization method sample data set having similar value of the given statistic a the original data set we use metropolis sampling based on local swap to achieve this we describe experiment on real data that demonstrate the usefulness of our approach our result indicate that in many case the result of e g clustering actually imply the result of say frequent pattern discovery 
in many application data appear with a huge number of instance a well a feature linear support vector machine svm is one of the most popular tool to deal with such large scale sparse data this paper present a novel dual coordinate descent method for linear svm with l and l loss function the proposed method is simple and reach an accurate solution in o log iteration experiment indicate that our method is much faster than state of the art solver such a pegasos tron svmperf and a recent primal coordinate descent implementation 
learning in real time application e g online approximation of the inverse dynamic model for model based robot control requires fast online regression technique inspired by local learning we propose a method to speed up standard gaussian process regression gpr with local gp model lgp the training data is partitioned in local region for each an individual gp model is trained the prediction for a query point is performed by weighted estimation using nearby local model unlike other gp approximation such a mixture of expert we use a distance based measure for partitioning of the data and weighted prediction the proposed method achieves online learning and prediction in real time comparison with other non parametric regression method show that lgp ha higher accuracy than lwpr and close to the performance of standard gpr and svr 
the label ranking problem consists of learning a model that map instance to total order over a finite set of predefined label this paper introduces new method for label ranking that complement and improve upon existing approach more specifically we propose extension of two method that have been used extensively for classification and regression so far namely instance based learning and decision tree induction the unifying element of the two method is a procedure for locally estimating predictive probability model for label ranking 
in this paper we point out that there exist scaling and initialization problem in most existing multiple kernel learning mkl approach which employ the large margin principle to jointly learn both a kernel and an svm classifier the reason is that the margin itself can not well describe how good a kernel is due to the negligence of the scaling we use the ratio between the margin and the radius of the minimum enclosing ball to measure the goodness of a kernel and present a new minimization formulation for kernel learning this formulation is invariant to scaling of learned kernel and when learning linear combination of basis kernel it is also invariant to scaling of basis kernel and to the type e g l or l of norm constraint on combination coefficient we establish the differentiability of our formulation and propose a gradient projection algorithm for kernel learning experiment show that our method significantly outperforms both svm with the uniform combination of basis kernel and other state of art mkl approach 
much of research in data mining and machine learning ha led to numerous practical application spam filtering fraud detection and user query intent analysis ha relied heavily on machine learned classifier and resulted in improvement in robust classification accuracy combining multiple classifier a k a ensemble learning is a well studied and ha been known to improve effectiveness of a classifier to address two key challenge in ensemble learning learning weight of individual classifier and the combination rule of their weighted response this paper proposes a novel ensemble classifier enlr that computes weight of response from discriminative classifier and combine their weighted response to produce a single response for a test instance the combination rule is based on aggregating weighted response where a weight of an individual classifier is inversely based on their respective variance around their response here variance quantifies the uncertainty of the discriminative classifier parameter which in turn depends on the training sample a opposed to other ensemble method where the weight of each individual classifier is learned a a part of parameter learning and thus the same weight is applied to all testing instance our model is actively adjusted a individual classifier become confident at it decision for a test instance our empirical experiment on various data set demonstrate that our combined classifier produce effective result when compared with a single classifier our novel classifier show statistically significant better accuracy when compared to well known ensemble method bagging and adaboost in addition to robust accuracy our model is extremely efficient dealing with high volume of training sample due to the independent learning paradigm among it multiple classifier it is simple to implement in a distributed computing environment such a hadoop 
we study an encoding decoding mechanism accounting for the relative spike timing of the signal propagating from peripheral nerve fiber to second order somatosensory neuron in the cuneate nucleus cn the cn is modeled a a population of spiking neuron receiving a input the spatiotemporal response of real mechanoreceptors obtained via microneurography recording in human the efficiency of the haptic discrimination process is quantified by a novel definition of entropy that take into full account the metrical property of the spike train space this measure prof to be a suitable decoding scheme for generalizing the classical shannon entropy to spike based neural code it permit an assessment of neurotransmission in the presence of a large output space i e hundred of spike train with m temporal precision it is shown that the cn population code performs a complete discrimination of distinct stimulus already within m of the first afferent spike whereas a partial discrimination of the maximum information transmission is possible a rapidly a m this study suggests that the cn may not constitute a mere synaptic relay along the somatosensory pathway but rather it may convey optimal contextual account in term of fast and reliable information transfer of peripheral tactile input to downstream structure of the central nervous system 
we present a data structure enabling efficient nearest neighbor nn retrieval for bregman divergence the family of bregman divergence includes many popular dissimilarity measure including kl divergence relative entropy mahalanobis distance and itakura saito divergence these divergence present a challenge for efficient nn retrieval because they are not in general metric for which most nn data structure are designed the data structure introduced in this work share the same basic structure a the popular metric ball tree but employ convexity property of bregman divergence in place of the triangle inequality experiment demonstrate speedup over brute force search of up to several order of magnitude 
at google experimentation is practically a mantra we evaluate almost every change that potentially affect what our user experience such change include not only obvious user visible change such a modification to a user interface but also more subtle change such a different machine learning algorithm that might affect ranking or content selection our insatiable appetite for experimentation ha led u to tackle the problem of how to run more experiment how to run experiment that produce better decision and how to run them faster in this paper we describe google s overlapping experiment infrastructure that is a key component to solving these problem in addition because an experiment infrastructure alone is insufficient we also discus the associated tool and educational process required to use it effectively we conclude by describing trend that show the success of this overall experimental environment while the paper specifically describes the experiment system and experimental process we have in place at google we believe they can be generalized and applied by any entity interested in using experimentation to improve search engine and other web application 
consider linear prediction model where the target function is a sparse linear combination of a set of basis function we are interested in the problem of identifying those basis function with non zero coefficient and reconstructing the target function from noisy observation two heuristic that are widely used in practice are forward and backward greedy algorithm first we show that neither idea is adequate second we propose a novel combination that is based on the forward greedy algorithm but take backward step adaptively whenever beneficial we prove strong theoretical result showing that this procedure is effective in learning sparse representation experimental result support our theory 
this paper address several key issue in the arnetminer system which aim at extracting and mining academic social network specifically the system focus on extracting researcher profile automatically from the web integrating the publication data into the network from existing digital library modeling the entire academic network and providing search service for the academic network so far researcher profile have been extracted using a unified tagging approach we integrate publication from online web database and propose a probabilistic framework to deal with the name ambiguity problem furthermore we propose a unified modeling approach to simultaneously model topical aspect of paper author and publication venue search service such a expertise search and people association search have been provided based on the modeling result in this paper we describe the architecture and main feature of the system we also present the empirical evaluation of the proposed method 
when individual learn fact e g foreign language vocabulary over multiple study session the temporal spacing of study ha a significant impact on memory retention behavioral experiment have shown a nonmonotonic relationship between spacing and retention short or long interval between study session yield lower cued recall accuracy than intermediate interval appropriate spacing of study can double retention on educationally relevant time scale we introduce a multiscale context model mcm that is able to predict the influence of a particular study schedule on retention for specific material mcm s prediction is based on empirical data characterizing forgetting of the material following a single study session mcm is a synthesis of two existing memory model staddon chelaru higa raaijmakers on the surface these model are unrelated and incompatible but we show they share a core feature that allows them to be integrated mcm can determine study schedule that maximize the durability of learning and ha implication for education and training mcm can be cast either a a neural network with input that fluctuate over time or a a cascade of leaky integrator mcm is intriguingly similar to a bayesian multiscale model of memory kording tenenbaum shadmehr yet mcm is better able to account for human declarative memory 
we propose a new method to quantify the solution stability of a large class of combinatorial optimization problem arising in machine learning a practical example we apply the method to correlation clustering clustering aggregation modularity clustering and relative performance significance clustering our method is extensively motivated by the idea of linear programming relaxation we prove that when a relaxation is used to solve the original clustering problem then the solution stability calculated by our method is conservative that is it never overestimate the solution stability of the true unrelaxed problem we also demonstrate how our method can be used to compute the entire path of optimal solution a the optimization problem is increasingly perturbed experimentally our method is shown to perform well on a number of benchmark problem 
the earth observing system data and information system eosdis is a comprehensive data and information system which archive manages and distributes earth science data from the eos spacecraft one non existent capability in the eosdis is the retrieval of satellite sensor data based on weather event such a tropical cyclone similarity query output in this paper we propose a framework to solve the similarity search problem given user defined instance level constraint for tropical cyclone event represented by arbitrary length multidimensional spatio temporal data sequence a critical component for such a problem is the similarity metric function to compare the data sequence we describe a novel longest common subsequence lcss parameter learning approach driven by nonlinear dimensionality reduction and distance metric learning intuitively arbitrary length multidimensional data sequence are projected into a fixed dimensional manifold for lcss parameter learning similarity search is achieved through consensus among the similar instance level constraint based on ranking order computed using the lcss based similarity measure experimental result using a combination of synthetic and real tropical cyclone event data sequence are presented to demonstrate the feasibility of our parameter learning approach and it robustness to variability in the instance constraint we then use a similarity query example on real tropical cyclone event data sequence from to to discus i a problem of scientific interest and ii challenge and issue related to the weather event similarity search problem 
compressive sensing c is an emerging eld that under appropriate condition can signi cantly reduce the number of measurement required for a given signal in many application one is interested in multiple signal that may be measured in multiple c type measurement where here each signal corresponds to a sensing task in this paper we propose a novel multitask compressive sensing framework based on a bayesian formalism where a dirichlet process dp prior is employed yielding a principled mean of simultaneously inferring the appropriate sharing mechanism a well a c inversion for each task a variational bayesian vb inference algorithm is employed to estimate the full posterior on the model parameter 
recent innovation have resulted in a plethora of social application on the web such a blog social network and community photo and video sharing application such application can typically be represented a evolving interaction graph with node denoting entity and edge representing their interaction the study of entity and community and how they evolve in such large dynamic graph is both important and challenging while much of the past work in this area ha focused on static analysis more recently researcher have investigated dynamic analysis in this paper in a departure from recent effort we consider the problem of analyzing pattern and critical event that affect the dynamic graph from the viewpoint of a single node or a selected subset of node defining and extracting a relevant viewpoint neighborhood efficiently while also quantifying the key relationship among node involved are the key challenge we address we also examine the evolution of viewpoint neighborhood for different entity over time to identify key structural and behavioral transformation that occur 
we cast model free reinforcement learning a the problem of maximizing the likelihood of a probabilistic mixture model via sampling addressing both the infinite and finite horizon case we describe a stochastic approximation em algorithm for likelihood maximization that in the tabular case is equivalent to a non bootstrapping optimistic policy iteration algorithm like sarsa that can be applied both in mdps and pomdps on the theoretical side by relating the proposed stochastic em algorithm to the family of optimistic policy iteration algorithm we provide new tool that permit the design and analysis of algorithm in that family on the practical side preliminary experiment on a pomdp problem demonstrated encouraging result 
this paper tackle the problem of selecting among several linear estimator in non parametric regression this includes model selection for linear regression the choice of a regularization parameter in kernel ridge regression or spline smoothing and the choice of a kernel in multiple kernel learning we propose a new algorithm which first estimate consistently the variance of the noise based upon the concept of minimal penalty which wa previously introduced in the context of model selection then plugging our variance estimate in mallow c l penalty is proved to lead to an algorithm satisfying an oracle inequality simulation experiment with kernel ridge regression and multiple kernel learning show that the proposed algorithm often improves significantly existing calibration procedure such a fold cross validation or generalized cross validation 
given a corpus of news item consisting of image accompanied by text caption we want to find out who s doing what i e associate name an d action verb in the caption to the face and body pose of the person in the image we present a joint model for simultaneously solving the image caption correspondence and learning visual appearance model for the face and pose class occurring in the corpus these model can then be used to recognize people and action in novel image without caption we demonstrate experimentally that our joint face and pose model solves the correspondence problem better than e arlier model covering only the face and that it can perform recognition of new uncaptioned image 
we consider the task of learning to accurately follow a trajectory in a vehicle such a a car or helicopter a number of dynamic programming algorithm such a differential dynamic programming ddp and policy search by dynamic programming psdp can efficiently compute non stationary policy for these task such policy in general are well suited to trajectory following since they can easily generate different control action at different time in order to follow the trajectory however a weakness of these algorithm is that their policy are time indexed in that they apply different policy depending on the current time this is problematic since the current time may not correspond well to where we are along the trajectory and the uncertainty over state can prevent these algorithm from finding any good policy at all in this paper we propose a method for space indexed dynamic programming that overcomes both these difficulty we begin by showing how a dynamical system can be rewritten in term of a spatial index variable i e how far along the trajectory we are rather than a a function of time we then use these space indexed dynamical system to derive space indexed version of the ddp and psdp algorithm finally we show that these algorithm perform well on a variety of control task both in simulation and on real system 
the method of common spatio spectral pattern cssps is an extension of common spatial pattern csps by utilizing the technique of delay embedding to alleviate the adverse effect of noise and artifact on the electroencephalogram eeg classification although the cssps method ha shown to be more powerful than the csps method in the eeg classification this method is only suitable for two class eeg classification problem in this paper we generalize the two class cssps method to multi class case to this end we first develop a novel theory of multi class bayes error estimation and then present the multi class cssps mc ssps method based on this bayes error theoretical framework by minimizing the estimated closed form bayes error we obtain the optimal spatio spectral filter of mcssps to demonstrate the effectiveness of the proposed method we conduct extensive experiment on the bci competition data set the experimental result show that our method significantly outperforms the previous multi class csps mcsps method in the eeg classification 
motor primitive or motion template have become an important concept for both modeling human motor control a well a generating robot behavior using imitation learning recent impressive result range from humanoid robot movement generation to timing model of human motion the automatic generation of skill library containing multiple motion template is an important step in robot learning such a skill learning system need to cluster similar movement together and represent each resulting motion template a a generative model which is subsequently used for the execution of the behavior by a robot system in this paper we show how human trajectoriescapturedas multi dimensionaltime series can be clustered using bayesian mixture of linear gaussian state space model based on the similarity of their dynamic the appropriate number of template is automatically determined by enforcing a parsimonious parametrization a the resulting model is intractable we introduce a novel approximation method based on variational bayes which is especially designed to enable the use of efficient inference algorithm on recorded human balero movement this method is not only capable of finding reasonable motion template but also yield a generative model which work well in the execution of this complex task on a simulated anthropomorphic sarcos arm 
we develop new technique for time series classification based on hierarchical bayesian generative model called mixed effect model and the fisher kernel derived from them a key advantage of the new formulation is that one can compute the fisher information matrix despite varying sequence length and varying sampling interval this avoids the commonly used ad hoc replacement of the fisher information matrix with the identity which destroys the geometric invariance of the kernel our construction retains the geometric invariance resulting in a kernel that is properly invariant under change of coordinate in the model parameter space experiment on detecting cognitive decline show that classifier based on the proposed kernel out perform those based on generative model and other feature extraction routine and on fisher kernel that use the identity in place of the fisher information 
one essential issue of document clustering is to estimate the appropriate number of cluster for a document collection to which document should be partitioned in this paper we propose a novel approach namely dpmfs to address this issue the proposed approach is designed to group document into a set of cluster while the number of document cluster is determined by the dirichlet process mixture model automatically to identify the discriminative word and separate them from irrelevant noise word via stochastic search variable selection technique we explore the performance of our proposed approach on both a synthetic dataset and several realistic document datasets the comparison between our proposed approach and stage of the art document clustering approach indicates that our approach is robust and effective for document clustering 
in high dimensional classification problem it is infeasible to include enough training sample to cover the class region densely irregularity in the resulting sparse sample distribution cause local classifier such a nearest neighbor nn and kernel method to have irregular decision boundary one solution is to fill in the hole by building a convex model of the region spanned by the training sample of each class and classifying example based on their distance to these approximate model method of this kind based on affine and convex hull and bounding hyperspheres have already been studied here we propose a method based on the bounding hyperdisk of each class the intersection of the affine hull and the smallest bounding hypersphere of it training sample we argue that in many case hyperdisks are preferable to affine and convex hull and hyperspheres they bound the class more tightly than affine hull or hyperspheres while avoiding much of the sample overfitting and computational complexity that is inherent in high dimensional convex hull we show that the hyperdisk method can be kernelized to provide nonlinear classifier based on non euclidean distance metric experiment on several classification problem show promising result 
in this paper we define and study a new opinionated text data analysis problem called latent aspect rating analysis lara which aim at analyzing opinion expressed about an entity in an online review at the level of topical aspect to discover each individual reviewer s latent opinion on each aspect a well a the relative emphasis on different aspect when forming the overall judgment of the entity we propose a novel probabilistic rating regression model to solve this new text mining problem in a general way empirical experiment on a hotel review data set show that the proposed latent rating regression model can effectively solve the problem of lara and that the detailed analysis of opinion at the level of topical aspect enabled by the proposed model can support a wide range of application task such a aspect opinion summarization entity ranking based on aspect rating and analysis of reviewer rating behavior 
a multi mode network typically consists of multiple heterogeneous social actor among which various type of interaction could occur identifying community in a multi mode network can help understand the structural property of the network address the data shortage and unbalanced problem and assist task like targeted marketing and finding influential actor within or between group in general a network and the membership of group often evolve gradually in a dynamic multi mode network both actor membership and interaction can evolve which pose a challenging problem of identifying community evolution in this work we try to address this issue by employing the temporal information to analyze a multi mode network a spectral framework and it scalability issue are carefully studied experiment on both synthetic data and real world large scale network demonstrate the efficacy of our algorithm and suggest it generality in solving problem with complex relationship 
in plenty of scenario data can be represented a vector and then mathematically abstracted a point in a euclidean space because a great number of machine learning and data mining application need proximity measure over data a simple and universal distance metric is desirable and metric learning method have been explored to produce sensible distance measure consistent with data relationship however most existing method suffer from limited labeled data and expensive training in this paper we address these two issue through employing abundant unlabeled data and pursuing sparsity of metric resulting in a novel metric learning approach called semi supervised sparse metric learning two important contribution of our approach are it propagates scarce prior affinity between data to the global scope and incorporates the full affinity into the metric learning and it us an efficient alternating linearization method to directly optimize the sparse metric compared with conventional method ours can effectively take advantage of semi supervision and automatically discover the sparse metric structure underlying input data pattern we demonstrate the efficacy of the proposed approach with extensive experiment carried out on six datasets obtaining clear performance gain over the state of the art 
roc curve are one of the most widely used display to evaluate performance of scoring function in the paper we propose a statistical method for directly optimizing the roc curve the target is known to be the regression function up to an increasing transformation and this boil down to recovering the level set of the latter we propose to use classifier obtained by empirical risk minimization of a weighted classification error and then to construct a scoring rule by overlaying these classifier we show the consistency and rate of convergence to the optimal roc curve of this procedure in term of supremum norm and also a a byproduct of the analysis we derive an empirical estimate of the optimal roc curve 
existing approach to analyzing the asymptotics of graph laplacians typically assume a well behaved kernel function with smoothness assumption we remove the smoothness assumption and generalize the analysis of graph laplacians to include previously unstudied graph including knn graph we also introduce a kernel free framework to analyze graph construction with shrinking neighborhood in general and apply it to analyze locally linear embedding lle we also describe how for a given limiting laplacian operator desirable property such a a convergent spectrum and sparseness can be achieved choosing the appropriate graph construction 
multi task learning mtl aim to improve generalization performance by learning multiple related task simultaneously in this paper we consider the problem of learning shared structure from multiple related task we present an improved formulation iaso for multi task learning based on the non convex alternating structure optimization aso algorithm in which all task are related by a shared feature representation we convert iaso a non convex formulation into a relaxed convex one which is however not scalable to large data set due to it complex constraint we propose an alternating optimization caso algorithm which solves the convex relaxation efficiently and further show that caso converges to a global optimum in addition we present a theoretical condition under which caso can find a globally optimal solution to iaso experiment on several benchmark data set confirm our theoretical analysis 
policy gradient pg reinforcement learning algorithm have strong local convergence guarantee but their learning performance is typically limited by a large variance in the estimate of the gradient in this paper we formulate the variance reduction problem by describing a signal to noise ratio snr for policy gradient algorithm and evaluate this snr carefully for the popular weight perturbation wp algorithm we confirm that snr is a good predictor of long term learning performance and that in our episodic formulation the cost to go function is indeed the optimal baseline we then propose two modification to traditional model free policy gradient algorithm in order to optimize the snr first we examine wp using anisotropic sampling distribution which introduces a bias into the update but increase the snr this bias can be interpreted a following the natural gradient of the cost function second we show that non gaussian distribution can also increase the snr and argue that the optimal isotropic distribution is a shell distribution with a constant magnitude and uniform distribution in direction we demonstrate that both modification produce substantial improvement in learning performance in challenging policy gradient experiment 
we consider the task of reinforcement learning in an environment in which rare significant event occur independently of the action selected by the controlling agent if these event are sampled according to their natural probability of occurring convergence of conventional reinforcement learning algorithm is likely to be slow and the learning algorithm may exhibit high variance in this work we assume that we have access to a simulator in which the rare event probability can be artificially altered then importance sampling can be used to learn with this simulation data we introduce algorithm for policy evaluation using both tabular and function approximation representation of the value function we prove that in both case the reinforcement learning algorithm converge in the tabular case we also analyze the bias and variance of our approach compared to td learning we evaluate empirically the performance of the algorithm on random markov decision process a well a on a large network planning task 
we study the problem of finding the dominant eigenvector of the sample covariance matrix under additional constraint on the vector a cardinality constraint limit the number of non zero element and non negativity force the element to have equal sign this problem is known a sparse and non negative principal component analysis pca and ha many application including dimensionality reduction and feature selection based on expectation maximization for probabilistic pca we present an algorithm for any combination of these constraint it complexity is at most quadratic in the number of dimension of the data we demonstrate significant improvement in performance and computational efficiency compared to other constrained pca algorithm on large data set from biology and computer vision finally we show the usefulness of non negative sparse pca for unsupervised feature selection in a gene clustering task 
in this paper we show how common speech recognition training criterion such a the minimum phone error criterion or the maximum mutual information criterion can be extended to incorporate a margin term different margin based training algorithm have been proposed to refine existing training algorithm for general machine learning problem however for speech recognition some special problem have to be addressed and all approach proposed either lack practical applicability or the inclusion of a margin term enforces significant change to the underlying model e g the optimization algorithm the loss function or the parameterization of the model in our approach the conventional training criterion are modified to incorporate a margin term this allows u to do large margin training in speech recognition using the same efficient algorithm for accumulation and optimization and to use the same software a for conventional discriminative training we show that the proposed criterion are equivalent to support vector machine with suitable smooth loss function approximating the non smooth hinge loss function or the hard error e g phone error experimental result are given for two different task the rather simple digit string recognition task sietill which severely suffers from overfitting and the large vocabulary european parliament plenary session english task which is supposed to be dominated by the risk and the generalization doe not seem to be such an issue 
we consider the problem of transforming a signal to a representation in which the component are statistically independent when the signal is generated a a linear transformation of independent gaussian or non gaussian source the solution may be computed using a linear transformation pca or ica respectively here we consider a complementary case in which the source is non gaussian but elliptically symmetric such a source cannot be decomposed into independent component using a linear transform but we show that a simple nonlinear transformation which we call radial gaussianization rg is able to remove all dependency we apply this methodology to natural signal demonstrating that the joint distribution of nearby bandpass filter response for both sound and image are closer to being elliptically symmetric than linearly transformed factorial source consistent with this we demonstrate that the reduction in dependency achieved by applying rg to either pair or block of bandpass filter response is significantly greater than that achieved by pca or ica 
we consider structured multi armed bandit problem based on the generalized linear model glm framework of statistic for these bandit we propose a new algorithm called glm ucb we derive finite time high probability bound on the regret of the algorithm extending previous analysis developed for the linear bandit to the non linear case the analysis highlight a key difficulty in generalizing linear bandit algorithm to the non linear case which is solved in glm ucb by focusing on the reward space rather than on the parameter space moreover a the actual effectiveness of current parameterized bandit algorithm is often poor in practice we provide a tuning method based on asymptotic argument which lead to significantly better practical performance we present two numerical experiment on real world data that illustrate the potential of the glm ucb approach 
we develop a cyclical blockwise coordinate descent algorithm for the multi task lasso that efficiently solves problem with thousand of feature and task the main result show that a closed form winsorization operator can be obtained for the sup norm penalized least square regression this allows the algorithm to find solution to very large scale problem far more efficiently than existing method this result complement the pioneering work of friedman et al for the single task lasso a a case study we use the multi task lasso a a variable selector to discover a semantic basis for predicting human neural activation the learned solution outperforms the standard basis for this task on the majority of test participant while requiring far fewer assumption about cognitive neuroscience we demonstrate how this learned basis can yield insight into how the brain represents the meaning of word 
selection of gene that are differentially expressed and critical to a particular biological process ha been a major challenge in post array analysis recent development in bioinformatics ha made various data source available such a mrna and mirna expression profile biological pathway and gene annotation etc efficient and effective integration of multiple data source help enrich our knowledge about the involved sample and gene for selecting gene bearing significant biological relevance in this work we studied a novel problem of multi source gene selection given multiple heterogeneous data source or data set select gene from expression profile by integrating information from various data source we investigated how to effectively employ information contained in multiple data source to extract an intrinsic global geometric pattern and use it in covariance analysis for gene selection we designed and conducted experiment to systematically compare the proposed approach with representative method in term of statistical and biological significance and showed the efficacy and potential of the proposed approach with promising finding 
radio frequency identification rfid promise optimization of commodity flow in all industry segment but due to physical constraint rfid technology cannot detect all rfid tag from an assembly of item this pose problem when integrating rfid data with enterprise backend system for task like inventory management or shelf replenishment in this paper we propose the tagmark method to accomplish this integration tagmark target at a retailer scenario where it estimate the number of tagged item from sample like the sale history or the tag read by smart shelf the problem is challenging because most existing estimation method depend on assumption that do not hold in typical rfid application e g static item set simple random sample or the availability of sample with user defined size tagmark adapts mark recapture method in order to provide guarantee for the accuracy of the estimation and bound for the sample size it can be implemented a a database extension allowing seamless integration into existing enterprise backend system a study with rfid equipped good acknowledges that our approach is effective in realistic scenario and database experiment with up to item confirm that it can be efficiently implemented finally we explore a broad range of extreme condition that might stress tagmark including a thief who know the location of unread item 
we consider the problem of multiple kernel learning mkl which can be formulated a a convex concave problem in the past two efficient method i e semi infinite linear programming silp and subgradient descent sd have been proposed for large scale multiple kernel learning despite their success both method have their own shortcoming a the sd method utilizes the gradient of only the current solution and b the silp method doe not regularize the approximate solution obtained from the cutting plane model in this work we extend the level method which wa originally designed for optimizing non smooth objective function to convex concave optimization and apply it to multiple kernel learning the extended level method overcomes the drawback of silp and sd by exploiting all the gradient computed in past iteration and by regularizing the solution via a projection to a level set empirical study with eight uci datasets show that the extended level method can significantly improve efficiency by saving on average of computational time over the silp method and over the sd method 
abstract a visual attention system should respond placidly when common stimulus are presented while at the same time keep alert to anomalous visual input in this paper a dynamic visual attention model based on the rarity of feature is proposed we introduce the incremental coding length icl to measure the perspective entropy gain of each feature the objective of our model is to maximize the entropy of the sampled visual feature in order to optimize energy consumption the limit amount of energy of the system is re distributed amongst feature according to their incremental coding length by selecting feature with large coding length increment the computational system can achieve attention selectivity in both static and dynamic scene we demonstrate that the proposed model achieves superior accuracy in comparison to mainstream approach in static saliency map generation moreover we also show that our model capture several le reported dynamic visual search behavior such a attentional swing and inhibition of return 
typically agent evaluation is done through monte carlo estimation however stochastic agent decision and stochastic outcome can make this approach inefficient requiring many sample for an accurate estimate we present a new technique that can be used to simultaneously evaluate many strategy while playing a single strategy in the context of an extensive game this technique is based on importance sampling but utilizes two new mechanism for significantly reducing variance in the estimate we demonstrate it effectiveness in the domain of poker where stochasticity make traditional evaluation problematic 
we extend the well known bfgs quasi newton method and it limited memory variant lbfgs to the optimization of non smooth convex objective this is done in a rigorous fashion by generalizing three component of bfgs to subdifferentials the local quadratic model the identification of a descent direction and the wolfe line search condition we apply the resulting sublbfgs algorithm to l regularized risk minimization with binary hinge loss and it direction finding component to l regularized risk minimization with logistic loss in both setting our generic algorithm perform comparable to or better than their counterpart in specialized state of the art solver 
this paper describes the minefleet distributed vehicle performance data mining system designed for commercial fleet minefleet analyzes high throughput data stream onboard the vehicle generates the analytics sends those to the remote server over the wide area wireless network and offer them to the fleet manager using stand alone and web based user interface the paper describes the overall architecture of the system business need and share experience from successful large scale commercial deployment minefleet is probably one of the first commercially successful distributed data stream mining system this patented technology ha been adopted productized and commercially offered by many large company in the mobile resource management and gps fleet tracking industry this paper offer an overview of the system and offer a detailed analysis of what made it work 
we consider the problem of identifying authoritative user in yahoo answer a common approach is to use link analysis technique in order to provide a ranked list of user based on their degree of authority a major problem for such an approach is determining how many user should be chosen a authoritative from a ranked list to address this problem we propose a method for automatic identification of authoritative actor in our approach we propose to model the authority score of user a a mixture of gamma distribution the number of component in the mixture is estimated by the bayesian information criterion bic while the parameter of each component are estimated using the expectation maximization em algorithm this method allows u to automatically discriminate between authoritative and non authoritative user the suitability of our proposal is demonstrated in an empirical study using datasets from yahoo answer 
we are interested in identifying the domain expertise of developer of a software system a developer gain expertise on the code base a well a the domain of the software system he she develops this information form a useful input in allocating software implementation task to developer domain concept represented by the system are discovered by taking into account the linguistic information available in the source code the vocabulary contained in source code a identifier such a class method variable name and comment are extracted concept present in the code base are identified and grouped based on a well known text processing hypothesis word are similar to the extent to which they share similar word the developer s association with the source code and the concept it represents is arrived at using the version repository information in this line the analysis first derives document from source code by discarding all the programming language construct kmeans clustering is further used to cluster document and extract closely related concept the key concept present in the document authored by the developer determine his her domain expertise to validate our approach we apply it on large software system two of which are presented in detail in this paper 
many machine learning algorithm can be formulated a a generalized eigenvalue problem one major limitation of such formulation is that the generalized eigenvalue problem is computationally expensive to solve especially for large scale problem in this paper we show that under a mild condition a class of generalized eigenvalue problem in machine learning can be formulated a a least square problem this class of problem include classical technique such a canonical correlation analysis cca partial least square pls and linear discriminant analysis lda a well a hypergraph spectral learning hsl a a result various regularization technique can be readily incorporated into the formulation to improve model sparsity and generalization ability in addition the least square formulation lead to efficient and scalable implementation based on the iterative conjugate gradient type algorithm we report experimental result that confirm the established equivalence relationship result also demonstrate the efficiency and effectiveness of the equivalent least square formulation on large scale problem 
we present a general pac bayes theorem from which all known pac bayes risk bound are obtained a particular case we also propose different learning algorithm for finding linear classifier that minimize these bound these learning algorithm are generally competitive with both adaboost and the svm 
the importance of bringing causality into play when designing feature selection method is more and more acknowledged in the machine learning community this paper proposes a filter approach based on information theory which aim to prioritise direct causal relationship in feature selection problem where the ratio between the number of feature and the number of sample is high this approach is based on the notion of interaction which is shown to be informative about the relevance of an input subset a well a it causal relationship with the target the resulting filter called mimr min interaction max relevance is compared with state of the art approach classification result on real microarray datasets show that the incorporation of causal aspect in the feature assessment is beneficial both for the resulting accuracy and stability a toy example of causal discovery show the effectiveness of the filter for identifying direct causal relationship 
opinion mining became an important topic of study in recent year due to it wide range of application there are also many company offering opinion mining service one problem that ha not been studied so far is the assignment of entity that have been talked about in each sentence let u use forum discussion about product a an example to make the problem concrete in a typical discussion post the author may give opinion on multiple product and also compare them the issue is how to detect what product have been talked about in each sentence if the sentence contains the product name they need to be identified we call this problem entity discovery if the product name are not explicitly mentioned in the sentence but are implied due to the use of pronoun and language convention we need to infer the product we call this problem entity assignment these problem are important because without knowing what product each sentence talk about the opinion mined from the sentence is of little use in this paper we study these problem and propose two effective method to solve the problem entity discovery is based on pattern discovery and entity assignment is based on mining of comparative sentence experimental result using a large number of forum post demonstrate the effectiveness of the technique our system ha also been successfully tested in a commercial setting 
the cluster assumption is exploited by most semi supervised learning ssl method however if the unlabeled data is merely weakly related to the target class it becomes questionable whether driving the decision boundary to the low density region of the unlabeled data will help the classification in such case the cluster assumption may not be valid and consequently how to leverage this type of unlabeled data to enhance the classification accuracy becomes a challenge we introduce semi supervised learning with weakly related unlabeled data sslw an inductive method that build upon the maximum margin approach towards a better usage of weakly related unlabeled information although the sslw could improve a wide range of classification task in this paper we focus on text categorization with a small training pool the key assumption behind this work is that even with different topic the word usage pattern across different corpus tends to be consistent to this end sslw estimate the optimal wordcorrelation matrix that is consistent with both the co occurrence information derived from the weakly related unlabeled document and the labeled document for empirical evaluation we present a direct comparison with a number of stateof the art method for inductive semi supervised learning and text categorization we show that sslw result in a significant improvement in categorization accuracy equipped with a small training set and an unlabeled resource that is weakly related to the test domain semi supervised learning ssl take advantage of a large amount of unlabeled data to enhance classification accuracy it application to text categorization is stimulated by the easy availability of an overwhelming number of unannotated web page in contrast to the limited number of annotated one intuitively corpus with different topic may not be content wise related however word usage exhibit consistent pattern within a language then the question is what would be an effective ssl strategy to extract these valuable word usage pattern embedded in the unlabeled corpus in this paper we aim to identify a new data representation that is on one hand informative to the target class category and on the other hand consistent with the feature coherence pattern exhibiting in the weakly related unlabeled data we further turn it into a convex optimization problem and solve it efficiently by an approximate approach in this section we first review the two type of semisupervised learning transductive ssl and inductive ssl then we state ssl with weakly related unlabeled data a a new challenge finally we provide a strategy of how to address this challenge in the domain of text categorization a well a a brief summary of related work in text categorization 
previous study on multi instance learning typically treated instance in the bag a independently and identically distributed the instance in a bag however are rarely independent in real task and a better performance can be expected if the instance are treated in an non i i d way that exploit relation among instance in this paper we propose two simple yet effective method in the first method we explicitly map every bag to an undirected graph and design a graph kernel for distinguishing the positive and negative bag in the second method we implicitly construct graph by deriving affinity matrix and propose an efficient graph kernel considering the clique information the effectiveness of the proposed method are validated by experiment 
data mining technique extract interesting pattern out of large data resource meaningful visualization and interactive exploration of pattern are crucial for knowledge discovery visualization technique exist for traditional clustering in low dimensional space in high dimensional data cluster typically only exist in subspace projection this subspace clustering however lack interactive visualization tool challenge arise from typically large result set in different subspace projection that hinder comparability visualization and understandability in this work we describe morpheus a tool that support the knowledge discovery process through visualization and interactive exploration of subspace clustering user may browse an overview of the entire subspace clustering analyze subspace cluster characteristic in depth and zoom into object grouping bracketing of different parameter setting enables user to immediately see the effect of parameter and to provide feedback to further improve the subspace clustering furthermore morpheus may serve a a teaching and exploration tool for the data mining community to visually ass different subspace clustering paradigm 
effective diagnosis of alzheimer s disease ad is of primary importance in biomedical research recent study have demonstrated that neuroimaging parameter are sensitive and consistent measure of ad in addition genetic and demographic information have also been successfully used for detecting the onset and progression of ad the research so far ha mainly focused on studying one type of data source only it is expected that the integration of heterogeneous data neuroimages demographic and genetic measure will improve the prediction accuracy and enhance knowledge discovery from the data such a the detection of biomarkers in this paper we propose to integrate heterogeneous data for ad prediction based on a kernel method we further extend the kernel framework for selecting feature biomarkers from heterogeneous data source the proposed method is applied to a collection of mri data from normal healthy control and ad patient the mri data are pre processed using tensor factorization in this study we treat the complementary voxel based data and region of interest roi data from mri a two data source and attempt to integrate the complementary information by the proposed method experimental result show that the integration of multiple data source lead to a considerable improvement in the prediction accuracy result also show that the proposed algorithm identifies biomarkers that play more significant role than others in ad diagnosis 
we address the problem of learning decision function from training data in which some attribute value are unobserved this problem can arise for instance when training data is aggregated from multiple source and some source record only a subset of attribute we derive a generic joint optimization problem in which the distribution governing the missing value is a free parameter we show that the optimal solution concentrate the density mass on finitely many imputation and provide a corresponding algorithm for learning from incomplete data we report on empirical result on benchmark data and on the email spam application that motivates our work 
model based algorithm are emerging a a preferred method for document clustering a computing resource improve method such a gibbs sampling have become more common for parameter estimation in these model gibbs sampling is well understood for many application but ha not been extensively studied for use in document clustering we explore the convergence rate the possibility of label switching and chain summarization methodology for document clustering on a particular model namely a mixture of multinomial model and show that fairly simple method can be employed while still producing clustering of superior quality compared to those produced with the em algorithm 
many recommendation and retrieval task can be represented a proximity query on a labeled directed graph with typed node representing document term and metadata and labeled edge representing the relationship between them recent work ha shown that the accuracy of the widely used random walk based proximity measure can be improved by supervised learning in particular one especially effective learning technique is based on path constrained random walk pcrw in which similarity is defined by a learned combination of constrained random walker each constrained to follow only a particular sequence of edge label away from the query node the pcrw based method significantly outperformed unsupervised random walk based query and model with learned edge weight unfortunately pcrw query system are expensive to evaluate in this study we evaluate the use of approximation to the computation of the pcrw distribution including fingerprinting particle filtering and truncation strategy in experiment on several recommendation and retrieval problem using two large scientific publication corpus we show speedup of factor of to with little loss in accuracy 
p when we perceive our environment make a decision or take an action our brain ha to deal with multiple source of uncertainty the bayesian framework of statistical estimation provides computational method for dealing optimally with uncertainty bayesian inference however is algorithmically quite complex and learning of bayesian inference involves the storage and updating of probability table or other data structure that are hard to implement in neural network hence it is unclear how our nervous system could acquire the capability to approximate optimal bayesian inference and action selection this article show that the simplest and experimentally best supported type of synaptic plasticity hebbian learning can in principle achieve this even inference in complex bayesian network can be approximated by hebbian learning in combination with population coding and lateral inhibition textquoteleft textquoteleft winner take all textquoteright textquoteright in cortical microcircuit that produce a sparse encoding of complex sensory stimulus we also show that a corresponding reward modulated hebbian plasticity rule provides a principled framework for understanding how bayesian inference could support fast reinforcement learning in the brain in particular we show that recent experimental result by yang and shadlen on reinforcement learning of probabilistic inference in primate can be modeled in this way p 
in an online rating system raters assign rating to object contributed by other user in addition raters can develop trust and distrust on object contributor depending on a few rating and trust related factor previous study ha shown that rating and trust link can influence each other but there ha been a lack of a formal model to relate these factor together in this paper we therefore propose trust antecedent factor taf model a novel probabilistic model that generate rating based on a number of rater s and contributor s factor we demonstrate that parameter of the model can be learnt by collapsed gibbs sampling we then apply the model to predict trust and distrust between raters and review contributor using a real data set our experiment have shown that the proposed model is capable of predicting both trust and distrust in a unified way the model can also determine user factor which otherwise cannot be observed from the rating and trust data 
in our work we address the problem of modeling social network generation which explains both link and group formation recent study on social network evolution propose generative model which capture the statistical property of real world network related only to node to node link formation we propose a novel model which capture the co evolution of social and affiliation network we provide surprising insight into group formation based on observation in several real world network showing that user often join group for reason other than their friend our experiment show that the model is able to capture both the newly observed and previously studied network property this work is the first to propose a generative model which capture the statistical property of these complex network the proposed model facilitates controlled experiment which study the effect of actor behavior on the evolution of affiliation network and it allows the generation of realistic synthetic datasets 
we study convergence property of empirical minimization of a stochastic strongly convex objective where the stochastic component is linear we show that the value attained by the empirical minimizer converges to the optimal value with rate n the result applies in particular to the svm objective thus we obtain a rate of n on the convergence of the svm objective with fixed regularization parameter to it infinite data limit we demonstrate how this is essential for obtaining certain type of oracle inequality for svms the result extend also to approximate minimization a well a to strong convexity with respect to an arbitrary norm and so also to objective regularized using other p norm 
the inverse dynamic problem for a robotic manipulator is to compute the torque needed at the joint to drive it along a given trajectory it i s beneficial to be able to learn this function for adaptive control a robotic manip ulator will often need to be controlled while holding different load in it end eff ector giving rise to a multi task learning problem by placing independent gaussian process prior over the latent function of the inverse dynamic we obtain a multi task gaussian process prior for handling multiple load where the inter ta k similarity depends on the underlying inertial parameter experiment demonstr ate that this multi task formulation is effective in sharing information among the various load and generally improves performance over either learning only on si ngle task or pooling the data over all task 
for some time there ha been increasing interest in the problem of monitoring the occurrence of topic in a stream of event such a a stream of news article this ha led to different model of burst in these stream i e period of elevated occurrence of event today there are several burst definition and detection algorithm and their difference can produce very different result in topic stream these definition also share a fundamental problem they define burst in term of an arrival rate this approach is limiting other stream dimension can matter we reconsider the idea of burst from the standpoint of a simple kind of physic instead of focusing on arrival rate we reconstruct burst a a dynamic phenomenon using kinetics concept from physic mass and velocity and derive momentum acceleration and force from these we refer to the result a topic dynamic permitting a hierarchical expressive model of burst a interval of increasing momentum a a sample application we present a topic dynamic model for the large pubmed medline database of biomedical publication using the mesh medical subject heading topic hierarchy we show our model is able to detect burst for mesh term accurately a well a efficiently 
the explosion of user generated content on the web ha led to new opportunity and significant challenge for company that are increasingly concerned about monitoring the discussion around their product tracking such discussion on weblogs provides useful insight on how to improve product or market them more effectively an important component of such analysis is to characterize the sentiment expressed in blog about specific brand and product sentiment analysis focus on this task of automatically identifying whether a piece of text express a positive or negative opinion about the subject matter most previous work in this area us prior lexical knowledge in term of the sentiment polarity of word in contrast some recent approach treat the task a a text classification problem where they learn to classify sentiment based only on labeled training data in this paper we present a unified framework in which one can use background lexical information in term of word class association and refine this information for specific domain using any available training example empirical result on diverse domain show that our approach performs better than using background knowledge or training data in isolation a well a alternative approach to using lexical knowledge with text classification 
we adapt a probabilistic latent variable model namely gap gamma poisson to ad targeting in the context of sponsored search s and behaviorally targeted bt display advertising we also approach the important problem of ad positional bias by formulating a one latent dimension gap factorization learning from click through data is intrinsically large scale even more so for ad we scale up the algorithm to terabyte of real world s and bt data that contains hundred of million of user and hundred of thousand of feature by leveraging the scalability characteristic of the algorithm and the inherent structure of the problem including data sparsity and locality specifically we demonstrate two somewhat orthogonal philosophy of scaling algorithm to large scale problem through the s and bt implementation respectively finally we report the experimental result using yahoo s vast datasets and show that our approach substantially outperform the state of the art method in prediction accuracy for bt in particular the roc area achieved by gap is exceeding while one prior approach using poisson regression yielded for computational performance we compare a single node sparse implementation with a parallel implementation using hadoop mapreduce the result are counterintuitive yet quite interesting we therefore provide insight into the underlying principle of large scale learning 
a electronic communication medium and commerce increasingly permeate every aspect of modern life real time personalization of consumer experience through data mining becomes practical effective classification prediction and change modeling of consumer interest behavior and purchasing habit using machine learning and statistical method drive efficiency insight and consumer relevance that were never before possible the internet ha brought on a rapid evolution in advertising everything about behavior on the internet can be quantified and response to behavior can occur in real time this dynamic interaction with the user ha created opportunity to better understand the way in which individual move from awareness of a product to considering a purchase through to intent and ultimately a sale for the marketer when a marketer can answer the question did those tv ad cause consumer to switch shampoo brand they can model behavior change and adjust marketing strategy accordingly underpinning this shift in how the world s trillion dollar marketing budget is spent is transactional data on an unprecedented scale creating new challenge for software that must interpret this stream and make real time decision ten even hundred of thousand of time every second i will explore advance in modeling medium consumption advertising response and the real time evaluation of medium opportunity through reference to quantcast a business launched in september which today interprets in excess of billion new digital medium consumption record every day we will examine the challenge of applying machine learning to non search advertising and in doing so explore the creation of business environment organization infrastructure tool process and cost consideration in which scientist can quickly develop new petabyte scale algorithmic approach migrate them rapidly to real time production and deliver fully customized experience for marketer publisher and consumer alike 
given ann vertex weighted tree with structural diameter and a subset ofm vertex we present a technique to compute a correspondingm m gram matrix of the pseudoinverse of the graph laplacian in o n m m time we discus the application of this technique to fast label prediction on a generic graph we approximate the graph with a spanning tree and then we predict with the kernel perceptron we address the approximation of the graph with either a minimum spanning tree or a shortest path tree the fast computation of the pseudoinverse enables u to address prediction problem on large graph we present experiment on two web spam classification task one of which includes a graph with vertex and more than edge the result indicate that the accuracy of our technique is competitive with previous method using the full graph information 
an anytime algorithm is capable of returning a response to the given task at essentially any time typically the quality of the response improves a the time increase here we consider the challenge of learning when we should terminate such algorithm on each of a sequence of iid task to optimize the expected average reward per unit time we provide a system for addressing this challenge which combine the global optimizer cross entropy method with local gradient ascent this paper theoretically investigates how far the estimated gradient is from the true gradient then empirically demonstrates that this system is effective by applying it to a toy problem a well a on a real world face detection task 
we consider the general problem of learning from both pairwise constraint and unlabeled data the pairwise constraint specify whether two object belong to the same class or not known a the must link constraint and the cannot link constraint we propose to learn a mapping that is smooth over the data graph and map the data onto a unit hypersphere where two must link object are mapped to the same point while two cannot link object are mapped to be orthogonal we show that such a mapping can be achieved by formulating a semidefinite programming problem which is convex and can be solved globally our approach can effectively propagate pairwise constraint to the whole data set it can be directly applied to multi class classification and can handle data label pairwise constraint or a mixture of them in a unified framework promising experimental result are presented for classification task on a variety of synthetic and real data set 
various online social network osns have been developed rapidly on the internet researcher have analyzed different property of such osns mainly focusing on the formation and evolution of the network a well a the information propagation over the network in knowledge sharing osns such a blog and question answering system issue on how user participate in the network and how user generate contribute knowledge are vital to the sustained and healthy growth of the network however related discussion have not been reported in the research literature in this work we empirically study workload from three popular knowledge sharing osns including a blog system a social bookmark sharing network and a question answering social network to examine these property our analysis consistently show that user posting behavior in these network exhibit strong daily and weekly pattern but the user active time in these osns doe not follow exponential distribution the user posting behavior in these osns follows stretched exponential distribution instead of power law distribution indicating the influence of a small number of core user cannot dominate the network the distribution of user contribution on high quality and effort consuming content in these osns have smaller stretch factor for the stretched exponential distribution our study provides insight into user activity pattern and lay out an analytical foundation for further understanding various property of these osns 
we consider a supervised machine learning scenario where label are provided by a heterogeneous set of teacher some of which are mediocre incompetent or perhaps even malicious we present an algorithm built on the svm framework that explicitly attempt to cope with low quality and malicious teacher by decreasing their influence on the learning process our algorithm doe not receive any prior information on the teacher nor doe it resort to repeated labeling where each example is labeled by multiple teacher we provide a theoretical analysis of our algorithm and demonstrate it merit empirically finally we present a second algorithm with promising empirical result but without a formal analysis 
in many domain data are distributed among datasets that share only some variable other recorded variable may occur in only one dataset while there are asymptotically correct informative algorithm for disco vering causal relationship from a single dataset even with missing value and hidden variable there have been no such reliable procedure for distributed data with overlapping variable we present a novel asymptotically correct procedure that discovers a minimal equivalence class of causal dag structure using local independence information from distributed data of this form and evaluate it performance using synthetic and real world data against causal discovery algori thm for single datasets and applying structural em a heuristic dag structure learning procedure for data with missing value to the concatenated data 
this paper tackle the problem of summarizing frequent itemsets we observe that previous notion of summary cannot be directly used for analyzing frequent itemsets in order to be used for analysis one requirement is that the analyst should be able to browse all frequent itemsets by only having the summary for this purpose we propose to build the summary based upon a novel formulation conditional profile or c profile several feature of our proposed summary are each profile in the summary can be analyzed independently it provides error guarantee adequate and it produce no false positive or false negative having the formulation the next challenge is to produce the most concise summary which satisfies the requirement in this paper we also designed an algorithm which is both effective and efficient for this task the quality of our approach is justified by extensive experiment the implementation for the algorithm are available from www cais ntu edu sg vivek pub cprofile 
we combine bayesian online change point detection with gaussian process to create a nonparametric time series model which can handle change point the model can be used to locate change point in an online manner and unlike other bayesian online change point detection algorithm is applicable when temporal correlation in a regime are expected we show three variation on how to apply gaussian process in the change point context each with their own advantage we present method to reduce the computational burden of these model and demonstrate it on several real world data set 
sharing healthcare data ha become a vital requirement in healthcare system management however inappropriate sharing and usage of healthcare data could threaten patient privacy in this paper we study the privacy concern of the blood transfusion information sharing system between the hong kong red cross blood transfusion service bts and public hospital and identify the major challenge that make traditional data anonymization method not applicable furthermore we propose a new privacy model called lkc privacy together with an anonymization algorithm to meet the privacy and information requirement in this bts case experiment on the real life data demonstrate that our anonymization algorithm can effectively retain the essential information in anonymous data for data analysis and is scalable for anonymizing large datasets 
tsochantaridis et al proposed two formulation for maximum margin training of structured space margin scaling and slack scaling while margin scaling ha been extensively used since it requires the same kind of map inference a normal structured prediction slack scaling is believed to be more accurate and better behaved we present an efficient variational approximation to the slack scaling method that solves it inference bottleneck while retaining it accuracy advantage over margin scaling we further argue that existing scaling approach do not separate the true labeling comprehensively while generating violating constraint we propose a new max margin trainer poslearn that generates violator to ensure separation at each position of a decomposable loss function empirical result on real datasets illustrate that poslearn can reduce test error by up to over margin scaling and over slack scaling further poslearn violator can be generated more efficiently than slack violator for many structured task the time required is just twice that of map inference 
the gaussian process gp is a popular way to specify dependency between random variable in a probabilistic model in the bayesian framework the covariance structure can be specified using unknown hyperparameters integrating over these hyperparameters considers different possible explanation for the data when making prediction this integration is often performed using markov chain monte carlo mcmc sampling however with non gaussian observation standard hyperparameter sampling approach require careful tuning and may converge slowly in this paper we present a slice sampling approach that requires little tuning while mixing well in both strong and weak data regime 
graph based semi supervised learning ssl method play an increasingly important role in practical machine learning system a crucial step in graph based ssl method is the conversion of data into a weighted graph however most of the ssl literature focus on developing label inference algorithm without extensively studying the graph building method and it effect on performance this article provides an empirical study of leading semi supervised method under a wide range of graph construction algorithm these ssl inference algorithm include the local and global consistency lgc method the gaussian random field grf method the graph transduction via alternating minimization gtam method a well a other technique several approach for graph construction sparsification and weighting are explored including the popular k nearest neighbor method knn and the b matching method a opposed to the greedily constructed knn graph the b matched graph ensures each node in the graph ha the same number of edge and produce a balanced or regular graph experimental result on both artificial data and real benchmark datasets indicate that b matching produce more robust graph and therefore provides significantly better prediction accuracy without any significant change in computation time 
we address the problem of computing the optimal q function in markov decision problem with infinite state space we analyze the convergence property of several variation of q learning when combined with function approximation extending the analysis of td learning in tsitsiklis van roy a to stochastic control setting we identify condition under which such approximate method converge with probability we conclude with a brief discussion on the general applicability of our result and compare them with several related work 
we combine the speed and scalability of information retrieval with the generally superior classification accuracy offered by machine learning yielding a two phase text classifier that can scale to very large document corpus we investigate the effect of different method of formulating the query from the training set a well a varying the query size in empirical test on the reuters rcv corpus of document we find runtime wa easily reduced by a factor of x with a somewhat surprising gain in f measure compared with traditional text classification 
counting the number of triangle in a graph is a beautiful algorithmic problem which ha gained importance over the last year due to it significant role in complex network analysis metric frequently computed such a the clustering coefficient and the transitivity ratio involve the execution of a triangle counting algorithm furthermore several interesting graph mining application rely on computing the number of triangle in the graph of interest in this paper we focus on the problem of counting triangle in a graph we propose a practical method out of which all triangle counting algorithm can potentially benefit using a straightforward triangle counting algorithm a a black box we performed experiment on real world network and on synthetic datasets a well where we show that our method work with high accuracy typically more than and give significant speedup resulting in even time faster performance 
this paper present a cutting plane algorithm for multiclass maximum margin clustering mmc the proposed algorithm construct a nested sequence of successively tighter relaxation of the original mmc problem and each optimization problem in this sequence could be efficiently solved using the constrained concave convex procedure cccp experimental evaluation on several real world datasets show that our algorithm converges much faster than existing mmc method with guaranteed accuracy and can thus handle much larger datasets efficiently 
a natural evaluation metric for statistical topic model is the probability of held out document given a trained model while exact computation of this probability is intractable several estimator for this probability have been used in the topic modeling literature including the harmonic mean method and empirical likelihood method in this paper we demonstrate experimentally that commonly used method are unlikely to accurately estimate the probability of held out document and propose two alternative method that are both accurate and efficient 
in this paper we develop an efficient moment based permutation test approach to improve the test s computational efficiency by approximating the permutation distribution of the test statistic with pearson distribution series this approach involves the calculation of the first four moment of the permutation distribution we propose a novel recursive method to derive these moment theoretically and analytically without any permutation experimental result using different test statistic are demonstrated using simulated data and real data the proposed strategy take advantage of nonparametric permutation test and parametric pearson distribution approximation to achieve both accuracy and efficiency 
we consider random projection in conjunction with classification specifically the analysis of fisher s linear discriminant fld classifier in randomly projected data space unlike previous analysis of other classifier in this setting we avoid the unnatural effect that arise when one insists that all pairwise distance are approximately preserved under projection we impose no sparsity or underlying low dimensional structure constraint on the data we instead take advantage of the class structure inherent in the problem we obtain a reasonably tight upper bound on the estimated misclassification error on average over the random choice of the projection which in contrast to early distance preserving approach tightens in a natural way a the number of training example increase it follows that for good generalisation of fld the required projection dimension grows logarithmically with the number of class we also show that the error contribution of a covariance misspecification is always no worse in the low dimensional space than in the initial high dimensional space we contrast our finding to previous related work and discus our insight 
consider the following problem given set of unlabeled observation each set with known label proportion predict the label of another set of observation also with known label proportion this problem appears in area like e commerce spam filtering and improper content detection we present consistent estimator which can reconstruct the correct label with high probability in a uniform convergence sense experiment show that our method work well in practice 
we present an algorithm for finding an s sparse vector x that minimizes the square error y x where satisfies the restricted isometry property rip with isometric constant s grade gradient descent with sparsification iteratively update x a equation where and h set all but s largest magnitude coordinate to zero grade converges to the correct solution in constant number of iteration the condition s near linear time algorithm is known in comparison the best condition under which a polynomial time algorithm is known is s our matlab implementation of grade outperforms previously proposed algorithm like subspace pursuit stomp omp and lasso by an order of magnitude curiously our experiment also uncovered case where l regularized regression lasso fails but grade find the correct solution 
high dimensionality can pose severe difficulty widely recognized a different aspect of the curse of dimensionality in this paper we study a new aspect of the curse pertaining to the distribution of k occurrence i e the number of time a point appears among the k nearest neighbor of other point in a data set we show that a dimensionality increase this distribution becomes considerably skewed and hub point emerge point with very high k occurrence we examine the origin of this phenomenon showing that it is an inherent property of high dimensional vector space and explore it influence on application based on measuring distance in vector space notably classification clustering and information retrieval 
this paper describes a recursive estimation procedure for multivariate binary density using orthogonalexpansions ford covariates there are d basis coefficient to estimate which render conventional approach computationally prohibitive when d is large however for a wide class of density that satisfy a certain sparsity condition our estimator run in probabilistic polynomial time and adapts to the unknown sparsity of the underlying density in two key way it attains near minimax mean squared error and the computational complexity is lower for sparser density our method also allows for flexible control of the trade off between mean squared error and computational complexity 
assume a network v e where a subset of the node in v are active we consider the problem of selecting a set of k active node that best explain the observed activation state under a given information propagation model we call these node effector we formally define the k effector problem and study it complexity for different type of graph we show that for arbitrary graph the problem is not only np hard to solve optimally but also np hard to approximate we also show that for some special case the problem can be solved optimally in polynomial time using a dynamic programming algorithm to the best of our knowledge this is the first work to consider the k effector problem in network we experimentally evaluate our algorithm using the dblp co authorship graph where we search for effector of topic that appear in research paper 
the maximum margin clustering approach is a recently proposed extension of the concept of support vector machine to the clustering problem briefly stated it aim at finding an optimal partition of the data into two class such that the margin induced by a subsequent application of a support vector machine is maximal we propose a method based on stochastic search to address this hard optimization problem while a direct implementation would be infeasible for large data set we present an efficient computational shortcut for assessing the quality of intermediate solution experimental result show that our approach outperforms existing method in term of clustering accuracy 
abstract intro an overview is given of the largest industrial ocr application world wide postal address reading how it came into being how it evolved rapidly to it current state of the art and what are it future prospect some prominent historical system methodological culturaland social aspect are illuminated 
we show that the brier game of prediction is mixable and flnd the optimal learning rate and substitution function for it the resulting prediction algorithm is applied to predict result of football and tennis match the theoretical performance guarantee turn out to be rather tight on these data set especially in the case of the more extensive tennis data 
for many supervised learning problem we posse prior knowledge about which feature yield similar information about the target variab le in predicting the topic of a document we might know that two word are synonym and when performing image recognition we know which pixel are adjacent such synonymous or neighboring feature are near duplicate and should be expected to have similar weight in an accurate model here we present a framework for regularized learning when one ha prior knowledge about which feature are expected to have similar and dissimilar weight the prior knowledge is encoded a a network whose vertex are feature and whose edge represent similariti e and dissimilarity between them during learning each feature s weight is penal ized by the amount it differs from the average weight of it neighbor for text classification regularization using network of word co occurrence outperforms manifold learning and compare favorably to other recently proposed semi supervised learning method for sentiment analysis feature network constructed from declarative human knowledge significantly improve prediction accuracy 
we consider the problem of learning from noisy side information in the form of pairwise constraint although many algorithm have been developed to learn from side information most of them assume perfect pairwise constraint given the pairwise constraint are often extracted from data source such a paper citation they tend to be noisy and inaccurate in this paper we introduce the generalization of maximum entropy model and propose a framework for learning from noisy side information based on the generalized maximum entropy model the theoretic analysis show that under certain assumption the classification model trained from the noisy side information can be very close to the one trained from the perfect side information extensive empirical study verify the effectiveness of the proposed framework 
we present an algorithm hi mat hierarchy induction via model and trajectory that discovers maxq task hierarchy by applying dynamic bayesian network model to a successful trajectory from a source reinforcement learning task hi mat discovers subtasks by analyzing the causal and temporal relationship among the action in the trajectory under appropriate assumption hi mat induces hierarchy that are consistent with the observed trajectory and have compact value function table employing safe state abstraction we demonstrate empirically that hi mat construct compact hierarchy that are comparable to manually engineered hierarchy and facilitate significant speedup in learning when transferred to a target task 
in this paper we study how to improve nearest neighbor classification by learning a mahalanobis distance metric we build on a recently proposed framework for distance metric learning known a large margin nearest neighbor lmnn classification our paper make three contribution first we describe a highly efficient solver for the particular instance of semidefinite programming that arises in lmnn classification our solver can handle problem with billion of large margin constraint in a few hour second we show how to reduce both training and testing time using metric ball tree the speedup from ball tree are further magnified by learning low dimensional representation of the input space third we show how to learn different mahalanobis distance metric in different part of the input space for large data set the use of locally adaptive distance metric lead to even lower error rate 
linked or networked data are ubiquitous in many application example include web data or hypertext document connected via hyperlink social network or user profile connected via friend link co authorship and citation information blog data movie review and so on in these datasets called information network closely related object that share the same property or interest form a community for example a community in blogsphere could be user mostly interested in cell phone review and news outlier detection in information network can reveal important anomalous and interesting behavior that are not obvious if community information is ignored an example could be a low income person being friend with many rich people even though his income is not anomalously low when considered over the entire population this paper first introduces the concept of community outlier interesting point or rising star for a more positive sense and then show that well known baseline approach without considering link or community information cannot find these community outlier we propose an efficient solution by modeling networked data a a mixture model composed of multiple normal community and a set of randomly generated outlier the probabilistic model characterizes both data and link simultaneously by defining their joint distribution based on hidden markov random field hmrf maximizing the data likelihood and the posterior of the model give the solution to the outlier inference problem we apply the model on both synthetic data and dblp data set and the result demonstrate importance of this concept a well a the effectiveness and efficiency of the proposed approach 
two aspect are crucial when constructing any real world supervised classification task the set of class whose distinction might be useful for the domain expert and the set of classification that can actually be distinguished by the data often a set of label is defined with some initial intuition but these are not the best match for the task for example label have been assigned for land cover classification of the earth but it ha been suspected that these label are not ideal and some class may be best split into subclass whereas others should be merged this paper formalizes this problem using three ingredient the existing class label the underlying separability in the data and a special type of input from the domain expert we require a domain expert to specify an l l matrix of pairwise probabilistic constraint expressing their belief a to whether the l class should be kept separate merged or split this type of input is intuitive and easy for expert to supply we then show that the problem can be solved by casting it a an instance of penalized probabilistic clustering ppc our method class level ppc cppc extends ppc showing how it time complexity can be reduced from o n to o nl for the problem of class re definition we further extend the algorithm by presenting a heuristic to measure adherence to constraint and providing a criterion for determining the model complexity number of class for constraint based clustering we demonstrate and evaluate cppc on artificial data and on our motivating domain of land cover classification for the latter an evaluation by domain expert show that the algorithm discovers novel class definition that are better suited to land cover classification than the original set of label 
social medium such a blog facebook flickr etc present data in a network format rather than classical iid distribution to address the interdependency among data instance relational learning ha been proposed and collective inference based on network connectivity is adopted for prediction however connection in social medium are often multi dimensional an actor can connect to another actor for different reason e g alumnus colleague living in the same city sharing similar interest etc collective inference normally doe not differentiate these connection in this work we propose to extract latent social dimension based on network information and then utilize them a feature for discriminative learning these social dimension describe diverse affiliation of actor hidden in the network and the discriminative learning can automatically determine which affiliation are better aligned with the class label such a scheme is preferred when multiple diverse relation are associated with the same network we conduct extensive experiment on social medium data one from a real world blog site and the other from a popular content sharing site our model outperforms representative relational learning method based on collective inference especially when few labeled data are available the sensitivity of this model and it connection to existing method are also examined 
relational learning is concerned with predicting unknown value of a relation given a database of entity and observed relation among entity an example of relational learning is movie rating prediction where entity could include user movie genre and actor relation encode user rating of movie movie genre and actor role in movie a common prediction technique given one pairwise relation for example a user x movie rating matrix is low rank matrix factorization in domain with multiple relation represented a multiple matrix we may improve predictive accuracy by exploiting information from one relation while predicting another to this end we propose a collective matrix factorization model we simultaneously factor several matrix sharing parameter among factor when an entity participates in multiple relation each relation can have a different value type and error distribution so we allow nonlinear relationship between the parameter and output using bregman divergence to measure error we extend standard alternating projection algorithm to our model and derive an efficient newton update for the projection furthermore we propose stochastic optimization method to deal with large sparse matrix our model generalizes several existing matrix factorization method and therefore yield new large scale optimization algorithm for these problem our model can handle any pairwise relational schema and a wide variety of error model we demonstrate it efficiency a well a the benefit of sharing parameter among relation 
we propose a general and efficient algorithm for learning low rank matrix the proposed algorithm converges super linearly and can keep the matrix to be learned in a compact factorized representation without the need of specifying the rank beforehand moreover we show that the framework can be easily generalized to the problem of learning multiple matrix and general spectral regularization empirically we show that we can recover a matrix from million observation in about minute furthermore we show that in a brain computer interface problem the proposed method can speed up the optimization by two order of magnitude against the conventional projected gradient method and produce more reliable solution 
we propose a new neural network architecture called simple recurrent temporal difference network sr tdns that learns to predict future observation in partially observable environment sr tdns incorporate the structure of simple recurrent neural network srns into temporal difference td network to use proto predictive representation of state although they deviate from the principle of predictive representation to ground state representation on observation they follow the same learning strategy a td network i e applying td learning to general prediction simulation experiment revealed that sr tdns can correctly represent state with an incomplete set of core test question network and consequently sr tdns have better on line learning capacity than td network in various environment 
motivation data center are a critical component of modern it infrastructure but are also among the worst environmental offender through their increasing energy usage and the resulting large carbon footprint efficient management of data center including power management networking and cooling infrastructure is hence crucial to sustainability in the absence of a first principle approach to manage these complex component and their interaction data driven approach have become attractive and tenable result we present a temporal data mining solution to model and optimize performance of data center chiller a key component of the cooling infrastructure it help bridge raw numeric time series information from sensor stream toward higher level characterization of chiller behavior suitable for a data center engineer to aid in this transduction temporal data stream are first encoded into a symbolic representation next run length encoded segment are mined to form frequent motif in time series and finally these metric are evaluated by their contribution to sustainability a key innovation in our application is the ability to intersperse don t care transition e g transient in continuous valued time series data an advantage we inherit by the application of frequent episode mining to symbolized representation of numeric time series our approach provides both qualitative and quantitative characterization of the sensor stream to the data center engineer to aid him in tuning chiller operating characteristic this system is currently being prototyped for a data center managed by hp and experimental result from this application reveal the promise of our approach 
we propose two approximation algorithm for identifying community in dynamic social network community are intuitively characterized a unusually densely knit subset of a social network this notion becomes more problematic if the social interaction change over time aggregating social network over time can radically misrepresent the existing and changing community structure recently we have proposed an optimization based framework for modeling dynamic community structure also we have proposed an algorithm for finding such structure based on maximum weight bipartite matching in this paper we analyze it performance guarantee for a special case where all actor can be observed at all time in such instance we show that the algorithm is a small constant factor approximation of the optimum we use a similar idea to design an approximation algorithm for the general case where some individual are possibly unobserved at time and to show that the approximation factor increase twofold but remains a constant regardless of the input size this is the first algorithm for inferring community in dynamic network with a provable approximation guarantee we demonstrate the general algorithm on real data set the result confirm the efficiency and effectiveness of the algorithm in identifying dynamic community 
we consider a sequential decision problem where the reward are generated by a piecewise stationary distribution however the different reward distribution are unknown and may change at unknown instant our approach us a limited number of side observation on past reward but doe not require prior knowledge of the frequency of change in spite of the adversarial nature of the reward process we provide an algorithm whose regret with respect to the baseline with perfect knowledge of the distribution and the change is o k log t where k is the number of change up to time t this is in contrast to the case where side observation are not available and where the regret is at least t 
we propose a family of novel cost sensitive boosting method for multi class classification by applying the theory of gradient boosting to p norm based cost functionals we establish theoretical guarantee including proof of convergence and convergence rate for the proposed method our theoretical treatment provides interpretation for some of the existing algorithm in term of the proposed family including a generalization of the costing algorithm dse and gbse t and the average cost method we also experimentally evaluate the performance of our new algorithm against existing method of cost sensitive boosting including adacost csb and adaboost m with cost sensitive weight initialization we show that our proposed scheme generally achieves superior result in term of cost minimization and with the use of higher order p norm loss in certain case consistently outperforms the comparison method thus establishing it empirical advantage 
we present simple and computationally efficient nonparametric estimator of r e nyi entropy and mutual information based on an i i d sample drawn from an unknown absolutely continuous distribution over r d the estimator are calculated a the sum of p th power of the euclidean length of the edge of the generalized nearest neighbor graph of the sample and the empirical copula of the sample respectively for the first time we prove the almost sure consistency of these estimator and upper bound on their rate of convergence the latter of which under the assumption that the density underlying the sample is lipschitz continuous experiment demonstrate their usefulness in independent subspace analysis 
one of the original goal of computer vision wa to fully understand a natural scene this requires solving several sub problem simulta neously including object detection region labeling and geometric reasoning the last few decade have seen great progress in tackling each of these problem i n isolation only recently have researcher returned to the difficult task of con sidering them jointly in this work we consider learning a set of related model in suc h that they both solve their own problem and help each other we develop a framework called cascaded classification model ccm where repeated instantiation of these classifier are coupled by their input output variable in a cascade tha t improves performance at each level our method requires only a limited black box interface with the model allowing u to use very sophisticated state of th e art classifier without having to look under the hood we demonstrate the effectiveness of our method on a large set of natural image by combining the subtasks of scene categorization object detection multiclass image segmentation and d reconstruction 
in this paper we introduce proto transfer leaning a new framework for transfer learning we explore solution to transfer learning within reinforcement learning through the use of spectral method proto value function pvfs are basis function computed from a spectral analysis of random walk on the state space graph they naturally lead to the ability to transfer knowledge and representation between related task or domain we investigate task transfer by using the same pvfs in markov decision process mdps with different reward function additionally our experiment in domain transfer explore applying the nystrom method for interpolation of pvfs between mdps of different size 
recent approach to learning structured predictor often require approximate inference for tractability yet it effect on the learned model are unclear meanwhile most learning algorithm act a if computational cost wa constant within the model class this paper shed some light on the first issue by establishing risk bound for max margin learning with lp relaxed inference and address the second issue by proposing a new paradigm that attempt to penalize time consuming hypothesis our analysis relies on a geometric characterization of the outer polyhedron associated with the lp relaxation we then apply these technique to the problem of dependency parsing for which a concise lp formulation is provided that handle non local output feature a significant improvement is shown over arc factored model 
we introduce a new approach to non linear regression called function factorization that is suitable for problem where an output variable can reasonably be modeled by a number of multiplicative interaction term between non linear function of the input the idea is to approximate a complicated function on a high dimensional space by the sum of product of simpler function on lower dimensional subspace function factorization can be seen a a generalization of matrix and tensor factorization method in which the data are approximated by the sum of outer product of vector we present a non parametric bayesian approach to function factorization where the prior over the factorizing function are warped gaussian process and we do inference using hamiltonian markov chain monte carlo we demonstrate the superior predictive performance of the method on a food science data set compared to gaussian process regression and tensor factorization using parafac and gemanova model 
clustering problem often involve datasets where only a part of the data is relevant to the problem e g in microarray data analysis only a subset of the gene show cohesive expression within a subset of the condition feature the existence of a large number of non informative data point and feature make it challenging to hunt for coherent and meaningful cluster from such datasets additionally since cluster could exist in different subspace of the feature space a co clustering algorithm that simultaneously cluster object and feature is often more suitable a compared to one that is restricted to traditional one sided clustering we propose robust overlapping co clustering rocc a scalable and very versatile framework that address the problem of efficiently mining dense arbitrarily positioned possibly overlapping co cluster from large noisy datasets rocc ha several desirable property that make it extremely well suited to a number of real life application 
multi core processor with ever increasing number of core per chip are becoming prevalent in modern parallel computing our goal is to make use of the multi core a well a multi processor architecture to speed up data mining algorithm specifically we present a parallel algorithm for approximate learning of linear dynamical system lds also known a kalman filter kf ldss are widely used in time series analysis such a motion capture modeling visual tracking etc we propose cut and stitch ca a novel method to handle the data dependency from the chain structure of hidden variable in lds so a to parallelize the em based parameter learning algorithm we implement the algorithm using openmp on both a supercomputer and a quad core commercial desktop the experimental result show that parallel algorithm using cut and stitch achieve comparable accuracy and almost linear speedup over the serial version in addition cut and stitch can be generalized to other model with similar linear structure such a hidden markov model hmm and switching kalman filter skf 
feature selection is an important task in order to achieve better generalizability in high dimensional learning and structure learning of markov random field mrfs can automatically discover the inherent structure underlying complex data both problem can be cast a solving an l norm regularized parameter estimation problem the existing grafting method can avoid doing inference on dense graph in structure learning by incrementally selecting new feature however grafting performs a greedy step to optimize over free parameter once new feature are included this greedy strategy result in low efficiency when parameter learning is itself non trivial such a in mrfs in which parameter learning depends on an expensive subroutine to calculate gradient the complexity of calculating gradient in mrfs is typically exponential to the size of maximal clique in this paper we present a fast algorithm called grafting light to solve the l norm regularized maximum likelihood estimation of mrfs for efficient feature selection and structure learning grafting light iteratively performs one step of orthant wise gradient descent over free parameter and selects new feature this lazy strategy is guaranteed to converge to the global optimum and can effectively select significant feature on both synthetic and real data set we show that grafting light is much more efficient than grafting for both feature selection and structure learning and performs comparably with the optimal batch method that directly optimizes over all the feature for feature selection but is much more efficient and accurate for structure learning of mrfs 
existing approach to nonrigid structure from motion assume that the instantaneous d shape of a deforming object is a linear combination of basis shape which have to be estimated anew for each video sequence in contrast we propose that the evolving d structure be described by a linear combination of basis trajectory the principal advantage of this approach is that we do not need to estimate any basis vector during computation we show that generic base over trajectory such a the discrete cosine transform dct basis can be used to compactly describe most real motion this result in a significant reduction in unknown and corresponding stability in estimation we report empirical performance quantitatively using motion capture data and qualitatively on several video sequence exhibiting nonrigid motion including piece wise rigid motion partially nonrigid motion such a a facial expression and highly nonrigid motion such a a person dancing 
the effectiveness of knowledge transfer using classification algorithm depends on the difference between the distribution that generates the training example and the one from which test example are to be drawn the task can be especially difficult when the training example are from one or several domain different from the test domain in this paper we propose a locally weighted ensemble framework to combine multiple model for transfer learning where the weight are dynamically assigned according to a model s predictive power on each test example it can integrate the advantage of various learning algorithm and the labeled information from multiple training domain into one unified classification model which can then be applied on a different domain importantly different from many previously proposed method none of the base learning method is required to be specifically designed for transfer learning we show the optimality of a locally weighted ensemble framework a a general approach to combine multiple model for domain transfer we then propose an implementation of the local weight assignment by mapping the structure of a model onto the structure of the test domain and then weighting each model locally according to it consistency with the neighborhood structure around the test example experimental result on text classification spam filtering and intrusion detection data set demonstrate significant improvement in classification accuracy gained by the framework on a transfer learning task of newsgroup message categorization the proposed locally weighted ensemble framework achieves accuracy when the best single model predicts correctly only on of the test example in summary the improvement in accuracy is over and up to across different problem 
we propose a novel dependent hierarchical pitman yor process model for discrete data an incremental monte carlo inference procedure for this model is developed we show that inference in this model can be performed in constant space and linear time the model is demonstrated in a discrete sequence prediction task where it is shown to achieve state of the art sequence prediction performance while using signicantly le memory 
the study of land cover change is an important problem in the earth science domain because of it impact on local climate radiation balance biogeochemistry hydrology and the diversity and abundance of terrestrial specie most well known change detection technique from statistic signal processing and control theory are not well suited for the massive high dimensional spatio temporal data set from earth science due to limitation such a high computational complexity and the inability to take advantage of seasonality and spatio temporal autocorrelation inherent in earth science data in our work we seek to address these challenge with new change detection technique that are based on data mining approach specifically in this paper we have performed a case study for a new change detection technique for the land cover change detection problem we study land cover change in the state of california focusing on the san francisco bay area and perform an extended study on the entire state we also perform a comparative evaluation on forest in the entire state these result demonstrate the utility of data mining technique for the land cover change detection problem 
we present an active learning scheme that exploit cluster structure in data 
uncertainty arises in reinforcement learning from various source and therefore it is necessary to consider statistic based on several roll out for evaluating behavioral policy we add an adaptive uncertainty handling based on hoeffding and empirical bernstein race to the cma e a variable metric evolution strategy proposed for direct policy search the uncertainty handling adjusts individually the number of episode considered for the evaluation of a policy the performance estimation is kept just accurate enough for a sufficiently good ranking of candidate policy which is in turn sufficient for the cma e to find better solution this increase the learning speed a well a the robustness of the algorithm 
researcher in the social and behavioral science routinely rely on quasi experimental design to discover knowledge from large data base quasi experimental design qed exploit fortuitous circumstance in non experimental data to identify situation sometimes called natural experiment that provide the equivalent of experimental control and randomization qed allow researcher in domain a diverse a sociology medicine and marketing to draw reliable inference about causal dependency from non experimental data unfortunately identifying and exploiting qed ha remained a painstaking manual activity requiring researcher to scour available database and apply substantial knowledge of statistic however recent advance in the expressiveness of database and increase in their size and complexity provide the necessary condition to automatically identify qed in this paper we describe the first system to discover knowledge by applying quasi experimental design that were identified automatically we demonstrate that qed can be identified in a traditional database schema and that such identification requires only a small number of extension to that schema knowledge about quasi experimental design encoded in first order logic and a theorem proving engine we describe several key innovation necessary to enable this system including method for automatically constructing appropriate experimental unit and for creating aggregate variable on those unit we show that applying the resulting design can identify important causal dependency in real domain and we provide example from academic publishing movie making and marketing and peer production system finally we discus the integration of qed with other approach to causal discovery including joint modeling and directed experimentation 
probabilistic topic model have become popular a method for dimensionality reduction in collection of text document or image these model are usually treated a generative model and trained using maximum likelihood or bayesian method in this paper we discus an alternative a discriminative framework in which we assume that supervised side information is present and in which we wish to take that side information into account in finding a re duced dimensionality representation specifically we present disclda a dis criminative variation on latent dirichlet allocation lda in which a class dependent linear transformation is introduced on the topic mixture proportion this parameter is estimated by maximizing the conditional likelihood by using the transformed topic mixture proportion a a new representation of document we obtain a supervised dimensionality reduction algorithm that uncovers the latent structure in a document collection while preserving predictive power for the task of classification we compare the predictive power of the latent structure of disclda with unsupervised lda on the newsgroups document classification ta sk and show how our model can identify shared topic across class a well a class dependent topic 
we present the tagflake system which support semantically informed navigation within a tag cloud tagflake relies on tmine for organizing tag extracted from textual content in hierarchical organization suitable for navigation visualization classification and tracking tmine extract the most significant tag term from text document and map them onto a hierarchy in such a way that descendant term are contextually dependent on their ancestor within the given corpus of document this provides tagflake with a mechanism for enabling navigation within the tag space and for classification of the text document based on the contextual structure captured by the created hierarchy tagflake is language neutral since it doe not rely on any natural language processing technique and is unsupervised 
em algorithm is a very popular iteration based method to estimate the parameter of gaussian mixture model from a large observation set however in most case em algorithm is not guaranteed to converge to the global optimum instead it stop at some local optimum which can be much worse than the global optimum therefore it is usually required to run multiple procedure of em algorithm with different initial configuration and return the best solution to improve the efficiency of this scheme we propose a new method which can estimate an upper bound on the logarithm likelihood of the local optimum based on the current configuration after the latest em iteration this is accomplished by first deriving some region bounding the possible location of local optimum followed by some upper bound estimation on the maximum likelihood with this estimation we can terminate an em algorithm procedure if the estimated local optimum is definitely worse than the best solution seen so far extensive experiment show that our method can effectively and efficiently accelerate conventional multiple restart em algorithm 
all netflix prize algorithm proposed so far are prohibitively costly for large scale production system in this paper we describe an efficient dataflow implementation of a collaborative filtering cf solution to the netflix prize problem based on weighted coclustering the dataflow library we use facilitates the development of sophisticated parallel program designed to fully utilize commodity multicore hardware while hiding traditional difficulty such a queuing threading memory management and deadlock the dataflow cf implementation first compress the large sparse training dataset into co cluster then it generates recommendation by combining the average rating of the co cluster with the bias of the user and movie when configured to identify x co cluster in the netflix training dataset the implementation predicted over million rating in minute and achieved an rmse of without any fine tuning or domain knowledge this is an effective real time prediction runtime of u per rating which is far superior to previously reported result moreover the implemented co clustering framework support a wide variety of other large scale data mining application and form the basis for predictive modeling on large dyadic datasets 
overall performance of the data mining process depends not just on the value of the induced knowledge but also on various cost of the process itself such a the cost of acquiring and pre processing training example the cpu cost of model induction and the cost of committed error recently several progressive sampling strategy for maximizing the overall data mining utility have been proposed all these strategy are based on repeated acquisition of additional training example until a utility decrease is observed in this paper we present an alternative projective sampling strategy which fit function to a partial learning curve and a partial run time curve obtained from a small subset of potentially available data and then us these projected function to analytically estimate the optimal training set size the proposed approach is evaluated on a variety of benchmark datasets using the rapidminer environment for machine learning and data mining process the result show that the learning and run time curve projected from only several data point can lead to a cheaper data mining process than the common progressive sampling method 
a new herding algorithm is proposed which directly convert observed moment into a sequence of pseudo sample the pseudo sample respect the moment constraint and may be used to estimate unobserved quantity of interest the procedure allows u to sidestep the usual approach of first learning a joint model which is intractable and then sampling from that model which can easily get stuck in a local mode moreover the algorithm is fully deterministic avoiding random number generation and doe not need expensive operation such a exponentiation 
we present a sparse approximation approach for dependent output gaussian process gp employing a latent function framework we apply the convolution process formalism to establish dependency between output variable where each latent function is represented a a gp based on these latent function we establish an approximation scheme using a conditional independence assumption between the output process leading to an approximation of the full covariance which is determined by the location at which the latent function are evaluated we show result of the proposed methodology for synthetic data and real world application on pollution prediction and a sensor network 
with the proliferation of mobile device and wireless technology mobile social network system are increasingly available a mobile social network play an essential role a the spread of information and influence in the form of word of mouth it is a fundamental issue to find a subset of influential individual in a mobile social network such that targeting them initially e g to adopt a new product will maximize the spread of the influence further adoption of the new product the problem of finding the most influential node is unfortunately np hard it ha been shown that a greedy algorithm with provable approximation guarantee can give good approximation however it is computationally expensive if not prohibitive to run the greedy algorithm on a large mobile network in this paper we propose a new algorithm called community based greedy algorithm for mining top k influential node the proposed algorithm encompasses two component an algorithm for detecting community in a social network by taking into account information diffusion and a dynamic programming algorithm for selecting community to find influential node we also provide provable approximation guarantee for our algorithm empirical study on a large real world mobile social network show that our algorithm is more than an order of magnitude faster than the state of the art greedy algorithm for finding top k influential node and the error of our approximate algorithm is small 
in traditional text clustering method document are represented a bag of word without considering the semantic information of each document for instance if two document use different collection of core word to represent the same topic they may be falsely assigned to different cluster due to the lack of shared core word although the core word they use are probably synonym or semantically associated in other form the most common way to solve this problem is to enrich document representation with the background knowledge in an ontology there are two major issue for this approach the coverage of the ontology is limited even for wordnet or mesh using ontology term a replacement or additional feature may cause information loss or introduce noise in this paper we present a novel text clustering method to address these two issue by enriching document representation with wikipedia concept and category information we develop two approach exact match and relatedness match to map text document to wikipedia concept and further to wikipedia category then the text document are clustered based on a similarity metric which combine document content information concept information a well a category information the experimental result using the proposed clustering framework on three datasets newsgroup tdt and la time show that clustering performance improves significantly by enriching document representation with wikipedia concept and category 
we often seek to identify co occurring hidden feature in a set of observation the indian buffet process ibp provides a non parametric prior on the feature present in each observation but current inference technique for the ibp often scale poorly the collapsed gibbs sampler for the ibp ha a running time cubic in the number of observation and the uncollapsed gibbs sampler while linear is often slow to mix we present a new linear time collapsed gibbs sampler for conjugate likelihood model and demonstrate it efficacy on large real world datasets 
rapid growth in the amount of data available on social networking site ha made information retrieval increasingly challenging for user in this paper we propose a collaborative filtering method combinational collaborative filtering ccf to perform personalized community recommendation by considering multiple type of co occurrence in social data at the same time this filtering method fuse semantic and user information then applies a hybrid training strategy that combine gibbs sampling and expectation maximization algorithm to handle the large scale dataset parallel computing is used to speed up the model training through an empirical study on the orkut dataset we show ccf to be both effective and scalable 
we introduce a measure of how well a combinatorial graph fit a collection of vector the optimal graph under this measure may be computed by solving convex quadratic program and have many interesting property for vector in d dimensional space the graph always have average degree at most d and for vector in dimension they are always planar we compute these graph for many standard data set and show that they can be used to obtain good solution to classification regression and clustering problem 
we present novel semi supervised boosting algorithm that incrementally build linear combination of weak classifier through generic functional gradient descent using both labeled and unlabeled training data our approach is based on extending information regularization framework to boosting bearing loss function that combine log loss on labeled data with the information theoretic measure to encode unlabeled data even though the information theoretic regularization term make the optimization non convex we propose simple sequential gradient descent optimization algorithm and obtain impressively improved result on synthetic benchmark and real world task over supervised boosting algorithm which use the labeled data alone and a state of the art semi supervised boosting algorithm 
internet user regularly have the need to find biography and fact of people of interest wikipedia ha become the first stop for celebrity biography and fact however wikipedia can only provide information for celebrity because of it neutral point of view npov editorial policy in this paper we propose an integrated bootstrapping framework named biosnowball to automatically summarize the web to generate wikipedia style page for any person with a modest web presence in biosnowball biography ranking and fact extraction are performed together in a single integrated training and inference process using markov logic network mlns a it underlying statistical model the bootstrapping framework start with only a small number of seed and iteratively find new fact and biography a biography paragraph on the web are composed of the most important fact our joint summarization model can improve the accuracy of both fact extraction and biography ranking compared to decoupled method in the literature empirical result on both a small labeled data set and a real web scale data set show the effectiveness of biosnowball we also empirically show that biosnowball outperforms the decoupled method 
motivated by application in guaranteed delivery in computational advertising we consider the general problem of balanced allocation in a bipartite supply demand setting our formulation capture the notion of deviation from being balanced by a convex penalty function while this formulation admits a convex programming solution we strive for more robust and scalable algorithm for the case of l penalty function we obtain a simple combinatorial algorithm based on min cost flow in graph and show how to precompute a linear amount of information such that the allocation along any edge can be approximated in constant time we then extend our combinatorial solution to any convex function by solving a convex cost flow these scalable method may have application in other context stipulating balanced allocation we study the performance of our algorithm on large real world graph and show that they are efficient scalable and robust in practice 
text classification ha matured well a a research discipline over the year at the same time business intelligence over database ha long been a source of insight for enterprise with the growing importance of the service industry customer relationship management and contact center operation have become very important specifically the voice of the customer and customer satisfaction c sat have emerged a invaluable source of insight about how an enterprise s product and service are percieved by customer in this demonstration we present the ibm technology to automate customer satisfaction analysis itacs system that combine text classification technology and a business intelligence solution along with an interactive document labeling interface for automating c sat analysis this system ha been successfully deployed in client account in large contact center and can be extended to any service industry setting for analyzing unstructured text data this demonstration will highlight the importance of intervention and interactivity in real world text classification setting we will point out unique research challenge in this domain regarding label set measuring accuracy and interpretability of result and we will discus solution and open question 
statistical evolutionary model provide an important mechanism for describing and understanding the escape response of a viral population under a particular therapy we present a new hierarchical model that incorporates spatially varying mutation and recombination rate at the nucleotide level it also maintains separate parameter for treatment and control group which allows u to estimate treatment effect explicitly we use the model to investigate the sequence evolution of hiv population exposed to a recently developed antisense gene therapy a well a a more conventional drug therapy the detection of biologically relevant and plausible signal in both therapy study demonstrates the effectiveness of the method 
we propose a formal bayesian definition of surprise to capture subjective aspect of sensory information surprise measure how data affect an observer in term of difference between posterior and prior belief about the world only data observation which substantially affect the observer s belief yield surprise irrespectively of how rare or informative in shannon s sense these observation are we test the framework by quantifying the extent to which human may orient attention and gaze towards surprising event or item while watching television to this end we implement a simple computational model where a low level sensory form of surprise is computed by simple simulated early visual neuron bayesian surprise is a strong attractor of human attention with of all gaze shift directed towards location more surprising than the average a figure rising to when focusing the analysis onto region simultaneously selected by all observer the proposed theory of surprise is applicable across different spatio temporal scale modality and level of abstraction 
in this paper we show how to boost product of simple base learner similarly to tree we call the base learner a a subroutine but in an iterative rather than recursive fashion the main advantage of the proposed method is it simplicity and computational efficiency on benchmark datasets our boosted product of decision stump clearly outperform boosted tree and on the mnist dataset the algorithm achieves the second best result among no domain knowledge algorithm after deep belief net a a second contribution we present an improved base learner for nominal feature and show that boosting the product of two of these new subset indicator base learner solves the maximum margin matrix factorization problem used to formalize the collaborative filtering task on a small benchmark dataset we get experimental result comparable to the semi definite programming based solution but at a much lower computational cost 
neuroimaging datasets often have a very large number of voxels and a very small number of training case which mean that overfitting of model for this data can become a very serious problem working with a set of fmri image from a study on stroke recovery we consider a classification task for which logistic regression performs poorly even when l or l regularized we show that much better discrimination can be achieved by fitting a generative model to each separate condition and then seeing which model is most likely to have generated the data we compare discriminative training of exactly the same set of model and we also consider convex blend of generative and discriminative training 
we propose a novel bound on single variable marginal probability distribution in factor graph with discrete variable the bound is obtained by propagating local bound convex set of probability distribution over a subtree of the factor graph rooted in the variable of interest by construction the method not only bound the exact marginal probability distribution of a variable but also it approximate belief propagation marginal belief thus apart from providing a practical mean to calculate bound on marginals our contribution also lie in providing a better understanding of the error made by belief propagation we show that our bound outperforms the state of the art on some inference problem arising in medical diagnosis 
ranking is at the heart of many information retrieval application unlike standard regression or classification in which we predict output ind ependently in ranking we are interested in predicting structured output so that m isranking one object can significantly affect whether we correctly rank the other object in practice the problem of ranking involves a large number of object to be ranked and either approximate structured prediction method are required or assumption of independence between object score must be made in order to make the problem tractable we present a probabilistic method for learning t o rank using the graphical modelling framework of cumulative distribution network cdns where we can take into account the structure inherent to the problem of ranking by modelling the joint cumulative distribution function cdfs over multiple pairwise preference we apply our framework to the problem of document retrieval in the case of the ohsumed benchmark dataset we will show that the ranknet listnet and listmle probabilistic model can be viewed a particular instance of cdns and that our proposed framework allows for the exploration of a broad class of flexible structured loss functionals for learning t o rank 
information theoretic clustering aim to exploit information theoretic measure a the clustering criterion a common practice on this topic is so called info k mean which performs k mean clustering with the kl divergence a the proximity function while expert effort on info k mean have shown promising result a remaining challenge is to deal with high dimensional sparse data indeed it is possible that the centroid contain many zero value feature for high dimensional sparse data this lead to infinite kl divergence value which create a dilemma in assigning object to the centroid during the iteration process of k mean to meet this dilemma in this paper we propose a summation based incremental learning sail method for info k mean clustering specifically by using an equivalent objective function sail replaces the computation of the kl divergence by the computation of the shannon entropy this can avoid the zero value dilemma caused by the use of the kl divergence our experimental result on various real world document data set have shown that with sail a a booster the clustering performance of k mean can be significantly improved also sail lead to quick convergence and a robust clustering performance on high dimensional sparse data 
we introduce the first temporal difference learning algorithm that converge with smooth value function approximators such a neural network conventional temporal difference td method such a td lambda q learning and sarsa have been used successfully with function approximation in many application however it is well known that off policy sampling a well a non linear function approximation can cause these algorithm to become unstable i e the parameter of the approximator may diverge sutton et al a b solved the problem of off policy learning with linear td algorithm by introducing a new objective function related to the bellman error and algorithm that perform stochastic gradient descent on this function in this paper we generalize their work to non linear function approximation we present a bellman error objective function and two gradient descent td algorithm that optimize it we prove the asymptotic almost sure convergence of both algorithm for any finite markov decision process and any smooth value function approximator under usual stochastic approximation condition the computational complexity per iteration scale linearly with the number of parameter of the approximator the algorithm are incremental and are guaranteed to converge to locally optimal solution 
we study the convergence and the rate of convergence of a particular manifoldbased learning algorithm ltsa the main technical tool is the perturbation analysis on the linear invariant subspace that corresponds to the solution of ltsa we derive the upper bound for error under the worst case for ltsa it naturally lead to a convergence result we then derive the rate of convergence for ltsa in a special case 
the singular value decomposition is a key operation in many machine learning method it computational cost however make it unscalable and impractical for application involving large datasets or real time responsiveness which are becoming increasingly common we present a new method quic svd for fast approximation of the whole matrix svd based on a new sampling mechanism called the cosine tree our empirical test show speedup of several order of magnitude over exact svd such scalability should enable quic svd to accelerate and enable a wide array of svd based method and application notwithstanding the utility of the svd it is critically bottlenecked by a computational complexity that render it impractical on massive datasets yet massive datasets are increasingly common in application many of which require real time responsiveness such application could use svdbased method more liberally if the svd were not so slow to compute we present a new method quic svd for fast sample based svd approximation with automatic relative error control this algorithm is based on a new type of data partitioning tree the cosine tree that show excellent ability to home in on the subspace needed for good svd approximation we demonstrate several orderof magnitude speedup on medium sized datasets and verify that approximation error is properly controlled based on these result quic svd seems able to help address the scale of modern problem and datasets with the potential to benefit a wide array of method and application 
periodicity is a frequently happening phenomenon for moving object finding periodic behavior is essential to understanding object movement however periodic behavior could be complicated involving multiple interleaving period partial time span and spatiotemporal noise and outlier in this paper we address the problem of mining periodic behavior for moving object it involves two sub problem how to detect the period in complex movement and how to mine periodic movement behavior our main assumption is that the observed movement is generated from multiple interleaved periodic behavior associated with certain reference location based on this assumption we propose a two stage algorithm periodica to solve the problem at the first stage the notion of observation spot is proposed to capture the reference location through observation spot multiple period in the movement can be retrieved using a method that combine fourier transform and autocorrelation at the second stage a probabilistic model is proposed to characterize the periodic behavior for a specific period periodic behavior are statistically generalized from partial movement sequence through hierarchical clustering empirical study on both synthetic and real data set demonstrate the effectiveness of our method 
neuronal connection weight exhibit short term depression std the present study investigates the impact of std on the dynamic of a continuous attractor neural network cann and it potential role in neural information processing we find that the network with std can generate both static and traveling bump and std enhances the performance of the network in tracking external input in particular we find that std endows the network with slow decaying plateau behavior namely the network being initially stimulated to an active state will decay to silence very slowly in the time scale of std rather than that of neural signaling we argue that this provides a mechanism for neural system to hold short term memory easily and shut off persistent activity naturally 
modeling long term dependency in time series ha proved very difficult to achieve with traditional machine learning method this problem occurs when considering music data in this paper we introduce a model for rhythm based on the distribution of distance between subsequence a specific implementation of the model when considering hamming distance over a simple rhythm representation is described the proposed model consistently outperforms a standard hidden markov model in term of conditional prediction accuracy on two different music database 
nowadays for many task such a object recognition or language modeling data is plentiful a such an important challenge ha become to find learning algorithm which can make use of all the available data in this setting called large scale learning by bottou bousquet learning and optimization become different and powerful optimization algorithm are suboptimal learning algorithm while most effort are focused on adapting optimization algorithm for learning by efficiently using the information contained in the hessian le roux et al exploited the special structure of the learning problem to achieve faster convergence in this paper we investigate a natural way of combining these two direction to yield fast and robust learning algorithm 
in this paper we perform an empirical evaluation of supervised learning on high dimensional data we evaluate performance on three metric accuracy auc and squared loss and study the effect of increasing dimensionality on the performance of the learning algorithm our finding are consistent with previous study for problem of relatively low dimension but suggest that a dimensionality increase the relative performance of the learning algorithm change to our surprise the method that performs consistently well across all dimension is random forest followed by neural net boosted tree and svms 
we describe a single convolutional neural network architecture that given a sentence output a host of language processing prediction part of speech tag chunk named entity tag semantic role semantically similar word and the likelihood that the sentence make sense grammatically and semantically using a language model the entire network is trained jointly on all these task using weight sharing an instance of multitask learning all the task use labeled data except the language model which is learnt from unlabeled text and represents a novel form of semi supervised learning for the shared task we show how both multitask learning and semi supervised learning improve the generalization of the shared task resulting in state of the art performance 
social network have become a major focus of research in recent year initially directed towards static network but increasingly towards dynamic one in this paper we investigate how different pre processing decision and different network force such a selection and influence affect the modeling of dynamic network we also present empirical justification for some of the modeling assumption made in dynamic network analysis e g first order markovian assumption and develop metric to measure the alignment between link and attribute under different strategy of using the historical network data we also demonstrate the effect of attribute drift that is the importance of individual attribute in forming link change over time 
sampling function in gaussian process gp model is challenging because of the highly correlated posterior distribution we describe an efficient markov chain monte carlo algorithm for sampling from the posterior process of the gp model this algorithm us control variable which are auxiliary function value that provide a low dimensional representation of the function at each iteration the algorithm proposes new value for the control variable and generates the function from the conditional gp prior the control variable input location are found by minimizing an objective function we demonstrate the algorithm on regression and classification problem and we use it to estimate the parameter of a differential equation model of gene regulation gaussian process gps are used for bayesian non parametric estimation of unobserved or latent function in regression problem with gaussian likelihood inference in gp model is analytically tractable while for classification deterministic approximate inference algorithm are widely used however in recent application of gp model in system biology that require the estimation of ordinary differential equation model the development of deterministic approximation is difficult since the likelihood can be highly complex other application of gaussian process where inference is intractable arise in spatio temporal model and geostatistics and deterministic approximation have also been developed there in this paper we consider markov chain monte carlo mcmc algorithm for inference in gp model an advantage of mcmc over deterministic approximate inference is that it provides an arbitrarily precise approximation to the posterior distribution in the limit of long run another advantage is that the sampling scheme will often not depend on detail of the likelihood function and is therefore very generally applicable in order to benefit from the advantage of mcmc it is necessary to develop an efficient sampling strategy this ha proved to be particularly difficult in many gp application because the posterior distribution describes a highly correlated high dimensional variable thus simple mcmc sampling scheme such a gibbs sampling can be very inefficient in this contribution we describe an efficient mcmc algorithm for sampling from the posterior process of a gp model which construct the proposal distribution by utilizing the gp prior this algorithm us control variable which are auxiliary function value at each iteration the algorithm proposes new value for the control variable and sample the function by drawing from the conditional gp prior the control variable are highly informative point that provide a low dimensional representation of the function the control input location are found by minimizing an objective function the objective function used is the expected least square error of reconstructing the function value from the control variable where the expectation is over the gp prior 
in apprenticeship learning the goal is to learn a policy in a markov decision process that is at least a good a a policy demonstrated by an expert the difficulty arises in that the mdp s true reward function is assumed to be unknown we show how to frame apprenticeship learning a a linear programming problem and show that using an off the shelf lp solver to solve this problem result in a substantial improvement in running time over existing method up to two order of magnitude faster in our experiment additionally our approach produce stationary policy while all existing method for apprenticeship learning output policy that are mixed i e randomized combination of stationary policy the technique used is general enough to convert any mixed policy to a stationary policy 
we introduce a novel active learning algorithm for classication of network data in this setting training instance are connected by a set of link to form a network the label of linked node are correlated and the goal is to exploit these dependency and accurately label the node this problem arises in many domain including social and biological network analysis and document classication and there ha been much recent interest in method that collectively classify the node in the network while in many case labeled example are expensive often network information is available we show how an active learning algorithm can take advantage of network structure our algorithm eectively exploit the link between instance and the interaction between the local and collective aspect of a classier to improve the accuracy of learning from fewer labeled example we experiment with two real world benchmark collective classication domain and show that we are able to achieve extremely accurate result even when only a small fraction of the data is labeled 
in em and related algorithm e step computation distribute easily because data item are independent given parameter for very large data set however even storing all of the parameter in a single node for the m step can be impractical we present a framework that fully distributes the entire em procedure each node interacts only with parameter relevant to it data sending message to other node along a junction tree topology we demonstrate improvement over a mapreduce topology on two task word alignment and topic modeling 
the hierarchical dirichlet process hdp is a bayesian nonparametric mixed membership model each data point is modeled with a collection of component of different proportion though powerful the hdp make an assumption that the probability of a component being exhibited by a data point is positively correlated with it proportion within that data point this might be an undesirable assumption for example in topic modeling a topic component might be rare throughout the corpus but dominant within those document data point where it occurs we develop the ibp compound dirichlet process icd a bayesian nonparametric prior that decouples across data prevalence and within data proportion in a mixed membership model the icd combine property from the hdp and the indian buffet process ibp a bayesian nonparametric prior on binary matrix the icd assigns a subset of the shared mixture component to each data point this subset the data point s focus is determined independently from the amount that each of it component contribute we develop an icd mixture model for text the focused topic model ftm and show superior performance over the hdp based topic model 
for undiscounted reinforcement learning in markov decision process mdps we consider the total regret of a learning algorithm with respect to an optimal policy in order to describe the transition structure of an mdp we propose a new parameter an mdp ha diameter d if for any pair of state s s there is a policy which move from s to s in at most d step on average we present a reinforcement learning algorithm with total regret o d p at after t step for any unknown mdp with s state a action per state and diameter d this bound hold with high probability we also present a corresponding lower bound of p dsat on the total regret of any learning algorithm 
this paper considers the problem of publishing transaction data for research purpose each transaction is an arbitrary set of item chosen from a large universe detailed transaction data provides an electronic image of one s life this ha two implication one transaction data are excellent candidate for data mining research two use of transaction data would raise serious concern over individual privacy therefore before transaction data is released for data mining it must be made anonymous so that data subject cannot be re identified the challenge is that transaction data ha no structure and can be extremely high dimensional traditional anonymization method lose too much information on such data to date there ha been no satisfactory privacy notion and solution proposed for anonymizing transaction data this paper proposes one way to address this issue 
we develop the distance dependent chinese restaurant process crp a flexible class of distribution over partition that allows for non exchangeability this class can be used to model many kind of dependency between data in infinite clustering model including dependency across time or space we examine the property of the distance dependent crp discus it connection to bayesian nonparametric mixture model and derive a gibbs sampler for both observed and mixture setting we study it performance with three text corpus we show that relaxing the assumption of exchangeability with distance dependent crp can provide a better fit to sequential data we also show it alternative formulation of the traditional crp lead to a faster mixing gibbs sampling algorithm than the one based on the original formulation 
in this paper we propose a new incremental spike sorting model that automatically eliminates refractory period violation account for action potential waveform drift and can handle appearance and disappearance of neuron our approach is to augment a known time varying dirichlet process that tie together a sequence of infinite gaussian mixture model one per action potential waveform observation with an interspike interval dependent likelihood that prohibits refractory period violation we demonstrate this model by showing result from sorting two publicly available neural data recording for which a partial ground truth labeling is known 
we present a generalized version of spectral clustering using the graph p laplacian a nonlinear generalization of the standard graph laplacian we show that the second eigenvector of the graph p laplacian interpolates between a relaxation of the normalized and the cheeger cut moreover we prove that in the limit a p the cut found by thresholding the second eigenvector of the graph p laplacian converges to the optimal cheeger cut furthermore we provide an efficient numerical scheme to compute the second eigenvector of the graph p laplacian the experiment show that the clustering found by p spectral clustering is at least a good a normal spectral clustering but often lead to significantly better result 
in analogy to the pca setting the sparse pca problem is often solved by iteratively alternating between two subtasks cardinality co nstrained rank one variance maximization and matrix deflation while the former ha r eceived a great deal of attention in the literature the latter is seldom ana lyzed and is typically borrowed without justification from the pca context in this work we demonstrate that the standard pca deflation procedure is seldom ap propriate for the sparse pca setting to rectify the situation we first develo p several deflation alternative better suited to the cardinality constrained c ontext we then reformulate the sparse pca optimization problem to explicitly reflect th e maximum additional variance objective on each round the result is a generalized deflation procedure that typically outperforms more standard technique on real world datasets 
we consider the problem of data mining with formal privacy guarantee given a data access interface based on the differential privacy framework differential privacy requires that computation be insensitive to change in any particular individual s record thereby restricting data leak through the result the privacy preserving interface ensures unconditionally safe access to the data and doe not require from the data miner any expertise in privacy however a we show in the paper a naive utilization of the interface to construct privacy preserving data mining algorithm could lead to inferior data mining result we address this problem by considering the privacy and the algorithmic requirement simultaneously focusing on decision tree induction a a sample application the privacy mechanism ha a profound effect on the performance of the method chosen by the data miner we demonstrate that this choice could make the difference between an accurate classifier and a completely useless one moreover an improved algorithm can achieve the same level of accuracy and privacy a the naive implementation but with an order of magnitude fewer learning sample 
exponential family psr efpsr model capture stochastic dynamical system by representing state a the parameter of an exponential family distribution over a shortterm window of future observation they are appealing from a learning perspective because they are fully observed meaning expression for maximum likelihood do not involve hidden quantity but are still expressive enough to both capture existing model and predict new model while maximum likelihood learning algorithm for efpsrs exist they are not computationally feasible we present a new computationally efficient learning algorithm based on an approximate likelihood function the algorithm can be interpreted a attempting to induce stationary distribution of observation feature and state which match their empirically observed counterpart the approximate likelihood and the idea of matching stationary distribution may apply to other model 
we describe a system that monitor social and mainstream medium to determine shift in what people are thinking about a product or company we process over news article blog post review site and tweet a day for mention of item e g product of interest extract phrase that are mentioned near them and determine which of the phrase are of greatest possible interest to for example brand manager case study show a good ability to rapidly pinpoint emerging subject buried deep in large volume of data and then highlight those that are rising or falling in significance a they relate to the firm interest the tool and algorithm improves the signal to noise ratio and pinpoint precisely the opportunity and risk that matter most to communication professional and their organization 
we present a novel technique for automated problem decomposition to address the problem of scalability in reinforcement learning our technique make use of a set of near optimal trajectory to discover option and incorporates them into the learning process dramatically reducing the time it take to solve the underlying problem we run a series of experiment in two different domain and show that our method offer up to fold speedup over the baseline 
tensorial data are frequently encountered in various machine learning task today and dimensionality reduction is one of their most important application this paper extends the classical principal component analysis pca to it multilinear version by proposing a novel unsupervised dimensionality reduction algorithm for tensorial data named a uncorrelated multilinear pca umpca umpca seek a tensor to vector projection that capture most of the variation in the original tensorial input while producing uncorrelated feature through successive variance maximization we evaluate the umpca on a second order tensorial problem face recognition and the experimental result show it superiority especially in low dimensional space through the comparison with three other pca based algorithm 
this paper address the problem of active model selection for nonlinear dynamical system we propose a novel learning approach that selects the most informative subset of time dependent variable for the purpose of bayesian model inference the model selection criterion maximizes the expected kullback leibler divergence between the prior and the posterior probability over the model the proposed strategy generalizes the standard d optimal design which is obtained from a uniform prior with gaussian noise in addition our approach allows u to determine an information halting criterion for model identification we illustrate the benefit of our approach by differentiating between published biochemical model of the tor signaling pathway a model selection problem in system biology by generating pivotal selection experiment our strategy outperforms the standard aoptimal d optimal and e optimal sequential design technique 
successful software maintenance is becoming increasingly critical due to the increasing dependence of our society and economy on software system one key problem of software maintenance is the difficulty in understanding the evolving software system program workflow can help system operator and administrator to understand system behavior and verify system execution so a to greatly facilitate system maintenance in this paper we propose an algorithm to automatically discover program workflow from event trace that record system event during system execution different from existing workflow mining algorithm our approach can construct concurrent workflow from trace of interleaved event our workflow mining approach is a three step coarse to fine algorithm at first we mine temporal dependency for each pair of event then based on the mined pair wise tem poral dependency we construct a basic workflow model by a breadth first path pruning algorithm after that we refine the workflow by verifying it with all training event trace the re finement algorithm try to find out a workflow that can interpret all event trace with minimal state transition and thread the result of both simulation data and real program data show that our algorithm is highly effective 
singular value decomposition svd principal component analysis pca have played a vital role in finding pattern from many datasets recently tensor factorization ha been used for data mining and pattern recognition in high index order data high order svd hosvd is a commonly used tensor factorization method and ha recently been used in numerous application like graph video social network etc in this paper we prove that hosvd doe simultaneous subspace selection data compression and k mean clustering widely used for unsupervised learning task we show how to utilize this new feature of hosvd for clustering we demonstrate these new result using three real and large datasets two on face image datasets and one on hand written digit dataset using this new hosvd clustering feature we provide a dataset quality assessment on many frequently used experimental datasets with expected noise level 
supervised topic model utilize document s side information for discovering predictive low dimensional representation of document and existing model apply likelihood based estimation in this paper we present a max margin supervised topic model for both continuous and categorical response variable our approach the maximum entropy discrimination latent dirichlet allocation medlda utilizes the max margin principle to train supervised topic model and estimate predictive topic representation that are arguably more suitable for prediction we develop efficient variational method for posterior inference and demonstrate qualitatively and quantitatively the advantage of medlda over likelihood based topic model on movie review and newsgroups data set 
confidence weighted cw learning an online learning method for linear classifier maintains a gaussian distribution over weight vector with a covariance matrix that represents uncertainty about weight and correlation confidence constraint ensure that a weight vector drawn from the hypothesis distribution correctly classifies example with a specified probability within this framework we derive a new convex form of the constraint and analyze it in the mistake bound model empirical evaluation with both synthetic and text data show our version of cw learning achieves lower cumulative and out of sample error than commonly used first order and second order online method 
we address the problem of learning classifier using several kernel function on the contrary to many contribution in the field of learning from different source of information using kernel we here do not assume that the kernel used are positive definite the learning problem that we are interested in involves a misclassification loss term and a regularization term that is expressed by mean of a mixed norm the use of a mixed norm allows u to enforce some sparsity structure a particular case of which is for instance the group lasso we solve the convex problem by employing proximal minimization algorithm which can be viewed a refined version of gradient descent procedure capable of naturally dealing with nondifferentiability a numerical simulation on a uci dataset show the modularity of our approach 
low rank matrix approximation is an effective tool in alleviating the memory and computational burden of kernel method and sampling a the mainstream of such algorithm ha drawn considerable attention in both theory and practice this paper present detailed study on the nystrom sampling scheme and in particular an error analysis that directly relates the nystr om approximation quality with the encoding power of the landmark point in summarizing the data the resultant error bound suggests a simple and efficient sampling scheme the k mean clustering algorithm for nystr om low rank approximation we compare it with state of the art approach that range from greedy scheme to probabilistic sampling our algorithm achieves significant performance gain in a number of supervised unsupervised learning task including kernel pca and least square svm 
customer preference for product are drifting over time product perception and popularity are constantly changing a new selection emerges similarly customer inclination are evolving leading them to ever redefine their taste thus modeling temporal dynamic is essential for designing recommender system or general customer preference model however this raise unique challenge within the ecosystem intersecting multiple product and customer many different characteristic are shifting simultaneously while many of them influence each other and often those shift are delicate and associated with a few data instance this distinguishes the problem from concept drift exploration where mostly a single concept is tracked classical time window or instance decay approach cannot work a they lose too many signal when discarding data instance a more sensitive approach is required which can make better distinction between transient effect and long term pattern we show how to model the time changing behavior throughout the life span of the data such a model allows u to exploit the relevant component of all data instance while discarding only what is modeled a being irrelevant accordingly we revamp two leading collaborative filtering recommendation approach evaluation is made on a large movie rating dataset underlying the netflix prize contest result are encouraging and better than those previously reported on this dataset in particular method described in this paper play a significant role in the solution that won the netflix contest 
stability is an important yet under addressed issue in feature selection from high dimensional and small sample data in this paper we show that stability of feature selection ha a strong dependency on sample size we propose a novel framework for stable feature selection which flrst identifles consensus feature group from subsampling of training sample and then performs feature selection by treating each consensus feature group a a single entity experiment on both synthetic and real world data set show that an algorithm developed under this framework is efiective at alleviating the problem of small sample size and lead to more stable feature selection result and comparable or better generalization performance than state of the art feature selection algorithm synthetic data set and algorithm source code are available at http www c binghamton edu lyu kdd 
discovery of alternative clustering is an important method for exploring complex datasets it provides the capability for the user to view clustering behaviour from different perspective and thus explore new hypothesis however current algorithm for alternative clustering have focused mainly on linear scenario and may not perform a desired for datasets containing cluster with non linear shape our goal in this paper is to address this challenge of non linearity in particular we propose a novel algorithm to uncover an alternative clustering that is distinctively different from an existing reference clustering our technique is information theory based and aim to ensure alternative clustering quality by maximizing the mutual information between clustering label and data observation whilst at the same time ensuring alternative clustering distinctiveness by minimizing the information sharing between the two clustering we perform experiment to ass our method against a large range of alternative clustering algorithm in the literature we show our technique s performance is generally better for non linear scenario and furthermore is highly competitive even for simpler linear scenario 
in an idealized gated radiotherapy treatment radiation is delivered only when the tumor is at the right position for gated lung cancer radiotherapy it is difficult to generate accurate gating signal due to the large uncertainty when using external surrogate and the risk of pneumothorax when using implanted fiducial marker in this paper we investigate machine learning algorithm for markerless gated radiotherapy with fluoroscopic image previous approach utilizes template matching to localize the tumor position here we investigate two way to improve the precision of tumor target localization by applying an ensemble of template where the representative template are selected by gaussian mixture clustering and a support vector machine svm classifier with radial basis kernel template matching only considers image inside the gating window but image outside the gating window might provide additional information we take advantage of both state and re cast the gating problem into a classification problem thus we are able to use the svm classifier for gated radiotherapy to verify the effectiveness of the two proposed technique we apply them on five sequence of fluoroscopic image from five lung cancer patient against the gating signal of manually contoured tumor a ground truth our five patient case study show that both ensemble template matching and svm are reasonable tool for image guided markerless gated radiotherapy with an average of approximately precision in term of delivered target dose at approximately duty cycle 
learning undirected graphical model such a markov random field is an important machine learning task with application in many domain since it is usually intractable to learn these model exactly various approximate learning technique have been developed such a contrastive divergence cd and markov chain monte carlo maximum likelihood estimation mcmc mle in this paper we introduce particle filtered mcmc mle which is a sampling importanceresampling version of mcmc mle with additional mcmc rejuvenation step we also describe a unified view of mcmc mle our particle filtering approach and a stochastic approximation procedure known a persistent contrastive divergence we show how these approach are related to each other and discus the relative merit of each approach empirical result on various undirected model demonstrate that the particle filtering technique we propose in this paper can significantly outperform mcmc mle furthermore in certain case the proposed technique is faster than persistent cd 
we present a theory of compositionality in stochastic optimal control showing how task optimal controller can be constructed from certain primitive the primitive are themselves feedback controller pursuing their own agenda they are mixed in proportion to how much progress they are making towards their agenda and how compatible their agenda are with the present task the resulting composite control law is provably optimal when the problem belongs to a certain class this class is rather general and yet ha a number of unique property one of which is that the bellman equation can be made linear even for non linear or discrete dynamic this give rise to the compositionality developed here in the special case of linear dynamic and gaussian noise our framework yield analytical solution i e non linear mixture of lqg controller without requiring the final cost to be quadratic more generally a natural set of control primitive can be constructed by applying svd to green s function of the bellman equation we illustrate the theory in the context of human arm movement the idea of optimality and compositionality are both very prominent in the field of motor control yet they have been difficult to reconcile our work make this possible 
the relationship between constraint based mining and constraint programming is explored by showing how the typical constraint used in pattern mining can be formulated for use in constraint programming environment the resulting framework is surprisingly flexible and allows u to combine a wide range of mining constraint in different way we implement this approach in off the shelf constraint programming system and evaluate it empirically the result show that the approach is not only very expressive but also work well on complex benchmark problem 
statistical and computational concern have motivated parameter estimator based on various form of likelihood e g joint conditional and pseudolikelihood in this paper we present a unified framework for studying these estimator which allows u to compare their relative statistical efficiency our asymptotic analysis suggests that modeling more of the data tends to reduce variance but at the cost of being more sensitive to model misspecification we present experiment validating our analysis 
this paper address feature selection technique for classification of high dimensional data such a those produced by microarray experiment some prior knowledge may be available in this context to bias the selection towards some dimension gene a priori assumed to be more relevant we propose a feature selection method making use of this partial supervision it extends previous work on embedded feature selection with linear model including regularization to enforce sparsity a practical approximation of this technique reduces to standard svm learning with iterative rescaling of the input the scaling factor depend here on the prior knowledge but the final selection may depart from it practical result on several microarray data set show the benefit of the proposed approach in term of the stability of the selected gene list with improved classification performance 
pomdps are the model of choice for reinforcement learning rl task where the environment cannot be observed directly in many application we need to learn the pomdp structure and parameter from experience and this is considered to be a difcult problem in this paper we address this issue by modeling the hidden environment with a novel class of model that are le expressive but easier to learn and plan with than pomdps we call these model deterministic markov model dmms which are deterministic probabilistic nite automaton from learning theory extended with action to the sequential rather than i i d setting conceptually we extend the utile sux memory method of mccallum to handle long term memory we describe dmms give bayesian algorithm for learning and planning with them and also present experimental result for some standard pomdp task and task to illustrate it ecacy 
many social network can be characterized by a sequence of dyadic interaction between individual technique for analyzing such event are of increasing interest in this paper we describe a generative model for dyadic event where each event arises from one of c latent class and the property of the event sender recipient and type are chosen from distribution over these entity conditioned on the chosen class we present two algorithm for inference in this model an expectation maximization algorithm a well a a markov chain monte carlo procedure based on collapsed gibbs sampling to analyze the model s predictive accuracy the algorithm are applied to multiple real world data set involving email communication international political event and animal behavior data 
the essence of exploration is acting to try to decrease uncertainty we propose a new methodology for representing uncertainty in continuous state control problem our approach multi resolution exploration mre us a hierarchical mapping to identify region of the state space that would benefit from additional sample we demonstrate mre s broad utility by using it to speed up learning in a prototypical model based and value based reinforcement learning method empirical result show that mre improves upon state of the art exploration approach 
data analytics tool and framework abound yet rapid deployment of analytics solution that deliver actionable insight from business data remains a challenge the primary reason is that on field practitioner are required to be both technically proficient and knowledgeable about the business the recent abundance of unstructured business data ha thrown up new opportunity for analytics but ha also multiplied the deployment challenge since interpretation of concept derived from textual source require a deep understanding of the business in such a scenario a managed service for analytics come up a the best alternative a managed analytics service is centered around a business analyst who act a a liaison between the business and the technology this call for new tool that assist the analyst to be efficient in the task that she need to execute also the analytics need to be repeatable in that the delivered insight should not depend heavily on the expertise of specific analyst these factor lead u to identify new area that open up for kdd research in term of time to insight and repeatability for these analyst we present our analytics framework in the form of a managed service offering for crm analytics we describe different analyst centric tool using a case study from real life engagement and demonstrate their effectiveness 
this paper present a theoretical analysis of the problem of domain adaptation with multiple source for each source domain the distribution over the input point a well a a hypothesis with error at most are given the problem consists of combining these hypothesis to derive a hypothesis with small error with respect to the target domain we present several theoretical result relating to this problem in particular we prove that standard convex combination of the source hypothesis may in fact perform very poorly and that instead combination weighted by the source distribution benefit from favorable theoretical guarantee our main result show that remarkably for any fixed target f unction there exists a distribution weighted combining rule that ha a loss of at most with respect to any target mixture of the source distribution we further generalize the setting from a single target function to multiple consistent target function and show the existence of a combining rule with error at most finally we report empirical result for a multiple source adaptation problem with a real world dataset 
we describe and analyze two stochastic method for regularized loss minimization problem such a the lasso the first method update the weight of a single feature at each iteration while the second method update the entire weight vector but only us a single training example at each iteration in both method the choice of feature example is uniformly at random our theoretical runtime analysis suggests that the stochastic method should outperform state of the art deterministic approach including their deterministic counterpart when the size of the problem is large we demonstrate the advantage of stochastic method by experimenting with synthetic and natural data set 
in this paper we propose a method where the labeling of the data set is carried out in a semi supervised manner with user specified guarantee about the quality of the labeling in our scheme we assume that for each class we have some heuristic available each of which can identify instance of one particular class the heuristic are assumed to have reasonable performance but they do not need to cover all instance of the class nor do they need to be perfectly reliable we further assume that we have an infallible expert who is willing to manually label a few instance the aim of the algorithm is to exploit the cluster structure of the problem the prediction by the imperfect heuristic and the limited perfect label provided by the expert to classify label the instance of the data set with guaranteed precision specificed by the user with regard to each class the specified precision is not always attainable so the algorithm is allowed to classify some instance a dontknow the algorithm is evaluated by the number of instance labeled by the expert the number of dontknow instance global coverage and the achieved quality of the labeling on the kdd cup network intrusion data set containing instance we managed to label of the instance while guaranteeing a nominal precision of with confidence by having the expert label instance and by having the expert label instance we managed to guarantee nominal precision while labeling of the data we also provide a case study of applying our scheme to label the network traffic collected at a large campus network 
transfer learning can be described a the distillation of abstract knowledge from one learning domain or task and the reuse of that knowledge in a related domain or task in categorization setting transfer learning is the modication by past experience of prior expectation about what type of category are likely to exist in the world while transfer learning is an important and active research topic in machine learning there have been few study of transfer learning in human categorization we propose an explanation for transfer learning eects in human categorization implementing a model from the statistical machine learning literature the hierarchical dirichlet process hdp to make empirical evaluation of it ability to explain these eects we present two laboratory experiment which measure the degree to which people engage in transfer learning in a controlled setting and we compare our model to their performance we nd that the hdp provides a good explanation for transfer learning exhibited by human learner 
the most commonly used learning algorithm for restricted boltzmann machine is contrastive divergence which start a markov chain at a data point and run the chain for only a few iteration to get a cheap low variance estimate of the sufficient statistic under the model tieleman showed that better learning can be achieved by estimating the model s statistic using a small set of persistent fantasy particle that are not reinitialized to data point after each weight update with sufficiently small weight update the fantasy particle represent the equilibrium distribution accurately but to explain why the method work with much larger weight update it is necessary to consider the interaction between the weight update and the markov chain we show that the weight update force the markov chain to mix fast and using this insight we develop an even faster mixing chain that us an auxiliary set of fast weight to implement a temporary overlay on the energy landscape the fast weight learn rapidly but also decay rapidly and do not contribute to the normal energy landscape that defines the model 
continuously adaptive discretization for message passing cad mp is a new message passing algorithm for approximate inference most message passing algorithm approximate continuous probability distribution using either a family of continuous distribution such a the exponential family a particle set of discrete sample or a fixed uniform discretization in contra st cad mp us a discretization that is i non uniform and ii adaptive to th e structure of the marginal distribution non uniformity allows cad mp to localize interesting feature such a sharp peak in the marginal belief distribution with time complexity that scale logarithmically with precision a opposed to unifo rm discretization which scale at best linearly we give a principled method for altering the non uniform discretization according to information based measure cad mp is shown in experiment to estimate marginal belief much more precisely than competing approach for the same computational expense 
we consider a generalization of stochastic bandit problem where the set of arm x is allowed to be a generic topological space we constraint the mean payoff function with a dissimilarity function over x in a way that is more general than lipschitz we construct an arm selection policy whose regret improves upon previous result for a large class of problem in particular our result imply that if x is the unit hypercube in a euclidean space and the mean payoff function ha a finite number of global maximum around which the behavior of the function is locally h older with a known exponent then the expected regret is bounded up to a logarithmic factor by p n i e the rate of the growth of the regret is independent of the dimension of the space moreover we prove the minimax optimality of our algorithm for the class of mean payoff function we consider 
for a training dataset with a nonexhaustive list of class i e some class are not yet known and hence are not represented the resulting learning problem is ill defined in this case a sample from a missing class is incorrectly classified to one of the existing class for some application the cost of misclassifying a sample could be negligible however the significance of this problem can better be acknowledged when the potentially undesirable consequence of incorrectly classifying a food pathogen a a nonpathogen are considered our research is directed towards the real time detection of food pathogen using optical scattering technology bacterial colony consisting of the progeny of a single parent cell scatter light at nm to produce unique forward scatter signature these spectral signature contain descriptive characteristic of bacterial colony which can be used to identify bacteria culture in real time one bottleneck that remains to be addressed is the nonexhaustive nature of the training library it is very difficult if not impractical to collect sample from all possible bacteria colony and construct a digital library with an exhaustive set of scatter signature this study deal with the real time detection of sample from a missing class and the associated problem of learning with a nonexhaustive training dataset our proposed method assumes a common prior for the set of all class known and missing the parameter of the prior are estimated from the sample of the known class this prior is then used to generate a large number of sample to simulate the space of missing class finally a bayesian maximum likelihood classifier is implemented using sample from real a well a simulated class experiment performed with sample collected for bacteria subclass favor the proposed approach over the state of the art 
haussler s convolution kernel provides a successful framework for engineering new positive semidefinite kernel and ha been applied to a wide range of data type and application in the framework each data object represents a finite set of finer grained component then haussler s convolution kernel take a pair of data object a input and return the sum of the return value of the predetermined primitive positive semidefinite kernel calculated for all the possible pair of the component of the input data object on the other hand the mapping kernel that we introduce in this paper is a natural generalization of haussler s convolution kernel in that the input to the primitive kernel move over a predetermined subset rather than the entire cross product although we have plural instance of the mapping kernel in the literature their positive semidefiniteness wa investigated in case by case manner and worse yet wa sometimes incorrectly concluded in fact there exists a simple and easily checkable necessary and sufficient condition which is generic in the sense that it enables u to investigate the positive semidefiniteness of an arbitrary instance of the mapping kernel this is the first paper that present and prof the validity of the condition in addition we introduce two important instance of the mapping kernel which we refer to a the size of index structure distribution kernel and the editcost distribution kernel both of them are naturally derived from well known dis similarity measurement in the literature e g the maximum agreement tree the edit distance and are reasonably expected to improve the performance of the existing measure by evaluating their distributional feature rather than their peak maximum minimum feature 
a popular approach to collaborative filtering is matrix factorization in this paper we develop a non linear probabilistic matrix factorization using gaussian process latent variable model we use stochastic gradient descent sgd to optimize the model sgd allows u to apply gaussian process to data set with million of observation without approximate method we apply our approach to benchmark movie recommender data set the result show better than previous state of the art performance 
one class collaborative filtering occf is a task that naturally emerges in recommender system setting typical characteristic include only positive example can be observed class are highly imbalanced and the vast majority of data point are missing the idea of introducing weight for missing part of a matrix ha recently been shown to help in occf while existing weighting approach mitigate the first two problem above a sparsity preserving solution that would allow to efficiently utilize data set with e g hundred thousand of user and item ha not yet been reported in this paper we study three different collaborative filtering framework low rank matrix approximation probabilistic latent semantic analysis and maximum margin matrix factorization we propose two novel algorithm for large scale occf that allow to weight the unknown our experimental result demonstrate their effectiveness and efficiency on different problem including the netflix prize data 
we present an algorithm for exact bayes optimal classification from a hypothesis space of decision tree satisfying leaf constraint our contribution is that we reduce this classification problem to the problem of finding a rule based classifier with appropriate weight we show that these rule and weight can be computed in linear time from the output of a modified frequent itemset mining algorithm which mean that we can compute the classifier in practice despite the exponential worst case complexity in experiment we compare the bayes optimal prediction with those of the maximum a posteriori hypothesis 
efficient training of direct multi class formulation of linear support vector machine is very useful in application such a text classification with a huge number example a well a feature this paper present a fast dual method for this training the main idea is to sequentially traverse through the training set and optimize the dual variable associated with one example at a time the speed of training is enhanced further by shrinking and cooling heuristic experiment indicate that our method is much faster than state of the art solver such a bundle cutting plane and exponentiated gradient method 
most learning algorithm assume that a training dataset is given initially we address the common situation where data is not available initially but can be obtained at a cost we focus on learning bayesian belief network bns over discrete variable a such bns are model of probabilistic distribution we consider the generative challenge of learning the parameter for a fixed structure that best match the true distribution we focus on the budgeted learning setting where there is a known fixed cost ci for acquiring the value of the i th feature for any specified instance and a known total budget to spend acquiring all information after formally defining this problem from a bayesian perspective we first consider non sequential algorithm that must decide before seeing any result which feature of which instance to probe we show this is np hard even if all variable are independent then prove that the greedy allocation algorithm iga is optimal here when the cost are uniform but can otherwise be sub optimal we then show that general sequential policy perform better than non sequential and explore the challenge of learning the parameter for general belief network in this sequential setting describing condition for when the obvious round robin algorithm will versus will not work optimally we also explore the eectiveness of this and various other heuristic algorithm 
learning to rank ha become an important research topic in machine learning while most learning to rank method learn the ranking function by minimizing loss function it is the ranking measure such a ndcg and map that are used to evaluate the performance of the learned ranking function in this work we reveal the relationship between ranking measure and loss function in learning to rank method such a ranking svm rankboost ranknet and listmle we show that the loss function of these method are upper bound of the measure based ranking error a a result the minimization of these loss function will lead to the maximization of the ranking measure the key to obtaining this result is to model ranking a a sequence of classification task and define a so called essential loss for ranking a the weighted sum of the classification error of individual task in the sequence we have proved that the essential loss is both an upper bound of the measure based ranking error and a lower bound of the loss function in the aforementioned method our proof technique also suggests a way to modify existing loss function to make them tighter bound of the measure based ranking error experimental result on benchmark datasets show that the modification can lead to better ranking performance demonstrating the correctness of our theoretical analysis 
hierarchical probabilistic modeling of discrete data ha emerged a a powerful tool for text analysis posterior inference in such model is intractable and practitioner rely on approximate posterior inference method such a variational inference or gibbs sampling there ha been much research in designing better approximation but there is yet little theoretical understanding of which of the available technique are appropriate and in which data analysis setting in this paper we provide the beginning of such understanding we analyze the improvement that the recently proposed collapsed variational inference cvb provides over mean field variational inference vb in latent dirichlet allocation we prove that the difference in the tightness of the bound on the likelihood of a document decrease aso k p logm m wherek is the number of topic in the model andm is the number of word in a document a a consequence the advantage of cvb over vb is lost for long document but increase with the number of topic we demonstrate empirically that the theory hold using simulated text data and two text corpus we provide practical guideline for choosing an approximation 
we propose two transductive bound on the risk of majority vote that are estimated over partially labeled training set the first one involves the margin distribution of the classifier and a risk bound on it associate gibbs classifier the bound is tight when so is the gibbs s bound and when the error of the majority vote classifier is concentrated on a zone of low margin in semi supervised learning considering the margin a an indicator of confidence constitutes the working hypothesis of algorithm which search the decision boundary on low density region following this assumption we propose to bound the error probability of the voted classifier on the example for whose margin are above a fixed threshold a an application we propose a self learning algorithm which iteratively assigns pseudo label to the set of unlabeled training example that have their margin above a threshold obtained from this bound empirical result on different datasets show the effectiveness of our approach compared to the same algorithm and the tsvm in which the threshold is fixed manually 
we prove rate of convergence in the statistical sense for kernel based least square regression using a conjugate gradient algorithm where regularization against overfitting is obtained by early stopping this method is directly related to kernel partial least square a regression method that combine supervised dimensionality reduction with least square projection the rate depend on two key quantity first on the regularity of the target regression function and second on the intrinsic dimensionality of the data mapped into the kernel space lower bound on attainable rate depending on these two quantity were established in earlier literature and we obtain upper bound for the considered method that match these lower bound up to a log factor if the true regression function belongs to the reproducing kernel hilbert space if this assumption is not fulfilled we obtain similar convergence rate provided additional unlabeled data are available the order of the learning rate match state of the art result that were recently obtained for least square support vector machine and for linear regularization operator 
we consider the problem of estimating the policy gradient in partially observable markov decision process pomdps with a special class of policy that are based on predictive state representation psrs we compare psr policy to finite state controller fscs which are considered a a standard model for policy gradient method in pomdps we present a general actor critic algorithm for learning both fscs and psr policy the critic part computes a value function that ha a variable the parameter of the policy these latter parameter are gradually updated to maximize the value function we show that the value function is polynomial for both fscs and psr policy with a potentially smaller degree in the case of psr policy therefore the value function of a psr policy can have le local optimum than the equivalent fsc and consequently the gradient algorithm is more likely to converge to a global optimal solution 
in a statistical world faced with an explosion of data regularization ha become an important ingredient in a wide variety of problem we have many more input feature than observation and the lasso penalty and it hybrid have become increasingly useful for both feature selection and regularization this talk present some effective algorithm based on coordinate descent for fitting large scale regularization path for a variety of problem 
in classification with monotonicity constraint it is assumed that the class label should increase with increasing value on the attribute in this paper we aim at formalizing the approach to learning with monotonicity constraint from statistical point of view motivated by the statistical analysis we present an algorithm for learning rule ensemble the algorithm first monotonizes the data using a nonparametric classification procedure and then generates a rule ensemble consistent with the training set the procedure is justified by a theoretical analysis and verified in a computational experiment 
data mining research ha developed many algorithm for various analysis task on large and complex datasets however assessing the significance of data mining result ha received le attention analytical method are rarely available and hence one ha to use computationally intensive method randomization approach based on null model provide at least in principle a general approach that can be used to obtain empirical p value for various type of data mining approach i review some of the recent work in this area outlining some of the open question and problem 
minimum rank problem arise frequently in machine learning application and are notoriously difficult to solve due to the non convex nature of the rank objective in this paper we present the first online learning approach for the problem of rank minimization of matrix over polyhedral set in particular we present two online learning algorithm for rank minimization our first algorithm is a multiplicative update method based on a generalized expert framework while our second algorithm is a novel application of the online convex programming framework zinkevich in the latter we flip the role of the decision maker by making the decision maker search over the constraint space instead of feasible point a is usually the case in online convex programming a salient feature of our online learning approach is that it allows u to give provable approximation guarantee for the rank minimization problem over polyhedral set we demonstrate the effectiveness of our method on synthetic example and on the real life application of low rank kernel learning 
graph clustering method such a spectral clustering are defined for general weighted graph in machine learning however data often is not given in form of a graph but in term of similarity or distance value between point in this case first a neighborhood graph is constructed using the similarity between the point and then a graph clustering algorithm is applied to this graph in this paper we investigate the influence of the construction of the similarity graph on the clustering result we first study the convergence of graph clustering criterion such a the normalized cut ncut a the sample size tends to infinity we find that the limit expression are different for different type of graph for example the r neighborhood graph or the k nearest neighbor graph in plain word ncut on a knn graph doe something systematically different than ncut on an r neighborhood graph this finding show that graph clustering criterion cannot be studied independently of the kind of graph they are applied to we also provide example which show that these difference can be observed for toy and real data already for rather small sample size 
in partially observable world with many agent nested belief are formed when agent simultaneously reason about the unknown state of the world and the belief of the other agent the multi agent filtering problem is to efficiently represent and update these belief through time a the agent act in the world in this paper we formally define an infinite sequence of nested belief about the state of the world at the current time t and present a filtering algorithm that maintains a finite representation which can be used to generate these belief in some case this representation can be updated exactly in constant time we also present a simple approximation scheme to compact belief if they become too complex in experiment we demonstrate efficient filtering in a range of multi agent domain the multi agent filtering problem is to efficiently represent and update these nested belief through time in general an agent s belief depend on it entire history of action and observation one approach to computing these belief would be to remember the entire history and perform inference to compute whatever probability are needed at each time step but the time required for this computation would grow with the history length instead we maintain a belief state that is sufficient for predicting future belief and can be approximated to achieve constant time belief update we begin by defining an infinite sequence of nested belief about the current state st and showing that it is sufficient for predicting future belief we then present a multi agent filtering algorithm that maintains a compact representation sufficient for generating this sequence although in the worst case this representation grows exponentially in the history length we show that it size remains constant for several interesting problem we also describe an approximate algorithm that always 
dependency among neighbouring label in a sequence is an important source of information for sequence labeling problem however only dependency between adjacent label are commonly exploited in practice because of the high computational complexity of typical inference algorithm when longer distance dependency are taken into account in this paper we show that it is possible to design efficient inference algorithm for a conditional random field using feature that depend on long consecutive label sequence high order feature a long a the number of distinct label sequence used in the feature is small this lead to efficient learning algorithm for these conditional random field we show experimentally that exploiting dependency using high order feature can lead to substantial performance improvement for some problem and discus condition under which high order feature can be effective 
many learning algorithm rely on the curvature in particular strong convexity of regularized objective function to provide good theoretical performance guarantee in practice the choice of regularization penalty that give the best testing set performance may result in objective function with little or even no curvature in these case algorithm designed specifically for regularized objective often either fail completely or require some modification that involves a substantial compromise in performance we present new online and batch algorithm for training a variety of supervised learning model such a svms logistic regression structured prediction model and crfs under condition where the optimal choice of regularization parameter result in function with low curvature we employ a technique called proximal regularization in which we solve the original learning problem via a sequence of modified optimization task whose objective are chosen to have greater curvature than the original problem theoretically our algorithm achieve low regret bound in the online setting and fast convergence in the batch setting experimentally our algorithm improve upon state of the art technique including pegasos and bundle method on medium and large scale svm and structured learning task 
in this paper we address the problem of provably correct feature selection in arbitrary domain an optimal solution to the problem is a markov boundary which is a minimal set of feature that make the probability distri bution of a target variable conditionally invariant to the state of all other featu re in the domain while numerous algorithm for this problem have been proposed their theoretical correctness and practical behavior under arbitrary probabili ty distribution is unclear we address this by introducing the markov boundary theorem that precisely characterizes the property of an ideal markov boundary and use it to develop algorithm that learn a more general boundary that can capture complex interaction that only appear when the value of multiple feature are considered together we introduce two algorithm an exact provably correct one a well a more practical randomized anytime version and show that they perform well on artificial a well a benchmark and real world data set throughout the paper we make minimal assumption that consist of only a general set of axiom that hold for every probability distribution which give these algorithm un iversal applicability 
for any outsourcing service privacy is a major concern this paper focus on outsourcing frequent itemset mining and examines the issue on how to protect privacy against the case where the attacker have precise knowledge on the support of some item we propose a new approach referred to a k support anonymity to protect each sensitive item with k other item of similar support to achieve k support anonymity we introduce a pseudo taxonomy tree and have the third party mine the generalized frequent itemsets under the corresponding generalized association rule instead of association rule the pseudo taxonomy is a construct to facilitate hiding of the original item where each original item can map to either a leaf node or an internal node in the taxonomy tree the rationale for this approach is that with a taxonomy tree the k node to satisfy the k support anonymity may be any k node in the taxonomy tree with the appropriate support so this approach can provide more candidate for k support anonymity with limited fake item a only the leaf node not the internal node of the taxonomy tree need to appear in the transaction otherwise for the association rule mining the k node to satisfy the k support anonymity have to correspond to the leaf node in the taxonomy tree this is far more restricted the challenge is thus on how to generate the pseudo taxonomy tree to facilitate k support anonymity and to ensure the conservation of original frequent itemsets the experimental result showed that our method of k support anonymity can achieve very good privacy protection with moderate storage overhead 
in many data analysis task one is often confronted with very high dimensional data feature selection technique are designed to find the relevant feature subset of the original feature which can facilitate clustering classification and retrieval in this paper we consider the feature selection problem in unsupervised learning scenario which is particularly difficult due to the absence of class label that would guide the search for relevant information the feature selection problem is essentially a combinatorial optimization problem which is computationally expensive traditional unsupervised feature selection method address this issue by selecting the top ranked feature based on certain score computed independently for each feature these approach neglect the possible correlation between different feature and thus can not produce an optimal feature subset inspired from the recent development on manifold learning and l regularized model for subset selection we propose in this paper a new approach called multi cluster feature selection mcfs for unsupervised feature selection specifically we select those feature such that the multi cluster structure of the data can be best preserved the corresponding optimization problem can be efficiently solved since it only involves a sparse eigen problem and a l regularized least square problem extensive experimental result over various real life data set have demonstrated the superiority of the proposed algorithm 
discovering frequent pattern from data is a popular exploratory technique in datamining however if the data are sensitive e g patient health record user behavior record releasing information about significant pattern or trend carry significant risk to privacy this paper show how one can accurately discover and release the most significant pattern along with their frequency in a data set containing sensitive information while providing rigorous guarantee of privacy for the individual whose information is stored there we present two efficient algorithm for discovering the k most frequent pattern in a data set of sensitive record our algorithm satisfy differential privacy a recently introduced definition that provides meaningful privacy guarantee in the presence of arbitrary external information differentially private algorithm require a degree of uncertainty in their output to preserve privacy our algorithm handle this by returning noisy list of pattern that are close to the actual list of k most frequent pattern in the data we define a new notion of utility that quantifies the output accuracy of private top k pattern mining algorithm in typical data set our utility criterion implies low false positive and false negative rate in the reported list we prove that our method meet the new utility criterion we also demonstrate the performance of our algorithm through extensive experiment on the transaction data set from the fimi repository while the paper focus on frequent pattern mining the technique developed here are relevant whenever the data mining output is a list of element ordered according to an appropriately robust measure of interest 
dual supervision refers to the general setting of learning from both labeled example a well a labeled feature labeled feature are naturally available in task such a text classification where it is frequently possible to provide domain knowledge in the form of word that associate strongly with a class in this paper we consider the novel problem of active dual supervision or how to optimally query an example and feature labeling oracle to simultaneously collect two different form of supervision with the objective of building the best classifier in the most cost effective manner we apply classical uncertainty and experimental design based active learning scheme to graph kernel based dual supervision model empirical study confirm the potential of these scheme to significantly reduce the cost of acquiring labeled data for training high quality model 
pattern discovery in sequence is an important problem in many application especially in computational biology and text mining however due to the noisy nature of data the traditional sequential pattern model may fail to reflect the underlying characteristic of sequence data in these application there are two challenge first the mutation noise exists in the data and therefore symbol may be misrepresented by other symbol secondly the order of symbol in sequence could be permutated to address the above problem in this paper we propose a new sequential pattern model called mutable permutation pattern since the apriori property doe not hold for our permutation pattern model a novel permu pattern algorithm is devised to mine frequent mutable permutation pattern from sequence database a reachability property is identified to prune the candidate set last but not least we apply the permutation pattern model to a real genome dataset to discover gene cluster which show the effectiveness of the model a large amount of synthetic data is also utilized to demonstrate the efficiency of the permu pattern algorithm 
principal component analysis pca is the predominant linear dimensionality reduction technique and ha been widely applied on datasets in all scientific domain we consider both theoretically and empirically the topic of unsupervised feature selection for pca by leveraging algorithm for the so called column subset selection problem cssp in word the cssp seek the best subset of exactly k column from an m x n data matrix a and ha been extensively studied in the numerical linear algebra community we present a novel two stage algorithm for the cssp from a theoretical perspective for small to moderate value of k this algorithm significantly improves upon the best previously existing result for the cssp from an empirical perspective we evaluate this algorithm a an unsupervised feature selection strategy in three application domain of modern statistical data analysis finance document term data and genetics we pay particular attention to how this algorithm may be used to select representative or landmark feature from an object feature matrix in an unsupervised manner in all three application domain we are able to identify k landmark feature i e column of the data matrix that capture nearly the same amount of information a doe the subspace that is spanned by the top k eigenfeatures 
we propose a novel latent factor model to accurately predict response for large scale dyadic data in the presence of feature our approach is based on a model that predicts response a a multiplicative function of row and column latent factor that are estimated through separate regression on known row and column feature in fact our model provides a single unified framework to address both cold and warm start scenario that are commonplace in practical application like recommender system online advertising web search etc we provide scalable and accurate model fitting method based on iterated conditional mode and monte carlo em algorithm we show our model induces a stochastic process on the dyadic space with kernel covariance given by a polynomial function of feature method that generalize our procedure to estimate factor in an online fashion for dynamic application are also considered our method is illustrated on benchmark datasets and a novel content recommendation application that arises in the context of yahoo front page we report significant improvement over several commonly used method on all datasets 
this paper is concerned with the generalization ability of learning to rank algorithm for information retrieval ir we point out that the key for addressing the learning problem is to look at it from the viewpoint of query we define a number of new concept including query level loss query level risk and query level stability we then analyze the generalization ability of learning to rank algorithm by giving query level generalization bound to them using query level stability a a tool such an analysis is very helpful for u to derive more advanced algorithm for ir we apply the proposed theory to the existing algorithm of ranking svm and irsvm experimental result on the two algorithm verify the correctness of the theoretical analysis 
a clustering method are often sensitive to parameter tuning obtaining stability in clustering result is an important task in this work we aim at improving clustering stability by attempting to diminish the influence of algorithmic inconsistency and enhance the signal that come from the data we propose a mechanism that take m clustering a input and output m clustering of comparable quality which are in higher agreement with each other we call our method the clustering agreement process cap to preserve the clustering quality cap us the same optimization procedure a used in clustering in particular we study the stability problem of randomized clustering method which usually produce different result at each run we focus on method that are based on inference in a combinatorial markov random field or comraf for short of a simple topology we instantiate cap a inference within a more complex bipartite comraf we test the resulting system on four datasets three of which are medium sized text collection while the fourth is a large scale user movie dataset first in all the four case our system significantly improves the clustering stability measured in term of the macro averaged jaccard index second in all the four case our system managed to significantly improve clustering quality a well achieving the state of the art result third our system significantly improves stability of consensus clustering built on top of the randomized clustering solution 
this paper is devoted to thoroughly investigating how to bootstrap the roc curve a widely used visual tool for evaluating the accuracy of test scoring statistic in the bipartite setup the issue of confidence band for the roc curve is considered and a resampling procedure based on a smooth version of the empirical distribution called the smoothed bootstrap is introduced theoretical argument and simulation result are presented to show that the smoothed bootstrap is preferable to a naive bootstrap in order to construct accurate confidence band 
we propose a method that detects the true direction of time series by tting an autoregressive moving average model to the data whenever the noise is independent of the previous sample for one ordering of the observation but dependent for the opposite ordering we infer the former direction to be the true one we prove that our method work in the population case a long a the noise of the process is not normally distributed for the latter case the direction is not identiable a new and important implication of our result is that it conrms a fundamental conjecture in causal reasoning if after regression the noise is independent of signal for one direction and dependent for the other then the former represents the true causal direction in the case of time series we test our approach on two type of data simulated data set conforming to our modeling assumption and real world eeg time series our method make a decision for a signicant fraction of both data set and these decision are mostly correct for real world data our approach outperforms alternative solution to the problem of time direction recovery 
we introduce a new approach to analyzing click log by examining both the document that are clicked and those that are bypassed document returned higher in the ordering of the search result but skipped by the user this approach complement the popular click through rate analysis and help to draw negative inference in the click log we formulate a natural objective that find set of result that are unlikely to be collectively bypassed by a typical user this is closely related to the problem of reducing query abandonment we analyze a greedy approach to optimizing this objective and establish theoretical guarantee of it performance we evaluate our approach on a large set of query and demonstrate that it compare favorably to the maximal marginal relevance approach on a number of metric including mean average precision and mean reciprocal rank 
a goal of central importance in the study of hierarchical model for object recognition and indeed the mammalian visual cortex is that of understanding quantitatively the trade off between invariance and selectivity and how invariance and discrimination property contribute towards providing an improved representation useful for learning from data in this work we provide a general group theoretic framework for characterizing and understanding invariance in a family of hierarchical model we show that by taking an algebraic perspective one can provide a concise set of condition which must be met to establish invariance a well a a constructive prescription for meeting those condition analysis in specific case of particular relevance to computer vision and text processing are given yielding insight into how and when invariance can be achieved we find that the minimal intrinsic property of a hierarchical model needed to support a particular invariance can be clearly described thereby encouraging efficient computational implementation 
semi supervised support vector machine s vm attempt to learn a decision boundary that traverse through low data density region by maximizing the margin over labeled and unlabeled example traditionally s vm is formulated a a non convex integer programming problem and is thus difficult to solve in this paper we propose the cutting plane semi supervised support vector machine cut vm algorithm to solve the s vm problem specifically we construct a nested sequence of successively tighter relaxation of the original s vm problem and each optimization problem in this sequence could be efficiently solved using the constrained concave convex procedure cccp moreover we prove theoretically that the cut vm algorithm take time o sn to converge with guaranteed accuracy where n is the total number of sample in the dataset and s is the average number of non zero feature i e the sparsity experimental evaluation on several real world datasets show that cut vm performs better than existing s vm method both in efficiency and accuracy 
in this work we extend the ellipsoid method which wa originally designed for convex optimization for online learning the key idea is to approximate by an ellipsoid the classification hypothesis that are consistent with all the training example received so far this is in contrast to most online learning algorithm where only a single classifier is maintained at each iteration efficient algorithm are presented for updating both the centroid and the positive definite matrix of ellipsoid given a misclassified example in addition to the classical ellipsoid method an improved version for online learning is also presented mistake bound for both ellipsoid method are derived evaluation with the usps dataset and three uci data set show encouraging result when comparing the proposed online learning algorithm to two state of the art online learner 
accurately capturing user preference over time is a great practical challenge in recommender system simple correlation over time is typically not meaningful since user change their preference due to different external event user behavior can often be determined by individual s long term and short term preference how to represent user long term and short term preference how to leverage them for temporal recommendation to address these challenge we propose session based temporal graph stg which simultaneously model user long term and short term preference over time based on the stg model framework we propose a novel recommendation algorithm injected preference fusion ipf and extend the personalized random walk for temporal recommendation finally we evaluate the effectiveness of our method using two real datasets on citation and social bookmarking in which our proposed method ipf give improvement over the previous state of the art 
we propose an online topic model for sequentially analyzing the time evolution of topic in document collection topic naturally evolve with multiple timescales for example some word may be used consistently over one hundred year while other word emerge and disappear over period of a few day thus in the proposed model current topic specific distribution over word are assumed to be generated based on the multiscale word distribution of the previous epoch considering both the long timescale dependency a well a the short timescale dependency yield a more robust model we derive efficient online inference procedure based on a stochastic em algorithm in which the model is sequentially updated using newly obtained data this mean that past data are not required to make the inference we demonstrate the effectiveness of the proposed method in term of predictive performance and computational efficiency by examining collection of real document with timestamps 
this paper discus non parametric regression between riemannian manifold this learning problem arises frequently in many application area ranging from signal processing computer vision over robotics to computer graphic we present a new algorithmic scheme for the solution of this general learning problem based on regularized empirical risk minimization the regularization functional take into account the geometry of input and output manifold and we show that it implement a prior which is particularly natural moreover we demonstrate that our algorithm performs well in a difficult surface registration problem 
structure learning of dynamic bayesian network provide a principled mechanism for identifying conditional dependency in time series data this learning procedure assumes that the data are generated by a stationary process however there are interesting and important circumstance where that assumption will not hold and potential non stationarity cannot be ignored here we introduce a new class of graphical model called non stationary dynamic bayesian network in which the conditional dependence structure of the underlying data generation process is permitted to change or evolve over time some example of evolving network are transcriptional regulatory network during development neural pathway during learning and traffic pattern during the day we define the non stationary dbn model present an mcmc sampling algorithm for efficiently learning the structure of an nsdbn and the time of non stationarities transition time under different assumption and demonstrate the effectiveness of the algorithm on simulated data previous work in recent work from the social network community a generalization of the p or exponential random graph model ergm to account for temporal dynamic ha been used to model the temporal progression of network however this algorithm requires the network structure a input a requirement which limit usability in other field this approach wa recently extended by assuming that the network are latent unobserved variable which generate the observed time series data while this technique allows for the underlying network structure to be identified there are some drawback the correlation structure between variable is assumed to remain constant over time only undirected edge are predicted and the transition time must be identified a priori in the continuous domain there ha been some research focused on learning the structure of a time varying gaussian graphical model these author use a reversible jump mcmc to estimate the time varying variance structure of the data however some limitation of this method include network evolution is restricted to changing at most a single edge at a time and the total number of segment is assumed known a priori a similar algorithm using gaussian graphical model ha been developed to segment multivariate time series data this work us an iterative approach that switch between a convex optimization for determining the graph structure and a dynamic programming algorithm for calculating the segmentation this approach ha some notable advantage speed no single edge change restriction and number of segment calculated a posteriori however it requires that the graph structure is decomposable additionally both of these approach only identify undirected edge and assume that the network in each segment are independent by extending bayesian network we are able to make directed prediction and we allow the network in one segment to depend on those in adjacent segment 
we consider approximate policy evaluation for finite state and action markov decision process mdp in the o policy learning context and with the simulation based least square temporal dierence algorithm lstd we establish for the discounted cost criterion that the o policy lstd converges almost surely under mild minimal condition we also analyze other convergence and boundedness property of the iterates involved in the algorithm and based on them we suggest a modification in it practical implementation our analysis us theory of both finite space markov chain and markov chain on topological space 
ticket resolution is a critical yet challenging aspect of the delivery of it service a large service provider need to handle on a daily basis thousand of ticket that report various type of problem many of those ticket bounce among multiple expert group before being transferred to the group with the right expertise to solve the problem finding a methodology that reduces such bouncing and hence shortens ticket resolution time is a long standing challenge in this paper we present a unified generative model the optimized network model onm that characterizes the lifecycle of a ticket using both the content and the routing sequence of the ticket onm us maximum likelihood estimation to represent how the information contained in a ticket is used by human expert to make ticket routing decision based on onm we develop a probabilistic algorithm to generate ticket routing recommendation for new ticket in a network of expert group our algorithm calculates all possible route to potential resolvers and make globally optimal recommendation in contrast to existing classification method that make static and locally optimal recommendation experiment show that our method significantly outperforms existing solution 
at kdd in paris a panel on open standard and cloud computing addressed emerging trend for data mining application in science and industry this report summarizes the answer from a distinguished group of thought leader representing key software vendor in the data mining industry supporting open standard and the predictive model markup language pmml in particular the panel member discus topic regarding the adoption of prevailing standard benefit of interoperability for business user and the practical application of predictive model we conclude with an assessment of emerging technology trend and the impact that cloud computing will have on application a well a licensing model for the predictive analytics industry 
this paper introduces the banditron a variant of the perceptron rosenblatt for the multiclass bandit setting the multiclass bandit setting model a wide range of practical supervised learning application where the learner only receives partial feedback referred to a bandit feedback in the spirit of multi armed bandit model with respect to the true label e g in many web application user often only provide positive click feedback which doe not necessarily fully disclose a true label the banditron ha the ability to learn in a multiclass classification setting with the bandit feedback which only reveals whether or not the prediction made by the algorithm wa correct or not but doe not necessarily reveal the true label we provide relative mistake bound which show how the banditron enjoys favorable performance and our experiment demonstrate the practicality of the algorithm furthermore this paper pay close attention to the important special case when the data is linearly separable a problem which ha been exhaustively studied in the full information setting yet is novel in the bandit setting 
a large fraction of the url on the web contain duplicate or near duplicate content de duping url is an extremely important problem for search engine since all the principal function of a search engine including crawling indexing ranking and presentation are adversely impacted by the presence of duplicate url traditionally the de duping problem ha been addressed by fetching and examining the content of the url our approach here is different given a set of url partitioned into equivalence class based on the content url in the same equivalence class have similar content we address the problem of mining this set and learning url rewrite rule that transform all url of an equivalence class to the same canonical form these rewrite rule can then be applied to eliminate duplicate among url that are encountered for the first time during crawling even without fetching their content in order to express such transformation rule we propose a simple framework that is general enough to capture the most common url rewrite pattern occurring on the web in particular it encapsulates the dust different url with similar text framework we provide an efficient algorithm for mining and learning url rewrite rule and show that under mild assumption it is complete i e our algorithm learns every url rewrite rule that is correct for an appropriate notion of correctness we demonstrate the expressiveness of our framework and the effectiveness of our algorithm by performing a variety of extensive large scale experiment 
classification is a core task in knowledge discovery and data mining and there ha been substantial research effort in developing sophisticated classification model in a parallel thread recent work from the nlp community suggests that for task such a natural language disambiguation even a simple algorithm can outperform a sophisticated one if it is provided with large quantity of high quality training data in those application training data occurs naturally in text corpus and high quality training data set running into billion of word have been reportedly used we explore how we can apply the lesson from the nlp community to kdd task specifically we investigate how to identify data source that can yield training data at low cost and study whether the quantity of the automatically extracted training data can compensate for it lower quality we carry out this investigation for the specific task of inferring whether a search query ha commercial intent we mine toolbar and click log to extract query from site that are predominantly commercial e g amazon and non commercial e g wikipedia we compare the accuracy obtained using such training data against manually labeled training data our result show that we can have large accuracy gain using automatically extracted training data at much lower cost 
we tackle the computational problem of query conditioned search given a machine learned scoring rule and a query distribution we build a predictive index by precomputing list of potential result sorted based on an expected score of the result over future query the predictive index datastructure support an anytime algorithm for approximate retrieval of the top element the general approach is applicable to webpage ranking internet advertisement and approximate nearest neighbor search it is particularly effective in setting where standard technique e g inverted index are intractable we experimentally find substantial improvement over existing method for internet advertisement and approximate nearest neighbor 
the kernel stick breaking process ksbp is employed to segment general imagery imposing the condition that patch small block of pixel that are spatially proximate are more likely to be associated with the same cluster segment the number of cluster is not set a priori and is inferred from the hierarchical bayesian model further ksbp is integrated with a shared dirichlet process prior to simultaneously model multiple image inferring their inter relationship this latter application may be useful for sorting and learning relationship between multiple image the bayesian inference algorithm is based on a hybrid of variational bayesian analysis and local sampling in addition to providing detail on the model and associated inference framework example result are presented for several image analysis problem 
anomalous window are the contiguous grouping of data point in this paper we propose an approach for discovering anomalous window using scan statistic for linear intersecting path sslip a linear path refers to a path represented by a line with a single dimensional spatial coordinate marking an observation point our approach for discovering anomalous window along linear path comprises of the following distinct step a cross path discovery where we identify a subset of intersecting path to be considered b anomalous window discovery where we outline three order invariant algorithm namely sslip brute force sslip and central brute force sslip for the traversal of the cross path to identify varying size directional window along the path for identifying an anomalous window we compute an unusualness metric in the form of a likelihood ratio to indicate the degree of unusualness of this window with respect to the rest of the data we identify the window with the highest likelihood ratio a our anomalous window and c monte carlo simulation to ascertain whether this window is truly anomalous and not just a random occurrence we perform hypothesis testing by computing a p value using monte carlo simulation we present extensive experimental result in real world accident datasets for various highway with known issue code and data available from our result show that our approach indeed is effective in identifying anomalous traffic accident window along multiple intersecting highway 
we present new algorithm for inverse optimal control or inverse reinforcement learning irl within the framework of linearlysolvable mdps lmdps unlike most prior irl algorithm which recover only the control policy of the expert we recover the policy the value function and the cost function this is possible because here the cost and value function are uniquely dened given the policy despite these special property we can handle a wide variety of problem such a the grid world popular in rl and most of the nonlinear problem arising in robotics and control engineering direct comparison to prior irl algorithm show that our new algorithm provide more information and are order of magnitude faster indeed our fastest algorithm is the rst inverse algorithm which 
given a large scale linked document collection such a a collection of blog post or a research literature archive there are two fundamental problem that have generated a lot of interest in the research community one is to identify a set of high level topic covered by the document in the collection the other is to uncover and analyze the social network of the author of the document so far these problem have been viewed a separate problem and considered independently from each other in this paper we argue that these two problem are in fact inter dependent and should be addressed together we develop a bayesian hierarchical approach that performs topic modeling and author community discovery in one unified framework the effectiveness of our model is demonstrated on two blog data set in different domain and one research paper citation data from citeseer 
it is a big challenge to guarantee the quality of discovered relevance feature in text document for describing user preference because of the large number of term pattern and noise most existing popular text mining and classification method have adopted term based approach however they have all suffered from the problem of polysemy and synonymy over the year people have often held the hypothesis that pattern based method should perform better than term based one in describing user preference but many experiment do not support this hypothesis the innovative technique presented in paper make a breakthrough for this difficulty this technique discovers both positive and negative pattern in text document a higher level feature in order to accurately weight low level feature term based on their specificity and their distribution in the higher level feature substantial experiment using this technique on reuters corpus volume and trec topic show that the proposed approach significantly outperforms both the state of the art term based method underpinned by okapi bm rocchio or support vector machine and pattern based method on precision recall and f measure 
in this work we address the problem of joint modeling of text and citation in the topic modeling framework we present two different model called the pairwise link lda and the link plsa lda model the pairwise link lda model combine the idea of lda and mixed membership block stochastic model and allows modeling arbitrary link structure however the model is computationally expensive since it involves modeling the presence or absence of a citation link between every pair of document the second model solves this problem by assuming that the link structure is a bipartite graph a the name indicates link plsa lda model combine the lda and plsa model into a single graphical model our experiment on a subset of citeseer data show that both these model are able to predict unseen data better than the baseline model of erosheva and lafferty by capturing the notion of topical similarity between the content of the cited and citing document our experiment on two different data set on the link prediction task show that the link plsa lda model performs the best on the citation prediction task while also remaining highly scalable in addition we also present some interesting visualization generated by each of the model 
bayesian approach to utility elicitation typically adopt myopic expected value of information evoi a a natural criterion for selecting query however evoi optimization is usually computationally prohibitive in this paper we examine evoi optimization using choice query query in which a user is ask to select her most preferred product from a set we show that under very general assumption the optimal choice query w r t evoi coincides with the optimal recommendation set that is a set maximizing the expected utility of the user selection since recommendation set optimization is a simpler submodular problem this can greatly reduce the complexity of both exact and approximate greedy computation of optimal choice query we also examine the case where user response to choice query are error prone using both constant and mixed multinomial logit noise model and provide worst case guarantee finally we present a local search technique for query optimization that work extremely well with large outcome space 
we are often interested in casting classification and clustering problem a a regression framework because it is feasible to achieve some statistical property in this framework by imposing some penalty criterion in this paper we illustrate optimal scoring which wa originally proposed for performing the fisher linear discriminant analysis by regression in the application of unsupervised learning in particular we devise a novel clustering algorithm that we call optimal discriminant clustering we associate our algorithm with the existing unsupervised learning algorithm such a spectral clustering discriminative clustering and sparse principal component analysis experimental result on a collection of benchmark datasets validate the effectiveness of the optimal discriminant clustering algorithm 
we describe efficient algorithm for projecting a vector onto the l ball we present two method for projection the first performs exact projection in o n expected time where n is the dimension of the space the second work on vector k of whose element are perturbed outside the l ball projecting in o k log n time this setting is especially useful for online learning in sparse feature space such a text categorization application we demonstrate the merit and effectiveness of our algorithm in numerous batch and online learning task we show that variant of stochastic gradient projection method augmented with our efficient projection procedure outperform interior point method which are considered state of the art optimization technique we also show that in online setting gradient update with l projection outperform the exponentiated gradient algorithm while obtaining model with high degree of sparsity 
this talk describes the optimal revenue maximizing auction for sponsored search advertising we show that a search engine s optimal reserve price is independent of the number of bidder using simulation we consider the change that result from a search engine s choice of reserve price and from change in the number of participating advertiser 
a nonparametric model is introduced that allows multiple related regression task to take input from a common data space traditional transfer learning model can be inappropriate if the dependence among the output cannot be fully resolved by known input specific and task specific predictor the proposed model treat such output response a conditionally independent given known predictor and appropriate unobserved random effect the model is nonparametric in the sense that the dimensionality of random effect is not specified a priori but is instead determined from data an approach to estimating the model is presented us an em algorithm that is efficient on a very large scale collaborative prediction problem the obtained prediction accuracy is competitive with state of the art result 
we present a detailed study of network evolution by analyzing four large online social network with full temporal information about node and edge arrival for the first time at such a large scale we study individual node arrival and edge creation process that collectively lead to macroscopic property of network using a methodology based on the maximum likelihood principle we investigate a wide variety of network formation strategy and show that edge locality play a critical role in evolution of network our finding supplement earlier network model based on the inherently non local preferential attachment based on our observation we develop a complete model of network evolution where node arrive at a prespecified rate and select their lifetime each node then independently initiate edge according to a gap process selecting a destination for each edge according to a simple triangle closing model free of any parameter we show analytically that the combination of the gap distribution with the node lifetime lead to a power law out degree distribution that accurately reflects the true network in all four case finally we give model parameter setting that allow automatic evolution and generation of realistic synthetic network of arbitrary scale 
this paper introduces mass estimation a base modelling mechanism in data mining it provides the theoretical basis of mass and an efficient method to estimate mass we show that it solves problem very effectively in task such a information retrieval regression and anomaly detection the model which use mass in these three task perform at least a good a and often better than a total of eight state of the art method in term of task specific performance measure in addition mass estimation ha constant time and space complexity 
activity of a neuron even in the early sensory area is not simply a function of it local receptive field or tuning property but depends on global context of the stimulus a well a the neural context this suggests the activity of the surrounding neuron and global brain state can exert considerable influence on the activity of a neuron in this paper we implemented an l regularized point process model to ass the contribution of multiple factor to the firing rate of many individual unit recorded simultaneously from v with a electrode utah array we found that the spike of surrounding neuron indeed provide strong prediction of a neuron s response in addition to the neuron s receptive field transfer function we also found that the same spike could be accounted for with the local field potential a surrogate measure of global network state this work show that accounting for network fluctuation can improve estimate of single trial firing rate and stimulus response transfer function 
in many online social system social tie between user play an important role in dictating their behavior one of the way this can happen is through social influence the phenomenon that the action of a user can induce his her friend to behave in a similar way in system where social influence exists idea mode of behavior or new technology can diffuse through the network like an epidemic therefore identifying and understanding social influence is of tremendous interest from both analysis and design point of view this is a difficult task in general since there are factor such a homophily or unobserved confounding variable that can induce statistical correlation between the action of friend in a social network distinguishing influence from these is essentially the problem of distinguishing correlation from causality a notoriously hard statistical problem in this paper we study this problem systematically we define fairly general model that replicate the aforementioned source of social correlation we then propose two simple test that can identify influence a a source of social correlation when the time series of user action is available we give a theoretical justification of one of the test by proving that with high probability it succeeds in ruling out influence in a rather general model of social correlation we also simulate our test on a number of example designed by randomly generating action of node on a real social network from flickr according to one of several model simulation result confirm that our test performs well on these data finally we apply them to real tagging data on flickr exhibiting that while there is significant social correlation in tagging behavior on this system this correlation cannot be attributed to social influence 
in this paper we introduce a novel approach to manifold alignment based on procrustes analysis our approach differs from semi supervised alignment in that it result in a mapping that is defined everywhere when used with a suitable dimensionality reduction method rather than just on the training data point we describe and evaluate our approach both theoretically and experimentally providing result showing useful knowledge transfer from one domain to another novel application of our method including cross lingual information retrieval and transfer learning in markov decision process are presented 
we develop a penalized kernel smoothing method for the problem of selecting nonzero element of the conditional precision matrix known a conditional covariance selection this problem ha a key role in many modern application such a finance and computational biology however it ha not been properly addressed our estimator is derived under minimal assumption on the underlying probability distribution and work well in the high dimensional setting the efficiency of the algorithm is demonstrated on both simulation study and the analysis of the stock market 
identification and comparison of nonlinear dynamical system model using noisy and sparse experimental data is a vital task in many field however current method are computationally expensive and prone to error due in part to the nonlinear nature of the likelihood surface induced we present an accelerated sampling procedure which enables bayesian inference of parameter in nonlinear ordinary and delay differential equation via the novel use of gaussian process gp our method involves gp regression over time series data and the resulting derivative and time delay estimate make parameter inference possible without solving the dynamical system explicitly resulting in dramatic saving of computational time we demonstrate the speed and statistical accuracy of our approach using example of both ordinary and delay differential equation and provide a comprehensive comparison with current state of the art method 
previous algorithm for learning lexicographic preference model lpms produce a best guess lpm that is consistent with the observation our approach is more democratic we do not commit to a single lpm instead we approximate the target using the vote of a collection of consistent lpms we present two variation of this method variable voting and model voting and empirically show that these democratic algorithm outperform the existing method we also introduce an intuitive yet powerful learning bias to prune some of the possible lpms we demonstrate how this learning bias can be used with variable and model voting and show that the learning bias improves the learning curve significantly especially when the number of observation is small 
we introduce a family of unsupervised algorithm numerical taxonomy clustering to simultaneously cluster data and to learn a taxonomy that encodes the relationship between the cluster the algorithm work by maximizing the dependence between the taxonomy and the original data the resulting taxonomy is a more informative visualization of complex data than simple clustering in addition taking into account the relation between different cluster is shown to substantially improve the quality of the clustering when compared with state ofthe art algorithm in the literature both spectral clustering and a previous dependence maximization approach we demonstrate our algorithm on image and text data 
for century scholar have explored the deep link among human language in this paper we present a class of probabilistic model that use these link a a form of naturally occurring supervision these model allow u to substantially improve performance for core text processing task such a morphological segmentation part of speech tagging and syntactic parsing besides these traditional nlp task we also present a multilingual model for the computational decipherment of lost language 
the dynamic marketplace in online advertising call for ranking system that are optimized to consistently promote and capitalize better performing ad the streaming nature of online data inevitably make an advertising system choose between maximizing it expected revenue according to it current knowledge in short term exploitation and trying to learn more about the unknown to improve it knowledge exploration since the latter might increase it revenue in the future the exploitation and exploration ee tradeoff ha been extensively studied in the reinforcement learning community however not been paid much attention in online advertising until recently in this paper we develop two novel ee strategy for online advertising specifically our method can adaptively balance the two aspect of ee by automatically learning the optimal tradeoff and incorporating confidence metric of historical performance within a deliberately designed offline simulation framework we apply our algorithm to an industry leading performance based contextual advertising system and conduct extensive evaluation with real online event log data the experimental result and detailed analysis reveal several important finding of ee behavior in online advertising and demonstrate that our algorithm perform superiorly in term of ad reach and click through rate ctr 
abstract we explore a recently proposed mixture model approach to understanding interaction between conflicting sensory cue alternative model formulation differing in their sensory noise model and inference method are compared based on their fit to experimental data heavy tailed sensory likelihood yield a better description of the subject response behavior than standard gaussian noise model we study the underlying cause for this result and then present several testable prediction of these model 
kernel method have been applied successfully in many data mining task subspace kernel learning wa recently proposed to discover an effective low dimensional subspace of a kernel feature space for improved classification in this paper we propose to construct a subspace kernel using the hilbert schmidt independence criterion hsic we show that the optimal subspace kernel can be obtained efficiently by solving an eigenvalue problem one limitation of the existing subspace kernel learning formulation is that the kernel learning and classification are independent and the subspace kernel may not be optimally adapted for classification to overcome this limitation we propose a joint optimization framework in which we learn the subspace kernel and subsequent classifier simultaneously in addition we propose a novel learning formulation that extract an uncorrelated subspace kernel to reduce the redundant information in a subspace kernel following the idea from multiple kernel learning we extend the proposed formulation to the case when multiple kernel are available and need to be combined we show that the integration of subspace kernel can be formulated a a semidefinite program sdp which is computationally expensive to improve the efficiency of the sdp formulation we propose an equivalent semi infinite linear program silp formulation which can be solved efficiently by the column generation technique experimental result on a collection of benchmark data set demonstrate the effectiveness of the proposed algorithm 
logistic regression is a well known classification method that ha been used widely in many application of data mining machine learning computer vision and bioinformatics sparse logistic regression embeds feature selection in the classification framework using the l norm regularization and is attractive in many application involving high dimensional data in this paper we propose lassplore for solving large scale sparse logistic regression specifically we formulate the problem a the l ball constrained smooth convex optimization and propose to solve the problem using the nesterov s method an optimal first order black box method for smooth convex optimization one of the critical issue in the use of the nesterov s method is the estimation of the step size at each of the optimization iteration previous approach either applies the constant step size which assumes that the lipschitz gradient is known in advance or requires a sequence of decreasing step size which lead to slow convergence in practice in this paper we propose an adaptive line search scheme which allows to tune the step size adaptively and meanwhile guarantee the optimal convergence rate empirical comparison with several state of the art algorithm demonstrate the efficiency of the proposed lassplore algorithm for large scale problem 
similarity measure in many real application generate indefinite similarity matrix in this paper we consider the problem of classification based on such indefinite similarity these indefinite kernel can be problematic for standard kernel based algorithm a the optimization problem become non convex and the underlying theory is invalidated in order to adapt kernel method for similarity based learning we introduce a method that aim to simultaneously find a reproducing kernel hilbert space based on the given similarity and train a classifier with good generalization in that space the method is formulated a a convex optimization problem we propose a simplified version that can reduce overfitting and whose associated convex conic program can be solved efficiently we compare the proposed simplified version with six other method on a collection of real data set 
we present a discriminative online algorithm with a bounded memory growth which is based on the kernel based perceptron generally the required memory of the kernel based perceptron for storing the online hypothesis is not bounded previous work ha been focused on discarding part of the instance in order to keep the memory bounded in the proposed algorithm the instance are not discarded but projected onto the space spanned by the previous online hypothesis we derive a relative mistake bound and compare our algorithm both analytically and empirically to the state of the art forgetron algorithm dekel et al the first variant of our algorithm called projectron outperforms the forgetron the second variant called projectron outperforms even the perceptron 
we propose laplace max margin markov network lapm n and a general class of bayesian m n bm n of which the lapm n is a special case with sparse structural bias for robust structured prediction bm n generalizes extant structured prediction rule based on point estimator to a bayes predictor using a learnt distribution of rule we present a novel structured maximum entropy discrimination smed formalism for combining bayesian and max margin learning of markov network for structured prediction and our approach subsumes the conventional m n a a special case an ecient learning algorithm based on variational inference and standard convex optimization solver for m n and a generalization bound are oered our method outperforms competing one on both synthetic and real ocr data 
kernel supervised learning method can be unified by utilizing the tool from regularization theory the duality between regularization and prior lead to interpreting regularization method in term of maximum a posteriori estimation and ha motivated bayesian interpretation of kernel method in this paper we pursue a bayesian interpretation of sparsity in the kernel setting by making use of a mixture of a point mass distribution and prior that we refer to a silverman sg prior we provide a theoretical analysis of the posterior consistency of a bayesian model choice procedure based on this prior we also establish the asymptotic relationship between this procedure and the bayesian information criterion 
in recent year the blogosphere ha experienced a substantial increase in the number of post published daily forcing user to cope with information overload the task of guiding user through this flood of information ha thus become critical to address this issue we present a principled approach for picking a set of post that best cover the important story in the blogosphere we define a simple and elegant notion of coverage and formalize it a a submodular optimization problem for which we can efficiently compute a near optimal solution in addition since people have varied interest the ideal coverage algorithm should incorporate user preference in order to tailor the selected post to individual taste we define the problem of learning a personalized coverage function by providing an appropriate user interaction model and formalizing an online learning framework for this task we then provide a no regret algorithm which can quickly learn a user s preference from limited feedback we evaluate our coverage and personalization algorithm extensively over real blog data result from a user study show that our simple coverage algorithm doe a well a most popular blog aggregation site including google blog search yahoo buzz and digg furthermore we demonstrate empirically that our algorithm can successfully adapt to user preference we believe that our technique especially with personalization can dramatically reduce information overload 
with the increased availability of data for complex domain it is desirable to learn bayesian network structure that are sufficiently expressive for generalization while at the same time allow for tractable inference while the method of thin junction tree can in principle be used for this purpose it fully greedy nature make it prone to overfitting particularly when data is scarce in this work we present a novel method for learning bayesian network of bounded treewidth that employ global structure modification and that is polynomial both in the size of the graph and the treewidth bound at the heart of our method is a dynamic triangulation that we update in a way that facilitates the addition of chain structure that increase the bound on the model s treewidth by at most one we demonstrate the effectiveness of our treewidth friendly method on several real life data set and show that it is superior to the greedy approach a soon a the bound on the treewidth is nontrivial importantly we also show that by making use of global operator we are able to achieve better generalization even when learning bayesian network of unbounded treewidth 
this paper study the problem of frequent pattern mining with uncertain data we will show how broad class of algorithm can be extended to the uncertain data setting in particular we will study candidate generate and test algorithm hyper structure algorithm and pattern growth based algorithm one of our insightful observation is that the experimental behavior of different class of algorithm is very different in the uncertain case a compared to the deterministic case in particular the hyper structure and the candidate generate and test algorithm perform much better than tree based algorithm this counter intuitive behavior is an important observation from the perspective of algorithm design of the uncertain variation of the problem we will test the approach on a number of real and synthetic data set and show the effectiveness of two of our approach over competitive technique 
we provide some insight into how task correlation in multi task gaussian process gp regression affect the generalization error and the learning curve we analyze the asymmetric two task case where a secondary task is to help the learning of a primary task within this setting we give bound on the generalization error and the learning curve of the primary task our approach admits intuitive understanding of the multi task gp by relating it to single task gps for the case of one dimensional input space under optimal sampling with data only for the secondary task the limitation of multi task gp can be quantified explicitly 
causal analysis of continuous valued variable typically us either autoregressive model or linear gaussian bayesian network with instantaneous effect estimation of gaussian bayesian network pose serious identifiability problem which is why it wa recently proposed to use non gaussian model here we show how to combine the non gaussian instantaneous model with autoregressive model we show that such a non gaussian model is identifiable without prior knowledge of network structure and we propose an estimation method shown to be consistent this approach also point out how neglecting instantaneous effect can lead to completely wrong estimate of the autoregressive coefficient 
we present a theoretical analysis of supervised ranking providing necessary and sufficient condition for the asymptotic consistency of algorithm based on minimizing a surrogate loss function we show that many commonly used surrogate loss are inconsistent surprisingly we show inconsistency even in low noise setting we present a new value regularized linear loss establish it consistency under reasonable assumption on noise and show that it outperforms conventional ranking loss in a collaborative filtering experiment 
a new algorithm for training restricted boltzmann machine is introduced the algorithm named persistent contrastive divergence is different from the standard contrastive divergence algorithm in that it aim to draw sample from almost exactly the model distribution it is compared to some standard contrastive divergence and pseudo likelihood algorithm on the task of modeling and classifying various type of data the persistent contrastive divergence algorithm outperforms the other algorithm and is equally fast and simple 
we show that linear value function approximation is equivalent to a form of linear model approximation we then derive a relationship between the model approximation error and the bellman error and show how this relationship can guide feature selection for model improvement and or value function improvement we also show how these result give insight into the behavior of existing feature selection algorithm 
time series prediction is an important issue in a wide range of area there are various real world process whose state vary continuously and those process may have influence on each other if the past information of one process x improves the predictability of another process y x is said to have a causal influence on y in order to make good prediction it is necessary to identify the appropriate causal relationship in addition the process to be modeled may include symbolic data a well a numerical data therefore it is important to deal with symbolic and numerical time series seamlessly when attempting to detect causality in this paper we propose a new method for quantifying the strength of the causal influence from one time series to another the proposed method can represent the strength of causality a the number of bit whether each of two time series is symbolic or numerical the proposed method can quantify causality even from a small number of sample in addition we propose structuring and modeling method for multivariate time series using causal relationship of two time series our structuring and modeling method can also deal with data set which include both type of time series experimental result demonstrate that our method can perform well even if the number of sample is small 
the input to an algorithm that learns a binary classifier normally consists of two set of example where one set consists of positive example of the concept to be learned and the other set consists of negative example however it is often the case that the available training data are an incomplete set of positive example and a set of unlabeled example some of which are positive and some of which are negative the problem solved in this paper is how to learn a standard binary classifier given a nontraditional training set of this nature under the assumption that the labeled example are selected randomly from the positive example we show that a classifier trained on positive and unlabeled example predicts probability that differ by only a constant factor from the true conditional probability of being positive we show how to use this result in two different way to learn a classifier from a nontraditional training set we then apply these two new method to solve a real world problem identifying protein record that should be included in an incomplete specialized molecular biology database our experiment in this domain show that model trained using the new method perform better than the current state of the art biased svm method for learning from positive and unlabeled example 
multi label problem arise in various domain such a multi topic document categorization and protein function prediction one natural way to deal with such problem is to construct a binary classifier for each label resulting in a set of independent binary classification problem since the multiple label share the same input space and the semantics conveyed by different label are usually correlated it is essential to exploit the correlation information contained in different label in this paper we consider a general framework for extracting shared structure in multi label classification in this framework a common subspace is assumed to be shared among multiple label we show that the optimal solution to the proposed formulation can be obtained by solving a generalized eigenvalue problem though the problem is non convex for high dimensional problem direct computation of the solution is expensive and we develop an efficient algorithm for this case one appealing feature of the proposed framework is that it includes several well known algorithm a special case thus elucidating their intrinsic relationship we have conducted extensive experiment on eleven multi topic web page categorization task and result demonstrate the effectiveness of the proposed formulation in comparison with several representative algorithm 
this paper address the repeated acquisition of label for data item when the labeling is imperfect we examine the improvement or lack thereof in data quality via repeated labeling and focus especially on the improvement of training label for supervised induction with the outsourcing of small task becoming easier for example via rent a coder or amazon s mechanical turk it often is possible to obtain le than expert labeling at low cost with low cost labeling preparing the unlabeled part of the data can become considerably more expensive than labeling we present repeated labeling strategy of increasing complexity and show several main result i repeated labeling can improve label quality and model quality but not always ii when label are noisy repeated labeling can be preferable to single labeling even in the traditional setting where label are not particularly cheap iii a soon a the cost of processing the unlabeled data is not free even the simple strategy of labeling everything multiple time can give considerable advantage iv repeatedly labeling a carefully chosen set of point is generally preferable and we present a robust technique that combine different notion of uncertainty to select data point for which quality should be improved the bottom line the result show clearly that when labeling is not perfect selective acquisition of multiple label is a strategy that data miner should have in their repertoire for certain label quality cost regime the benefit is substantial 
this work proposes a learning method for deep architecture that take advantage of sequential data in particular from the temporal coherence that naturally exists in unlabeled video recording that is two successive frame are likely to contain the same object or object this coherence is used a a supervisory signal over the unlabeled data and is used to improve the performance on a supervised task of interest we demonstrate the effectiveness of this method on some pose invariant object and face recognition task 
markov decision process mdps have been extensively studied and used in the context of planning and decision making and many method exist to find the optimal policy for problem modelled a mdps although finding the optimal policy is sufficient in many domain in certain application such a decision support system where the policy is executed by a human rather than a machine finding all possible near optimal policy might be useful a it provides more flexibility to the person executing the policy in this paper we introduce the new concept of non deterministic mdp policy and address the question of finding near optimal non deterministic policy we propose two solution to this problem one based on a mixed integer program and the other one based on a search algorithm we include experimental result obtained from applying this framework to optimize treatment choice in the context of a medical decision support system 
we present an algorithm for on line incremental discovery of temporal difference td network the key contribution is the establishment of three criterion to expand a node in td network a node is expanded when the node is well known independent and ha a prediction error that requires further explanation since none of these criterion requires centralized calculation operation they are easily computed in a parallel and distributed manner and scalable for bigger problem compared to other discovery method of predictive state representation through computer experiment we demonstrate the empirical effectiveness of our algorithm 
many machine learning algorithm require the summation of gaussian kernel function an expensive operation if implemented straightforwardly several method have been proposed to reduce the computationalcomplexity of evaluating such sum including tree and analysis based method these achieve varying speedup depending on the bandwidth dimension and prescribed error making the choice between method difficult for machine learning task we provide an algorithm that combine tree method with the improved fast gauss transform ifgt a originally proposed the ifgt suffers from two problem the taylor series expansion doe not perform well for very low bandwidth and parameter selection is not trivial and can drastically affect performance and ease of use we address the first problem by employing a tree data structure resulting in four evaluation method whose performance varies based on the distribution of source and target and input parameter such a desired accuracy and bandwidth to solve the second problem we present an online tuning approach that result in a black box method that automatically chooses the evaluation method and it parameter to yield the best performance for the input data desired accuracy and bandwidth in addition the new ifgt parameter selection approach allows for tighter error bound our approach chooses the fastest method at negligible additional cost and ha superior performance in comparison with previous approach 
this paper address the problem of approximate singular value decomposition of large dense matrix that arises naturally in many machine learning application we discus two recently introduced sampling based spectral decomposition technique the nystr m and the column sampling method we present a theoretical comparison between the two method and provide novel insight regarding their suitability for various application we then provide experimental result motivated by this theory finally we propose an efficient adaptive sampling technique to select informative column from the original matrix this novel technique outperforms standard sampling method on a variety of datasets 
the support vector classification svc algorithm wa shown to work well and provide intuitive interpretation e g the parameter roughly specifies the fraction of support vector although corresponds to a fraction it cannot take the entire range between and in it original form this problem wa settled by a non convex extension of svc and the extended method wa experimentally shown to generalize better than original svc however it good generalization performance and convergence property of the optimization algorithm have not been studied yet in this paper we provide new theoretical insight into these issue and propose a novel svc algorithm that ha guaranteed generalization performance and convergence property 
reinforcement learning method for controlling stochastic process typically assume a small and discrete action space while continuous action space are quite common in real world problem the most common approach still employed in practice is coarse discretization of the action space this paper present a novel method called binary action search for realizing continuousaction policy by searching efficiently the entire action range through increment and decrement modification to the value of the action variable according to an internal binary policy defined over an augmented state space the proposed approach essentially approximates any continuous action space to arbitrary resolution and can be combined with any discrete action reinforcement learning algorithm for learning continuous action policy binary action search eliminates the restrictive modification step of adaptive action modification and requires no temporal action locality in the domain our approach is coupled with two well known reinforcement learning algorithm least square policy iteration and fitted q iteration and it use and property are thoroughly investigated and demonstrated on the continuous state action inverted pendulum double integrator and car on the hill domain 
motivated by the success of large margin method in supervised learning maximum margin clustering mmc is a recent approach that aim at extending large margin method to unsupervised learning however it optimization problem is nonconvex and existing mmc method all rely on reformulating and relaxing the nonconvex optimization problem a semidefinite program sdp though sdp is convex and standard solver are available they are computationally very expensive and only small data set can be handled to make mmc more practical we avoid sdp relaxation and propose in this paper an efficient approach that performs alternating optimization directly on the original nonconvex problem a key step to avoid premature convergence in the resultant iterative procedure is to change the loss function from the hinge loss to the laplacian square loss so that overconfident prediction are penalized experiment on a number of synthetic and real world data set demonstrate that the proposed approach is more accurate much faster hundred to ten of thousand of time faster and can handle data set that are hundred of time larger than the largest data set reported in the mmc literature 
the group lasso method for finding important explanatory factor suffers from the potential non uniqueness of solution and also from high computational cost we formulate condition for the uniqueness of group lasso solution which lead to an easily implementable test procedure that allows u to identify all potentially active group these result are used to derive an efficient algorithm that can deal with input dimension in the million and can approximate the solution path efficiently the derived method are applied to large scale learning problem where they exhibit excellent performance and where the testing procedure help to avoid misinterpretation of the solution 
we present a multi label multiple kernel learning mkl formulation in which the data are embedded into a low dimensional space directed by the instancelabel correlation encoded into a hypergraph we formulate the problem in the kernel induced feature space and propose to learn the kernel matrix a a linear combination of a given collection of kernel matrix in the mkl framework the proposed learning formulation lead to a non smooth min max problem which can be cast into a semi infinite linear program silp we further propose an approximate formulation with a guaranteed error bound which involves an unconstrained convex optimization problem in addition we show that the objective function of the approximate formulation is differentiable with lipschitz continuous gradient and hence existing method can be employed to compute the optimal solution efficiently we apply the proposed formulation to the automated annotation of drosophila gene expression pattern image and promising result have been reported in comparison with representative algorithm 
distributed processing in mobile environment dprime is a framework for processing large data set across an ad hoc network developed to address the shortcoming of google s mapreduce outside of a fully connected network dprime separate node on the network into a master and worker the master distributes section of the data to available one hop worker to process in parallel upon returning result to it master a worker is assigned an unfinished task five data mining classifier were implemented to process the data decision tree k mean knearest neighbor na ive bayes and artificial neural network ensemble were used so the classification task could be performed in parallel this framework is well suited for many task because it handle communication node movement node failure packet loss data partitioning and result collection automatically therefore dprime allows user with little knowledge of networking or distributed system to harness the processing power of an entire network of singleand multi hop node index term ad hoc network classifier data mining mapreduce ensemble f 
search query are typically very short which mean they are often underspecified or have sens that the user did not think of a broad latent query aspect is a set of keywords that succinctly represents one particular sense or one particular information need that can aid user in reformulating such query we extract such broad latent aspect from query reformulations found in historical search session log we propose a framework under which the problem of extracting such broad latent aspect reduces to that of optimizing a formal objective function under constraint on the total number of aspect the system can store and the number of aspect that can be shown in response to any given query we present algorithm to find a good set of aspect and also to pick the best k aspect matching any query empirical result on real world search engine log show significant gain over a strong baseline that us single keyword reformulations a gain of and in term of human judged accuracy and click through data respectively and around in term of consistency among aspect predicted for similar query this demonstrates both the importance of broad query aspect and the efficacy of our algorithm for extracting them 
in this study we introduce a novel algorithm for learning a polyhedron to describe the target class the proposed approach take advantage of the limited subclass information made available for the negative sample and jointly optimizes multiple hyperplane classifier each of which is designed to classify positive sample from a subclass of the negative sample the flat face of the polyhedron provides robustness whereas multiple face contributes to the flexibility required to deal with complex datasets apart from improving the prediction accuracy of the system the proposed polyhedral classifier also provides run time speedup a a by product when executed in a cascaded framework in real time we evaluate the performance of the proposed technique on a real world colon dataset both in term of prediction accuracy and online execution speed 
we show how improved sequence for magnetic resonance imaging can be found through optimization of bayesian design score combining approximate bayesian inference and natural image statistic with high performance numerical computation we propose the first bayesian experimental design framework for this problem of high relevance to clinical and brain research our solution requires large scale approximate inference for dense non gaussian model we propose a novel scalable variational inference algorithm and show how powerful method of numerical mathematics can be modified to compute primitive in our framework our approach is evaluated on raw data from a t mr scanner 
algorithm for learning to rank web document usually assume a document s relevance is independent of other document this lead to learned ranking function that produce ranking with redundant result in contrast user study have shown that diversity at high rank is often preferred we present two online learning algorithm that directly learn a diverse ranking of document based on user clicking behavior we show that these algorithm minimize abandonment or alternatively maximize the probability that a relevant document is found in the top k position of a ranking moreover one of our algorithm asymptotically achieves optimal worst case performance even if user interest change 
gaussian graphical model with sparsity in the inverse covariance matrix are of significant interest in many modern application for the problem of recovering the graphical structure information criterion provide useful optimization objective for algorithm searching through set of graph or for selection of tuning parameter of other method such a the graphical lasso which is a likelihood penalization technique in this paper we establish the consistency of an extended bayesian information criterion for gaussian graphical model in a scenario where both the number of variable p and the sample size n grow compared to earlier work on the regression case our treatment allows for growth in the number of non zero parameter in the true model which is necessary in order to cover connected graph we demonstrate the performance of this criterion on simulated data when used in conjunction with the graphical lasso and verify that the criterion indeed performs better than either cross validation or the ordinary bayesian information criterion when p and the number of non zero parameter q both scale with n 
conditional random sampling cr wa originally proposed for efficiently computing pairwise l l distance in static large scale and sparse data this study modifies the original cr and extends cr to handle dynamic or streaming data which much better reflect the real world situation than assuming static data compared with many other sketching algorithm for dimension reduction such a stable random projection cr exhibit a significant advantage in that it is one sketch for all in particular we demonstrate the effectiveness of cr in efficiently computing the hamming norm the hamming distance the lp distance and the distance a generic estimator and an approximate variance formula are also provided for approximating any type of distance we recommend cr a a promising tool for building highly scalable system in machine learning data mining recommender system and information retrieval 
we propose a max margin formulation for the multi label classification problem where the goal is to tag a data point with a set of pre specified label given a set of l label a data point can be tagged with any of the l possible subset the main challenge therefore lie in optimising over this exponentially large label space subject to label correlation existing solution take either of two approach the first assumes a priori that there are no label correlation and independently train a classifier for each label a is done in the v all heuristic this reduces the problem complexity from exponential to linear and such method can scale to large problem the second approach explicitly model correlation by pairwise label interaction however the complexity remains exponential unless one assumes that label correlation are sparse furthermore the learnt correlation reflect the training set bias we take a middle approach that assumes label are correlated but doe not incorporate pairwise label term in the prediction function we show that the complexity can still be reduced from exponential to linear while modelling dense pairwise label correlation by incorporating correlation prior we can overcome training set bias and improve prediction accuracy we provide a principled interpretation of the v all method and show appearing in proceeding of the th international conference on machine learning haifa israel copyright by the author s owner s that it arises a a special case of our formulation we also develop efficient optimisation algorithm that can be order of magnitude faster than the state of the art 
in many real world application such a image retrieval it would be natural to measure the distance from one instance to others using instance specific distance which capture the distinction from the perspective of the concerned instance however there is no complete framework for learning instance specific distance since existing method are incapable of learning such distance for test instance and unlabeled data in this paper we propose the isd method to address this issue the key of isd is metric propagation that is propagating and adapting metric of individual labeled example to individual unlabeled instance we formulate the problem into a convex optimization framework and derive efficient solution experiment show that isd can effectively learn instance specific distance for labeled a well a unlabeled instance the metric propagation scheme can also be used in other scenario 
we address the challenge of assessing conservation of gene expression in complex non homogeneous datasets recent study have demonstrated the success of probabilistic model in studying the evolution of gene expression in simple eukaryotic organism such a yeast for which measurement are typically scalar and independent model capable of studying expression evolution in much more complex organism such a vertebrate are particularly important given the medical and scientific interest in specie such a human and mouse we present brownian factor phylogenetic analysis a statistical model that make a number of significant extension to previous model to enable characterization of change in expression among highly complex organism we demonstrate the efficacy of our method on a microarray dataset profiling diverse tissue from multiple vertebrate specie we anticipate that the model will be invaluable in the study of gene expression pattern in other diverse organism a well such a worm and insect 
cost curve have recently been introduced a an alternative or complement to roc curve in order to visualize binary classifier performance of importance to both cost and roc curve is the computation of confidence interval along with the curve themselves so that the reliability of a classifier s performance can be assessed computing confidence interval for the difference in performance between two classifier allows the determination of whether one classifier performs significantly better than another a simple procedure to obtain confidence interval for cost or the difference between two cost under various operating condition is to perform bootstrap resampling of the test set in this paper we derive exact bootstrap distribution for these value and use these dstributions to obtain confidence interval under various operating condition performance of these confidence interval are measured in term of coverage accuracy simulation show excellent result 
data mining technique use score function to quantify how well a model fit a given data set parameter are estimated by optimising the fit a measured by the chosen score function and model choice is guided by the size of the score for the different model since different score function summarise the fit in different way it is important to choose a function which match the objective of the data mining exercise for predictive classification problem a wide variety of score function exist including measure such a precision and recall the f measure misclassification rate the area under the roc curve the auc and others the first four of these require a classification threshold to be chosen a choice which may not be easy or may even be impossible especially when the classification rule is to be applied in the future in contrast the auc doe not require the specification of a classification threshold but summarises performance over the range of possible threshold choice however unfortunately and despite the widespread use of the auc it ha a previously unrecognised fundamental incoherence lying at the core of it definition this mean that using the auc can lead to poor model choice and unecessary misclassifications the auc is set in context it deficiency explained and the implication illustrated with the bottom line being that the auc should not be used a family of coherent alternative score is described the idea are illustrated with example from bank loan fraud face recognition and health screening 
in a variety of application kernel machine such a support vector machine svms have been used with great success often delivering stateof the art result using the kernel trick they work on several domain and even enable heterogeneous data fusion by concatenating feature space or multiple kernel learning unfortunately they are not suited for truly large scale application since they suffer from the curse of supporting vector i e the speed of applying svms decay linearly with the number of support vector in this paper we develop coffin a new training strategy for linear svms that effectively allows the use of on demand computed kernel feature space and virtual example in the primal with linear training and prediction effort this framework leverage svm application to truly large scale problem a an example we train svms for human splice site recognition involving million example and sophisticated string kernel additionally we learn an svm based gender detector on million example on low tech hardware and achieve beyond the stateof the art accuracy on both task source code data set and script are freely available from http sonnenburgs de soeren coffin 
randomly connected recurrent neural circuit have proven to be very powerful model for online computation when a trained memoryless readout function is appended such reservoir computing rc system are commonly used in two flavor with analog or binary spiking neuron in the recurrent circuit previous work showed a fundamental difference between these two incarnation of the rc idea the performance of a rc system built from binary neuron seems to depend strongly on the network connectivity structure in network of analog neuron such dependency ha not been observed in this article we investigate this apparent dichotomy in term of the in degree of the circuit node our analysis based amongst others on the lyapunov exponent reveal that the phase transition between ordered and chaotic network behavior of binary circuit qualitatively differs from the one in analog circuit this explains the observed decreased computational performance of binary circuit of high node in degree furthermore a novel mean field predictor for computational performance is introduced and shown to accurately predict the numerically obtained result 
recently many data type arising from data mining and web search application can be modeled a bipartite graph example include query and url in query log and author and paper in scientific literature however one of the issue is that previous algorithm only consider the content and link information from one side of the bipartite graph there is a lack of constraint to make sure the final relevance of the score propagation on the graph a there are many noisy edge within the bipartite graph in this paper we propose a novel and general co hit algorithm to incorporate the bipartite graph with the content information from both side a well a the constraint of relevance moreover we investigate the algorithm based on two framework including the iterative and the regularization framework and illustrate the generalized co hit algorithm from different view for the iterative framework it contains hit and personalized pagerank a special case in the regularization framework we successfully build a connection with hit and develop a new cost function to consider the direct relationship between two entity set which lead to a significant improvement over the baseline method to illustrate our methodology we apply the co hit algorithm with many different setting to the application of query suggestion by mining the aol query log data experimental result demonstrate that coregu i e a model of the regularization framework achieves the best performance with consistent and promising improvement 
a good distance measure for time series need to properly incorporate the temporal structure and should be applicable to sequence with unequal length in this paper we propose a distance measure a a principled solution to the two requirement unlike the conventional feature vector representation our approach represents each time series with a summarizing smooth curve in a reproducing kernel hilbert space rkhs and therefore translate the distance between time series into distance between curve moreover we propose to learn the kernel of this rkhs from a population of time series with discrete observation using gaussian process based non parametric mixed effect model experiment on two vastly different real world problem show that the proposed distance measure lead to improved classification accuracy over the conventional distance measure 
we consider feature extraction dimensionality reduction for compositional data where the data vector are constrained to be positive and constant sum in real world problem the data component variable usually have complicated correlation while their total number is huge such scenario demand feature extraction that is we shall de correlate the component and reduce their dimensionality traditional technique such a the principle component analysis pca are not suitable for these problem due to unique statistical property and the need to satisfy the constraint in compositional data this paper present a novel approach to feature extraction for compositional data our method first identifies a family of dimensionality reduction projection that preserve all relevant constraint and then find the optimal projection that maximizes the estimated dirichlet precision on projected data it reduces the compositional data to a given lower dimensionality while the component in the lower dimensional space are de correlated a much a possible we develop theoretical foundation of our approach and validate it effectiveness on some synthetic and real world datasets 
we propose a new class of consistency constraint for linear programming lp relaxation for finding the most probable map configuration in graphical model usual cluster based lp relaxation enforce joint consistency on the belief of a cluster of variable with computational cost increasing exponentially with the size of the cluster by partitioning the state space of a cluster and enforcing consistency only across partition we obtain a class of constraint which although le tight are computationally feasible for large cluster we show how to solve the cluster selection and partitioning problem monotonically in the dual lp using the current belief to guide these choice we obtain a dual message passing algorithm and apply it to protein design problem where the variable have large state space and the usual cluster based relaxation are very costly the resulting method solves many of these problem exactly and significantly faster than a method that doe not use partitioning 
we present a model that describes the structure in the response of different brain area to a set of stimulus in term of stimulus category cluster of stimulus and functional unit cluster of voxels we assume that voxels within a unit respond similarly to all stimulus from the same category and design a nonparametric hierarchical model to capture inter subject variability among the unit the model explicitly encodes the relationship between brain activation and fmri time course a variational inference algorithm derived based on the model learns category unit and a set of unit category activation probability from data when applied to data from an fmri study of object recognition the method find meaningful and consistent clustering of stimulus into category and voxels into unit 
the border gateway protocol bgp is one of the fundamental computer communication protocol monitoring and mining bgp update message can directly reveal the health and stability of internet routing here we make two contribution firstly we find pattern in bgp update like self similarity power law and lognormal marginals secondly using these pattern we find anomaly specifically we develop bgp lens an automated bgp update analysis tool that ha three desirable property a it is effective able to identify phenomenon that would otherwise go unnoticed such a a peculiar clothesline behavior or prolonged spike that last a long a hour b it is scalable using algorithm are all linear on the number of time tick and c it is admin friendly giving useful lead for phenomenon of interest we showcase the capability of bgp lens by identifying surprising phenomenon verified by syadmins over a massive trace of bgp update spanning year from the publicly available site datapository net 
this paper analysis alternative technique for deploying low cost human resource for data acquisition for classifier induction in domain exhibiting extreme class imbalance where traditional labeling strategy such a active learning can be ineffective consider the problem of building classifier to help brand control the content adjacent to their on line advertisement although frequent enough to worry advertiser objectionable category are rare in the distribution of impression encountered by most on line advertiser so rare that traditional sampling technique do not find enough positive example to train effective model an alternative way to deploy human resource for training data acquisition is to have them guide the learning by searching explicitly for training example of each class we show that under extreme skew even basic technique for guided learning completely dominate smart active strategy for applying human resource to select case for labeling therefore it is critical to consider the relative cost of search versus labeling and we demonstrate the tradeoff for different relative cost we show that in cost skew setting where the choice between search and active labeling is equivocal a hybrid strategy can combine the benefit 
we continue our study of online prediction of the labelling of a graph we show a fundamental limitation of laplacian based algorithm if the graph ha a large diameter then the number of mistake made by such algorithm may be proportional to the square root of the number of vertex even when tackling simple problem we overcome this drawback by mean of an efficient algorithm which achieves a logarithmic mistake bound it is based on the notion of a spine a path graph which provides a linear embedding of the original graph in practice graph may exhibit cluster structure thus in the last part we present a modified algorithm which achieves the best of both world it performs well locally in the presence of cluster structure and globally on large diameter graph 
large scale sensor deployment and an increased use of privacy preserving transformation have led to an increasing interest in mining uncertain time series data traditional distance measure such a euclidean distance or dynamic time warping are not always effective for analyzing uncertain time series data recently some measure have been proposed to account for uncertainty in time series data however we show in this paper that their applicability is limited in specific these approach do not provide an intuitive way to compare two uncertain time series and do not easily accommodate multiple error function in this paper we provide a theoretical framework that generalizes the notion of similarity between uncertain time series secondly we propose dust a novel distance measure that accommodates uncertainty and degenerate to the euclidean distance when the distance is large compared to the error we provide an extensive experimental validation of our approach for the following application classification top k motif search and top k nearest neighbor query 
bayesian network learning algorithm have been widely used for causal discovery since the pioneer work among all existing algorithm three phase dependency analysis algorithm tpda is the most efficient one in the sense that it ha polynomial time complexity however there are still some limitation to be improved first tpda depends on mutual information based conditional independence ci test and so is not easy to be applied to continuous data in addition tpda us two phase to get approximate skeleton of bayesian network which is not efficient in practice in this paper we propose a two phase algorithm with partial correlation based ci test the first phase of the algorithm construct a markov random field from data which provides a close approximation to the structure of the true bayesian network at the second phase the algorithm remove redundant edge according to ci test to get the true bayesian network we show that two phase algorithm with partial correlation based ci test can deal with continuous data following arbitrary distribution rather than only gaussian distribution 
we propose a new algorithm for independent component and independent subspace analysis problem this algorithm us a contrast based on the schweizer wolff measure of pairwise dependence schweizer wolff a non parametric measure computed on pairwise rank of the variable our algorithm frequently outperforms state of the art ica method in the normal setting is significantly more robust to outlier in the mixed signal and performs well even in the presence of noise our method can also be used to solve independent subspace analysis isa problem by grouping signal recovered by ica method we provide an extensive empirical evaluation using simulated sound and image data 
merchant selling product on the web often ask their customer to share their opinion and hand on experience on product they have purchased a e commerce is becoming more and more popular the number of customer review a product receives grows rapidly this make it difficult for a potential customer to read them to make an informed decision on whether to purchase the product in this research we aim to mine customer review of a product and extract highly specific product related entity on which reviewer express their opinion opinion expression and sentence are also identified and opinion orientation for each recognized product entity are classified a positive or negative different from previous approach that have mostly relied on natural language processing technique or statistic information we propose a novel machine learning framework using lexicalized hmms the approach naturally integrates linguistic feature such a part ofspeech and surrounding contextual clue of word into automatic learning the experimental result demonstrate the effectiveness of the proposed approach in web opinion mining and extraction from product review 
deep belief network dbns are hierarchical generative model which have been used successfully to model high dimensional visual data however they are not robust to common variation such a occlusion and random noise we explore two strategy for improving the robustness of dbns first we show that a dbn with sparse connection in the rst layer is more robust to variation that are not in the training set second we develop a probabilistic denoising algorithm to determine a subset of the hidden layer node to unclamp we show that this can be applied to any feedforward network classier with localized rst layer connection recognition result after denoising are signicantly better over the standard dbn implementation for various source of noise 
not only is wikipedia a comprehensive source of quality information it ha several kind of internal structure e g relational summary known a infoboxes which enable self supervised information extraction while previous effort at extraction from wikipedia achieve high precision and recall on well populated class of article they fail in a larger number of case largely because incomplete article and infrequent use of infoboxes lead to insufficient training data this paper present three novel technique for increasing recall from wikipedia s long tail of sparse class shrinkage over an automatically learned subsumption taxonomy a retraining technique for improving the training data and supplementing result by extracting from the broader web our experiment compare design variation and show that used in concert these technique increase recall by a factor of to while maintaining or increasing precision 
psychophysical experiment show that human are better at perceiving rotation and expansion than translation these finding are inconsistent with standard model of motion integration which predict best performance for translation to explain this discrepancy our theory formulates motion perception at two level of inference we first perform model selection between the competing model e g translation rotation and expansion and then estimate the velocity using the selected model we define novel prior model for smooth rotation and expansion using technique similar to those in the slow and smooth model e g green function of differential operator the theory give good agreement with the trend observed in human experiment 
sparsity is a desirable property in high dimensional learning the l norm regularization can lead to primal sparsity while max margin method achieve dual sparsity combining these two method an l norm max margin markov network l m n can achieve both type of sparsity this paper analyzes it connection to the laplace max margin markov network lapm n which inherits the dual sparsity of max margin model but is pseudo primal sparse and to a novel adaptive m n adapm n we show that the l m n is an extreme case of the lapm n and the l m n is equivalent to an adapm n based on this equivalence we develop a robust em style algorithm for learning an l m n we demonstrate the advantage of the simultaneously pseudo primal and dual sparse model over the one which enjoy either primal or dual sparsity on both synthetic and real data set 
the control of neuroprosthetic device from the activity of motor cortex neuron benefit from learning effect where the function of these neuron is adapted to the control task it wa recently shown that tuning property of neuron in monkey motor cortex are adapted selectively in order to compensate for an erroneous interpretation of their activity in particular it wa shown that the tuning curve of those neuron whose preferred direction had been misinterpreted changed more than those of other neuron in this article we show that the experimentally observed self tuning property of the system can be explained on the basis of a simple learning rule this learning rule utilizes neuronal noise for exploration and performs hebbian weight update that are modulated by a global reward signal in contrast to most previously proposed reward modulated hebbian learning rule this rule doe not require extraneous knowledge about what is noise and what is signal the learning rule is able to optimize the performance of the model system within biologically realistic period of time and under high noise level when the neuronal noise is fitted to experimental data the model produce learning effect similar to those found in monkey experiment 
the detection of repeated subsequence time series motif is a problem which ha been shown to have great utility for several higher level data mining algorithm including classification clustering segmentation forecasting and rule discovery in recent year there ha been significant research effort spent on efficiently discovering these motif in static offline database however for many domain the inherent streaming nature of time series demand online discovery and maintenance of time series motif in this paper we develop the first online motif discovery algorithm which monitor and maintains motif exactly in real time over the most recent history of a stream our algorithm ha a worst case update time which is linear to the window size and is extendible to maintain more complex pattern structure in contrast the current offline algorithm either need significant update time or require very costly pre processing step which online algorithm simply cannot afford our core idea allow useful extension of our algorithm to deal with arbitrary data rate and discovering multidimensional motif we demonstrate the utility of our algorithm with a variety of case study in the domain of robotics acoustic monitoring and online compression 
this paper address the problem of sparsity pattern detection for unknown ksparse n dimensional signal observed through m noisy random linear measurement sparsity pattern recovery arises in a number of setti ng including statistical model selection pattern detection and image acquisition the main result in this paper are necessary and sufficient condition for asymptoti cally reliable sparsity pattern recovery in term of the dimension m n and k a well a the signal tonoise ratio snr and the minimum to average ratio mar of the nonzero entry of the signal we show that m klog n k snrmar is necessary for any algorithm to succeed regardless of complexity this match a previous sufficient condition for maximum likelihood estimation within a constant factor under certain scaling of k snr and mar with n we also show a sufficient condition for a computationally trivial thresholding algorithm tha t is larger than the previous expression by only a factor of snr and larger than the requirement for lasso by only a factor of mar this provides insight on the precise value and limitation of convex programming based algorithm this paper considers the problem of estimating sparse signal in the presence of noise we are specifically concerned with understanding the theoretical estimation limit and how far practical algorithm are from those limit in the context of visual co rtex modeling this analysis may help u understand what visual feature are resolvable from visual data to keep the analysis general we consider the following abstract estimation problem an unknown sparse signal x is modeled a an n dimensional real vector with k nonzero component the location of the nonzero component is called the sparsity pattern we consider the problem of detecting the sparsity pattern of x from an m dimensional measurement vector y ax d where a rm n is a known measurement matrix and d rm is an additive noise vector with a known distribution we are interested in this work wa supported in part by a university of california president s postdoctoral fellowship nsf 
we describe a method for inferring linear causal relation among multi dimensional variable the idea is to use an asymmetry between the distribution of cause and effect that occurs if both the covariance matrix of the cause and the structure matrix mapping cause to the effect are independently chosen the method work for both stochastic and deterministic causal relation provided that the dimensionality is sufficiently high in some experiment wa enough it is applicable to gaussian a well a non gaussian data 
we present a novel commentator system that learns language from sportscast of simulated soccer game the system learns to parse and generate commentary without any engineered knowledge about the english language training is done using only ambiguous supervision in the form of textual human commentary and simulation state of the soccer game the system simultaneously try to establish correspondence between the commentary and the simulation state a well a build a translation model we also present a novel algorithm iterative generation strategy learning igsl for deciding which event to comment on human evaluation of the generated commentary indicate they are of reasonable quality compared to human commentary 
we propose dirichlet bernoulli alignment dba a generative model for corpus in which each pattern e g a document contains a set of instance e g paragraph in the document and belongs to multiple class by casting predefined class a latent dirichlet variable i e instance level label and modeling the multi label of each pattern a bernoulli variable conditioned on the weighted empirical average of topic assignment dba automatically aligns the latent topic discovered from data to human defined class dba is useful for both pattern classification and instance disambiguation which are tested on text classification and named entity disambiguation in web search query respectively 
the pervasiveness of mobile device and location based service is leading to an increasing volume of mobility data this side eect provides the opportunity for innovative method that analyse the behavior of movement in this paper we propose wherenext which is a method aimed at predicting with a certain level of accuracy the next location of a moving object the prediction us previously extracted movement pattern named trajectory pattern which are a concise representation of behavior of moving object a sequence of region frequently visited with a typical travel time a decision tree named t pattern tree is built and evaluated with a formal training and test process the tree is learned from the trajectory pattern that hold a certain area and it may be used a a predictor of the next location of a new trajectory finding the best matching path in the tree three dierent best matching method to classify a new moving object are proposed and their impact on the quality of prediction is studied extensively using trajectory pattern a predictive rule ha the following implication i the learning depends on the movement of all available object in a certain area instead of on the individual history of an object ii the prediction tree intrinsically contains the spatio temporal property that have emerged from the data and this allows u to define matching method that striclty depend on the property of such movement in addition we propose a set of other measure that evaluate a priori the predictive power of a set of trajectory pattern this measure were tuned on a real life case study finally an exhaustive set of experiment and result on the real dataset are presented 
most model of utility elicitation in decision support and interactive optimization assume a predefined set of catalog feature over which user preference are expressed however user may differ in the feature over which they are most comfortable expressing their preference in this work we consider the problem of feature elicitation a user s utility function is expressed using feature whose definition in term of catalog feature are unknown we cast this a a problem of concept learning but whose goal is to identify only enough about the concept to enable a good decision to be recommended we describe computational procedure for identifying optimal alternative w r t minimax regret in the presence of concept uncertainty and describe several heuristic query strategy that focus on reduction of relevant concept uncertainty 
in many real world domain undirected graphical model such a markov random field provide a more natural representation of the statistical dependency structure than directed graphical model unfortunately structure learning of undirected graph using likelihood based score remains difficult because of the intractability of computing the partition function we describe a new markov random field structure learning algorithm motivated by canonical parameterization of abbeel et al we provide computational improvement on their parameterization by learning per variable canonical factor which make our algorithm suitable for domain with hundred of node we compare our algorithm against several algorithm for learning undirected and directed model on simulated and real datasets from biology our algorithm frequently outperforms existing algorithm producing higher quality structure suggesting that enforcing consistency during structure learning is beneficial for learning undirected graph 
distance metric learning play a very crucial role in many data mining algorithm because the performance of an algorithm relies heavily on choosing a good metric however the labeled data available in many application is scarce and hence the metric learned are often unsatisfactory in this paper we consider a transfer learning setting in which some related source task with labeled data are available to help the learning of the target task we first propose a convex formulation for multi task metric learning by modeling the task relationship in the form of a task covariance matrix then we regard transfer learning a a special case of multi task learning and adapt the formulation of multi task metric learning to the transfer learning setting for our method called transfer metric learning tml in tml we learn the metric and the task covariance between the source task and the target task under a unified convex formulation to solve the convex optimization problem we use an alternating method in which each subproblem ha an efficient solution experimental result on some commonly used transfer learning application demonstrate the effectiveness of our method 
this paper address named entity mining nem in which we mine knowledge about named entity such a movie game and book from a huge amount of data nem is potentially useful in many application including web search online advertisement and recommender system there are three challenge for the task finding suitable data source coping with the ambiguity of named entity class and incorporating necessary human supervision into the mining process this paper proposes conducting nem by using click through data collected at a web search engine employing a topic model that generates the click through data and learning the topic model by weak supervision from human specifically it characterizes each named entity by it associated query and url in the click through data it us the topic model to resolve ambiguity of named entity class by representing the class a topic it employ a method referred to a weakly supervised latent dirichlet allocation w lda to accurately learn the topic model with partially labeled named entity experiment on a large scale click through data containing over billion query url pair show that the proposed approach can conduct very accurate nem and significantly outperforms the baseline 
spatial classification is the task of learning model to predict class label based on the feature of entity a well a the spatial relationship to other entity and their feature spatial data can be represented a multi relational data however it present novel challenge not present in multi relational problem one such problem is that spatial relationship are embedded in space unknown a priori and it is part of the algorithm s task to determine which relationship are important and what property to consider in order to determine when two entity are spatially related in an adaptive and non parametric way we propose a voronoi based neighbourhood definition upon which spatial literal can be built property of these neighbourhood also need to be described and used for classification purpose non spatial aggregation literal already exist within the multi relational framework but are not sufficient for comprehensive spatial classification a formal set of addition to the multi relational data mining framework is proposed to be able to represent spatial aggregation a well a spatial feature and literal these addition allow for capturing more complex interaction and spatial occurrence such a spatial trend in order to more efficiently perform the rule learning and exploit powerful multi processor machine a scalable parallelized method capable of reducing the runtime by several factor is presented the method is compared against existing method by experimental evaluation on a real world crime dataset which demonstrate the importance of the neighbourhood definition and the advantage of parallelization 
automatic news extraction from news page is important in many web application such a news aggregation however the existing news extraction method based on template level wrapper induction have three serious limitation first the existing method cannot correctly extract page belonging to an unseen template second it is costly to maintain up to date wrapper for a large amount of news website because any change of a template may invalidate the corresponding wrapper last the existing method can merely extract unformatted plain text and thus are not user friendly in this paper we tackle the problem of template independent web news extraction in a user friendly way we formalize web news extraction a a machine learning problem and learn a template independent wrapper using a very small number of labeled news page from a single site novel feature dedicated to news title and body are developed correlation between news title and news body are exploited our template independent wrapper can extract news page from different site regardless of template moreover our approach can extract not only text but also image and animates within the news body and the extracted news article are in the same visual style a in the original page in our experiment a wrapper learned from page from a single news site achieved an accuracy of on news page from news site 
relational world model that can be learned from experience in stochastic domain have received significant attention recently however efficient planning using these model remains a major issue we propose to convert learned noisy probabilistic relational rule into a structured dynamic bayesian network representation predicting the effect of action sequence using approximate inference allows for planning in complex world we evaluate the effectiveness of our approach for online planning in a d simulated blocksworld with an articulated manipulator and realistic physic empirical result show that our method can solve problem where existing method fail 
we consider the problem of predicting the likelihood that a company will purchase a new product from a seller the statistical model we have developed at ibm for this purpose rely on historical transaction data coupled with structured firmographic information like the company revenue number of employee and so on in this paper we extend this methodology to include additional text based feature based on analysis of the content on each company s website empirical result demonstrate that incorporating such web content can significantly improve customer targeting furthermore we present method to actively select only the web content that is likely to improve our model while reducing the cost of acquisition and processing 
we present a demonstration of an interactive wrapper induction system called pictor which is able to minimize labeling cost yet extract data with high accuracy from a website our demonstration will introduce two proposed technology record level wrapper and a wrapper assisted labeling strategy these approach allow pictor to exploit previously generated wrapper in order to predict similar label in a partially labeled webpage or a completely new webpage our experiment result show the effectiveness of the pictor system 
many modern visual recognition algorithm incorporate a step of spatial pooling where the output of several nearby feature detector are combined into a local or global bag of feature in a way that preserve task related information while removing irrelevant detail pooling is used to achieve invariance to image transformation more compact representation and better robustness to noise and clutter several paper have shown that the detail of the pooling operation can greatly influence the performance but study have so far been purely empirical in this paper we show that the reason underlying the performance of various pooling method are obscured by several confounding factor such a the link between the sample cardinality in a spatial pool and the resolution at which low level feature have been extracted we provide a detailed theoretical analysis of max pooling and average pooling and give extensive empirical comparison for object recognition task 
recently fitted q iteration fqi based method have becom e more popular due to their increased sample efficiency a more stable learning process and the higher quality of the resulting policy however these method remain hard to use for continuous action space which frequently occur in real world task e g in robotics and other technical application the greedy action select ion commonly used for the policy improvement step is particularly problematic a it is expensive for continuous action can cause an unstable learning process in troduces an optimization bias and result in highly non smooth policy unsuitable f or real world system in this paper we show that by using a soft greedy action selection the policy improvement step used in fqi can be simplified to an inexpensi ve advantageweighted regression with this result we are able to derive a new computationally efficient fqi algorithm which can even deal with high dimensi onal action space 
everyday social interaction are heavily influenced by our snap judgment about others goal even young infant can infer the goal of intentional agent from observing how they interact with object and other agent in their environment e g that one agent is helping or hindering another s attempt to get up a hill or open a box we propose a model for how people can infer these social goal from action based on inverse planning in multiagent markov decision problem mdps the model infers the goal most likely to be driving an agent s behavior by assuming the agent act approximately rationally given environmental constraint and it model of other agent present we also present behavioral evidence in support of this model over a simpler perceptual cue based alternative 
many popular optimization algorithm like the levenberg marquardt algorithm lma use heuristic based controller that modulate the behavior of the optimizer during the optimization process for example in the lma a damping parameter is dynamically modified based on a set of rule that were developed using heuristic argument reinforcement learning rl is a machine learning approach to learn optimal controller from example and thus is an obvious candidate to improve the heuristic based controller implicit in the most popular and heavily used optimization algorithm improving the performance of off the shelf optimizers is particularly important for time constrained optimization problem for example the lma algorithm ha become popular for many real time computer vision problem including object tracking from video where only a small amount of time can be allocated to the optimizer on each incoming video frame here we show that a popular modern reinforcement learning technique using a very simple state space can dramatically improve the performance of general purpose optimizers like the lma surprisingly the controller learned for a particular domain also work well in very different optimization domain for example we used rl method to train a new controller for the damping parameter of the lma this controller wa trained on a collection of classic relatively small non linear regression problem the modified lma performed better than the standard lma on these problem this controller also dramatically outperformed the standard lma on a difficult computer vision problem for which it had not been trained thus the controller appeared to have extracted control rule that were not just domain specific but generalized across a range of optimization domain 
lately there exist increasing demand for online abnormality monitoring over trajectory stream which are obtained from moving object tracking device this problem is challenging due to the requirement of high speed data processing within limited space cost in this paper we present a novel framework for monitoring anomaly over continuous trajectory stream first we illustrate the importance of distance based anomaly monitoring over moving object trajectory then we utilize the local continuity characteristic of trajectory to build local cluster upon trajectory stream and monitor anomaly via efficient pruning strategy finally we propose a piecewise metric index structure to reschedule the joining order of local cluster to further reduce the time cost our extensive experiment demonstrate the effectiveness and efficiency of our method 
this paper introduces two new method for label ranking based on a probabilistic model of ranking data called the plackett luce model the idea of the first method is to use the pl model to fit locally constant probability model in the context of instance based learning a opposed to this the second method estimate a global model in which the pl parameter are represented a function of the instance comparing our method with previous approach to label ranking we find that they offer a number of advantage experimentally we moreover show that they are highly competitive to start of the art method in term of predictive accuracy especially in the case of training data with incomplete ranking information 
merchant selling product on the web often ask their customer to share their opinion and hand on experience on product they have purchased unfortunately reading through all customer review is difficult especially for popular item the number of review can be up to hundred or even thousand this make it difficult for a potential customer to read them to make an informed decision the opinionminer system designed in this work aim to mine customer review of a product and extract high detailed product entity on which reviewer express their opinion opinion expression are identified and opinion orientation for each recognized product entity are classified a positive or negative different from previous approach that employed rule based or statistical technique we propose a novel machine learning approach built under the framework of lexicalized hmms the approach naturally integrates multiple important linguistic feature into automatic learning in this paper we describe the architecture and main component of the system the evaluation of the proposed method is presented based on processing the online product review from amazon and other publicly available datasets 
a challenging problem in estimating high dimensional graphical model is to choose the regularization parameter in a data dependent way the standard technique include k fold cross validation k cv akaike information criterion aic and bayesian information criterion bic though these method work well for low dimensional problem they are not suitable in high dimensional setting in this paper we present star a new stability based method for choosing the regularization parameter in high dimensional inference for undirected graph the method ha a clear interpretation we use the least amount of regularization that simultaneously make a graph sparse and replicable under random sampling this interpretation requires essentially no condition under mild condition we show that star is partially sparsistent in term of graph estimation i e with high probability all the true edge will be included in the selected model even when the graph size diverges with the sample size empirically the performance of star is compared with the state of the art model selection procedure including k cv aic and bic on both synthetic data and a real microarray dataset star outperforms all these competing procedure 
motivated by causal inference problem we propose a novel method for regression that minimizes the statistical dependence between regressors and residual the key advantage of this approach to regression is that it doe not assume a particular distribution of the noise i e it is non parametric with respect to the noise distribution we argue that the proposed regression method is well suited to the task of causal inference in additive noise model a practical disadvantage is that the resulting optimization problem is generally non convex and can be difficult to solve nevertheless we report good result on one of the task of the nip causality challenge where the goal is to distinguish cause from effect in pair of statistically dependent variable in addition we propose an algorithm for efficiently inferring causal model from observational data for more than two variable the required number of regression and independence test is quadratic in the number of variable which is a significant improvement over the simple method that test all possible dag 
in most cognitive and motor task speed accuracy tradeoff are observed individual can respond slowly and accurately or quickly yet be prone to error control mechanism governing the initiation of behavioral response are sensitive not only to task instruction and the stimulus being processed but also to the recent stimulus history when stimulus can be characterized on an easy hard dimension e g word frequency in a naming task item preceded by easy trial are responded to more quickly and with more error than item preceded by hard trial we propose a rationally motivated mathematical model of this sequential adaptation of control based on a diffusion model of the decision process in which difficulty corresponds to the drift rate for the correct response the model assumes that responding is based on the posterior distribution over which response is correct conditioned on the accumulated evidence we derive this posterior a a function of the drift rate and show that higher estimate of the drift rate lead to normatively faster responding trial by trial tracking of difficulty thus lead to sequential effect in speed and accuracy simulation show the model explains a variety of phenomenon in human speeded decision making we argue this passive statistical mechanism provides a more elegant and parsimonious account than extant theory based on elaborate control structure 
sparse coding that is modelling data vector a sparse linear combination of basis element is widely used in machine learning neuroscience signal processing and statistic this paper focus on learning the basis set also called dictionary to adapt it to specific data an approach that ha recently proven to be very effective for signal reconstruction and classification in the audio and image processing domain this paper proposes a new online optimization algorithm for dictionary learning based on stochastic approximation which scale up gracefully to large datasets with million of training sample a proof of convergence is presented along with experiment with natural image demonstrating that it lead to faster performance and better dictionary than classical batch algorithm for both small and large datasets 
algorithm based on simulating stochastic flow are a simple and natural solution for the problem of clustering graph but their widespread use ha been hampered by their lack of scalability and fragmentation of output in this article we present a multi level algorithm for graph clustering using flow that delivers significant improvement in both quality and speed the graph is first successively coarsened to a manageable size and a small number of iteration of flow simulation is performed on the coarse graph the graph is then successively refined with flow from the previous graph used a initialization for brief flow simulation on each of the intermediate graph when we reach the final refined graph the algorithm is run to convergence and the high flow region are clustered together with region without any flow forming the natural boundary of the cluster extensive experimental result on several real and synthetic datasets demonstrate the effectiveness of our approach when compared to state of the art algorithm 
locality sensitive hash function are invaluable tool for approximate near neighbor problem in high dimensional space in this work we are focused on lsh scheme where the similarity metric is the cosine measure the contribution of this work is a new class of locality sensitive hash function for the cosine similarity measure based on the theory of concomitant which arises in order statistic consider n i i d sample pair x y x y xn yn obtained from a bivariate distribution f x y concomitant theory capture the relation between the order statistic of x and y in the form of a rank distribution given by prob rank yi j rank xi k we exploit property of the rank distribution towards developing a locality sensitive hash family that ha excellent collision rate property for the cosine measure the computational cost of the basic algorithm is high for high hash length we introduce several approximation based on the property of concomitant order statistic and discrete transforms that perform almost a well with significantly reduced computational cost we demonstrate the practical applicability of our algorithm by using it for finding similar image in an image repository 
randomized neural network are immortalized in this well known ai koan in the day when sussman wa a novice minsky once came to him a he sat hacking at the pdp what are you doing asked minsky i am training a randomly wired neural net to play tic tac toe sussman replied why is the net wired randomly asked minsky sussman replied i do not want it to have any preconception of how to play minsky then shut his eye why do you close your eye sussman asked his teacher so that the room will be empty replied minsky at that moment sussman wa enlightened we analyze shallow random network with the help of concentration of measure inequality specifically we consider architecture that compute a weighted sum of their input after passing them through a bank of arbitrary randomized nonlinearities we identify condition under which these network exhibit good classification performance and bound their test error in term of the size of the dataset and the number of random nonlinearities 
we show how nonlinear embedding algorithm popular for use with shallow semi supervised learning technique such a kernel method can be applied to deep multilayer architecture either a a regularizer at the output layer or on each layer of the architecture this provides a simple alternative to existing approach to deep learning whilst yielding competitive error rate compared to those method and existing shallow semi supervised technique 
the euclidean minimum spanning tree problem ha application in a wide range of field and many efficient algorithm have been developed to solve it we present a new fast general emst algorithm motivated by the clustering and analysis of astronomical data large scale astronomical survey including the sloan digital sky survey and large simulation of the early universe such a the millennium simulation can contain million of point and fill terabyte of storage traditional emst method scale quadratically and more advanced method lack rigorous runtime guarantee we present a new dual tree algorithm for efficiently computing the emst use adaptive algorithm analysis to prove the tightest and possibly optimal runtime bound for the emst problem to date and demonstrate the scalability of our method on astronomical data set 
we present a discriminative part based approach for human action recognition from video sequence using motion feature our model is based on the recently proposed hidden conditional random field hcrf for object recognition similar to hcrf for object recognition we model a human action by a flexible constellation of part conditioned on image observation different from object recognition our model combine both large scale global feature and local patch feature to distinguish various action our experimental result show that our model is comparable to other state of the art approach in action recognition in particular our experimental result demonstrate that combining large scale global feature and local patch feature performs significantly better than directly applying hcrf on local patch alone 
we develop a nd order optimization method based on the hessian free approach and apply it to training deep auto encoders without using pre training we obtain result superior to those reported by hinton salakhutdinov on the same task they considered our method is practical easy to use scale nicely to very large datasets and isn t limited in applicability to autoencoders or any specific model class we also discus the issue of pathological curvature a a possible explanation for the difficulty of deeplearning and how nd order optimization and our method in particular effectively deal with it 
abstract many machine learning algorithm can be formulated in the framework of statistical independence such a the hilbert schmidt independence criterion in this paper we extend this criterion to deal with structured and interdependent observation this is achieved by modeling the structure using undirected graphical model and comparing the hilbert space embeddings of distribution we apply this new criterion to independent component analysis and sequence clustering 
spontaneous brain activity a observed in functional neuroimaging ha been shown to display reproducible structure that express brain architecture and carry marker of brain pathology an important view of modern neuroscience is that such large scale structure of coherent activity reflects modularity property of brain connectivity graph however to date there ha been no demonstration that the limited and noisy data available in spontaneous activity observation could be used to learn full brain probabilistic model that generalize to new data learning such model entail two main challenge i modeling full brain connectivity is a difficult estimation problem that face the curse of dimensionality and ii variability between subject coupled with the variability of functional signal between experimental run make the use of multiple datasets challenging we describe subject level brain functional connectivity structure a a multivariate gaussian process and introduce a new strategy to estimate it from group data by imposing a common structure on the graphical model in the population we show that individual model learned from functional magnetic resonance imaging fmri data using this population prior generalize better to unseen data than model based on alternative regularization scheme to our knowledge this is the first report of a cross validated model of spontaneous brain activity finally we use the estimated graphical model to explore the large scale characteristic of functional architecture and show for the first time that known cognitive network appear a the integrated community of functional connectivity graph 
in this paper we propose a novel algorithm for multi task learning with boosted decision tree we learn several different learning task with a joint model explicitly addressing the specific of each learning task with task specific parameter and the commonality between them through shared parameter this enables implicit data sharing and regularization we evaluate our learning method on web search ranking data set from several country here multitask learning is particularly helpful a data set from different country vary largely in size because of the cost of editorial judgment our experiment validate that learning various task jointly can lead to significant improvement in performance with surprising reliability 
in this paper we extend the hilbert space embedding approach to handle conditional distribution we derive a kernel estimate for the conditional embedding and show it connection to ordinary embeddings conditional embeddings largely extend our ability to manipulate distribution in hilbert space and a an example we derive a nonparametric method for modeling dynamical system where the belief state of the system is maintained a a conditional embedding our method is very general in term of both the domain and the type of distribution that it can handle and we demonstrate the effectiveness of our method in various dynamical system we expect that conditional embeddings will have wider application beyond modeling dynamical system 
matching functional brain region across individual is a challenging task largely due to the variability in their location and extent it is particularly difficult but highly relevant for patient with pathology such a brain tumor which can cause substantial reorganization of functional system in such case spatial registration based on anatomical data is only of limited value if the goal is to establish correspondence of functional area among different individual or to localize potentially displaced active region rather than rely on spatial alignment we propose to perform registration in an alternative space whose geometry is governed by the functional interaction pattern in the brain we first embed each brain into a functional map that reflects connectivity pattern during a fmri experiment the resulting functional map are then registered and the obtained correspondence are propagated back to the two brain in application to a language fmri experiment our preliminary result suggest that the proposed method yield improved functional correspondence across subject this advantage is pronounced for subject with tumor that affect the language area and thus cause spatial reorganization of the functional region 
we study the problem of uncertainty in the entry of the kernel matrix arising in svm formulation using chance constraint programming and a novel large deviation inequality we derive a formulation which is robust to such noise the resulting formulation applies when the noise is gaussian or ha finite support the formulation in general is non convex but in several case of interest it reduces to a convex program the problem of uncertainty in kernel matrix is motivated from the real world problem of classifying protein when the structure are provided with some uncertainty the formulation derived here naturally incorporates such uncertainty in a principled manner leading to significant improvement over the state of the art 
we present a general bayesian framework for hyperparameter tuning in l regularized supervised learning model paradoxically our algorithm work by first analytically integrating out the hyperparameters from the model we find a local optimum of the resulting non convex optimization problem efficiently using a majorization minimization mm algorithm in which the non convex problem is reduced to a series of convex l regularized parameter estimation task the principal appeal of our method is it simplicity the update for choosing the l regularized subproblems in each step are trivial to implement or even perform by hand and each subproblem can be efficiently solved by adapting existing solver empirical result on a variety of supervised learning model show that our algorithm is competitive with both grid search and gradient based algorithm but is more efficient and far easier to implement 
current technique for cyclone detection and tracking employ ncep national center for environmental prediction model from in situ measurement this solution doe not provide true global coverage unlike remote satellite observation however it is impractical to use a single earth orbiting satellite to detect and track event such a cyclone in a continuous manner due to limited spatial and temporal coverage one solution to alleviate such persistent problem is to utilize heterogeneous sensor data from multiple orbiting satellite however this solution requires overcoming other new challenge such a varying spatial and temporal resolution between satellite sensor data the need to establish correspondence between feature from different satellite sensor and the lack of definitive indicator for cyclone event in some sensor data we describe an automated cyclone discovery and tracking approach using heterogeneous near real time sensor data from multiple satellite this approach address the unique challenge associated with knowledge discovery and mining from heterogeneous satellite data stream we consider two remote sensor measurement in our current implementation namely quikscat wind satellite measurement and merged precipitation data from trmm and other satellite more satellite will be incorporated in the near future and our solution is sufficiently powerful that it generalizes to multiple sensor measurement modality our approach consists of three main component i feature extraction from each sensor measurement ii an ensemble classifier for cyclone discovery and iii knowledge sharing between the different remote sensor measurement based on a linear kalman filter for predictive cyclone tracking experimental result on historical hurricane datasets demonstrate the superior performance of our approach compared to previous work 
linear classifier have been shown to be effective for many discrimination task irrespective of the learning algorithm itself the final classifier ha a weight to multiply by each feature this suggests that ideally each input feature should be linearly correlated with the target variable or anti correlated whereas raw feature may be highly non linear in this paper we attempt to re shape each input feature so that it is appropriate to use with a linear weight and to scale the different feature in proportion to their predictive value we demonstrate that this pre processing is beneficial for linear svm classifier on a large benchmark of text classification task a well a uci datasets 
the success of popular algorithm such a k mean clustering or nearest neighbor search depend on the assumption that the underlying distance function reflect domain specific notion of similarity for the problem at hand the distance metric learning problem seek to optimize a distance function subject to constraint that arise from fully supervised or semisupervised information several recent algorithm have been proposed to learn such distance function in low dimensional setting one major shortcoming of these method is their failure to scale to high dimensional problem that are becoming increasingly ubiquitous in modern data mining application in this paper we present metric learning algorithm that scale linearly with dimensionality permitting efficient optimization storage and evaluation of the learned metric this is achieved through our main technical contribution which provides a framework based on the log determinant matrix divergence which enables efficient optimization of structured low parameter mahalanobis distance experimentally we evaluate our method across a variety of high dimensional domain including text statistical software analysis and collaborative filtering showing that our method scale to data set with ten of thousand or more feature we show that our learned metric can achieve excellent quality with respect to various criterion for example in the context of metric learning for nearest neighbor classification we show that our method achieve higher accuracy over the baseline distance additionally our method yield very good precision while providing recall measure up to higher than other baseline method such a latent semantic analysis 
tracking new topic idea and meme across the web ha been an issue of considerable interest recent work ha developed method for tracking topic shift over long time scale a well a abrupt spike in the appearance of particular named entity however these approach are le well suited to the identification of content that spread widely and then fade over time scale on the order of day the time scale at which we perceive news and event we develop a framework for tracking short distinctive phrase that travel relatively intact through on line text developing scalable algorithm for clustering textual variant of such phrase we identify a broad class of meme that exhibit wide spread and rich variation on a daily basis a our principal domain of study we show how such a meme tracking approach can provide a coherent representation of the news cycle the daily rhythm in the news medium that have long been the subject of qualitative interpretation but have never been captured accurately enough to permit actual quantitative analysis we tracked million mainstream medium site and blog over a period of three month with the total of million article and we find a set of novel and persistent temporal pattern in the news cycle in particular we observe a typical lag of hour between the peak of attention to a phrase in the news medium and in blog respectively with divergent behavior around the overall peak and a heartbeat like pattern in the handoff between news and blog we also develop and analyze a mathematical model for the kind of temporal variation that the system exhibit 
influence maximization defined by kempe kleinberg and tardos is the problem of finding a small set of seed node in a social network that maximizes the spread of influence under certain influence cascade model the scalability of influence maximization is a key factor for enabling prevalent viral marketing in large scale online social network prior solution such a the greedy algorithm of kempe et al and it improvement are slow and not scalable while other heuristic algorithm do not provide consistently good performance on influence spread in this paper we design a new heuristic algorithm that is easily scalable to million of node and edge in our experiment our algorithm ha a simple tunable parameter for user to control the balance between the running time and the influence spread of the algorithm our result from extensive simulation on several real world and synthetic network demonstrate that our algorithm is currently the best scalable solution to the influence maximization problem a our algorithm scale beyond million sized graph where the greedy algorithm becomes infeasible and b in all size range our algorithm performs consistently well in influence spread it is always among the best algorithm and in most case it significantly outperforms all other scalable heuristic to a much a increase in influence spread 
this paper study web object classification problem with the novel exploration of social tag automatically classifying web object into manageable semantic category ha long been a fundamental preprocess for indexing browsing searching and mining these object the explosive growth of heterogeneous web object especially non textual object such a product picture and video ha made the problem of web classification increasingly challenging such object often suffer from a lack of easy extractable feature with semantic information interconnection between each other a well a training example with category label in this paper we explore the social tagging data to bridge this gap we cast web object classification problem a an optimization problem on a graph of object and tag we then propose an efficient algorithm which not only utilizes social tag a enriched semantic feature for the object but also infers the category of unlabeled object from both homogeneous and heterogeneous labeled object through the implicit connection of social tag experiment result show that the exploration of social tag effectively boost web object classification our algorithm significantly outperforms the state of the art of general classification method 
we study the problem of building the classification model for a target class in the absence of any labeled training example for that class to address this difficult learning problem we extend the idea of transfer learning by assuming that the following side information is available i a collection of labeled example belonging to other class in the problem domain called the auxiliary class ii the class information including the prior of the target class and the correlation between the target class and the auxiliary class our goal is to construct the classification model for the target class by leveraging the above data and information we refer to this learning problem a unsupervised transfer classification our framework is based on the generalized maximum entropy model that is effective in transferring the label information of the auxiliary class to the target class a theoretical analysis show that under certain assumption the classification model obtained by the proposed approach converges to the optimal model when it is learned from the labeled example for the target class empirical study on text categorization over four different data set verifies the effectiveness of the proposed approach 
we present a reinforcement learning architecture dyna that encompasses both sample based learning and sample based search and that generalises across state during both learning and search we apply dyna to high performance computer go in this domain the most successful planning method are based on sample based search algorithm such a uct in which state are treated individually and the most successful learning method are based on temporal difference learning algorithm such a sarsa in which linear function approximation is used in both case an estimate of the value function is formed but in the first case it is transient computed and then discarded after each move whereas in the second case it is more permanent slowly accumulating over many move and game the idea of dyna is for the transient planning memory and the permanent learning memory to remain separate but for both to be based on linear function approximation and both to be updated by sarsa to apply dyna to x computer go we use a million binary feature in the function approximator based on template matching small fragment of the board using only the transient memory dyna performed at least a well a uct using both memory combined it significantly outperformed uct our program based on dyna achieved a higher rating on the computer go online server than any handcrafted or traditional search based program 
we propose a novel bayesian multiple instance learning mil algorithm this algorithm automatically identifies the relevant feature subset and utilizes inductive transfer when learning multiple conceptually related classifier experimental result indicate that the proposed mil method is more accurate than previous mil algorithm and selects a much smaller set of useful feature inductive transfer further improves the accuracy of the classifier a compared to learning each task individually 
the minimum description length mdl principle selects the model that ha the shortest code for data plus model we show that for a countable class of model mdl prediction are close to the true distribution in a strong sense the result is completely general no independence ergodicity stationarity identifiability or other assumption on the model class need to be made more formally we show that for any countable class of model the distribution selected by mdl or map asymptotically predict merge with the true measure in the class in total variation distance implication for non i i d domain like time series forecasting discriminative learning and reinforcement learning are discussed 
markov logic network mlns combine logic and probability by attaching weight to first order clause and viewing these a template for feature of markov network learning mln structure from a relational database involves learning the clause and weight the state of the art mln structure learner all involve some element of greedily generating candidate clause and are susceptible to local optimum to address this problem we present an approach that directly utilizes the data in constructing candidate a relational database can be viewed a a hypergraph with constant a node and relation a hyperedges we find path of true ground atom in the hypergraph that are connected via their argument to make this tractable there are exponentially many path in the hypergraph we lift the hypergraph by jointly clustering the constant to form higherlevel concept and find path in it we variabilize the ground atom in each path and use them to form clause which are evaluated using a pseudo likelihood measure in our experiment on three real world datasets we find that our algorithm outperforms the state of the art approach 
we present a unified framework for learning link prediction and edge weight prediction function in large network based on the transformation of a graph s algebraic spectrum our approach generalizes several graph kernel and dimensionality reduction method and provides a method to estimate their parameter efficiently we show how the parameter of these prediction function can be learned by reducing the problem to a one dimensional regression problem whose runtime only depends on the method s reduced rank and that can be inspected visually we derive variant that apply to undirected weighted unweighted unipartite and bipartite graph we evaluate our method experimentally using example from social network collaborative filtering trust network citation network authorship graph and hyperlink network 
in this paper we propose a new probabilistic generative model called topic perspective model for simulating the generation process of social annotation different from other generative model in our model the tag generation process is separated from the content term generation process while content term are only generated from resource topic social tag are generated by resource topic and user perspective together the proposed probabilistic model can produce more useful information than any other model proposed before the parameter learned from this model include the topical distribution of each document the perspective distribution of each user the word distribution of each topic the tag distribution of each topic the tag distribution of each user perspective and the probabilistic of each tag being generated from resource topic or user perspective experimental result show that the proposed model ha better generalization performance or tag prediction ability than other two model proposed in previous research 
privacy preserving data mining ppdm is an emergent research area that address the incorporation of privacy preserving concern to data mining technique in this paper we propose a privacy preserving pp cox model for survival analysis and consider a real clinical setting where the data is horizontally distributed among different institution the proposed model is based on linearly projecting the data to a lower dimensional space through an optimal mapping obtained by solving a linear programming problem our approach differs from the commonly used random projection approach since it instead find a projection that is optimal at preserving the property of the data that are important for the specific problem at hand since our proposed approach produce an sparse mapping it also generates a pp mapping that not only project the data to a lower dimensional space but it also depends on a smaller subset of the original feature it provides explicit feature selection real data from several european healthcare institution are used to test our model for survival prediction of non small cell lung cancer patient these result are also confirmed using publicly available benchmark datasets our experimental result show that we are able to achieve a near optimal performance without directly sharing the data across different data source this model make it possible to conduct large scale multi centric survival analysis without violating privacy preserving requirement 
mining user preference play a critical role in many important application such a customer relationship management crm product and service recommendation and marketing campaign in this paper we identify an interesting and practical problem of mining user preference in a multidimensional space where the user preference on some categorical attribute are unknown from some superior and inferior example provided by a user can we learn about the user s preference on those categorical attribute we model the problem systematically and show that mining user preference from superior and inferior example is challenging although the problem ha great potential in practice to the best of our knowledge it ha not been explored systematically before a the first attempt to tackle the problem we propose a greedy method and show that our method is practical using real data set and synthetic data set 
for implementing content management solution and enabling new application associated with data retention regulatory compliance and litigation issue enterprise need to develop advanced analytics to uncover relationship among the document e g content similarity provenance and clustering in this paper we evaluate the performance of four syntactic similarity algorithm three algorithm are based on broder s shingling technique while the fourth algorithm employ a more recent approach content based chunking for our experiment we use a specially designed corpus of document that includes a set of similar document with a controlled number of modification our performance study reveals that the similarity metric of all four algorithm is highly sensitive to setting of the algorithm parameter sliding window size and fingerprint sampling frequency we identify a useful range of these parameter for achieving good practical result and compare the performance of the four algorithm in a controlled environment we validate our result by applying these algorithm to finding near duplicate in two large collection of hp technical support document 
in data publishing anonymization technique such a generalization and bucketization have been designed to provide privacy protection in the meanwhile they reduce the utility of the data it is important to consider the tradeoff between privacy and utility in a paper that appeared in kdd brickell and shmatikov proposed an evaluation methodology by comparing privacy gain with utility gain resulted from anonymizing the data and concluded that even modest privacy gain require almost complete destruction of the data mining utility this conclusion seems to undermine existing work on data anonymization in this paper we analyze the fundamental characteristic of privacy and utility and show that it is inappropriate to directly compare privacy with utility we then observe that the privacy utility tradeoff in data publishing is similar to the risk return tradeoff in financial investment and propose an integrated framework for considering privacy utility tradeoff borrowing concept from the modern portfolio theory for financial investment finally we evaluate our methodology on the adult dataset from the uci machine learning repository our result clarify several common misconception about data utility and provide data publisher useful guideline on choosing the right tradeoff between privacy and utility 
the aim of data mining is to find novel and actionable insight in data however most algorithm typically just find a single possibly non novel actionable interpretation of the data even though alternative could exist the problem of finding an alternative to a given original clustering ha received little attention in the literature current technique including our previous work are unfocused unrefined in that they broadly attempt to find an alternative clustering but do not specify which property of the original clustering should or should not be retained in this work we explore a principled and flexible framework in order to find alternative clustering of the data the approach is principled since it pose a constrained optimization problem so it exact behavior is understood it is flexible since the user can formally specify positive and negative feedback based on the existing clustering which range from which cluster to keep or not to making a trade off between alternativeness and clustering quality 
the innite factorial hidden markov model is a non parametric extension of the factorial hidden markov model our model denes a probability distribution over an innite number of independent binary hidden markov chain which together produce an observable sequence of random variable central to our model is a new type of non parametric prior distribution inspired by the indian buffet process which we call the indian buet markov process 
we describe an algorithm for learning in the presence of multiple criterion our technique generalizes previous approach in that it can learn optimal policy for all linear preference assignment over the multiple reward criterion at once the algorithm can be viewed a an extension to standard reinforcement learning for mdps where instead of repeatedly backing up maximal expected reward we back up the set of expected reward that are maximal for some set of linear preference given by a weight vector w we present the algorithm along with a proof of correctness showing that our solution give the optimal policy for any linear preference function the solution reduces to the standard value iteration algorithm for a specific weight vector w 
we propose a nonparametric bayesian factor regression model that account for uncertainty in the number of factor and the relationship b etween factor to accomplish this we propose a sparse variant of the indian buffet process and couple this with a hierarchical model over factor based on kingman s coalescent we apply this model to two problem factor analysis and factor regression in gene expression data analysis 
from an information theoretic perspective a noisy transmission system such a a visual brain computer interface bci speller could benefi t from the use of errorcorrecting code however optimizing the code solely according to the maximal minimum hamming distance criterion tends to lead to an overall increase in target frequency of target stimulus and hence a significan tly reduced average target to target interval tti leading to difficulty i n classifying the individual event related potential erps due to overlap and refractory effect clearly any change to the stimulus setup must also respect the possible psychophysiological consequence here we report new eeg data from experiment in which we explore stimulus type and codebooks in a within subject design finding an interaction between the two factor our data demonstrate that the traditional rowcolumn code ha particular spatial property that lead to b etter performance than one would expect from it ttis and hamming distance alone but nonetheless error correcting code can improve performance provided the right stimulus type is used 
many nonlinear dynamical phenomenon can be effectively modeled by a system that switch among a set of conditionally linear dynamical mode we consider two such model the switching linear dynamical system slds and the switching vector autoregressive var process our nonparametric bayesian approach utilizes a hierarchical dirichlet process prior to l earn an unknown number of persistent smooth dynamical mode we develop a sampling algorithm that combine a truncated approximation to the dirichlet process with efficient joint sampling of the mode and state sequence the utility and flex ibility of our model are demonstrated on synthetic data sequence of dancing honey bee and the ibovespa stock index 
in kernel based regression learning optimizing each kernel individually is useful when the data density curvature of regression surface or decision boundary or magnitude of output noise varies spatially previous work ha suggested gradient descent technique or complex statistical hypothesis method for local kernel shaping typically requiring some amount of manual tuning of meta parameter we introduce a bayesian formulation of nonparametric regression that with the help of variational approximation result in an em like algorithm for simultaneous estimation of regression and kernel parameter the algorithm is computationally efficient requires no sampling automatically re jects outlier and ha only one prior to be specified it can be used for nonparametric reg ression with local polynomial or a a novel method to achieve nonstationary regression with gaussian process our method are particularly useful for lea rning control where reliable estimation of local tangent plane is essential fo r adaptive controller and reinforcement learning we evaluate our method on several synthetic data set and on an actual robot which learns a task level control law 
detecting outlier in a large set of data object is a major data mining task aiming at finding different mechanism responsible for different group of object in a data set all existing approach however are based on an assessment of distance sometimes indirectly by assuming certain distribution in the full dimensional euclidean data space in high dimensional data these approach are bound to deteriorate due to the notorious curse of dimensionality in this paper we propose a novel approach named abod angle based outlier detection and some variant assessing the variance in the angle between the difference vector of a point to the other point this way the effect of the curse of dimensionality are alleviated compared to purely distance based approach a main advantage of our new approach is that our method doe not rely on any parameter selection influencing the quality of the achieved ranking in a thorough experimental evaluation we compare abod to the well established distance based method lof for various artificial and a real world data set and show abod to perform especially well on high dimensional data 
before the age of month infant make inductive inference about the motion of physical object developmental psychologist have provided verbal account of the knowledge that support these inference but often these account focus on categorical rather than probabilistic principle we propose that infant object perception is guided in part by probabilistic principle like persistence thing tend to remain the same and when they change they do so gradually to illustrate this idea we develop an ideal observer model that incorporates probabilistic principle of rigidity and inertia like previous researcher we suggest that rigid motion are expected from an early age but we challenge the previous claim that the inertia principle is relatively slow to develop we support these argument by modeling several experiment from the developmental literature 
motivated by the insufficiency of the existing quasi identifier sensitive attribute qi sa framework on modeling real world privacy requirement for data publishing we propose a novel versatile publishing scheme with which privacy requirement can be specified a an arbitrary set of privacy rule over attribute in the microdata table to enable versatile publishing we introduce the guardian normal form gnf a novel method of publishing multiple sub table such that each sub table is anonymized by an existing qi sa publishing algorithm while the combination of all published table guarantee all privacy rule we devise two algorithm guardian decomposition gd and utility aware decomposition uad for decomposing a microdata table into gnf and present extensive experiment over real world datasets to demonstrate the effectiveness of both algorithm 
point process are difficult to analyze because they provide only a sparse and noisy observation of the intensity function driving the process gaussian process offer an attractive framework within which to infer underlying intensity function the result of this inference is a continuous function defined across time that is typically more amenable to analytical effort however a naive implementation will become computationally infeasible in any problem of reasonable size both in memory and run time requirement we demonstrate problem specific method for a class of renewal process that eliminate the memory burden and reduce the solve time by order of magnitude 
the kernel perceptron is an appealing online learning algorithm that ha a drawback whenever it make an error it must increase it support set which slows training and testing if the number of error is large the forgetron and the randomized budget perceptron algorithm overcome this problem by restricting the number of support vector the perceptron is allowed to have these algorithm have regret bound whose proof are dissimilar in this paper we propose a unified analysis of both of these algorithm by observing that the way in which they remove support vector can be seen a type of l regularization by casting these algorithm a instance of online convex optimization problem and applying a variant of zinkevich s theorem for noisy and incorrect gradient we can bound the regret of these algorithm more easily than before our bound are similar to the existing one but the proof are le technical 
in the past few year there ha been an increasing interest in the analysis of process log several proposed technique such a workflow mining are aimed at automatically deriving the underlying workflow model however current approach only pay little attention on an important piece of information contained in process log the timestamps which are used to define a sequential ordering of the performed task in this work we try to overcome these limitation by explicitly including time in the extracted knowledge thus making the temporal information a first class citizen of the analysis process this make it possible to discern between apparently identical process execution that are performed with different transition time between consecutive task this paper proposes a framework for the user interactive exploration of a condensed representation of group of execution of a given process the framework is based on the use of an existing mining paradigm temporally annotated sequence ta these are aimed at extracting sequential pattern where each transition between two event is annotated with a typical transition time that emerges from input data with the extracted ta which represent set of possible frequent execution with their typical transition time a few factorizing operator are built these operator condense such execution according to possible parallel or possible mutual exclusive execution lastly such condensed representation is rendered to the user via the exploration graph namely the temporally annotated graph tag the user the domain expert is allowed to explore the different and alternative factorization corresponding to different interpretation of the actual execution according to the user choice the system discard or retains certain hypothesis on actual execution and show the consequent scenario resulting from the coresponding re aggregation of the actual data 
we present cutoff averaging a technique for converting any conservative online learning algorithm into a batch learning algorithm most online to batch conversion technique work well with certain type of online learning algorithm and not with others whereas cutoff averaging explicitly try to a dapt to the characteristic of the online algorithm being converted an attractive property of our technique is that it preserve the efficiency of the original online alg orithm making it appropriate for large scale learning problem we provide a statistical analysis of our technique and back our theoretical claim with experimental result 
the vast majority of earlier work ha focused on graph which are both connected typically by ignoring all but the giant connected component and unweighted here we study numerous real weighted graph and report surprising discovery on the way in which new node join and form link in a social network the motivating question were the following how do connected component in a graph form and change over time what happens after new node join a networkhow common are repeated edge we study numerous diverse real graph citation network network in social medium internet traffic and others and make the following contribution a we observe that the non giant connected component seem to stabilize in size b we observe the weight on the edge follow several power law with surprising exponent and c we propose an intuitive generative model for graph growth that obeys observed pattern category and subject descriptor i computing methodology simulation and modeling model validation and analysis i pattern recognition miscellaneous 
we address the problem of learning structured unsupervised model with moment sparsity typical in many natural language induction task for example in unsupervised part of speech po induction using hidden markov model we introduce a bias for word to be labeled by a small number of tag in order to express this bias of posterior sparsity a opposed to parametric sparsity we extend the posterior regularization framework we evaluate our method on three language english bulgarian and portuguese showing consistent and significant accuracy improvement over em trained hmms and hmms with sparsity inducing dirichlet prior trained by variational em we increase accuracy with respect to em by in a purely unsupervised setting a well a in a weaklysupervised setting where the closed class word are provided finally we show improvement using our method when using the induced cluster a feature of a discriminative model in a semi supervised setting 
we apply robust bayesian decision theory to improve both generative and discriminative learner under bias in class proportion in labeled training data when the true class proportion are unknown for the generative case we derive an entropybased weighting that maximizes expected log likelihood under the worst case true class proportion for the discriminative case we derive a multinomial logistic model that minimizes worst case conditional log loss we apply our theory to the modeling of specie geographic distribution from presence data an extreme case of labeling bias since there is no absence data on a benchmark dataset we find that entropy based weighting offer an improvement over constant estimate of class proportion consistently reducing log loss on unbia sed test data 
finding maximally sparse representation from overcomplete feature dictionary frequently involves minimizing a cost function composed of a likelihood or data t term and a prior or penalty function that favor sparsity while typically the prior is factorial here we examine non factorial alternative that have a number of desirable property relevant to sparse estimation and are easily implemented using an efcient and globally convergent reweighted norm minimization procedure the rst method under consideration arises from the sparse bayesian learning sbl framework although based on a highly non convex underlying cost function in the context of canonical sparse estimation problem we prove uniform superiority of this method over the lasso in that i it can never do worse and ii for any dictionary and sparsity prole there will always exist case where it doe better these result challenge the prevailing reliance on strictly convex penalty function for nding sparse solution we then derive a new non factorial variant with similar property that exhibit further performance improvement in some empirical test for both of these method a well a traditional factorial analog we demonstrate the effectiveness of reweighted norm algorithm in handling more general sparse estimation problem involving classication group feature selection and non negativity constraint a a byproduct of this development a rigorous reformulation of sparse bayesian classication e g the relevance vector machine is derived that unlike the original involves no approximation step and descends a well dened objective function 
stationarity is often an unrealistic prior assumption for gaussian process regression one solution is to predefine an explicit nonstationary covariance function but such covariance function can be difficult to specify and require detailed prior knowledge of the nonstationarity we propose the gaussian process product model gppm which model data a the pointwise product of two latent gaussian process to nonparametrically infer nonstationary variation of amplitude this approach differs from other nonparametric approach to covariance function inference in that it operates on the output rather than the input resulting in a significant reduction in computational cost and required data for inference we present an approximate inference scheme using expectation propagation this variational approximation yield convenient gp hyperparameter selection and compact approximate predictive distribution 
we present a principled bayesian framework for modeling partial membership of data point to cluster unlike a standard mixture model which assumes that each data point belongs to one and only one mixture component or cluster a partial membership model allows data point to have fractional membership in multiple cluster algorithm which assign data point partial membership to cluster can be useful for task such a clustering gene based on microarray data gasch eisen our bayesian partial membership model bpm us exponential family distribution to model each cluster and a product of these distibtutions with weighted parameter to model each datapoint here the weight correspond to the degree to which the datapoint belongs to each cluster all parameter in the bpm are continuous so we can use hybrid monte carlo to perform inference and learning we discus relationship between the bpm and latent dirichlet allocation mixed membership model exponential family pca and fuzzy clustering lastly we show some experimental result and discus nonparametric extension to our model 
most existing sparse gaussian process g p model seek computational advantage by basing their computation on a set of m basis function that are the covariance function of the g p with one of it two input fixed we generalise this for the case of gaussian covariance function by basing our computation on m gaussian basis function with arbitrary diagonal covariance matrix or length scale for a fixed number of basis function and any given criterion this additional flexibility permit approximation no worse and typically better than wa previously possible we perform gradient based optimisation of the marginal likelihood which cost o m n time where n is the number of data point and compare the method to various other sparse g p method although we focus on g p regression the central idea is applicable to all kernel based algorithm and we also provide some result for the support vector machine s v m and kernel ridge regression k r r our approach outperforms the other method particularly for the case of very few basis function i e a very high sparsity ratio 
we propose a multiple source domain adaptation method referred to a domain adaptation machine dam to learn a robust decision function referred to a target classifier for label prediction of pattern from the target domain by leveraging a set of pre computed classifier referred to a auxiliary source classifier independently learned with the labeled pattern from multiple source domain we introduce a new data dependent regularizer based on smoothness assumption into least square svm l svm which enforces that the target classifier share similar decision value with the auxiliary classifier from relevant source domain on the unlabeled pattern of the target domain in addition we employ a sparsity regularizer to learn a sparse target classifier comprehensive experiment on the challenging trecvid corpus demonstrate that dam outperforms the existing multiple source domain adaptation method for video concept detection in term of effectiveness and efficiency 
account of how people learn functional relationship between continuous variable have tended to focus on two possibility that people are estimating explicit function or that they are performing associative learning supported by similarity we provide a rational analysis of function learning drawing on work on regression in machine learning and statistic using the equivalence of bayesian linear regression and gaussian process we show that learning explicit rule and using similarity can be seen a two view of one solution to this problem we use this insight to define a gaussian process model of human function learning that combine the strength of both approach 
this paper explores an important and relatively unstudied quality measure of a sponsored search advertisement bounce rate the bounce rate of an ad can be informally defined a the fraction of user who click on the ad but almost immediately move on to other task a high bounce rate can lead to poor advertiser return on investment and suggests search engine user may be having a poor experience following the click in this paper we first provide quantitative analysis showing that bounce rate is an effective measure of user satisfaction we then address the question can we predict bounce rate by analyzing the feature of the advertisement an affirmative answer would allow advertiser and search engine to predict the effectiveness and quality of advertisement before they are shown we propose solution to this problem involving large scale learning method that leverage feature drawn from ad creatives in addition to their keywords and landing page 
to handle massive data a variety of sparse gaussian process gp method have been proposed to reduce the computational cost many of them essentially map the large dataset into a small set of basis point a common approach to learn these basis point is evidence maximization nevertheless evidence maximization may lead to overfitting and cause a high computational cost in this paper we propose a novel sparse gp regression approach gplasso that explicitly represents the trade off between it approximation quality and the model sparsity gplasso minimizes a penalized kl divergence between the exact and sparse gp posterior process optimizing this convex cost function lead to sparse gp parameter furthermore we use incomplete cholesky factorization to obtain low rank matrix approximation to speed up the optimization procedure experimental result on synthetic and real data demonstrate that compared with several state of the art sparse gp method and a direct low rank matrix approximation method gplasso achieves a significantly improved trade off between prediction accuracy and computational cost 
controlled experiment also called randomized experiment and a b test have had a profound influence on multiple field including medicine agriculture manufacturing and advertising while the theoretical aspect of offline controlled experiment have been well studied and documented the practical aspect of running them in online setting such a web site and service are still being developed a the usage of controlled experiment grows in these online setting it is becoming more important to understand the opportunity and pitfall one might face when using them in practice a survey of online controlled experiment and lesson learned were previously documented in controlled experiment on the web survey and practical guide kohavi et al in this follow on paper we focus on pitfall we have seen after running numerous experiment at microsoft the pitfall include a wide range of topic such a assuming that common statistical formula used to calculate standard deviation and statistical power can be applied and ignoring robot in analysis a problem unique to online setting online experiment allow for technique like gradual ramp up of treatment to avoid the possibility of exposing many customer to a bad e g buggy treatment with that ability we discovered that it s easy to incorrectly identify the winning treatment because of simpson s paradox 
spectral clustering is useful for a wide ranging set of appl ications in area such a biological data analysis image processing and data mining however the computational and or communication resource required by the method in processing large scale data are often prohibitively high and practit ioners are often required to perturb the original data in various way quantization downsampling etc before invoking a spectral algorithm in this paper we use stochastic perturbation theory to study the effect of data perturbation on the performance of spectral clustering we show that the error under perturbation of spectral clustering is closely related to the perturbation of the eigenvectors of the laplacian matrix from this result we derive approximate upper bound on the clustering error we show that this bound is tight empirically across a wide range of problem suggesting that it can be used in practical setting to determine the amount of data reduction allowed in order to meet a specification of permitted loss in clustering performance 
clustering with advice often known a constrained clustering ha been a recent focus of the data mining community success ha been achieved incorporating advice into the k mean and spectral clustering framework although the theory community ha explored inconsistent advice it ha not yet been incorporated into spectral clustering extending work of de bie and cristianini we set out a framework for finding minimum normalised cut subject to inconsistent advice 
we present a characterization of a useful class of skill based on a graphical representation of an agent s interaction with it environment our characterization us betweenness a measure of centrality on graph it capture and generalizes at least intuitively the bottleneck concept which ha inspired many of the existing skill discovery algorithm our characterization may be used directly to form a set of skill suitable for a given task more importantly it serf a a useful guide for developing incremental skill discovery algorithm that do not rely on knowing or representing the interaction graph in it entirety 
we present the rst temporal dierenc e learning algorithm for o policy control with unrestricted linear function approximation whose per time step complexity is linear in the number of feature our algorithm greedy gq is an extension of recent work on gradient temporal dierenc e learning which 
this paper us the notion of algorithmic stability to derive novel generalization bound for several family of transductive regression algorithm both by using convexity and closed form solution our analysis help compare the stability of these algorithm it suggests that several existing algorithm might not be stable but prescribes a technique to make them stable it also report the result of experiment with local transductive regression demonstrating the benefit of our stability bound for model selection in particular for determining the radius of the local neighborhood used by the algorithm 
abstract large network of spiking neuron show abrupt change in their collective dynamic resembling phase transition studied in statistical physic an example of this phenomenon is the transition from irregular noise driven dynamic to regular self sustained behavior observed in network of integrate and fire neuron a the interaction strength between the neuron increase in this work we show how a network of spiking neuron is able to self organize towards a critical state for which the range of possible inter spike interval dynamic range is maximized self organization occurs via synaptic dynamic that we analytically derive the resulting plasticity rule is defined locally so that global homeostasis near the critical state is achieved by local regulation of individual synapsis 
in this paper we introduce a novel collapsed gibbs sampling method for the widely used latent dirichlet allocation lda model our new method result in significant speedup on real world text corpus conventional gibbs sampling scheme for lda require o k operation per sample where k is the number of topic in the model our proposed method draw equivalent sample but requires on average significantly le then k operation per sample on real word corpus fastlda can be a much a time faster than the standard collapsed gibbs sampler for lda no approximation are necessary and we show that our fast sampling scheme produce exactly the same result a the standard but slower sampling scheme experiment on four real world data set demonstrate speedup for a wide range of collection size for the pubmed collection of over million document with a required computation time of cpu month for lda our speedup of can save cpu month of computation 
research in visualization often revolves around visualizing information however visualization is a process that extends over time from initial exploration to hypothesis confirmation and even to result presentation it is rare that the final phase of visualization are solely about information in this paper we present a more biased kind of visualization in which there is a message or set of assumption behind the presentation that is of interest to both the presenter and the viewer and emphasizes point that the presenter want to convey to the viewer this kind of persuasive visualization presenting data in a way that emphasizes a point or message is not only common in visualization but also often expected by the viewer persuasive visualization is implicit in the deliberate emphasis on interestingness and also in the deliberate use of graphical element that are processed preattentively by the human visual system which automatically group these element and guiding attention so that they stand out we discus how these idea have been implemented in the morpherspective system for automated generation of information graphic 
user of topic modeling method often have knowledge about the composition of word that should have high or low probability in various topic we incorporate such domain knowledge using a novel dirichlet forest prior in a latent dirichlet allocation framework the prior is a mixture of dirichlet tree distribution with special structure we present it construction and inference via collapsed gibbs sampling experiment on synthetic and real datasets demonstrate our model s ability to follow and generalize beyond user specified domain knowledge 
we propose a method for support vector machine classification using indefinite kernel instead of directly minimizing or stabilizing a nonconvex loss function our algorithm simultaneously computes support vector and a proxy kernel matrix used in forming the loss this can be interpreted a a penalized kernel learning problem where indefinite kernel matrix are treated a a noisy observation of a true mercer kernel our formulation keep the problem convex and relatively large problem can be solved efficiently using theprojected gradient or analytic center cutting plane method we compare the performance of our technique with other method on several classic data set 
large margin structured estimation method work by minimizing a convex upper bound of loss function while they allow for efficient optimization algorithm these convex formulation are nottight and sacrifice the ability to accurately model the true loss we present tighter non convex bound based on generalizing the notion of a ramp loss from binary classification to structured estimation we show that a small modification of existing optimization algorithm suffices to solve this modified problem on structured prediction task such a protein sequence alignment and web page ranking our algorithm lead to improved accuracy 
topic model provide a powerful tool for analyzing large text collection by representing high dimensional data in a low dimensional subspace fitting a topic model given a set of training document requires approximate inference technique that are computationally expensive with today s large scale constantly expanding document collection it is useful to be able to infer topic distribution for new document without retraining the model in this paper we empirically evaluate the performance of several method for topic inference in previously unseen document including method based on gibbs sampling variational inference and a new method inspired by text classification the classification based inference method produce result similar to iterative inference method but requires only a single matrix multiplication in addition to these inference method we present sparselda an algorithm and data structure for evaluating gibbs sampling distribution empirical result indicate that sparselda can be approximately time faster than traditional lda and provide twice the speedup of previously published fast sampling method while also using substantially le memory 
mining concept drifting data stream is a defining challenge for data mining research recent year have seen a large body of work on detecting change and building prediction model from stream data with a vague understanding on the type of the concept drifting and the impact of different type of concept drifting on the mining algorithm in this paper we first categorize concept drifting into two scenario loose concept drifting lcd and rigorous concept drifting rcd and then propose solution to handle each of them separately for lcd data stream because concept in adjacent data chunk are sufficiently close to each other we apply kernel mean matching kmm method to minimize the discrepancy of the data chunk in the kernel space such a minimization process will produce weighted instance to build classifier ensemble and handle concept drifting data stream for rcd data stream because genuine concept in adjacent data chunk may randomly and rapidly change we propose a new optimal weight adjustment owa method to determine the optimum weight value for classifier trained from the most recent up to date data chunk such that those classifier can form an accurate classifier ensemble to predict instance in the yet to come data chunk experiment on synthetic and real world datasets will show that weighted instance approach is preferable when the concept drifting is mainly caused by the changing of the class prior probability whereas the weighted classifier approach is preferable when the concept drifting is mainly triggered by the changing of the conditional probability 
in dimensionality reduction approach the data are typically embedded in a euclidean latent space however for some data set this is inappropriate for example in human motion data we expect latent space that are cylindrical or a toroidal that are poorly captured with a euclidean space in this paper we present a range of approach for embedding data in a non euclidean latent space our focus is the gaussian process latent variable model in the context of human motion modeling this allows u to a learn model with interpretable latent direction enabling for example style content separation and b generalise beyond the data set enabling u to learn transition between motion style even though such transition are not present in the data 
the main objective of transfer in reinforcement learning is to reduce the complexity of learning the solution of a target task by effectively reusing the knowledge retained from solving a set of source task in this paper we introduce a novel algorithm that transfer sample i e tuples s a s r from source to target task under the assumption that task have similar transition model and reward function we propose a method to select sample from the source task that are mostly similar to the target task and then to use them a input for batch reinforcement learning algorithm a a result the number of sample an agent need to collect from the target task to learn it solution is reduced we empirically show that following the proposed approach the transfer of sample is effective in reducing the learning complexity even when some source task are significantly different from the target task 
cognitive control refers to the flexible deployment of memory and attention in response to task demand and current goal control is often studied experimentally by presenting sequence of stimulus some demanding a response and others modulating the stimulus response mapping in these task participant must maintain information about the current stimulus response mapping in working memory prominent theory of cognitive control use recurrent neural net to implement working memory and optimize memory utilization via reinforcement learning we present a novel perspective on cognitive control in which working memory representation are intrinsically probabilistic and control operation that maintain and update working memory are dynamically determined via probabilistic inference we show that our model provides a parsimonious account of behavioral and neuroimaging data and suggest that it offer an elegant conceptualization of control in which behavior can be cast a optimal subject to limitation on learning and the rate of information processing moreover our model provides insight into how task instruction can be directly translated into appropriate behavior and then efficiently refined with subsequent task experience 
the dynamic hierarchical dirichlet process dhdp is developed to model the time evolving statistical property of sequential data set the data collected at any time point are represented via a mixture associated with an appropriate underlying model in the framework of hdp the statistical property of data collected at consecutive time point are linked via a random parameter that control their probabilistic similarity the sharing mechanism of the time evolving data are derived and a relatively simple markov chain monte carlo sampler is developed experimental result are presented to demonstrate the model 
there are billion of image on the internet today searching for a desired image is largely based on textual data such a filename or associated text on the web page not much use is made of the image content there are good reason for this the field of content based image retrieval which emerged during the s focused primarily on color and texture cue these were easier to model than shape but they turned out to be much le useful than originally hoped i shall review some of the recent development in the field of visual object recognition in the computer vision community that offer greater promise much better image feature for characterizing shape advance in machine learning technique and the availability of large amount of training data lie at the heart of these approach 
we present the gaussian process density sampler gpds an exchangeable generative model for use in nonparametric bayesian density estimation sample drawn from the gpds are consistent with exact independent sample from a fixed density function that is a transformation of a function drawn from a gaussian process prior our formulation allows u to infer an unknown density from data using markov chain monte carlo which give sample from the posterior distribution over density function and from the predictive distributio n on data space we can also infer the hyperparameters of the gaussian process we compare this density modeling technique to several existing technique on a toy problem and a skullreconstruction task 
in this study we formalize a multi focal learning problem where training data are partitioned into several different focal group and the prediction model will be learned within each focal group the multi focal learning problem is motivated by numerous real world learning application for instance for the same type of problem encountered in a customer service center the problem description from different customer can be quite different the experienced customer usually give more precise and focused description about the problem in contrast the inexperienced customer usually provide more diverse description in this case the example from the same class in the training data can be naturally in different focal group a a result it is necessary to identify those natural focal group and exploit them for learning at different focus the key developmental challenge is how to identify those focal group in the training data a a case study we exploit multi focal learning for profiling problem in customer service center the result show that multifocal learning can significantly boost the learning accuracy of existing learning algorithm such a support vector machine svms for classifying customer problem 
we consider the problem of estimating rate of rare event for high dimensional multivariate categorical data where several dimension are hierarchical such problem are routine in several data mining application including computational advertising our main focus in this paper we propose lmmh a novel log linear modeling method that scale to massive data application with billion of training record and several million potential predictor in a map reduce framework our method exploit correlation in aggregate observed at multiple resolution when working with multiple hierarchy stable estimate at coarser resolution provide informative prior information to improve estimate at finer resolution other than prediction accuracy and scalability our method ha an inbuilt variable screening procedure based on a spike and slab prior that provides parsimony by removing non informative predictor without hurting predictive accuracy we perform large scale experiment on data from real computational advertising application and illustrate our approach on datasets with several billion record and hundred of million of predictor extensive comparison with other benchmark method show significant improvement in prediction accuracy 
while million of dollar have been invested in information technology to improve intelligence information sharing among law enforcement agency at the federal tribal state and local level there remains a hesitation to share information between agency this lack of coordination hinders the ability to prevent and respond to crime and terrorism work to date ha not produced solution nor widely accepted paradigm for understanding the problem therefore to enhance the current intelligence information sharing service between government entity in this interdisciplinary research we have identified three major area of influence technical social and legal furthermore we have developed a preliminary model and theory of intelligence information sharing through a literature review experience and interview with practitioner in the field this model and theory should serve a a basic conceptual framework for further academic work and lead to further investigation and clarification of the identified factor and the degree of impact they exert on the system so that actionable solution can be identified and implemented 
this paper examines two stage technique for learning kernel based on a notion of alignment it present a number of novel theoretical algorithmic and empirical result for alignmentbased technique our result build on previous work by cristianini et al but we adopt a different definition of kernel alignment and significantly extend that work in several direction we give a novel and simple concentration bound for alignment between kernel matrix show the existence of good predictor for kernel with high alignment both for classification and for regression give algorithm for learning a maximum alignment kernel by showing that the problem can be reduced to a simple qp and report the result of extensive experiment with this alignment based method in classification and regression task which show an improvement both over the uniform combination of kernel and over other state of the art learning kernel method 
similarity matrix generated from many application may not be positive semidefinite and hence can t fit into the kernel machine framework in this paper we study the problem of training support vector machine with an indefinite kernel we consider a regularized svm formulation in which the indefinite kernel matrix is treated a a noisy observation of some unknown positive semidefinite one proxy kernel and the support vector and the proxy kernel can be computed simultaneously we propose a semi infinite quadratically constrained linear program formulation for the optimization which can be solved iteratively to find a global optimum solution we further propose to employ an additional pruning strategy which significantly improves the efficiency of the algorithm while retaining the convergence property of the algorithm in addition we show the close relationship between the proposed formulation and multiple kernel learning experiment on a collection of benchmark data set demonstrate the efficiency and effectiveness of the proposed algorithm 
the central issue in representing graphstructured data instance in learning algorithm is designing feature which are invariant to permuting the numbering of the vertex we present a new system of invariant graph feature which we call the skew spectrum of graph the skew spectrum is based on mapping the adjacency matrix of any weigted directed unlabeled graph to a function on the symmetric group and computing bispectral invariant the reduced form of the skew spectrum is computable in o n time and experiment show that on several benchmark datasets it can outperform state of the art graph kernel 
matching record that refer to the same entity across data base is becoming an increasingly important part of many data mining project a often data from multiple source need to be matched in order to enrich data or improve it quality significant advance in record linkage technique have been made in recent year however many new technique are either implemented in research proof of concept system only or they are hidden within expensive black box commercial software this make it difficult for both researcher and practitioner to experiment with new record linkage technique and to compare existing technique with new one the febrl freely extensible biomedical record linkage system aim to fill this gap it contains many recently developed technique for data cleaning deduplication and record linkage and encapsulates them into a graphical user interface gui febrl thus allows even inexperienced user to learn and experiment with both traditional and new record linkage technique because febrl is written in python and it source code is available it is fairly easy to integrate new record linkage technique into it therefore febrl can be seen a a tool that allows researcher to compare various existing record linkage technique with their own one enabling the record linkage research community to conduct their work more efficiently additionally febrl is suitable a a training tool for new record linkage user and it can also be used for practical linkage project with data set that contain up to several hundred thousand record 
traditionally research in identifying structured entity in document ha proceeded independently of document categorization research in this paper we observe that these two task have much to gain from each other apart from direct reference to entity in a database such a name of person entity document often also contain word that are correlated with discriminative entity attribute such age group and income level of person this happens naturally in many enterprise domain such a crm banking etc then entity identification which is typically vulnerable against noise and incompleteness in direct reference to entity in document can benefit from document categorization with respect to such attribute in return entity identification enables document to be categorized according to different label set arising from entity attribute without requiring any supervision in this paper we propose a probabilistic generative model for joint entity identification and document categorization we show how the parameter of the model can be estimated using an em algorithm in an unsupervised fashion using extensive experiment over real and semi synthetic data we demonstrate that the two task can benefit immensely from each other when performed jointly using the proposed model 
in this paper we present a robust feature extraction framework based on information theoretic learning it formulated objective aim at simultaneously maximizing the renyi s quadratic information potential of feature and the renyi s cross information potential between feature and class label this objective function reaps the advantage in robustness from both redescending m estimator and manifold regularization and can be efficiently optimized via half quadratic optimization in an iterative manner in addition the popular algorithm lpp srda and laprls for feature extraction are all justified to be the special case within this framework extensive comparison experiment on several real world data set with contaminated feature or label well validate the encouraging gain in algorithmic robustness from this proposed framework 
in solving complex visual learning task adopting multiple descriptor to more precisely characterize the data ha been a feasible way for i mproving performance these representation are typically high dimensional and assume diverse form thus finding a way to transform them into a unified space of lowe r dimension generally facilitates the underlying task such a object recognition or clustering we describe an approach that incorporates multiple kernel learning with dimensionality reduction mkl dr while the proposed framework is flexible in simultaneously tackling data in various feature represe ntations the formulation itself is general in that it is established upon graph embedding it follows that any dimensionality reduction technique explainable by graph embedding can be generalized by our method to consider data in multiple feature representation 
this paper present a theoretical framework for ranking and demonstrates how to perform generalization analysis of listwise ranking algorithm using the framework many learning to rank algorithm have been proposed in recent year among them the listwise approach ha shown higher empirical ranking performance when compared to the other approach however there is no theoretical study on the listwise approach a far a we know in this paper we propose a theoretical framework for ranking which can naturally describe various listwise learning to rank algorithm with this framework we prove a theorem which give a generalization bound of a listwise ranking algorithm on the basis of rademacher average of the class of compound function the compound function take listwise loss function a outer function and ranking model a inner function we then compute the rademacher average for existing listwise algorithm of listmle listnet and rankcosine we also discus the tightness of the bound in different situation with regard to the list length and transformation function 
fault tolerant frequent itemsets ftfi are variant of frequent itemsets for representing and discovering generalized knowledge however despite growing interest in this field no previous approach mine proportional ftfis with their exact support ft support this problem is difficult because of two concern a non anti monotonic property of ft support when relaxation is proportional and b difficulty in computing ft support previous effort on this problem either simplify the general problem by adding constraint or provide approximate solution without any error guarantee in this paper we address these concern in the general ftfi mining problem we limit the search space by providing provably correct anti monotone bound for ft support and develop practically efficient mean of achieving them besides we also provide an efficient and exact ft support counting procedure extensive experiment using real datasets validate that our solution is reasonably efficient for completely mining ftfis implementation for the algorithm are available from www cais ntu edu sg vivek pub ftfim 
we propose to combine two approach for modeling data admitting sparse representation on the one hand dictionary learning ha proven effective for various signal processing task on the other hand recent work on structured sparsity provides a natural framework for modeling dependency between dictionary element we thus consider a tree structured sparse regularization to learn dictionary embedded in a hierarchy the involved proximal operator is computable exactly via a primal dual method allowing the use of accelerated gradient technique experiment show that for natural image patch learned dictionary element organize themselves in such a hierarchical structure leading to an improved performance for restoration task when applied to text document our method learns hierarchy of topic thus providing a competitive alternative to probabilistic topic model 
we provide a new analysis of an efficient margin based algori thm for selective sampling in classification problem using the so called t ybakov low noise condition to parametrize the instance distribution we show bound on the convergence rate to the bayes risk of both the fully supervised and the selective sampling version of the basic algorithm our analysis reveals that excluding logarithmic factor the average risk of the selective sampler converge s to the bayes risk at rate n where n denotes the number of queried label and is the exponent in the low noise condition for all this convergence rate is asymptotically faster than the rate n achieved by the fully supervised version of the same classifier which query all label and for the two rate exhibit an exponential gap experiment on textual data reveal that simple variant of the proposed selective sampler perform much better than popular and similarly efficient competitor 
this paper proposes an efficient sparse metric learning algorithm in high dimensional space via an l penalized log determinant regularization compare to the most existing distance metric learning algorithm the proposed algorithm exploit the sparsity nature underlying the intrinsic high dimensional feature space this sparsity prior of learning distance metric serf to regularize the complexity of the distance model especially in the le example number p and high dimension d setting theoretically by analogy to the covariance estimation problem we find the proposed distance learning algorithm ha a consistent result at rate o m log d n to the target distance matrix with at most m nonzeros per row moreover from the implementation perspective this l penalized log determinant formulation can be efficiently optimized in a block coordinate descent fashion which is much faster than the standard semi definite programming which ha been widely adopted in many other advanced distance learning algorithm we compare this algorithm with other state of the art one on various datasets and competitive result are obtained 
prediction market are used in real life to predict outcome of interest such a presidential election in this work we introduce a mathematical theory for artificial prediction market for supervised classifier aggregation and probability estimation we introduce the artificial prediction market a a novel way to aggregate classifier we derive the market equation to enforce total budget conservation show the market price uniqueness and give efficient algorithm for computing it we show how to train the market participant by updating their budget using training example we introduce classifier specialization a a new differentiating characteristic between classifier finally we present experiment using random decision rule a specialized classifier and show that the prediction market consistently outperforms random forest on real and synthetic data of varying degree of difficulty 
we propose a new fast gaussian summation algorithm for high dimensional datasets with high accuracy first we extend the original fast multipole type method to use approximation scheme with both hard and probabilistic error second we utilize a new data structure called subspace tree which map each data point in the node to it lower dimensional mapping a determined by any linear dimension reduction method such a pca this new data structure is suitable for reducing the cost of each pairwise distance computation the most dominant cost in many kernel method our algorithm guarantee probabilistic relative error on each kernel sum and can be applied to high dimensional gaussian summation which are ubiquitous inside many kernel method a the key computational bottleneck we provide empirical speedup result on low to high dimensional datasets up to dimension 
learning graphical model with hidden variable can offer semantic insight to complex data and lead to salient structured predictor without relying on expensive sometime unattainable fully annotated training data while likelihood based method have been extensively explored to our knowledge learning structured prediction model with latent variable based on the max margin principle remains largely an open problem in this paper we present a partially observed maximum entropy discrimination markov network pomen model that attempt to combine the advantage of bayesian and margin based paradigm for learning markov network from partially labeled data pomen lead to an averaging prediction rule that resembles a bayes predictor that is more robust to overfitting but is also built on the desirable discriminative law resemble those of the m n we develop an em style algorithm utilizing existing convex optimization algorithm for m n a a subroutine we demonstrate competent performance of pomen over existing method on a real world web data extraction task 
our dynamic graph based relational mining approach ha been developed to learn structural pattern in biological network a they change over time the analysis of dynamic network is important not only to understand life at the system level but also to discover novel pattern in other structural data most current graph based data mining approach overlook dynamic feature of biological network because they are focused on only static graph our approach analyzes a sequence of graph and discovers rule that capture the change that occur between pair of graph in the sequence these rule represent the graph rewrite rule that the first graph must go through to be isomorphic to the second graph then our approach feed the graph rewrite rule into a machine learning system that learns general transformation rule describing the type of change that occur for a class of dynamic biological network the discovered graph rewriting rule show how biological network change over time and the transformation rule show the repeated pattern in the structural change in this paper we apply our approach to biological network to evaluate our approach and to understand how the biosystems change over time we evaluate our result using coverage and prediction metric and compare to biological literature 
is accurate classification possible in the absence of hand labeled data this paper introduces the monotonic feature mf abstraction where the probability of class membership increase monotonically with the mf s value the paper prof that when an mf is given pac learning is possible with no hand labeled data under certain assumption we argue that mf arise naturally in a broad range of textual classification application on the classic newsgroups data set a learner given an mf and unlabeled data achieves classification accuracy equal to that of a state of the art semi supervised learner relying on hand labeled example even when mf are not given a input their presence or absence can be determined from a small amount of hand labeled data which yield a new semi supervised learning method that reduces error by on the newsgroups data 
regularized risk minimization often involves non smooth optimization either because of the loss function e g hinge loss or the regularizer e g regularizer gradient method though highly scalable and easy to implement are known to converge slowly in this paper we develop a novel accelerated gradient method for stochastic optimization while still preserving their computational simplicity and scalability the proposed algorithm called sage stochastic accelerated gradient exhibit fast convergence rate on stochastic composite optimization with convex or strongly convex objective experimental result show that sage is faster than recent sub gradient method including folos smidas and scd moreover sage can also be extended for online learning resulting in a simple algorithm but with the best regret bound currently known for these problem 
in this paper we address the question of what kind of knowledge is generally transferable from unlabeled text we suggest and analyze the semantic correlation of word a a generally transferable structure of the language and propose a new method to learn this structure using an appropriately chosen latent variable model this semantic correlation contains structural information of the language space and can be used to control the joint shrinkage of model parameter for any specific task in the same space through regularization in an empirical study we construct different text classification task from a real world benchmark and the unlabeled document are a mixture from all these task we test the ability of various algorithm to use the mixed unlabeled text to enhance all classification task empirical result show that the proposed approach is a reliable and scalable method for semi supervised learning regardless of the source of unlabeled data the specific task to be enhanced and the prediction model used 
a hypergraph is a generalization of the traditional graph in which the edge are arbitrary non empty subset of the vertex set it ha been applied successfully to capture high order relation in various domain in this paper we propose a hypergraph spectral learning formulation for multi label classification where a hypergraph is constructed to exploit the correlation information among different label we show that the proposed formulation lead to an eigenvalue problem which may be computationally expensive especially for large scale problem to reduce the computational cost we propose an approximate formulation which is shown to be equivalent to a least square problem under a mild condition based on the approximate formulation efficient algorithm for solving least square problem can be applied to scale the formulation to very large data set in addition existing regularization technique for least square can be incorporated into the model for improved generalization performance we have conducted experiment using large scale benchmark data set and experimental result show that the proposed hypergraph spectral learning formulation is effective in capturing the high order relation in multi label problem result also indicate that the approximate formulation is much more efficient than the original one while keeping competitive classification performance 
recent work ha shown that one can learn the structure of gaussian graphical model by imposing an l penalty on the precision matrix and then using efficient convex optimization method to find the penalized maximum likelihood estimate this is similar to performing map estimation with a prior that prefers sparse graph in this paper we use the stochastic block model a a prior this prefer graph that are blockwise sparse but unlike previous work it doe not require that the block or group be specified a priori the resulting problem is no longer convex but we devise an efficient variational bayes algorithm to solve it we show that our method ha better test set likelihood on two different datasets motion capture and gene expression compared to independent l and can match the performance of group l using manually created group 
classic mixture model assume that the prevalence of the various mixture component is fixed and doe not vary over time this present problem for application where the goal is to learn how complex data distribution evolve we develop model and bayesian learning algorithm for inferring the temporal trend of the component in a mixture model a a function of time we show the utility of our model by applying them to the real life problem of tracking change in the rate of antibiotic resistance in escherichia coli and staphylococcus aureus the result show that our method can derive meaningful temporal antibiotic resistance pattern 
frequent pattern provide solution to datasets that do not have well structured feature vector however frequent pattern mining is non trivial since the number of unique pattern is exponential but many are non discriminative and correlated currently frequent pattern mining is performed in two sequential step enumerating a set of frequent pattern followed by feature selection although many method have been proposed in the past few year on how to perform each separate step efficiently there is still limited success in eventually finding highly compact and discriminative pattern the culprit is due to the inherent nature of this widely adopted two step approach this paper discus these problem and proposes a new and different method it build a decision tree that partition the data onto different node then at each node it directly discovers a discriminative pattern to further divide it example into purer subset since the number of example towards leaf level is relatively small the new approach is able to examine pattern with extremely low global support that could not be enumerated on the whole dataset by the two step method the discovered feature vector are more accurate on some of the most difficult graph a well a frequent itemset problem than most recently proposed algorithm but the total size is typically or more smaller importantly the minimum support of some discriminative pattern can be extremely low e g in order to enumerate these low support pattern state of the art frequent pattern algorithm either cannot finish due to huge memory consumption or have to enumerate to time more pattern before they can even be found software and datasets are available by contacting the author 
traditional association mining algorithm use a strict definition of support that requires every item in a frequent itemset to occur in each supporting transaction in real life datasets this limit the recovery of frequent itemset pattern a they are fragmented due to random noise and other error in the data hence a number of method have been proposed recently to discover approximate frequent itemsets in the presence of noise these algorithm use a relaxed definition of support and additional parameter such a row and column error threshold to allow some degree of error in the discovered pattern though these algorithm have been shown to be successful in finding the approximate frequent itemsets a systematic and quantitative approach to evaluate them ha been lacking in this paper we propose a comprehensive evaluation framework to compare different approximate frequent pattern mining algorithm the key idea is to select the optimal parameter for each algorithm on a given dataset and use the itemsets generated with these optimal parameter in order to compare different algorithm we also propose simple variation of some of the existing algorithm by introducing an additional post processing step subsequently we have applied our proposed evaluation framework to a wide variety of synthetic datasets with varying amount of noise and a real dataset to compare existing and our proposed variation of the approximate pattern mining algorithm source code and the datasets used in this study are made publicly available 
text classification ha matured a a research discipline over the last decade independently business intelligence over structured database ha long been a source of insight for enterprise in this work we bring the two together for customer satisfaction c sat analysis in the service industry we present itacs a solution combining text classification and business intelligence integrated with a novel interactive text labeling interface itacs ha been deployed in multiple client account in contact center it can be extended to any service industry setting to analyze unstructured text data and derive operational and business insight we highlight importance of interactivity in real life text classification setting we bring out some unique research challenge about label set measuring accuracy and interpretability that need serious attention in both academic and industrial research we recount invaluable experience and lesson learned a data mining researcher working toward seeing research technology deployed in the service industry 
this demo present pattern miner an integrated environment for pattern management and mining that deal with the whole lifecycle of pattern from their generation using data mining technique to their storage and querying putting also emphasis on the comparison between pattern and meta mining operation over the extracted pattern pattern comparison comparing result of the data mining process and meta mining are high level pattern operation that can be applied in a variety of application from database change management to image comparison and retrieval 
we study the profit maximization problem of a monopolistic market maker the sequential decision problem is hard because the state space is a function we demonstrate that the belief state is well approximated by a gaussian distribution we prove a key monotonicity property of the gaussian state update which make the problem tractable the algorithm lead to a surprising insight an optimal monopolist can provide more liquidity than perfectly competitive market maker because a monopolist is willing to absorb initial loss in order to learn a new valuation rapidly so she can extract higher profit later 
in this paper we introduce a modular highly flexible open source environment for data generation using an existing graphical data flow tool the user can combine various type of module for numeric and categorical data generator additional functionality is added via the data processing framework in which the generator module are embedded the resulting data flow can be used to document deploy and reuse the resulting data generator we describe the overall environment and individual module and demonstrate how they can be used for the generation of a sample complex customer product database with corresponding shopping basket data including various artifact and outlier 
in capital market surveillance an emerging trend is that a group of hidden manipulator collaborate with each other to manipulate three trading sequence buy order sell order and trade through carefully arranging their price volume and time in order to mislead other investor affect the instrument movement and thus maximize personal benefit if the focus is on only one of the above three sequence in attempting to analyze such hidden group based behavior or if they are merged into one sequence a per an investor the coupling relationship among them indicated through trading action and their price volume time would be missing and the resulting finding would have a high probability of mismatching the genuine fact in business therefore typical sequence analysis approach which mainly identify pattern on a single sequence cannot be used here this paper address a novel topic namely coupled behavior analysis in hidden group in particular we propose a coupled hidden markov model hmm based approach to detect abnormal group based trading behavior the resulting model cater for multiple sequence from a group of people interaction among them sequence item property and significant change among coupled sequence we demonstrate our approach in detecting abnormal manipulative trading behavior on orderbook level stock data the result are evaluated against alert generated by the exchange s surveillance system from both technical and computational perspective it show that the proposed coupled and adaptive hmms outperform a standard hmm only modeling any single sequence or the hmm combining multiple single sequence without considering the coupling relationship further work on coupled behavior analysis including coupled sequence event analysis hidden group analysis and behavior dynamic are very critical 
in large social network node user entity are influenced by others for various reason for example the colleague have strong influence on one s work while the friend have strong influence on one s daily life how to differentiate the social influence from different angle topic how to quantify the strength of those social influence how to estimate the model on real large network to address these fundamental question we propose topical affinity propagation tap to model the topic level social influence on large network in particular tap can take result of any topic modeling and the existing network structure to perform topic level influence propagation with the help of the influence analysis we present several important application on real data set such a what are the representative node on a given topic how to identify the social influence of neighboring node on a particular node to scale to real large network tap is designed with efficient distributed learning algorithm that is implemented and tested under the map reduce framework we further present the common characteristic of distributed learning algorithm for map reduce finally we demonstrate the effectiveness and efficiency of tap on real large data set 
there is a growing number of service provider that a consumer can interact with over the web to learn their service term the service term such a price and time to completion of the service depend on the consumer s particular specification for instance a printing service provider would need from it customer specification such a the size of paper type of ink proofing and perforation in a few sector there exist marketplace site that provide consumer with specification form which the consumer can fill out to learn the service term of multiple service provider unfortunately there are only a few such marketplace site and they cover a few sector at hp lab we are working towards building a universal marketplace site i e a marketplace site that cover thousand of sector and hundred of provider per sector one issue in this domain is the automated discovery retrieval of the specification for each sector we address it through extracting and analyzing content from the website of the service provider listed in business directory the challenge is that each service provider is often listed under multiple service category in a business directory making it infeasible to utilize standard supervised learning technique we address this challenge through employing a multilabel statistical clustering approach within an expectation maximization framework we implement our solution to retrieve specification for sector representing more than service provider we discus our result within the context of the service needed to design a marketing campaign for a small business 
actor critic algorithm for reinforcement learning are ac hieving renewed popularity due to their good convergence property in situation where other approach often fail e g when function approximation is involved interestingly there is growing evidence that actor critic approach based on phasic dopamine signal play a key role in biological learning through cortical and basal ganglion loop we derive a temporal difference based actor critic learning algorithm for which convergence can be proved without assuming widely separated time scale for the actor and the critic the approach is demonstrated by applying it to network of spiking neuron the established relation between phasic dopamine and the temporal difference signal lends support to the biological relevance of such algorithm 
we consider the problem of binary classification where the classifier may abstain instead of classifying each observation the bayes decision rule for this setup known a chow s rule is defined by two threshold on posterior probability from simple desideratum namely the consistency and the sparsity of the classifier we derive the double hinge loss function that focus on estimating conditional probability only in the vicinity of the threshold point of the optimal decision rule we show that for suitable kernel machine our approach is universally consistent we cast the problem of minimizing the double hinge loss a a quadratic program akin to the standard svm optimization problem and propose an active set method to solve it efficiently we finally provide preliminary experimental result illustrating the interest of our constructive approach to devising loss function 
most of recommender system try to find item that are most relevant to the older choice of a given user here we focus on the surprise me query a user may be bored with his her usual genre of item e g book movie hobby and may want a recommendation that is related but off the beaten path possibly leading to a new genre of book movie hobby how would we define a well a automate this seemingly selfcontradicting request we introduce tangent a novel recommendation algorithm to solve this problem the main idea behind tangent is to envision the problem a node selection on a graph giving high score to node that are well connected to the older choice and at the same time well connected to unrelated choice the method is carefully designed to be a parameter free b effective and c fast we illustrate the benefit of tangent with experiment on both synthetic and real data set we show that tangent make reasonable yet surprising horizon broadening recommendation moreover it is fast and scalable since it can easily use existing fast algorithm on graph node proximity 
in this paper we consider a novel scheme referred to a cartesian contour to concisely represent the collection of frequent itemsets different from the existing work this scheme provides a complete view of these itemsets by covering the entire collection of them more interestingly it take a first step in deriving a generative view of the frequent pattern formulation i e how a small number of pattern interact with each other and produce the complexity of frequent itemsets we perform a theoretical investigation of the concise representation problem and link it to the biclique set cover problem and prove it np hardness we develop a novel approach utilizing the technique developed in frequent itemset mining set cover and max k cover to approximate the minimal biclique set cover problem in addition we consider several heuristic technique to speedup the construction of cartesian contour the detailed experimental study demonstrates the effectiveness and efficiency of our approach 
many practical application of classification require the classifier to produce a very low false positive rate although the support vector machine svm ha been widely applied to these application due to it superiority in handling high dimensional data there are relatively little effort other than setting a threshold or changing the cost of slack to ensure the low false positive rate in this paper we propose the notion of asymmetric support vectormachine asvm that take into account the false positive and the user tolerance in it objective such a new objective formulation allows u to raise the confidence in predicting the positive and therefore obtain a lower chance of false positive we study the effect of the parameter in asvm objective and address some implementation issue related to the sequential minimal optimization smo to cope with large scale data an extensive simulation is conducted and show that asvm is able to yield either noticeable improvement in performance or reduction in training time a compared to the previous art 
link prediction personalized graph search fraud detection and many such graph mining problem revolve around the computation of the most similar k node to a given query node one widely used class of similarity measure is based on random walk on graph e g personalized pagerank hitting and commute time and simrank there are two fundamental problem associated with these measure first existing online algorithm typically examine the local neighborhood of the query node which can become significantly slower whenever high degree node are encountered a common phenomenon in real world graph we prove that turning high degree node into sink result in only a small approximation error while greatly improving running time the second problem is that of computing similarity at query time when the graph is too large to be memory resident the obvious solution is to split the graph into cluster of node and store each cluster on a disk page ideally random walk will rarely cross cluster boundary and cause page fault our contribution here are twofold a we present an efficient deterministic algorithm to find the k closest neighbor in term of personalized pagerank of any query node in such a clustered graph and b we develop a clustering algorithm rwdisk that us only sequential sweep over data file empirical result on several large publicly available graph like dblp citeseer and live journal m edge demonstrate that turning high degree node into sink not only improves running time of rwdisk by a factor of but also boost link prediction accuracy by a factor of on average we also show that rwdisk return more desirable high conductance and small size cluster than the popular clustering algorithm metis while requiring much le memory finally our deterministic algorithm for computing nearest neighbor incurs far fewer page fault factor of than actually simulating random walk 
recommender problem with large and dynamic item pool are ubiquitous in web application like content optimization online advertising and web search despite the availability of rich item meta data excess heterogeneity at the item level often requires inclusion of item specific factor or weight in the model however since estimating item factor is computationally intensive it pose a challenge for time sensitive recommender problem where it is important to rapidly learn factor for new item e g news article event update tweet in an online fashion in this paper we propose a novel method called fobfm fast online bilinear factor model to learn item specific factor quickly through online regression the online regression for each item can be performed independently and hence the procedure is fast scalable and easily parallelizable however the convergence of these independent regression can be slow due to high dimensionality the central idea of our approach is to use a large amount of historical data to initialize the online model based on offline feature and learn linear projection that can effectively reduce the dimensionality we estimate the rank of our linear projection by taking recourse to online model selection based on optimizing predictive likelihood through extensive experiment we show that our method significantly and uniformly outperforms other competitive method and obtains relative lift that are in the range of in term of predictive log likelihood for a rank correlation metric on a proprietary my yahoo dataset it obtains reduction in root mean squared error over the previously best method on a benchmark movielens dataset using a time based train test data split 
while discriminative training e g crf structural svm hold much promise for machine translation image segmentation and clustering the complex inference these application require make exact training intractable this lead to a need for approximate training method unfortunately knowledge about how to perform efficient and effective approximate training is limited focusing on structural svms we provide and explore algorithm for two different class of approximate training algorithm which we call undergenerating e g greedy and overgenerating e g relaxation algorithm we provide a theoretical and empirical analysis of both type of approximate trained structural svms focusing on fully connected pairwise markov random field we find that model trained with overgenerating method have theoretic advantage over undergenerating method are empirically robust relative to their undergenerating brother and relaxed trained model favor non fractional prediction from relaxed predictor 
covariance estimation for high dimensional vector is a classically difcult problem in statistical analysis and machine learning in this paper we propose a maximum likelihood ml approach to covariance estimation which employ a novel sparsity constraint more specically the covariance is constrained to have an eigen decomposition which can be represented a a sparse matrix transform smt the smt is formed by a product of pairwise coordinate rotation known a given rotation using this framework the covariance can be efciently estimated using greedy minimization of the log likelihood function and the number of given rotation can be efciently computed using a cross validation procedure the resulting estimator is positive denite and well conditioned even when the sample size is limited experiment on standard hyperspectral data set show that the smt covariance estimate is consistently more accurate than both traditional shrinkage estimate and recently proposed graphical lasso estimate for a variety of different class and sample size 
the roc curve is known to be the golden standard for measuring performance of a test scoring statistic regarding it capacity of discrimination between two population in a wide variety of application ranging from anomaly detection in signal processing to information retrieval through medical diagnosis most practical performance measure used in scoring application such a the auc the local auc the p norm push the dcg and others can be seen a summary of the roc curve this paper highlight the fact that many of these empirical criterion can be expressed a conditional linear rank statistic we investigate the property of empirical maximizers of such performance criterion and provide preliminary result for the concentration property of a novel class of random variable that we will call a linear rank process 
we introduce a learning framework that combine element of the well known pac and mistake bound model the kwik know what it know framework wa designed particularly for it utility in learning setting where active exploration can impact the training example the learner is exposed to a is true in reinforcement learning and active learning problem we catalog several kwik learnable class and open problem 
we describe a probabilistic approach for supervised learning when we have multiple expert annotator providing possibly noisy label but no absolute gold standard the proposed algorithm evaluates the different expert and also give an estimate of the actual hidden label experimental result indicate that the proposed method is superior to the commonly used majority voting baseline 
this paper aim to conduct a study on the listwise approach to learning to rank the listwise approach learns a ranking function by taking individual list a instance and minimizing a loss function defined on the predicted list and the ground truth list existing work on the approach mainly focused on the development of new algorithm method such a rankcosine and listnet have been proposed and good performance by them have been observed unfortunately the underlying theory wa not sufficiently studied so far to amend the problem this paper proposes conducting theoretical analysis of learning to rank algorithm through investigation on the property of the loss function including consistency soundness continuity differentiability convexity and efficiency a sufficient condition on consistency for ranking is given which seems to be the first such result obtained in related research the paper then conduct analysis on three loss function likelihood loss cosine loss and cross entropy loss the latter two were used in rankcosine and listnet the use of the likelihood loss lead to the development of a new listwise method called listmle whose loss function offer better property and also lead to better experimental result 
corruption of data by class label noise is an important practical concern impacting many classification problem study of data cleaning technique often assume a uniform label noise model however which is seldom realized in practice relatively little is understood a to how the natural label noise distribution can be measured or simulated using email spam filtering data we demonstrate that class noise can have substantial content specific bias we also demonstrate that noise detection technique based on classifier confidence tend to identify instance that human assessor are likely to label in error we show that genre modeling can be very informative in identifying potential area of mislabeling moreover we are able to show that genre decomposition can also be used to substantially improve spam filtering accuracy with our result outperforming the best published figure for the trec p and ceas benchmark collection 
medical coding or classification is the process of transforming information contained in patient medical record into standard predefined medical code there are several worldwide accepted medical coding convention associated with diagnosis and medical procedure however in the united state the ninth revision of icd icd provides the standard for coding clinical record accurate medical coding is important since it is used by hospital for insurance billing purpose since after discharge a patient can be assigned or classified to several icd code the coding problem can be seen a a multi label classification problem in this paper we introduce a multi label large margin classifier that automatically learns the underlying inter code structure and allows the controlled incorporation of prior knowledge about medical code relationship in addition to refining and learning the code relationship our classifier can also utilize this shared information to improve it performance experiment on a publicly available dataset containing clinical free text and their associated medical code showed that our proposed multi label classifier outperforms related multi label model in this problem 
the affinity propagation ap clustering algorithm proposed by frey and dueck provides an understandable nearly optimal summary of a dataset albeit with quadratic computational complexity this paper motivated by autonomic computing extends ap to the data streaming framework firstly a hierarchical strategy is used to reduce the complexity to o n the distortion loss incurred is analyzed in relation with the dimension of the data item secondly a coupling with a change detection test is used to cope with non stationary data distribution and rebuild the model a needed the presented approach strap is applied to the stream of job submitted to the egee grid providing an understandable description of the job flow and enabling the system administrator to spot online some source of failure 
this paper address exact learning of bayesian network structure from data and expert s knowledge based on score function that are decomposable first it describes useful property that strongly reduce the time and memory cost of many known method such a hill climbing dynamic programming and sampling variable ordering secondly a branch and bound algorithm is presented that integrates parameter and structural constraint with data in a way to guarantee global optimality with respect to the score function it is an any time procedure because if stopped it provides the best current solution and an estimation about how far it is from the global solution we show empirically the advantage of the property and the constraint and the applicability of the algorithm to large data set up to one hundred variable that cannot be handled by other current method limited to around variable 
we describe an application of probabilistic modeling and inference technology to the problem of analyzing sensor data in the setting of an intensive care unit icu in particular we consider the arterial line blood pressure sensor which is subject to frequent data artifact that cause false alarm in the icu and make the raw data almost useless for automated decision making the problem is complicated by the fact that the sensor data are averaged over fixed interval whereas the event causing data artifact may occur at any time and often have duration significantly shorter than the data collection interval we show that careful modeling of the sensor combined with a general technique for detecting sub interval event and estimating their duration enables detection of artifact and accurate estimation of the underlying blood pressure value our model s performance identifying artifact is superior to two other classifier and about a good a a physician s 
when a spike is initiated near the soma ofa cortical pyramidal neuron it may back propagate up dendrite toward distal synapsis where strong depolarization can trigger spike timing dependent hebbian plasticity at recently activated synapsis we show that a these mechanism can implement a temporal difference algorithm for sequence learning and b a population of recurrently connected neuron with this form of synaptic plasticity can learn to predict spatiotemporal input pattern using biophysical simulation we demonstrate that a network of cortical neuron can develop direction selectivity similar to that observed in complex cell in alert monkey visual cortex a a consequence of learning to predict moving stimulus 
information theoretic based measure form a fundamental class of similarity measure for comparing clustering beside the class of pair counting based and set matching based measure in this paper we discus the necessity of correction for chance for information theoretic based measure for clustering comparison we observe that the baseline for such measure i e average value between random partition of a data set doe not take on a constant value and tends to have larger variation when the ratio between the number of data point and the number of cluster is small this effect is similar in some other non information theoretic based measure such a the well known rand index assuming a hypergeometric model of randomness we derive the analytical formula for the expected mutual information value between a pair of clustering and then propose the adjusted version for several popular information theoretic based measure some example are given to demonstrate the need and usefulness of the adjusted measure 
it is well known that user behavior action in a social network are influenced by various factor such a personal interest social influence and global trend however few publication systematically study how social action evolve in a dynamic social network and to what extent different factor affect the user action in this paper we propose a noise tolerant time varying factor graph model ntt fgm for modeling and predicting social action ntt fgm simultaneously model social network structure user attribute and user action history for better prediction of the user future action more specifically a user s action at time t is generated by her latent state at t which is influenced by her attribute her own latent state at time t and her neighbor state at time t and t based on this intuition we formalize the social action tracking problem using the ntt fgm model then present an efficient algorithm to learn the model by combining the idea from both continuous linear system and markov random field finally we present a case study of our model on predicting future social action we validate the model on three different type of real world data set qualitatively our model can uncover some interesting pattern of the social dynamic quantitatively experimental result show that the proposed method outperforms several baseline method for action prediction 
sequential optimal design method hold great promise for improving the efficiency of neurophysiology experiment however previous method for optimal experimental design have incorporated only weak prior information about the underlying neural system e g the sparseness or smoothness of the receptive field here we describe how to use stronger prior information in the form of parametric model of the receptive field in order to construct optimal stimulus and further improve the efficiency of our experiment for example if we believe that the receptive field is well approximated by a gabor function then our method construct stimulus that optimally constrain the gabor parameter orientation spatial frequency etc using a few experimental trial a possible more generally we may believe a priori that the receptive field lie near a known sub manifold of the full parameter space in this case our method chooses stimulus in order to reduce the uncertainty along the tangent space of this sub manifold a rapidly a possible application to simulated and real data indicate that these method may in many case improve the experimental efficiency 
large margin learning of continuous density hmms with a partially labeled dataset ha been extensively studied in the speech and handwriting recognition field yet due to the non convexity of the optimization problem previous work usually rely on severe approximation so that it is still an open problem we propose a new learning algorithm that relies on non convex optimization and bundle method and allows tackling the original optimization problem a is it is proved to converge to a solution with accuracy with a rate o we provide experimental result gained on speech and handwriting recognition that demonstrate the potential of the method 
discriminative training for structured output ha found increasing application in area such a natural language processing bioinformatics information retrieval and computer vision focusing on large margin method the most general in term of loss function and model structure training algorithm known to date are based on cutting plane approach while these algorithm are very efficient for linear model their training complexity becomes quadratic in the number of example when kernel are used to overcome this bottleneck we propose new training algorithm that use approximate cutting plane and random sampling to enable efficient training with kernel we prove that these algorithm have improved time complexity while providing approximation guarantee in empirical evaluation our algorithm produced solution with training and test error rate close to those of exact solver even on binary classification problem where highly optimized conventional training method exist e g svm light our method are about an order of magnitude faster than conventional training method on large datasets while remaining competitive in speed on datasets of medium size 
the world wide aviation system is one of the most complex dynamical system ever developed and is generating data at an extremely rapid rate most modern commercial aircraft record several hundred flight parameter including information from the guidance navigation and control system the avionics and propulsion system and the pilot input into the aircraft these parameter may be continuous measurement or binary or categorical measurement recorded in one second interval for the duration of the flight currently most approach to aviation safety are reactive meaning that they are designed to react to an aviation safety incident or accident in this paper we discus a novel approach based on the theory of multiple kernel learning to detect potential safety anomaly in very large data base of discrete and continuous data from world wide operation of commercial fleet we pose a general anomaly detection problem which includes both discrete and continuous data stream where we assume that the discrete stream have a causal influence on the continuous stream we also assume that atypical sequence of event in the discrete stream can lead to off nominal system performance we discus the application domain novel algorithm and also discus result on real world data set our algorithm uncovers operationally significant event in high dimensional data stream in the aviation industry which are not detectable using state of the art method 
inference in graphical model ha emerged a a promising technique for planning a recent approach to decision theoretic planning in relational domain us forward inference in dynamic bayesian network compiled from learned probabilistic relational rule inspired by work in non relational domain with small state space we derive a backpropagation method for such net in relational domain starting from a goal state mixture distribution we combine this with forward reasoning in a bidirectional two lter approach we perform experiment in a complex d simulated desktop environment with an articulated manipulator and realistic physic empirical result show that bidirectional probabilistic reasoning can lead to more ecient and accurate planning in comparison to pure forward reasoning 
we address the problem of classification in partially labeled network a k a within network classification where observed class label are sparse technique for statistical relational learning have been shown to perform well on network classification task by exploiting dependency between class label of neighboring node however relational classifier can fail when unlabeled node have too few labeled neighbor to support learning during training phase and or inference during testing phase this situation arises in real world problem when observed label are sparse in this paper we propose a novel approach to within network classification that combine aspect of statistical relational learning and semi supervised learning to improve classification performance in sparse network our approach work by adding ghost edge to a network which enable the flow of information from labeled to unlabeled node through experiment on real world data set we demonstrate that our approach performs well across a range of condition where existing approach such a collective classification and semi supervised learning fail on all task our approach improves area under the roc curve auc by up to point over existing approach furthermore we demonstrate that our approach run in time proportional to l e where l is the number of labeled node and e is the number of edge 
while many perceptual and cognitive phenomenon are well described in term of bayesian inference the necessary computation are intractable at the scale of realworld task and it remains unclear how the human mind approximates bayesian computation algorithmically we explore the proposal that for some task human use a form of markov chain monte carlo to approximate the posterior distribution over hidden variable a a case study we show how several phenomenon of perceptual multistability can be explained a mcmc inference in simple graphical model for low level vision 
the world wide web ha become the most important information source for most of u unfortunately there is no guarantee for the correctness of information on the web moreover different website often provide conflicting information on a subject such a different specification for the same product in this paper we propose a new problem called veracity i e conformity to truth which study how to find true fact from a large amount of conflicting information on many subject that is provided by various website we design a general framework for the veracity problem and invent an algorithm called truthflnder which utilizes the relationship between website and their information i e a website is trustworthy if it provides many piece of true information and a piece of information is likely to be true if it is provided by many trustworthy website an iterative method is used to infer the trustworthiness of website and the correctness of information from each other our experiment show that truthflnder successfully find true fact among conflicting information and identifies trustworthy website better than the popular search engine 
the purpose of the paper is to explore the connection between multivariate homogeneity test and auc optimization the latter problem ha recently received much attention in the statistical learning literature from the elementary observation that in the two sample problem setup the null assumption corresponds to the situation where the area under the optimal roc curve is equal to we propose a two stage testing method based on data splitting a nearly optimal scoring function in the auc sense is first learnt from one of the two half sample data from the remaining half sample are then projected onto the real line and eventually ranked according to the scoring function computed at the first stage the last step amount to performing a standard mann whitney wilcoxon test in the onedimensional framework we show that the learning step of the procedure doe not affect the consistency of the test a well a it property in term of power provided the ranking produced is accurate enough in the auc sense the result of a numerical experiment are eventually displayed in order to show the efficiency of the method 
when labeled example are limited and difficult to obtain transfer learning employ knowledge from a source domain to improve learning accuracy in the target domain however the assumption made by existing approach that the marginal and conditional probability are directly related between source and target domain ha limited applicability in either the original space or it linear transformation to solve this problem we propose an adaptive kernel approach that map the marginal distribution of target domain and source domain data into a common kernel space and utilize a sample selection strategy to draw conditional probability between the two domain closer we formally show that under the kernel mapping space the difference in distribution between the two domain is bounded and the prediction error of the proposed approach can also be bounded experimental result demonstrate that the proposed method outperforms both traditional inductive classifier and the state of the art boosting based transfer algorithm on most domain including text categorization and web page rating in particular it can achieve around higher accuracy than other approach for the text categorization problem the source code and datasets are available from the author 
in this paper we present a system called cro chinese review observer for online product review structurization by structurization we mean identifying extracting and summarizing information from unstructured review text to a structured table the core task include review collection product feature and user opinion extraction and polarity analysis of opinion existing research in this area is mainly english text oriented to deal with chinese effectively we propose several novel approach for fulfilling the core task then we integrated these approach and implement the whole procedure of review structurization in the system cro running result for review of real product show it performance is satisfactory 
the promise of unsupervised learning method lie in their potential to use vast amount of unlabeled data to learn complex highly nonlinear model with million of free parameter we consider two well known unsupervised learning model deep belief network dbns and sparse coding that have recently been applied to a flurry of machine learning application hinton salakhutdinov raina et al unfortunately current learning algorithm for both model are too slow for large scale application forcing researcher to focus on smaller scale model or to use fewer training example in this paper we suggest massively parallel method to help resolve these problem we argue that modern graphic processor far surpass the computational capability of multicore cpu and have the potential to revolutionize the applicability of deep unsupervised learning method we develop general principle for massively parallelizing unsupervised learning task using graphic processor we show that these principle can be applied to successfully scaling up learning algorithm for both dbns and sparse coding our implementation of dbn learning is up to time faster than a dual core cpu implementation for large model for example we are able to reduce the time required to learn a four layer dbn with million free parameter from several week to around a single day for sparse coding we develop a simple inherently parallel algorithm that lead to a to fold speedup over previous method 
we discus the problem of clustering element according to the source that have generated them for element that are characterized by independent binary attribute a closed form bayesian solution exists we derive a solution for the case of dependent attribute that is based on a transformation of the instance into a space of independent feature function we derive an optimization problem that produce a mapping into a space of independent binary feature vector the feature can reflect arbitrary dependency in the input space this problem setting is motivated by the application of spam filtering for email service provider spam trap deliver a real time stream of message known to be spam if element of the same campaign can be recognized reliably entire spam and phishing campaign can be contained we present a case study that evaluates bayesian clustering for this application 
existing model of categorization typically represent to be classified item a point in a multidimensional space while from a mathematical point of view an infinite number of basis set can be used to represent point in this space the choice of basis set is psychologically crucial people generally choose the same basis dimension and have a strong preference to generalize along the ax of these dimension but not diagonally what make some choice of dimension special we explore the idea that the dimension used by people echo the natural variation in the environment specifically we present a rational model that doe not assume dimension but learns the same type of dimensional generalization that people display this bias is shaped by exposing the model to many category with a structure hypothesized to be like those which child encounter the learning behaviour of the model capture the developmental shift from roughly isotropic for child to the axis aligned generalization that adult show 
temporal text data is often generated by a time changing process or distribution such a drift in the underlying distribution cannot be captured by stationary likelihood technique we consider the application of local likelihood method to generative and conditional modeling of temporal document sequence we examine the asymptotic bias and variance and present an experimental study using the rcv dataset containing a temporal sequence of reuters news story 
in this paper we develop a spectral framework for estimating mixture distribution specifically gaussian mixture model in physic spectroscopy is often used for the identification of substance through their spectrum treating a kernel function k x y a light and the sampled data a substance the spectrum of their interaction eigenvalue and eigenvectors of the kernel matrix k unveils certain aspect of the underlying parametric distribution p such a the parameter of a gaussian mixture our approach extends the intuition and analysis underlying the existing spectral technique such a spectral clustering and kernel principal component analysis kpca we construct algorithm to estimate parameter of gaussian mixture model including the number of mixture component their mean and covariance matrix which are important in many practical application we provide a theoretical framework and show encouraging experimental result 
traditional method for analyzing population structure such a the structure program ignore the influence of mutational effect we propose mstruct an admixture of population specific mixture of inheritance model that address the task of structure inference and mutation estimation jointly through a hierarchical bayesian framework and a variational algorithm for inference we validated our method on synthetic data and used it to analyze the hgdp ceph cell line panel of microsatellites used in rosenberg et al and the hgdp snp data used in conrad et al a comparison of the structural map of world population estimated by mstruct and structure is presented and we also report potentially interesting mutation pattern in world population estimated by mstruct which is not possible by structure 
this paper is concerned with the generalization analysis on learning to rank for information retrieval ir in ir data are hierarchically organized i e consisting of query and document previous generalization analysis for ranking however ha not fully considered this structure and cannot explain how the simultaneous change of query number and document number in the training data will affect the performance of the learned ranking model in this paper we propose performing generalization analysis under the assumption of two layer sampling i e the i i d sampling of query and the conditional i i d sampling of document per query such a sampling can better describe the generation mechanism of real data and the corresponding generalization analysis can better explain the real behavior of learning to rank algorithm however it is challenging to perform such analysis because the document associated with different query are not identically distributed and the document associated with the same query become no longer independent after represented by feature extracted from query document matching to tackle the challenge we decompose the expected risk according to the two layer and make use of the new concept of two layer rademacher average the generalization bound we obtained are quite intuitive and are in accordance with previous empirical study on the performance of ranking algorithm 
many structured prediction task involve complex model where inference is computationally intractable but where it can be well approximated using a linear programming relaxation previous approach for learning for structured prediction e g cuttingplane subgradient method perceptron repeatedly make prediction for some of the data point these approach are computationally demanding because each prediction involves solving a linear program to optimality we present a scalable algorithm for learning for structured prediction the main idea is to instead solve the dual of the structured prediction loss we formulate the learning task a a convex minimization over both the weight and the dual variable corresponding to each data point a a result we can begin to optimize the weight even before completely solving any of the individual prediction problem we show how the dual variable can be eciently optimized using coordinate descent our algorithm is competitive with state of the art method such a stochastic subgradient and cutting plane 
given multiple time sequence with missing value we propose dynammo which summarizes compress and find latent variable the idea is to discover hidden variable and learn their dynamic making our algorithm able to function even when there are missing value we performed experiment on both real and synthetic datasets spanning several megabyte including motion capture sequence and chlorine level in drinking water we show that our proposed dynammo method a can successfully learn the latent variable and their evolution b can provide high compression for little loss of reconstruction accuracy c can extract compact but powerful feature for segmentation interpretation and forecasting d ha complexity linear on the duration of sequence 
we consider gaussian multiresolution mr model in which coarser hidden variable serve to capture statistical dependency among the finest scale variable tree structured mr model have limited modeling capability a variable at one scale are forced to be uncorrelated with each other conditioned on other scale we propose a new class of gaussian mr model that capture the residual correlation within each scale using sparse covariance structure our goal is to learn a tree structured graphical model connecting variable across different scale while at the same time learning sparse structure for the conditional covariance within each scale conditioned on other scale this model lead to an efficient new inference algorithm that is similar to multipole method in computational physic 
cross domain collaborative filtering solves the sparsity problem by transferring rating knowledge across multiple domain in this paper we propose a rating matrix generative model rmgm for effective cross domain collaborative filtering we first show that the relatedness across multiple rating matrix can be established by finding a shared implicit cluster level rating matrix which is next extended to a cluster level rating model consequently a rating matrix of any related task can be viewed a drawing a set of user and item from a user item joint mixture model a well a drawing the corresponding rating from the cluster level rating model the combination of these two model give the rmgm which can be used to fill the missing rating for both existing and new user a major advantage of rmgm is that it can share the knowledge by pooling the rating data from multiple task even when the user and item of these task do not overlap we evaluate the rmgm empirically on three real world collaborative filtering data set to show that rmgm can outperform the individual model trained separately 
we propose an efficient sequential monte carlo inference scheme for the recently proposed coalescent clustering model our algorithm ha a quadratic runtime while those in is cubic in experiment we were surprised to find that in addition to being more efficient it is also a better sequential monte carlo sampler than the best in when measured in term of variance of estimated likelihood and effective sample size 
this paper proposes a general framework called eigentransfer to tackle a variety of transfer learning problem e g cross domain learning self taught learning etc our basic idea is to construct a graph to represent the target transfer learning task by learning the spectrum of a graph which represents a learning task we obtain a set of eigenvectors that reflect the intrinsic structure of the task graph these eigenvectors can be used a the new feature which transfer the knowledge from auxiliary data to help classify target data given an arbitrary non transfer learner e g svm and a particular transfer learning task eigentransfer can produce a transfer learner accordingly for the target transfer learning task we apply eigentransfer on three different transfer learning task cross domain learning cross category learning and self taught learning to demonstrate it unifying ability and show through experiment that eigentransfer can greatly outperform several representative non transfer learner 
this paper give an efficient bayesian method for inferring the parameter of a plackett luce ranking model such model are parameterised distribution over ranking of a finite set of object and have typically been studied and applied within the psychometric sociometric and econometric literature the inference scheme is an application of power ep expectation propagation the scheme is robust and can be readily applied to large scale data set the inference algorithm extends to variation of the basic plackett luce model including partial ranking we show a number of advantage of the ep approach over the traditional maximum likelihood method we apply the method to aggregate ranking of nascar racing driver over the season and also to ranking of movie genre 
in this paper we study the problem of learning a low rank sparse distance matrix we propose a novel metric learning model which can simultaneously conduct dimension reduction and learn a distance matrix the sparse representation involves a mixed norm regularization which is non convex we then show that it can be equivalently formulated a a convex saddle min max problem from this saddle representation we develop an efficient smooth optimization approach for sparse metric learning although the learning model is based on a non differentiable loss function finally we run experiment to validate the effectiveness and efficiency of our sparse metric learning model on various datasets 
we discus how the runtime of svm optimization should decrease a the size of the training data increase we present theoretical and empirical result demonstrating how a simple subgradient descent approach indeed display such behavior at least for linear kernel 
we explore a new bayesian model for probabilistic grammar a family of distribution over discrete structure that includes hidden markov model and probabilistic context free grammar our model extends the correlated topic model framework to probabilistic grammar exploiting the logistic normal distribution a a prior over the grammar parameter we derive a variational em algorithm for that model and then experiment with the task of unsupervised grammar induction for natural language dependency parsing we show that our model achieves superior result over previous model that use dierent prior 
although user of online communication tool rarely categorize their contact into group such a family co worker or jogging buddy they nonetheless implicitly cluster contact by virtue of their interaction with them forming implicit group in this paper we describe the implicit social graph which is formed by user interaction with contact and group of contact and which is distinct from explicit social graph in which user explicitly add other individual a their friend we introduce an interaction based metric for estimating a user s affinity to his contact and group we then describe a novel friend suggestion algorithm that us a user s implicit social graph to generate a friend group given a small seed set of contact which the user ha already labeled a friend we show experimental result that demonstrate the importance of both implicit group relationship and interaction based affinity ranking in suggesting friend finally we discus two application of the friend suggest algorithm that have been released a gmail lab feature 
we describe a manifold learning framewor that naturally accommodates supervised learning partially supervised learning and unsupervised clustering a particular case our method chooses a function by minimizing loss subject to a manifold regularization penalty this augmented cost is minimized using a greedy stagewise functional minimization procedure a in gradientboost each stage of boosting is fast and efficient we demonstrate our approach using both radial basis function approximation and tree the performance of our method is at the state of the art on many standard semi supervised learning benchmark and we produce result for large scale datasets 
mining order preserving submatrix opsm pattern ha received much attention from researcher since in many scientific application such a those involving gene expression data it is natural to express the data in a matrix and also important to find the order preserving submatrix pattern however most current work assumes the noise free opsm model and thus is not practical in many real situation when sample contamination exists in this paper we propose a relaxed opsm model called ropsm the ropsm model support mining more reasonable noise corrupted opsm pattern than another well known model called aopc approximate order preserving cluster while opsm mining is known to be an np hard problem mining ropsm pattern is even a harder problem we propose a novel method called ropsm growth to mine ropsm pattern specifically two pattern growing strategy such a column centric strategy and row centric strategy are presented which are effective to grow the seed opsms into significant ropsms an effective median rank based method is also developed to discover the underlying true order of condition involved in an ropsm pattern our experiment on a biological dataset show that the ropsm model better capture the characteristic of noise in gene expression data matrix compared to the aopc model importantly we find that our approach is able to detect more quality biologically significant pattern with comparable efficiency with the counterpart of aopc specifically at least out of of the pattern mined by our approach are strongly associated with more than gene category high biological significance which is time better than that obtained from using the aopc approach 
we present an approach to reconstructing chemical reaction network from time series measurement of the concentration of the molecule involved our solution strategy combine technique from numerical sensitivity analysis and probabilistic graphical model by modeling a chemical reaction system a a markov network undirected graphical model we show how systematically probing for sensitivity between molecular specie can identify the topology of the network given the topology our approach next us detailed sensitivity profile to characterize property of reaction such a reversibility enzyme catalysis and the precise stoichiometry of the reactant and product we demonstrate application to reconstructing key biological system including the yeast cell cycle in addition to network reconstruction our algorithm find application in model reduction and model comprehension we argue that our reconstruction algorithm can serve a an important primitive for data mining in system biology application 
in this paper we propose an algorithm for polynomial time reinforcement learning in factored markov decision process fmdps the factored optimistic initial model foim algorithm maintains an empirical model of the fmdp in a conventional way and always follows a greedy policy with respect to it model the only trick of the algorithm is that the model is initialized optimistically we prove that with suitable initialization i foim converges to the fixed point of approximate value iteration avi ii the number of step when the agent make non near optimal decision with respect to the solution of avi is polynomial in all relevant quantity iii the per step cost of the algorithm are also polynomial to our best knowledge foim is the first algorithm with these property 
user browsing information particularly their non search related activity reveals important contextual information on the preference and the intent of web user in this paper we expand the use of browsing information for web search ranking and other application with an emphasis on analyzing individual user session for creating aggregate model in this context we introduce clickrank an efficient scalable algorithm for estimating web page and web site importance from browsing information we lay out the theoretical foundation of clickrank based on an intentional surfer model and analyze it property we evaluate it effectiveness for the problem of web search ranking showing that it contributes significantly to retrieval performance a a novel web search feature we demonstrate that the result produced by clickrank for web search ranking are highly competitive with those produced by other approach yet achieved at better scalability and substantially lower computational cost finally we discus novel application of clickrank in providing enriched user web search experience highlighting the usefulness of our approach for non ranking task 
we present a new view of gaussian belief propagation gabp based on a representation of the determinant a a product over orbit of a graph we show that the gabp determinant estimate capture totally backtracking orbit of the graph and consider how to correct this estimate we show that the missing orbit may be grouped into equivalence class corresponding to backtrackless orbit and the contribution of each equivalence class is easily determined from the gabp solution furthermore we demonstrate that this multiplicative correction factor can be interpreted a the determinant of a backtrackless adjacency matrix of the graph with edge weight based on gabp finally an efficient method is proposed to compute a truncated correction factor including all backtrackless orbit up to a specified length 
we study the problem of finding the k most frequent item in a stream of item for the recently proposed max frequency measure based on the property of an item the max frequency of an item is counted over a sliding window of which the length change dynamically besides being parameterless this way of measuring the support of item wa shown to have the advantage of a faster detection of burst in a stream especially if the set of item is heterogeneous the algorithm that wa proposed for maintaining all frequent item however scale poorly when the number of item becomes large therefore in this paper we propose instead of reporting all frequent item to only mine the top k most frequent one first we prove that in order to solve this problem exactly we still need a prohibitive amount of memory at least linear in the number of item yet under some reasonable condition we show both theoretically and empirically that a memory efficient algorithm exists a prototype of this algorithm is implemented and we present it performance w r t memory efficiency on real life data and in controlled experiment with synthetic data 
contour have been established in the biological and computer vision literature a a compact yet descriptive representation of object shape while individual contour provide structure they lack the large spatial support of region segment which lack internal structure we present a method for further grouping of contour in an image using their relationship to the contour of a second related image stereo motion and similarity all provide cue that can aid this task contour that have similar transformation relating them to their matching contour in the second image likely belong to a single group to find match for contour we rely only on shape which applies directly to all three modality without modification in contrast to the specialized approach developed for each independently visually salient contour are extracted in each image along with a set of candidate transformation for aligning subset of them for each transformation group of contour with matching shape across the two image are identified to provide a context for evaluating match of individual contour point across the image the resulting context of contour are used to perform a final grouping on contour in the original image while simultaneously finding match in the related image again by shape matching we demonstrate grouping result on image pair consisting of stereo motion and similar image our method also produce qualitatively better result against a baseline method that doe not use the inferred context 
probabilistic grammatical formalism such a hidden markov model hmms and stochastic context free grammar scfgs have been extensively studied and widely applied in a number of field here we introduce a new algorithmic problem on hmms and scfgs that arises naturally from protein and rna design and which ha not been previously studied the problem can be viewed a an inverse to the one solved by the viterbi algorithm on hmms or by the cky algorithm on scfgs we study this problem theoretically and obtain the first algorithmic result we prove that the problem is np complete even for a letter emission alphabet via a reduction from sat a result that ha implication for the hardness of rna secondary structure design we then develop a number of approach for making the problem tractable in particular for hmms we develop a branch and bound algorithm which can be shown to have fixed parameter tractable worst case running time exponential in the number of state of the hmm but linear in the length of the structure we also show how to cast the problem a a mixed integer linear program 
this paper examines the generalization property of online convex programming algorithm when the loss function is lipschitz and strongly convex our main result is a sharp bound that hold with high probability on the excess risk of the output of an online algorithm in term of the average regret this allows one to use recent algorithm with logarithmic cumulative regret guarantee to achieve fast convergence rate for the excess risk with high probability a a corollary we characterize the convergence rate of p egasos with high probability a recently proposed method for solving the svm optimization problem 
we present a new co clustering problem of image and visual feature the problem involves a set of non object image in addition to a set of object image and feature to be co clustered co clustering is performed in a way that maximises discrimination of object image from non object image thus emphasizing discriminative feature this provides a way of obtaining perceptual joint cluster of object image and feature we tackle the problem by simultaneously boosting multiple strong classifier which compete for image by t heir expertise each boosting classifier is an aggregation of weak learner i e simple visual feature the obtained classifier are useful for object detection ta k which exhibit multimodalities e g multi category and multi view object detection task experiment on a set of pedestrian image and a face data set demonstrate that the method yield intuitive image cluster with associated feature and is much superior to conventional boosting classifier in object detec tion task 
abstract in recent year the l norm ha been proposed for joint regularization in essence this type of regularization aim at extending the l framework for learning sparse model to a setting where the goal is to learn a set of jointly sparse model in this paper we derive a simple and effective projected gradient method for optimization of l regularized problem the main challenge in developing such a method resides on being able to compute efficient projection to thel ball we present an algorithm that work in o nlog n time and o n memory where n is the number of parameter we test our algorithm in a multi task image annotation problem our result show that l lead to better performance than both l and l regularization and that it is is effective in discovering jointly sparse solution 
we present an algorithm for solving a broad class of online resource allocation problem our online algorithm can be applied in environment where abstract job arrive one at a time and one can complete the job by investing time in a number of abstract activity according to some schedule we assume that the fraction of job completed by a schedule is a monotone submodular function of a set of pair v where is the time invested in activity v under this assumption our online algorithm performs near optimally according to two natural metric i the fraction of job completed within time t for some fixed deadline t and ii the average time required to complete each job we evaluate our algorithm experimentally by using it to learn online a schedule for allocating cpu time among solver entered in the sat solver competition 
sutton szepesv ri and maei recently introduced the first temporal difference learning algorithm compatible with both linear function approximation and off policy training and whose complexity scale only linearly in the size of the function approximator although their gradient temporal difference gtd algorithm converges reliably it can be very slow compared to conventional linear td on on policy problem where td is convergent calling into question it practical utility in this paper we introduce two new related algorithm with better convergence rate the first algorithm gtd is derived and proved convergent just a gtd wa but us a different objective function and converges significantly faster but still not a fast a conventional td the second new algorithm linear td with gradient correction or tdc us the same update rule a conventional td except for an additional term which is initially zero in our experiment on small test problem and in a computer go application with a million feature the learning rate of this algorithm wa comparable to that of conventional td this algorithm appears to extend linear td to off policy learning with no penalty in performance while only doubling computational requirement 
this paper address the issue of unsupervised network anomaly detection in recent year network have played more and more critical role since their outage cause serious economic loss it is quite significant to monitor their change over time and to detect anomaly a early a possible in this paper we specifically focus on the management of the whole network in it it is important to detect anomaly which make great impact on the whole network and the other local anomaly should be ignored further when we detect the former anomaly it is required to localize node responsible for them it is challenging to simultaneously perform the above two task taking into account the nonstationarity and strong correlation between node we propose a network anomaly detection method which resolve the above two task in a unified way the key idea of the method are construction of quantity representing feature of a whole network and each node from the same input based on eigen equation compression and incremental anomalousness scoring based on learning the probability distribution of the quantity we demonstrate through the experimental result using two benchmark data set and a simulation data set that anomaly of a whole network and node responsible for them can be detected by the proposed method 
imaging technique such a optical imaging of intrinsic signal photon calcium imaging and voltage sensitive dye imaging can be used to measure the functional organization of visual cortex across different spatial and temporal scale here we present bayesian method based on gaussian process for extracting topographic map from functional imaging data in particular we focus on the estimation of orientation preference map opms from intrinsic signal imaging data we model the underlying map a a bivariate gaussian process with a prior covariance function that reflects known property of opms and a noise covariance adjusted to the data the posterior mean can be interpreted a an optimally smoothed estimate of the map and can be used for model based interpolation of the map from sparse measurement by sampling from the posterior distribution we can get error bar on statistical property such a preferred orientation pinwheel location or pinwheel count finally the use of an explicit probabilistic model facilitates interpretation of parameter and quantitative model comparison we demonstrate our model both on simulated data and on intrinsic signaling data from ferret visual cortex 
we propose a new stopping condition for a support vector machine svm solver which precisely reflects the objective of the leave one out error computation the stopping condition guarantee that the output on an intermediate svm solution is identical to the output of the optimal svm solution with one data point excluded from the training set a simple augmentation of a general svm training algorithm allows one to use a stopping criterion equivalent to the proposed sufficient condition a comprehensive experimental evaluation of our method show consistent speedup of the exact loo computation by our method up to the factor of for the linear kernel the new algorithm can be seen a an example of constructive guidance of an optimization algorithm towards achieving the best attainable expected risk at optimal computational cost 
in several application involving regression or classification along with making prediction it is important to ass how accurate or reliable individual prediction are this is particularly important in case where due to finite resource or domain requirement one want to make decision based only on the most reliable rather than on the entire set of prediction this paper introduces novel and effective way of ranking prediction by their accuracy for problem involving large scale heterogeneous data with a dyadic structure i e where the independent variable can be naturally decomposed into three group associated with two set of element and their combination these approach are based on modeling the data by a collection of localized model learnt while simultaneously partitioning co clustering the data for regression this lead to the concept of certainty lift we also develop a robust predictive modeling technique that identifies and model only the most coherent region of the data to give high predictive accuracy on the selected subset of response value extensive experimentation on real life datasets highlight the utility of our proposed approach 
dimensionality reduction play an important role in many data mining application involving high dimensional data many existing dimensionality reduction technique can be formulated a a generalized eigenvalue problem which doe not scale to large size problem prior work transforms the generalized eigenvalue problem into an equivalent least square formulation which can then be solved efficiently however the equivalence relationship only hold under certain assumption without regularization which severely limit their applicability in practice in this paper an efficient two stage approach is proposed to solve a class of dimensionality reduction technique including canonical correlation analysis orthonormal partial least square linear discriminant analysis and hypergraph spectral learning the proposed two stage approach scale linearly in term of both the sample size and data dimensionality the main contribution of this paper include we rigorously establish the equivalence relationship between the proposed two stage approach and the original formulation without any assumption and we show that the equivalence relationship still hold in the regularization setting we have conducted extensive experiment using both synthetic and real world data set our experimental result confirm the equivalence relationship established in this paper result also demonstrate the scalability of the proposed two stage approach 
the class imbalance problem is encountered in a large number of practical application of machine learning and data mining for example information retrieval and filtering and the detection of credit card fraud it ha been widely realized that this imbalance raise issue that are either nonexistent or le severe compared to balanced class case and often result in a classifier s suboptimal performance this is even more true when the imbalanced data are also high dimensional in such case feature selection method are critical to achieve optimal performance in this paper we propose a new feature selection method feature assessment by sliding threshold fast which is based on the area under a roc curve generated by moving the decision boundary of a single feature classifier with threshold placed using an even bin distribution fast is compared to two commonly used feature selection method correlation coefficient and relevance in estimating feature relief for imbalanced data classification the experimental result obtained on text mining mass spectrometry and microarray data set showed that the proposed method outperformed both relief and correlation method on skewed data set and wa comparable on balanced data set when small number of feature is preferred the classification performance of the proposed method wa significantly improved compared to correlation and relief based method 
the security demand on modern system administration are enormous and getting worse chief among these demand administrator must monitor the continual ongoing disclosure of software vulnerability that have the potential to compromise their system in some way such vulnerability include buffer overflow error improperly validated input and other unanticipated attack modality in over new vulnerability were disclosed well over per week while no enterprise is affected by all of these disclosure administrator commonly face many outstanding vulnerability across the software system they manage vulnerability can be addressed by patch reconfigurations and other workarounds however these action may incur down time or unforeseen side effect thus a key question for system administrator is which vulnerability to prioritize from publicly available database that document past vulnerability we show how to train classifier that predict whether and how soon a vulnerability is likely to be exploited a input our classifier operate on high dimensional feature vector that we extract from the text field time stamp cross reference and other entry in existing vulnerability disclosure report compared to current industry standard heuristic based on expert knowledge and static formula our classifier predict much more accurately whether and how soon individual vulnerability are likely to be exploited 
the problem of optimally managing the collection process by taxation authority is one of prime importance not only for the revenue it brings but also a a mean to administer a fair taxing system the analogous problem of debt collection management in the private sector such a bank and credit card company is also increasingly gaining attention with the recent success in the application of data analytics and optimization to various business area the question arises to what extent such collection process can be improved by use of leading edge data modeling and optimization technique in this paper we propose and develop a novel approach to this problem based on the framework of constrained markov decision process mdp and report on our experience in an actual deployment of a tax collection optimization system at new york state department of taxation and finance ny dtf 
the problem of obtaining the maximum a posteriori map estimate of a discrete random field is of fundamental importance in many area of computer science in this work we build on the tree reweighted message passing trw framework of kolmogorov wainwright et al trw iteratively optimizes the lagrangian dual of a linear programming relaxation for map estimation we show how the dual formulation of trw can be extended to include cycle inequality barahona mahjoub and some recently proposed second order cone soc constraint kumar et al we propose efficient iterative algorithm for solving the resulting duals similar to the method described in kolmogorov these algorithm are guaranteed to converge we test our approach on a large set of synthetic data a well a real data our experiment show that the additional constraint i e cycle inequality and soc constraint provide better result in case where the trw framework fails namely map estimation for non submodular energy function 
the problem of identifying the minimal gene set required to sustain life is of crucial importance in understanding cellular mechanism and designing therapeutic drug this work describes several kernel based solution for predicting essential gene that outperform existing model while using le training data our first solution is based on a semi manually designed kernel derived from the pfam database which includes several pfam domain we then present novel and general domain based sequence kernel that capture sequence similarity with respect to several domain made of large set of protein sequence we show how to deal with the large size of the problem several thousand of domain with individual domain sometimes containing thousand of sequence by representing and efficiently computing these kernel using automaton we report result of extensive experiment demonstrating that they compare favorably with the pfam kernel in predicting protein essentiality while requiring no manual tuning 
many time series prediction method have focused on single step or short term prediction problem due to the inherent difficulty in controlling the propagation of error from one prediction step to the next step yet there is a broad range of application such a climate impact assessment and urban growth planning that require long term forecasting capability for strategic decision making training an accurate model that produce reliable long term prediction would require an extensive amount of historical data which are either unavailable or expensive to acquire for some of these domain there are alternative way to generate potential scenario for the future using computer driven simulation model such a global climate and traffic demand model however the data generated by these model are currently utilized in a supervised learning setting where a predictive model trained on past observation is used to estimate the future value in this paper we present a semi supervised learning framework for long term time series forecasting based on hidden markov model regression a covariance alignment method is also developed to deal with the issue of inconsistency between historical and model simulation data we evaluated our approach on data set from a variety of domain including climate modeling our experimental result demonstrate the efficacy of the approach compared to other supervised learning method for long term time series forecasting 
reinforcement learning rl method based on least square temporal difference lstd have been developed recently and have shown good practical performance however the quality of their estimation ha not been well elucidated in this article we discus lstd based policy evaluation from the new view point of semiparametric statistical inference in fact the estimator can be obtained from a particular estimating function which guarantee it convergence to the true value asymptotically without specifying a model of the environment based on these observation we analyze the asymptotic variance of an lstd based estimator derive the optimal estimating function with the minimum asymptotic estimation variance and derive a suboptimal estimator to reduce the computational burden in obtaining the optimal estimating function 
implicit user feedback including click through and subsequent browsing behavior is crucial for evaluating and improving the quality of result returned by search engine several recent study have used post result browsing behavior including the site visited the number of click and the dwell time on site in order to improve the ranking of search result in this paper we first study user behavior on sponsored search result i e the advertisement displayed by search engine next to the organic result and compare this behavior to that of organic result second to exploit post result user behavior for better ranking of sponsored result we focus on identifying pattern in user behavior and predict expected on site action in future instance in particular we show how post result behavior depends on various property of the query advertisement site and user and build a classifier using property such a these to predict certain aspect of the user behavior additionally we develop a generative model to mimic trend in observed user activity using a mixture of pareto distribution we conduct experiment based on billion of real navigation trail collected by a major search engine s browser toolbar 
many feature selection algorithm have been proposed in the past focusing on improving classification accuracy in this work we point out the importance of stable feature selection for knowledge discovery from high dimensional data and identify two cause of instability of feature selection algorithm selection of a minimum subset without redundant feature and small sample size we propose a general framework for stable feature selection which emphasizes both good generalization and stability of feature selection result the framework identifies dense feature group based on kernel density estimation and treat feature in each dense group a a coherent entity for feature selection an efficient algorithm drag dense relevant attribute group selector is developed under this framework we also introduce a general measure for assessing the stability of feature selection algorithm our empirical study based on microarray data verifies that dense feature group remain stable under random sample hold out and the drag algorithm is effective in identifying a set of feature group which exhibit both high classification accuracy and stability 
in inductive logic programming subsumption is a widely used coverage test unfortunately testing subsumption is np complete which represents a crucial efficiency bottleneck for many relational learner in this paper we present a probabilistic estimator of clause coverage based on a randomized restarted search strategy under a distribution assumption our algorithm can estimate clause coverage without having to decide subsumption for all example we implement this algorithm in program recover on generated graph data and real world datasets we show that recover provides reasonably accurate estimate while achieving dramatic runtimes improvement compared to a state of the art algorithm 
kernel based bayesian method for reinforcement learning rl such a gaussian process temporal difference gptd are particularly promising because they rigorously treat uncertainty in the value function and make it easy to specify prior knowledge however the choice of prior distribution significantly affect the empirical performance of the learning agent and little work ha been done extending existing method for prior model selection to the online setting this paper develops replacing kernel rl an online model selection method for gptd using sequential monte carlo method replacing kernel rl is compared to standard gptd and tile coding on several rl domain and is shown to yield significantly better asymptotic performance for many different kernel family furthermore the resulting kernel capture an intuitively useful notion of prior state covariance that may nevertheless be difficult to capture manually 
we develop a statistical framework for the simultaneous unsupervised segmentation and discovery of visual object category from image database examining a large set of manually segmented scene we show that object frequency and segment size both follow power law distribution which are well modeled by the pitman yor py process this nonparametric prior distribution lead to learning algorithm which discover an unknown set of object and segmentation method which automatically adapt their resolution to each image generalizing previous application of py process we use gaussian process to discover spatially contiguous segment which respect image boundary using a novel family of variational approximation our approach produce segmentation which compare favorably to state of the art method while simultaneously discovering category shared among natural scene 
hierarchical decomposition promise to help scale reinforcement learning algorithm naturally to real world problem by exploiting their underlying structure model based algorithm which provided the first finite time convergence guarantee for reinforcement learning may also play an important role in coping with the relative scarcity of data in large environment in this paper we introduce an algorithm that fully integrates modern hierarchical and model learning method in the standard reinforcement learning setting our algorithm r maxq inherits the efficient model based exploration of the r max algorithm and the opportunity for abstraction provided by the maxq framework we analyze the sample complexity of our algorithm and our experiment in a standard simulation environment illustrate the advantage of combining hierarchy and model 
we consider the problem of optimizing multilabel mrfs which is in general np hard and ubiquitous in low level computer vision one approach for it solution is to formulate it a an integer linear programming and relax the integrality constraint the approach we consider in this paper is to first convert the multi label mrf into an equivalent binary label mrf and then to relax it the resulting relaxation can be efficiently solved using a maximum flow algorithm it solution provides u with a partially optimal labelling of the binary variable this partial labelling is then easily transferred to the multi label problem we study the theoretical property of the new relaxation and compare it with the standard one specifically we compare tightness and characterize a subclass of problem where the two relaxation coincide we propose several combined algorithm based on the technique and demonstrate their performance on challenging computer vision problem 
bayesian network classifier have been widely used for classification problem given a fixed bayesian network structure parameter learning can take two different approach generative and discriminative learning while generative parameter learning is more efficient discriminative parameter learning is more effective in this paper we propose a simple efficient and effective discriminative parameter learning method called discriminative frequency estimate dfe which learns parameter by discriminatively computing frequency from data empirical study show that the dfe algorithm integrates the advantage of both generative and discriminative learning it performs a well a the state of the art discriminative parameter learning method elr in accuracy but is significantly more efficient 
this paper develops a generalized apprenticeship learning protocol for reinforcementlearning agent with access to a teacher who provides policy trace transition and reward observation we characterize sufficient condition of the underlying model for efficient apprenticeship learning and link this criterion to two established learnability class kwik and mistake bound we then construct efficient apprenticeship learning algorithm in a number of domain including two type of relational mdps we instantiate our approach in a software agent and a robot agent that learn effectively from a human teacher literature equivalence query angluin played the role of teacher and were shown to increase the class of learnable concept in the supervised learning setting in this work we expand the apprenticeship protocol of abbeel ng to cover a wider array of model class we consider a teacher with a policy t that can deliver a trace a sequence of state action and reward obtained by executing t from a start state to the learning agent after seeing it behaving suboptimally we note that this scenario is different from inverse reinforcement learning abbeel ng where the reward function is inferred from sequence of state and action instead our agent see the actual reward and transition induced by the teacher s policy and act to try to maximize this observable reward function we characterize a class of reinforcement learning environment for which an agent can guarantee that only a polynomial number of example trace are needed to act near optimally specifically this class includes all kwik learnable domain from the autonomous case and all deterministic domain from the mistake bound mb learning class a set that contains many model that thwart autonomous agent these result generalize earlier theoretical result in a handful of rl representation including flat mdps linear mdps stochastic strip and deterministic oomdps 
the computational role of the local recurrent network in primary visual cortex is still a matter of debate to address this issue we analyze intracellular recording data of cat v which combine measuring the tuning of a range of neuronal property with a precise localization of the recording site in the orientation preference map for the analysis we consider a network model of hodgkin huxley type neuron arranged according to a biologically plausible two dimensional topographic orientation preference map we then systematically vary the strength of the recurrent excitation and inhibition relative to the strength of the afferent input each parametrization give rise to a different model instance for which the tuning of model neuron at different location of the orientation map is compared to the experimentally measured orientation tuning of membrane potential spike output excitatory and inhibitory conductance a quantitative analysis show that the data provides strong evidence for a network model in which the afferent input is dominated by strong balanced contribution of recurrent excitation and inhibition this recurrent regime is close to a regime of instability where strong self sustained activity of the network occurs the firing rate of neuron in the best fitting network is particularly sensitive to small modulation of model parameter which could be one of the functional benefit of a network operating in this particular regime 
training principle for unsupervised learning are often derived from motivation that appear to be independent of supervised learning in this paper we present a simple unification of several supervised and unsupervised training principle through the concept of optimal reverse prediction predict the input from the target label optimizing both over model parameter and any missing label in particular we show how supervised least square principal component analysis k mean clustering and normalized graph cut can all be expressed a instance of the same training principle natural form of semi supervised regression and classification are then automatically derived yielding semi supervised learning algorithm for regression and classification that surprisingly are novel and refine the state of the art these algorithm can all be combined with standard regularizers and made non linear via kernel 
advanced analysis of data stream is quickly becoming a key area of data mining research a the number of application demanding such processing increase online mining when such data stream evolve over time that is when concept drift or change completely is becoming one of the core issue when tackling non stationary concept ensemble of classifier have several advantage over single classifier method they are easy to scale and parallelize they can adapt to change quickly by pruning under performing part of the ensemble and they therefore usually also generate more accurate concept description this paper proposes a new experimental data stream framework for studying concept drift and two new variant of bagging adwin bagging and adaptive size hoeffding tree asht bagging using the new experimental framework an evaluation study on synthetic and real world datasets comprising up to ten million example show that the new ensemble method perform very well compared to several known method 
local based approach is a major category of method for spatial outlier detection sod currently there is a lack of systematic analysis on the statistical property of this framework for example most method assume identical and independent normal distribution i i d normal for the calculated local difference but no justification for this critical assumption have been presented the method detection performance on geostatistic data with linear or nonlinear trend is also not well studied in addition there is a lack of theoretical connection and empirical comparison between local and global based sod approach this paper discus all these fundamental issue under the proposed generalized local statistical gls framework furthermore robust estimation and outlier detection method are designed for the new gls model extensive simulation demonstrated that the sod method based on the gls model significantly outperformed all existing approach when the spatial data exhibit a linear or nonlinear trend 
address standardization is a very challenging task in data cleansing to provide better customer relationship management and business intelligence for customer oriented cooperates million of free text address need to be converted to a standard format for data integration de duplication and householding existing commercial tool usually employ lot of hand craft domain specific rule and reference data dictionary of city state etc these rule work better for the region they are designed however rule based method usually require more human effort to rewrite these rule for each new domain since address data are very irregular and varied with country and region supervised learning method usually are more adaptable than rule based approach however supervised method need large scale labeled training data it is a labor intensive and time consuming task to build a large scale annotated corpus for each target domain for minimizing human effort and the size of labeled training data set we present a free text address standardization method with latent semantic association lasa lasa model is constructed to capture latent semantic association among word from the unlabeled corpus the original term space of the target domain is projected to a concept space using lasa model at first then the address standardization model is active learned from lasa feature and informative sample the proposed method effectively capture the data distribution of the domain experimental result on large scale english and chinese corpus show that the proposed method significantly enhances the performance of standardization with le effort and training data 
there is an exploding amount of user generated content on theweb due to the emergence of web service such a blogger myspace flickr and del icio u the participation of a large number of user in sharing their opinion on the web ha inspired researcher to build an effective information filter by aggregating these independent opinion however given the diverse group of user on the web nowadays the global aggregation of the information may not be of much interest to different group of user in this paper we explore the possibility of computing personalized aggregation over the opinion expressed on the web based on a user s indication of trust over the information source the hope is that by employing such personalized aggregation we can make the recommendation more likely to be interesting to the user we address the challenging scalability issue by proposing an efficient method that utilizes two core technique non negative matrix factorization and threshold algorithm to compute personalized aggregation when there are potentially million of user and million of source within a system we show that through experiment on real life dataset our personalized aggregation approach indeed make a significant difference in the item that are recommended and it reduces the query computational cost significantly often more than while the result of personalized aggregation is kept accurate enough 
recent year have witnessed increased interest in computing strongly correlated pair in very large database most previous study have been focused on static data set however in real world application input data are often dynamic and must continually be updated with such large and growing data set new research effort are expected to develop an incremental solution for correlation computing along this line in this paper we propose a check point algorithm that can efficiently incorporate new transaction for correlation computing a they become available specifically we set a checkpoint to establish a computation buffer which can help u determine an upper bound for the correlation this checkpoint bound can be exploited to identify a list of candidate pair which will be maintained and computed for correlation a new transaction are added into the database however if the total number of new transaction is beyond the buffer size a new upper bound is computed by the new checkpoint and a new list of candidate pair is identified experimental result on real world data set show that check point can significantly reduce the correlation computing cost in dynamic data set and ha the advantage of compacting the use of memory space 
a lot of research in graph mining ha been devoted in the discovery of community most of the work ha focused in the scenario where community need to be discovered with only reference to the input graph however for many interesting application one is interested in finding the community formed by a given set of node in this paper we study a query dependent variant of the community detection problem which we call the community search problem given a graph g and a set of query node in the graph we seek to find a subgraph of g that contains the query node and it is densely connected we motivate a measure of density based on minimum degree and distance constraint and we develop an optimum greedy algorithm for this measure we proceed by characterizing a class of monotone constraint and we generalize our algorithm to compute optimum solution satisfying any set of monotone constraint finally we modify the greedy algorithm and we present two heuristic algorithm that find community of size no greater than a specified upper bound our experimental evaluation on real datasets demonstrates the efficiency of the proposed algorithm and the quality of the solution we obtain 
classifying node in network is a task with a wide range of application it can be particularly useful in anomaly and fraud detection many resource are invested in the task of fraud detection due to the high cost of fraud and being able to automatically detect potential fraud quickly and precisely allows human investigator to work more efficiently many data analytic scheme have been put into use however scheme that bolster link analysis prove promising this work build upon the belief propagation algorithm for use in detecting collusion and other fraud scheme we propose an algorithm called snare social network analysis for risk evaluation by allowing one to use domain knowledge a well a link knowledge the method wa very successful for pinpointing misstated account in our sample of general ledger data with a significant improvement over the default heuristic in true positive rate and a lift factor of up to more than twice that of the default heuristic we also apply snare to the task of graph labeling in general on publicly available datasets we show that with only some information about the node themselves in a network we get surprisingly high accuracy of label not only is snare applicable in a wide variety of domain but it is also robust to the choice of parameter and highly scalable linearly with the number of edge in a graph 
we present tight surrogate regret bound for the class of proper i e fisher consistent loss the bound generalise the margin based bound due to bartlett et al the proof us taylor s theorem and lead to new representation for loss and regret and a simple proof of the integral representation of proper loss we also present a different formulation of a duality result of bregman divergence which lead to a simple demonstration of the convexity of composite loss using canonical link function 
quite a bit is known about minimizing different kind of regret in expert problem and how these regret type relate to type of equilibrium in the multiagent setting of repeated matrix game much le is known about the possible kind of regret in online convex programming problem ocps or about equilibrium in the analogous multiagent setting of repeated convex game this gap is unfortunate since convex game are much more expressive than matrix game and since many important machine learning problem can be expressed a ocps in this paper we work to close this gap we analyze a spectrum of regret type which lie between external and swap regret along with their corresponding equilibrium which lie between coarse correlated and correlated equilibrium we also analyze algorithm for minimizing these regret type a example of our framework we derive algorithm for learning correlated equilibrium in polyhedral convex game and extensive form correlated equilibrium in extensive form game the former is exponentially more efficient than previous algorithm and the latter is the first of it type 
crisis management and disaster recovery have gained immense importance in the wake of recent man and nature inflicted calamity a critical problem in a crisis situation is how to efficiently discover collect organize search and disseminate real time disaster information in this paper we address several key problem which inhibit better information sharing and collaboration between both private and public sector participant for disaster management and recovery we design and implement a web based prototype implementation of a business continuity information network bcin system utilizing the latest advance in data mining technology to create a user friendly internet based information rich service and acting a a vital part of a company s business continuity process specifically information extraction is used to integrate the input data from different source the content recommendation engine and the report summarization module provide user personalized and brief view of the disaster information the community generation module develops spatial clustering technique to help user build dynamic community in disaster currently bcin ha been exercised at miami dade county emergency management 
classical dynamic bayesian network dbns are based on the homogeneous markov assumption and cannot deal with heterogeneity and non stationarity in temporal process various approach to relax the homogeneity assumption have recently been proposed the present paper aim to improve the shortcoming of three recent version of heterogeneous dbns along the following line i avoiding the need for data discretization ii increasing the exibility over a time invariant network structure iii avoiding over exibility and overfltting by introducing a regularization scheme based in inter time segment information sharing the improved method is evaluated on synthetic data and compared with alternative published method on gene expression time series from drosophila melanogaster 
classification is one of the most essential task in data mining unlike other method associative classification try to find all the frequent pattern existing in the input categorical data satisfying a user specified minimum support and or other discrimination measure like minimum confidence or information gain those pattern are used later either a rule for rule based classifier or training feature for support vector machine svm classifier after a feature selection procedure which usually try to cover a many a the input instance with the most discriminative pattern in various manner several algorithm have also been proposed to mine the most discriminative pattern directly without costly feature selection previous empirical result show that associative classification could provide better classification accuracy over many datasets recently many study have been conducted on uncertain data where field of uncertain attribute no longer have certain value instead probability distribution function are adopted to represent the possible value and their corresponding probability the uncertainty is usually caused by noise measurement limit or other possible factor several algorithm have been proposed to solve the classification problem on uncertain data recently for example by extending traditional rule based classifier and decision tree to work on uncertain data in this paper we propose a novel algorithm uharmony which mine discriminative pattern directly and effectively from uncertain data a classification feature rule to help train either svm or rule based classifier since pattern are discovered directly from the input database feature selection usually taking a great amount of time could be avoided completely effective method for computation of expected confidence of the mined pattern used a the measurement of discrimination is also proposed empirical result show that using svm classifier our algorithm uharmony outperforms the state of the art uncertain data classification algorithm significantly with to improvement on average in accuracy on categorical datasets under varying uncertain degree and uncertain attribute number 
in this paper we present a novel exploratory visual analytic system called tiara text insight via automated responsive analytics which combine text analytics and interactive visualization to help user explore and analyze large collection of text given a collection of document tiara first us topic analysis technique to summarize the document into a set of topic each of which is represented by a set of keywords in addition to extracting topic tiara derives time sensitive keywords to depict the content evolution of each topic over time to help user understand the topic based summarization result tiara employ several interactive text visualization technique to explain the summarization result and seamlessly link such result to the original text we have applied tiara to several real world application including email summarization and patient record analysis to measure the effectiveness of tiara we have conducted several experiment our experimental result and initial user feedback suggest that tiara is effective in aiding user in their exploratory text analytic task 
we present a mixture model whose component are restricted boltzmann machine rbms this possibility ha not been considered before because computing the partition function of an rbm is intractable which appears to make learning a mixture of rbms intractable a well surprisingly when formulated a a third order boltzmann machine such a mixture model can be learned tractably using contrastive divergence the energy function of the model capture threeway interaction among visible unit hidden unit and a single hidden discrete variable that represents the cluster label the distinguis hing feature of this model is that unlike other mixture model the mixing proportion are not explicitly parameterized instead they are defined implicitly via the energy function and depend on all the parameter in the model we present result for the mnist and norb datasets showing that the implicit mixture of rbms learns cluster that reflect the class structure in the data the mixture is created by assigning a mixing proportion to each of the component model and it is typically fitted by using the em algorithm that alternat e between two step the e step us property to compute the posterior probability that each datapoint came from each of the component model the posterior is also called the responsibility o f each model for a datapoint the m step us property to update the parameter of each model to raise the responsibility weighted sum of the log probability it assigns to the datapoints the m st ep also change the mixing proportion of the component model to match the proportion of the training data that they are responsible for restricted boltzmann machine model binary data vector using binary latent variable they are considerably more powerful than mixture of multivariate bernoulli model because they allow many of the latent variable to be on simultaneously so the number of alternative latent state vector is exponential in the number of latent variable rather than being linear in this number a it is with a mixture of bernoulli an rbm withn hidden unit can be viewed a a mixture of n bernoulli model one per latent state vector with a lot of parameter s haring between the n component model and with the n mixing proportion being implicitly determined by the same parameter 
this paper aim at discovering community structure in rich medium social network through analysis of time varying multi relational data community structure represents the latent social context of user action it ha important application in information task such a search and recommendation social medium ha several unique challenge a in social medium the context of user action is constantly changing and co evolving hence the social context contains time evolving multi dimensional relation b the social context is determined by the available system feature and is unique in each social medium website in this paper we propose metafac metagraph factorization a framework that extract community structure from various social context and interaction our work ha three key contribution metagraph a novel relational hypergraph representation for modeling multi relational and multi dimensional social data an efficient factorization method for community extraction on a given metagraph an on line method to handle time varying relation through incremental metagraph factorization extensive experiment on real world social data collected from the digg social medium website suggest that our technique is scalable and is able to extract meaningful community based on the social medium context we illustrate the usefulness of our framework through prediction task we outperform baseline method including aspect model and tensor analysis by an order of magnitude 
multi view learning ha become a hot topic during the past few year in this paper we first characterize the sample complexity of multi view active learning under the expansion assumption we get an exponential improvement in the sample complexity from usual to log requiring neither strong assumption on data distribution such a the data is distributed uniformly over the unit sphere in rd nor strong assumption on hypothesis class such a linear separator through the origin we also give an upper bound of the error rate when the expansion assumption doe not hold then we analyze the combination of multi view active learning and semi supervised learning and get a further improvement in the sample complexity finally we study the empirical behavior of the two paradigm which verifies that the combination of multi view active learning and semi supervised learning is efficient 
we study the behavior of block regularization for multivariate regression where a k dimensional response vector is regressed upon a fixed set of p covariates the problem of support union recovery is to recover the subset of covariates that are active in at least one of the regression problem studying this problem under high dimensional scaling where the problem parameter a well a sample size n tend to infinity simultaneously our main result is to show that exact recovery is possible once the order parameter given by n p s n b log p s exceeds a critical threshold here n is the sample size p is the ambient dimension of the regression model s is the size of the union of support and b is a sparsity overlap function that measure a combination of the sparsity and overlap of the k regression coefficient vector that constitute the model this sparsity overlap function reveals that block regularization for multivariate regression never harm performance relative to a naive approach and can yield substantial improvement in sample complexity up to a factor of k when the regression vector are suitably orthogonal relative to the design we complement our theoretical result with simulation that demonstrate the sharpness of the result even for relatively small problem 
metal binding is important for the structural and functional characterization of protein previous prediction effort have only focused on bonding state i e deciding which protein residue act a metal ligand in some binding site identifying the geometry of metal binding site i e deciding which residue are jointly involved in the coordination of a metal ion is a new prediction problem that ha been never attempted before from protein sequence alone in this paper we formulate it in the framework of learning with structured output our solution relies on the fact that from a graph theoretical perspective metal binding ha the algebraic property of a matroid enabling the application of greedy algorithm for learning structured output on a data set of non redundant metalloproteins we obtained precision recall level of correct ligand ion assignment which improves to in the setting where the metal binding state is known 
query result clustering ha recently attracted a lot of attention to provide user with a succinct overview of relevant result however little work ha been done on organizing the query result for object level search object level search result clustering is challenging because we need to support diverse similarity notion over object specific feature such a the price and weight of a product of heterogeneous domain to address this challenge we propose a hybrid subspace clustering algorithm called hydra algorithm hydra capture the user perception of diverse similarity notion from million of web page and disambiguates different sens using feature based subspace locality measure our proposed solution by combining wisdom of crowd and wisdom of data achieves robustness and efficiency over existing approach we extensively evaluate our proposed framework and demonstrate how to enrich user experience in object level search using a real world product search scenario 
neural probabilistic language model nplms have been shown to be competitive with and occasionally superior to the widely used n gram language model the main drawback of nplms is their extremely long training and testing time morin and bengio have proposed a hierarchical language model built around a binary tree of word which wa two order of magnitude faster than the nonhierarchical model it wa based on however it performed considerably worse than it non hierarchical counterpart in spite of using a wo rd tree created using expert knowledge we introduce a fast hierarchical language model along with a simple feature based algorithm for automatic construction of word tree from the data we then show that the resulting model can outperform non hierarchical neural model a well a the best n gram model 
in the context of large space mdps with linear value function approximation we introduce a new approximate version of policy iteration bertsekas ioffe a method that generalizes value iteration and policy iteration with a parameter our approach called least squarespolicy iteration generalizes lspi lagoudakis parr which make efficient use of training sample compared to classical temporaldifferences method the motivation of our work is to exploit theparameter within the least square context and without having to generate new sample at each iteration or to know a model of the mdp we provide a performance bound that show the soundness of the algorithm we show empirically on a simple chain problem and on the tetri game that thisparameter act a a bias variance trade off that may improve the convergence and the performance of the policy obtained 
this paper study efficient mining of negative correlation that pace in collaboration a collaborating negative correlation is a negative correlation between two set of variable rather than traditionally between a pair of variable it signifies a synchronized value rise or fall of all variable within one set whenever all variable in the other set go jointly at the opposite trend the time complexity is exponential in mining the high efficiency of our algorithm is attributed to two factor i the transformation of the original data into a bipartite graph database and ii the mining of transpose closure from a wide transactional database applying to a yeast gene expression data we evaluate by using pearson s correlation coefficient and p value the biological relevance of collaborating negative correlation a an example among many real life domain 
practical data mining rarely fall exactly into the supervised learning scenario rather the growing amount of unlabeled data pose a big challenge to large scale semi supervised learning ssl we note that the computational intensiveness of graph based ssl arises largely from the manifold or graph regularization which in turn lead to large model that are difficult to handle to alleviate this we proposed the prototype vector machine pvm a highly scalable graph based algorithm for large scale ssl our key innovation is the use of prototype vector for efficient approximation on both the graph based regularizer and model representation the choice of prototype are grounded upon two important criterion they not only perform effective low rank approximation of the kernel matrix but also span a model suffering the minimum information loss compared with the complete model we demonstrate encouraging performance and appealing scaling property of the pvm on a number of machine learning benchmark data set 
we address the problem of learning classifier for a large number of task we derive a solution that produce resampling weight which match the pool of all example to the target distribution of any given task our work is motivated by the problem of predicting the outcome of a therapy attempt for a patient who carry an hiv virus with a set of observed genetic property such prediction need to be made for hundred of possible combination of drug some of which use similar biochemical mechanism multi task learning enables u to make prediction even for drug combination with few or no training example and substantially improves the overall prediction accuracy 
in recent year the number of patent filed by the business enterprise in the technology industry are growing rapidly thus providing unprecedented opportunity for knowledge discovery in patent data one important task in this regard is to employ data mining technique to rank patent in term of their potential to earn money through licensing availability of such ranking can substantially reduce enterprise ip intellectual property management cost unfortunately the existing software system in the ip domain do not address this task directly through our research we build a patent ranking software named coa claim originality analysis that rate a patent based on it value by measuring the recency and the impact of the important phrase that appear in the claim section of a patent experiment show that coa produce meaningful ranking when comparing it with other indirect patent evaluation metric citation count patent status and attorney s rating in reallife setting this tool wa used by beta tester in the ibm ip department lawyer found it very useful in patent rating specifically in highlighting potentially valuable patent in a patent cluster in this article we describe the ranking technique and system architecture of coa we also present the result that validate it effectiveness 
the precision recall pr curve is a widely used visual tool to evaluate the performance of scoring function in regard to their capacity to discriminate between two population the purpose of this paper is to examine both theoretical and practical issue related to the statistical estimation of pr curve based on classification data consistency and asymptotic normality of the empirical counterpart of the pr curve in sup norm are rigorously established eventually the issue of building confidence band in the pr space is considered and a specific resampling procedure based on a smoothed and truncated version of the empirical distribution of the data is promoted argument of theoretical and computational nature are presented to explain why such a bootstrap is preferable to a naive bootstrap in this setup 
given a spatial data set placed on an n x n grid our goal is to find the rectangular region within which subset of the data set exhibit anomalous behavior we develop algorithm that given any user supplied arbitrary likelihood function conduct a likelihood ratio hypothesis test lrt over each rectangular region in the grid rank all of the rectangle based on the computed lrt statistic and return the top few most interesting rectangle to speed this process we develop method to prune rectangle without computing their associated lrt statistic 
we propose an unbounded depth hierarchical bayesian nonparametric model for discrete sequence data this model can be estimated from a single training sequence yet share statistical strength between subsequent symbol predictive distribution in such a way that predictive performance generalizes well the model build on a specific parameterization of an unbounded depth hierarchical pitman yor process we introduce analytic marginalization step using coagulation operator to reduce this model to one that can be represented in time and space linear in the length of the training sequence we show how to perform inference in such a model without truncation approximation and introduce fragmentation operator necessary to do predictive inference we demonstrate the sequence memoizer by using it a a language model achieving state of the art result 
stroke is the third leading cause of death and the principal cause of serious long term disability in the united state accurate prediction of stroke is highly valuable for early intervention and treatment in this study we compare the cox proportional hazard model with a machine learning approach for stroke prediction on the cardiovascular health study chs dataset specifically we consider the common problem of data imputation feature selection and prediction in medical datasets we propose a novel automatic feature selection algorithm that selects robust feature based on our proposed heuristic conservative mean combined with support vector machine svms our proposed feature selection algorithm achieves a greater area under the roc curve auc a compared to the cox proportional hazard model and l regularized cox feature selection algorithm furthermore we present a margin based censored regression algorithm that combine the concept of margin based classifier with censored regression to achieve a better concordance index than the cox model overall our approach outperforms the current state of the art in both metric of auc and concordance index in addition our work ha also identified potential risk factor that have not been discovered by traditional approach our method can be applied to clinical prediction of other disease where missing data are common and risk factor are not well understood 
health insurance cost across the world have increased alarmingly in recent year a major cause of this increase are payment error made by the insurance company while processing claim these error often result in extra administrative effort to re process or rework the claim which account for up to of the administrative staff in a typical health insurer we describe a system that help reduce these error using machine learning technique by predicting claim that will need to be reworked generating explanation to help the auditor correct these claim and experiment with feature selection concept drift and active learning to collect feedback from the auditor to improve over time we describe our framework problem formulation evaluation metric and experimental result on claim data from a large u health insurer we show that our system result in an order of magnitude better precision hit rate over existing approach which is accurate enough to potentially result in over million in saving for a typical insurer we also describe interesting research problem in this domain a well a design choice made to make the system easily deployable across health insurance company 
a fundamental open question in the analysis of social network is to understand the interplay between similarity and social tie people are similar to their neighbor in a social network for two distinct reason first they grow to resemble their current friend due to social influence and second they tend to form new link to others who are already like them a process often termed selection by sociologist while both factor are present in everyday social process they are in tension social influence can push system toward uniformity of behavior while selection can lead to fragmentation a such it is important to understand the relative effect of these force and this ha been a challenge due to the difficulty of isolating and quantifying them in real setting we develop technique for identifying and modeling the interaction between social influence and selection using data from online community where both social interaction and change in behavior over time can be measured we find clear feedback effect between the two factor with rising similarity between two individual serving in aggregate a an indicator of future interaction but with similarity then continuing to increase steadily although at a slower rate for long period after initial interaction we also consider the relative value of similarity and social influence in modeling future behavior for instance to predict the activity that an individual is likely to do next is it more useful to know the current activity of their friend or of the people most similar to them 
we present multi task structure learning for gaussian graphical model we discus uniqueness and boundedness of the optimal solution of the maximization problem a block coordinate descent method lead to a provably convergent algorithm that generates a sequence of positive deflnite solution thus we reduce the original problem into a sequence of strictly convex regularized quadratic minimization subproblems we further show that this subproblem lead to the continuous quadratic knapsack problem for which very e cient method exist finally we show promising result in a dataset that capture brain function of cocaine addicted and control subject under condition of monetary reward 
extensive labeled data for image annotation system which learn to assign class label to image region is difficult to obtain we explore a h ybrid model framework for utilizing partially labeled data that integrates a generative topic model for image appearance with discriminative label prediction we propose three alternative formulation for imposing a spatial smoothness prior on the image label test of the new model and some baseline approach on three real image datasets demonstrate the effectiveness of incorporating t he latent structure 
we introduce a novel framework for estimating vector field using sparse basis field expansion s flex the notion of basis field which are an extension of scalar basis function arises naturally in our framework from a rotational invariance requirement we consider a regression setting a well a inverse problem all variant discussed lead to second order cone programming formulation while our framework is generally applicable to any type of vector field we focus in this paper on applying it to solving the eeg meg inverse problem it is shown that significantly more precise and neurophysiologically more plausible location and shape estimate of cerebral current source from eeg meg measurement become possible with our method when comparing to the state of the art 
we provide a theoretical analysis of the chance accuracy of large collection of classifier we show that on problem with small number of example some classifier can perform well by random chance and we derive a theorem to explicitly calculate this accuracy we use this theorem to provide a principled feature selection criterion for sparse high dimensional problem we evaluate this method on microarray and fmri datasets and show that it performs very close to the optimal accuracy obtained from an oracle we also show that on the fmri dataset this technique chooses relevant feature successfully while another state of the art method the false discovery rate fdr completely fails at standard significance level 
the online service industry is a rapidly growing industry with a worldwide online ad market projected to grow from billion in to billion in of which will come from display advertising and from search advertising online service division osd within microsoft is a leader in the consumer cloud space today with a strong portfolio of a set of mutually reinforcing business search portal advertising they are supported by a shared foundational asset of intent knowledge store and a shared technology platform supporting large scale data and high performance system msn portal and bing search generate the content traffic and data that make for an exciting fertile environment for large scale data mining practice and system development our advertiser are thus given more valuable targeting opportunity and better roi which in turn provide better economics usability data and allows for a higher quality service for our advertiser and experience for our user the ability to transform data into meaningful actionable insight is an important source of competitive advantage for osd the data mining initiative within the division continue to strive for excellence around the following goal actionable insight through deep data analysis data mining and data modeling at scale and with speed increased productivity from deployed large scale data system and tool improved product and service development and decision making gained from effective measurement and experimentation and a mature data culture in product team that made the above possible with many technical and data challenge ahead of u we are committed to utilizing our huge data asset well to understand the need intent and behavior of our user for the purpose of serving them better 
although tagging ha become increasingly popular in online image and video sharing system tag are known to be noisy ambiguous incomplete and subjective these factor can seriously affect the precision of a social tag based web retrieval system therefore improving the precision performance of these social tag based web retrieval system ha become an increasingly important research topic to this end we propose a shared subspace learning framework to leverage a secondary source to improve retrieval performance from a primary dataset this is achieved by learning a shared subspace between the two source under a joint nonnegative matrix factorization in which the level of subspace sharing can be explicitly controlled we derive an efficient algorithm for learning the factorization analyze it complexity and provide proof of convergence we validate the framework on image and video retrieval task in which tag from the labelme dataset are used to improve image retrieval performance from a flickr dataset and video retrieval performance from a youtube dataset this ha implication for how to exploit and transfer knowledge from readily available auxiliary tagging resource to improve another social web retrieval system our shared subspace learning framework is applicable to a range of problem where one need to exploit the strength existing among multiple and heterogeneous datasets 
we derive a generalization bound for multi classification scheme based on grid clustering in categorical parameter product space grid clustering partition the parameter space in the form of a cartesian product of partition for each of the parameter the derived bound provides a mean to evaluate clustering solution in term of the generalization power of a built on classifier for classification based on a single feature the bound serf to find a globally optimal classification rule comparison of the generalization power of individual feature can then be used for feature ranking our experiment show that in this role the bound is much more precise than mutual information or normalized correlation index 
web content analysis often ha two sequential and separate step web classification to identify the target web page and web information extraction to extract the metadata contained in the target web page this decoupled strategy is highly ineffective since the error in web classification will be propagated to web information extraction and eventually accumulate to a high level in this paper we study the mutual dependency between these two step and propose to combine them by using a model of conditional random field crfs this model can be used to simultaneously recognize the target web page and extract the corresponding metadata systematic experiment in our project ofcourse for online course search show that this model significantly improves the f value for both of the two step we believe that our model can be easily generalized to many web application 
the task of linking database is an important step in an increasing number of data mining project because linked data can contain information that is not available otherwise or that would require time consuming and expensive collection of specific data the aim of linking is to match and aggregate all record that refer to the same entity one of the major challenge when linking large database is the efficient and accurate classification of record pair into match and non match while traditionally classification wa based on manually set threshold or on statistical procedure many of the more recently developed classification method are based on supervised learning technique they therefore require training data which is often not available in real world situation or ha to be prepared manually an expensive cumbersome and time consuming process the author ha previously presented a novel two step approach to automatic record pair classification in the first step of this approach training example of high quality are automatically selected from the compared record pair and used in the second step to train a support vector machine svm classifier initial experiment showed the feasibility of the approach achieving result that outperformed k mean clustering in this paper two variation of this approach are presented the first is based on a nearest neighbour classifier while the second improves a svm classifier by iteratively adding more example into the training set experimental result show that this two step approach can achieve better classification result than other unsupervised approach 
active and semi supervised learning are important technique when labeled data are scarce recently a method wa suggested for combining active learning with a semi supervised learning algorithm that us gaussian field and harmonic function this classifier is relational in nature it relies on having the data presented a a partially labeled graph also known a a within network learning problem this work showed yet again that empirical risk minimization erm wa the best method to find the next instance to label and provided an efficient way to compute erm with the semi supervised classifier the computational problem with erm is that it relies on computing the risk for all possible instance if we could limit the candidate that should be investigated then we can speed up active learning considerably in the case where the data is graphical in nature we can leverage the graph structure to rapidly identify instance that are likely to be good candidate for labeling this paper describes a novel hybrid approach of using of community finding and social network analytic centrality measure to identify good candidate for labeling and then using erm to find the best instance in this candidate set we show on real world data that we can limit the erm computation to a fraction of instance with comparable performance 
we present a new family of linear time algorithm for string comparison with mismatch under the string kernel framework based on sufficient statistic our algorithm improve theoretical complexity bound of existing approach while scaling well in sequence alphabet size the number of allowed mismatch and the size of the dataset in particular on large alphabet and under loose mismatch constraint our algorithm are several order of magnitude faster than the existing algorithm for string comparison under the mismatch similarity measure we evaluate our algorithm on synthetic data and real application in music genre classification protein remote homology detection and protein fold prediction the scalability of the algorithm allows u to consider complex sequence transformation modeled using longer string feature and larger number of mismatch leading to a state of the art performance with significantly reduced running time 
given a quarter of petabyte click log data how can we estimate the relevance of each url for a given query in this paper we propose the bayesian browsing model bbm a new modeling technique with following advantage a it doe exact inference b it is single pas and parallelizable c it is effective we present two set of experiment to test model effectiveness and efficiency on the first set of over million search instance of million distinct query bbm out performs the state of the art competitor by in log likelihood while being time faster on the second click log set spanning a quarter of petabyte data we showcase the scalability of bbm we implemented it on a commercial mapreduce cluster and it took only hour to compute the relevance for billion distinct query url pair 
we introduce a novel pattern discovery methodology for event history data focusing explicitly on the detailed temporal relationship between pair of event at the core is a graphical statistical approach to summarising and visualising event history data which contrast the observed to the expected incidence of the event of interest before and after an index event thus pattern discovery is not restricted to a specific time window of interest but encompasses extended part of the underlying event history in order to effectively screen large collection of event history data for interesting temporal relationship we introduce a new measure of temporal association the proposed measure contrast the observed to expected ratio in a time period of interest to that in a pre defined control period an important feature of both the observed to expected graph itself and the measure of association is a statistical shrinkage towards the null hypothesis of no association this provides protection against spurious association and is an extension of the statistical shrinkage successfully applied to large scale screening for association between event in cross sectional data such a large collection of adverse drug reaction report we demonstrate the usefulness of the proposed pattern discovery methodology by a set of example from a collection of over two million patient record in the united kingdom the identified pattern include temporal relationship between drug prescription and medical event suggestive of persistent or transient risk of adverse event a well a temporal relationship between prescription of different drug 
how can we automatically spot all outstanding observation in a data set this question arises in a large variety of application e g in economy biology and medicine existing approach to outlier detection suffer from one or more of the following drawback the result of many method strongly depend on suitable parameter setting being very difficult to estimate without background knowledge on the data e g the minimum cluster size or the number of desired outlier many method implicitly assume gaussian or uniformly distributed data and or their result is difficult to interpret to cope with these problem we propose coco a technique for parameter free outlier detection the basic idea of our technique relates outlier detection to data compression outlier are object which can not be effectively compressed given the data set to avoid the assumption of a certain data distribution coco relies on a very general data model combining the exponential power distribution with independent component we define an intuitive outlier factor based on the principle of the minimum description length together with an novel algorithm for outlier detection an extensive experimental evaluation on synthetic and real world data demonstrates the benefit of our technique availability the source code of coco and the data set used in the experiment are available at http www db ifi lmu de forschung kdd boehm coco 
research in animal learning and behavioral neuroscience ha distinguished between two form of action control a habit based form which relies on stored actio n value and a goal dir ected form which forecast and compare action outcome based on a model of the environment while habit based control ha been the subject of extensive computational research the computational principle underlying goal directed control in animal have so far received le attention in the present paper we advance a computational framework for goal directed control in animal and human we take three empirically motivated point a founding premise neuron in dorsolateral prefrontal cortex represent action policy neuron in orbitofrontal cortex represent reward and neural computation across domain can be appropriately understood a performing structured probabilistic inference on a purely computational level the resulting account relates closely to previous work using bayesian inference to solve markov decision problem but extends this work by introducing a new algorithm which provably converges on optimal plan on a cognitive and neuroscientific level the theory provides a unifying framework for several different form of goal directed action selection placing emphasis on a novel form within which orbitofrontal reward representation directly drive policy selection 
bartlett et al recently proved that a ground condition for convex surrogate classification calibration tie up the minimization of the surrogate and classification risk and left a an important problem the algorithmic question about the minimization of these surrogate in this paper we propose an algorithm which provably minimizes any classification calibrated surrogate strictly convex and differentiable a set whose loss span the exponential logistic and squared loss with boosting type guaranteed convergence rate under a weak learning assumption a particular subclass of these surrogate that we call balanced convex surrogate ha a key rationale that tie it to maximum likelihood estimation zerosum game and the set of loss that satisfy some of the most common requirement for loss in supervised learning we report experiment on more than readily available domain of flavor of the algorithm that shed light on new surrogate and the potential of data dependent strategy to tune surrogate 
we propose a method to train a cascade of classifier by simultaneously optimizing all it stage the approach relies on the idea of optimizing soft cascade in particular instead of optimizing a deterministic hard cascade we optimize a stochastic soft cascade where each stage accepts or reject sample according to a probability distribution induced by the previous stage specific classifier the overall system accuracy is maximized while explicitly controlling the expected cost for feature acquisition experimental result on three clinically relevant problem show the effectiveness of our proposed approach in achieving the desired tradeoff between accuracy and feature acquisition cost 
a key aspect of semantic image segmentation is to integrate local and global feature for the prediction of local segment label we present an approach to multi class segmentation which combine two method for this integration a conditional random field crf which couple to local image feature and an image classification method which considers global feature the crf follows the approach of reynolds murphy and is based on an unsupervised multi scale pre segmentation of the image into patch where patch label correspond to the random variable of the crf the output of the classifier is used to constraint this crf we demonstrate and compare the approach on a standard semantic segmentation data set 
most algorithm for solving markov decision process rely on a discount factor which ensures their convergence it is generally assumed that using an artificially low discount factor will improve the convergence rate while sacrificing the solution quality we however demonstrate that using an artificially low discount factor may significantly improve the solution quality when used in approximate dynamic programming we propose two explanation of this phenomenon the first justification follows directly from the standard approximation error bound using a lower discount factor may decrease the approximation error bound however we also show that these bound are loose thus their decrease doe not entirely justify the improved solution quality we thus propose another justification when the reward are received only sporadically a in the case of tetri we can derive tighter bound which support a significant improvement in the solution quality with a decreased discount factor 
policy gradient approach are a powerful instrument for learning how to interact with the environment existing approach have focused on propositional and continuous domain only without extensive feature engineering it is difficult if not impossible to apply them within structured domain in which e g there is a varying number of object and relation among them in this paper we describe a non parametric policy gradient approach called nppg that overcomes this limitation the key idea is to apply friedmann s gradient boosting policy are represented a a weighted sum of regression model grown in an stage wise optimization employing off the shelf regression learner nppg can deal with propositional continuous and relational domain in a unified way our experimental result show that it can even improve on established result 
aiming towards the development of a general clustering theory we discus abstract axiomatization for clustering in this respect we follow up on the work of kleinberg that showed an impossibility result for such axiomatization we argue that an impossibility result is not an inherent feature of clustering but rather to a large extent it is an artifact of the specific formalism used in a opposed to previous work focusing on clustering function we propose to address clustering quality measure a the object to be axiomatized we show that principle like those formulated in kleinberg s axiom can be readily expressed in the latter framework without leading to inconsistency a clustering quality measure cqm is a function that given a data set and it partition into cluster return a non negative real number representing how strong or conclusive the clustering is we analyze what clustering quality measure should look like and introduce a set of requirement axiom for such measure our axiom capture the principle expressed by kleinberg s axiom while retaining consistency we propose several natural clustering quality measure all satisfying the proposed axiom in addition we analyze the computational complexity of evaluating the quality of a given clustering and show that for the proposed cqms it can be computed in polynomial time 
scalable similarity search is the core of many large scale learning or data mining application recently many research result demonstrate that one promising approach is creating compact and efficient hash code that preserve data similarity by efficient we refer to the low correlation and thus low redundancy among generated code however most existing hash method are designed only for vector data in this paper we develop a new hashing algorithm to create efficient code for large scale data of general format with any kernel function including kernel on vector graph sequence set and so on starting with the idea analogous to spectral hashing novel formulation and solution are proposed such that a kernel based hash function can be explicitly represented and optimized and directly applied to compute compact hash code for new sample of general format moreover we incorporate efficient technique such a nystrom approximation to further reduce time and space complexity for indexing and search making our algorithm scalable to huge data set another important advantage of our method is the ability to handle diverse type of similarity according to actual task requirement including both feature similarity and semantic similarity like label consistency we evaluate our method using both vector and non vector data set at a large scale up to million sample our comprehensive result show the proposed method outperforms several state of the art approach for all the task with a significant gain for most task 
recommender system provide user with personalized suggestion for product or service these system often rely on collaborating filtering cf where past transaction are analyzed in order to establish connection between user and product the two more successful approach to cf are latent factor model which directly profile both user and product and neighborhood model which analyze similarity between product or user in this work we introduce some innovation to both approach the factor and neighborhood model can now be smoothly merged thereby building a more accurate combined model further accuracy improvement are achieved by extending the model to exploit both explicit and implicit feedback by the user the method are tested on the netflix data result are better than those previously published on that dataset in addition we suggest a new evaluation metric which highlight the difference among method based on their performance at a top k recommendation task 
language comprehension in human is significantly constrai ned by memory yet rapid highly incremental and capable of utilizing a wide r ange of contextual information to resolve ambiguity and form expectation about future input in contrast most of the leading psycholinguistic model and fi elded algorithm for natural language parsing are non incremental have run time superlinear in input length and or enforce structural locality constraint on probabilistic dependency between event we present a new limited memory model of sentence comprehension which involves an adaptation of the particle filter a se quential monte carlo method to the problem of incremental parsing we show that this model can reproduce classic result in online sentence comprehension and that it naturally provides the first rational account of an outstanding proble m in psycholinguistics in which the preferred alternative in a syntactic ambiguity seems to grow more attractive over time even in the absence of strong disambiguating information 
we analyze a massive social network gathered from the record of a large mobile phone operator with more than a million user and ten of million of call we examine the distribution of the number of phone call per customer the total talk minute per customer and the distinct number of calling partner per customer we find that these distribution are skewed and that they significantly deviate from what would be expected by power law and lognormal distribution to analyze our observed distribution of number of call distinct call partner and total talk time we propose powertrack a method which fit a lesser known but more suitable distribution namely the double pareto lognormal dpln distribution to our data and track it parameter over time using powertrack we find that our graph change over time in a way consistent with a generative process that naturally result in the dpln distribution we observe furthermore we show that this generative process lends itself to a natural and appealing social wealth interpretation in the context of social network such a ours we discus the application of those result to our model and to forecasting 
the problem of choosing fast implementation for a class of recursive algorithm such a the fast fourier transforms can be formulated a an optimization problem over the language generated by a suitably defined grammar we propose a novel algorithm that solves this problem by reducing it to maximizing an objective function over the sink of a directed acyclic graph this algorithm valuates node using monte carlo and grows a subgraph in the most promising direction by considering local maximum k armed bandit when used inside an adaptive linear transform library it cut down the search time by an order of magnitude compared to the existing algorithm in some case the performance of the implementation found is also increased by up to which is of considerable practical importance since it consequently improves the performance of all application using the library 
markov logic network mlns are an expressive representation for statistical relational learning that generalizes both first order logic and graphical model existing method for learning the logical structure of an mln are not discriminative however many relational learning problem involve specific target predicate that must be inferred from given background information we found that existing mln method perform very poorly on several such ilp benchmark problem and we present improved discriminative method for learning mln clause and weight that outperform existing mln and traditional ilp method 
language and image understanding are two major goal of artificial intelligence which can both be conceptually formulated in term of parsing the input signal into a hierarchical representation natural language researcher have made great progress by exploiting the d structure of language to design efficient polynomialtime parsing algorithm by contrast the two dimensional nature of image make it much harder to design efficient image parser and the form of the hierarchical representation is also unclear attempt to adapt representation and algorithm from natural language have only been partially successful in this paper we propose a hierarchical image model him for d image parsing which output image segmentation and object recognition this him is represented by recursive segmentation and recognition template in multiple layer and ha advantage for representation inference and learning firstly the him ha a coarse to fine representation which is capable of capturing long range dependency and exploiting different level of contextual information secondly the structure of the him allows u to design a rapid inference algorithm based on dynamic programming which enables u to parse the image rapidly in polynomial time thirdly we can learn the him efficiently in a discriminative manner from a labeled dataset we demonstrate that him outperforms other state of the art method by evaluation on the challenging public msrc image dataset finally we sketch how the him architecture can be extended to model more complex image phenomenon 
we describe the design and implementation of a high performance cloud that we have used to archive analyze and mine large distributed data set by a cloud we mean an infrastructure that provides resource and or service over the internet a storage cloud provides storage service while a compute cloud provides compute service we describe the design of the sector storage cloud and how it provides the storage service required by the sphere compute cloud we also describe the programming paradigm supported by the sphere compute cloud sector and sphere are designed for analyzing large data set using computer cluster connected with wide area high performance network for example gb s we describe a distributed data mining application that we have developed using sector and sphere finally we describe some experimental study comparing sector sphere to hadoop 
we consider the task of reinforcement learning with linear value function approximation temporal difference algorithm and in particular the least square temporal difference lstd algorithm provide a method for learning the parameter of the value function but when the number of feature is large this algorithm can over fit to the data and is computationally expensive in this paper we propose a regularization framework for the lstd algorithm that overcomes these difficulty in particular we focus on the case of l regularization which is robust to irrelevant feature and also serf a a method for feature selection although the l regularized lstd solution cannot be expressed a a convex optimization problem we present an algorithm similar to the least angle regression lars algorithm that can efficiently compute the optimal solution finally we demonstrate the performance of the algorithm experimentally 
we present a novel feature selection algorithm for the k mean clustering problem our algorithm is randomized and assuming an accuracy parameter selects and appropriately rescales in an unsupervised manner k log k feature from a dataset of arbitrary dimension we prove that if we run any approximate k mean algorithm on the feature selected using our method we can find a approximate partition with high probability 
despite the pervasiveness of network a model for real world system ranging from the internet the world wide web to gene regulation and scientific collaboration only a limited number of metric capable of characterizing these system are available the existing metric for characterizing network have broad specificity and lack the selectivity for many application the purpose of this paper is to identify and critically evaluate a metric termed bridging centrality which is highly selective for identifying bridge in network the property of bridge are unique compared to the other network metric for a diverse range of data set we found that network are highly susceptible to disruption but robust to loss structural integrity upon targeted deletion of bridging node a novel graph clustering approach termed bridge cut utilizing bridging edge a module boundary is also proposed the module identified by the bridge cut algorithm are more effective than the other graph clustering method thus bridging centrality is a network metric with unique property that may aid in network analysis from element to group level in various area including system biology and national security application 
this paper intends to provide some insight of a scientific problem how likely one s interest can be inferred from his her social connection friend friend friend degree friend etc is bird of a feather flock together a norm we do not consider the friending activity on online social networking site instead we conduct this study by implementing a privacy preserving large distribute social sensor system in a large global it company to capture the multifaceted activity of people including communication e g email instant messaging etc and web activity e g social bookmarking file sharing blogging etc these activity occupy the majority of employee time in work and thus provide a high quality approximation to the real social connection of employee in the workplace context in addition to such informal network we investigated the formal network such a their hierarchical structure a well a the demographic profile data such a geography job role self specified interest etc because user id matching across multiple source on the internet is very difficult and most user activity log have to be anonymized before they are processed no prior study could collect comparable multifaceted activity data of individual that make this study unique in this paper we present a technique to predict the inference quality by utilizing network analysis and network autocorrelation modeling of informal and formal network and regression model to predict user interest inference quality from network characteristic we verify our finding with experiment on both implicit user interest indicated by the content of communication or web activity and explicit user interest specified in user profile we demonstrate that the inference quality prediction increase the inference quality of implicit interest by and inference quality of explicit interest by up to 
one important approach for knowledge discovery and data mining is to estimate unobserved variable because latent variable can indicate hidden specific property of observed data the latent factor model assumes that each item in a record ha a latent factor the co occurrence of item can then be modeled by latent factor in document modeling a record indicates a document represented a a bag of word meaning that the order of word is ignored an item indicates a word and a latent factor indicates a topic latent dirichlet allocation lda is a widely used bayesian topic model applying the dirichlet distribution over the latent topic distribution of a document having multiple topic lda assumes that latent topic i e discrete latent variable are distributed according to a multinomial distribution whose parameter are generated from the dirichlet distribution lda also model a word distribution by using a multinomial distribution whose parameter follows the dirichlet distribution this dirichlet multinomial setting however cannot capture the power law phenomenon of a word distribution which is known a zipf s law in linguistics we therefore propose a novel topic model using the pitman yor py process called the py topic model the py topic model capture two property of a document a power law word distribution and the presence of multiple topic in an experiment using real data this model outperformed lda in document modeling in term of perplexity 
we propose a generalization of multilabel classification that we refer to a graded multilabel classification the key idea is that instead of requesting a yes no answer to the question of class membership or say relevance of a class label for an instance we allow for a graded membership of an instance measured on an ordinal scale of membership degree this extension is motivated by practical application in which a graded or partial class membership is natural apart from introducing the basic setting we propose two general strategy for reducing graded multilabel problem to conventional multilabel classification problem moreover we address the question of how to extend performance metric commonly used in multilabel classification to the graded setting and present first experimental result 
virtually all method of learning dynamic system from data start from the same basic assumption that the learning algorithm will be provided with a sequence or trajectory of data generated from the dynamic system in this paper we consider the case where the data is not sequenced the learning algorithm is presented a set of data point from the system s operation but with no temporal ordering the data are simply drawn a individual disconnected point while making this assumption may seem absurd at first glance we observe that many scientific modeling task have exactly this property in this paper we restrict our attention to learning linear discrete time model we propose several algorithm for learning these model based on optimizing approximate likelihood function and test the method on several synthetic data set 
a broad class of boosting algorithm can be interpreted a performing coordinate wise gradient descent to minimize some potential function of the margin of a data set this class includes adaboost logitboost and other widely used and well studied booster in this paper we show that for a broad class of convex potential function any such boosting algorithm is highly susceptible to random classification noise we do this by showing that for any such booster and any nonzero random classification noise rate there is a simple data set of example which is efficiently learnable by such a booster if there is no noise but which cannot be learned to accuracy better than if there is random classification noise at rate this negative result is in contrast with known branching program based booster which do not fall into the convex potential function framework and which can provably learn to high accuracy in the presence of random classification noise 
compressive sensing c combine sampling and compression into a single subnyquist linear measurement process for sparse and compressible signal in this paper we extend the theory of c to include signal that are concisely represented in term of a graphical model in particular we use markov random field mrfs to represent sparse signal whose nonzero coefficien t are clustered our new model based recovery algorithm dubbed lattice matching pursuit lamp stably recovers mrf modeled signal using many fewer measurement and computation than the current state of the art algorithm 
a large body of past work ha focused on the first order tree based lp relaxation for the map problem in markov random field this paper develops a family of super linearly convergent lp solver based on proximal minimization scheme using bregman divergence that exploit the underlying graphical structure and so scale well to large problem all of our algorithm have a double loop character with the outer loop corresponding to the proximal sequence and an inner loop of cyclic bregman divergence used to compute each proximal update the inner loop update are distributed and respect the graph structure and thus can be cast a message passing algorithm we establish various convergence guarantee for our algorithm illustrate their performance and also present rounding scheme with provable optimality guarantee 
the biojournalmonitor is a decision support system for the analysis of trend and topic in the biomedical literature it main goal is to identify potential diagnostic and therapeutic biomarkers for specific disease several data source are continuously integrated to provide the user with up to date information on current research in this field state of the art text mining technology are deployed to provide added value on top of the original content including named entity detection relation extraction classification clustering ranking summarization and visualization we present two novel technology that are related to the analysis of temporal dynamic of text archive and associated ontology currently the mesh ontology is used to annotate the scientific article entering the pubmed database with medical term both the maintenance of the ontology a well a the annotation of new article is performed largely manually we describe how probabilistic topic model can be used to annotate recent article with the most likely mesh term this provides our user with a competitive advantage because when searching for mesh term article are found long before they are manually annotated we further present a study on how to predict the inclusion of new term in the mesh ontology the result suggest that early prediction of emerging trend is possible the trend ranking function are deployed in our system to enable interactive search for the hottest new trend relating to a disease 
collaborative filtering is the most popular approach to build recommender system and ha been successfully employed in many application however it cannot make recommendation for so called cold start user that have rated only a very small number of item in addition these method do not know how confident they are in their recommendation trust based recommendation method assume the additional knowledge of a trust network among user and can better deal with cold start user since user only need to be simply connected to the trust network on the other hand the sparsity of the user item rating force the trust based approach to consider rating of indirect neighbor that are only weakly trusted which may decrease it precision in order to find a good trade off we propose a random walk model combining the trust based and the collaborative filtering approach for recommendation the random walk model allows u to define and to measure the confidence of a recommendation we performed an evaluation on the epinions dataset and compared our model with existing trust based and collaborative filtering method 
many scalable data mining task rely on active learning to provide the most useful accurately labeled instance however what if there are multiple labeling source oracle or expert with different but unknown reliability with the recent advent of inexpensive and scalable online annotation tool such a amazon s mechanical turk the labeling process ha become more vulnerable to noise and without prior knowledge of the accuracy of each individual labeler this paper address exactly such a challenge how to jointly learn the accuracy of labeling source and obtain the most informative label for the active learning task at hand minimizing total labeling effort more specifically we present iethresh interval estimate threshold a a strategy to intelligently select the expert s with the highest estimated labeling accuracy iethresh estimate a confidence interval for the reliability of each expert and filter out the one s whose estimated upper bound confidence interval is below a threshold which jointly optimizes expected accuracy mean and need to better estimate the expert s accuracy variance our framework is flexible enough to work with a wide range of different noise level and outperforms baseline such a asking all available expert and random expert selection in particular iethresh achieves a given level of accuracy with le than half the query issued by all expert labeling and le than a third the query required by random expert selection on datasets such a the uci mushroom one the result show that our method naturally balance exploration and exploitation a it gain knowledge of which expert to rely upon and selects them with increasing frequency 
abstraction of complex longer motor task into simpler elemental movement enables human and animal to exhibit motor skill which have not yet been matched by robot human intuitively decompose complex motion into smaller simpler segment for example when describing simple movement like drawing a triangle with a pen we can easily name the basic step of this movement surprisingly such abstraction have rarely been used in artificial motor skill learning algorithm these algorithm typically choose a new action such a a torque or a force at a very fast time scale a a result both policy and temporal credit assignment problem become unnecessarily complex often beyond the reach of current machine learning method we introduce a new framework for temporal abstraction in reinforcement learning rl i e rl with motion template we present a new algorithm for this framework which can learn high quality policy by making only few abstract decision 
studying the association between quantitative phenotype such a height or weight and single nucleotide polymorphism snp is an important problem in biology to understand underlying mechanism of complex phenotype it is often necessary to consider joint genetic effect across multiple snp anova analysis of variance test is routinely used in association study important finding from studying gene gene snp pair interaction are appearing in the literature however the number of snp can be up to million evaluating joint effect of snp is a challenging task even for snp pair moreover with large number of snp correlated permutation procedure is preferred over simple bonferroni correction for properly controlling family wise error rate and retaining mapping power which dramatically increase the computational cost of association study in this paper we study the problem of finding snp pair that have significant association with a given quantitative phenotype we propose an efficient algorithm fastanova for performing anova test on snp pair in a batch mode which also support large permutation test we derive an upper bound of snp pair anova test which can be expressed a the sum of two term the first term is based on single snp anova test the second term is based on the snp and independent of any phenotype permutation furthermore snp pair can be organized into group each of which share a common upper bound this allows for maximum reuse of intermediate computation efficient upper bound estimation and effective snp pair pruning consequently fastanova only need to perform the anova test on a small number of candidate snp pair without the risk of missing any significant one extensive experiment demonstrate that fastanova is order of magnitude faster than the brute force implementation of anova test on all snp pair 
the discovery of causal relationship between a set of observed variable is a fundamental problem in science for continuous valued data linear acyclic causal model with additive noise are often used because these model are well understood and there are well known method to fit them to data in reality of course many causal relationship are more or le nonlinear raising some doubt a to the applicability and usefulness of purely linear method in this contribution we show that the basic linear framework can be generalized to nonlinear model in this extended framework nonlinearities in the data generating process are in fact a blessing rather than a curse a they typically provide information on the underlying causal system and allow more aspect of the true data generating mechanism to be identified in addition to theoretical result we show simulation and some simple real data experiment illustrating the identification power provided by nonlinearities 
one of the main problem in probabilistic grammatical inference consists in inferring a stochastic language i e a probability distribution in some class of probabilistic model from a sample of string independently drawn according to a fixed unknown target distribution p here we consider the class of rational stochastic language composed of stochastic language that can be computed by multiplicity automaton which can be viewed a a generalization of probabilistic automaton rational stochastic language p have a useful algebraic characterization all the mapping up v p uv lie in a finite dimensional vector subspace vp of the vector space composed of all real valued function defined over hence a first step in the grammatical inference process can consist in identifying the subspace vp in this paper we study the possibility of using principal component analysis to achieve this task we provide an inference algorithm which computes an estimate of this space and then build a multiplicity automaton which computes an estimate of the target distribution we prove some theoretical property of this algorithm and we provide result from numerical simulation that confirm the relevance of our approach 
projected and subspace clustering algorithm search for cluster of point in subset of attribute projected clustering computes several disjoint cluster plus outlier so that each cluster exists in it own subset of attribute subspace clustering enumerates cluster of point in all subset of attribute typically producing many overlapping cluster one problem of existing approach is that their objective are stated in a way that is not independent of the particular algorithm proposed to detect such cluster a second problem is the definition of cluster density based on user defined parameter which make it hard to ass whether the reported cluster are an artifact of the algorithm or whether they actually stand out in the data in a statistical sense we propose a novel problem formulation that aim at extracting axis parallel region that stand out in the data in a statistical sense the set of axis parallel statistically significant region that exist in a given data set is typically highly redundant therefore we formulate the problem of representing this set through a reduced non redundant set of axis parallel statistically significant region a an optimization problem exhaustive search is not a viable solution due to computational infeasibility and we propose the approximation algorithm statpc our comprehensive experimental evaluation show that statpc significantly outperforms existing projected and subspace clustering algorithm in term of accuracy 
partially observable markov decision process pomdps have succeeded in planning domain that require balancing action that increase an agent s knowledge and action that increase an agent s reward unfortunately most pomdps are defined with a large number of parameter which are difficult to specify only from domain knowledge in this paper we present an approximation approach that allows u to treat the pomdp model parameter a additional hidden state in a model uncertainty pomdp coupled with model directed query our planner actively learns good policy we demonstrate our approach on several pomdp problem 
we propose a new penalty function which when used a regularization for empirical risk minimization procedure lead to sparse estimator the support of the sparse vector is typically a union of potentially overlapping group of co variate defined a priori or a set of covariates which tend to be connected to each other when a graph of covariates is given we study theoretical property of the estimator and illustrate it behavior on simulated and breast cancer gene expression data 
codebook based representation are widely employed in the classification of complex object such a image and document most previous codebook based method construct a single codebook via clustering that map a bag of low level feature into a fixed length histogram that describes the distribution of these feature this paper describes a simple yet effective framework for learning multiple non redundant codebooks that produce surprisingly good result in this framework each codebook is learned in sequence to extract discriminative information that wa not captured by preceding codebooks and their corresponding classifier we apply this framework to two application domain visual object categorization and document classification experiment on large classification task show substantial improvement in performance compared to a single codebook or codebooks learned in a bagging style 
the need to meaningfully combine set of ranking often come up when one deal with ranked data although a number of heuristic and supervised learning approach to rank aggregation exist they require domain knowledge or supervised ranked data both of which are expensive to acquire in order to address these limitation we propose a mathematical and algorithmic framework for learning to aggregate partial ranking without supervision we instantiate the framework for the case of combining permutation and combining top k list and propose a novel metric for the latter experiment in both scenario demonstrate the effectiveness of the proposed formalism 
this demo present spam miner an online system designed for real time monitoring and characterization of spam traffic over the internet our system is based on high level abstraction such a spam message attribute spam campaign and spamming strategy a campaign is a cluster of message that are generated from a single message template campaign identification is a challenging problem because it ha to handle spammer evolution while seeking for a spam similarity function that combine different message characteristic and for strategy that efficiently process large volume of spam moreover spam campaign need to be identified on the fly to allow incident response team and security specialist to react to the threat adequately spam miner address campaign identification a a data clustering problem and campaign are identified dynamically using a novel incremental approach based on the concept of frequent pattern tree spam miner is being used by nic br brazilian network information center and mined more than million spam message detecting meaningful cluster and pattern and helping the organization to better understand the spam problem in brazil and how the brazilian internet infrastructure is being abused by spammer 
current research in indexing and mining time series data ha produced many interesting algorithm and representation however the algorithm and the size of data considered have generally not been representative of the increasingly massive datasets encountered in science engineering and business domain in this work we show how a novel multi resolution symbolic representation can be used to index datasets which are several order of magnitude larger than anything else considered in the literature our approach allows both fast exact search and ultra fast approximate search we show how to exploit the combination of both type of search a sub routine in data mining algorithm allowing for the exact mining of truly massive real world datasets containing million of time series 
online ad server attempt to find best ad to serve for a given triggering user event the performance of ad may be measured in several way we suggest a formulation in which the ad network try to maximize revenue subject to relevance constraint we describe several algorithm for ad selection and review their complexity we tested these algorithm using microsoft ad network from october to february over billion impression million combination of trigger with ad and a number of algorithm were tested over this period we discover curious difference between ad server aimed at revenue versus clickthrough rate 
inspired by co training many multi view semi supervised kernel method implement the following idea find a function in each of multiple reproducing kernel hilbert space rkhss such that a the chosen function make similar prediction on unlabeled example and b the average prediction given by the chosen function performs well on labeled example in this paper we construct a single rkhs with a data dependent co regularization norm that reduces these approach to standard supervised learning the reproducing kernel for this rkhs can be explicitly derived and plugged into any kernel method greatly extending the theoretical and algorithmic scope of coregularization in particular with this development the rademacher complexity bound for co regularization given in rosenberg bartlett follows easily from wellknown result furthermore more refined bound given by localized rademacher complexity can also be easily applied we propose a co regularization based algorithmic alternative to manifold regularization belkin et al sindhwani et al a that lead to major empirical improvement on semi supervised task unlike the recently proposed transductive approach of yu et al our rkhs formulation is truly semi supervised and naturally extends to unseen test data 
physic based simulation code are widely used in science and engineering to model complex system that would be infeasible to study otherwise such code provide the highest fidelity representation of system behavior but are often so slow to run that insight into the system is limited for example conducting an exhaustive sweep over a d dimensional input parameter space with k step along each dimension requires kd simulation trial translating into kd cpu day for one of our current simulation an alternative is directed exploration in which the next simulation trial are cleverly chosen at each step given the result of previous trial supervised learning technique svm kde gp are applied to build up simplified predictive model of system behavior these model are then used within an active learning framework to identify the most valuable trial to run next several active learning strategy are examined including a recently proposed information theoretic approach performance is evaluated on a set of thirteen synthetic oracle which serve a surrogate for the more expensive simulation and enable the experiment to be replicated by other researcher 
in this paper we consider the problem of combining link and content analysis for community detection from networked data such a paper citation network and word wide web most existing approach combine link and content information by a generative model that generates both link and content via a shared set of community membership these generative model have some shortcoming in that they failed to consider additional factor that could affect the community membership and isolate the content that are irrelevant to community membership to explicitly address these shortcoming we propose a discriminative model for combining the link and content analysis for community detection first we propose a conditional model for link analysis and in the model we introduce hidden variable to explicitly model the popularity of node second to alleviate the impact of irrelevant content attribute we develop a discriminative model for content analysis these two model are unified seamlessly via the community membership we present efficient algorithm to solve the related optimization problem based on bound optimization and alternating projection extensive experiment with benchmark data set show that the proposed framework significantly outperforms the state of the art approach for combining link and content analysis for community detection 
previous study of non parametric kernel npk learning usually reduce to solving some semi definite programming sdp problem by a standard sdp solver however time complexity of standard interior point sdp solver could be a high a o n such intensive computation cost prohibits npk learning applicable to real application even for data set of moderate size in this paper we propose an efficient approach to npk learning from side information referred to a simplenpkl which can efficiently learn non parametric kernel from large set of pairwise constraint in particular we show that the proposed simplenpkl with linear loss ha a closed form solution that can be simply computed by the lanczos algorithm moreover we show that the simplenpkl with square hinge loss can be re formulated a a saddle point optimization task which can be further solved by a fast iterative algorithm in contrast to the previous approach our empirical result show that our new technique achieves the same accuracy but is significantly more efficient and scalable 
semi supervised support vector machine s vms typically directly estimate the label assignment for the unlabeled instance this is often inefficient even with recent advance in the efficient training of the supervised svm in this paper we show that s vms with knowledge of the mean of the class label of the unlabeled data is closely related to the supervised svm with known label on all the unlabeled data this motivates u to first estimate the label mean of the unlabeled data two version of the mean vm which work by maximizing the margin between the label mean are proposed the first one is based on multiple kernel learning while the second one is based on alternating optimization experiment show that both of the proposed algorithm achieve highly competitive and sometimes even the best performance a compared to the state of the art semi supervised learner moreover they are more efficient than existing s vms 
this work characterizes the generalization ability of algorithm whose prediction are linear in the input vector to this end we provide sharp bound for rademacher and gaussian complexity of constrained linear class which directly lead to a number of generalization bound this derivation provides simplified proof of a number of corollary including risk bound for linear prediction including setting where the weight vector are constrained by either l or l constraint margin bound including both l and l margin along with more general notion based on relative entropy a proof of the pac bayes theorem and upper bound on l covering number with lp norm constraint and relative entropy constraint in addition to providing a unified analysis the result herein provide some of the sharpest risk and margin bound interestingly our result show that the uniform convergence rate of empirical risk minimization algorithm tightly match the regret bound of online learning algorithm for linear prediction up to a constant factor of 
this paper explores online learning approach for detecting malicious web site those involved in criminal scam using lexical and host based feature of the associated url we show that this application is particularly appropriate for online algorithm a the size of the training data is larger than can be efficiently processed in batch and because the distribution of feature that typify malicious url is changing continuously using a real time system we developed for gathering url feature combined with a real time source of labeled url from a large web mail provider we demonstrate that recently developed online algorithm can be a accurate a batch technique achieving classification accuracy up to over a balanced data set 
the united state national basketball association nba is one of the most popular sport league in the world and is well known for moving a millionary betting market that us the countless statistical data generated after each game to feed the wager this lead to the existence of a rich historical database that motivates u to discover implicit knowledge in it in this paper we use complex network statistic to analyze the nba database in order to create model to represent the behavior of team in the nba result of complex network based model are compared with box score statistic such a point rebound and assist per game we show the box score statistic play a significant role for only a small fraction of the player in the league we then propose new model for predicting a team success based on complex network metric such a clustering coefficient and node degree complex network based model present good result when compared to box score statistic which underscore the importance of capturing network relationship in a community such a the nba 
the most common environment in which ranking is used take a very specific form user sequentially generate query in a digital library for each query ranking is applied to order a set of relevant item from which the user selects his favorite this is the case when ranking search result for page on the world wide web or for merchandize on an e commerce site in this work we present a new online ranking algorithm called noregret klrank our algorithm is designed to use clickthrough information a it is provided by the user to improve future ranking decision more importantly we show that it long term average performance will converge to the best rate achievable by any competing fixed ranking policy selected with the benefit of hindsight we show how to ensure that this property continues to hold a new item are added to the set thus requiring a richer class of ranking policy finally our empirical result show that while in some context noregret klrank might be considered conservative a greedy variant of this algorithm actually outperforms many popular ranking algorithm 
in this paper we investigate how given an image similar image sharing the same global description can help with unsupervised scene segmentation in contrast to recent work in semantic alignment of scene we allow an input image to be explainedbypartial matchesof similar scene this allows forum betterexplanation of the input scene we perform mrf based segmentation that optimizes over match while respectingboundaryinformation therecoveredsegmentsarethen used to re querya large database of image to retrieve better match for the target region we show improved performance in detecting the principal occluding and contact boundary for the scene over previous method on data gathered from the labelme database 
functional segregation and integration are fundamental characteristic of the human brain studying the connectivity among segregated region and the dynamic of integrated brain network ha drawn increasing interest a very controversial yet fundamental issue in these study is how to determine the best functional brain region or roi region of interest for individual essentially the computed connectivity pattern and dynamic of brain network are very sensitive to the location size and shape of the roi this paper present a novel methodology to optimize the location of an individual s roi in the working memory system our strategy is to formulate the individual roi optimization a a group variance minimization problem in which group wise functional and structural connectivity pattern and anatomic profile are defined a optimization constraint the optimization problem is solved via the simulated annealing approach our experimental result show that the optimized roi have significantly improved consistency in structural and functional profile across subject and have more reasonable localization and more consistent morphological and anatomic profile 
a common representation used in text categorization is the bag of word model aka unigram model learning with this particular representation involves typically some preprocessing e g stopwords removal stemming this result in one explicit tokenization of the corpus in this work we introduce a logistic regression approach where learning involves automatic tokenization this allows u to weaken the a priori required knowledge about the corpus and result in a tokenization with variable length word or character n gram a basic token we accomplish this by solving logistic regression using gradient ascent in the space of all ngrams we show that this can be done very efficiently using a branch and bound approach which chooses the maximum gradient ascent direction projected onto a single dimension i e candidate feature although the space is very large our method allows u to investigate variable length n gram learning we demonstrate the efficiency of our approach compared to state of the art classifier used for text categorization such a cyclic coordinate descent logistic regression and support vector machine 
contextual advertising on web page ha become very popular recently and it pose it own set of unique text mining challenge often advertiser wish to either target or avoid some specific content on web page which may appear only in a small part of the page learning for these targeting task is difficult since most training page are multi topic and need expensive human labeling at the sub document level for accurate training in this paper we investigate way to learn for sub document classification when only page level label are available these label only indicate if the relevant content exists in the given page or not we propose the application of multiple instance learning to this task to improve the effectiveness of traditional method we apply sub document classification to two different problem in contextual advertising one is sensitive content detection where the advertiser want to avoid content relating to war violence pornography etc even if they occur only in a small part of a page the second problem involves opinion mining from review site the advertiser want to detect and avoid negative opinion about their product when positive negative and neutral sentiment co exist on a page in both these scenario we present experimental result to show that our proposed system is able to get good block level labeling for free and improve the performance of traditional learning method 
the success of kernel method including support vector machine svms strongly depends on the design of appropriate kernel while initially kernel were designed in order to handle fixed length data their extension to unordered variable length data became more than necessary for real pattern recognition problem such a object recognition and bioinformatics we focus in this paper on object recognition using a new type of kernel referred to a context dependent object seen a constellation of local feature interest point region etc are matched by minimizing an energy function mixing a fidelity term which measure the quality of feature matching a neighborhood criterion which capture the object geometry and a regularization term we will show that the fixed point of this energy is a context dependent kernel cdk which also satisfies the mercer condition experiment conducted on object recognition show that when plugging our kernel in svms we clearly outperform svms with context free kernel 
low rank approximation of the adjacency matrix of a graph are essential in finding pattern such a community and detecting anomaly additionally it is desirable to track the low rank structure a the graph evolves over time efficiently and within limited storage real graph typically have thousand or million of node but are usually very sparse however standard decomposition such a svd do not preserve sparsity this ha led to the development of method such a cur and cmd which seek a non orthogonal basis by sampling the column and or row of the sparse matrix however these approach will typically produce overcomplete base which waste both space and time in this paper we propose the family of colibri method to deal with these challenge our version for static graph colibri s iteratively find a non redundant basis and we prove that it ha no loss of accuracy compared to the best competitor cur and cmd while achieving significant saving in space and time on real data colibri s requires much le space and is order of magnitude faster in proportion to the square of the number of non redundant column additionally we propose an efficient update algorithm for dynamic time evolving graph colibri d our evaluation on a large real network traffic dataset show that colibri d is over time faster than the best published competitor cmd 
almost all successful machine learning algorithm and cognitive model require powerful representation capturing the feature that are relevant to a particular problem we draw on recent work in nonparametric bayesian statistic to define a rational model of human feature learning that form a featural representation from raw sensory data without pre specifying the number of feature by comparing how the human perceptual system and our rational model use distributional and category information to infer feature representation we seek to identify some of the force that govern the process by which people separate and combine sensory primitive to form feature 
the infinite hidden markov model is a non parametric extension of the widely used hidden markov model our paper introduces a new inference algorithm for the infinite hidden markov model called beam sampling beam sampling combine slice sampling which limit the number of state considered at each time step to a finite number with dynamic programming which sample whole state trajectory efficiently our algorithm typically outperforms the gibbs sampler and is more robust we present application of ihmm inference using the beam sampler on changepoint detection and text prediction problem 
to optimize unknown fitness function we present natural evolution strategy a novel algorithm that constitutes a principled alternative to standard stochastic search method it maintains a multinormal distribution on the set of solution candidate the natural gradient is used to update the distribution s parameter in the direction of higher expected fitness by efficiently calculating the inverse of the exact fisher information matrix whereas previous method had to use approximation other novel aspect of our method include optimal fitness baseline and importance mixing a procedure adjusting batch with minimal number of fitness evaluation the algorithm yield competitive result on a number of benchmark 
we introduce the first temporal difference learning algorithm that is stable with linear function approximation and off policy training for any finite markov decision process behavior policy and target policy and whose complexity scale linearly in the number of parameter we consider an i i d policy evaluation setting in which the data need not come from on policy experience the gradient temporal difference gtd algorithm estimate the expected update vector of the td algorithm and performs stochastic gradient descent on it l norm we prove that this algorithm is stable and convergent under the usual stochastic approximation condition to the same least square solution a found by the lstd but without lstd s quadratic computational complexity gtd is online and incremental and doe not involve multiplying by product of likelihood ratio a in importance sampling method 
